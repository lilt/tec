How To Add Authentication to Your App with Flask-Login
Allowing users to log in to your app is one of the most common features you'll add to your web application.
This article will cover how to add authentication to your Flask app with the Flask-Login package.
3576
Animated gif of the Flask app and login box
We're going to build some sign-up and login pages that allow users to log in and access protected pages that users who aren't logged in can't see.
We'll grab information from the user model and display it on our protected pages when the user logs in to simulate what a profile would look like.
We will cover the following in this article:
Use the Flask-Login library for session management
Use the built-in Flask utility for hashing passwords
Add protected pages to our app for logged in users only
Use Flask-SQLAlchemy to create a user model
Create sign up and login forms for our users to create accounts and log in
Flash error messages back to users when something goes wrong
Use information from the user's account to display on the profile page
The source code for this project is available on GitHub.
Python installed on a local environment.
Knowledge of Basic Linux Navigation and File Management is helpful but not required.
Familiarity with an editor like Visual Studio Code is helpful but not required.
Our app will use the Flask app factory pattern with blueprints.
We'll have one blueprint that handles everything auth related, and we'll have another for our regular routes, which include the index and the protected profile page.
In a real app, you can break down the functionality in any way you like, but the solution covered here will work well for this tutorial.
Here is a diagram to provide a sense of what your project's file structure will look like once you have completed the tutorial:
As we progress through the tutorial, we will create these directories and files.
Step 1 - Installing Packages
There are three main packages we need for our project:
Flask
Flask-Login: to handle the user sessions after authentication
Flask-SQLAlchemy: to represent the user model and interface with our database
We'll be using SQLite to avoid having to install any extra dependencies for the database.
First, we will start with creating the project directory:
Next, we need to navigate to the project directory:
You will want to create a Python environment if you don't have one.
Depending on how Python was installed on your machine, your commands will look similar to:
Note: You can consult the tutorial relevant to your local environment for setting up venv.
Run the following commands from your virtual environment to install the needed packages:
Now that you've installed the packages, you are ready to create the main app file.
Step 2 - Creating the Main App File
Let's start by creating a project directory:
The first file we will work on will be the _ _ init _ _ .py file for our project:
This file will have the function to create our app, which will initialize the database and register our blueprints.
At the moment, this won't do much, but it will be needed for the rest of our app. We need to initialize SQLAlchemy, set some configuration values, and register our blueprints here.
Now that we have the main app file, we can start adding in our routes.
Step 3 - Adding Routes
For our routes, we'll use two blueprints.
For our main blueprint, we'll have a home page (/) and profile page (/ profile) for after we log in.
If the user tries to access the profile page without being logged in, they'll be sent to the login route.
For our auth blueprint, we'll have routes to retrieve both the login page (/ login) and the sign-up page (/ sign-up).
We'll also have routes for handling the POST requests from both of those two routes.
Finally, we'll have a logout route (/ logout) to log out an active user.
For the time being, we will define login, signup, and logout with simple returns.
We will revisit them at a later step and update them with desired functionality.
First, create main.py for your main _ blueprint:
Next, create auth.py for your auth _ blueprint:
In a terminal, you can set the FLASK _ APP and FLASK _ DEBUG values:
The FLASK _ APP environment variable instructs Flask how to load the app. It should point to where create _ app is located.
For our needs, we will be pointing to the project directory.
The FLASK _ DEBUG environment variable is enabled by setting it to 1. This will enable a debugger that will display application errors in the browser.
Ensure that you are in the < ^ > flask _ auth _ app < ^ > directory and then run the project:
Now, in a web browser, you should be able to navigate to the five possible URLs and see the text returned that was defined in auth.py and main.py.
For example, visiting localhost: 5000 / profile displays: Profile:
Screenshot of project at localhost port 5000 in browser
Now that we have verified that our routes are behaving as expected, we can move on to creating templates.
Step 4 - Creating Templates
Let's go ahead and create the templates that are used in our app. This is the first step before we can implement the actual login functionality.
Our app will use four templates:
index.html
profile.html
login.html
signup.html
We'll also have a base template that will have code common to each of the pages.
In this case, the base template will have navigation links and the general layout of the page.
Let's create them now.
First, create a templates directory under the project directory:
Then create base.html:
Next, add the following code to the base.html file:
This code will create a series of menu links to each page of the application and an area where content will appear.
Note: Behind the scenes, we are using Bulma to handle styling and layout.
For a deeper dive into Bulma, consider reading the official Bulma documentation.
Next, create templates / index.html:
Add the following code to the newly create file to add content to the page:
This code will create a basic index page with a title and subtitle.
Next, create templates / login.html:
This code generates a login page with fields for Email and Password.
There is also a checkbox to "remember" a logged in session.
Next, create templates / signup.html:
Add the following code to create a sign-up page with fields for email, name, and password:
Next, create templates / profile.html:
Add this code to create a simple page with a title that is hardcoded to welcome Anthony:
Later, we will add code to dynamically greet any user.
Once you've added the templates, we can update the return statements in each of the routes we have to return the templates instead of the text.
Next, update main.py by modifying the import line and the routes for index and profile:
Now you will update auth.py by modifying the import line and routes for login and signup:
Once you've made these changes, here is what the sign-up page looks like if you navigate to / sign-up:
Sign up page at / signup
You should be able to see the pages for /, / login, and / profile as well.
We'll leave / logout alone for now because it won't display a template when it's done.
Step 5 - Creating User Models
Our user model represents what it means for our app to have a user.
We'll have fields for an email address, password, and name.
In your application, you may decide you want much more information to be stored per user.
You can add things like birthday, profile picture, location, or any user preferences.
Models created in Flask-SQLAlchemy are represented by classes that then translate to tables in a database.
The attributes of those classes then turn into columns for those tables.
Let's go ahead and create that user model:
This code creates a user model with columns for an id, email, password, and name:
Now that you've created a user model, you can move on to configuring your database.
Step 6 - Configuring the Database
As stated in the Prerequisites, we'll be using a SQLite database.
We could create a SQLite database on our own, but let's have Flask-SQLAlchemy do it for us.
We already have the path of the database specified in the _ _ init _ _ .py file, so we just need to tell Flask-SQLAlchemy to create the database in the Python REPL.
If you stop your app and open up a Python REPL, we can create the database using the create _ all method on the db object.
Ensure that you are still in the virtual environment and in the < ^ > flask _ auth _ app < ^ > directory.
Note: If using the Python interpreter is new to you, you can consult the official documentation.
You will now see a db.sqlite file in your project directory.
This database will have our user table in it.
Step 7 - Setting Up the Authorization Function
For our sign up function, we're going to take the data the user types into the form and add it to our database.
Before we add it, we need to make sure the user doesn't already exist in the database.
If it doesn't, then we need to make sure we hash the password before placing it into the database because we don't want our passwords stored in plaintext.
Let's start by adding a second function to handle the POST form data. In this function, we will gather the data passed from the user first.
Create the function and add a redirect to the bottom.
This will provide a user experience of a successful signup and being directed to the Login Page.
Update auth.py by modifying the import line and implementing signup _ post:
Now, let's add the rest of the code necessary for signing up a user.
To start, we'll have to use the request object to get the form data.
Continue to update auth.py by adding imports and implementing signup _ post:
Note: Storing passwords in plaintext is considered a poor security practice.
You will generally want to utilize a complex hashing algorithm and a password salt to keep passwords secure.
Step 8 - Testing the Sign Up Method
Now that we have the sign-up method done, we should be able to create a new user.
Use the form to create a user.
There are two ways you can verify if the sign up worked: you can use a database viewer to look at the row that was added to your table, or you can try signing up with the same email address again, and if you get an error, you know the first email was saved properly.
So let's take that approach.
We can add code to let the user know the email already exists and tell them to go to the login page.
By calling the flash function, we will send a message to the next request, which in this case, is the redirect.
The page we land on will then have access to that message in the template.
First, we add the flash before we redirect back to our sign-up page.
To get the flashed message in the template, we can add this code above the form. This will display the message directly above the form.
Sign up box showing a message that the "Email address already exists.
Go to login page "in a dark pink box
Step 9 - Adding the Login Method
The login method is similar to the sign-up function in that we will take the user information and do something with it. In this case, we will compare the email address entered to see if it's in the database.
If so, we will test the password the user provided by hashing the password the user passes in and comparing it to the hashed password in the database.
We know the user has entered the correct password when both hashed passwords match.
Once the user has passed the password check, we know that they have the correct credentials and we can log them in using Flask-Login.
By calling login _ user, Flask-Login will create a session for that user that will persist as the user stays logged in, which will allow the user to view protected pages.
We can start with a new route for handling the POSTed data. We'll redirect to the profile page when the user successfully logs in:
Now, we need to verify if the user has the correct credentials:
Let's add in the block in the template so the user can see the flashed message.
Like the sign-up form, let's add the potential error message directly above the form:
We now have the ability to say a user has been logged in successfully, but there is nothing to log the user into.
This is where we bring in Flask-Login to manage user sessions.
Before we get started, we need a few things for Flask-Login to work.
Start by adding the UserMixin to your User model.
The UserMixin will add Flask-Login attributes to the model so that Flask-Login will be able to work with it.
Then, we need to specify our user loader.
A user loader tells Flask-Login how to find a specific user from the ID that is stored in their session cookie.
We can add this in our create _ app function along with init code for Flask-Login:
Finally, we can add the login _ user function just before we redirect to the profile page to create the session:
With Flask-Login setup, we can use the / login route.
When everything is in place, you will see the profile page.
Profile page with "Welcome, Anthony!"
Step 10 - Protecting Pages
If your name isn't also Anthony, then you'll see that your name is wrong.
What we want is the profile to display the name in the database.
So first, we need to protect the page and then access the user's data to get the name.
To protect a page when using Flask-Login, we add the @ login _ requried decorator between the route and the function.
This will prevent a user who isn't logged in from seeing the route.
If the user isn't logged in, the user will get redirected to the login page, per the Flask-Login configuration.
With routes that are decorated with the @ login _ required decorator, we then have the ability to use the current _ user object inside of the function.
This current _ user represents the user from the database, and we can access all of the attributes of that user with dot notation.
For example, current _ user.email, current _ user.password, and current _ user.name, and current _ user.id will return the actual values stored in the database for the logged-in user.
Let's use the name of the current user and send it to the template.
We will then use that name and display its value.
Then in the profile.html file, update the page to display the name value:
Once we go to our profile page, we then see that the user's name appears.
User welcome page with the name of the currently logged-in user
The final thing we can do is update the logout view.
We can call the logout _ user function in a route for logging out. We have the @ login _ required decorator because it doesn't make sense to logout a user who isn't logged in to begin with.
After we log out and try viewing the profile page again, we see an error message appear.
This is because Flask-Login flashes a message for us when the user isn't allowed to access a page.
Login page with a message showing that user must log in to access page
One last thing we can do is put if statements in the templates to display only the links relevant to the user.
So before the user logs in, they will have the option to log in or sign up.
After they have logged in, they can go to their profile or log out:
Home page with the Home, Login, and Sign Up nav at the top of the screen
With that, you have successfully built your app with authentication.
We've used Flask-Login and Flask-SQLAlchemy to build a login system for our app. We covered how to authenticate a user by first creating a user model and storing the user information.
Then we had to verify the user's password was correct by hashing the password from the form and comparing it to the one stored in the database.
Finally, we added authorization to our app by using the @ login _ required decorator on a profile page so only logged-in users can see that page.
What we created in this tutorial will be sufficient for smaller apps, but if you wish to have more functionality from the beginning, you may want to consider using either the Flask-User or Flask-Security libraries, which are both built on top of the Flask-Login library.
How To Format Code with Prettier in Visual Studio Code
Formatting code consistently is a pain, but modern developer tools like Prettier make it possible to automatically maintain consistency across your team's codebase.
3544
Formatting code consistently is a challenge, but modern developer tools make it possible to automatically maintain consistency across your team's codebase.
In this article, you'll set up Prettier to automatically format your code in Visual Studio Code, also known as VS Code.
For demonstration purposes, here's the sample code you will be formatting:
If you're familiar with code formatting, you may notice some missteps:
A mix of single and double-quotes.
The first property of the person object should be on its own line.
The console statement inside of the function should be indented.
You may or may not like the optional parenthesis surrounding the parameter of the arrow function.
To follow this tutorial, you will need to download and install Visual Studio Code.
To work with Prettier in Visual Studio Code, you'll need to install the extension.
To do this, search for Prettier - Code Formatter in the extension panel of VS Code.
Prettier extension readme
Step 1 - Using the Format Document Command
With the Prettier extension installed, you can now leverage it to format your code.
To start, let's explore using the Format Document command.
This command will make your code more consistent with formatted spacing, line wrapping, and quotes.
In the command palette, search for format and then choose Format Document.
Command palette opened with results for format
You may then be prompted to choose which format to use.
To do so, click the Configure button:
Prompt for selecting a default formatter
Then choose Prettier - Code Formatter.
Selecting Prettier
Note: If you do not see a prompt for selecting a default format, you can manually change this in your Settings.
Set Editor: Default Formatter to ebsenp.prettier-vscode.
Your code is now formatted with spacing, line wrapping, and consistent quotes:
This also works on CSS files.
You can turn something with inconsistent indentation, braces, new lines, and semicolons into well-formatted code.
Will be reformatted as:
Now that we've explored this command, let's look at how this can me implemented to run automatically.
Step 2 - Formatting Code on Save
So far, you've had to manually run a command to format your code.
To automate this process, you can choose a setting in VS Code to have your files automatically formatted when you save.
This also ensures that code doesn't get checked to version control that's not formatted.
Once the menu is open, search for Editor: Format On Save and make sure that option is checked:
Editor: Format On Save checked
Once this is set, you can write your code as usual and it will be automatically formatted when you save the file.
Step 3 - Changing the Prettier Configuration Settings
Prettier does a lot of things for you by default, but you can also customize the settings.
Open the Settings menu.
Then, search for Prettier.
This will bring up all of the settings that you can change:
Configuration Settings for Prettier
Here are a few of the most common settings:
Single Quote - Choose between single and double-quotes.
Semi - Choose whether or not to include semicolons at the end of lines.
Tab Width - Specify how many spaces you want a tab to insert.
The downside to using the built-in settings menu in VS Code is that it doesn't ensure consistency across developers on your team.
Step 4 - Creating a Prettier Configuration File
If you change settings in your VS Code, someone else could have an entirely different configuration on their machine.
You can establish consistent formatting across your team by creating a configuration file for your project.
Create a new file called .prettierrc. < ^ > extension < ^ > with one of the following extensions:
yml
yaml
json
js
toml
Here's an example of a simple configuration file using JSON:
For more specifics on the configuration files, check out the Prettier Docs.
After creating one of these and checking it into your project, you can ensure that every team member follows the same formatting rules.
Having consistent code is a good practice.
It is particularly beneficial when working on a project with multiple collaborators.
Agreeing upon a set of configurations helps with legibility and understanding of code.
More time can be devoted to solving challenging technical problems instead of wrestling over solved problems like code indentation.
Prettier ensures consistency in your code formatting and makes the process automatic.
How To Access Front and Rear Cameras with JavaScript's getUserMedia ()
With HTML5 came the introduction of APIs with access to device hardware, including the MediaDevices API.
This API provides access to media input devices like audio and video.
In this tutorial, you'll see how to get access to the video feeds from a user's device cameras.
3579
With this API's help, developers can access audio and video devices to stream and display live video feeds in the browser.
In this tutorial, you'll access the video feed from the user's device and display it in the browser using the getUserMedia method.
The getUserMedia API makes use of the media input devices to produce a MediaStream.
This MediaStream contains the requested media types, whether audio or video.
Using the stream returned from the API, video feeds can be displayed on the browser, which is useful for real-time communication on the browser.
When used along with the MediaStream Recording API, you can record and store media data captured on the browser.
This API only works on secure origins like the rest of the newly introduced APIs, but it also works on localhost and file URLs.
A basic knowledge of JavaScript.
If you are new to JavaScript, try checking out the How To Code in JavaScript series.
This tutorial will first explain concepts and demonstrate examples with Codepen.
In the final step, you will create a functioning video feed for the browser.
Step 1 - Checking Device Support
First, you will see how to check if the user's browser supports the mediaDevices API.
This API exists within the navigator interface and contains the current state and identity of the user agent.
The check is performed with the following code that can be pasted into Codepen:
First, this checks if the mediaDevices API exists within the navigator and then checks if the getUserMedia API is available within the mediaDevices.
If this returns true, you can get started.
Step 2 - Requesting User Permission
After confirming the browser's support for getUserMedia, you need to request permission to make use of the media input devices on the user agent.
Typically, after a user grants permission, a Promise is returned which resolves to a media stream.
This Promise isn't returned when the permission is denied by the user, which blocks access to these devices.
Paste the following line into Codepen to request permission:
The object provided as an argument for the getUserMedia method is called constraints.
This determines which of the media input devices you are requesting permissions to access.
For example, if the object contains audio: true, the user will be asked to grant access to the audio input device.
Step 3 - Understanding Media Constraints
This section will cover the general concept of contraints.
The constraints object is a MediaStreamConstraints object that specifies the types of media to request and the requirements of each media type.
You can specify requirements for the requested stream using the constraints object, like the resolution of the stream to use (front, back).
You must specify either audio or video when making the request.
A NotFoundError will be returned if the requested media types can't be found on the user's browser.
If you intend to request a video stream of 1280 x 720 resolution, you can update the constraints object to look like this:
With this update, the browser will try to match the specified quality settings for the stream.
If the video device can't deliver this resolution, the browser will return other available resolutions.
To ensure that the browser returns a resolution not lower than the one provided you will have to make use of the min property.
Here is how you could update the constraints object to include the min property:
This will ensure that the stream resolution returned will be at least 1280 x 720. If this minimum requirement can't be met, the promise will be rejected with an OverconstrainedError.
In some cases you may be concerned about saving data and need the stream to not exceed a set resolution.
This can come in handy when the user is on a limited plan.
To enable this functionality, update the constraints object to contain a max field:
With these settings, the browser will ensure that the return stream doesn't go below 1280 x 720 and doesn't exceed 1920 x 1080.
Other terms that can be used includes exact and ideal.
The ideal setting is typically used along with the min and max properties to find the best possible setting closest to the ideal values provided.
You can update the constraints to use the ideal keyword:
To tell the browser to make use of the front or back (on mobile) camera on devices, you can specify a facingMode property in the video object:
This setting will make use of the front-facing camera at all times in all devices.
To make use of the back camera on mobile devices, you can alter the facingMode property to environment.
Step 4 - Using the enumerateDevices Method
When the enumerateDevices method is called, it returns all of the available input media devices available on the user's PC.
With the method, you can provide the user options on which input media device to use for streaming audio or video content.
This method returns a Promise resolved to a MediaDeviceInfo array containing information about each device.
An example of how to make a use of this method is shown in the snippet below:
A sample response for each of the devices would look like the following:
Note: A label won't be returned unless an available stream is available, or if the user has granted device access permissions.
Step 5 - Displaying the Video Stream on the Browser
You have gone through the process of requesting and getting access to the media devices, configured constraints to include required resolutions, and selected the camera you will need to record video.
After going through all these steps, you'll at least want to see if the stream is delivering based on the configured settings.
To ensure this, you will make use of the < video > element to display the video stream on the browser.
Like mentioned earlier, the getUserMedia method returns a Promise that can be resolved to a stream.
The returned stream can be converted to an object URL using the createObjectURL method.
This URL will be set as a video source.
You will create a short demo where we let the user choose from their available list of video devices. using the enumerateDevices method.
This is a navigator.mediaDevices method.
It lists the available media devices, such as microphones and cameras.
It returns a Promise resolvable to an array of objects detailing the available media devices.
Create an index.html file and update the contents with the code below:
In the snippet above, you have set up the elements you will need and a couple of controls for the video.
Also included is a button for taking screenshots of the current video feed.
Now, let's style up these components a bit.
Create a style.css file and copy the following styles into it. Bootstrap was included to reduce the amount of CSS you will need to write to get the components going.
The next step is to add functionality to the demo.
Using the enumerateDevices method, you will get the available video devices and set it as the options within the select element.
Create a file called script.js and update it with the following snippet:
In the snippet above, there are a couple of things going on.
Let's break them down:
feather.replace (): this method call instantiates feather, which is an icon set for web development.
The constraints variable holds the initial configuration for the stream.
This will be extended to include the media device the user chooses.
getCameraSelection: this function calls the enumerateDevices method.
Then, you filter through the array from the resolved Promise and select video input devices.
From the filtered results, you create < option > for the < select > element.
Calling the getUserMedia method happens within the onclick listener of the play button.
Here, you will check if this method is supported by the user's browser before starting the stream.
Next, you will call the startStream function that takes a constraints argument.
It calls the getUserMedia method with the provided constraints. handleStream is called using the stream from the resolved Promise.
This method sets the returned stream to the video element's srcObject.
Next, you will add click listeners to the button controls on the page to pause, stop, and take screenshots.
Also, you will add a listener to the < select > element to update the stream constraints with the selected video device.
Update the script.js file with the code below:
Now, when you open the index.html file in the browser, clicking the Play button will start the stream.
Here is a complete demo:
https: / / codepen.io / chrisbeast / pen / ebYwpX
This tutorial introduced the getUserMedia API.
It is an interesting addition to HTML5 that eases the process of capturing media on the web.
The API takes a parameter (constraints) that can be used to configure the access to audio and video input devices.
It can also be used to specify the video resolution required for your application.
You can extend the demo further to give the user an option to save the screenshots taken, as well as recording and storing video and audio data with the help of MediaStream Recording API.
How to use Git Integration in Visual Studio Code
Visual Studio Code has become one of the most popular editors out there for Web Development.
It has gained such popularity thanks to its many built in features, including source control integration, namely with Git.
With Git being one of the most popular and powerful Source Control providers, harnessing its power from within VS Code is just icing on the cake.
3652
Visual Studio Code (VS Code) has become one of the most popular editors out there for web development.
It has gained such popularity thanks to its many built-in features such as source control integration, namely with Git.
Harnessing the power of Git from within VS Code can make your workflow more efficient and robust.
In this tutorial, you will explore using Source Control Integration in VS Code with Git.
Git installed on your machine.
For more details on accomplishing this, review the Getting Started with Git tutorial.
The latest version of Visual Studio Code installed on your machine.
Step 1 - Familiarizing with the Source Control Tab
The first thing you need to do to take advantage of source control integration is initialize a project as a Git repository.
Open Visual Studio Code and access the built-in terminal.
You can open this by using the keyboard shortcut CTRL + 'on Linux, macOS, or Windows.
In your terminal, make a directory for a new project and change into that directory:
Then, create a Git repository:
Another way to accomplish this with Visual Studio Code is by opening up the Source Control tab (the icon looks like a split in the road) in the left-side panel:
Source Control icon
Next, select Open Folder:
Screenshot depicting Open Folder button
This will open up your file explorer to the current directory.
Select the preferred project directory and click Open.
Then, select Initialize Repository:
Screenshot depicting Initialize Repository button
If you now check your file system, you will see that it includes a .git directory.
To do this, use the terminal to navigate to your project directory and list all of the contents:
You will see the .git directory that was created:
Now that the repo has been initialized, add a file called index.html.
After doing so, you'll see in the Source Control panel that your new file shows up with the letter U beside it. U stands for untracked file, meaning a file that is new or changed, but has not yet been added to the repository:
Screenshot of an untracked file with the letter U indicator
You can now click the plus icon (+) by the index.html file listing to track the file by the repository.
Once added, the letter next to the file will change to an A. A represents a new file that has been added to the repository.
To commit your changes, type a commit message into the input box at the top of the Source Control panel.
Then, click the check icon to perform the commit.
Screenshot of an added file with the letter A indicator and commit message
After doing so, you will notice that are no pending changes.
Next, add a bit of content to your index.html file.
You can use an Emmet shortcut to generate an HTML5 skeleton in VS Code by pressing the!
key followed by Tab key.
Go ahead and add something in the < body > like a < h1 > heading and save it.
In the source control panel, you will see that your file has been changed.
It will show the letter M next to it, which stands for a file that has been modified:
Screenshot of modified file with the letter M indicator
For practice, go ahead and commit this change as well.
Now that you're familiar interacting with the source control panel, you will move on to interpreting gutter indicators.
Step 2 - Intepreting Gutter Indicators
In this step you will take a look at what's called the "Gutter" in VS Code.
The gutter is the skinny area to the right of the line number.
If you've used code folding before, the maximize and minimize icons are located in the gutter.
Let's start by making a small change to your index.html file, such as a change to the content within the < h1 > tag.
After doing so, you will notice a blue vertical mark in the gutter of the line that you changed.
The vertical blue mark signifies that the corresponding line of code has been changed.
Now, try deleting a line of code.
You can delete one of the lines in the < body > section of your index.html file.
Notice now in the gutter that there is a red triangle.
The red triangle signifies a line or group of lines that has been deleted.
Lastly, at the bottom of your < body > section, add a new line of code and notice the green bar.
The vertical green bar signifies a line of code that has been added.
This example depicts gutter indicators for a modified line, a removed line, and a new line:
Screenshot with examples of the three types of gutter indicators
Step 3 - Diffing Files
VS Code also has the ability to perform a diff on a file.
Typically, you would have to download a separate diff tool to do this, so this built-in feature can help you work more efficiently.
To view a diff, open up the source control panel and double-click a changed file.
In this case, double-click the index.html file.
You will be brought to a typical diff view with the current version of the file on the left and the previously committed version of the file on the right.
This example shows that a line has been added in the current version:
Screenshot with example of a split-screen view of a diff
Step 4 - Working with Branches
Moving to the bottom bar, you have the ability to create and switch branches.
If you take a look at the very bottom left of the editor, you should see the source control icon (the one that looks like a split in the road) followed most likely by master or the name of the current working branch.
Branch indicator in bottom bar of VS Code displaying: master
To create a branch, click on that branch name.
A menu should pop up giving you the ability to create a new branch:
Prompt to create a new branch
Go ahead and create a new branch called test.
Now, make a change to your index.html file that signifies you are in the new test branch, such as adding the text this is the new test branch.
Commit those changes to the test branch.
Then, click the branch name in the bottom left again to switch back to the master branch.
After switching back to the master branch, you'll notice that the this is the new test branch text committed to the test branch is no longer present.
Step 5 - Working with Remote Repositories
This tutorial won't touch on it in-depth, but through the Source Control panel, you do have access to work with remote repositories.
If you've worked with a remote repository before you'll notice familiar commands like pull, sync, publish, stash, etc.
Step 6 - Installing Useful Extensions
Not only does VS Code come with lots of built-in functionality for Git, there are also several very popular extensions to add additional functionality.
Git Blame
This extension provides the ability to view Git Blame information in the status bar for the currently selected line.
This may sound intimidating, but not to worry, the Git Blame extension is much more about practicality than it is about making someone feel bad.
The idea of "blaming" someone for a code change is less about shaming them, and more about figuring out the right person to ask questions to for certain pieces of code.
As you can see in the screenshot, this extension provides a subtle message related to the current line of code you are working on in the bottom toolbar explaining who made the change and when they made it.
Git Blame in the bottom toolbar
Git History
Although you can view current changes, perform diffs, and manage branches with the built-in features in VS Code, it does not provide an in-depth view into your Git history.
The Git History extension solves that issue.
As you can see in the image below, this extension allows you to thoroughly explore the history of a file, a given author, a branch, etc. To activate the Git History window below, right-click on a file and choose Git: View File History:
Results of the Git History extension
Additionally, you can compare branches and commits, create branches from commits, and more.
Git Lens
GitLens supercharges the Git capabilities built into Visual Studio Code.
It helps you to visualize code authorship at a glance via Git blame annotations and code lens, seamlessly navigate and explore Git repositories, gain valuable insights via powerful comparison commands, and so much more.
The Git Lens extension is one of the most popular in the community and is also the most powerful.
In most ways, it can replace each of the previous two extension with its functionality.
For "blame" information, a subtle message appears to the right of the line you are currently working on to inform you of who made the change, when they made it, and the associated commit message.
There are some additional pieces of information that pop up when hovering over this message like the code change itself, the timestamp, and more.
Git Blame functionality in Git Lens
For Git history information, this extension provides a lot of functionality.
You have easy access to tons of options including showing file history, performing diffs with previous versions, opening a specific revision, and more.
To open up these options you can click the text in the bottom status bar that contains the author who edited the line of code and how long ago it was edited.
This will open up the following window:
Git History functionality in Git Lens
This extension is packed with functionality, and it will take a while to take in all that it has to offer.
In this tutorial, you explored how to use source control integration with VS Code.
VS Code can handle many features that previously would have required the download of a separate tool.
How To Use Axios with React
Axios is a lightweight, promise-based HTTP client.
In this article, you will explore examples of how to use Axios to access the popular JSON Placeholder API within a React application.
5112
Many projects on the web need to interface with a REST API at some stage in their development.
Axios is a lightweight HTTP client based on the $http service within Angular.js v1.x and is similar to the native JavaScript Fetch API.
Axios is promise-based, which gives you the ability to take advantage of JavaScript's async and await for more readable asynchronous code.
You can also intercept and cancel requests, and there's built-in client-side protection against cross-site request forgery.
In this article, you will see examples of how to use Axios to access the popular JSON Placeholder API within a React application.
To follow along with this article, you'll need the following:
A new React project set up with Create React App by following the How to Set up a React Project with Create React App tutorial.
Step 1 - Adding Axios to the Project
In this section, you will add Axios to the < ^ > digital-ocean-tutorial < ^ > React project you created following the How to Set up a React Project with Create React App tutorial.
To add Axios to the project, open your terminal and change directories into your project:
Then run this command to install Axios:
Next, you will need to import Axios into the file you want to use it in.
Step 2 - Making a GET Request
In this example, you create a new component and import Axios into it to send a GET request.
Inside the src folder of your React project, create a new component named PersonList.js:
Add the following code to the component:
First, you import React and Axios so that both can be used in the component.
Then you hook into the componentDidMount lifecycle hook and perform a GET request.
You use axios.get (url) with a URL from an API endpoint to get a promise which returns a response object.
Inside the response object, there is data that is then assigned the value of person.
You can also get other information about the request, such as the status code under res.status or more information inside of res.request.
Step 3 - Making a POST Request
In this step, you will use Axios with another HTTP request method called POST.
Remove the previous code in PersonList and add the following to create a form that allows for user input and subsequently POSTs the content to an API:
Inside the handleSubmit function, you prevent the default action of the form. Then update the state to the user input.
Using POST gives you the same response object with information that you can use inside of a then call.
To complete the POST request, you first capture the user input.
Then you add the input along with the POST request, which will give you a response.
You can then console.log the response, which should show the user input in the form.
Step 4 - Making a DELETE Request
In this example, you will see how to delete items from an API using axios.delete and passing a URL as a parameter.
Change the code for the form from the POST example to delete a user instead of adding a new one:
Again, the res object provides you with information about the request.
You can then console.log that information again after the form is submitted.
Step 5 - Using a Base Instance in Axios
In this example, you will see how you can set up a base instance in which you can define a URL and any other configuration elements.
Create a separate file named api.js:
Export a new axios instance with these defaults:
Once the default instance is set up, it can then be used inside of the PersonList component.
You import the new instance like this:
Because http: / / jsonplaceholder.typicode.com / is now the base URL, you no longer need to type out the whole URL each time you want to hit a different endpoint on the API.
Step 6 - Using async and await
In this example, you will see how you can use async and await to work with promises.
The await keyword resolves the promise and returns the value.
The value can then be assigned to a variable.
In this code sample, the .then () is replaced.
The promise is resolved, and the value is stored inside the response variable.
In this tutorial, you explored several examples on how to use Axios inside a React application to create HTTP requests and handle responses.
If you'd like to learn more about React, check out the How To Code in React.js series, or check out the React topic page for more exercises and programming projects.
How To Use Rsync to Sync Local and Remote Directories
Rsync is a tool for intelligently syncing local and remote directories.
In this article we will explore the basic usage of this utility to copy files from directory to directory and from system to system.
582
Rsync, which stands for "remote sync", is a remote and local file synchronization tool.
It uses an algorithm that minimizes the amount of data copied by only moving the portions of files that have changed.
In this guide, we will cover the basic usage of this powerful utility.
What Is Rsync?
Rsync is a very flexible network-enabled syncing tool.
Due to its ubiquity on Linux and Unix-like systems and its popularity as a tool for system scripts, it is included on most Linux distributions by default.
The basic syntax of rsync is very straightforward, and operates in a way that is similar to ssh, scp, and cp.
We will create two test directories and some test files with the following commands:
We now have a directory called dir1 with 100 empty files in it.
We also have an empty directory called dir2.
To sync the contents of dir1 to dir2 on the same system, type:
The -r option means recursive, which is necessary for directory syncing.
We could also use the -a flag instead:
The -a option is a combination flag.
It stands for "archive" and syncs recursively and preserves symbolic links, special and device files, modification times, group, owner, and permissions.
It is more commonly used than -r and is usually what you want to use.
An Important Note
You may have noticed that there is a trailing slash (/) at the end of the first argument in the above commands:
This is necessary to mean "the contents of dir1".
The alternative, without the trailing slash, would place dir1, including the directory, within dir2.
This would create a hierarchy that looks like:
Always double-check your arguments before executing an rsync command.
Rsync provides a method for doing this by passing the -n or --dry-run options.
The -v flag (for verbose) is also necessary to get the appropriate output:
Compare this output to the output we get when we remove the trailing slash:
You can see here that the directory itself is transferred.
How To Use Rsync to Sync with a Remote System
Syncing to a remote system is trivial if you have SSH access to the remote machine and rsync installed on both sides.
Once you have SSH access verified between the two machines, you can sync the dir1 folder from earlier to a remote computer by using this syntax (note that we want to transfer the actual directory in this case, so we omit the trailing slash):
This is called a "push" operation because it pushes a directory from the local system to a remote system.
The opposite operation is "pull".
It is used to sync a remote directory to the local system.
If the dir1 were on the remote system instead of our local system, the syntax would be:
Like cp and similar tools, the source is always the first argument, and the destination is always the second.
Useful Options for Rsync
Rsync provides many options for altering the default behavior of the utility.
We have already discussed some of the more necessary flags.
If you are transferring files that have not already been compressed, like text files, you can reduce the network transfer by adding compression with the -z option:
The -P flag is very helpful.
It combines the flags --progress and --partial.
The first of these gives you a progress bar for the transfers and the second allows you to resume interrupted transfers:
If we run the command again, we will get a shorter output, because no changes have been made.
This illustrates rsync's ability to use modification times to determine if changes have been made.
We can update the modification time on some of the files and see that rsync intelligently re-copies only the changed files:
In order to keep two directories truly in sync, it is necessary to delete files from the destination directory if they are removed from the source.
By default, rsync does not delete anything from the destination directory.
We can change this behavior with the --delete option.
Before using this option, use the --dry-run option and do testing to prevent data loss:
If you wish to exclude certain files or directories located inside a directory you are syncing, you can do so by specifying them in a comma-separated list following the --exclude = option:
If we have specified a pattern to exclude, we can override that exclusion for files that match a different pattern by using the --include = option.
Finally, rsync "s --backup option can be used to store backups of important files.
It is used in conjunction with the --backup-dir option, which specifies the directory where the backup files should be stored.
Rsync can simplify file transfers over networked connections and add robustness to local directory syncing.
The flexibility of rsync makes it a good option for many different file-level operations.
A mastery of rsync allows you to design complex backup operations and obtain fine-grained control over what is transferred and how.
6864
This containerized setup was scaled and secured with an Nginx reverse-proxy and Let's Encrypt-signed TLS certificates in How To Scale and Secure a Django Application with Docker, Nginx, and Let's Encrypt.
TLS encryption is enabled with an Ingress object and the ingress-nginx open-source Ingress Controller.
The cert-manager Kubernetes add-on renews and issues certificates using the free Let's Encrypt certificate authority.
A Kubernetes 1.15 + cluster with role-based access control (RBAC) enabled.
The kubectl command-line tool installed on your local machine and configured to connect to your cluster.
An A DNS record with your _ domain.com pointing to the Ingress Load Balancer's public IP address.
A PostgreSQL server instance, database, and user for your Django app. With minor changes, you can use any database that Django supports.
For more information on creating these, please see Repositories from the Docker documentation.
We'll also upload static assets like stylesheets and images to object storage.
Push the image to the repo:
Now that your image is available to Kubernetes on Docker Hub, you can begin rolling it out in your cluster.
When we ran the Django container locally, we passed the env file into docker run to inject configuration variables into the runtime environment.
To begin, create a directory called yaml in which we'll store our Kubernetes manifests.
Copy in the same values entered into the env file in the previous step.
For testing purposes leave DJANGO _ ALLOWED _ HOSTS as * to disable Host header-based filtering.
Create the ConfigMap in your cluster using kubectl apply:
With the ConfigMap created, we'll create the Secret used by our app in the next step.
You can repeat the process from the previous step, manually base64-encoding Secret values and pasting them into a manifest file.
You can inspect the Secret using kubectl describe:
At this point you've stored your app's configuration in your Kubernetes cluster using the Secret and ConfigMap object types.
Here we define a Kubernetes Deployment called polls-app and label it with the key-value pair app: polls.
Finally, we expose containerPort 8000 and name it gunicorn.
To learn more about configuring Kubernetes Deployments, please consult Deployments from the Kubernetes documentation.
When you're done editing the file, save and close it.
Two replicas of your Django app are now up and running in the cluster.
To access the app, you need to create a Kubernetes Service, which we "ll do next.
Roll out the Service using kubectl apply:
This output shows the Service's cluster-internal IP and NodePort (32654).
At this stage, you've rolled out two replicas of the Django Polls app container using a Deployment.
Step 8 - Configuring HTTPS Using Nginx Ingress and cert-manager
This is accomplished using Ingress objects, which define rules for routing HTTP and HTTPS traffic to Kubernetes Services, and Ingress Controllers, which implement the rules by load balancing traffic and routing it to the appropriate backend Services.
Before continuing with this step, you should delete the echo-ingress Ingress created in the prerequisite tutorial:
If you are also using DigitalOcean to manage your domain "s DNS records, consult How to Manage DNS Records to learn how to create A records.
To send a test request, we'll use wget from the command-line:
We "ll use the suggested --no-check-certificate flag to bypass certificate validation:
Navigate to your _ domain.com / polls in your web browser to confirm that HTTPS encryption is enabled and everything is working as expected.
Verify that HTTPS encryption is active in your web browser.
If you're using Google Chrome, arriving at the above page without any errors confirms that everything is working correctly.
The only way to access it is via your domain and the Ingress created in this step.
How To Configure SSH Key-Based Authentication on a Linux Server
SSH, or secure shell, is the most common way of administering remote Linux servers.
Although passwords are sent to the server in a secure manner, they are generally not complex or long enough to be resistant to repeated, persistent attackers.
Modern processing power combined with automated scripts make brute forcing a password-protected account very possible.
Although there are other methods of adding additional security (fail2ban, etc.), SSH keys prove to be a reliable and secure alternative.
SSH key pairs are two cryptographically secure keys that can be used to authenticate a client to an SSH server.
Each key pair consists of a public key and a private key.
The public key can be used to encrypt messages that only the private key can decrypt.
The key is added to a special file within the user account you will be logging into called ~ / .ssh / authorized _ keys.
Usually, it is best to stick with the default location at this stage.
If you would like to choose a non-standard path, type that in now, otherwise, press ENTER to accept the default.
The passphrase serves as an additional layer of protection in case these conditions are compromised.
If you are starting up a new DigitalOcean server, you can automatically embed your SSH public key in your new server's root account.
SSH key selection
In the "SSH Key content" box, paste the content of your SSH public key.
Paste this value, in its entirety, into the larger box.
When you create your Droplet, the public SSH keys that you selected will be placed in the ~ / .ssh / authorized _ keys file of the root user's account.
The method you use depends largely on the tools you have available and the details of your current configuration.
Because of its simplicity, this method is recommended if available.
You will see output that looks like this:
For instance, if your server is a DigitalOcean Droplet, you can log in using the web console in the control panel:
Afterwards, a new shell session should be spawned for you with the account on the remote system.
Before completing the steps in this section, make sure that you either have SSH key-based authentication configured for the root account on this server, or preferably, that you have SSH key-based authentication configured for an account on this server with sudo access.
From here, there are many directions you can head.
Using for loops and while loops in Python allow you to automate and repeat tasks in an efficient manner.
You "ll put the break statement within the block of code under your loop statement, usually after a conditional if statement.
This shows that once the integer number is evaluated as equivalent to 5, the loop breaks, as the program is told to do so with the break statement.
The continue statement will be within the block of code under the loop statement, usually after a conditional if statement.
Pass Statement
The pass statement occurring after the if conditional statement is telling the program to continue to run the loop and ignore the fact that the variable number evaluates as equivalent to 5 during one of its iterations.
We "ll run the program and consider the output:
By using the pass statement in this program, we notice that the program runs exactly as it would if there were no conditional statement in the program.
How To Use Journalctl to View and Manipulate Systemd Logs
Some of the most compelling advantages of systemd are those involved with process and system logging.
The journal is implemented with the journald daemon, which handles all of the messages produced by the kernel, initrd, services, etc. In this guide, we will discuss how to use the journalctl utility, which can be used to access and manipulate the data held within the journal.
By interacting with the data using a single utility, administrators are able to dynamically display log data according to their needs.
Storing the log data in a binary format also means that the data can be displayed in arbitrary output formats depending on what you need at the moment.
For instance, for daily log management you may be used to viewing the logs in the standard syslog format, but if you decide to graph service interruptions later on, you can output each entry as a JSON object to make it consumable to your graphing service.
For instance, you may have a centralized syslog server that you use to compile data from multiple servers, but you also may wish to interleave the logs from multiple services on a single system with the systemd journal.
When you find the one that matches the location of your server, you can set it by using the set-timezone option:
Basic Log Viewing
However, this actually collects data from more sources than traditional syslog implementations are capable of.
If you want to display the timestamps in UTC, you can use the --utc flag:
This will help you identify and manage information that is pertinent to your current environment.
While you will commonly want to display the information from the current boot, there are certainly times when past boots would be helpful as well.
Some distributions enable saving previous boot information by default, while others disable this feature.
To enable persistent boot information, you can either create the directory to store the journal by typing:
If you need an absolute reference, the boot ID is in the second column.
You can tell the time that the boot session refers to with the two time specifications listed towards the end.
For instance, to see the journal from the previous boot, use the -1 relative pointer with the -b flag:
For absolute time values, you should use the following format:
The seconds field can be left off as well to default to "00 ":
For instance, you can use the words "yesterday", "today", "tomorrow", or "now".
To get the data from yesterday, you could type:
As you can see, it's relatively easy to define flexible windows of time to filter the entries you wish to see.
The systemd journal provides a variety of ways of doing this.
We can use the -u option to filter in this way.
Typically, you would probably want to filter by time as well in order to display the lines you are interested in.
By Process, User, or Group ID
To do this we can filter by specifying the _ PID field.
The -F option can be used to show all of the available values for a given journal field.
This will show you all of the values that the journal has stored for the group ID field.
We can also filter by providing a path location.
By default, this will display the kernel messages from the current boot.
For instance, to get the messages from five boots ago, you could type:
You can use either the priority name or its corresponding numeric value.
In order of highest to lowest priority, these are:
Selecting a priority will display messages marked at the specified level and those above it.
There are other ways we can modify the output though.
Truncate or Expand Output
If you'd rather have the output truncated, inserting an ellipsis where information has been removed, you can use the --no-full option:
These options allow you to display the journal entries in the whatever format best suits your current needs.
To actively follow the logs as they are being written, you can use the -f flag.
Journal Maintenance
If you wish to shrink your journal, you can do that in two different ways (available with systemd version 218 and later).
By setting these values, you can control how journald consumes and preserves space on your server.
This is important to remember when interpreting file counts after a vacuuming operation.
Using Grep & Regular Expressions to Search for Text Patterns in Linux
Grep is a tool used to search for specified patterns within text input using regular expressions.
Regular expressions are a system for describing complex text patterns.
Regular expressions are a powerful tool that can be used in many different text programs.
In this tutorial, you'll use grep and regular expressions to search through text.
396
The grep command is one of the most useful commands in a Linux terminal environment.
The name grep stands for "global regular expression print".
This means that you can use grep to see if the input it receives matches a specified pattern.
This seemingly trivial program is extremely powerful; its ability to sort input based on complex rules makes it a popular link in many command chains.
In this tutorial, you will explore the grep command's options, and then you'll dive into using regular expressions to do more advanced searching.
interactive _ terminal bash
Basic Usage
In this tutorial, you'll use grep to search the GNU General Public License version 3 for various words and phrases.
If you're on an Ubuntu system, you can find the file in the / usr / share / common-licenses folder.
Copy it to your home directory:
If you're on another system, use the curl command to download a copy:
You'll also use the BSD license file in this tutorial.
On Linux, you can copy that to your home directory with the following command:
If you're on another system, create the file with the following command:
Now that you have the files, you can start working with grep.
In the most basic form, you use grep to match literal patterns within a text file.
This means that if you pass grep a word to search for, it will print out every line in the file containing that word.
Execute the following command to use grep to search for every line that contains the word GNU:
The first argument, GNU, is the pattern you're searching for, while the second argument, GPL-3, is the input file you wish to search.
The resulting output will be every line containing the pattern text:
On some systems, the pattern you searched for will be highlighted in the output.
Common Options
By default, grep will search for the exact specified pattern within the input file and return the lines it finds.
You can make this behavior more useful though by adding some optional flags to grep.
If you want grep to ignore the "case" of your search parameter and search for both upper- and lower-case variations, you can specify the -i or --ignore-case option.
Search for each instance of the word license (with upper, lower, or mixed cases) in the same file as before with the following command:
The results contain: LICENSE, license, and License:
If there was an instance with LiCeNsE, that would have been returned as well.
If you want to find all lines that do not contain a specified pattern, you can use the -v or --invert-match option.
Search for every line that does not contain the word the in the BSD license with the following command:
Since you did not specify the "ignore case" option, the last two items were returned as not having the word the.
It is often useful to know the line number that the matches occur on.
You can do this by using the -n or --line-number option.
Re-run the previous example with this flag added:
You'll see the following text:
Now you can reference the line number if you want to make changes to every line that does not contain the.
This is especially handy when working with source code.
Regular Expressions
In the introduction, you learned that grep stands for "global regular expression print".
A "regular expression" is a text string that describes a particular search pattern.
Different applications and programming languages implement regular expressions slightly differently.
In this tutorial you will only be exploring a small subset of the way that grep describes its patterns.
Literal Matches
In the previous examples in this tutorial, when you searched for the words GNU and the, you were actually searching for basic regular expressions which matched the exact string of characters GNU and the.
Patterns that exactly specify the characters to be matched are called "literals" because they match the pattern literally, character-for-character.
It is helpful to think of these as matching a string of characters rather than matching a word.
This will become a more important distinction as you learn more complex patterns.
All alphabetic and numerical characters (as well as certain other
characters) are matched literally unless modified by other expression
mechanisms.
Anchor Matches
Anchors are special characters that specify where in the line a match must occur to be valid.
For instance, using anchors, you can specify that you only want to know about the lines that match GNU at the very beginning of the line.
To do this, you could use the ^ anchor before the literal string.
Run the following command to search the GPL-3 file and find lines where GNU occurs at the very beginning of a line:
You'll see these two lines:
Similarly, you use the $anchor at the end of a pattern to indicate that the match will only be valid if it occurs at the very end of a line.
This command will match every line ending with the word and in the GPL-3 file:
Matching Any Character
The period character (.) is used in regular expressions to mean that any single character can exist at the specified location.
For example, to match anything in the GPL-3 file that has two characters and then the string cept, you would use the following pattern:
As you can see, the output has instances of both accept and except and variations of the two words.
The pattern would also have matched z2cept if that was found as well.
Bracket Expressions
By placing a group of characters within brackets (\ [and\]), you can specify that the character at that position can be any one character found within the bracket group.
For example, to find the lines that contain too or two, you would specify those variations succinctly by using the following pattern:
The output shows that both variations exist in the file:
Bracket notation gives you some interesting options.
You can have the pattern match anything except the characters within a bracket by beginning the list of characters within the brackets with a ^ character.
This example is like the pattern .ode, but will not match the pattern code:
Here's the output you'll see:
Notice that in the second line returned, there is, in fact, the word code.
This is not a failure of the regular expression or grep.
Rather, this line was returned because earlier in the line, the pattern mode, found within the word model, was found.
The line was returned because there was an instance that matched the pattern.
Another helpful feature of brackets is that you can specify a range of characters instead of individually typing every available character.
This means that if you want to find every line that begins with a capital letter, you can use the following pattern:
Due to some legacy sorting issues, it is often more accurate to use POSIX character classes instead of character ranges like you just used.
There are many character classes that are outside of the scope of this guide, but an example that would accomplish the same procedure as the previous example uses the\ [: upper:\] character class within a bracket selector:
The output will be the same as before.
Repeat Pattern Zero or More Times
Finally, one of the most commonly used meta-characters is the asterisk, or *, which means "repeat the previous character or expression zero or more times".
To find each line in the GPL-3 file that contains an opening and closing parenthesis, with only letters and single spaces in between, use the following expression:
So far you've used periods, asterisks, and other characters in your expressions, but sometimes you need to search for those characters specifically.
Escaping Meta-Characters
There are times where you'll need to search for a literal period or a literal opening bracket, especially when working with source code or configuration files.
Because these characters have special meaning in regular expressions, you need to "escape" these characters to tell grep that you do not wish to use their special meaning in this case.
You escape characters by using the backslash character (\) in front of the character that would normally have a special meaning.
For instance, to find any line that begins with a capital letter and ends with a period, use the following expression which escapes the ending period so that it represents a literal period instead of the usual "any character" meaning:
This is the output you'll see:
Now let's look at other regular expression options.
Extended Regular Expressions
The grep command supports a more extensive regular expression language by using the -E flag or by calling the egrep command instead of grep.
These options open up the capabilities of "extended regular expressions".
Extended regular expressions include all of the basic meta-characters, along with additional meta-characters to express more complex matches.
Grouping
One of the most useful abilities that extended regular expressions open up is the ability to group expressions together to manipulate or reference as one unit.
Group expressions together using parentheses.
If you would like to use
parentheses without using extended regular expressions, you can escape
them with the backslash to enable this functionality.
The following three expressions are functionally equivalent:
Alternation
Similar to how bracket expressions can specify different possible choices for single character matches, alternation allows you to specify alternative matches for strings or expression sets.
To indicate alternation, use the pipe character |.
These are often used within parenthetical grouping to specify that one of two or more possibilities should be considered a match.
The following will find either GPL or General Public License in the
text:
Alternation can select between more than two choices by adding additional choices within the selection group separated by additional pipe (|) characters.
Quantifiers
Like the * meta-character that matched the previous character or character set zero or more times, there are other meta-characters available in extended regular expressions that specify the number of occurrences.
To match a character zero or one times, you can use the?
character.
This makes character or character sets that came before optional, in essence.
The following matches copyright and right by putting copy in an optional group:
The + character matches an expression one or more times.
This is almost like the * meta-character, but with the + character, the expression must match at least once.
The following expression matches the string free plus one or more characters that are not white space characters:
Specifying Match Repetition
To specify the number of times that a match is repeated, use the brace characters ({and}).
These characters let you specify an exact number, a range, or an upper or lower bounds to the amount of times an expression can match.
Use the following expression to find all of the lines in the GPL-3 file that contain triple-vowels:
Each line returned has a word with three vowels:
To match any words that have between 16 and 20 characters, use the following expression:
Only lines containing words within that length are displayed.
grep is useful in finding patterns within files or within the file system hierarchy, so it's worth spending time getting comfortable with its options and syntax.
Regular expressions are even more versatile, and can be used with many popular programs.
For instance, many text editors implement regular expressions for searching and replacing text.
Furthermore, most modern programming languages use regular expressions to perform procedures on specific pieces of data. Once you understand regular expressions, you'll be able to transfer that knowledge to many common computer-related tasks, from performing advanced searches in your text editor to validating user input.
UFW Essentials: Common Firewall Rules and Commands
UFW is a firewall configuration tool for iptables that is included with Ubuntu by default.
This cheat sheet-style guide provides a quick reference to UFW commands that will create iptables firewall rules are useful in common, everyday scenarios.
This includes UFW examples of allowing and blocking various services by port, network interface, and source IP address.
1737
How To Use This Guide
If you are just getting started with using UFW to configure your firewall, check out our introduction to UFW
Most of the rules that are described here assume that you are using the default UFW ruleset.
That is, it is set to allow outgoing and deny incoming traffic, through the default policies, so you have to selectively allow traffic in
Use whichever subsequent sections are applicable to what you are trying to achieve.
Most sections are not predicated on any other, so you can use the examples below independently
Use the Contents menu on the right side of this page (at wide page widths) or your browser's find function to locate the sections you need
Copy and paste the command-line examples given, substituting the values in red with your own values
Remember that you can check your current UFW ruleset with sudo ufw status or sudo ufw status verbose.
Block an IP Address
To block all network connections that originate from a specific IP address, 15.15.15.51 for example, run this command:
In this example, from 15.15.15.51 specifies a source IP address of "15.15.15.51".
If you wish, a subnet, such as 15.15.15.0 / 24, may be specified here instead.
The source IP address can be specified in any firewall rule, including an allow rule.
Block Connections to a Network Interface
To block connections from a specific IP address, e.g. 15.15.15.51, to a specific network interface, e.g. eth0, use this command:
This is the same as the previous example, with the addition of in on eth0.
The network interface can be specified in any firewall rule, and is a great way to limit the rule to a particular network.
Service: SSH
If you're using a cloud server, you will probably want to allow incoming SSH connections (port 22) so you can connect to and manage your server.
This section covers how to configure your firewall with various SSH-related rules.
Allow SSH
To allow all incoming SSH connections run this command:
An alternative syntax is to specify the port number of the SSH service:
Allow Incoming SSH from Specific IP Address or Subnet
To allow incoming SSH connections from a specific IP address or subnet, specify the source.
For example, if you want to allow the entire 15.15.15.0 / 24 subnet, run this command:
Allow Incoming Rsync from Specific IP Address or Subnet
Rsync, which runs on port 873, can be used to transfer files from one computer to another.
To allow incoming rsync connections from a specific IP address or subnet, specify the source IP address and the destination port. For example, if you want to allow the entire 15.15.15.0 / 24 subnet to be able to rsync to your server, run this command:
Service: Web Server
Web servers, such as Apache and Nginx, typically listen for requests on port 80 and 443 for HTTP and HTTPS connections, respectively.
If your default policy for incoming traffic is set to drop or deny, you will want to create rules that will allow your server to respond to those requests.
Allow All Incoming HTTP
To allow all incoming HTTP (port 80) connections run this command:
An alternative syntax is to specify the port number of the HTTP service:
Allow All Incoming HTTPS
To allow all incoming HTTPS (port 443) connections run this command:
An alternative syntax is to specify the port number of the HTTPS service:
Allow All Incoming HTTP and HTTPS
If you want to allow both HTTP and HTTPS traffic, you can create a single rule that allows both ports.
To allow all incoming HTTP and HTTPS (port 443) connections run this command:
Note that you need to specify the protocol, with proto tcp, when specifying multiple ports.
Service: MySQL
MySQL listens for client connections on port 3306.
If your MySQL database server is being used by a client on a remote server, you need to be sure to allow that traffic.
Allow MySQL from Specific IP Address or Subnet
To allow incoming MySQL connections from a specific IP address or subnet, specify the source.
Allow MySQL to Specific Network Interface
To allow MySQL connections to a specific network interface - say you have a private network interface eth1, for example - use this command:
Service: PostgreSQL
PostgreSQL listens for client connections on port 5432.
If your PostgreSQL database server is being used by a client on a remote server, you need to be sure to allow that traffic.
PostgreSQL from Specific IP Address or Subnet
To allow incoming PostgreSQL connections from a specific IP address or subnet, specify the source.
The second command, which allows the outgoing traffic of established PostgreSQL connections, is only necessary if the OUTPUT policy is not set to ACCEPT.
Allow PostgreSQL to Specific Network Interface
To allow PostgreSQL connections to a specific network interface - say you have a private network interface eth1, for example - use this command:
Service: Mail
Mail servers, such as Sendmail and Postfix, listen on a variety of ports depending on the protocols being used for mail delivery.
If you are running a mail server, determine which protocols you are using and allow the appropriate types of traffic.
We will also show you how to create a rule to block outgoing SMTP mail.
Block Outgoing SMTP Mail
If your server shouldn't be sending outgoing mail, you may want to block that kind of traffic.
To block outgoing SMTP mail, which uses port 25, run this command:
This configures your firewall to drop all outgoing traffic on port 25. If you need to reject a different service by its port number, instead of port 25, simply replace it.
Allow All Incoming SMTP
To allow your server to respond to SMTP connections, port 25, run this command:
Note: It is common for SMTP servers to use port 587 for outbound mail.
Allow All Incoming IMAP
To allow your server to respond to IMAP connections, port 143, run this command:
Allow All Incoming IMAPS
To allow your server to respond to IMAPS connections, port 993, run this command:
Allow All Incoming POP3
To allow your server to respond to POP3 connections, port 110, run this command:
Allow All Incoming POP3S
To allow your server to respond to POP3S connections, port 995, run this command:
That should cover many of the commands that are commonly used when using UFW to configure a firewall.
Of course, UFW is a very flexible tool so feel free to mix and match the commands with different options to match your specific needs if they aren't covered here.
Good luck!
How To Configure a Production-Ready Mesosphere Cluster on Ubuntu 14.04
In this guide, we will go over how to configure a highly available cluster in Mesosphere.
This configuration will set us up with failover in case any of our master nodes go down as well as a pool of slave servers to handle the tasks that are scheduled.
1321
Mesosphere is a system that combines a number of components to effectively manage server clustering and highly available deployments on top of an existing operating system layer.
Unlike systems like CoreOS, Mesosphere is not a specialized operating system and is instead a set of packages.
We will be using Ubuntu 14.04 servers for this guide.
Before you complete this guide, it is strongly recommended that you review our introduction to Mesosphere.
This is a good way to get familiar with the components that the system is comprised of and assist you in identifying what each unit is responsible for.
During this tutorial, we will be using six Ubuntu servers.
This fulfills the Apache Mesos recommendation of having at least three masters for a production environment.
It also provides a pool of three worker or slave servers, which will be assigned work when tasks are sent to the cluster.
The six servers we will be using will use zookeeper to keep track of the current leader of the master servers.
The Mesos layer, built on top of this, will provide distributed synchronization and resource handling.
It is responsible for managing the cluster.
Marathon, the cluster's distributed init system, is used to schedule tasks and hand work to the slave servers.
For the sake of this guide, we will be assuming that our machines have the following configuration:
Hostname
Function
IP Address
master1
Mesos master
192.0.2.1
master2
192.0.2.2
master3
192.0.2.3
slave1
Mesos slave
192.0.2.51
slave2
192.0.2.52
slave3
192.0.2.53
Each of these machines should have Ubuntu 14.04 installed.
You will want to complete the basic configuration items listed in our Ubuntu 14.04 initial server setup guide.
When you are finished with the above steps, continue on with this guide.
Install Mesosphere on the Servers
The first step to getting your cluster up and running is to install the software.
Fortunately, the Mesosphere project maintains an Ubuntu repository with up-to-date packages that are easy to install.
Add the Mesosphere Repositories to your Hosts
On all of the hosts (masters and slaves), complete the following steps.
First, add the Mesosphere repository to your sources list.
This process involves downloading the Mesosphere project's key from the Ubuntu keyserver and then crafting the correct URL for our Ubuntu release.
The project provides a convenient way of doing this:
Install the Necessary Components
After you have the Mesosphere repository added to your system, you must update your local package cache to gain access to the new component:
Next, you need to install the necessary packages.
The components you need will depend on the role of the host.
For your master hosts, you need the mesosphere meta package.
This includes the zookeeper, mesos, marathon, and chronos applications:
For your slave hosts, you only need the mesos package, which also pulls in zookeeper as a dependency:
Set up the Zookeeper Connection Info for Mesos
The first thing we are going to do is configure our zookeeper connection info.
This is the underlying layer that allows all of our hosts to connect to the correct master servers, so it makes sense to start here.
Our master servers will be the only members of our zookeeper cluster, but all of our servers will need some configuration to be able to communicate using the protocol.
The file that defines this is / etc / mesos / zk.
On all of your hosts, complete the following step.
Open the file with root privileges:
Inside, you will find the connection URL is set by default to access a local instance.
We need to modify this to point to our three master servers.
This is done by replacing localhost with the IP address of our first Mesos master server.
We can then add a comma after the port specification and replicate the format to add our second and third masters to the list.
For our guide, our masters have IP addresses of 192.0.2.1, 192.168.2.2, and 192.168.2.3.
Using these values, our file will look like this:
The line must start with zk: / / and end with / mesos.
In between, your master servers' IP addresses and zookeeper ports (2181 by default) are specified.
Use this identical entry in each of your masters and slaves.
This will help each individual server connect to the correct master servers to communicate with the cluster.
Configure the Master Servers' Zookeeper Configuration
On your master servers, we will need to do some additional zookeeper configuration.
The first step is to define a unique ID number, from 1 to 255, for each of your master servers.
This is kept in the / etc / zookeeper / conf / myid file.
Open it now:
Delete all of the info in this file and replace it with a single number, from 1 to 255. Each of your master servers must have a unique number.
For the sake of simplicity, it is easiest to start at 1 and work your way up.
We will be using 1, 2, and 3 for our guide.
Our first server will just have this in the file:
Do this on each of your master servers.
Next, we need to modify our zookeeper configuration file to map our zookeeper IDs to actual hosts.
This will ensure that the service can correctly resolve each host from the ID system that it uses.
Open the zookeeper configuration file now:
Within this file, you need to map each ID to a host.
The host specification will include two ports, the first for communicating with the leader, and the second for handling elections when a new leader is required.
The zookeeper servers are identified by "server" followed by a dot and their ID number.
For our guide, we will be using the default ports for each function and our IDs are 1-3.
Our file will look like this:
Add these same mappings in each of your master servers' configuration files.
With that, our zookeeper configuration is complete.
We can begin focusing on Mesos and Marathon.
Configure Mesos on the Master Servers
Next, we will configure Mesos on the three master servers.
These steps should be taken on each of your master servers.
Modify the Quorum to Reflect your Cluster Size
First, we need to adjust the quorum necessary to make decisions.
This will determine the number of hosts necessary for the cluster to be in a functioning state.
The quorum should be set so that over 50 percent of the master members must be present to make decisions.
However, we also want to build in some fault tolerance so that if all of our masters are not present, the cluster can still function.
We have three masters, so the only setting that satisfies both of these requirements is a quorum of two.
Since the initial configuration assumes a single server setup, the quorum is currently set to one.
Open the quorum configuration file:
Change the value to "2 ":
Repeat this on each of your master servers.
Configure the Hostname and IP Address
Next, we'll specify the hostname and IP address for each of our master servers.
We will be using the IP address for the hostname so that our instances will not have trouble resolving correctly.
For our master servers, the IP address needs to be placed in these files:
/ etc / mesos-master / ip
/ etc / mesos-master / hostname
First, add each master node's individual IP address in the / etc / mesos-master / ip file.
Remember to change this for each server to match the appropriate value:
Now, we can copy this value to the hostname file:
Configure Marathon on the Master Servers
Now that Mesos is configured, we can configure Marathon, Mesosphere's clustered init system implementation.
Marathon will run on each of our master hosts, but only the leading master server will be able to actually schedule jobs.
The other Marathon instances will transparently proxy requests to the master server.
First, we need to set the hostname again for each server's Marathon instance.
Again, we will use the IP address, which we already have in a file.
We can copy that to the file location we need.
However, the Marathon configuration directory structure we need is not created automatically.
We will have to create the directory and then we can copy the file over:
Next, we need to define the list of zookeeper masters that Marathon will connect to for information and scheduling.
This is the same zookeeper connection string that we've been using for Mesos, so we can just copy the file.
We need to place it in a file called master:
This will allow our Marathon service to connect to the Mesos cluster.
However, we also want Marathon to store its own state information in zookeeper.
For this, we will use the other zookeeper connection file as a base, and just modify the endpoint.
First, copy the file to the Marathon zookeeper location:
Next, open the file in your editor:
The only portion we need to modify in this file is the endpoint.
We will change it from / mesos to / marathon:
This is all we need to do for our Marathon configuration.
Configure Service Init Rules and Restart Services
Next, we will restart our master servers' services to use the settings that we have been configuring.
First, we will need to make sure that our master servers are only running the Mesos master process, and not running the slave process.
We can stop any currently running slave processes (this might fail, but that's okay since this is just to ensure the process is stopped).
We can also ensure that the server doesn't start the slave process at boot by creating an override file:
Now, all we need to do is restart zookeeper, which will set up our master elections.
We can then start our Mesos master and Marathon processes:
To get a peak at what you have just set up, visit one of your master servers in your web browser at port 5050:
You should see the main Mesos interface.
You may be told you are being redirected to the active master depending on whether you connected to the elected leader or not.
Either way, the screen will look similar to this:
Mesos main interface
This is a view of your cluster currently.
There is not much to see because there are no slave nodes available and no tasks started.
We have also configured Marathon, Mesosphere's long-running task controller.
This will be available at port 8080 on any of your masters:
Marathon main interface
We will briefly go over how to use these interfaces once we get our slaves set up.
Configure the Slave Servers
Now that we have our master servers configured, we can begin configuring our slave servers.
We have already configured our slaves with our masters servers' zookeeper connection information.
The slaves themselves do not run their own zookeeper instances.
We can stop any zookeeper process currently running on our slave nodes and create an override file so that it will not automatically start when the server reboots:
Next, we want to create another override file to make sure the Mesos master process doesn't start on our slave servers.
We will also ensure that it is stopped currently (this command may fail if the process is already stopped.
This is not a problem):
Next, we need to set the IP address and hostname, just as we did for our master servers.
This involves putting each node's IP address into a file, this time under the / etc / mesos-slave directory.
We will use this as the hostname as well, for easy access to services through the web interface:
Again, use each slave server's individual IP address for the first command.
This will ensure that it is being bound to the correct interface.
Now, we have all of the pieces in place to start our Mesos slaves.
We just need to turn on the service:
Do this on each of your slave machines.
To see whether your slaves are successfully registering themselves in your cluster, go back to your one of your master servers at port 5050:
You should see the number of active slaves at "3" now in the interface:
Mesos three slaves
You can also see that the available resources in the interface has been updated to reflect the pooled resources of your slave machines:
Mesos resources
To get additional information about each of your slave machines, you can click on the "Slaves" link at the top of the interface.
This will give you an overview of each machine's resource contribution, as well as links to a page for each slave:
Mesos slaves page
Starting Services on Mesos and Marathon
Marathon is Mesosphere's utility for scheduling long-running tasks.
It is easy to think of Marathon as the init system for a Mesosphere cluster because it handles starting and stopping services, scheduling tasks, and making sure applications come back up if they go down.
You can add services and tasks to Marathon in a few different ways.
We will only be covering basic services.
Docker containers will be handled in a future guide.
Starting a Service through the Web Interface
The most straight forward way of getting a service running quickly on the cluster is to add an application through the Marathon web interface.
First, visit the Marathon web interface on one of your the master servers.
Remember, the Marathon interface is on port 8080:
From here, you can click on the "New App" button in the upper-right corner.
This will pop up an overlay where you can add information about your new application:
Marathon new app
Fill in the fields with the requirements for your app. The only fields that are mandatory are:
ID: A unique ID selected by the user to identify a process.
This can be whatever you'd like, but must be unique.
Command: This is the actual command that will be run by Marathon.
This is the process that will be monitored and restarted if it fails.
Using this information, you can set up a simple service that just prints "hello" and sleeps for 10 seconds.
We will call this "hello ":
Marathon simple app
When you return to the interface, the service will go from "Deploying" to "Running ":
Marathon app running
Every 10 seconds or so, the "Tasks / Instances" reading will go from "1 / 1" to "0 / 1" as the sleep amount passes and the service stops.
Marathon then automatically restarts the task again.
We can see this process more clearly in the Mesos web interface at port 5050:
Here, you can see the process finishing and being restarted:
Mesos restart task
If you click on "Sandbox" and then "stdout" on any of the tasks, you can see the "hello" output being produced:
Mesos output
Starting a Service through the API
We can also submit services through Marathon's API.
This involves passing in a JSON object containing all of the fields that the overlay contained.
This is a relatively simple process.
Again, the only required fields are id for the process identifier and cmd which contains the actual command to run.
So we could create a JSON file called hello.json with this information:
Inside, the bare minimum specification would look like this:
This service will work just fine.
However, if we truly want to emulate the service we created in the web UI, we have to add some additional fields.
These were defaulted in the web UI and we can replicate them here:
Save and close the JSON file when you are finished.
Next, we can submit it using the Marathon API.
The target is one of our master's Marathon service at port 8080 and the endpoint is / v2 / apps.
The data payload is our JSON file, which we can read into curl by using the -d flag with the @ flag to indicate a file.
The command to submit will look like this:
If we look at the Marathon interface, we can see that it was successfully added.
It seems to have the exact same properties as our first service:
Marathon two services
The new service can be monitored and accessed in exactly the same way as the first service.
At this point, you should have a production-ready Mesosphere cluster up and running.
We have only covered the basic configuration at this point, but you should be able to see the possibilities of leveraging the Mesosphere system.
In future guides, we will cover how to deploy Docker containers on your cluster and how to use some of the tools in more depth.
In the second example, we used new String () to create a string object and assign it to a variable.
Most of the time you will be creating string primitives.
Essentially, there are methods and properties available to all strings, and in the background JavaScript will perform a conversion to object and back to primitive every time a method or property is called.
How Strings are Indexed
Each of the characters in a string correspond to an index number, starting with 0.
w
e
5
10
11
Accessing Characters
Using square bracket notation, we can access any character in the string.
Alternatively, we can use indexOf () to return the index number by the first instance of a character.
Converting to Upper or Lower Case
JavaScript has a very useful method for splitting a string by a character and creating a new array out of the sections.
In addition to being able to replace a value with another string value, we can also use Regular Expressions to make replace () more powerful.
This is a very common task that makes use of Regular Expressions.
Visit Regexr to practice more examples of RegEx.
For a more general overview on strings, read the tutorial "How To Work with Strings in JavaScript."
In this article we'll show you how to back up, restore, and migrate your MongoDB databases.
7055
The simplest solution to this problem is to run the exports and backups during the night or non-peak hours.
Note that in the above directory path, we have used date + "% m-% d-% y" which automatically gets the current date.
This will allow us to have backups inside the directory like / var / backups / < ^ > 10-29-20 < ^ > /.
Inside the crontab prompt, insert the following mongodump command:
That's why it's also recommended to clean the old backups regularly or to compress them.
For example, to delete all the backups older than seven days, you can use the following bash command:
It should run just before you start the next backup, e.g., at 03: 01 AM.
Completing all the tasks in this step will ensure a proper backup solution for your MongoDB databases.
We'll be using newdb. * to restore all collections.
Then, using --drop, we'll make sure that the target database is first dropped so that the backup is restored in a clean database.
Once you have a timestamped backup, you can restore it using this command:
How To Install MongoDB from the Default APT Repositories on Ubuntu 20.04
6903
MongoDB is a free and open-source NoSQL document database used commonly in modern web applications.
Step 1 - Installing MongoDB
Now install the MongoDB package itself:
To do so, press Y and then ENTER.
Step 3 - Managing the MongoDB Service
To start the server when it is stopped, type:
Step 4 - Adjusting the Firewall (Optional)
If you intend to use the MongoDB server only locally with applications running on the same server, this is the recommended and secure setting.
In most cases, MongoDB should be accessed only from certain trusted locations, such as another server hosting an application.
You can verify the change in firewall settings with ufw:
You should see traffic to port 27017 allowed in the output.
Note that if you have decided to allow only a certain IP address to connect to MongoDB server, the IP address of the allowed location will be listed instead of Anywhere in this command's output:
You can find more advanced firewall settings for restricting access to services in UFW Essentials: Common Firewall Rules and Commands.
Open the MongoDB configuration file in your preferred text editor.
This example command uses nano:
Be sure to place a comma between the existing IP address and the one you added:
How To Install and Use PostgreSQL on Ubuntu 18.04
2628
It is a popular choice for many small and large projects and has the advantage of being standards-compliant and having many advanced features like reliable transactions and concurrency without read locks.
This guide demonstrates how to install Postgres on an Ubuntu 18.04 VPS instance and also provides instructions for basic database administration.
To follow along with this tutorial, you will need one Ubuntu 18.04 server that has been configured by following our Initial Server Setup for Ubuntu 18.04 guide.
Since this is your first time using apt in this session, refresh your local package index.
Now that the software is installed, we can go over how it works and how it may be different from similar database management systems you may have used.
By default, Postgres uses a concept called "roles" to handle in authentication and authorization.
You can now access a Postgres prompt immediately by typing:
This means that, if the user you created in the last section is called sammy, that role will attempt to connect to a database which is also called "sammy" by default.
First, create a table to store some data. As an example, a table that describes some playground equipment.
The basic syntax for this command is as follows:
For demonstration purposes, create a simple table like this:
These commands will create a table that inventories playground equipment.
This starts with an equipment ID, which is of the serial type.
This data type is an auto-incrementing integer.
You "ve also given this column the constraint of primary key which means that the values must be unique and not null.
For two of the columns (equip _ id and install _ date), the commands do not specify a field length.
This is because some column types don't require a set length because the length is implied by the type.
The next two commands create columns for the equipment type and color respectively, each of which cannot be empty.
The command after these creates a location column and create a constraint that requires the value to be one of eight possible values.
The last command creates a date column that records the date on which you installed the equipment.
Now that you have a table, you can insert some data into it.
As an example, add a slide and a swing by calling the table you want to add to, naming the columns and then providing data for each column, like this:
This is because this is automatically generated whenever a new row in the table is created.
You notice that your slide is no longer a part of the table.
After creating a table, you can modify it to add or remove columns relatively easily.
If you view your table information again, you will see the new column has been added (but no data has been entered):
Deleting a column is just as simple.
You can query for the "swing" record (this will match every swing in your table) and change its color to "red".
This could be useful if you gave the swing set a paint job:
As you can see, your slide is now registered as being red.
You are now set up with PostgreSQL on your Ubuntu 18.04 server.
However, there is still much more to learn with Postgres.
Here are some more guides that cover how to use Postgres:
Learn how to create and manage tables with Postgres
Get better at managing roles and permissions
Craft queries with Postgres with Select
Learn how to secure PostgreSQL
Learn how to backup a Postgres database
How To Install Git on Ubuntu 18.04 Quickstart
2712
Version control systems help you share and collaborate on software development projects.
Git is one of the most popular version control systems currently available.
This tutorial will walk you through installing and configuring Git on an Ubuntu 18.04 server.
For a more detailed version of this tutorial, with better explanations of each step, please refer to How To Install Git on Ubuntu 18.04.
Step 1 - Update Default Packages
Logged into your Ubuntu 18.04 server as a sudo non-root user, first update your default packages.
Step 2 - Install Git
Step 3 - Confirm Successful Installation
You can confirm that you have installed Git correctly by running this command and receiving output similar to the following:
Step 4 - Set Up Git
Now that you have Git installed and to prevent warnings, you should configure it with your information.
If you need to edit this file, you can use a text editor such as nano:
How To Install Git on 18.04
How To Use Git Effectively
How To Use Git Branches
An Introduction to Open Source
How To Install Node.js on Ubuntu 18.04
2605
Node.js is a JavaScript platform for general-purpose programming that allows users to build network applications quickly.
By leveraging JavaScript on both the front and backend, Node.js makes development more consistent and integrated.
In this guide, we'll show you how to get started with Node.js on an Ubuntu 18.04 server.
This guide assumes that you are using Ubuntu 18.04.
You can learn how to do this by following the initial server setup tutorial for Ubuntu 18.04.
Installing the Distro-Stable Version for Ubuntu
Ubuntu 18.04 contains a version of Node.js in its default repositories that can be used to provide a consistent experience across multiple systems.
At the time of writing, the version in the repositories is 8.10.0.
Install Node.js from the repositories:
Because of a conflict with another package, the executable from the Ubuntu repositories is called nodejs instead of node.
Keep this in mind as you are running software.
To check which version of Node.js you have installed after these initial steps, type:
Once you have established which version of Node.js you have installed from the Ubuntu repositories, you can decide whether or not you would like to work with different versions, package archives, or version managers.
Next, we'll discuss these elements, along with more flexible and robust methods of installation.
Installing Using a PPA
To get a more recent version of Node.js you can add the PPA (personal package archive) maintained by NodeSource.
This will have more up-to-date versions of Node.js than the official Ubuntu repositories, and will allow you to choose between Node.js v6.x (supported until April of 2019), Node.js v8.x (the current LTS version, supported until December of 2019), Node.js v10.x (the second current LTS version, supported until April of 2021), and Node.js v11.x (the current release, supported until June 2019).
First, install the PPA in order to get access to its contents.
From your home directory, use curl to retrieve the installation script for your preferred version, making sure to replace < ^ > 10.x < ^ > with your preferred version string (if different):
You can inspect the contents of this script with nano (or your preferred text editor):
Run the script under sudo:
After running the setup script from Nodesource, you can install the Node.js package in the same way you did above:
The nodejs package contains the nodejs binary as well as npm, so you don't need to install npm separately.
npm uses a configuration file in your home directory to keep track of updates.
It will be created the first time you run npm.
Execute this command to verify that npm is installed and to create the configuration file:
In order for some npm packages to work (those that require compiling code from source, for example), you will need to install the build-essential package:
You now have the necessary tools to work with npm packages that require compiling code from source.
Installing Using NVM
An alternative to installing Node.js with apt is to use a tool called nvm, which stands for "Node.js Version Manager".
Rather than working at the operating system level, nvm works at the level of an independent directory within your home directory.
This means that you can install multiple self-contained versions of Node.js without affecting the entire system.
Controlling your environment with nvm allows you to access the newest versions of Node.js and retain and manage previous releases.
It is a different utility from apt, however, and the versions of Node.js that you manage with it are distinct from the versions you manage with apt.
To download the nvm installation script from the project's GitHub page, you can use curl.
Note that the version number may differ from what is highlighted here:
Inspect the installation script with nano:
Run the script with bash:
It will install the software into a subdirectory of your home directory at ~ / .nvm.
It will also add the necessary lines to your ~ / .profile file to use the file.
To gain access to the nvm functionality, you'll need to either log out and log back in again or source the ~ / .profile file so that your current session knows about the changes:
With nvm installed, you can install isolated Node.js versions.
For information about the versions of Node.js that are available, type:
As you can see, the current LTS version at the time of this writing is v8.11.1.
You can install that by typing:
Usually, nvm will switch to use the most recently installed version.
You can tell nvm to use the version you just downloaded by typing:
When you install Node.js using nvm, the executable is called node.
You can see the version currently being used by the shell by typing:
If you have multiple Node.js versions, you can see what is installed by typing:
If you wish to default one of the versions, type:
This version will be automatically selected when a new session spawns.
You can also reference it by the alias like this:
Each version of Node.js will keep track of its own packages and has npm available to manage these.
You can also have npm install packages to the Node.js project's. / node _ modules directory.
Use the following syntax to install the express module:
If you'd like to install the module globally, making it available to other projects using the same version of Node.js, you can add the -g flag:
This will install the package in:
Installing the module globally will let you run commands from the command line, but you'll have to link the package into your local sphere to require it from within a program:
You can learn more about the options available to you with nvm by typing:
Removing Node.js
You can uninstall Node.js using apt or nvm, depending on the version you want to target.
To remove the distro-stable version, you will need to work with the apt utility at the system level.
To remove the distro-stable version, type the following:
This command will remove the package and retain the configuration files.
These may be of use to you if you intend to install the package again at a later point.
If you don "t want to save the configuration files for later use, then run the following:
This will uninstall the package and remove the configuration files associated with it.
As a final step, you can remove any unused packages that were automatically installed with the removed package:
To uninstall a version of Node.js that you have enabled using nvm, first determine whether or not the version you would like to remove is the current active version:
If the version you are targeting is not the current active version, you can run:
This command will uninstall the selected version of Node.js.
If the version you would like to remove is the current active version, you must first deactivate nvm to enable your changes:
You can now uninstall the current version using the uninstall command above, which will remove all files associated with the targeted version of Node.js except the cached files that can be used for reinstallment.
There are a quite a few ways to get up and running with Node.js on your Ubuntu 18.04 server.
While using the packaged version in Ubuntu's repository is the easiest method, using nvm offers additional flexibility.
Containerizing a Ruby on Rails Application for Development with Docker Compose
3332
If you are actively developing an application, using Docker can simplify your workflow and the process of deploying your application to production.
Working with containers in development offers the following benefits:
Environments are consistent, meaning that you can choose the languages and dependencies you want for your project without worrying about system conflicts.
Environments are isolated, making it easier to troubleshoot issues and onboard new team members.
Environments are portable, allowing you to package and share your code with others.
This tutorial will show you how to set up a development environment for a Ruby on Rails application using Docker.
You will create multiple containers - for the application itself, the PostgreSQL database, Redis, and a Sidekiq service - with Docker Compose.
The setup will do the following:
Synchronize the application code on the host with the code in the container to facilitate changes during development.
Persist application data between container restarts.
Configure Sidekiq workers to process jobs as expected.
At the end of this tutorial, you will have a working shark information application running on Docker containers:
Sidekiq App Home
A local development machine or server running Ubuntu 18.04, along with a non-root user with sudo privileges and an active firewall.
Docker installed on your local machine or server, following Steps 1 and 2 of How To Install and Use Docker on Ubuntu 18.04.
Docker Compose installed on your local machine or server, following Step 1 of How To Install Docker Compose on Ubuntu 18.04.
Step 1 - Cloning the Project and Adding Dependencies
Our first step will be to clone the rails-sidekiq repository from the DigitalOcean Community GitHub account.
This repository includes the code from the setup described in How To Add Sidekiq and Redis to a Ruby on Rails Application, which explains how to add Sidekiq to an existing Rails 5 project.
Clone the repository into a directory called < ^ > rails-docker < ^ >:
Navigate to the < ^ > rails-docker < ^ > directory:
In this tutorial we will use PostgreSQL as a database.
In order to work with PostgreSQL instead of SQLite 3, you will need to add the pg gem to the project's dependencies, which are listed in its Gemfile.
Open that file for editing using nano or your favorite editor:
Add the gem anywhere in the main project dependencies (above development dependencies):
We can also comment out the sqlite gem, since we won't be using it anymore:
Finally, comment out the spring-watcher-listen gem under development:
If we do not disable this gem, we will see persistent error messages when accessing the Rails console.
These error messages derive from the fact that this gem has Rails use listen to watch for changes in development, rather than polling the filesystem for changes.
Because this gem watches the root of the project, including the node _ modules directory, it will throw error messages about which directories are being watched, cluttering the console.
If you are concerned about conserving CPU resources, however, disabling this gem may not work for you.
In this case, it may be a good idea to upgrade your Rails application to Rails 6.
Save and close the file when you are finished editing.
With your project repository in place, the pg gem added to your Gemfile, and the spring-watcher-listen gem commented out, you are ready to configure your application to work with PostgreSQL.
Step 2 - Configuring the Application to Work with PostgreSQL and Redis
To work with PostgreSQL and Redis in development, we will want to do the following:
Configure the application to work with PostgreSQL as the default adapter.
Add an .env file to the project with our database username and password and Redis host.
Create an init.sql script to create a sammy user for the database.
Add an initializer for Sidekiq so that it can work with our containerized redis service.
Add the .env file and other relevant files to the project's gitignore and dockerignore files.
Create database seeds so that our application has some records for us to work with when we start it up.
First, open your database configuration file, located at config / database.yml:
Currently, the file includes the following default settings, which are applied in the absence of other settings:
We need to change these to reflect the fact that we will use the postgresql adapter, since we will be creating a PostgreSQL service with Docker Compose to persist our application data.
Delete the code that sets SQLite as the adapter and replace it with the following settings, which will set the adapter appropriately and the other variables necessary to connect:
Next, we'll modify the setting for the development environment, since this is the environment we're using in this setup.
Delete the existing SQLite database configuration so that section looks like this:
Finally, delete the database settings for the production and test environments as well:
These modifications to our default database settings will allow us to set our database information dynamically using environment variables defined in .env files, which will not be committed to version control.
Note that if you are creating a Rails project from scratch, you can set the adapter with the rails new command, as described in Step 3 of How To Use PostgreSQL with Your Ruby on Rails Application on Ubuntu 18.04.
This will set your adapter in config / database.yml and automatically add the pg gem to the project.
Now that we have referenced our environment variables, we can create a file for them with our preferred settings.
Extracting configuration settings in this way is part of the 12 Factor approach to application development, which defines best practices for application resiliency in distributed environments.
Now, when we are setting up our production and test environments in the future, configuring our database settings will involve creating additional .env files and referencing the appropriate file in our Docker Compose files.
Open an .env file:
Add the following values to the file:
In addition to setting our database name, user, and password, we've also set a value for the DATABASE _ HOST.
The value, database, refers to the database PostgreSQL service we will create using Docker Compose.
We've also set a REDIS _ HOST to specify our redis service.
To create the sammy database user, we can write an init.sql script that we can then mount to the database container when it starts.
Open the script file:
Add the following code to create a sammy user with administrative privileges:
This script will create the appropriate user on the database and grant this user administrative privileges.
Set appropriate permissions on the script:
Next, we'll configure Sidekiq to work with our containerized redis service.
We can add an initializer to the config / initializers directory, where Rails looks for configuration settings once frameworks and plugins are loaded, that sets a value for a Redis host.
Open a sidekiq.rb file to specify these settings:
Add the following code to the file to specify values for a REDIS _ HOST and REDIS _ PORT:
Much like our database configuration settings, these settings give us the ability to set our host and port parameters dynamically, allowing us to substitute the appropriate values at runtime without having to modify the application code itself.
In addition to a REDIS _ HOST, we have a default value set for REDIS _ PORT in case it is not set elsewhere.
Next, to ensure that our application's sensitive data is not copied to version control, we can add .env to our project's .gitignore file, which tells Git which files to ignore in our project.
Open the file for editing:
At the bottom of the file, add an entry for .env:
Next, we'll create a .dockerignore file to set what should not be copied to our containers.
Add the following code to the file, which tells Docker to ignore some of the things we don't need copied to our containers:
Add .env to the bottom of this file as well:
As a final step, we will create some seed data so that our application has a few records when we start it up.
Open a file for the seed data in the db directory:
Add the following code to the file to create four demo sharks and one sample post:
This seed data will create four sharks and one post that is associated with the first shark.
With your application configured to work with PostgreSQL and your environment variables created, you are ready to write your application Dockerfile.
Step 3 - Writing the Dockerfile and Entrypoint Scripts
Your Dockerfile specifies what will be included in your application container when it is created.
Using a Dockerfile allows you to define your container environment and avoid discrepancies with dependencies or runtime versions.
Following these guidelines on building optimized containers, we will make our image as efficient as possible by using an Alpine base and attempting to minimize our image layers generally.
Open a Dockerfile in your current directory:
Docker images are created using a succession of layered images that build on one another.
Our first step will be to add the base image for our application, which will form the starting point of the application build.
Add the following code to the file to add the Ruby alpine image as a base:
The alpine image is derived from the Alpine Linux project, and will help us keep our image size down.
For more information about whether or not the alpine image is the right choice for your project, please see the full discussion under the Image Variants section of the Docker Hub Ruby image page.
Some factors to consider when using alpine in development:
Keeping image size down will decrease page and resource load times, particularly if you also keep volumes to a minimum.
This helps keep your user experience in development quick and closer to what it would be if you were working locally in a non-containerized environment.
Having parity between development and production images facilitates successful deployments.
Since teams often opt to use Alpine images in production for speed benefits, developing with an Alpine base helps offset issues when moving to production.
Next, set an environment variable to specify the Bundler version:
This is one of the steps we will take to avoid version conflicts between the default bundler version available in our environment and our application code, which requires Bundler 2.0.2.
Next, add the packages that you need to work with the application to the Dockerfile:
These packages include nodejs and yarn, among others.
Since our application serves assets with webpack, we need to include Node.js and Yarn for the application to work as expected.
Keep in mind that the alpine image is extremely minimal: the packages listed here are not exhaustive of what you might want or need in development when you are containerizing your own application.
Next, install the appropriate bundler version:
This step will guarantee parity between our containerized environment and the specifications in this project's Gemfile.lock file.
Now set the working directory for the application on the container:
Copy over your Gemfile and Gemfile.lock:
Copying these files as an independent step, followed by bundle install, means that the project gems do not need to be rebuilt every time you make changes to your application code.
This will work in conjunction with the gem volume that we will include in our Compose file, which will mount gems to your application container in cases where the service is recreated but project gems remain the same.
Next, set the configuration options for the nokogiri gem build:
This step builds nokigiri with the libxml2 and libxslt library versions that we added to the application container in the RUN apk add... step above.
Next, install the project gems:
This instruction checks that the gems are not already installed before installing them.
Next, we'll repeat the same procedure that we used with gems with our JavaScript packages and dependencies.
First we "ll copy package metadata, then we" ll install dependencies, and finally we "ll copy the application code into the container image.
To get started with the Javascript section of our Dockerfile, copy package.json and yarn.lock from your current project directory on the host to the container:
Then install the required packages with yarn install:
This instruction includes a --check-files flag with the yarn command, a feature that makes sure any previously installed files have not been removed.
As in the case of our gems, we will manage the persistence of the packages in the node _ modules directory with a volume when we write our Compose file.
Finally, copy over the rest of the application code and start the application with an entrypoint script:
Using an entrypoint script allows us to run the container as an executable.
The final Dockerfile will look like this:
Next, create a directory called entrypoints for the entrypoint scripts:
This directory will include our main entrypoint script and a script for our Sidekiq service.
Open the file for the application entrypoint script:
The first important line is set -e, which tells the / bin / sh shell that runs the script to fail fast if there are any problems later in the script.
Next, the script checks that tmp / pids / server.pid is not present to ensure that there won't be server conflicts when we start the application.
Finally, the script starts the Rails server with the bundle exec rails s command.
We use the -b option with this command to bind the server to all IP addresses rather than to the default, localhost.
This invocation makes the Rails server route incoming requests to the container IP rather than to the default localhost.
Make the script executable:
Next, we will create a script to start our sidekiq service, which will process our Sidekiq jobs.
For more information about how this application uses Sidekiq, please see How To Add Sidekiq and Redis to a Ruby on Rails Application.
Open a file for the Sidekiq entrypoint script:
Add the following code to the file to start Sidekiq:
This script starts Sidekiq in the context of our application bundle.
Make it executable:
With your entrypoint scripts and Dockerfile in place, you are ready to define your services in your Compose file.
Step 4 - Defining Services with Docker Compose
Using Docker Compose, we will be able to run the multiple containers required for our setup.
We will define our Compose services in our main docker-compose.yml file.
A service in Compose is a running container, and service definitions - which you will include in your docker-compose.yml file - contain information about how each container image will run.
The Compose tool allows you to define multiple services to build multi-container applications.
Our application setup will include the following services:
The application itself
The PostgreSQL database
Redis
Sidekiq
We will also include a bind mount as part of our setup, so that any code changes we make during development will be immediately synchronized with the containers that need access to this code.
Note that we are not defining a test service, since testing is outside of the scope of this tutorial and series, but you could do so by following the precedent we are using here for the sidekiq service.
Open the docker-compose.yml file:
First, add the application service definition:
The app service definition includes the following options:
build: This defines the configuration options, including the context and dockerfile, that will be applied when Compose builds the application image.
If you wanted to use an existing image from a registry like Docker Hub, you could use the image instruction instead, with information about your username, repository, and image tag.
context: This defines the build context for the image build - in this case, the current project directory.
dockerfile: This specifies the Dockerfile in your current project directory as the file Compose will use to build the application image.
depends _ on: This sets up the database and redis containers first so that they are up and running before app.
ports: This maps port 3000 on the host to port 3000 on the container.
volumes: We are including two types of mounts here:
The first is a bind mount that mounts our application code on the host to the / app directory on the container.
This will facilitate rapid development, since any changes you make to your host code will be populated immediately in the container.
The second is a named volume, gem _ cache.
When the bundle install instruction runs in the container, it will install the project gems.
Adding this volume means that if you recreate the container, the gems will be mounted to the new container.
This mount presumes that there haven't been any changes to the project, so if you do make changes to your project gems in development, you will need to remember to delete this volume before recreating your application service.
The third volume is a named volume for the node _ modules directory.
Rather than having node _ modules mounted to the host, which can lead to package discrepancies and permissions conflicts in development, this volume will ensure that the packages in this directory are persisted and reflect the current state of the project.
Again, if you modify the project's Node dependencies, you will need to remove and recreate this volume.
env _ file: This tells Compose that we would like to add environment variables from a file called .env located in the build context.
environment: Using this option allows us to set a non-sensitive environment variable, passing information about the Rails environment to the container.
Next, below the app service definition, add the following code to define your database service:
Unlike the app service, the database service pulls a postgres image directly from Docker Hub.
Note that we're also pinning the version here, rather than setting it to latest or not specifying it (which defaults to latest).
This way, we can ensure that this setup works with the versions specified here and avoid unexpected surprises with breaking code changes to the image.
We are also including a db _ data volume here, which will persist our application data in between container starts.
Additionally, we've mounted our init.sql startup script to the appropriate directory, docker-entrypoint-initdb.d / on the container, in order to create our sammy database user.
After the image entrypoint creates the default postgres user and database, it will run any scripts found in the docker-entrypoint-initdb.d / directory, which you can use for necessary initialization tasks.
For more details, look at the Initialization scripts section of the PostgreSQL image documentation
Next, add the redis service definition:
Like the database service, the redis service uses an image from Docker Hub.
In this case, we are not persisting the Sidekiq job cache.
Finally, add the sidekiq service definition:
Our sidekiq service resembles our app service in a few respects: it uses the same build context and image, environment variables, and volumes.
However, it is dependent on the app, redis, and database services, and so will be the last to start.
Additionally, it uses an entrypoint that will override the entrypoint set in the Dockerfile.
This entrypoint setting points to entrypoints / sidekiq-entrypoint.sh, which includes the appropriate command to start the sidekiq service.
As a final step, add the volume definitions below the sidekiq service definition:
Our top-level volumes key defines the volumes gem _ cache, db _ data, and node _ modules.
When Docker creates volumes, the contents of the volume are stored in a part of the host filesystem, / var / lib / docker / volumes /, that "s managed by Docker.
The contents of each volume are stored in a directory under / var / lib / docker / volumes / and get mounted to any container that uses the volume.
In this way, the shark information data that our users will create will persist in the db _ data volume even if we remove and recreate the database service.
The finished file will look like this:
With your service definitions written, you are ready to start the application.
Step 5 - Testing the Application
With your docker-compose.yml file in place, you can create your services with the docker-compose up command and seed your database.
You can also test that your data will persist by stopping and removing your containers with docker-compose down and recreating them.
First, build the container images and create the services by running docker-compose up with the -d flag, which will run the containers in the background:
You will see output that your services have been created:
You can also get more detailed information about the startup processes by displaying the log output from the services:
You will see something like this if everything has started correctly:
You can also check the status of your containers with docker-compose ps:
You will see output indicating that your containers are running:
Next, create and seed your database and run migrations on it with the following docker-compose exec command:
The docker-compose exec command allows you to run commands in your services; we are using it here to run rake db: setup and db: migrate in the context of our application bundle to create and seed the database and run migrations.
As you work in development, docker-compose exec will prove useful to you when you want to run migrations against your development database.
You will see the following output after running this command:
With your services running, you can visit localhost: 3000 or http: / / your _ server _ ip: 3000 in the browser.
You will see a landing page that looks like this:
We can now test data persistence.
Create a new shark by clicking on Get Shark Info button, which will take you to the sharks / index route:
Sharks Index Page with Seeded Data
To verify that the application is working, we can add some demo information to it. Click on New Shark.
You will be prompted for a username (sammy) and password (shark), thanks to the project's authentication settings.
On the New Shark page, input "Mako" into the Name field and "Fast" into the Facts field.
Click on the Create Shark button to create the shark.
Once you have created the shark, click Home on the site's navbar to get back to the main application landing page.
We can now test that Sidekiq is working.
Click on the Which Sharks Are in Danger?
button.
Since you have not uploaded any endangered sharks, this will take you to the endangered index view:
Endangered Index View
Click on Import Endangered Sharks to import the sharks.
You will see a status message telling you that the sharks have been imported:
Begin Import
You will also see the beginning of the import.
Refresh your page to see the entire table:
Refresh Table
Thanks to Sidekiq, our large batch upload of endangered sharks has succeeded without locking up the browser or interfering with other application functionality.
Click on the Home button at the bottom of the page, which will bring you back to the application main page:
From here, click on Which Sharks Are in Danger?
You will see the uploaded sharks once again.
Now that we know our application is working properly, we can test our data persistence.
Back at your terminal, type the following command to stop and remove your containers:
Note that we are not including the --volumes option; hence, our db _ data volume is not removed.
The following output confirms that your containers and network have been removed:
Recreate the containers:
Open the Rails console on the app container with docker-compose exec and bundle exec rails console:
At the prompt, inspect the last Shark record in the database:
You will see the record you just created:
You can then check to see that your Endangered sharks have been persisted with the following command:
Your db _ data volume was successfully mounted to the recreated database service, making it possible for your app service to access the saved data. If you navigate directly to the index shark page by visiting localhost: 3000 / sharks or http: / / your _ server _ ip: 3000 / sharks you will also see that record displayed:
Sharks Index Page with Mako
Your endangered sharks will also be at the localhost: 3000 / endangered / data or http: / / your _ server _ ip: 3000 / endangered / data view:
Your application is now running on Docker containers with data persistence and code synchronization enabled.
You can go ahead and test out local code changes on your host, which will be synchronized to your container thanks to the bind mount we defined as part of the app service.
By following this tutorial, you have created a development setup for your Rails application using Docker containers.
You "ve made your project more modular and portable by extracting sensitive information and decoupling your application" s state from your code.
You have also configured a boilerplate docker-compose.yml file that you can revise as your development needs and requirements change.
As you develop, you may be interested in learning more about designing applications for containerized and Cloud Native workflows.
Please see Architecting Applications for Kubernetes and Modernizing Applications for Kubernetes for more information on these topics.
Or, if you would like to invest in a Kubernetes learning sequence, please have a look at out Kubernetes for Full-Stack Developers curriculum.
To learn more about the application code itself, please see the other tutorials in this series:
How To Build a Ruby on Rails Application
How To Create Nested Resources for a Ruby on Rails Application
How To Add Stimulus to a Ruby on Rails Application
How To Add Bootstrap to a Ruby on Rails Application
How To Add Sidekiq and Redis to a Ruby on Rails Application
Kubernetes for Full-Stack Developers
3688
Kubernetes for Full-Stack Developers eBook in EPUB format
Kubernetes for Full-Stack Developers eBook in PDF format < $>
This book is designed to help newcomers and experienced users alike learn about Kubernetes.
Its chapters are designed to introduce core Kubernetes concepts and to build on them to a level where running an application on a production cluster is a familiar, repeatable, and automated process.
From there, more advanced topics are introduced, like how to manage a Kubernetes cluster itself.
There are numerous tools, networking configurations, and processes that can help make Kubernetes more approachable.
This book will examine each topic in turn so that anyone who follows along will be able to build, manage, and monitor a Kubernetes cluster on their own.
This book is based on the Kubernetes for Full-Stack Developers curriculum found on DigitalOcean Community.
It is structured around a few central topics:
Learning Kubernetes core concepts
Modernizing applications to work with containers
Containerizing applications
Deploying applications to Kubernetes
Managing cluster operations
You should not feel obliged to follow the topics in any particular order.
If one section is more interesting or relevant to you, explore it and come back to the others later if you prefer.
Likewise, if you are already familiar with the concepts and tools in a given section, feel free to skip that one and focus on other topics.
For additional Kubernetes resources and to participate in the DigitalOcean community of other developers using Kubernetes, check out our growing library of tutorials, questions, and projects with the Kubernetes tag.
How To Get Started With the Requests Library in Python
3540
In many web apps, it's normal to connect to various third-party services by using APIs.
When you use these APIs you can get access to data like weather information, sports scores, movie listings, tweets, search engine results, and pictures.
You can also use APIs to add functionality to your app. Examples of these are payments, scheduling, emails, translations, maps, and file transfers.
If you were to create any of those on your own it would take a ton of time, but with APIs, it can take only minutes to connect to one and access its features and data.
In this article, we'll learn about the Python Requests library, which allows you to send HTTP requests in Python.
And since using an API is sending HTTP requests and receiving responses, Requests allows you to use APIs in Python.
We'll demonstrate the use of a language translation API here so you can see an example of how it works.
Quick Overview of HTTP Requests
HTTP requests are how the web works.
Every time you navigate to a web page, your browser makes multiple requests to the web page's server.
The server then responds with all the data necessary to render the page, and your browser then actually renders the page so you can see it.
The generic process is this: a client (like a browser or Python script using Requests) will send some data to a URL, and then the server located at the URL will read the data, decide what to do with it, and return a response to the client.
Finally, the client can decide what to do with the data in the response.
Part of the data the client sends in a request is the request method.
Some common request methods are GET, POST, and PUT.
GET requests are normally for reading data only without making a change to something, while POST and PUT requests generally are for modifying data on the server.
So for example, the Stripe API allows you to use POST requests to create a new charge so a user can purchase something from your app.
< $> note Note: This article will cover GET requests, because we won't be modifying any data on a server.
When sending a request from a Python script or inside a web app, you, the developer, gets to decide what gets sent in each request and what to do with the response.
So let's explore that by first sending a request to Scotch.io and then by using a language translation API.
Install Python Requests
Before we can do anything, we need to install the library.
So let's go ahead and install requests using pip.
It's a good idea to create a virtual environment first if you don't already have one.
Our First Request
To start, let's use Requests for requesting the Scotch.io site.
Create a file called script.py and add the following code to it. In this article, we won't have much code to work with, so when something changes you can just update the existing code instead of adding new lines.
So all this code is doing is sending a GET request to Scotch.io.
This is the same type of request your browser sent to view this page, but the only difference is that Requests can't actually render the HTML, so instead you will just get the raw HTML and the other response information.
We're using the .get () function here, but Requests allows you to use other functions like .post () and .put () to send those requests as well.
You can run it by executing the script.py file.
And here's what you get in return: script run with output of Response 200
Status Codes
The first thing we can do is check the status code.
HTTP codes range from the 1XX to 5XX.
Common status codes that you have probably seen are 200, 404, and 500.
Here's a quick overview of what each status code means:
1XX - Information
2XX - Success
3XX - Redirect
4XX - Client Error (you made an error)
5XX - Server Error (they made an error)
Generally, what you're looking for when you perform your own requests are status codes in the 200s.
Requests recognizes that 4XX and 5XX status codes are errors, so if those status codes get returned, the response object from the request evaluates to False.
You can test if a request responded successfully by checking the response for truth.
The Response 200 output with a follow up of Response OK
The message "Response Failed" will only appear if a 400 or 500 status code returns.
Try changing the URL to some nonsense to see the response fail with a 404.
You can take a look at the status code directly by adding:
This will show you the status code directly so you can check the number yourself.
Failure output with 404
Headers
Another thing you can get from the response are the headers.
You can take a look at them by using the headers dictionary on the response object.
output with headers printed in standard out
Headers are sent along with the request and returned in the response.
Headers are used so both the client and the server know how to interpret the data that is being sent and received in the response / response.
We see the various headers that are returned.
A lot of times you won't need to use the header information directly, but it's there if you need it.
The content type is usually the one you may need because it reveals the format of the data, for example HTML, JSON, PDF, text, etc. But the content type is normally handled by Requests so you can access the data that gets returned.
Response Text
And finally, if we take a look at res.text (this works for textual data, like a HTML page like we are viewing) we can see all the HTML needed to build the home page of Scotch.
It won't be rendered, but we see that it looks like it belongs to Scotch.
If you saved this to a file and opened it, you would see something that resembled the Scotch site.
In a real situation, multiple requests are made for a single web page to load things like images, scripts, and stylesheets, so if you save only the HTML to a file, it won't look anything like what the Scotch.io page looks like in your browser because only a single request was performed to get the HTML data.
Printed HTML data on the command line
Using the Translate API
So now let's move on to something more interesting.
We'll use the Yandex Translate API to perform a request to translate some text to a different language.
To use the API, first you need to sign up.
After you sign up, go to the Translate API and create an API key.
Once you have the API key, add it to your file as a constant.
Here's the link where you can do all those things: https: / / tech.yandex.com / translate /
The reason why we need an API key is so Yandex can authenticate us every time we want to use their API.
The API key is a lightweight form of authentication, because it's added on to the end of the request URL when being sent.
To know which URL we need to send to use the API, we can look at the documentation for Yandex.
If we look there, we'll see all the information needed to use their Translate API to translate text.
Request syntax for using the API
When we see a URL with ampersands (&), question marks (?), and equals signs (=), you can be sure that the URL is for GET requests.
Those symbols specify the parameters that go along with the URL.
Normally things in square brackets () will be optional.
In this case, format, options, and callback are optional, while the key, text, and lang are required for the request.
So let's add some code to send to that URL.
You can replace the first request we created with this:
There are two ways we can add the parameters.
We can either append it to the end of the URL directly, or we can have Requests do it for us.
To do the latter, we can create a dictionary for our parameters.
The three items we need are the key, the text, and the language.
Let's create the dictionary using the API key, 'Hello' for the text, and 'en-es' as the lang, which means we want to translate from English to Spanish.
If you need to know any other language codes, you can look here.
You are looking for the 639-1 column.
We create a params dictionary by using the dict () function and passing in the keys and values we want in our dictionary.
Now we take the parameters dictionary and pass it to the .get () function.
When we pass the parameters this way, Requests will go ahead and add the parameters to the URL for us.
Now let's add a print statement for the response text and view what gets returned in the response.
output dictionary with the values inputted
We see three things.
We see the status code, which is exactly the same status code of the response itself, we see the language that we specified, and we see the translated text inside of the list.
So you should see 'Hola' for the translated text.
Try again with en-fr as the language code, and you should see 'Bonjour' in the response now.
French translated text
Let's take a look at the headers for this particular response.
Headers printed in output
Obviously the headers should be different because we're communicating with a different server, but in this case the content type is application / json instead of text / html.
What this means that the data can be interpreted as JSON.
When application / json is the content type of the response, we are able to have Requests convert the response to a dictionary and list so we can access the data easier.
To have the data parsed as JSON, we use the .json () method on the response object.
If you print it, you'll see that the data looks the same, but the format is slightly different.
The reason why it's different is because it's no longer plain text that you get from res.text.
This time it's a printed version of a dictionary.
Let's say we want to access the text.
Since this is now a dictionary, we can use the text key.
And now we only see the data for that one key.
In this case we are looking at a list of one item, so if we wanted to get that text in the list directly, we can access it by the index.
"Bonjour" without the square brackets
And now the only thing we see is the translated word.
So of course if we change things in our parameters, we'll get different results.
Let's change the text to be translated from Hello to Goodbye, change the target language back to Spanish, and send the request again.
"Adios" printed to output Try translating longer text in different languages and see what responses the API gives you.
Translate API Error Cases
Finally, we'll take a look at an error case.
Everything doesn't always work, so we need to know when that happens.
Try changing your API key by removing one character.
When you do this your API key will no longer be valid.
Then try sending a request.
If you take a look at the status code, this is what you get:
403 error So when you are using the API, you'll want to check if things are successful or not so you can handle the error cases according to the needs of your app.
Here's what we learned:
How HTTP requests work
The various status codes possible in a response
How to send requests and receive responses using the Python Requests library
How to use a language translation API to translate text
How to convert application / JSON content responses to dictionaries
If you want to do more, check out this list to see different APIs that are available, and try to use them with Python Requests.
How To Install and Use Radamsa to Fuzz Test Programs and Network Services on Ubuntu 18.04
3537
Security threats are continually becoming more sophisticated, so developers and systems administrators need to take a proactive approach in defending and testing the security of their applications.
A common method for testing the security of client applications or network services is fuzzing, which involves repeatedly sending invalid or malformed data to the application and analyzing its response.
This is useful to help test how resilient and robust the application is to unexpected input, which may include corrupted data or actual attacks.
Radamsa is an open-source fuzzing tool that can generate test cases based on user-specified input data. Radamsa is fully scriptable, and so far has been successful in finding vulnerabilities in real-world applications, such as Gzip.
In this tutorial, you will install and use Radamsa to fuzz test command-line and network-based applications using your own test cases.
< $> warning Warning: Radamsa is a penetration testing tool which may allow you to identify vulnerabilities or weaknesses in certain systems or applications.
You must not use vulnerabilities found with Radamsa for any form of reckless behavior, harm, or malicious exploitation.
Vulnerabilities should be ethically reported to the maintainer of the affected application, and not disclosed publicly without explicit permission.
One Ubuntu 18.04 server set up by following the Initial Server Setup with Ubuntu 18.04, including a sudo non-root user and enabled firewall to block non-essential ports.
A command-line or network-based application that you wish to test, for example Gzip, Tcpdump, Bind, Apache, jq, or any other application of your choice.
As an example for the purposes of this tutorial, we'll use jq.
< $> warning Warning: Radamsa can cause applications or systems to run unstably or crash, so only run Radamsa in an environment where you are prepared for this, such as a dedicated server.
Please also ensure that you have explicit written permission from the owner of a system before conducting fuzz testing against it. < $>
Step 1 - Installing Radamsa
Firstly, you will download and compile Radamsa in order to begin using it on your system.
The Radamsa source code is available in the official repository on GitLab.
Then, install the gcc, git, make, and wget packages needed to compile the source code into an executable binary:
After confirming the installation, apt will download and install the specified packages and all of their required dependencies.
Next, you'll download a copy of the source code for Radamsa by cloning it from the repository hosted on GitLab:
This will create a directory called radamsa, containing the source code for the application.
Move into the directory to begin compiling the code:
Next, you can start the compilation process using make:
Finally, you can install the compiled Radamsa binary to your $PATH:
If you see a radamsa: command not found error, double-check that all required dependencies were installed and that there were no errors during compilation.
Now that you've installed Radamsa, you can begin to generate some sample test cases to understand how Radamsa works and what it can be used for.
Step 2 - Generating Fuzzing Test Cases
Now that Radamsa has been installed, you can use it to generate some fuzzing test cases.
A test case is a piece of data that will be used as input to the program that you are testing.
For example, if you are fuzz testing an archiving program such as Gzip, a test case may be a file archive that you are attempting to decompress.
< $> note Note: Radamsa will manipulate input data in a wide variety of unexpected ways, including extreme repetition, bit flips, control character injection, and so on.
This may cause your terminal session to break or become unstable, so be aware of this before proceeding.
Firstly, pass a simple piece of text to Radamsa to see what happens:
This will manipulate (or fuzz) the inputted data and output a test case, for example:
In this case, Radamsa added an extra comma between Hello and world.
This may not seem like a significant change, but in some applications this may cause the data to be interpreted incorrectly.
Let's try again by running the same command.
You'll see different output:
This time, multiple single quotes (') were inserted into the string, including one that overwrote the l in world.
This particular test case is more likely to result in problems for an application, as single / double quotes are often used to separate different pieces of data in a list.
Let's try one more time:
In this case, Radamsa inserted a shell injection string, which will be useful to test for command injection vulnerabilities in the application that you are testing.
You've used Radamsa to fuzz an input string and produce a series of test cases.
Next, you will use Radamsa to fuzz a command-line application.
Step 3 - Fuzzing a Command-line Application
In this step, you'll use Radamsa to fuzz a command-line application and report on any crashes that occur.
The exact technique for fuzzing each program varies massively, and different methods will be most effective for different programs.
However, in this tutorial we will use the example of jq, which is a command-line program for processing JSON data.
You may use any other similar program as long as it follows the general principle of taking some form of structured or unstructured data, doing something with it, and then outputting a result.
For instance this example would also work with Gzip, Grep, bc, tr, and so on.
If you don't already have jq installed, you can install it using apt:
jq will now be installed.
To begin fuzzing, create a sample JSON file that you'll use as the input to Radamsa:
Then, add the following sample JSON data to the file:
You can parse this file using jq if you wish to check that the JSON syntax is valid:
If the JSON is valid, jq will output the file.
Otherwise, it will display an error, which you can use to correct the syntax where required.
Next, fuzz the test JSON file using Radamsa and then pass it to jq.
This will cause jq to read the fuzzed / manipulated test case, rather than the original valid JSON data:
If Radamsa fuzzes the JSON data in a way that it is still syntactically valid, jq will output the data, but with whatever changes Radamsa made to it.
Alternatively, if Radamsa causes the JSON data to become invalid, jq will display a relevant error.
The alternate outcome would be that jq is unable to correctly handle the fuzzed data, causing it to crash or misbehave.
This is what you're really looking for with fuzzing, as this may be indicative of a security vulnerability such as a buffer overflow or command injection.
In order to more efficiently test for vulnerabilities like this, a Bash script can be used to automate the fuzzing process, including generating test cases, passing them to the target program and capturing any relevant output.
Create a file named jq-fuzz.sh:
The exact script content will vary depending on the type of program that you're fuzzing and the input data, but in the case of jq and other similar programs, the following script suffices.
Copy the script into your jq-fuzz.sh file:
This script contains a while to make the contents loop repeatedly.
Each time the script loops, Radamsa will generate a test case based on test.json and save it to input.txt.
The input.txt test case will then be run through jq, with all standard and error output redirected to / dev / null to avoid filling up the terminal screen.
Finally, the exit value of jq is checked.
If the exit value is greater than 127, this is indicative of a fatal termination (a crash), then the input data is saved for review at a later date in a file named crash- followed by the current date in Unix seconds and nanoseconds.
Mark the script as executable and set it running in order to begin automatically fuzz testing jq:
You can issue CTRL + C at any time to terminate the script.
You can then check whether any crashes have been found by using ls to display a directory listing containing any crash files that have been created.
You may wish to improve your JSON input data since using a more complex input file is likely to improve the quality of your fuzzing results.
Avoid using a large file or one that contains a lot of repeated data - an ideal input file is one that is small in size, yet still contains as many 'complex' elements as possible.
For example, a good input file will contain samples of data stored in all formats, including strings, integers, booleans, lists, and objects, as well as nested data where possible.
You've used Radamsa to fuzz a command-line application.
Next, you'll use Radamsa to fuzz requests to network services.
Step 4 - Fuzzing Requests to Network Services
Radamsa can also be used to fuzz network services, either acting as a network client or server.
In this step, you'll use Radamsa to fuzz a network service, with Radamsa acting as the client.
The purpose of fuzzing network services is to test how resilient a particular network service is to clients sending it malformed and / or malicious data. Many network services such as web servers or DNS servers are usually exposed to the internet, meaning that they are a common target for attackers.
A network service that is not sufficiently resistant to receiving malformed data may crash, or even worse fail in an open state, allowing attackers to read sensitive data such as encryption keys or user data.
The specific technique for fuzzing network services varies enormously depending on the network service in question, however in this example we will use Radamsa to fuzz a basic web server serving static HTML content.
Firstly, you need to set up the web server to use for testing.
You can do this using the built-in development server that comes with the php-cli package.
You'll also need curl in order to test your web server.
If you don't have php-cli and / or curl installed, you can install them using apt:
Next, create a directory to store your web server files in and move into it:
Then, create a HTML file containing some sample text:
Add the following to the file:
You can now run your PHP web server.
You'll need to be able to view the web server log while still using another terminal session, so open another terminal session and SSH to your server for this:
You can now switch back to your original terminal session and test that the web server is working using curl:
This will output the sample index.html file that you created earlier:
Your web server only needs to be accessible locally, so you should not open any ports on your firewall for it.
Now that you've set up your test web server, you can begin to fuzz test it using Radamsa.
First, you'll need to create a sample HTTP request to use as the input data for Radamsa.
Create a new file to store this in:
Then, copy the following sample HTTP request into the file:
Next, you can use Radamsa to submit this HTTP request to your local web server.
In order to do this, you'll need to use Radamsa as a TCP client, which can be done by specifying an IP address and port to connect to:
< $> note Note: Be aware that using Radamsa as a TCP client will potentially cause malformed / malicious data to be transmitted over the network.
This may break things, so be very careful to only access networks that you are authorized to test, or preferably, stick to using the localhost (127.0.0.1) address.
Finally, if you view the outputted logs for your local web server, you'll see that it has received the requests, but most likely not processed them as they were invalid / malformed.
The outputted logs will be visible in your second terminal window:
For optimal results and to ensure that crashes are recorded, you may wish to write an automation script similar to the one used in Step 3. You should also consider using a more complex input file, which may contain additions such as extra HTTP headers.
You've fuzzed a network service using Radamsa acting as a TCP client.
Next, you will fuzz a network client with Radamsa acting as a server.
Step 5 - Fuzzing Network Client Applications
In this step, you will use Radamsa to fuzz test a network client application.
This is achieved by intercepting responses from a network service and fuzzing them before they are received by the client.
The purpose of this kind of fuzzing is to test how resilient network client applications are to receiving malformed or malicious data from network services.
For example, testing a web browser (client) receiving malformed HTML from a web server (network service), or testing a DNS client receiving malformed DNS responses from a DNS server.
As was the case with fuzzing command-line applications or network services, the exact technique for fuzzing each network client application varies considerably, however in this example you will use whois, which is a simple TCP-based send / receive application.
The whois application is used to make requests to WHOIS servers and receive WHOIS records as responses.
WHOIS operates over TCP port 43 in clear text, making it a good candidate for network-based fuzz testing.
If you don't already have whois available, you can install it using apt:
First, you'll need to acquire a sample whois response to use as your input data. You can do this by making a whois request and saving the output to a file.
You can use any domain you wish here as you're testing the whois program locally using sample data:
Next, you'll need to set up Radamsa as a server that serves fuzzed versions of this whois response.
You'll need to be able to continue using your terminal once Radamsa is running in server mode, so it is recommended to open another terminal session and SSH connection to your server for this:
Radamsa will now be running in TCP server mode, and will serve a fuzzed version of whois.txt each time a connection is made to the server, no matter what request data is received.
You can now proceed to testing the whois client application.
You'll need to make a normal whois request for any domain of your choice (it doesn't have to be the same one that the sample data is for), but with whois pointed to your local Radamsa server:
The response will be your sample data, but fuzzed by Radamsa.
You can continue to make requests to the local server as long as Radamsa is running, and it will serve a different fuzzed response each time.
As with fuzzing network services, to improve the efficiency of this network client fuzz testing and ensure that any crashes are captured, you may wish to write an automation script similar to the one used in Step 3.
In this final step, you used Radamsa to conduct fuzz testing of a network client application.
In this article you set up Radamsa and used it to fuzz a command-line application, a network service, and a network client.
You now have the foundational knowledge required to fuzz test your own applications, hopefully with the result of improving their robustness and resistance to attack.
If you wish to explore Radamsa further, you may wish to review the Radamsa README file in detail, as it contains further technical information and examples of how the tool can be used:
Radamsa README File
You may also wish to check out some other fuzzing tools such as American Fuzzy Lop (AFL), which is an advanced fuzzing tool designed for testing binary applications at extremely high speed and accuracy:
American Fuzzy Lop
How To Install Docker Compose on Debian 10
3541
Docker is a great tool for automating the deployment of Linux applications inside software containers, but to take full advantage of its potential each component of an application should run in its own individual container.
For complex applications with a lot of components, orchestrating all the containers to start up, communicate, and shut down together can quickly become unwieldy.
The Docker community came up with a popular solution called Fig, which allowed you to use a single YAML file to orchestrate all of your Docker containers and configurations.
This became so popular that the Docker team decided to make Docker Compose based on the Fig source, which is now deprecated.
Docker Compose lets users orchestrate the processes of Docker containers, including starting up, shutting down, and setting up intra-container linking and volumes.
In this tutorial, you'll install the latest version of Docker Compose to help you manage multi-container applications on a Debian 10 server.
A Debian 10 server and a non-root user with sudo privileges.
This initial server setup with Debian 10 tutorial explains how to set this up.
Docker installed with the instructions from Step 1 and Step 2 of How To Install and Use Docker on Debian 10
< $> note Note: Even though the Prerequisites give instructions for installing Docker on Debian 10, the docker commands in this article should work on other operating systems as long as Docker is installed.
Although you can install Docker Compose from the official Debian repositories, it is several minor versions behind the latest release, so in this tutorial you'll install it from Docker's GitHub repository.
The command that follows is slightly different than the one you'll find on the Releases page.
By using the -o flag to specify the output file first rather than redirecting the output, this syntax avoids running into a "permission denied" error caused when using sudo.
Check the current release and, if necessary, update it in the command that follows:
Next we'll set the permissions:
Then we'll verify that the installation was successful by checking the version:
This will print out the version we installed:
Now that we have Docker Compose installed, we're ready to run a "Hello World" example.
Step 2 - Running a Container with Docker Compose
The public Docker registry, Docker Hub, includes a Hello World image for demonstration and testing.
It illustrates the minimal configuration required to run a container using Docker Compose: a YAML file that calls a single image.
We'll create this minimal configuration to run our hello-world container.
First, create a directory for the YAML file and switch to it:
Then create the YAML file:
Put the following contents into the file, save the file, and exit the text editor:
The first line in the YAML file is used as part of the container name.
The second line specifies which image to use to create the container.
When we run the docker-compose up command, it will look for a local image by the name we specified, hello-world.
With this in place, we "ll save and exit the file.
You can look manually at images on our system with the docker images command:
When there are no local images at all, only the column headings display:
Now, while still in the ~ / hello-world directory, execute the following command:
The first time you run the command, if there's no local image named hello-world, Docker Compose will pull it from the Docker Hub public repository:
After pulling the image, docker-compose creates a container, attaches, and runs the hello program, which in turn confirms that the installation appears to be working:
Then it prints an explanation of what it did:
Docker containers only run as long as the command is active, so once hello finished running, the container stopped.
Consequently, when we look at active processes, the column headers will appear, but the hello-world container won't be listed because it's not running:
You can see the container information, which you'll need in the next step, by using the -a flag.
This shows all containers, not just active ones:
This displays the information you'll need to remove the container when you're done with it.
Step 3 - Removing the Image (Optional)
To avoid using unnecessary disk space, we'll remove the local image.
To do so, we'll need to delete all the containers that reference the image using the docker rm command, followed by either the CONTAINER ID or the NAME.
In the following example, we're using the CONTAINER ID from the docker ps -a command we just ran.
Be sure to substitute the ID of your container:
Once all containers that reference the image have been removed, we can remove the image:
You've installed Docker Compose on Debian 10, tested your installation by running a Hello World example, and removed the test image and container.
While the Hello World example confirmed your installation, this basic configuration does not show one of the main benefits of Docker Compose - being able to bring a group of Docker containers up and down all at the same time.
To see how to use Docker Compose in more detail, take a look at How To Install WordPress With Docker Compose.
How To Install Linux, Nginx, MySQL, PHP (LEMP) Stack on CentOS 8 Quickstart
3856
In this tutorial, you'll install a LEMP stack on a CentOS 8 server.
For a more detailed version of this tutorial, with more explanations of each step, please refer to How To Install Linux, Nginx, MySQL, PHP (LEMP) Stack on CentOS 8.
To follow this guide, you'll need access to a CentOS 8 server as a sudo user.
Step 1 - Install Nginx
If firewalld is active, you'll need to run the following command to allow external access on port 80 (HTTP):
Step 2 - Install MariaDB
We'll now install MariaDB, a community-developed fork of the original MySQL server by Oracle.
Start the interactive script with:
Step 3 - Install PHP-FPM
We'll install nano to facilitate editing these files:
Look for the user and group directives.
Make sure to change both values from apache to nginx:
Step 4 - Test PHP with Nginx
We'll only need to modify the default owner and group on Nginx's document root, so that you can create and modify files in that location using your regular non-root system user:
Copy this content to your info.php file, and don't forget to save it when you're done.
How to Install Tinc and Set Up a Basic VPN on Ubuntu 18.04
3338
Tinc is an open-source Virtual Private Network (VPN) daemon with useful features like encryption, optional compression, and automatic mesh routing that can opportunistically route VPN traffic directly between servers.
These features differentiate tinc from other VPN solutions, and make it a good choice for creating a VPN out of many small, geographically distributed networks.
In this tutorial, we will go over how to use tinc to create a secure VPN on which your servers can communicate as if they were on a local network.
We will also demonstrate how to use tinc to set up a secure tunnel into a private network.
We will be using Ubuntu 18.04 servers, but the configurations can be adapted for use with any other OS.
Goals
In order to cover multiple use cases, this tutorial outlines how to connect one client node to the VPN over a private network interface and another over a public one.
You can, however, adapt this setup to suit your own needs.
You'll just need to plan out how you want your servers to access each other and adapt the examples presented in this tutorial to your own needs.
If you are adapting this to your own setup, be sure to substitute the highlighted values in the examples with your own values.
It may be in your interest, though, to first follow the tutorial as it's written to make sure you understand the components and processes involved before modifying these instructions.
To help keep things clear, this tutorial will refer to the servers like this:
server-01: All of the VPN nodes will connect to this machine, and the connection must be maintained for proper VPN functionality.
Additional servers can be configured in the same way as this one to provide redundancy, if desired
client-01: Connects to the server-01 VPN node using its private network interface
client-02: Connects to the server-01 VPN node over the public network interface
< $> note Note: Tinc itself doesn't differentiate between servers (machines that host and deliver VPN services) and clients (the machines that connect to and use the secure private network), but it can be helpful to understand and visualize how tinc works by thinking of your servers like this.
Here is a diagram of the VPN that we want to set up:
Tinc VPN Setup
The blue box represents our VPN and the pink represents the underlying private network.
All three servers can communicate on the VPN, even though the private network is otherwise inaccessible to client-02.
If you would like to follow this tutorial exactly, provision two Ubuntu 18.04 servers (server-01 and client-01) in the same datacenter and enable private networking on each.
Then, create another Ubuntu 18.04 server (client-02) in a separate datacenter.
Each server should have an administrative user and a firewall configured with ufw.
To set this up, follow our initial server setup guide for Ubuntu 18.04.
Additionally, later on in this tutorial we'll need to transfer a few files between each machine using scp.
Because of this, you'll need to generate SSH keys on each of your servers, add both client-01 and client-02's SSH keys to server-01's authorized _ keys file, and then add server-01's SSH key to both client-01 and client-02's authorized _ keys files.
For help setting this up, see our guide on How to Set Up SSH Keys on Ubuntu 18.04.
Step 1 - Installing Tinc
Tinc is available from the default Ubuntu APT repositories, which means we can install it with just a few commands.
If you've not done so recently, run the following command on each server to update their respective package indexes:
Then install tinc on each server by running the following command:
With that, you've installed tinc on each of your servers.
However, you'll need to make some changes to tinc's configuration on each machine in order to get your VPN up and running.
Let's begin with updating server-01.
Step 2 - Configuring the Tinc Server
Tinc requires that every machine that will be part of the VPN has the following three configuration components:
Tinc configuration files: There are three distinct files that configure the tinc daemon:
tinc.conf, which defines the netname, the network device over which the VPN will run, and other VPN options;
tinc-up, a script that activates the network device defined in tinc.conf after tinc is started;
tinc-down, which deactivates the network device whenever tinc stops.
Public / private key pairs: Tinc uses public / private key pairs to ensure that only users with valid keys are able to access the VPN.
Host configuration files: Each machine (or host) on the VPN has its own configuration file that holds the host's actual IP address and the subnet where tinc will serve it
Tinc uses a netname to distinguish one tinc VPN from another.
This is helpful in cases where you want to set up multiple VPNs, but it's recommended that you use a netname even if you are only planning on configuring one VPN.
You can give your VPN whatever netname you like, but for simplicity we will call our VPN < ^ > netname < ^ >.
On server-01, create the configuration directory structure for the VPN:
Use your preferred text editor to create a tinc.conf file.
Add the following lines to the empty file.
These configure a tinc node named < ^ > server _ 01 < ^ > with a network interface called tun0 which will use IPv4:
< $> warning Warning: Note how the value after the Name directive includes an underscore (_) rather than a hyphen (-).
This is important, since tinc requires that the Name value contain only alphanumeric or underscore characters.
If you use a hyphen here, you'll encounter an error when you try to start the VPN later in this guide.
Save and close the file after adding these lines.
Next, create a host configuration file named server _ 01 in the hosts subdirectory.
Ultimately, the client nodes will use this file to communicate with server-01:
Again, note that the name of this file contains an underscore rather than a hyphen.
This way, it aligns with the Name directive in the tinc.conf file which will allow tinc to automatically append the server's public RSA key to this file when we generate later on.
Add the following lines to the file, making sure to include server-01's public IP address:
The Address field specifies how other nodes will connect to this server, and Subnet specifies which subnet this daemon will serve.
Next, generate a pair of public and private RSA keys for this host with the following command:
After running this command, you'll be prompted to enter filenames where tinc will save the public and private RSA keys:
Press ENTER to accept the default locations at each prompt; doing so will tell tinc to store the private key in a file named rsa _ key.priv and append the public key to the server _ 01 host configuration file.
Next, create tinc-up, the script that will run whenever the < ^ > netname < ^ > VPN is started:
Here's what each of these lines do:
ip link...: sets the status of tinc's virtual network interface as up
ip addr...: adds the IP address 10.0.0.1 with a netmask of 32 to tinc's virtual network interface, which will cause the other machines on the VPN to see server-01's IP address as 10.0.0.1
ip route...: adds a route (10.0.0.0 / 24) which can be reached on tinc's virtual network interface
Next, create a script to remove the virtual network interface when your VPN is stopped:
These lines have the opposite effects as those in the tinc-up script:
ip route...: deletes the 10.0.0.0 / 24 route
ip addr...: deletes the IP address 10.0.0.1 from tinc's virtual network interface
ip link...: sets the status of tinc's virtual network interface as down
Save and close the file, then make both of these new network scripts executable:
As a final step of configuring server-01, add a firewall rule that will allow traffic through port 655, tinc's default port:
server-01 is now fully configured and you can move on to setting up your client nodes.
Step 3 - Configuring the Client Nodes
Both of your client machines will require a slightly different configuration than the server, although the process will generally be quite similar.
Because of the setup we're aiming for in this guide, we will configure client-01 and client-02 almost identically with only a few slight differences between them.
Hence, many of the commands given in this step must be run on both machines.
Note, though, that if client-01 or client-02 require a specific command or special configuration, those instructions will be shown in a blue or red command block, respectively.
On both client-01 and client-02, replicate the directory structure you created on server-01:
Then create a tinc.conf file:
Add the following lines to the file on both machines:
Be sure to substitute < ^ > node _ name < ^ > with the respective client node's name.
Again, make sure this name uses an underscore (_) rather than a hyphen.
Note that this file contains a ConnectTo directive pointing to server _ 01, while server-01's tinc.conf file didn't include this directive.
By not including a ConnectTo statement on server-01, it means that server-01 will only listen for incoming connections.
This works for our setup since it won't connect to any other machines.
Next, create a host configuration file on each client node.
Again, make sure the file name is spelled with an underscore instead of a hyphen:
For client-01, add this line:
For client-02, add this line:
Note that each client has a different subnet that tinc will serve.
Next, generate the keypairs on each client machine:
Again as you did with server-01, when prompted to select files to store the RSA keys, press ENTER to accept the default choices.
Following that, create the network interface start script on each client:
For client-01, add these lines:
For client-02, add the following:
Save and close each file.
Next, create the network interface stop script on each client:
On client-01, add the following content to the empty file:
On client-02, add the following::
Save and close the files.
Make networking scripts executable by running the following command on each client machine:
Lastly, open up port 655 on each client:
At this point, the client nodes are almost, although not quite, set up.
They still need the public key that we created on server-01 in the previous step in order to authenticate the connection to the VPN.
Step 4 - Distributing the Keys
Each node that wants to communicate directly with another node must have exchanged public keys, which are inside of the host configuration files.
In our case, server-01 needs to exchange public keys with the other nodes.
Exchange Keys Between server-01 and client-01
On client-01, copy its host configuration file to server-01.
Because both client-01 and server-01 are in the same data center and both have private networking enabled, you can use server01's private IP address here:
Then on server-01, copy the client-01 host configuration file into the / etc / tinc / < ^ > netname < ^ > / hosts / directory:
Then, while still on server-01, copy its host configuration file to client-01:
On client-01, copy server-01's file to the appropriate location:
On client-01, edit server-01's host configuration file so the Address field is set to server-01's private IP address.
This way, client-01 will connect to the VPN via the private network:
Change the Address directive to point to server-01's private IP address:
Save and quit.
Now let's move on to our remaining node, client-02.
Exchange Keys Between server-01 and client-02
On client-02, copy its host configuration file to server-01:
Then on server-01, copy the client _ 02 host configuration file into the appropriate location:
Then copy server-01's host configuration file to client-02:
On client-02, copy server-01's file to the appropriate location:
Assuming you're only setting up two client nodes, you're finished distributing public keys.
If, however, you're creating a larger VPN, now is a good time to exchange the keys between those other nodes.
Remember that if you want two nodes to directly communicate with each other (without a forwarding server between), they need to have exchanged their keys / hosts configuration files, and they need to be able to access each other's real network interfaces.
Also, it is fine to just copy each host's configuration file to every node in the VPN.
Step 5 - Testing the Configuration
On each node, starting with server-01, start tinc with the following command:
This command includes the -n flag, which points to the netname for our VPN, < ^ > netname < ^ >.
This is useful if you have more than one VPN set up and you need to specify which one you want to start.
It also includes the -D flag, which prevents tinc from forking and detaching, as well as disables tinc's automatic restart mechanism.
Lastly, it includes the -d flag, which tells tinc to run in debug mode, with a debug level of 3.
< $> note Note: When it comes to the tinc daemon, a debug level of 3 will show every request exchanged between any two of the servers, including authentication requests, key exchanges, and connection list updates.
Higher debug levels show more information regarding network traffic, but for now we're only concerned with whether the nodes can communicate with one another, so a level of 3 will suffice.
In a production scenario, though, you would want to change to a lower debug level so as not to fill disks with log files.
You can learn more about tinc's debug levels by reviewing the official documentation.
After starting the daemon on each node, you should see output with the names of each node as they connect to server-01.
Now let's test the connection over the VPN.
In a separate window, on client-02, ping client-01's VPN IP address.
We assigned this to be 10.0.0.2, earlier:
The ping should work correctly, and you should see some debug output in the other windows about the connection on the VPN.
This indicates that client-02 is able to communicate over the VPN through server-01 to client-01.
Press CTRL + C to quit pinging.
You may also use the VPN interfaces to do any other network communication, like application connections, copying files, and SSH.
On each tinc daemon debug window, quit the daemon by pressing CTRL +\.
Step 6 - Configuring Tinc To Start Up on Boot
Ubuntu servers use systemd as the default system manager to control starting and running processes.
Because of this, we can enable the < ^ > netname < ^ > VPN to start up automatically at boot with a single systemctl command.
Run the following command on each node to set the tinc VPN to start up whenever the machines boot:
Tinc is configured to start at boot on each of your machines and you can control it with the systemctl command.
If you would like to start it now, run the following command on each of your nodes:
< $> note Note: If you have multiple VPNs you enable or start each of them at once, like this:
With that, your tinc VPN fully configured and running on each of your nodes.
Now that you have gone through this tutorial, you should have a good foundation to build out your VPN to meet your needs.
Tinc is very flexible, and any node can be configured to connect to any other node (that it can access over the network) so it can act as a mesh VPN without relying on one individual node.
How To Install WordPress with OpenLiteSpeed on Ubuntu 18.04
3273
WordPress is an open-source content management system (CMS).
The most popular CMS in the world, WordPress allows you to set up blogs and websites on top of a MySQL database backend, using PHP to execute scripts and process dynamic content.
OpenLiteSpeed is an optimized open-source web server that you can use to manage and serve websites.
OpenLiteSpeed has some useful features that make it a solid choice for many installations: Apache-compatible rewrite rules, a built-in web-based administration interface, and customized PHP processing optimized for the server.
This guide will walk through the process of installing and setting up a WordPress instance on Ubuntu 18.04 using the OpenLiteSpeed web server.
Because both WordPress and OpenLiteSpeed can be administered through a web browser, this configuration is ideal for those who do not have regular access to an SSH session or those who may not feel comfortable managing a web server via the command line.
One server running Ubuntu 18.04 with an administrative, non-root user and a firewall configured using ufw.
To set up this environment, follow our Initial Server Setup for Ubuntu 18.04 tutorial.
OpenLiteSpeed installed on your server.
See our guide on How To Install the OpenLiteSpeed Web Server on Ubuntu 18.04 for instructions on installing and configuring OpenLiteSpeed.
MySQL installed on your server.
Follow our How To Install MySQL on Ubuntu 18.04 tutorial to set this up.
Step 1 - Creating a Database and Database User for WordPress
You already have MySQL installed, but as a preparatory step you need to make a database and a user for WordPress to use.
To get started, connect to your server using SSH:
Then log in to the root MySQL account:
< $> note Note: If you completed Step 3 in the prerequisite MySQL tutorial and configured your root MySQL user to authenticate with the mysql _ native _ password plugin, you'll need to log in with the following command:
Then enter your root user's password when prompted.
From the MySQL prompt, create a database with the following command.
Here, we will name this database < ^ > wordpress < ^ > to keep things simple, but you can name it whatever you'd like:
Then, create a user and grant it privileges over the database you just created.
Again, you can give this user any name, but for simplicity we will name it < ^ > wordpressuser < ^ >.
Also, be sure to change < ^ > password < ^ > to a strong password of your own choosing:
Following that, you can close the MySQL prompt:
You're done setting up your MySQL installation to work with WordPress.
Next, we will install a few PHP extensions.
Step 2 - Installing Additional PHP Extensions
In the prerequisite OpenLiteSpeed tutorial, you installed the lsphp73 package.
This is a compilation of PHP optimized for OpenLiteSpeed which uses the LiteSpeed SAPI to communicate with external applications.
Depending on your needs, WordPress may require other certain PHP extensions in order to function as desired.
To install some PHP extensions commonly used with WordPress, run the following command:
< $> note Note: The packages in this command may not cover every use case.
For a full list of the PHP 7.3 extensions available from the LiteSpeed repository you added to your server in the prerequisite tutorial, see the LiteSpeed Wiki.
Following this, you can move on to downloading and setting up WordPress on your server.
Step 3 - Downloading WordPress
Now that your server software is configured, you can install and set up WordPress.
For security reasons in particular, it is always recommended that you get the latest version of WordPress directly from their site.
We will move these files into our document root momentarily, but first we will create a few files and directories that the WordPress installation will depend on.
OpenLiteSpeed supports .htaccess files.
This is important for our purposes, since WordPress uses .htaccess files to create and manage permalinks.
Add a dummy .htaccess file so that it will be available for WordPress to use later:
Next, copy over the sample configuration file to the filename that WordPress actually reads:
Additionally, create the upgrade directory so that WordPress won't run into permissions issues when trying to do this on its own following an update to its software:
Then, copy the entire contents of the directory into your document root.
OpenLiteSpeed comes with a default virtual host named Example located in the / usr / local / lsws / directory.
The document root for the Example virtual host is the html subdirectory:
Notice that this command includes a dot at the end of the source directory to indicate that everything within the directory should be copied, including hidden files (like the .htaccess file you created):
With that, you "ve successfully installed WordPress onto your web server and performed some of the initial configuration steps.
Next, we "ll go through some further configuration changes that will give WordPress the privileges it needs to function securely and access the MySQL database and user account you created previously.
Step 4 - Configuring the WordPress Directory
Before we can go through the web-based setup process for WordPress, we need to adjust some items in our WordPress directory.
Start by giving ownership of all the files in the directory to the nobody user and the nogroup group, which the OpenLiteSpeed web server runs as by default.
The following chown command will grant OpenLiteSpeed the ability to read and write files in the wordpress directory, allowing it to serve the website and perform automatic updates:
Next run two find commands to set the correct permissions on the WordPress directories and files:
These should be a reasonable permissions set to start with, although some plugins and procedures might require additional tweaks.
Following this, you will need to make some changes to the main WordPress configuration file.
When you open the file, your first objective will be to adjust some secret keys to provide some security for your installation.
These are only used internally, so it doesn't hurt usability to have complex, secure values here.
These are configuration lines that you will paste directly into your configuration file to set secure keys.
Copy the output you received to your clipboard, and then open the WordPress configuration file located in your document root:
Next, modify the database connection settings at the top of the file.
You need to adjust the database name, database user, and the associated password that you configured within MySQL.
The other change you must make is to set the method that WordPress should use to write to the filesystem.
Since we've given the web server permission to write where it needs to, we can explicitly set the filesystem method to direct.
Failure to set this with our current settings would result in WordPress prompting for FTP credentials when you perform certain actions.
At this point, WordPress is not quite fully configured on your system, as you still need to apply a few finishing touches before you can start publishing content.
In order to do that, though, you'll first need to make a few configuration changes to your OpenLiteSpeed installation.
Step 6 - Configuring OpenLiteSpeed
Currently, you have WordPress installed on your Ubuntu server, but your OpenLiteSpeed installation hasn't yet been configured to serve it. In this step, we'll access the OpenLiteSpeed administrative interface and make a few changes to your server's configuration.
In your preferred web browser, navigate to the OpenLiteSpeed administrative interface.
You can find this by entering your server's public IP address or the domain name associated with it, followed by: 7080, in your browser's address bar:
There, you will be presented with a login screen.
Enter the username and password you defined in the prerequisite OpenLiteSpeed installation tutorial:
OpenLiteSpeed login screen
From the OpenLiteSpeed console, find and click on Server Configuration in the left-hand sidebar menu.
Then navigate to the External App tab, find the row of the LiteSpeed SAPI App, and click on its Edit button:
Server Configuration page
Recall that in the prerequisite OpenLiteSpeed tutorial, you installed the lsphp73 package, a compilation of PHP optimized to work with OpenLiteSpeed through the LiteSpeed SAPI.
However, the default settings in the External App page point to lsphp rather than lsphp73.
Because of this, your OpenLiteSpeed installation won't be able to correctly execute PHP scripts.
To correct this, change the Name field to lsphp73, change the Address field to uds: / / tmp / lshttpd / lsphp73.sock, and change the Command field to read $SERVER _ ROOT / lsphp73 / bin / lsphp:
External App changes
After making those changes, click on the Save icon in the upper right-hand corner of the LiteSpeed SAPI App box.
Next, click on Virtual Hosts in the left-hand menu.
On the Virtual Hosts page, find the virtual host you plan to use and click on its View icon.
Here, we'll use the default Example virtual host:
Virtual Hosts page
Navigate to the virtual host's General tab. There, find the General section, and click on its Edit button:
Virtual Hosts General tab
OpenLiteSpeed looks at the contents of the Document Root field for content to serve.
Because all of your WordPress content and files are stored in the wordpress directory you created earlier, update the Document Root field to point to that directory.
To do this, all you need to do is append wordpress / to the end of the default value:
Virtual Hosts General changes
Click the Save icon to save this change.
Next, you need to enable index.php files so that they can be used to process requests that aren "t handled by static files.
This will allow the main logic of WordPress to function correctly.
While still in the General tab, scroll down to find the Index Files section and click on its Edit icon:
Virtual hosts Index Files page
In the Index Files field, precede index.html with index.php,.
By putting index.php before index.html, you're allowing PHP index files to take precedence.
After updating this field, it will look like this:
Virtual Hosts index files changes
Be sure to click the Save icon before continuing.
Next, navigate to the virtual host's Rewrite tab. Find the Rewrite Control section and press the Edit button:
Virtual Hosts Rewrite page
Set both the Enable Rewrite and Auto Load from .htaccess options to Yes by clicking the respective radial buttons.
Configuring rewrite instructions in this fashion will allow you to use permalinks within your WordPress installation:
Virtual Hosts rewrite changes
Click the Save icon after making those changes.
The default virtual host that is included with the OpenLiteSpeed installation includes some password protected areas to showcase OpenLiteSpeed's user authentication features.
WordPress includes its own authentication mechanisms and we will not be using the file-based authentication included in OpenLiteSpeed.
We should get rid of these in order to minimize the stray configuration fragments active on our WordPress installation.
First, click on the Security tab, and then click the Delete button next to SampleProtectedArea within the Realms List table:
OpenLiteSpeed security realm list
You will be asked to confirm the deletion.
Click Delete to proceed.
Next, click on the Context tab. In the Context List, delete the / protected / context that was associated with the security realm you just deleted:
OpenLiteSpeed delete protected context
Again, you will have to confirm the deletion by clicking Delete.
You can safely delete any or all of the other contexts as well using the same technique, as we will not be needing them.
We specifically deleted the / protected / context because otherwise an error would be produced due to the deletion of its associated security realm (which we just removed in the Security tab).
Following that, press the green Graceful Restart icon in the upper-right corner of the OpenLiteSpeed console.
This will restart the OpenLiteSpeed server, causing the changes you've made to go into effect:
Graceful Restart icon location
With that, your OpenLiteSpeed server is fully configured.
You're ready to finish setting up WordPress in your browser.
Step 7 - Completing the Installation through the WordPress Interface
When ready, click the Install WordPress button.
You "ll be taken to a page that prompts you to log in:
From the dashboard, you can begin making changes to your site "s theme and publishing content.
By completing this guide, you installed and configured a WordPress instance on Ubuntu 18.04 server running OpenLiteSpeed.
Some common next steps are to choose the permalinks setting for your posts (which can be found in Settings > Permalinks) or to select a new theme (in Appearance > Themes).
To enhance your new WordPress site's security, we recommend that you configure it to function with SSL so it can serve content over HTTPS.
Check out this tutorial from the OpenLiteSpeed documentation to install LetsEncrypt and set this up.
How To Optimize MySQL Queries with ProxySQL Caching on Ubuntu 16.04
3339
The author selected the Free Software Foundation to receive a donation as part of the Write for DOnations program.
ProxySQL is a SQL-aware proxy server that can be positioned between your application and your database.
It offers many features, such as load-balancing between multiple MySQL servers and serving as a caching layer for queries.
This tutorial will focus on ProxySQL's caching feature, and how it can optimize queries for your MySQL database.
MySQL caching occurs when the result of a query is stored so that, when that query is repeated, the result can be returned without needing to sort through the database.
This can significantly increase the speed of common queries.
But in many caching methods, developers must modify the code of their application, which could introduce a bug into the codebase.
To avoid this error-prone practice, ProxySQL allows you to set up transparent caching.
In transparent caching, only database administrators need to change the ProxySQL configuration to enable caching for the most common queries, and these changes can be done through the ProxySQL admin interface.
All the developer needs to do is connect to the protocol-aware proxy, and the proxy will decide if the query can be served from the cache without hitting the back-end server.
In this tutorial, you will use ProxySQL to set up transparent caching for a MySQL server on Ubuntu 16.04.
You will then test its performance using mysqlslap with and without caching to demonstrate the effect of caching and how much time it can save when executing many similar queries.
One Ubuntu 16.04 server with at least 2 GB of RAM, set up with a non-root user with sudo privileges and a firewall, as instructed in our Ubuntu 16.04 Initial Server Setup guide.
Step 1 - Installing and Setting Up the MySQL Server
First, you will install MySQL server and configure it to be used by ProxySQL as a back-end server for serving client queries.
On Ubuntu 16.04, mysql-server can be installed using this command:
Press Y to confirm the installation.
You will then be prompted for your MySQL root user password.
Enter a strong password and save it for later use.
Now that you have your MySQL server ready, you will configure it for ProxySQL to work correctly.
You need to add a monitor user for ProxySQL to monitor the MySQL server, since ProxySQL listens to the back-end server via the SQL protocol, rather than using a TCP connection or HTTP GET requests to make sure that the backend is running. monitor will use a dummy SQL connection to determine if the server is alive or not.
First, log in to the MySQL shell:
-uroot logs you in using the MySQL root user, and -p prompts for the root user's password.
This root user is different from your server's root user, and the password is the one you entered when installing the mysql-server package.
Enter the root password and press ENTER.
Now you will create two users, one named monitor for ProxySQL and another that you will use to execute client queries and grant them the right privileges.
This tutorial will name this user sammy.
Create the monitor user:
The CREATE USER query is used to create a new user that can connect from specific IPs.
Using% denotes that the user can connect from any IP address.
IDENTIFIED BY sets the password for the new user; enter whatever password you like, but make sure to remember it for later use.
With the user monitor created, next make the sammy user:
Next, grant privileges to your new users.
Run the following command to configure monitor:
The GRANT query is used to give privileges to users.
Here you granted only SELECT on all tables in the sys database to the monitor user; it only needs this privilege to listen to the back-end server.
Now grant all privileges to all databases to the user sammy:
This will allow sammy to make the necessary queries to test your database later.
Apply the privilege changes by running the following:
Finally, exit the mysql shell:
You've now installed mysql-server and created a user to be used by ProxySQL to monitor your MySQL server, and another one to execute client queries.
Next you will install and configure ProxySQL.
Step 2 - Installing and Configuring ProxySQL Server
Now you can install ProxySQL server, which will be used as a caching layer for your queries.
A caching layer exists as a stop between your application servers and database back-end servers; it is used to connect to the database and to save the results of some queries in its memory for fast access later.
The ProxySQL releases Github page offers installation files for common Linux distributions.
For this tutorial, you will use wget to download the ProxySQL version 2.0.4 Debian installation file:
Next, install the package using dpkg:
Once it is installed, start ProxySQL with this command:
You can check if ProxySQL started correctly with this command:
Now it is time to connect your ProxySQL server to the MySQL server.
For this purpose, use the ProxySQL admin SQL interface, which by default listens to port 6032 on localhost and has admin as its username and password.
Connect to the interface by running the following:
Enter admin when prompted for the password.
-uadmin sets the username as admin, and the -h flag specifies the host as localhost.
The port is 6032, specified using the -P flag.
Here you had to specify the host and port explicitly because, by default, the MySQL client connects using a local sockets file and port 3306.
Now that you are logged into the mysql shell as admin, configure the monitor user so that ProxySQL can use it. First, use standard SQL queries to set the values of two global variables:
The variable mysql-monitor _ username specifies the MySQL username that will be used to check if the back-end server is alive or not.
The variable mysql-monitor _ password points to the password that will be used when connecting to the back-end server.
Use the password you created for the monitor username.
Every time you create a change in the ProxySQL admin interface, you need to use the right LOAD command to apply changes to the running ProxySQL instance.
You changed MySQL global variables, so load them to RUNTIME to apply changes:
Next, SAVE the changes to the on-disk database to persist changes between restarts.
ProxySQL uses its own SQLite local database to store its own tables and variables:
Now, you will tell ProxySQL about the back-end server.
The table mysql _ servers holds information about each back-end server where ProxySQL can connect and execute queries, so add a new record using a standard SQL INSERT statement with the following values for hostgroup _ id, hostname, and port:
To apply the changes, run LOAD and SAVE again:
Finally, you will tell ProxySQL which user will connect to the back-end server; set sammy as the user, and replace < ^ > sammy _ password < ^ > with the password you created earlier:
The table mysql _ users holds information about users used to connect to the back-end servers; you specified the username, password, and default _ hostgroup.
LOAD and SAVE the changes:
Then exit the mysql shell:
To test that you can connect to your back-end server using ProxySQL, execute the following test query:
In this command, you used the -e flag to execute a query and close the connection.
The query prints the hostname of the back-end server.
< $> note Note: ProxySQL uses port 6033 by default for listening to incoming connections.
The output will look like this, with < ^ > your _ hostname < ^ > replaced by your hostname:
To learn more about ProxySQL configuration, see Step 3 of How To Use ProxySQL as a Load Balancer for MySQL on Ubuntu 16.04.
So far, you configured ProxySQL to use your MySQL server as a backend and connected to the backend using ProxySQL.
Now, you are ready to use mysqlslap to benchmark the query performance without caching.
Step 3 - Testing Using mysqlslap Without Caching
In this step, you will download a test database so you can execute queries against it with mysqlslap to test the latency without caching, setting a benchmark for the speed of your queries.
You will also explore how ProxySQL keeps records of queries in the stats _ mysql _ query _ digest table.
mysqlslap is a load emulation client that is used as a load testing tool for MySQL.
It can test a MySQL server with auto-generated queries or with some custom queries executed on a database.
It comes installed with the MySQL client package, so you do not need to install it; instead, you will download a database for testing purposes only, on which you can use mysqlslap.
In this tutorial, you will use a sample employee database.
You will be using this employee database because it features a large data set that can illustrate differences in query optimization.
The database has six tables, but the data it contains has more than 300,000 employee records.
This will help you emulate a large-scale production workload.
To download the database, first clone the Github repository using this command:
Then enter the test _ db directory and load the database into the MySQL server using these commands:
This command uses shell redirection to read the SQL queries in employees.sql file and execute them on the MySQL server to create the database structure.
Once the database is loaded into your MySQL server, test that mysqlslap is working with the following query:
mysqlslap has similar flags to the mysql client; here are the ones used in this command:
-u specifies the user used to connect to the server.
-p prompts for the user's password.
-P connects using the specified port.
-h connects to the specified host.
--auto-generate-sql lets MySQL perform load testing using its own generated queries.
--verbose makes the output show more information.
In this output, you can see the average, minimum, and maximum number of seconds spent to execute all queries.
This gives you an indication about the amount of time needed to execute the queries by a number of clients.
In this output, only one client was used to execute queries.
Next, find out what queries mysqlslap executed in the last command by looking at ProxySQL's stats _ mysql _ query _ digest.
This will give us information like the digest of the queries, which is a normalized form of the SQL statement that can be referenced later to enable caching.
Enter the ProxySQL admin interface with this command:
Then execute this query to find information in the stats _ mysql _ query _ digest table:
The previous query selects data from the stats _ mysql _ query _ digest table, which contains information about all executed queries in ProxySQL.
Here you have five columns selected:
count _ star: The number of times this query was executed.
sum _ time: Total time in milliseconds that this query took to execute.
hostgroup: The hostgroup used to execute the query.
digest: A digest of the executed query.
digest _ text: The actual query.
In this tutorial's example, the second query is parameterized using?
marks in place of variable parameters. select @ @ version _ comment limit 1 and select @ @ version _ comment limit 2, therefore, are grouped together as the same query with the same digest.
Now that you know how to check query data in the stats _ mysql _ query _ digest table, exit the mysql shell:
The database you downloaded contains some tables with demo data. You will now test queries on the dept _ emp table by selecting any records whose from _ date is greater than 2000-04-20 and recording the average execution time.
Use this command to run the test:
Here you are using some new flags:
--concurrency = 100: This sets the number of users to simulate, in this case 100.
--iterations = 20: This causes the test to run 20 times and calculate results from all of them.
--create-schema = employees: Here you selected the employees database.
--query = "SELECT * from dept _ emp WHERE from _ date > '2000-04-20' ": Here you specified the query executed in the test.
The test will take a few minutes.
After it is done, you will get results similar to the following:
Your numbers could be a little different.
Keep these numbers somewhere in order to compare them with the results from after you enable caching.
After testing ProxySQL without caching, it is time to run the same test again, but this time with caching enabled.
Step 4 - Testing Using mysqlslap With Caching
In this step, caching will help us to decrease latency when executing similar queries.
Here, you will identify the queries executed, take their digests from ProxySQL's stats _ mysql _ query _ digest table, and use them to enable caching.
Then, you will test again to check the difference.
To enable caching, you need to know the digests of the queries that will be cached.
Log in to the ProxySQL admin interface using this command:
Then execute this query again to get a list of queries executed and their digests:
You will get a result similar to this:
Look at the first row.
It is about a query that was executed 2000 times.
This is the benchmarked query executed previously.
Take its digest and save it to be used in adding a query rule for caching.
The next few queries will add a new query rule to ProxySQL that will match the digest of the previous query and put a cache _ ttl value for it. cache _ ttl is the number of milliseconds that the result will be cached in memory:
In this command you are adding a new record to the mysql _ query _ rules table; this table holds all the rules applied before executing a query.
In this example, you are adding a value for the cache _ ttl column that will cause the matched query by the given digest to be cached for a number of milliseconds specified in this column.
You put 1 in the apply column to make sure that the rule is applied to queries.
LOAD and SAVE these changes, then exit the mysql shell:
Now that caching is enabled, re-run the test again to check the result:
Here you can see the big difference in average execution time: it dropped from < ^ > 18.117 < ^ > seconds to < ^ > 7.020 < ^ >.
In this article, you set up transparent caching with ProxySQL to cache database query results.
You also tested the query speed with and without caching to see the difference that caching can make.
You've used one level of caching in this tutorial.
You could also try, web caching, which sits in front of a web server and caches the responses to similar requests, sending the response back to the client without hitting the back-end servers.
This is very similar to ProxySQL caching but at a different level.
To learn more about web caching, check out our Web Caching Basics: Terminology, HTTP Headers, and Caching Strategies primer.
MySQL server also has its own query cache; you can learn more about it in our How To Optimize MySQL with Query Cache on Ubuntu 18.04 tutorial.
3754
The Apache web server is a popular method for serving websites on the internet.
As of 2019, it is estimated to serve 29% of all active websites and it offers robustness and flexibility for developers.
Using Apache, an administrator can set up one server to host multiple domains or sites off of a single interface or IP by using a matching system.
Each domain or individual site - known as a "virtual host" - that is configured using Apache will direct the visitor to a specific directory holding that site "s information.
This is done without indicating that the same server is also responsible for other sites.
This scheme is expandable without any software limit as long as your server can handle the load.
The basic unit that describes an individual site or domain is called a virtual host.
In this guide, we will walk you through how to set up Apache virtual hosts on an Ubuntu 18.04 server.
Before you begin this tutorial, you should create a non-root user.
You will also need to have Apache installed in order to work through these steps.
If you haven't already done so, you can get Apache installed on your server through the apt package manner:
If you would like more detailed instructions as well as firewall setup, please refer to our guide How To Install the Apache Web Server on Ubuntu 18.04.
For the purposes of this guide, our configuration will make a virtual host for example.com and another for test.com.
These will be referenced throughout the guide, but you should substitute your own domains or values while following along.
If you are using DigitalOcean, you can learn how to set up domains by following the product documentation, How to Add Domains.
For other providers, refer to their relevant product documentation If you do not have domains available at this time, you can use test values.
We will show how to edit your local hosts file later on to test the configuration if you are using test values.
This will allow you to validate your configuration from your home computer, even though your content won "t be available through the domain name to other visitors.
Step One - Create the Directory Structure
The first step that we are going to take is to make a directory structure that will hold the site data that we will be serving to visitors.
Our document root (the top-level directory that Apache looks at to find content to serve) will be set to individual directories under the / var / www directory.
We will create a directory here for both of the virtual hosts we plan on making.
Within each of these directories, we will create a public _ html folder that will hold our actual files.
This gives us some flexibility in our hosting.
For instance, for our sites, we're going to make our directories as follows.
If you are using actual domains or alternate values, swap out the highlighted text for these.
The portions in red represent the domain names that we want to serve from our VPS.
Step Two - Grant Permissions
Now we have the directory structure for our files, but they are owned by our root user.
If we want our regular user to be able to modify files in our web directories, we can change the ownership by doing this:
The $USER variable will take the value of the user you are currently logged in as when you press ENTER.
By doing this, our regular user now owns the public _ html subdirectories where we will be storing our content.
We should also modify our permissions to ensure that read access is permitted to the general web directory and all of the files and folders it contains so that pages can be served correctly:
Your web server should now have the permissions it needs to serve content, and your user should be able to create content within the necessary folders.
Step Three - Create Demo Pages for Each Virtual Host
We now have our directory structure in place.
Let's create some content to serve.
For demonstration purposes, we "ll make an index.html page for each site.
Let "s begin with example.com.
We can open up an index.html file in a text editor, in this case we "ll use nano:
Within this file, create an HTML document that indicates the site it is connected to, like the following:
Save and close the file (in nano, press CTRL + X then Y then ENTER) when you are finished.
We can copy this file to use as the basis for our second site by typing:
We can then open the file and modify the relevant pieces of information:
You now have the pages necessary to test the virtual host configuration.
Step Four - Create New Virtual Host Files
Virtual host files are the files that specify the actual configuration of our virtual hosts and dictate how the Apache web server will respond to various domain requests.
Apache comes with a default virtual host file called 000-default.conf that we can use as a jumping off point.
We are going to copy it over to create a virtual host file for each of our domains.
We will start with one domain, configure it, copy it for our second domain, and then make the few further adjustments needed.
The default Ubuntu configuration requires that each virtual host file end in .conf.
Open the new file in your editor with root privileges:
With comments removed, the file will look similar to this:
Within this file, we will customize the items for our first domain and add some additional directives.
This virtual host section matches any requests that are made on port 80, the default HTTP port.
First, we need to change the ServerAdmin directive to an email that the site administrator can receive emails through.
After this, we need to add two directives.
The first, called ServerName, establishes the base domain that should match for this virtual host definition.
This will most likely be your domain.
The second, called ServerAlias, defines further names that should match as if they were the base name.
This is useful for matching hosts you defined, like www:
The only other thing we need to change for our virtual host file is the location of the document root for this domain.
We already created the directory we need, so we just need to alter the DocumentRoot directive to reflect the directory we created:
When complete, our virtual host file should look like this:
When you are finished, it should look like this:
Step Five - Enable the New Virtual Host Files
Now that we have created our virtual host files, we must enable them.
Apache includes some tools that allow us to do this.
We "ll be using the a2ensite tool to enable each of our sites.
If you would like to read more about this script, you can refer to the a2ensite documentation.
Step Six - Set Up Local Hosts File (Optional)
If you haven't been using actual domain names that you own to test this procedure and have been using some example domains instead, you can at least test the functionality of this process by temporarily modifying the hosts file on your local computer.
This will intercept any requests for the domains that you configured and point them to your VPS server, just as the DNS system would do if you were using registered domains.
This will only work from your local computer though, and only for testing purposes.
Make sure you are operating on your local computer for these steps and not your VPS server.
You will need to know the computer's administrative password or otherwise be a member of the administrative group.
If you are on a Mac or Linux computer, edit your local file with administrative privileges by typing:
If you are on a Windows machine, you can find instructions on altering your hosts file here.
The details that you need to add are the public IP address of your server followed by the domain you want to use to reach that server.
This is what we want if we are not actually the owners of these domains in order to test our virtual hosts.
Step Seven - Test your Results
If both of these sites work as expected, you "ve successfully configured two virtual hosts on the same server.
If you adjusted your home computer "s hosts file, you may want to delete the lines you added now that you verified that your configuration works.
This will prevent your hosts file from being filled with entries that are no longer necessary.
If you need to access this long term, consider adding a domain name for each site you need and setting it up to point to your server.
If you followed along, you should now have a single server handling two separate domain names.
You can expand this process by following the steps we outlined above to make additional virtual hosts.
There is no software limit on the number of domain names Apache can handle, so feel free to make as many as your server is capable of handling.
3692
As a web administrator, you may find it valuable to restrict some parts of a website from visitors, whether temporarily or on a permanent basis.
While web applications may provide their own authentication and authorization methods, you can also rely on the web server itself to restrict access if these are inadequate or unavailable.
This tutorial will walk you through password-protecting assets on an Apache web server running on Ubuntu 18.04 in order to provide your server with additional security.
In addition, you will need the following setup before you can begin:
A sudo user on your server: You can create a user with sudo privileges by following the Ubuntu 18.04 initial server setup guide.
An Apache2 web server: If you haven "t already set one up, the How To Install the Apache Web Server on Ubuntu 18.04 tutorial can guide you.
A site secured with SSL: How you set this up depends on whether you have a domain name for your site.
If you have a domain name, you can secure your site with Let's Encrypt, which provides free, trusted certificates.
Follow the Let's Encrypt guide for Apache to set this up.
If you do not have a domain and you are just using this configuration for testing or personal use, you can use a self-signed certificate instead.
Follow the self-signed SSL guide for Apache to get set up.
When all of these are in place, log into your server as the sudo user and continue below.
Step 1 - Installing the Apache Utilities Package
Let "s begin by updating our server and installing a package that we" ll need.
In order to complete this tutorial, we will be using a utility called htpasswd, part of the apache2-utils package, to create the file and manage the username and passwords needed to access restricted content.
With this installed, we now have access to the htpasswd command.
The htpasswd command will allow us to create a password file that Apache can use to authenticate users.
We will create a hidden file for this purpose called .htpasswd within our / etc / apache2 configuration directory.
The first time we use this utility, we need to add the -c option to create the specified passwdfile.
We specify a username (sammy in this example) at the end of the command to create a new entry within the file:
If we view the contents of the file, we can see the username and the encrypted password for each record:
We now have our users and passwords in a format that Apache can read.
Step 3 - Configuring Apache Password Authentication
We can do this in one of two ways: either directly in a site "s virtual host file or by placing .htaccess files in the directories that need restriction.
It "s generally best to use the virtual host file, but if you need to allow non-root users to manage their own access restrictions, check the restrictions into version control alongside the website, or have a web application using .htaccess files for other purposes already, check out the second option.
Choose the option that best suits your needs.
Option 1: Configuring Access Control within the Virtual Host Definition (Preferred)
The first option is to edit the Apache configuration and add the password protection to the virtual host file.
This will generally give better performance because it avoids the expense of reading distributed configuration files.
This option requires access to the configuration, which isn "t always available, but when you do have access, it" s recommended.
Begin by opening up the virtual host file that you wish to add a restriction to.
For our example, we'll be using the default-ssl.conf file that holds the default virtual host installed through Ubuntu's apache package.
Open up the file with a command-line text editor such as nano:
Inside, with the comments stripped, the file should look similar to this:
To set up authentication, you will need to target the directory you wish to restrict with a < Directory _ _ _ > block.
In our example, we'll restrict the entire document root, but you can modify this listing to only target a specific directory within the web space:
Within this directory block, specify that we are setting up Basic authentication.
For the AuthName, choose a realm name that will be displayed to the user when prompting for credentials.
Use the AuthUserFile directive to point Apache to the password file we created.
Finally, make it a requirement that only a valid-user may access this resource, which means anyone who can verify their identity with a password will be allowed in:
If you are using nano, you can do so by pressing CTRL + X followed by Y then ENTER.
Before restarting the web server, you can check the configuration with the following command:
If everything checks out and you get Syntax OK as output, you can restart the server to implement your password policy.
Since systemctl doesn't display the outcome of all service management commands, we "ll use the the status to be sure the server is running:
Now, the directory you specified should be password protected.
Option 2: Configuring Access Control with .htaccess Files
Apache can use .htaccess files in order to allow certain configuration items to be set within a content directory.
Since Apache has to re-read these files on every request that involves the directory, which can negatively impact performance, Option 1 is preferred, but if you are already using .htaccess file or need to allow non-root users to manage restrictions, .htaccess files make sense.
To enable password protection using .htaccess files, open the main Apache configuration file with a command-line text editor such as nano:
Find the < Directory > block for the / var / www directory that holds the document root.
Turn on .htaccess processing by changing the AllowOverride directive within that block from None to All:
Next, we need to add an .htaccess file to the directory we wish to restrict.
In our demonstration, we "ll restrict the entire document root (the entire website) which is based at / var / www / html, but you can place this file in any directory where you wish to restrict access:
Within this file, specify that we wish to set up Basic authentication.
Finally, we will require a valid-user to access this resource, which means anyone who can verify their identity with a password will be allowed in:
Restart the web server to password protect all content in or below the directory with the .htaccess file and use systemctl status to verify the success of the restart:
The directory you specified should now be password protected.
Step 4 - Confirming Password Authentication
You should be presented with a username and password prompt that looks like this:
If you enter the correct credentials, you will be allowed to access the content.
If you enter the wrong credentials or hit "Cancel", you will see the "Unauthorized" error page:
Apache2 unauthorized error
If you "ve followed along, you" ve now set up basic authentication for your site.
There is much more that you can do with Apache configuration and .htaccess.
To learn more about the flexibility and power available in Apache configuration, try one of these tutorials:
For a better understanding of the with the main configuration file, read the section about Getting Familiar with Important Apache Files and Directories in our Apache installation guide.
Learn more about the virtual host files in How To Set Up Apache Virtual Hosts on Ubuntu 16.04
Learn about rewriting URLs, customizing error pages like the "Unauthorized" message above, or including common elements on all your pages with Server Side Includes in our guide How To Use the .htaccess File.
How To Set Up the code-server Cloud IDE Platform on CentOS 7
3333
In this tutorial, you will set up the code-server cloud IDE platform on your CentOS 7 machine and expose it at your domain, secured with free Let's Encrypt TLS certificates.
In the end, you'll have Microsoft Visual Studio Code running on your CentOS 7 server, available at your domain and protected with a password.
A server running CentOS 7 with at least 2GB RAM, root access, and a sudo, non-root account.
For a guide on how to do this, see How To Install Nginx on CentOS 7.
Download it using curl by running the following command:
You'll store the service configuration in a file named code-server.service, in the / usr / lib / systemd / system directory, where systemd stores its services.
Create it using the vi editor:
Type: wq and then ENTER to save and close the file.
As you have learned in the Nginx prerequisite step, its site configuration files are stored under / etc / nginx / conf.d and will automatically be loaded when Nginx starts.
You'll store the configuration for exposing code-server at your domain in a file named code-server.conf, under / etc / nginx / conf.d.
CentOS 7 comes with SELinux turned on, with a strict ruleset, which by default does not permit Nginx to connect to local TCP sockets.
Nginx needs to do in order to serve as a reverse proxy for code-server.
Run the following command to relax the rule permanently:
Now you'll make Certbot automatically renew the certificates before they expire.
To run the renewal check daily, you'll use cron, a standard system service for running periodic jobs.
You direct cron by opening and editing a file called a crontab:
This command will open the default crontab, which is currently an empty text file.
Add the following line, then save and close it:
15 3 * * * will run the following command at 3: 15 am every day - you can adapt this to any time.
The renew command for Certbot will check all certificates installed on the system and update any that are set to expire in less than thirty days. --quiet tells Certbot not to output information or wait for user input.
cron will now run this command daily.
All installed certificates will be automatically renewed and reloaded when they have thirty days or less before they expire.
You now have code-server, a versatile cloud IDE, installed on your CentOS 7 server, exposed at your domain and secured using Let's Encrypt certificates.
How To Set Up the code-server Cloud IDE Platform on Ubuntu 18.04
3329
In this tutorial, you will set up the code-server cloud IDE platform on your Ubuntu 18.04 machine and expose it at your domain, secured with free Let's Encrypt TLS certificates.
In the end, you'll have Microsoft Visual Studio Code running on your Ubuntu 18.04 server, available at your domain and protected with a password.
To install the latest version of Certbot, you'll need to add its package repository to your server by running the following command:
You'll need to press ENTER to accept.
Then, install Certbot and its Nginx plugin:
How To Set Up the Eclipse Theia Cloud IDE Platform on CentOS 7
3532
With developer tools moving to the cloud, adoption of cloud IDE (Integrated Development Environment) platforms is growing.
Visually, it "s designed to look and behave similarly to Microsoft Visual Studio Code, which means that it supports many programming languages, has a flexible layout, and has an integrated terminal.
In this tutorial, you'll deploy Eclipse Theia to your CentOS 7 server using Docker Compose, a container orchestration tool.
You'll expose it at your domain using nginx-proxy, an automated system for Docker that simplifies the process of configuring Nginx to serve as a reverse proxy for a container.
You'll also secure it using a free Let's Encrypt TLS certificate, which you'll provision using its specialized add-on.
In the end, you'll have Eclipse Theia running on your CentOS 7 server available via HTTPS and requiring the user to log in.
An CentOS 7 server with root privileges, and a secondary, non-root account.
You can set this up by following our Initial Server Setup Guide for CentOS 7. For this tutorial the non-root user is < ^ > sammy < ^ >.
Follow Step 1 and Step 2 of How To Install Docker on CentOS 7. Remember to log out and log back in after executing this prerequisite.
For an introduction to Docker, see The Docker Ecosystem: An Introduction to Common Components.
Docker Compose installed on your server.
Follow Step 1 of How To Install Docker Compose on CentOS 7.
This tutorial will use < ^ > theia.your-domain < ^ > throughout.
An A DNS record with < ^ > theia.your-domain < ^ > pointing to your server's public IP address.
Step 1 - Deploying nginx-proxy with Let's Encrypt
In this section, you'll deploy nginx-proxy and its Let's Encrypt add-on using Docker Compose.
This will enable automatic TLS certificate provisioning and renewal, so that when you deploy Eclipse Theia it will be accessible at your domain via HTTPS.
For the purposes of this tutorial, you'll store all files under ~ / eclipse-theia.
You'll store the Docker Compose configuration for nginx-proxy in a file named nginx-proxy-compose.yaml.
Here you're defining two services that Docker Compose will run, nginx-proxy and its Let's Encrypt companion.
For the proxy, you specify jwilder / nginx-proxy as the image, map HTTP and HTTPS ports, and define volumes that will be accessible to it during runtime.
Volumes are directories on your server that the defined service will have full access to, which you'll later use to set up user authentication.
To achieve that, you'll make use of the first volume from the list, which maps the local / etc / nginx / htpasswd directory to the same one in the container.
In that folder, nginx-proxy expects to find a file named exactly as the target domain, containing log-in credentials for user authentication in the htpasswd format (username: hashed _ password).
For the add-on, you name the Docker image and allow access to Docker's socket by defining a volume.
Then, you specify that the add-on should inherit access to the volumes defined for nginx-proxy.
Both services have restart set to always, which orders Docker to restart the containers in case of crash or system reboot.
Deploy the configuration by running:
Here you pass in the nginx-proxy-compose.yaml filename to the -f parameter of the docker-compose command, which specifies the file to run.
Then, you pass the up verb that instructs it to run the containers.
The -d flag enables detached mode, which means that Docker Compose will run the containers in the background.
The final output will look like this:
You've deployed nginx-proxy and its Let's Encrypt companion using Docker Compose.
Now you'll move on to set up Eclipse Theia at your domain and secure it.
Step 2 - Deploying Dockerized Eclipse Theia
In this section, you'll create a file containing any allowed log-in combinations that a user will need to input.
Then, you'll deploy Eclipse Theia to your server using Docker Compose and expose it at your secured domain using nginx-proxy.
As explained in the previous step, nginx-proxy expects log-in combinations to be in a file named after the exposed domain, in the htpasswd format and stored under the / etc / nginx / htpasswd directory in the container.
The local directory which maps to the virtual one does not need to be the same, as was specified in the nginx-proxy config.
To create log-in combinations, you'll first need to install htpasswd by running the following command:
The httpd-tools package contains the htpasswd utility.
Create the / etc / nginx / htpasswd directory:
Create a file that will store the logins for your domain:
Remember to replace < ^ > theia.your-domain < ^ > with your Eclipse Theia domain.
To add a username and password combination, run the following command:
Replace < ^ > username < ^ > with the username you want to add.
You'll be asked for a password twice.
After providing it, htpasswd will add the username and hashed password pair to the end of the file.
You can repeat this command for as many logins as you wish to add.
Now, you'll create configuration for deploying Eclipse Theia.
You'll store it in a file named eclipse-theia-compose.yaml.
In this config, you define a single service called eclipse-theia with restart set to always and theiaide / theia: next as the container image.
You also set init to true to instruct Docker to use init as the main process manager when running Eclipse Theia inside the container.
Then, you specify two environment variables in the environment section: VIRTUAL _ HOST and LETSENCRYPT _ HOST.
The former is passed on to nginx-proxy and tells it at what domain the container should be exposed, while the latter is used by its Let's Encrypt add-on and specifies for which domain to request TLS certificates.
Unless you specify a wildcard as the value for VIRTUAL _ HOST, they must be the same.
Remember to replace < ^ > theia.your-domain < ^ > with your desired domain, then save and close the file.
Now deploy Eclipse Theia by running:
The final output will look like:
Then, in your browser, navigate to the domain you're using for Eclipse Theia.
Your browser will show you a prompt asking you to log in.
After providing the correct credentials, you'll enter Eclipse Theia and immediately see its editor GUI.
In your address bar you'll see a padlock indicating that the connection is secure.
If you don't see this immediately, wait a few minutes for the TLS certificates to provision, then reload the page.
Eclipse Theia GUI
Now that you can securely access your cloud IDE, you'll start using the editor in the next step.
Step 3 - Using the Eclipse Theia Interface
After creating a new file through the File menu, you'll see an empty file open in a new tab. Once saved, you can view the file's name in the Explorer side panel.
You now have Eclipse Theia, a versatile cloud IDE, installed on your CentOS 7 server using Docker Compose and nginx-proxy.
You've secured it with a free Let's Encrypt TLS certificate and set up the instance to require log-in credentials from the user.
How To Set Up the Eclipse Theia Cloud IDE Platform on Debian 10
3682
In this tutorial, you'll deploy Eclipse Theia to your Debian 10 server using Docker Compose, a container orchestration tool.
In the end, you'll have Eclipse Theia running on your Debian 10 server available via HTTPS and requiring the user to log in.
A Debian 10 server with root privileges, and a secondary, non-root account.
You can set this up by following our Initial Server Setup Guide for Debian 10. For this tutorial the non-root user is < ^ > sammy < ^ >.
Follow Step 1 and Step 2 of How To Install Docker on Debian 10. For an introduction to Docker, see The Docker Ecosystem: An Introduction to Common Components.
Follow Step 1 of How To Install Docker Compose on Debian 10.
An A DNS record with < ^ > theia.your _ domain < ^ > pointing to your server's public IP address.
The apache2-utils package contains the htpasswd utility.
Remember to replace < ^ > theia.your _ domain < ^ > with your Eclipse Theia domain.
Remember to replace < ^ > theia.your _ domain < ^ > with your desired domain, then save and close the file.
You now have Eclipse Theia, a versatile cloud IDE, installed on your Debian 10 server using Docker Compose and nginx-proxy.
How To Set Up the Eclipse Theia Cloud IDE Platform on Ubuntu 18.04 Quickstart
3758
Visually, it "s designed to look and behave similarly to Microsoft Visual Studio Code.
In this tutorial, you'll deploy Eclipse Theia to your Ubuntu 18.04 server using Docker Compose.
You'll expose it at your domain using nginx-proxy and secure it with a Let's Encrypt TLS certificate, which you'll provision with an add-on.
For a more detailed version of this tutorial, please refer to How To Set Up the Eclipse Theia Cloud IDE Platform on Ubuntu 18.04.
An Ubuntu 18.04 server with root privileges, and a secondary, non-root account, following the Initial Server Setup Guide for Ubuntu 18.04.
Docker installed on your server, follow Step 1 and Step 2 of How To Install Docker on Ubuntu 18.04.
Docker Compose installed on your server, follow Step 1 of How To Install Docker Compose on Ubuntu 18.04.
Create the directory to store all data for Eclipse Theia:
Create nginx-proxy-compose.yaml to store the Docker Compose configuration for nginx-proxy:
Deploy the configuration:
nginx-proxy expects log-in combinations to be in a file named after the exposed domain, in the htpasswd format and stored under the / etc / nginx / htpasswd directory in the container.
Install htpasswd:
Create a file to store the logins for your domain:
Run the following command with a username and password combination:
htpasswd will add the username and hashed password pair to the end of the file.
Create the configuration for Eclipse Theia's deployment:
You define a single service called eclipse-theia with restart set to always and theiaide / theia: next as the container image.
You also set init to true.
Navigate to the domain you're using for Eclipse Theia.
You'll enter Eclipse Theia and see its editor GUI.
You'll also see a padlock indicating that the connection is secure.
You now have Eclipse Theia, a versatile cloud IDE, installed on your Ubuntu 18.04 server using Docker Compose and nginx-proxy.
How To Set Up the Eclipse Theia Cloud IDE Platform on Ubuntu 18.04
3342
In this tutorial, you'll deploy Eclipse Theia to your Ubuntu 18.04 server using Docker Compose, a container orchestration tool.
In the end, you'll have Eclipse Theia running on your Ubuntu 18.04 server available via HTTPS and requiring the user to log in.
An Ubuntu 18.04 server with root privileges, and a secondary, non-root account.
You can set this up by following our Initial Server Setup Guide for Ubuntu 18.04.
For this tutorial the non-root user is < ^ > sammy < ^ >.
Follow Step 1 and Step 2 of How To Install Docker on Ubuntu 18.04.
Follow Step 1 of How To Install Docker Compose on Ubuntu 18.04.
How to Use Ansible to Install and Set Up LAMP on Ubuntu 18.04
3328
This guide explains how to use Ansible to automate the steps contained in our guide on How To Install Linux, Apache, MySQL and PHP (LAMP) on Ubuntu 18.04.
A "LAMP" stack is a group of open-source software that is typically installed together to enable a server to host dynamic websites and web apps.
This Ansible playbook provides an alternative to manually running through the procedure outlined in our guide on How To Install Linux, Apache, MySQL and PHP (LAMP) on Ubuntu 18.04.
Install the required LAMP packages.
Create a new Apache VirtualHost and set up a dedicated document root for that.
Enable the new VirtualHost.
Disable the default Apache website, when the disable _ default variable is set to true.
Set up a PHP test script using the provided template.
Once the playbook has finished running, you will have a web PHP environment running on top of Apache, based on the options you defined within your configuration variables.
The first thing we need to do is obtain the LAMP playbook and its dependencies from the do-community / ansible-playbooks repository.
The files we're interested in are located inside the lamp _ ubuntu1804 folder, which has the following structure:
files / info.php.j2: Template file for setting up a PHP test page on the web server's root
We'll edit the playbook's variable file to customize the configurations of both MySQL and Apache.
Access the lamp _ ubuntu1804 directory and open the vars / default.yml file using your command line editor of choice:
app _ user: A remote non-root user on the Ansible host that will be set as the owner of the application files.
disable _ default: Whether or not to disable the default website that comes with Apache.
When the playbook is finished running, go to your web browser and access the host or IP address of the server, as configured in the playbook variables, followed by / info.php:
phpinfo page
Because this page contains sensitive information about your PHP environment, it is recommended that you remove it from the server by running an rm -f / var / www / info.php command once you have finished setting it up.
You can find the LAMP server setup featured in this tutorial in the lamp _ ubuntu1804 folder inside the DigitalOcean Community Playbooks repository.
The default.yml variable file contains values that will be used within the playbook tasks, such as the password for the MySQL root account and the domain name to configure within Apache.
files / info.php.j2
The info.php.j2 file is another Jinja template, used to set up a test PHP script in the document root of the newly configured LAMP server.
In this guide, we used Ansible to automate the process of installing and setting up a LAMP environment on a remote server.
Because each individual typically has different needs when working with MySQL databases and users, we encourage you to check out the official Ansible documentation for more information and use cases of the mysql _ user Ansible module.
How To Use Cron to Automate Tasks on Ubuntu 18.04
3538
To complete this guide, you'll need access to a computer running Ubuntu 18.04.
This could be your local machine, a virtual machine, or a virtual private server.
To set this up, follow our Initial Server Setup guide for Ubuntu 18.04.
However, if you "re using an Ubuntu machine on which cron isn't installed, you can install it using APT.
Before installing cron on an Ubuntu machine, update the computer's local package index:
Then install cron with the following command:
You "ll need to make sure it's set to run in the background too:
Each user profile on the system can have their own crontab where they can schedule jobs, which is stored under / var / spool / cron / crontabs /.
If this is the first time you're running the crontab command under this user profile, it will prompt you to select a default text editor to use when editing your crontab:
Enter the number corresponding to the editor of your choice.
Alternatively, you could just press ENTER to accept the default choice, nano.
After making your selection, you'll be taken to a new crontab containing some commented-out instructions on how to use it:
When you run crontab -e in the future, it will bring up your crontab in this text editor automatically.
Otherwise, you can save and close the crontab for now (CTRL + X, Y, then ENTER if you selected nano).
< $> note Note: On Linux systems, there is another crontab stored under the / etc / directory.
For instance, you could write a shell script to send data backups to an object storage solution and then automate it with cron.
How To Use Database Migrations and Seeders to Abstract Database Setup in Laravel
3695
Migrations and seeders are powerful database utilities provided by the Laravel PHP framework to allow developers to quickly bootstrap, destroy and recreate an application's database.
These utilities help to minimize database inconsistency problems that can arise with multiple developers working on the same application: new contributors need only to run a couple artisan commands to set the database up on a fresh install.
In this guide, we'll create migrations and seeders to populate a Laravel demo application's database with sample data. At the end, you will be able to destroy and recreate your database tables as many times as you want, using only artisan commands.
Docker installed on your server, following Step 1 and Step 2 of How To Install and Use Docker on Ubuntu 18.04.
< $> note Note: In this guide, we'll use a containerized development environment managed by Docker Compose to run the application, but you may also opt to run the application on a LEMP server.
To set this up, you can follow our guide on How to Install and Configure Laravel with LEMP on Ubuntu 18.04.
We're interested in the tutorial-02 branch, which includes a Docker Compose setup to run the application on containers.
In this example, we'll download the application to our home folder, but you can use any directory of your choice:
Because we downloaded the application code as a .zip file, we'll need the unzip command to unpack it. If you haven't done so recently, update your machine's local package index:
Then install the unzip package:
Following that, unzip the contents of the application:
Then rename the unpacked directory to < ^ > travellist-demo < ^ > for easier access:
In Laravel, a .env file is used to set up environment-dependent configurations, such as credentials and any information that might vary between deployments.
Each installation on a new environment requires a tailored environment file to define things such as database connection settings, debug options, and the application URL, among other items that may vary depending on which environment the application is running.
This is how your .env file looks like now:
The current .env file from the travellist demo application contains settings to use the containerized environment we've created with Docker Compose in the last part of this series.
You don't need to change any of these values, but you are free to modify the DB _ DATABASE, DB _ USERNAME and DB _ PASSWORD if you wish, since these are pulled by our docker-compose.yml file automatically to set up the development database.
Just make sure the DB _ HOST variable remains unchanged, since it references the name of our database service within the Docker Compose environment.
If you make any changes to the file, make sure to save and close it by pressing CTRL + X, Y, then ENTER.
< $> note Note: If you have opted to run the application on a LEMP server, you'll need to change the highlighted values to reflect your own database settings, including the DB _ HOST variable.
Step 3 - Installing Application Dependencies with Composer
We'll now use Composer, PHP's dependency management tool, to install the application's dependencies and make sure we're able to execute artisan commands.
Bring up your Docker Compose environment with the following command.
This will build the travellist image for the app service and pull in the additional Docker images required by the nginx and db services, in order to create the application environment:
Once the process is finished, we can run Composer to install the application's dependencies.
To execute composer and other commands in the app service container, we'll use docker-compose exec.
The exec command allows us to execute any command of our choice on containers managed by Docker Compose.
It uses the following syntax: docker-compose exec < ^ > service _ name command < ^ >.
< $> note Note: In case you have opted to use a LEMP server to run the demo application, you should ignore the docker-compose exec app portion of the commands listed throughout this guide.
For example, instead of running the following command as it's written, you would just run:
To execute composer install in the app container, run:
When Composer is finished installing the application's dependencies, you'll be able to execute artisan commands.
To test that the application is able to connect to the database, run the following command which will clean up any pre-existing tables:
This command will drop any pre-existing tables on the configured database.
If it ran successfully and the application is able to connect to the database, you'll see output like this:
Now that you have installed the application dependencies with Composer, you can use the artisan tool to create migrations and seeders.
Step 4 - Creating Database Migrations
The artisan command line tool that ships with Laravel contains a series of helper commands that can be used to manage the application and bootstrap new classes.
To generate a new migration class, we can use the make: migration command as follows:
Laravel infers the operation to be executed (< ^ > create < ^ >), the name of the table (< ^ > places < ^ >), and whether this migration will create a new table or not, based on the descriptive name provided to the make: migration command.
This will generate a new file in the application's database / migrations directory.
The timestamp included in the auto-generated file is used by Laravel to determine in which order migrations should be executed.
Use your text editor of choice to open the generated migration file.
Remember to replace the highlighted value with your own migration file name:
The generated migration file contains a class called CreatePlacesTable:
This class has two methods: up and down.
Both methods contain bootstrap code that you can extend to customize what happens when that migration is executed and also what happens when it is rolled back.
We'll modify the up method so that the places table reflects the structure we're already using in the current application's version:
id: primary key field.
name: name of the place.
visited: whether or not this place was already visited.
The Laravel schema builder exposes methods for creating, updating and deleting tables in a database.
The Blueprint class defines the table's structure and it provides several methods to abstract the definition of each table field.
The auto-generated code sets up a primary id field called id. The timestamps method creates two datetime fields that are automatically updated by the underlying database classes when data is inserted or updated within that table.
In addition to these, we'll need to include a name and a visited field.
Our name field will be of type string, and our visited field will be set with the boolean type.
We'll also set a default value of 0 for the visited field, so that if no value is passed, it means the place was not visited yet.
This is how the up method will look like now:
< $> note Note: You can find the full list of available column types in the Laravel documentation.
After including the two highlighted lines on your own migration script, save and close the file.
Your migration is now ready to be executed via artisan migrate.
However, that would only create an empty table; we also need to be able to insert sample data for development and testing.
In the next step, we'll see how to do that using database seeders.
Step 5 - Creating Database Seeders
A seeder is a special class used to generate and insert sample data (seeds) in a database.
This is an important feature in development environments, since it allows you to recreate the application with a fresh database, using sample values that you'd otherwise have to manually insert each time the database is recreated.
We'll now use the artisan command to generate a new seeder class for our places table called PlacesTableSeeder:
The command will create a new file called PlacesTableSeeder.php inside the database / seeds directory.
Open that file using your text editor of choice:
This is what the auto-generated PlacesTableSeeder.php file looks like:
Our new seeder class contains an empty method named run.
This method will be called when the db: seed Artisan command is executed.
We need to edit the run method in order to include instructions to insert sample data in the database.
We'll use the Laravel query builder to streamline this process.
The Laravel query builder offers a fluent interface for database operations such as inserting, updating, deleting, and retrieving data. It also introduces safeguards against SQL injection attacks.
The query builder is exposed by the DB facade - a static proxy to underlying database classes in the service container.
To get started, we'll create a static class variable to hold all the sample places we want to insert into the database as an array.
This will allow us to use a foreach loop to iterate through all values, inserting each one in the database using the query builder.
We'll call this variable $places:
Next, we'll need to include a use statement at the top of our PlacesTableSeeder class to facilitate referencing the DB facade throughout the code:
We can now iterate through the $places array values using a foreach loop, and insert each one in our places table using the query builder:
The foreach loop iterates through each value of the $places static array.
At each iteration, we use the DB facade to insert a new row at the places table.
We set the name field to the name of the place we just obtained from the $places array, and we set the visited field to a random value of either 0 or 1.
This is what the full PlacesTableSeeder class will look like after all the updates:
Save and close the file when you're done making these changes.
Seeder classes aren't automatically loaded in the application.
We need to edit the main DatabaseSeeder class to include a call to the seeder we've just created.
Open the database / seeds / DatabaseSeeder.php file using nano or your favorite editor:
The DatabaseSeeder class looks like any other seeder: it extends from the Seeder class and has a run method.
We'll update this method to include a call to PlacesTableSeeder.
Update the current run method inside your DatabaseSeeder class by deleting the commented-out line and replacing it with the following highlighted code:
This is how the full DatabaseSeeder class will look like after the update:
Save and close the file when you're done updating its content.
We have now finished setting up both a migration and a seeder for our places table.
In the next step, we'll see how to execute them.
Step 6 - Running Database Migrations and Seeders
Before proceeding, we need to make sure your application is up and running.
We'll set up the application encryption key and then access the application from a browser to test the web server.
To generate the encryption key required by Laravel, you can use the artisan key: generate command:
Once the key has been generated, you'll be able to access the application by pointing your browser to your server hostname or IP address on port 8000:
MySQL error
That means the application is able to connect to the database, but it couldn't find a table called places.
We'll create the places table now, using the following migrate artisan command:
You'll notice that a few other migrations were executed along with the create _ places _ table migration we've set up.
These migrations are auto generated when Laravel is installed.
Although we won't be using these additional tables now, they will be needed in the future when we expand the application to have registered users and scheduled jobs.
For now, you can just leave them as is.
At this point our table is still empty.
We need to run the db: seed command to seed the database with our sample places:
This will run our seeder and insert the sample values we defined within our PlacesTableSeeder class.
Now, reload the application page on your browser.
Whenever you need to start from scratch, you can drop all your database tables with:
To run the app migrations and seed the tables in a single command, you can use:
If you want to roll back a migration, you can run:
This will trigger the down method for each migration class inside the migrations folder.
Typically, it will remove all the tables that were created through migration classes, leaving alone any other tables that might have been manually created.
The rollback command is especially useful when you're making changes to application models and a db: wipe command can't be used - for instance, if multiple systems depend on the same database.
In this guide, we've seen how to use database migrations and seeders to facilitate setting up development and testing databases for a Laravel 6 application.
As a next step, you might want to check the Laravel documentation for more details on how to use the query builder, and how to use Eloquent models to abstract your application's database schema even further.
How to Install Nginx on CentOS 8
3859
Nginx is one of the most popular web servers in the world and is responsible for hosting some of the largest and most popular sites on the internet.
It is more resource-friendly than Apache in most cases and can be used as a web server or reverse proxy.
In this guide, we'll discuss how to install Nginx on a CentOS 8 server.
In order to install Nginx, we'll use the dnf package manager, which is the new default package manager on CentOS 8.
After that, dnf will install Nginx and any required dependencies to your server.
After the installation is finished, run the following commands to enable and start the server:
This will make Nginx start at system boot.
Step 2 - Adjusting Firewall Rules
In case you have enabled the firewalld firewall as instructed in our initial server setup guide for CentOS 8, you will need to adjust the firewall settings in order to allow external connections on your Nginx web server, which runs on port 80 by default.
Run the following command to permanently enable HTTP connections on port 80:
To verify that the http firewall service was added correctly, you can run:
To apply the changes, you'll need to reload the firewall service:
Now your Nginx server is fully installed and ready to be accessed by external visitors.
You can now test if your Nginx web server is up and running by accessing your server "s public IP address or domain name from your web browser.
< $> note Note: In case you are using DigitalOcean as your DNS hosting provider, you can check our product docs for detailed instructions on how to set up a new domain name and point it to your server.
Step 4 - Managing the Nginx Process
Now that you have your web server up and running, we'll review how to manage the Nginx service through systemctl.
Whenever you need to stop your web server, you can use:
To stop and then start the service again, you can use:
Nginx can also reload configuration changes without dropping connections.
By default, Nginx is configured to start automatically when the server boots.
To re-enable the service and make Nginx start at boot again, you can use:
Step 5 - Getting Familiar with Important Nginx Files and Directories
Now that you know how to manage the Nginx service, you should take a few minutes to familiarize yourself with a few important directories and files.
Content
/ usr / share / nginx / html: The actual web content, which by default only consists of the default Nginx page you saw earlier, is served out of the / usr / share / nginx / html directory.
This can be changed by altering Nginx configuration files.
Server Configuration
/ etc / nginx: The Nginx configuration directory.
All of the Nginx configuration files reside here.
/ etc / nginx / nginx.conf: The main Nginx configuration file.
This can be modified to make changes to the Nginx global configuration.
/ etc / nginx / conf.d /: This directory contains server block configuration files, where you can define the websites that are hosted within Nginx.
A typical approach is to have each website in a separate file that is named after the website's domain name, such as your _ domain.conf.
Server Logs
/ var / log / nginx / access.log: Every request to your web server is recorded in this log file unless Nginx is configured to do otherwise.
/ var / log / nginx / error.log: Any Nginx errors will be recorded in this log.
Step 6 - Setting Up Server Blocks (Optional)
In case you'd like to host multiple websites within the same Nginx web server, you'll need to set up server blocks.
Nginx server blocks work in a similar way to Apache virtual hosts, allowing a single server to respond to multiple domain names and serving different content for each of them.
On CentOS 8, server blocks are defined in .conf files located at / etc / nginx / conf.d.
We will set up a server block for a domain called your _ domain.
By default, Nginx on CentOS 8 is configured to serve documents out of a directory at / usr / share / nginx / html.
While this works well for a single site, it can become unmanageable if you are hosting multiple sites.
Instead of modifying / usr / share / nginx / html, we'll create a directory structure within / var / www for the your _ domain website, leaving / usr / share / nginx / html in place as the default directory to be served if a client request doesn't match any other sites.
Create the directory for your _ domain as follows, using the -p flag to create any necessary parent directories:
Next, assign ownership of the directory with the $USER environment variable, which should reference your current system user:
Next, we'll create a sample index.html page to test the server block configuration.
Now you can use nano to create the sample index.html file:
Inside that file, add the following HTML code:
If you used nano, you can do so by pressing CTRL + X, Y, then ENTER.
In order for Nginx to serve this content, we need to create a server block with the correct directives that point to our custom web root.
We'll create a new server block at / etc / nginx / conf.d / < ^ > your _ domain.conf < ^ >:
Paste in the following configuration block:
Save and close the file when you're done editing its content.
To make sure that there are no syntax errors in any of your Nginx files, run:
If there aren't any problems, you will see the following output:
Once your configuration test passes, restart Nginx to enable your changes:
Before you can test the changes from your browser, you'll need to update your server's SELinux security contexts so that Nginx is allowed to serve content from the / var / www / < ^ > your _ domain < ^ > directory.
The following command will allow your custom document root to be served as HTTP content:
Now you can test your custom domain setup by navigating to http: / / < ^ > your _ domain < ^ >, where you will see something like this:
Nginx server block
This page is rendering the HTML code we've defined in the custom document root created for the server block.
If you're able to see this page, it means your Nginx server is correctly configured to serve your domain.
In this guide, we've seen how to install and set up Nginx, a high-performance web server and reverse proxy.
We reviewed how to manage the Nginx service running on your server, and what are the main directories used by Nginx to store configuration files, content, and logs.
From here, you have many options for the type of content and the technologies that you might want to use in the websites hosted within your web server.
How To Use nsh to Run Secure Remote Commands On Ubuntu 18.04
3825
It can often be difficult to manage multiple machines on a daily basis.
While Secure Shell (SSH) is a good choice for remote access, the protocol itself has some drawbacks in both convenience and security.
For instance, remote machines need to have a public IP address and a forwarded port in order to access them, which exposes them to the internet, or at least a larger network.
This is especially concerning if you use a password for authentication instead of a public and private key pair.
Furthermore, if you don "t know the remote machine" s public key in advance, you might be vulnerable to a "man-in-the-middle" attack.
And many remote machines you want to access either don "t have public IP address, or they have a dynamic IP address you might not know.
In addition, SSH requires one connection per remote session.
If a user needs to run a single command across hundreds or even thousands of machines, they must first establish a connection to each machine with a TCP handshake, which is less efficient.
NKN Shell, or nsh, is an alternative to SSH that provides a convenient and secure way to run remote commands. nsh takes advantage of NKN "s global public network which provides secure and decentralized data transmission.
The architecture uses unique addresses that contain a public key used for both routing and end-to-end encryption without any public key infrastructure (PKI).
The network also does not require the remote server to have a public IP address.
The remote server only needs to have Internet access and be able to establish outbound HTTP and websocket connections.
As a result, your remote machines are not exposed to the open Internet.
In this tutorial you will use the NKN shell daemon and the NKN Shell Client Xterm applications to execute commands on a remote machine.
To do so, you will install and configure the NKN Shell daemon on a remote machine with internet access, generate a key pair, and make your connection from a client.
A Web browser installed on your local machine.
Step 1 - Installing NKN Shell Daemon on a Remote Server
First, install the NKN shell daemon (nsd) on your server.
This application will invoke nkn-multiclient, which will connect to NKN's public network and obtain an address for routing.
The daemon will then listen for incoming shell commands from authenticated and whitelisted clients, execute those commands, and then send back results.
Start by downloading the latest pre-built nshd binary from GitHub:
Decompress the file:
Then move the files into the / usr / local / bin directory so they are available system wide:
Next, you'll configure this to run as a daemon process using Systemd so that it will restart if the server is reset.
Create a file called nshd.service in / etc / systemd / system:
Add the following service definition to the file to configure the service:
Learn more about Systemd unit files in Understanding Systemd Units and Unit Files.
Then enable and start the nshd service with the following commands:
Run the following command to ensure the service is active and started:
You'll see that the status is active:
In order to connect to your server, you'll need to get its NKN address, which you can find in the output of the previous command.
You can also run the following command to obtain the address:
You'll see your address appear:
Take note of this address as you will need it to connect to your server.
Now that the daemon is running and listening, you can configure the web-based client to talk to the server.
Step 2 - Configuring Permissions for NKN Shell Client
You'll need a compatible client that can connect to the remote machine.
In this tutorial you'll use NKN Shell Client Xterm, a web-based NKN shell client.
There are a few different ways to run it:
Use the hosted version at https: / / nsh.nkn.org /. Note that while this web page is hosted on a server, it "s actually a pure local web app that runs in your browser.
Get the source code and host it yourself.
Use the nShell Chrome extension.
In this tutorial you'll use the hosted version.
On your local machine, open your web browser and navigate to https: / / nsh.nkn.org. You'll see a welcome screen:
The Shell Client
Click Generate New Key Pair.
Your keys will be generated and displayed as shown in the following image:
The generated key pair
< $> note Note: When you generate a new key pair, you will see a Secret Seed.
Keep this secret seed secure and safe, just like you would with your SSH private key.
Anyone who has this secret seed can use it to regenerate your public key and then run commands on your remote machines.
Your browser will remember this seed, but you should copy it somewhere safe so you can use it again on a new machine.
Save the Secret Seed somewhere safe.
You can use it later to regenerate your public key so you can connect from a different client machine.
Since this is a new key pair, you must add the Public Key to the file / etc / nshd / authorized _ pubkeys on your server.
/ etc / nshd / authorized _ pubkeys has a similar role as the ~ / authorized _ keys file which controls which SSH public keys can log in.
The authorized _ pubkeys file can specify which user is associated with a key.
For security purposes, you'll want to log in using a non-root user in this tutorial, so you'll associate the generated public key with your sammy user you created in the Initial Server Setup guide in this article's prerequisite.
To associate a user with the public key, you'll need to get the user id (UID) and group id (GID) of this user.
Execute the following command on your server while logged in as the sammy user:
You'll see the UID and GID of the user:
Now open the authorized _ pubkeys file in your editor:
Add a single line containing the public key, uid, and gid, separated by spaces:
Verify that the file contains the correct content:
You'll see your key printed on the screen:
Then restart the nshd daemon to apply the changes:
Now let's test it out by connecting to the server and running a command.
Step 3 - Sending a Command to the remote machine and receive a response
In NKN Shell Client, enter your remote nshd address from Step 1, as well as an optional client identifier:
The nsh website with remote address filled in
Click Connect to initiate the connection.
You'll be connected to your remote machine and shown a terminal prompt within the browser.
From here you can use it just like SSH.
For example, execute the following command to switch to the / etc / nshd directory:
Then list its contents:
You'll see the contents of the directory:
You can disconnect by typing exit.
When you need to reconnect, revisit the web interface and enter your connection details.
If you generate a new key pair, you'll need to add the new public key to your server.
In this tutorial, you installed and configured nsh to securely and conveniently send commands to a remote machine. nsh is a great way to access your remote machines when you need to quickly run a command to get the latest status of a service or peek at some configuration settings.
The application is based on NKN "s global public network, and it" s free to use so you can incorporate it into your own application or workflow today.
You can also explore nkn-tunnel which supports SSH or any other TCP based applications.
Understanding Generators in JavaScript
3855
In ECMAScript 2015, generators were introduced to the JavaScript language.
A generator is a process that can be paused and resumed and can yield multiple values.
A generator in JavaScript consists of a generator function, which returns an iterable Generator object.
Generators can maintain state, providing an efficient way to make iterators, and are capable of dealing with infinite data streams, which can be used to implement infinite scroll on the frontend of a web application, to operate on sound wave data, and more.
Additionally, when used with Promises, generators can mimic the async / await functionality, which allows us to deal with asynchronous code in a more straightforward and readable manner.
Although async / await is a more prevalent way to deal with common, simple asynchronous use cases, like fetching data from an API, generators have more advanced features that make learning how to use them worthwhile.
In this article, we'll cover how to create generator functions, how to iterate over Generator objects, the difference between yield and return inside a generator, and other aspects of working with generators.
Generator Functions
A generator function is a function that returns a Generator object, and is defined by the function keyword followed by an asterisk (*), as shown in the following:
Occasionally, you will see the asterisk next to the function name, as opposed to the function keyword, such as function * generatorFunction ().
This works the same, but function * is a more widely accepted syntax.
Generator functions can also be defined in an expression, like regular functions:
Generators can even be the methods of an object or class:
The examples throughout this article will use the generator function declaration syntax.
< $> note Note: Unlike regular functions, generators cannot be constructed with the new keyword, nor can they be used in conjunction with arrow functions.
Now that you know how to declare generator functions, lets look at the iterable Generator objects that they return.
Generator Objects
Traditionally, functions in JavaScript run to completion, and calling a function will return a value when it arrives at the return keyword.
If the return keyword is omitted, a function will implicitly return undefined.
In the following code, for example, we declare a sum () function that returns a value that is the sum of two integer arguments:
Calling the function returns a value that is the sum of the arguments:
A generator function, however, does not return a value immediately, and instead returns an iterable Generator object.
In the following example, we declare a function and give it a single return value, like a standard function:
When we invoke the generator function, it will return the Generator object, which we can assign to a variable:
If this were a regular function, we would expect generator to give us the string returned in the function.
However, what we actually get is an object in a suspended state.
Calling generator will therefore give output similar to the following:
The Generator object returned by the function is an iterator.
An iterator is an object that has a next () method available, which is used for iterating through a sequence of values.
The next () method returns an object with value and done properties. value represent the returned value, and done indicates whether the iterator has run through all its values or not.
Knowing this, let's call next () on our generator and get the current value and state of the iterator:
The value returned from calling next () is Hello, Generator!, and the state of done is true, because this value came from a return that closed out the iterator.
Since the iterator is done, the generator function's status will change from suspended to closed.
Calling generator again will give the following:
As of right now, we've only demonstrated how a generator function can be a more complex way to get the return value of a function.
But generator functions also have unique features that distinguish them from normal functions.
In the next section, we'll learn about the yield operator and see how a generator can pause and resume execution.
yield Operators
Generators introduce a new keyword to JavaScript: yield. yield can pause a generator function and return the value that follows yield, providing a lightweight way to iterate through values.
In this example, we'll pause the generator function three times with different values, and return a value at the end.
Then we will assign our Generator object to the generator variable.
Now, when we call next () on the generator function, it will pause every time it encounters yield. done will be set to false after each yield, indicating that the generator has not finished.
Once it encounters a return, or there are no more yields encountered in the function, done will flip to true, and the generator will be finished.
Use the next () method four times in a row:
These will give the following four lines of output in order:
Note that a generator does not require a return; if omitted, the last iteration will return {value: undefined, done: true}, as will any subsequent calls to next () after a generator has completed.
Iterating Over a Generator
Using the next () method, we manually iterated through the Generator object, receiving all the value and done properties of the full object.
However, just like Array, Map, and Set, a Generator follows the iteration protocol, and can be iterated through with for... of:
This will return the following:
The spread operator can also be used to assign the values of a Generator to an array.
This will give the following array:
Both spread and for... of will not factor the return into the values (in this case, it would have been 'The Oracle ').
< $> note Note: While both of these methods are effective for working with finite generators, if a generator is dealing with an infinite data stream, it won't be possible to use spread or for... of directly without creating an infinite loop.
Closing a Generator
As we've seen, a generator can have its done property set to true and its status set to closed by iterating through all its values.
There are two additional ways to immediately cancel a generator: with the return () method, and with the throw () method.
With return (), the generator can be terminated at any point, just as if a return statement had been in the function body.
You can pass an argument into return (), or leave it blank for an undefined value.
To demonstrate return (), we'll create a generator with a few yield values but no return in the function definition:
The first next () will give us' Neo ', with done set to false.
If we invoke a return () method on the Generator object right after that, we'll now get the passed value and done set to true.
Any additional call to next () will give the default completed generator response with an undefined value.
To demonstrate this, run the following three methods on generator:
This will give the three following results:
The return () method forced the Generator object to complete and to ignore any other yield keywords.
This is particularly useful in asynchronous programming when you need to make functions cancelable, such as interrupting a web request when a user wants to perform a different action, as it is not possible to directly cancel a Promise.
If the body of a generator function has a way to catch and deal with errors, you can use the throw () method to throw an error into the generator.
This starts up the generator, throws the error in, and terminates the generator.
To demonstrate this, we will put a try... catch inside the generator function body and log an error if one is found:
Now, we will run the next () method, followed by throw ():
Using throw (), we injected an error into the generator, which was caught by the try... catch and logged to the console.
Generator Object Methods and States
The following table shows a list of methods that can be used on Generator objects:
next ()
Returns the next value in a generator
return ()
Returns a value in a generator and finishes the generator
throw ()
Throws an error and finishes the generator
The next table lists the possible states of a Generator object:
Status
suspended
Generator has halted execution but has not terminated
closed
Generator has terminated by either encountering an error, returning, or iterating through all values
yield Delegation
In addition to the regular yield operator, generators can also use the yield * expression to delegate further values to another generator.
When the yield * is encountered within a generator, it will go inside the delegated generator and begin iterating through all the yields until that generator is closed.
This can be used to separate different generator functions to semantically organize your code, while still having all their yields be iterable in the right order.
To demonstrate, we can create two generator functions, one of which will yield * operate on the other:
Next, let's iterate through the begin () generator function:
This will give the following values in the order they are generated:
The outer generator yielded the values 1 and 2, then delegated to the other generator with yield *, which returned 3 and 4.
yield * can also delegate to any object that is iterable, such as an Array or a Map.
Yield delegation can be helpful in organizing code, since any function within a generator that wanted to use yield would also have to be a generator.
Infinite Data Streams
One of the useful aspects of generators is the ability to work with infinite data streams and collections.
This can be demonstrated by creating an infinite loop inside a generator function that increments a number by one.
In the following code block, we define this generator function and then initiate the generator:
Now, iterate through the values using next ():
The function returns successive values in the infinite loop while the done property remains false, ensuring that it will not finish.
With generators, you don't have to worry about creating an infinite loop, because you can halt and resume execution at will.
However, you still have to have caution with how you invoke the generator.
If you use spread or for... of on an infinite data stream, you will still be iterating over an infinite loop all at once, which will cause the environment to crash.
For a more complex example of an infinite data stream, we can create a Fibonacci generator function.
The Fibonacci sequence, which continuously adds the two previous values together, can be written using an infinite loop within a generator as follows:
To test this out, we can loop through a finite number and print the Fibonacci sequence to the console.
The ability to work with infinite data sets is one part of what makes generators so powerful.
This can be useful for examples like implementing infinite scroll on the frontend of a web application.
Passing Values in Generators
Throughout this article, we've used generators as iterators, and we've yielded values in each iteration.
In addition to producing values, generators can also consume values from next ().
In this case, yield will contain a value.
It's important to note that the first next () that is called will not pass a value, but will only start the generator.
To demonstrate this, we can log the value of yield and call next () a few times with some values.
It is also possible to seed the generator with an initial value.
In the following example, we'll make a for loop and pass each value into the next () method, but pass an argument to the initial function as well:
We'll retrieve the value from next () and yield a new value to the next iteration, which is the previous value times ten.
Another way to deal with starting up a generator is to wrap the generator in a function that will always call next () once before doing anything else.
async / await with Generators
An asynchronous function is a type of function available in ES6 + JavaScript that makes working with asynchronous data easier to understand by making it appear synchronous.
Generators have a more extensive array of capabilities than asynchronous functions, but are capable of replicating similar behavior.
Implementing asynchronous programming in this way can increase the flexibility of your code.
In this section, we will demonstrate an example of reproducing async / await with generators.
Let's build an asynchronous function that uses the Fetch API to get data from the JSONPlaceholder API (which provides example JSON data for testing purposes) and logs the response to the console.
Start out by defining an asynchronous function called getUsers that fetches data from the API and returns an array of objects, then call getUsers:
This will give JSON data similar to the following:
Using generators, we can create something almost identical that does not use the async / await keywords.
Instead, it will use a new function we create and yield values instead of await promises.
In the following code block, we define a function called getUsers that uses our new asyncAlt function (which we will write later on) to mimic async / await.
As we can see, it looks almost identical to the async / await implementation, except that there is a generator function being passed in that yields values.
Now we can create an asyncAlt function that resembles an asynchronous function. asyncAlt has a generator function as a parameter, which is our function that yields the promises that fetch returns. asyncAlt returns a function itself, and resolves every promise it finds until the last one:
This will give the same output as the async / await version:
Note that this implementation is for demonstrating how generators can be used in place of async / await, and is not a production-ready design.
It does not have error handling set up, nor does it have the ability to pass parameters into the yielded values.
Though this method can add flexibility to your code, often async / await will be a better choice, since it abstracts implementation details away and lets you focus on writing productive code.
Generators are processes that can halt and resume execution.
They are a powerful, versatile feature of JavaScript, although they are not commonly used.
In this tutorial, we learned about generator functions and generator objects, methods available to generators, the yield and yield * operators, and generators used with finite and infinite data sets.
We also explored one way to implement asynchronous code without nested callbacks or long promise chains.
If you would like to learn more about JavaScript syntax, take a look at our Understanding This, Bind, Call, and Apply in JavaScript and Understanding Map and Set Objects in JavaScript tutorials.
How was the translation quality?
You flagged this translation.
You rated this translation helpful.
How To Install and Use TimescaleDB on Ubuntu 18.04
3219
Managing time series data has become an essential skill with the rise of the Internet of Things (IoT) and Industrial Internet of Things.
By following this tutorial, you'll set up TimescaleDB on Ubuntu 18.04, configure it, and learn how to work with it. You'll create time series databases and make simple queries.
Finally, you'll see how to get rid of unnecessary data.
One Ubuntu 18.04 server set up by following our Initial Server Setup Guide for Ubuntu 18.04, including a non-root user with sudo privileges and a firewall.
Follow Step 1 of How To Install and Use PostgreSQL on Ubuntu 18.04 to install it.
TimescaleDB is not available in Ubuntu's default package repositories, so in this step you will install it from the TimescaleDB Personal Packages Archive (PPA).
First, add Timescale's APT repository:
Confirm this action by hitting the ENTER key.
Next, refresh your APT cache to update your package lists:
This tutorial uses PostgreSQL version 10; if you are using a different version of PostgreSQL (11 or 9.6, for example), replace the value in the following command and run it:
The TimescaleDB module works fine with the default PostgreSQL configuration settings, but to improve performance and make better use of processor, memory, and disk resources, the developers of TimescaleDB suggest configuring some individual parameters.
In this tutorial, you will use the timescaledb-tune tool, which will read the postgresql.conf file and interactively suggest making changes.
Next, you will be prompted to change the shared _ preload _ libraries variable to preload the TimescaleDB module upon starting the PostgreSQL server:
shared _ preload _ libraries accepts a comma separated list of modules as a value, designating which modules PostgreSQL should load before starting the database server.
Making this change will add the timescaledb module to that list.
< $> note Note: If a library specified by shared _ preload _ libraries is not found, the database server will fail to start.
Keep this in mind when debugging applications that make use of shared _ preload _ libraries.
For more information on this, see this PostgresqlCO.NF article on shared _ preload _ libraries.
Enable the TimescaleDB module by typing y at this prompt and pressing ENTER:
Based on the characteristics of your server and the PostgreSQL version, the script will then offer to tune your settings.
timescaledb-tune will automatically detect the servers's available memory and calculate recommended values for a number of settings. shared _ buffers, for example, determines the amount of memory allocated for caching data. By default this setting is relatively low to account for a wider range of platforms, so timescaledb-tune has suggested increasing the value from 128MB to 1994MB, taking better advantage of resources by making more room to store cached information like repeated queries.
The work _ mem variable has been increased as well to allow for more complicated sorts.
If you would like to learn more about the process of tuning memory settings for PostgreSQL, see the Tuning Your PostgreSQL Server article on the PostgreSQL wiki.
Enter y to accept the values:
These settings determine how multiple CPUs can make simultaneous queries in parallel to scan databases and return the requested data quicker.
These settings regulate the number of workers, which process requests and background tasks.
WAL is a logging method in which PostgreSQL logs changes to data files before the changes are made to the database.
By prioritizing an up-to-date record of data changes, WAL ensures that you can reconstruct your database in the event of a crash.
In this way, it preserves data integrity.
However, the default settings can cause inefficient input / output (I / O) operations that slow down write performance.
To fix this, type and enter y:
As a result, you will get a ready-made configuration file at / etc / postgresql / < ^ > 10 < ^ > / main / postgresql.conf.
< $> note Note: If you are automating the installation, you could also run the initial command with the --quiet and --yes flags, which will automatically apply all the recommendations and will make changes to the postgresql.conf configuration file:
To demonstrate this, you will use PostgreSQL commands to create a database, then enable the TimescaleDB extension to create a hypertable, which is a higher-level abstraction of many individual tables.
Hypertables are the main structures you will work with in TimescaleDB.
Log into your PostgreSQL database:
As mentioned earlier, the primary points of interaction with your time series data are hypertables, which consist of many individual tables holding data, called chunks.
You can insert data into the hypertable using the standard INSERT SQL command.
After the delete operation, use the VACUUM command to reclaim space still used by data that has been deleted.
This is an official TimescaleDB dataset, made to test out their database.
In addition to standard SQL commands, TimescaleDB also provides a number of special functions that are useful for time series data analysis.
You've now set up TimescaleDB on your Ubuntu 18.04 server.
Now that you know how to store time series data, you could use the data to create graphs.
TimescaleDB is compatible with visualization tools that work with PostgreSQL, like Grafana.
You can use our How To Install and Secure Grafana on Ubuntu 18.04 tutorial to learn more about this popular visualization tool.
How To Set Up an Object Storage Server Using Minio on Ubuntu 18.04
3275
From cloud-based backup solutions to high-availability content delivery networks (CDNs), the ability to store unstructured blobs of object data and make them accessible through HTTP APIs, known as object storage, has become an integral part of the modern technology landscape.
Minio is a popular open-source object storage server compatible with the Amazon S3 cloud storage service.
Applications that have been configured to talk to Amazon S3 can also be configured to talk to Minio, allowing Minio to be a viable alternative to S3 if you want more control over your object storage server.
The service stores unstructured data such as photos, videos, log files, backups, and container / VM images, and can even provide a single object storage server that pools multiple drives spread across many servers.
Minio is written in Go, comes with a command line client plus a browser interface, and supports simple queuing service for Advanced Message Queuing Protocol (AMQP), Elasticsearch, Redis, NATS, and PostgreSQL targets.
For all of these reasons, learning to set up a Minio object storage server can add a wide range of flexibility and utility to your project.
In this tutorial, you will:
Install the Minio server on your Ubuntu 18.04 server and configure it as a systemd service.
Set up an SSL / TLS certificate using Let's Encrypt to secure communication between the server and the client.
Access Minio's browser interface via HTTPS to use and administrate the server.
One Ubuntu 18.04 server set up by following our Ubuntu 18.04 initial server setup tutorial, including a sudo non-root user and a firewall.
You can purchase one on Namecheap or get one for free on Freenom.
In this tutorial, your domain will be represented as < ^ > your _ domain < ^ >.
The following DNS records set up for your Minio server.
You can follow our DNS records documentation for details on how to add them for a DigitalOcean Droplet.
An A record with your server name (e.g. < ^ > minio-server.your _ domain < ^ >) pointing to your object server's IPv4 address.
(Optional) If you want your server reachable via IPv6, you'll need an AAAA record with your server name pointing to your object server's IPv6 address.
Step 1 & mdash; Installing and Configuring the Minio Server
You can install the Minio server by compiling the source code or via a binary file.
To install it from the source, you need to have at least Go 1.12 installed on your system.
In this step, you will install the server through the precompiled binary and then configure the Minio server afterward.
First, log in to your server, replacing < ^ > sammy < ^ > with your username and < ^ > your _ server _ ip < ^ > with your Ubuntu 18.04 server's IP address:
If you haven't updated the package database recently, update it now:
Next, download the Minio server's binary file from the official website:
Once the download is finished, a file named minio will be in your working directory.
Use the following command to make it executable:
Now, move the file into the / usr / local / bin directory where Minio's systemd startup script expects to find it:
This will allow us to write a service unit file later in this tutorial to automatically run Minio on startup.
For security reasons, it is best to avoid running the Minio server as root.
This will limit the damage that can be done to your system if compromised.
Since the systemd script you'll use in Step 2 looks for a user account and group called minio-user, make a new user with this name:
In this command, you used the -s flag to set / sbin / nologin as the shell for minio-user.
This is a shell that does not allow user login, which is not needed for minio-user.
Next, change ownership of the Minio binary to minio-user:
Next, you will create a directory where Minio will store files.
This will be the storage location for the buckets that you will use later to organize the objects you store on your Minio server.
This tutorial will name the directory < ^ > minio < ^ >:
Give ownership of that directory to minio-user:
Most server configuration files are stored in the / etc directory, so create your Minio configuration file there:
Give ownership of that directory to minio-user, too:
Use Nano or your favorite text editor to create the environment file needed to modify the default configuration:
Once the file is open, add in the following lines to set some important environment variables in your environment file:
Let's take a look at these variables and the values you set:
MINIO _ ACCESS _ KEY: This sets the access key you will use to access the Minio browser user interface.
MINIO _ SECRET _ KEY: This sets the private key you will use to complete your login credentials into the Minio interface.
This tutorial has set the value to < ^ > miniostorage < ^ >, but we advise choosing a different, more complicated password to secure your server.
MINIO _ VOLUMES: This identifies the storage directory that you created for your buckets.
MINIO _ OPTS: This changes where and how the server serves data. The -C flag points Minio to the configuration directory it should use, while the --address flag tells Minio the IP address and port to bind to.
If the IP address is not specified, Minio will bind to every address configured on the server, including localhost and any Docker-related IP addresses, so directly specifying the IP address here is recommended.
The default port 9000 can be changed if you would like.
Finally, save and close the environment file when you're finished making changes.
You've now installed Minio and set some important environment variables.
Next, you'll configure the server to run as a system service.
Step 2 & mdash; Installing the Minio Systemd Startup Script
In this step, you'll configure the Minio server to be managed as a systemd service.
First, download the official Minio service descriptor file using the following command:
After the download has finished, a file named minio.service will be in your working directory.
To audit the contents of minio.service before applying it, open it in a text editor to view its contents:
This service unit file starts the Minio server using the minio-user user that you created earlier.
It also implements the environment variables you set in the last step, and makes the server run automatically on startup.
For more information on systemd unit files, see our guide Understanding Systemd Units and Unit Files.
Once you've looked over the script's contents, close your text editor.
Systemd requires that unit files be stored in the systemd configuration directory, so move minio.service there:
Then, run the following command to reload all systemd units:
Finally, enable Minio to start on boot:
Now that the systemd script is installed and configured, it's time to start the server.
Step 3 & mdash; Starting the Minio Server
In this step, you'll start the server and modify the firewall to allow access through the browser interface.
First, start the Minio server:
Next, verify Minio's status, the IP address it's bound to, its memory usage, and more by running this command:
Next, enable access through the firewall to the Minio server on the configured port. In this tutorial, that's port < ^ > 9000 < ^ >.
First add the rule:
Then, enable the firewall:
You will get the following prompt:
Press y and ENTER to confirm this.
You will then get the following output:
Minio is now ready to accept traffic, but before connecting to the server, you will secure communication by installing an SSL / TLS certificate.
Step 4 & mdash; Securing Access to Your Minio Server With a TLS Certificate
In this step, you will secure access to your Minio server using a private key and public certificate that has been obtained from a certificate authority (CA), in this case Let's Encrypt.
To get a free SSL certificate, you will use Certbot.
First, allow HTTP and HTTPS access through your firewall.
To do this, open port 80, which is the port for HTTP:
Next, open up port 443 for HTTPS:
Once you've added these rules, check on your firewall's status with the following command:
This confirms that ports 80 and 443 are open, ensuring that your server accepts requests from the internet.
Next, you will install Certbot.
Since Certbot maintains a separate PPA repository, you will first have to add it to your list of repositories before installing Certbot as shown:
To prepare to add the PPA repository, first install software-properties-common, a package for managing PPAs:
This package provides some useful scripts for adding and removing PPAs instead of doing it manually.
Now add the Universe repository:
This repository contains free and open source software maintained by the Ubuntu community, but is not officially maintained by Canonical, the developers of Ubuntu.
This is where we will find the repository for Certbot.
Next, add the Certbot repository:
Press ENTER to accept.
Then update the package list:
Finally, install certbot:
Next, you will use certbot to generate a new SSL certificate.
Since Ubuntu 18.04 doesn't yet support automatic installation, you will use the certonly command and --standalone to obtain the certificate:
--standalone means that this certificate is for a built-in standalone web server.
For more information on this, see our How To Use Certbot Standalone Mode to Retrieve Let's Encrypt SSL Certificates on Ubuntu 18.04 tutorial.
Add your email and press ENTER.
Certbot will then ask you to register with Let's Encrypt:
Type A and press ENTER to agree.
Next, you will be asked if you are willing to share your email with the Electronic Frontier Foundation:
Once you answer Y or N, your public and private keys will be generated and saved in the / etc / letsencrypt / live / minio-server. < ^ > your _ domain _ name < ^ > directory.
Next, copy these two files (privkey.pem and fullchain.pem) into the certs directory under Minio's server configuration folder, which is / etc / minio for this tutorial.
Use the following to copy privkey.pem and rename the file private.key:
Then do the same for fullchain.pem, naming the result public.crt:
Now, change the ownership of the files to minio-user.
First, do this for private.key:
Then public.crt:
Restart the Minio server, so that it becomes aware of the certificate and starts using HTTPS:
Let "s Encrypt certificates are only valid for ninety days.
This is to encourage users to automate their certificate renewal process.
The Certbot package you installed automatically adds a renew script to / etc / cron.d.
With that, Minio's connection is now secure, and the SSL / TLS certificate will automatically renew for you.
In the next step, you'll connect to Minio through the browser to use the server.
Step 5 & mdash; Securely Connecting to Minio's Web Interface Using HTTPS
In this step, you'll securely connect to the Minio web interface via HTTPS, and then you'll create buckets and upload objects into them.
Access the web interface by pointing your browser to https: / / minio-server. < ^ > your _ domain < ^ >: < ^ > 9000 < ^ >.
You will see the Minio server login screen:
Minio login screen
Now, log in to the main interface by entering your credentials.
For Access Key, enter the MINIO _ ACCESS _ KEY you set in the / etc / default / < ^ > minio < ^ > environment file in Step 1. For Secret Key, type the MINIO _ SECRET _ KEY you set in the same file.
Once you've entered the credentials, click the round button with the arrow directly below the input fields.
You will then be presented with the Minio user interface.
To create a new bucket in which you can store objects, click the light-red + button on the bottom right of the main interface to bring up two additional yellow buttons.
Minio's main interface
Click the middle yellow button and enter a name for your new bucket in the prompt, pressing the ENTER key to save your response.
Your new bucket is now ready to be used for storage.
< $> note Note: When naming your Minio bucket, make sure that your name only contains lowercase letters, numbers, or hyphens.
Minio limits bucket naming conventions in order to be compatible with AWS S3 standards.
When you want to add objects into your bucket, click the same light-red button as before and then click the top yellow button to open a file-upload prompt.
At this point, you've worked through the entire basic web interface of creating buckets and uploading objects.
You now have your own Minio object storage server that you can connect to securely from the web interface using a Let's Encrypt SSL / TLS certificate.
Optionally, you may want to look at the Minio desktop clients for FreeBSD, Linux, Mac, and Windows as an alternative way to use and administrate your object storage server.
Additionally, if you'd like to increase your Minio installation's storage capacity beyond your server's disk size, you can use DigitalOcean's block storage service to attach a volume to your server, extending storage capacity by as much as 80 TB.
More information about Minio is available at the project's documentation website.
If you'd like to learn more about object storage, browse our Object Storage tutorials.
How To Build a Hashicorp Vault Server Using Packer and Terraform on DigitalOcean Quickstart
3926
For a more detailed version of this tutorial, please refer to How To Build a Hashicorp Vault Server Using Packer and Terraform on DigitalOcean.
Visit How to Create a Personal Access Token to create one.
Create and move into the ~ / vault-orchestration directory to store your Vault files:
Create separate directories for Packer and Terraform configuration by running:
Navigate to the Packer directory:
Create a variables.json in your packer subdirectory to store your private variable data:
You can edit the base image, region, and Droplet size values according to the developer docs.
Replace < ^ > your _ do _ api _ key < ^ > with your API key, then save and close the file.
Create your Packer template for Vault in a file named template.json:
You define a single digitalocean builder.
Packer will create a temporary Droplet of the defined size, image, and region using the provided API key.
The provisioner will connect to it using SSH with the specified username and will sequentially execute all defined provisioners before creating a DigitalOcean Snapshot from the Droplet and deleting it.
Check the official Vault download page for the most up-to-date version for Linux.
Verify the validity of your template:
Build your snapshot with the Packer build command:
The last line contains the name of the snapshot (such as packer-1581537927) and its ID in parentheses, highlighted here.
Note your ID of the snapshot, because you'll need it in the next step.
Navigate to the terraform subdirectory:
Create a file named do-provider.tf to store the provider:
This file provides the digitalocean provider with an API key.
To specify the values of these variables you'll create a variable definitions file similarly to Packer.
Create a variable definitions file:
Replace < ^ > your _ do _ api _ key < ^ >, < ^ > your _ ssh _ key _ fingerprint < ^ >, and < ^ > your _ do _ snapshot _ id < ^ > (the snapshot ID you noted from the previous step).
Create the following file to store the Vault snapshot deployment configuration:
You define a single resource of the type digitalocean _ droplet named vault.
You set its parameters according to the variable values and add an SSH key (using its fingerprint) from your DigitalOcean account to the Droplet resource.
You output the IP addresses of all newly deployed instances to the console.
Initialize the directory as a Terraform project:
Test the validity of your configuration:
Run the plan command to see what Terraform will attempt when it comes to provision the infrastructure:
Execute the plan:
The Droplet will finish provisioning and you'll see output similar to this:
Run the following to connect to your new Droplet:
Once you are logged in, run Vault with:
You'll see its "help" output:
3205
When developing a Ruby on Rails application, you may find you have application tasks that should be performed asynchronously.
Processing data, sending batch emails, or interacting with external APIs are all examples of work that can be done asynchronously with background jobs.
Using background jobs can improve your application's performance by offloading potentially time-intensive tasks to a background processing queue, freeing up the original request / response cycle.
Sidekiq is one of the more widely used background job frameworks that you can implement in a Rails application.
It is backed by Redis, an in-memory key-value store known for its flexibility and performance.
Sidekiq uses Redis as a job management store to process thousands of jobs per second.
In this tutorial, you will add Redis and Sidekiq to an existing Rails application.
You will create a set of Sidekiq worker classes and methods to handle:
A batch upload of endangered shark information to the application database from a CSV file in the project repository.
The removal of this data.
When you are finished, you will have a demo application that uses workers and jobs to process tasks asynchronously.
This will be a good foundation for you to add workers and jobs to your own application, using this tutorial as a jumping off point.
A local machine or development server running Ubuntu 18.04.
Your development machine should have a non-root user with administrative privileges and a firewall configured with ufw.
For instructions on how to set this up, see our Initial Server Setup with Ubuntu 18.04 tutorial.
Node.js and npm installed on your local machine or development server.
This tutorial uses Node.js version < ^ > 10.17.0 < ^ > and npm version < ^ > 6.11.3 < ^ >.
For guidance on installing Node.js and npm on Ubuntu 18.04, follow the instructions in the "Installing Using a PPA" section of How To Install Node.js on Ubuntu 18.04.
The Yarn package manager installed on your local machine or development server.
You can following the installation instructions in the official documentation.
Ruby, rbenv, and Rails installed on your local machine or development server, following Steps 1-4 in How To Install Ruby on Rails with rbenv on Ubuntu 18.04.
This tutorial uses Ruby < ^ > 2.5.1 < ^ >, rbenv < ^ > 1.1.2 < ^ >, and Rails < ^ > 5.2.3 < ^ >.
SQLite installed, following Step 1 of How To Build a Ruby on Rails Application.
This tutorial uses SQLite 3 < ^ > 3.22.0 < ^ >.
Redis installed, following Steps 1-3 of How To Install and Secure Redis on Ubuntu 18.04.
This tutorial uses Redis < ^ > 4.0.9 < ^ >.
Step 1 - Cloning the Project and Installing Dependencies
Our first step will be to clone the rails-bootstrap repository from the DigitalOcean Community GitHub account.
This repository includes the code from the setup described in How To Add Bootstrap to a Ruby on Rails Application, which explains how to add Bootstrap to an existing Rails 5 project.
Clone the repository into a directory called < ^ > rails-sidekiq < ^ >:
Navigate to the < ^ > rails-sidekiq < ^ > directory:
In order to work with the code, you will first need to install the project's dependencies, which are listed in its Gemfile.
You will also need to add the sidekiq gem to the project to work with Sidekiq and Redis.
Open the project's Gemfile for editing, using nano or your favorite editor:
Save and close the file when you are finished adding the gem.
Use the following command to install the gems:
You will see in the output that the redis gem is also installed as a requirement for sidekiq.
Next, you will install your Yarn dependencies.
Because this Rails 5 project has been modified to serve assets with webpack, its JavaScript dependencies are now managed by Yarn.
This means that it's necessary to install and verify the dependencies listed in the project's package.json file.
Run yarn install to install these dependencies:
Next, run your database migrations:
Once your migrations have finished, you can test the application to ensure that it is working as expected.
Start your server in the context of your local bundle with the following command if you are working locally:
If you are working on a development server, you can start the application with:
Navigate to localhost: 3000 or http: / / < ^ > your _ server _ ip < ^ >: 3000.
To create a new shark, click on the Get Shark Info button, which will take you to the sharks / index route:
Sharks Index Route
On the New Shark page, input "Great White" into the Name field and "Scary" into the Facts field:
Shark Create
Once you see that your shark has been created, you can kill the server with CTRL + C.
You have now installed the necessary dependencies for your project and tested its functionality.
Next, you can make a few changes to the Rails application to work with your endangered sharks resources.
Step 2 - Generating a Controller for Endangered Shark Resources
To work with our endangered shark resources, we will add a new model to the application and a controller that will control how information about endangered sharks is presented to users.
Our ultimate goal is to make it possible for users to upload a large batch of information about endangered sharks without blocking our application's overall functionality, and to delete that information when they no longer need it.
First, let's create an Endangered model for our endangered sharks.
We'll include a string field in our database table for the shark name, and another string field for the International Union for the Conservation of Nature (IUCN) categories that determine the degree to which each shark is at risk.
Ultimately, our model structure will match the columns in the CSV file that we will use to create our batch upload.
This file is located in the db directory, and you can check its contents with the following command:
The file contains a list of 73 endangered sharks and their IUCN statuses - vu for vulnerable, en for endangered, and cr for critically endangered.
Our Endangered model will correlate with this data, allowing us to create new Endangered instances from this CSV file.
Create the model with the following command:
Next, generate an Endangered controller with an index action:
This will give us a starting point to build out our application's functionality, though we will also need to add custom methods to the controller file that Rails has generated for us.
Open that file now:
Rails has provided us with a skeletal outline that we can begin to fill in.
First, we'll need to determine what routes we require to work with our data. Thanks to the generate controller command, we have an index method to begin with.
This will correlate to an index view, where we will present users with the option to upload endangered sharks.
However, we will also want to deal with cases where users may have already uploaded the sharks; they will not need an upload option in this case.
We will somehow need to assess how many instances of the Endangered class already exist, since more than one indicates that the batch upload has already occurred.
Let's start by creating a set _ endangered private method that will grab each instance of our Endangered class from the database.
Note that the before _ action filter will ensure that the value of @ endangered is only set for the index and data routes, which will be where we handle the endangered shark data.
Next, add the following code to the index method to determine the correct path for users visiting this part of the application:
If there are more than 0 instances of our Endangered class, we will redirect users to the data route, where they can view information about the sharks they've created.
Otherwise, they will see the index view.
Next, below the index method, add a data method, which will correlate to a data view:
Next, we will add a method to handle the data upload itself.
We'll call this method upload, and it will call a Sidekiq worker class and method to perform the data upload from the CSV file.
We will create the definition for this worker class, AddEndangeredWorker, in the next step.
For now, add the following code to the file to call the Sidekiq worker to perform the upload:
By calling the perform _ async method on the AddEndangeredWorker class, using the CSV file as an argument, this code ensures that the shark data and upload job get passed to Redis.
The Sidekiq workers that we will set up monitor the job queue and will respond when new jobs arise.
After calling perform _ async, our upload method redirects to the data path, where users will be able to see the uploaded sharks.
Next, we'll add a destroy method to destroy the data. Add the following code below the upload method:
Like our upload method, our destroy method includes a perform _ async call on a RemoveEndangeredWorker class - the other Sidekiq worker that we will create.
After calling this method, it redirects users to the root application path.
As a final step in solidifying our application's routes, we will modify the code in config / routes.rb, the file where our route declarations live.
The file currently looks like this:
We will need to update the file to include the routes that we've defined in our controller: data, upload, and destroy.
Our data route will match with a GET request to retrieve the shark data, while our upload and destroy routes will map to POST requests that upload and destroy that data.
Add the following code to the file to define these routes:
With your Endangered model and controller in place, you can now move on to defining your Sidekiq worker classes.
Step 3 - Defining Sidekiq Workers
We have called perform _ async methods on our Sidekiq workers in our controller, but we still need to create the workers themselves.
First, create a workers directory for the workers:
Open a file for the AddEndangeredWorker worker:
In this file, we will add code that will allow us to work with the data in our CSV file.
First, add code to the file that will create the class, include the Ruby CSV library, and ensure that this class functions as a Sidekiq Worker:
We're also including the retry: false option to ensure that Sidekiq does not retry the upload in the case of failure.
Next, add the code for the perform function:
The perform method receives arguments from the perform _ async method defined in the controller, so it's important that the argument values are aligned.
Here, we pass in csv _ file, the variable we defined in the controller, and we use the foreach method from the CSV library to read the values in the file.
Setting headers: true for this loop ensures that the first row of the file is treated as a row of headers.
The block then reads the values from the file into the columns we set for our Endangered model: name and iucn.
Running this loop will create Endangered instances for each of the entries in our CSV file.
Next, we will create a worker to handle deleting this data. Open a file for the RemoveEndangeredWorker class:
Add the code to define the class, and to ensure that it uses the CSV library and functions as a Sidekiq Worker:
Next, add a perform method to handle the destruction of the endangered shark data:
The perform method calls destroy _ all on the Endangered class, which will remove all instances of the class from the database.
With your workers in place, you can move on to creating a layout for your endangered views, and templates for your index and data views, so that users can upload and view endangered sharks.
Step 4 - Adding Layouts and View Templates
In order for users to enjoy their endangered shark information, we will need to address two things: the layout for the views defined in our endangered controller, and the view templates for the index and data views.
Currently, our application makes use of an application-wide layout, located at app / views / layouts / application.html.erb, a navigation partial, and a layout for sharks views.
The application layout checks for a content block, which allows us to load different layouts based on which part of the application our user is engaging with: for the home index page, they will see one layout, and for any views relating to individual sharks, they will see another.
We can repurpose the sharks layout for our endangered views since this format will also work for presenting shark data in bulk.
Copy the sharks layout file over to create an endangered layout:
Next, we'll work on creating the view templates for our index and data views.
Open the index template first:
Delete the boilerplate code and add the following code instead, which will give users some general information about the endangered categories and present them with the option to upload information about endangered sharks:
A form _ tag makes the data upload possible by pointing a post action to the endangered _ upload _ path - the route we defined for our uploads.
A submit button, created with the submit _ tag, prompts users to "Import Endangered Sharks".
In addition to this code, we've included some general information about ICUN codes, so that users can interpret the data they will see.
Next, open a file for the data view:
Add the following code, which will add a table with the endangered shark data:
This code includes the ICUN status codes once again, and a Bootstrap table for the outputted data. By looping through our @ endangered variable, we output the name and ICUN status of each shark to the table.
Below the table, we have another set of form _ tags and submit _ tags, which post to the destroy path by offering users the option to "Delete Endangered Sharks".
The last modification we'll make to our views will be in the index view associated with our home controller.
You may recall that this view is set as the root of the application in config / routes.rb.
Open this file for editing:
Find the column in the row that states Sharks are ancient:
We've included a call to action for users to learn more about endangered sharks, by first sharing a strong message, and then adding a button _ to helper that submits a GET request to our endangered index route, giving users access to that part of the application.
From there, they will be able to upload and view endangered shark information.
With your code in place, you are ready to start the application and upload some sharks!
Step 5 - Starting Sidekiq and Testing the Application
Before we start the application, we'll need to run migrations on our database and start Sidekiq to enable our workers.
Redis should already be running on the server, but we can check to be sure.
With all of these things in place, we'll be ready to test the application.
First, check that Redis is running:
You can now start Sidekiq in the context of your current project bundle by using the bundle exec sidekiq command:
You will see output like this, indicating that Sidekiq is ready to process jobs:
Open a second terminal window, navigate to the < ^ > rails-sidekiq < ^ > directory, and start your application server.
If you are running the application locally, use the following command:
If you are working with a development server, run the following:
Navigate to localhost: 3000 or http: / / < ^ > your _ server _ ip < ^ >: 3000 in the browser.
This will now take you directly to the data view, since you already uploaded the sharks.
To test the delete functionality, click on the Delete Endangered Sharks button below the table.
You should be redirected to the home application page once again.
Clicking on Which Sharks Are in Danger?
one last time will take you back to the index view, where you will have the option to upload sharks again:
Your application is now running with Sidekiq workers in place, which are ready to process jobs and ensure that users have a good experience working with your application.
You now have a working Rails application with Sidekiq enabled, which will allow you to offload costly operations to a job queue managed by Sidekiq and backed by Redis.
This will allow you to improve your site's speed and functionality as you develop.
If you would like to learn more about Sidekiq, the docs are a good place to start.
To learn more about Redis, check out our library of Redis resources.
You can also learn more about running a managed Redis cluster on DigitalOcean by looking at the product documentation.
How To Manage Sorted Sets in Redis
3142
Redis is an open-source, in-memory key-value data store.
In Redis, sorted sets are a data type similar to sets in that both are non repeating groups of strings.
The difference is that each member of a sorted set is associated with a score, allowing them to be sorted from the smallest score to the largest.
As with sets, every member of a sorted set must be unique, though multiple members can share the same score.
This tutorial explains how to create sorted sets, retrieve and remove their members, and create new sorted sets from existing ones.
This guide is written as a cheat sheet with self-contained examples.
We encourage you to jump to any section that is relevant to the task you're trying to complete.
The commands shown in this guide were tested on an Ubuntu 18.04 server running Redis version < ^ > 4.0.9 < ^ >.
To set up a similar environment, you can follow Step 1 of our guide on How To Install and Secure Redis on Ubuntu 18.04.
We will demonstrate how these commands behave by running them with redis-cli, the Redis command line interface.
Note that if you're using a different Redis interface - Redli, for example - the exact output of certain commands may differ.
Alternatively, you could provision a managed Redis database instance to test these commands, but note that depending on the level of control allowed by your database provider, some commands in this guide may not work as described.
To provision a DigitalOcean Managed Database, follow our Managed Databases product documentation.
Then, you must either install Redli or set up a TLS tunnel in order to connect to the Managed Database over TLS.
Creating Sorted Sets and Adding Members
To create a sorted set, use the zadd command. zadd accepts as arguments the name of the key that will hold the sorted set, followed by the score of the member you're adding and the value of the member itself.
The following command will create a sorted set key named faveGuitarists with one member, "Joe Pass", that has a score of 1:
zadd will return an integer that indicates how many members were added to the sorted set if it was created successfully.
You can add more than one member to a sorted set with zadd.
Note that their scores don't need to be sequential, there can be gaps between scores, and multiple members held in the same sorted set can share the same score:
zadd can accept the following options, which you must enter after the key name and before the first member score:
NX or XX: These options have opposite effects, so you can only include one of them in any zadd operation:
NX: Tells zadd not to update existing members.
With this option, zadd will only add new elements.
XX: Tells zadd to only update existing elements.
With this option, zadd will never add new members.
CH: Normally, zadd only returns the number of new elements added to the sorted set. With this option included, though, zadd will return the number changed elements.
This includes newly added members and members whose scores were changed.
INCR: This causes the command to increment the member's score value.
If the member doesn't yet exist, the command will add it to the sorted set with the increment as its score, as if its original score was 0. With INCR included, the zadd will return the member "s new score if it's successful.
Note that you can only include one score / member pair at a time when using this option.
Instead of passing the INCR option to zadd, you can instead use the zincrby command which behaves the exact same way.
Instead of giving the sorted set member the value indicated by the score value like zadd, it increments that member's score up by that value.
For example, the following command increments the score of the member "Stephen Malkmus", which was originally 4, up by 5 to 9.
As is the case with the zadd command's INCR option, if the specified member doesn't exist then zincrby will create it with the increment value as its score.
Retrieving Members from Sorted Sets
The most fundamental way to retrieve the members held within a sorted set is to use the zrange command.
This command accepts as arguments the name of the key whose members you want to retrieve and a range of members held within it. The range is defined by two numbers that represent zero-based indexes, meaning that 0 represents the first member in the sorted set (or, the member with the lowest score), 1 represents the next, and so on.
The following example will return the first four members from the faveGuitarists sorted set created in the previous section:
Note that if the sorted set you pass to zrange has two or more elements that share the same score, it will sort those elements in lexicographical, or alphabetical, order.
The start and stop indexes can also be negative numbers, with -1 representing the last member, -2 representing the second to last, and so on:
zrange can accept the WITHSCORES argument which, when included, will also return the members' scores:
zrange can only return a range of members in ascending numerical order.
To reverse this and return a range in descending order, you must use the zrevrange command.
Think of this command as temporarily reversing the order of the given sorted set before returning the members that fall within the specified range.
So with zrevrange, 0 will represent the last member held in the key, 1 will represent the second to last, and so on:
zrevrange can also accept the WITHSCORES option.
You can return a range of members based on their scores with the zrangebyscore command.
In the following example, the command will return any member held in the faveGuitarists key with a score of 2, 3, or 4:
The range is inclusive in this example, meaning that it will return members with scores of 2 or 4. You can exclude either end of the range by preceding it with an open parenthesis (().
The following example will return every member with a score greater than or equal to 2, but less than 4:
As with zrange, zrangebyscore can accept the WITHSCORES argument.
It also accepts the LIMIT option, which you can use to retrieve only a selection of elements from the zrangebyscore output.
This option accepts an offset, which marks the first member in the range that the command will return, and a count, which defines how many members the command will return in total.
For example, the following command will look at the first six members of the faveGuitarists sorted set but will only return 3 members from it, starting from the second member in the range, represented by 1:
The zrevrangebyscore command returns a reversed range of members based on their scores.
The following command returns every member of the set with a score between 10 and 6:
As with zrangebyscore, zrevrangebyscore can accept both the WITHSCORES and LIMIT options.
Additionally, you can exclude either end of the range by preceding it with an open parenthesis.
There may be times when all the members in a sorted set have the same score.
In such a case, you can force redis to return a range of elements sorted lexicographically, or in alphabetical order, with the zrangebylex command.
To try out this command, run the following zadd command to create a sorted set where each member has the same score:
zrangebylex must be followed by the name of a key, a start interval, and a stop interval.
The start and stop intervals must begin with an open parenthesis (() or an open bracket ([), like this:
Notice that this example returned only four of the eight members in the set, even though the command sought a range from a to z. This is because Redis values are case-sensitive, so the members that begin with uppercase letters were excluded from its output.
To return those, you could run the following:
zrangebylex also accepts the special characters -, which represents negative infinity, and +, which represents positive infinity.
Thus, the following command syntax will also return every member of the sorted set:
Note that zrangebylex cannot return sorted set members in reverse lexicographical (ascending alphabetical) order.
To do that, use zrevrangebylex:
Because it's intended for use with sorted sets where every member has the same score, zrangebylex does not accept the WITHSCORES option.
It does, however, accept the LIMIT option.
Retrieving Information about Sorted Sets
To find out how many members are in a given sorted set (or, in other words, to determine its cardinality), use the zcard command.
The following example shows how many members are held in the faveGuitarists key from the first section of this guide:
zcount can tell you how many elements are held within a given sorted set that fall within a range of scores.
The first number following the key is the start of the range and the second one is the end of the range:
zscore outputs the score of a specified member of a sorted set:
If either the specified member or key don't exist, zscore will return (nil).
zrank is similar to zscore, but instead of returning the given member's score, it instead returns its rank.
In Redis, a rank is a zero-based index of the members of a sorted set, ordered by their score.
For example, "Joe Pass" has a score of 1, but because that is the lowest score of any member in the key, it has a rank of 0:
There's another Redis command called zrevrank which performs the same function as zrank, but instead reverses the ranks of the members in the set. In the following example, the member "Joe Pass" has the lowest score, and consequently has the highest reversed rank:
The only relation between a member's score and their rank is where their score stands in relation to those of other members.
If there is a score gap between two sequential members, that won't be reflected in their rank.
Note that if two members have the same score, the one that comes first alphabetically will have the lower rank.
Like zscore, zrank and zrevrank will return (nil) if the key or member doesn't exist.
zlexcount can tell you how many members are held in a sorted set between a lexicographical range.
The following example uses the SomervilleSquares sorted set from the previous section:
This command follows the same syntax as the zrangebylex command, so refer to the previous section for details on how to define a string range.
Removing Members from Sorted Sets
The zrem command can remove one or more members from a sorted set:
zrem will return an integer indicating how many members it removed from the sorted set:
There are three Redis commands that allow you to remove members of a sorted set based on a range.
For example, if each member in a sorted set has the same score, you can remove members based on a lexicographical range with zremrangebylex.
This command uses the same syntax as zrangebylex.
The following example will remove every member that begins with a capital letter from the SomervilleSquares key created in the previous section:
zremrangebylex will output an integer indicating how many members it removed:
You can also remove members based on a range of scores with the zremrangebyscore command, which uses the same syntax as the zrangebyscore command.
The following example will remove every member held in faveGuitarists with a score of 4, 5, or 6:
You can remove members from a set based on a range of ranks with the zremrangebyrank command, which uses the same syntax as zrangebyrank.
The following command will remove the three members of the sorted set with the lowest rankings, which are defined by a range of zero-based indexes:
Note that numbers passed to remrangebyrank can also be negative, with -1 representing the highest rank, -2 the next highest, and so on.
Creating New Sorted Sets from Existing Ones
Redis includes two commands that allow you to compare members of multiple sorted sets and create new ones based on those comparisons: zinterstore and zunionstore.
To experiment with these commands, run the following zadd commands to create some example sorted sets.
zinterstore finds the members shared by two or more sorted sets - their intersection - and produces a new sorted set containing only those members.
This command must include, in order, the name of a destination key where the intersecting members will be stored as a sorted set, the number of keys being passed to zinterstore, and the names of the keys you want to analyze:
zinterstore will return an integer showing the number of elements stored to the destination sorted set. Because NewKids and Nsync only share one member, "Joey", the command will return 1:
Be aware that if the destination key already exists, zinterstore will overwrite its contents.
zunionstore will create a new sorted set holding every member of the keys passed to it. This command uses the same syntax as zinterstore, and requires the name of a destination key, the number of keys being passed to the command, and the names of the keys:
Like zinterstore, zunionstore will return an integer showing the number of elements stored in the destination key.
Even though both of the original sorted sets held five members, because sorted sets can't have repeating members and each key has one member named "Joey", the resulting integer will be 9:
Like zinterstore, zunionstore will overwrite the contents of the destination key if it already exists.
To give you more control over member scores when creating new sorted sets with zinterstore and zunionstore, both of these commands accept the WEIGHTS and AGGREGATE options.
The WEIGHTS option is followed by one number for every sorted set included in the command which weight, or multiply, the scores of each member.
The first number after the WEIGHTS option weights the scores of the first key passed to the command, the second number weights the second key, and so on.
The following example creates a new sorted set holding the intersecting keys from the NewKids and Nsync sorted sets.
It weights the scores in the NewKids key by a factor of three, and weights those in the Nsync key by a factor of seven:
If the WEIGHTS option isn't included, the weighting defaults to 1 for both zinterstore and zunionstore.
AGGREGATE accepts three sub-options.
The first of these, SUM, implements zinterstore and zunionstore's default behavior by adding the scores of matching members in the combined sets.
If you run a zinterstore or zunionstore operation on two sorted sets that share one member, but this member has a different score in each set, you can force the operation to assign the lower of the two scores in the new set with the MIN suboption.
Because the two sorted sets only have one matching member with the same score (3), this command will create a new set with a member that has the lower of the two weighted scores:
Likewise, AGGREGATE can force zinterstore or zunionstore to assign the higher of the two scores with the MAX option:
This command creates a new set with on one member, "Joey", that has the higher of the two weighted scores:
It can be helpful to think of WEIGHTS as a way to temporarily manipulate members' scores before they're analyzed.
Likewise, it's helpful to think of the AGGREGATE option as a way to decide how to control members' scores before they're added to their new sets.
This guide details a number of commands used to create and manage sorted sets in Redis.
If there are other related commands, arguments, or procedures you'd like to see outlined in this guide, please ask or make suggestions in the comments below.
For more information on Redis commands, see our tutorial series on How to Manage a Redis Database.
How To Use Node.js Modules with npm and package.json
3209
Because of such features as its speedy Input / Output (I / O) performance and its well-known JavaScript syntax, Node.js has quickly become a popular runtime environment for back-end web development.
But as interest grows, larger applications are built, and managing the complexity of the codebase and its dependencies becomes more difficult.
Node.js organizes this complexity using modules, which are any single JavaScript files containing functions or objects that can be used by other programs or modules.
A collection of one or more modules is commonly referred to as a package, and these packages are themselves organized by package managers.
The Node.js Package Manager (npm) is the default and most popular package manager in the Node.js ecosystem, and is primarily used to install and manage external modules in a Node.js project.
It is also commonly used to install a wide range of CLI tools and run project scripts. npm tracks the modules installed in a project with the package.json file, which resides in a project's directory and contains:
All the modules needed for a project and their installed versions
All the metadata for a project, such as the author, the license, etc.
Scripts that can be run to automate tasks within the project
As you create more complex Node.js projects, managing your metadata and dependencies with the package.json file will provide you with more predictable builds, since all external dependencies are kept the same.
The file will keep track of this information automatically; while you may change the file directly to update your project's metadata, you will seldom need to interact with it directly to manage modules.
In this tutorial, you will manage packages with npm.
The first step will be to create and understand the package.json file.
You will then use it to keep track of all the modules you install in your project.
Finally, you will list your package dependencies, update your packages, uninstall your packages, and perform an audit to find security flaws in your packages.
Step 1 - Creating a package.json File
We begin this tutorial by setting up the example project - a fictional Node.js locator module that gets the user's IP address and returns the country of origin.
You will not be coding the module in this tutorial.
However, the packages you manage would be relevant if you were developing it.
First, you will create a package.json file to store useful metadata about the project and help you manage the project's dependent Node.js modules.
As the suffix suggests, this is a JSON (JavaScript Object Notation) file.
JSON is a standard format used for sharing, based on JavaScript objects and consisting of data stored as key-value pairs.
If you would like to learn more about JSON, read our Introduction to JSON article.
Since a package.json file contains numerous properties, it can be cumbersome to create manually, without copy and pasting a template from somewhere else.
To make things easier, npm provides the init command.
This is an interactive command that asks you a series of questions and creates a package.json file based on your answers.
Using the init Command
First, set up a project so you can practice managing modules.
In your shell, create a new folder called locator:
Then move into the new folder:
Now, initialize the interactive prompt by entering:
< $> note Note: If your code will use Git for version control, create the Git repository first and then run npm init.
The command automatically understands that it is in a Git-enabled folder.
If a Git remote is set, it automatically fills out the repository, bugs, and homepage fields for your package.json file.
If you initialized the repo after creating the package.json file, you will have to add this information in yourself.
For more on Git version control, see our Introduction to Git: Installation, Usage, and Branches series.
You will first be prompted for the name of your new project.
By default, the command assumes it's the name of the folder you're in.
Default values for each property are shown in parentheses ().
Since the default value for name will work for this tutorial, press ENTER to accept it.
The next value to enter is version.
Along with the name, this field is required if your project will be shared with others in the npm package repository.
< $> note Note: Node.js packages are expected to follow the Semantic Versioning (semver) guide.
Therefore, the first number will be the MAJOR version number that only changes when the API changes.
The last number will be the PATCH version that changes when bugs are fixed.
Press ENTER so the default version is accepted.
The next field is description - a useful string to explain what your Node.js module does.
Our fictional locator project would get the user's IP address and return the country of origin.
A fitting description would be Finds the country of origin of the incoming request, so type in something like this and press ENTER.
The description is very useful when people are searching for your module.
The following prompt will ask you for the entry point.
If someone installs and requires your module, what you set in the entry point will be the first part of your program that is loaded.
The value needs to be the relative location of a JavaScript file, and will be added to the main property of the package.json.
Press ENTER to keep the default value.
< $> note Note: Most modules have an index.js file as the main point of entry.
This is the default value for a package.json's main property, which is the point of entry for npm modules.
If there is no package.json, Node.js will try to load index.js by default.
Next, you'll be asked for a test command, an executable script or command to run your project tests.
In many popular Node.js modules, tests are written and executed with Mocha, Jest, Jasmine, or other test frameworks.
Since testing is beyond the scope of this article, leave this option empty for now, and press ENTER to move on.
The init command will then ask for the project's GitHub Repository.
You won't use this in this example, so leave it empty as well.
After the repository prompt, the command asks for keywords.
This property is an array of strings with useful terms that people can use to find your repository.
It's best to have a small set of words that are really relevant to your project, so that searching can be more targeted.
List these keywords as a string with commas separating each value.
For this sample project, type ip, geo, country at the prompt.
The finished package.json will have three items in the array for keywords.
The next field in the prompt is author.
This is useful for users of your module who want to get in contact with you.
For example, if someone discovers an exploit in your module, they can use this to report the problem so that you can fix it. The author field is a string in the following format: "< ^ > Name < ^ >\ < < ^ > Email < ^ >\ > (< ^ > Website < ^ >)".
For example, "Sammy\ < sammy @ your _ domain\ > (https: / / your _ domain)" is a valid author.
The email and website data are optional - a valid author could just be a name.
Add your contact details as an author and confirm with ENTER.
Finally, you'll be prompted for the license.
This determines the legal permissions and limitations users will have while using your module.
Many Node.js modules are open source, so npm sets the default to ISC.
At this point, you would review your licensing options and decide what's best for your project.
For more information on different types of open source licenses, see this license list from the Open Source Initiative.
If you do not want to provide a license for a private repository, you can type UNLICENSED at the prompt.
For this sample, use the default ISC license, and press ENTER to finish this process.
The init command will now display the package.json file it's going to create.
It will look similar to this:
Once the information matches what you see here, press ENTER to complete this process and create the package.json file.
With this file, you can keep a record of modules you install for your project.
Now that you have your package.json file, you can test out installing modules in the next step.
Step 2 - Installing Modules
It is common in software development to use external libraries to perform ancillary tasks in projects.
This allows the developer to focus on the business logic and create the application more quickly and efficiently.
For example, if our sample locator module has to make an external API request to get geographical data, we could use an HTTP library to make that task easier.
Since our main goal is to return pertinent geographical data to the user, we could install a package that makes HTTP requests easier for us instead of rewriting this code for ourselves, a task that is beyond the scope of our project.
Let's run through this example.
In your locator application, you will use the axios library, which will help you make HTTP requests.
Install it by entering the following in your shell:
You begin this command with npm install, which will install the package (for brevity you can use npm i).
You then list the packages that you want installed, separated by a space.
In this case, this is axios.
Finally, you end the command with the optional --save parameter, which specifies that axios will be saved as a project dependency.
When the library is installed, you will see output similar to the following:
Now, open the package.json file, using a text editor of your choice.
You'll see a new property, as highlighted in the following:
The --save option told npm to update the package.json with the module and version that was just installed.
This is great, as other developers working on your projects can easily see what external dependencies are needed.
< $> note Note: You may have noticed the ^ before the version number for the axios dependency.
Recall that semantic versioning consists of three digits: MAJOR, MINOR, and PATCH.
The ^ symbol signifies that any higher MINOR or PATCH version would satisfy this version constraint.
If you see ~ at the beginning of a version number, then only higher PATCH versions satisfy the constraint.
When you are finished reviewing package.json, exit the file.
Development Dependencies
Packages that are used for the development of a project but not for building or running it in production are called development dependencies.
They are not necessary for your module or application to work in production, but may be helpful while writing the code.
For example, it's common for developers to use code linters to ensure their code follows best practices and to keep the style consistent.
While this is useful for development, this only adds to the size of the distributable without providing a tangible benefit when deployed in production.
Install a linter as a development dependency for your project.
Try this out in your shell:
In this command, you used the --save-dev flag.
This will save eslint as a dependency that is only needed for development.
Notice also that you added @ 6.0.0 to your dependency name.
When modules are updated, they are tagged with a version.
The @ tells npm to look for a specific tag of the module you are installing.
Without a specified tag, npm installs the latest tagged version.
Open package.json again:
eslint has been saved as a devDependencies, along with the version number you specified earlier.
Exit package.json.
Automatically Generated Files: node _ modules and package-lock.json
When you first install a package to a Node.js project, npm automatically creates the node _ modules folder to store the modules needed for your project and the package-lock.json file that you examined earlier.
Confirm these are in your working directory.
In your shell, type ls and press ENTER.
You will observe the following output:
The node _ modules folder contains every installed dependency for your project.
In most cases, you should not commit this folder into your version controlled repository.
As you install more dependencies, the size of this folder will quickly grow.
Furthermore, the package-lock.json file keeps a record of the exact versions installed in a more succinct way, so including node _ modules is not necessary.
While the package.json file lists dependencies that tell us the suitable versions that should be installed for the project, the package-lock.json file keeps track of all changes in package.json or node _ modules and tells us the exact version of the package installed.
You usually commit this to your version controlled repository instead of node _ modules, as it's a cleaner representation of all your dependencies.
Installing from package.json
With your package.json and package-lock.json files, you can quickly set up the same project dependencies before you start development on a new project.
To demonstrate this, move up a level in your directory tree and create a new folder named cloned _ locator in the same directory level as locator:
Move into your new directory:
Now copy the package.json and package-lock.json files from locator to cloned _ locator:
To install the required modules for this project, type:
npm will check for a package-lock.json file to install the modules.
If no lock file is available, it would read from the package.json file to determine the installations.
It is usually quicker to install from package-lock.json, since the lock file contains the exact version of modules and their dependencies, meaning npm does not have to spend time figuring out a suitable version to install.
When deploying to production, you may want to skip the development dependencies.
Recall that development dependencies are stored in the devDependencies section of package.json, and have no impact on the running of your app. When installing modules as part of the CI / CD process to deploy your application, omit the dev dependencies by running:
The --production flag ignores the devDependencies section during installation.
For now, stick with your development build.
Before moving to the next section, return to the locator folder:
Global Installations
So far, you have been installing npm modules for the locator project. npm also allows you to install packages globally.
This means that the package is available to your user in the wider system, like any other shell command.
This ability is useful for the many Node.js modules that are CLI tools.
For example, you may want to blog about the locator project that you're currently working on.
To do so, you can use a library like Hexo to create and manage your static website blog.
Install the Hexo CLI globally like this:
To install a package globally, you append the -g flag to the command.
< $> note Note: If you get a permission error trying to install this package globally, your system may require super user privileges to run the command.
Try again with sudo npm i hexo-cli -g.
Test that the package was successfully installed by typing:
So far, you have learned how to install modules with npm.
You can install packages to a project locally, either as a production or development dependency.
You can also install packages based on pre-existing package.json or package-lock.json files, allowing you to develop with the same dependencies as your peers.
Finally, you can use the -g flag to install packages globally, so you can access them regardless of whether you're in a Node.js project or not.
Now that you can install modules, in the next section you will practice techniques to administer your dependencies.
Step 3 - Managing Modules
A complete package manager can do a lot more than install modules. npm has over 20 commands relating to dependency management available.
In this step, you will:
List modules you have installed.
Update modules to a more recent version.
Uninstall modules you no longer need.
Perform a security audit on your modules to find and fix security flaws.
While these examples will be done in your locator folder, all of these commands can be run globally by appending the -g flag at the end of them, exactly like you did when installing globally.
Listing Modules
If you would like to know which modules are installed in a project, it would be easier to use the list or ls command instead of reading the package.json directly.
By default, ls shows the entire dependency tree - the modules your project depends on and the modules that your dependencies depend on.
This can be a bit unwieldy if you want a high-level overview of what's installed.
To only print the modules you installed without their dependencies, enter the following in your shell:
The --depth option allows you to specify what level of the dependency tree you want to see.
When it's 0, you only see your top level dependencies.
Updating Modules
It is a good practice to keep your npm modules up to date.
This improves your likelihood of getting the latest security fixes for a module.
Use the outdated command to check if any modules can be updated:
You will get output like the following:
This command first lists the Package that's installed and the Current version.
The Wanted column shows which version satisfies your version requirement in package.json.
The Latest column shows the most recent version of the module that was published.
The Location column states where in the dependency tree the package is located.
The outdated command has the --depth flag like ls.
By default, the depth is 0.
It seems that you can update eslint to a more recent version.
Use the update or up command like this:
The output of the command will contain the version installed:
If you wanted to update all modules at once, then you would enter:
Uninstalling Modules
The npm uninstall command can remove modules from your projects.
This means the module will no longer be installed in the node _ modules folder, nor will it be seen in your package.json and package-lock.json files.
Removing dependencies from a project is a normal activity in the software development lifecycle.
A dependency may not solve the problem as advertised, or may not provide a satisfactory development experience.
In these cases, it may better to uninstall the dependency and build your own module.
Imagine that axios does not provide the development experience you would have liked for making HTTP requests.
Uninstall axios with the uninstall or un command by entering:
It doesn't explicitly say that axios was removed.
To verify that it was uninstalled, list the dependencies once again:
Now, we only see that eslint is installed:
This shows that you have successfully uninstalled the axios package.
Auditing Modules
npm provides an audit command to highlight potential security risks in your dependencies.
To see the audit in action, install an outdated version of the request module by running the following:
When you install this outdated version of request, you'll notice output similar to the following:
npm is telling you that you have vulnerabilities in your dependencies.
To get more details, audit your entire project with:
The audit command shows tables of output highlighting security flaws:
You can see the path of the vulnerability, and sometimes npm offers ways for you to fix it. You can run the update command as suggested, or you can run the fix subcommand of audit.
In your shell, enter:
You will see similar output to:
npm was able to safely update two of the packages, decreasing your vulnerabilities by the same amount.
However, you still have four vulnerabilities in your dependencies.
The audit fix command does not always fix every problem.
Although a version of a module may have a security vulnerability, if you update it to a version with a different API then it could break code higher up in the dependency tree.
You can use the --force parameter to ensure the vulnerabilities are gone, like this:
As mentioned before, this is not recommended unless you are sure that it won't break functionality.
In this tutorial, you went through various exercises to demonstrate how Node.js modules are organized into packages, and how these packages are managed by npm.
In a Node.js project, you used npm packages as dependencies by creating and maintaining a package.json file - a record of your project's metadata, including what modules you installed.
You also used the npm CLI tool to install, update, and remove modules, in addition to listing the dependency tree for your projects and checking and updating modules that are outdated.
In the future, leveraging existing code by using modules will speed up development time, as you don't have to repeat functionality.
You will also be able to create your own npm modules, and these will in turn will be managed by others via npm commands.
As for next steps, experiment with what you learned in this tutorial by installing and testing the variety of packages out there.
See what the ecosystem provides to make problem solving easier.
For example, you could try out TypeScript, a superset of JavaScript, or turn your website into mobile apps with Cordova.
If you'd like to learn more about Node.js, see our other Node.js tutorials.
How To Host a Website with Caddy on Ubuntu 18.04
3942
Caddy is a web server designed around simplicity and security that comes with a number of features that are useful for hosting websites.
For example, it can automatically obtain and manage TLS certificates from Let's Encrypt to enable HTTPS, and includes support for HTTP / 2.
HTTPS is a system for securing traffic between your users and your server, and is quickly becoming a basic expectation of any website running in production - without it, Chrome and Firefox will warn that your website is "Not Secure" if users try to submit login information.
Previously, the recommended method for installing Caddy was to download pre-built binaries from the Caddy project website.
However, changes in how Caddy's licensing works means that you're no longer allowed to use these pre-built binaries for commercial purposes unless you pay a license fee, even if you're just using Caddy internally within a business.
Luckily, the Caddy source code is still fully open-source and you can build Caddy yourself to avoid running into licensing issues.
In this tutorial, you'll build Caddy from source and use it to host a website secured with HTTPS.
This entails compiling it, configuring it using a Caddyfile and installing plugins.
In the end, you'll learn how to upgrade your installation when a new version is released.
The Go language toolchain installed on your server.
Follow our guide on How To Install Go and Set Up a Local Programming Environment on Ubuntu 18.04 to set up Go.
You don't need to create any example projects.
Step 1 - Building Caddy
In this step, you'll build Caddy from source with the ability to later add plugins, all without changing Caddy's source code.
For the purposes of this tutorial, you'll store the source code under ~ / caddy.
Create that directory by running the following command:
You'll store the source code for running and customizing Caddy in a file named caddy.go.
This code imports Caddy directly from Github (using Git) and starts it from the entrance main function.
If you wish to enable telemetry, uncomment the caddymain.EnableTelemetry line and set the value to true.
For caddy.go to be able to use the imported dependencies, you'll need to initialize it as a module:
At this point, you're all set to build the stock version of Caddy from the above source code by running:
There will be a lot of output, detailing what libraries Go downloaded as dependencies necessary for compiling.
The resulting executable is stored under $GOPATH / bin, as explained in the prerequisites.
When it finishes, try running Caddy:
This means that Caddy started successfully, and is available on the 2015 port. You can ignore the warning message, because that limit will be adjusted in later steps without your intervention.
To exit, press CTRL + C.
You have now built and executed Caddy.
In the next step, you'll install Caddy as a service so that it starts automatically at boot, and then adjust its ownership and permissions settings to ensure the server's security.
Step 2 - Installing Caddy
Now that you've verified you're able to build and run Caddy, it's time to configure a systemd service so that Caddy can be launched automatically on system startup.
To understand more about systemd, visit our Systemd Essentials tutorial.
To begin, move the Caddy binary to / usr / local / bin, the standard location for binaries that are not managed by Ubuntu's package manager and aren't key to system operation:
Next, change ownership of the Caddy binary over to the root user:
This will prevent other accounts from modifying the executable.
However, even while the root user will own Caddy, it's advised to execute it only using other, non-root accounts present on the system.
This makes sure that in the event of Caddy (or another program) being compromised, the attacker won't be able to modify the binary, or execute commands as root.
Next, set the binary file's permissions to 755 - this gives root full read / write / execute permissions for the file, while other users will only be able to read and execute it:
Since the Caddy process will not be running as root, Linux will prevent it from binding to ports 80 and 443 (the standard ports for HTTP and HTTPS, respectively), as these are privileged operations.
In order to be easily accessible at your domain, Caddy needs to be bound to one of these ports, depending on the protocol.
Otherwise, you would need to add a specific port number to the domain URL in your browser to view the content it will serve.
To allow Caddy to bind to low ports without running as root, run the following command:
The setcap utility sets file capabilities.
In this command it assigns the CAP _ NET _ BIND _ SERVICE capability to the Caddy binary, which allows an executable to bind to a port lower than 1024.
You have now finished setting up the Caddy binary, and are ready to start writing Caddy configuration.
Create a directory where you'll store Caddy's configuration files by running the following command:
Then, set the correct user and group permissions for it:
Setting the user as root and the group as www-data ensures that Caddy will have read and write access to the folder (via the www-data group) and that only the superuser account will have the same rights to read and modify. www-data is the default user and group for web servers on Ubuntu.
In a later step, you'll enable automatic TLS certificate provisioning from Let's Encrypt.
In preparation for that, make a directory to store any TLS certificates that Caddy will obtain and give it the same ownership rules as the / etc / caddy directory:
Caddy must be able to write certificates to this directory and read from it in order to encrypt requests.
For this reason, modify the permissions for the / etc / ssl / caddy directory so that it's only accessible by root and www-data:
Next, create a directory to store the files that Caddy will host:
Then, set the directory's owner and group to www-data:
Caddy reads its configuration from a file called Caddyfile, stored under / etc / caddy.
Create the file on disk by running:
To install the Caddy service, download the systemd unit file from the Caddy Github repository to / etc / systemd / system by running:
Modify the service file's permissions so it can only be modified by its owner, root:
Then, reload systemd to detect the Caddy service:
Check whether systemd has detected the Caddy service by running systemctl status:
If you see this same output, then the new service was correctly detected by systemd.
As part of the initial server setup prerequisite, you enabled ufw, the uncomplicated firewall, and allowed SSH connections.
For Caddy to be able to serve HTTP and HTTPS traffic from your server, you'll need to allow them in ufw by running the following command:
Use ufw status to check whether your changes worked:
Your installation of Caddy is now complete, but it isn't configured to serve anything.
In the next step, you'll configure Caddy to serve files from the / var / www directory.
Step 3 - Configuring Caddy
In this section, you'll write basic Caddy configuration for serving static files from your server.
Start by creating a basic HTML file in / var / www, called index.html:
When shown in a web browser, this file will display a heading with the text This page is being served via Caddy.
Open the Caddyfile configuration file you created earlier for editing:
This is a basic Caddy config, and declares that the port 80 of your server should be served with files from / var / www and compressed using gzip to reduce page loading times on the client side.
In a majority of cases, Caddy allows you to further customize the config directives.
For instance, you can limit gzip compression to only HTML and PHP files and set the compression level to 6 (1 being the lowest and 9 being the highest) by extending the directive with curly braces and listing sub-directives underneath:
Caddy has a huge number of different directives for many use cases.
For example, the fastcgi directive could be useful for enabling PHP.
The markdown directive could be used to automatically convert Markdown files to HTML before serving them, which could be useful for creating a simple blog.
To test that everything is working correctly, start the Caddy service:
Next, run systemctl status to find information about the status of the Caddy service:
You'll see the following:
You can now browse to your server's IP in a web browser.
Your sample web page will display:
Message from Caddy
You have now configured Caddy to serve static files from your server.
In the next step, you'll extend Caddy's functionality through the use of plugins.
Step 4 - Using Plugins
Plugins offer a way of changing and extending Caddy's behavior.
Generally, they offer more config directives for you to use, according to your use case.
In this section, you'll add and use plugins by installing the minify plugin, which removes excess whitespace and tidies up the code that will be sent to the client, further reducing footprint and loading times.
The minify plugin's GitHub repository is hacdias / caddy-minify.
Navigate to the directory with the source code you created in step one:
To add a plugin to Caddy, you'll need to import it in the caddy.go file you used to build Caddy.
Open caddy.go for editing:
Import the minify plugin by adding the highlighted line, like so:
Some plugins may require some slight configuration tweaks, so be sure to read the documentation for whatever ones you install.
You can find a list of popular plugins in the left pane of the Caddy documentation, under Plugins.
Any time you add a new plugin, you have to rebuild Caddy.
This is because Go is a compiled programming language, meaning the source code is transformed into machine code before execution.
Your change to the import declaration has altered the source code, but won't affect the binary until it's compiled.
Use the go install command to compile Caddy:
When it finishes, move the generated binary to / usr / local / bin and set up permissions for the binary like you did previously.
You must take these steps every time you rebuild Caddy to ensure its functionality and security:
To start using the minify plugin, you'll need to add the minify directive to your Caddyfile.
Enable the plugin by adding the following line to the configuration block:
Now, restart your server using systemctl:
Caddy is now running and will minify any files that it serves, including the index.html file you created earlier.
You can observe the 'minification' at work by fetching the contents of your domain using curl:
You'll see the following output.
Notice that all unnecessary whitespace has been removed, showing that the minify plugin is working.
In this step, you learned how to extend Caddy with plugins.
Next, you'll enable HTTPS by installing the tls.dns.digitalocean plugin.
Step 5 - Enabling Automatic TLS with Let's Encrypt
In this section, you'll enable automatic Let's Encrypt certificate provisioning and renewal, using TXT DNS records for verification.
To verify using TXT DNS records, you'll install a plugin for interfacing with the DigitalOcean API, called tls.dns.digitalocean.
The procedure for installing it is almost identical to how you installed the minify plugin in the previous step.
To begin, open caddy.go:
Add the plugin's repository to imports:
Compile it by running:
Ensure Caddy is stopped via systemctl, then finish installing the plugin by copying the newly built Caddy binary and once more setting its ownership and permissions:
Next, configure Caddy to work with DigitalOcean's API to set DNS records.
Caddy needs to access this token as an environment variable to configure DigitalOcean's DNS, so you'll edit its systemd unit file:
Find the line beginning with Environment = in the [Service] section.
This line defines the environment variables that should be passed to the Caddy process.
Add a space at the end of this line, then add a DO _ AUTH _ TOKEN variable, followed by the token you just generated:
Save and close this file, then reload the systemd daemon as you did earlier to ensure the configuration is updated:
Run systemctl status to check that your configuration changes were OK:
You'll need to make a couple of slight changes to your Caddyfile, so open it up for editing:
Add the highlighted lines to the Caddyfile, making sure to replace < ^ > your _ domain < ^ > with your domain (instead of just the port: 80) and commenting gzip:
Using a domain rather than just a port for the hostname will cause Caddy to serve requests over HTTPS.
The tls directive configures Caddy's behavior when using TLS, and the dns subdirective specifies that Caddy should use the DNS-01 system, rather than HTTP-01.
With this, your website is ready to be deployed.
Start Caddy with systemctl and then enable it, so it runs on boot:
If you browse to your domain, you'll automatically be redirected to HTTPS, with the same message shown.
Your installation of Caddy is now complete and secured, and you can further customize according to your use case.
If you want to update Caddy when a new version comes out, you'll need to update the go.mod file (stored in the same directory), which looks like this:
The highlighted part is the version of Caddy you are using.
When a new version is released on Github (see the release tags page), you can replace the existing one in go.mod with it and compile Caddy according to the first two steps.
You can do the same for all imported plugins.
You now have Caddy installed and configured on your server, serving static pages at your desired domain, secured with free Let's Encrypt TLS certificates.
A good next step would be to find a way of being notified when new versions of Caddy are released.
For example, you could use the Atom feed for Caddy releases, or a dedicated service such as dependencies.io.
You can explore Caddy's documentation for more information on configuring Caddy.
How To Install and Configure an SNMP Daemon and Client on Ubuntu 18.04
3940
The author selected the Internet Archive to receive a donation as part of the Write for DOnations program.
A large part of being a system administrator is collecting accurate information about your servers and infrastructure.
There are a number of tools and options for gathering and processing this type of information.
Many of them are built upon a technology called SNMP.
SNMP stands for simple network management protocol.
It is a way that servers can share information about their current state, and also a channel through which an administer can modify pre-defined values.
While the protocol itself is lightweight, the structure of programs that implement SNMP can quickly grow in complexity.
For more information on the basics of the SNMP protocol, see our An Introduction to SNMP article.
In this guide, you will set up the tools to communicate using SNMP.
You will be using two Ubuntu 18.04 servers to demonstrate.
One will contain the SNMP manager, which will talk to the agent to implement network devices.
This will be called the manager server.
The other server will have the SNMP agent, which will act on the orders from the manager server.
This will be called the agent server.
You could choose to install the agent on the manager machine as well, but keeping them separate makes it easier to demonstrate what functionality is provided by each component.
Two Ubuntu 18.04 servers set up by following the Initial Server Setup Guide for Ubuntu 18.04, including a non-root user with sudo privileges and a firewall configured with ufw.
Step 1 - Installing the SNMP Daemon and Utilities
You can begin to explore how SNMP can be implemented on a system by installing the daemon and tools on your Ubuntu servers.
From your local machine, log into the manager server as your non-root user:
Update the package index for the APT package manager:
Next, install the SNMP software:
The snmp package provides a collection of command line tools for issuing SNMP requests to agents.
The snmp-mibs-downloader package will help to install and manage Management Information Base (MIB) files, which keep track of network objects.
Then, open up a new terminal on your local machine and log into the agent server:
On the agent server, update the package index:
Then, install the SNMP daemon
Note that you do not need the snmp-mibs-downloader package, since the agent server will not be managing MIB files.
Now that you have installed these components, you will configure your manager server.
Step 2 - Configuring the SNMP Manager Server
As mentioned before, most of the bulk of the work happens in the agent server, so your configuration on the manager server will be less involved.
You just need to modify one file to make sure that SNMP tools can use the extra MIB data you installed.
On your manager server, open the / etc / snmp / snmp.conf file in your text editor with sudo privileges.
In this file, there are a few comments and a single un-commented line.
To allow the manager to import the MIB files, comment out the mibs: line:
Save and close snmp.conf by pressing CTRL + X, followed by Y, and then ENTER if you're using nano.
You are now finished configuring the manager server, but you will still need to use this server to help configure your agent server, which you will do in the next step.
Step 3 - Configuring the SNMP Agent Server
As a true client-server system, the agent server does not have any of the external tools needed to configure its own SNMP setup.
You can modify some configuration files to make some changes, but most of the changes you need to make will be done by connecting to your agent server from your management server.
In this tutorial, you will use version 3 of the SNMP protocol.
Unlike SNMPv1 and v2, in SNMPv3 each message contains security parameters that are encoded.
In this step you will configure SNMPv3 authentication and access control rules.
To get started, on your agent server, open the daemon's configuration file with sudo privileges:
Inside, you will have to make a few changes.
These will mainly be used to bootstrap your configuration so that you can manage it from your other server.
First, you need to change the agentAddress directive.
Currently, it is set to only allow connections originating from the local computer.
You'll need to comment out the current line, and uncomment the line underneath, which allows all connections.
< $> note Note: Since allowing all connections like this is not a security best practice, it is best to make sure to lock this back down soon, after the bootstraping is complete.
Next, you will temporarily insert a createUser line.
These directives are not normally kept in this file; you will be removing it again in a moment.
The user you are creating will be called bootstrap and will be used as a template in which to create your first actual user.
The SNMP packages do this through a process of cloning the user's properties.
When defining a new user, you must specify the authentication type (MD5 or SHA) as well as supply a passphrase that must be at least eight characters.
If you plan on using encryption for the transfer, like you will in this tutorial, you also must specify the privacy protocol (DES or AES) and optionally a privacy protocol passphrase.
If no privacy protocol passphrase is supplied, the authentication passphrase will be used for the privacy protocol as well.
Add this createUser line to the end of the file:
Now that you have a new user specified, you can set up the level of access that this user will have.
In this tutorial you will set this up for your bootstrap user, and also for the new user you will be creating, called demo.
You will allow them read and write access by using the rwuser directive (the alternative is rouser for read-only access).
You will also enforce the use of encryption by specifying priv after your user.
If you wanted to restrict the user to a specific part of the MIB, you could specify the highest-level object identifier (OID) that the user should have access to at the end of the line.
For this tutorial's purposes, both of your lines will be as follows:
When you are finished making these changes, save and close the file.
To implement these changes, restart the snmpd service on your agent server:
The SNMP daemon will listen for connections on port: 161.
Configure UFW to allow connections from the manager server to this port:
You can learn more about UFW in How To Set Up a Firewall with UFW on Ubuntu 18.04.
Now that the agent server is configured, you can connect to your agent server from the manager server to verify the connection.
Step 4 - Verifying Authentication to the Agent Server
In this step, you will test to make sure you can connect with your bootstrap account to the agent server.
Before that, however, this tutorial will talk a bit about the general structure of sending an SNMP command.
When using the suite of tools included in the snmp package (the net-snmp software suite), there are a few patterns in the way you must call the commands.
The first thing to do is authenticate with the SNMP daemon that you wish to communicate with.
This usually involves supplying a few pieces of information.
The common ones are as follows:
-v: This flag is used to specify the version of the SNMP protocol that you would like to use.
This tutorial will be using v3.
-c: This flag is used if you are using SNMP v1 or v2-style community strings for authentication.
Since you are using v3-style user-based authentication, you don't need to do this.
-u: This parameter is used to specify the username that you wish to authenticate as.
To read or modify anything using SNMP, you must authenticate with a known username.
-l: This is used to specify the security level that you are connecting with.
The possible values are noAuthNoPriv for no authentication and no encryption, authNoPriv for authentication but no encryption, and authPriv for authentication and encryption.
The username that you are using must be configured to operate at the security level you specify, or else the authentication will not succeed.
-a: This parameter is used to specify the authentication protocol that is used.
The possible values are MD5 or SHA.
This must match the information that was specified when the user was created.
-x: This parameter is used to specify the encryption protocol that is used.
The possible values are DES or AES.
This is necessary whenever the user's privilege specification has priv after it, making encryption mandatory.
-A: This is used to give the authentication passphrase that was specified when the user was created.
-X: This is the encryption passphrase that was specified when the user was created.
If none was specified but an encryption algorithm was given, the authentication passphrase will be used.
This is required when the -x parameter is given or whenever a user's privilege specification has a priv after it, requiring encryption.
Using this information, you can construct your commands.
Given how you set up your bootstrap user, the commands you will be using with that account will look like this:
From your manager server, test to make sure your bootstrap account is available.
Type the following to display the system information for the agent server:
The 1.3.6.1.2.1.1.1.0 string is the OID that is responsible for displaying system information.
It will return the output of uname -a on the remote system.
Now that you have verified that you can authenticate to the server running the SNMP daemon, you can continue on to create your regular user account.
Step 5 - Setting Up the Regular User Account
Although you have specified the privileges for the demo user account in the snmpd.conf file, you haven't actually created this user yet.
In this step, you are going to use the bootstrap user as a template for your new user.
You will do this using the snmpusm tool, which is used for user management.
On the manager server, you can create the user from the template using the snmpusm tool and the following general syntax:
Using what you know about the authentication flags you need to pass, and leveraging the user account you already have (bootstrap), you can make a user that fits the user privileges you have already defined (demo).
The command will look like this:
You will receive the following message:
You now have a fully functioning user called demo on your agent server.
However, it is still using the same authentication information as the bootstrap account.
To increase security, you can change the password to something else.
This time, you will use the demo account to authenticate.
Remember, passwords must be at least eight characters long:
You will receive the following message back:
You can test your new credentials and password by asking the agent server how long the SNMP service has been running.
You will use the snmpget command to get a single value from the agent server.
This time, take advantage of the extra MIB definitions you downloaded to ask for the value by name instead of the OID numeric ID.
You will get back a value that represents the last time that the remote SNMP daemon was restarted:
You now have a working user account named demo.
In the next step, you will simplify working with SNMP commands by configuring the client.
Step 6 - Creating a Client Configuration File
You have probably noticed by this point that the authentication details for all of your SNMP commands will be fairly static with each request.
Rather than typing these in each time, you can create a client-side configuration file that will contain the credentials you are connecting with.
The client configuration file can be placed in two different locations depending on how wide-spread you wish to share it.
If you want to share your login credentials with any valid user on your management machine, you can place your configuration details into the global snmp.conf file on the manager server.
You would need to open that file with sudo privileges:
If, however, you want to define the authentication credentials for your user alone, you can create a hidden .snmp directory within your user's home directory on the manager server, and create the file there:
Regardless of your decision on where to place your configuration, the contents will be the same.
The commands that you will be using to authenticate are in the following table.
In the right-hand column, you can see the directive names used to set those configuration details within the snmp.conf file:
Command Flag
Translated snmp.conf directive
-u < ^ > username < ^ >
The SNMPv3 username to authenticate as.
defSecurityName < ^ > username < ^ >
-l authPriv
The security level to authenticate with.
defSecurityLevel authPriv
-a MD5
The authentication protocol to use.
defAuthType MD5
-x DES
The privacy (encryption) protocol to use.
defPrivType DES
-A < ^ > passphrase < ^ >
The authentication passphrase for the supplied username.
defAuthPassphrase < ^ > passphrase < ^ >
-X < ^ > passphrase < ^ >
The privacy passphrase from the supplied username.
defPrivPassphrase < ^ > passphrase < ^ >
Using this information, you can construct an appropriate snmp.conf file.
For this guide, it will look like this:
Now, you can issue commands without supplying the authentication details.
You will only need the SNMP command, the host, and the command arguments.
Instead of typing:
You can type:
As you can see, this significantly reduces the amount of information you need to supply in each request.
Next, you will remove the bootstrap account to tighten the network security.
Step 7 - Removing the Bootstrap Account
Now that your regular account is configured correctly, you can remove the insecure bootstrap account.
On your agent server, open the / etc / snmp / snmpd.conf file again with sudo privileges.
Find and comment out (or remove) both of the lines that you previously added that reference the bootstrap user:
Now, restart the SNMP daemon:
This will fulfill the recommendation of not having createUser directives in the normal snmpd.conf file.
It will also remove privileges from that temporary user.
If you want to completely remove the bootstrap user from the usmUserTable, you can do so by issuing this command from the manager server:
You will receive the following response:
At this point, you have a fully configured client-server setup that can communicate securely using the SNMP protocol.
You can now add additional daemons on other hosts and configure account access across your entire infrastructure.
For further study, you can use our How To Use the Net-SNMP Tool Suite To Manage and Monitor Servers tutorial to learn about SNMP tools and how to use them to retrieve values one-by-one or by bulk and how to modify data.
How To Record and Share Terminal Sessions Using Terminalizer on Ubuntu 18.04
3939
Terminalizer is a terminal recorder application that allows you to record your terminal session in real-time, and then play it back at a later date.
It works in the same way as a desktop screen recorder, but instead runs in your terminal.
Recording your terminal session is useful if you want to review a particular activity again, or to help debug a particularly tricky error.
Recordings made with Terminalizer can also be exported as animated GIFs, which are great for sharing online or adding to marketing material for your software.
In this tutorial, you will install Terminalizer, use it to record and play back terminal sessions, customize your recordings, and then export them to share online.
An Ubuntu 18.04 server set up by following the Initial Server Setup with Ubuntu 18.04, including a sudo non-root user.
Node.js and npm, which can be installed by following the Installing the Distro-Stable Version for Ubuntu section within How to Install Node.js on Ubuntu 18.04.
If you wish to share your recordings online, you'll also need:
A free account on the Terminalizer website.
Step 1 - Installing Terminalizer
In this step, you will download and install Terminalizer on your system.
Terminalizer is written using Node.js, and is available to install using the npm package manager.
To install Terminalizer globally on your system, run the following command:
Terminalizer uses the Electron application framework to export recorded terminal sessions into GIF format.
The --unsafe-perms = true command argument is required in order to install Electron globally on your system.
Once Terminalizer has been installed, you'll see similar output to the following:
Next, check your installation of Terminalizer by running:
This will display something similar to the following:
Finally, generate a default Terminalizer configuration file, which you can use for Terminalizer's advanced customization (detailed further in Step 4):
Now that you've installed Terminalizer, you can make your first terminal recording.
Step 2 - Recording and Playing Back a Terminal Session
In this step, you will record and playback a terminal session.
To begin, set up a new Terminalizer recording using a name of your choice:
This will output the following to indicate that the recording has started:
You can now proceed to do anything that you want within your terminal.
Each key press and command will be recorded in real-time by Terminalizer.
When you'd like to stop the recording, press CTRL + D.
Terminalizer will then save the recording to the specified file in YAML format, for example, < ^ > your-recording < ^ > .yml.
You may be prompted by Terminalizer to share your recording online.
Just press CTRL + C to cancel this for now, as you can playback the terminal recording locally first.
Next, play your recorded terminal session with the followng command:
This will replay the recorded session in real-time in your terminal:
You can also adjust the playback speed of your recording using the --speed-factor option.
For example, the following will playback your recording twice as slowly (half speed):
Alternatively, you can play back your recording twice as fast (double speed):
You've recorded and played back a terminal session.
Next, you can share a recorded terminal session online.
Step 3 - Sharing a Recorded Terminal Session
In this step, you'll share your recorded terminal session online on the Terminalizer Explore page.
Begin by selecting a recorded session to share:
You will then be prompted to provide some basic metadata about your recording, such as the title and description:
< $> warning Warning: Terminalizer recordings are shared publicly by default, so ensure that there are no personally identifiable or confidential details present in your terminal recording that you don't want to share.
If this is the first time that you've shared a recorded session using Terminalizer, you'll need to link your Terminalizer account.
Terminalizer will display a verification link if this is required:
< $> warning Warning: Ensure that you keep your Terminalizer token private, as it will allow anyone in possession of it to access your Terminalizer account.
Once you have visited the link in your web browser and signed in to your Terminalizer account, press any key to continue.
Terminalizer will now upload your recording and provide you the link to view it:
Visiting the link in a desktop web browser will allow you to view your shared recording:
A screenshot of the Terminalizer website, showing an example of a shared terminal recording
You've shared a recorded terminal session on the Terminalizer website and viewed it in your web browser.
Step 4 - Setting Advanced Terminalizer Configuration
Now that you've gained some familiarity with Terminalizer, you can begin to review some of the more advanced customization options, such as the ability to adjust the display colors and style.
Each recording inherits the default configuration from the global Terminalizer config file, which is located at ~ / .terminalizer / config.yml.
This means that you can edit the configuration for individual recordings directly by editing the recording file (e.g. < ^ > your-recording < ^ > .yml).
Alternatively, you can edit the global configuration, which will have an impact on all new recordings.
In this example you'll edit the global configuration file, but the same guidance applies to individual recording configuration files as well.
Begin by opening the global Terminalizer configuration file in your text editor, such as nano:
Each of the available configuration options within the file are commented in order to explain what they do.
There are several common configuration options that you may wish to adjust to your liking:
cols: Explicitly set the number of terminal columns used for your recording.
rows: Explicitly set the number of terminal rows used for your recording.
frameDelay: Override the delay between each keystroke during playback.
maxIdleTime: Specify a maximum time between keystrokes during playback.
cursorStyle: Specify the default terminal cursor style out of block, bar, and underline.
fontFamily: Specify a list of preferred playback fonts, in order of preference.
theme: Adjust the color scheme of the playback, for example to create a black-on-white terminal, etc.
As an example, you can achieve a white-on-black terminal display by configuring the following options:
This will produce a result similar to the following:
A screenshot of the Terminalizer website, showing an example of a recording with a black-on-white theme
You could adjust the cursor style to make the recording easier to understand, for example by swapping the default block-style cursor with an underlined one:
A screenshot of the Terminalizer website, showing an example of a recording with an underline-style cursor
Once you have made any desired changes, save the file and return to your terminal.
If you edited the global Terminalizer configuration, these settings will apply to all new recordings going forward.
If you're editing a specific recording configuration, Terminalizer will immediately apply the changes to that particular recording.
Note that custom playback styling only applies to shared recording sessions.
Playing them back directly in your terminal will always use your default terminal styling and color scheme.
In this final step, you reviewed some of the advanced configuration options for Terminalizer.
In this article you used Terminalizer to record and share a terminal session.
You now have the knowledge required to create recorded demos of your software for use in marketing material, or to share command-line tricks with friends.
If you wish to render and export Terminalizer recordings into GIF format, you can install Terminalizer on a machine with a graphical user interface / desktop and use the built-in rendering features:
Create GIFs Using Terminalizer
You may also wish to browse the Terminalizer website to see recorded terminal sessions shared by other users:
Explore Terminalizer Recordings
How To Test a Node.js Module with Mocha and Assert
3930
Testing is an integral part of software development.
It's common for programmers to run code that tests their application as they make changes in order to confirm it's behaving as they'd like.
With the right test setup, this process can even be automated, saving a lot of time.
Running tests consistently after writing new code ensures that new changes don't break pre-existing features.
This gives the developer confidence in their code base, especially when it gets deployed to production so users can interact with it.
A test framework structures the way we create test cases.
Mocha is a popular JavaScript test framework that organizes our test cases and runs them for us.
However, Mocha does not verify our code's behavior.
To compare values in a test, we can use the Node.js assert module.
In this article, you'll write tests for a Node.js TODO list module.
You will set up and use the Mocha test framework to structure your tests.
Then you'll use the Node.js assert module to create the tests themselves.
In this sense, you will be using Mocha as a plan builder, and assert to implement the plan.
This tutorial uses Node.js version 10.16.0.
A basic knowledge of JavaScript, which you can find in our How To Code in JavaScript series.
Step 1 - Writing a Node Module
Let's begin this article by writing the Node.js module we'd like to test.
This module will manage a list of TODO items.
Using this module, we will be able to list all the TODOs that we are keeping track of, add new items, and mark some as complete.
Additionally, we'll be able to export a list of TODO items to a CSV file.
If you'd like a refresher on writing Node.js modules, you can read our article on How To Create a Node.js Module.
First, we need to set up the coding environment.
Create a folder with the name of your project in your terminal.
This tutorial will use the name < ^ > todos < ^ >:
Now initialize npm, since we'll be using its CLI functionality to run the tests later:
We only have one dependency, Mocha, which we will use to organize and run our tests.
To download and install Mocha, use the following:
We install Mocha as a dev dependency, as it's not required by the module in a production setting.
If you would like to learn more about Node.js packages or npm, check out our guide on How To Use Node.js Modules with npm and package.json.
Finally, let's create our file that will contain our module's code:
With that, we're ready to create our module.
Open index.js in a text editor like nano:
Let's begin by defining the Todos class.
This class contains all the functions that we need to manage our TODO list.
Add the following lines of code to index.js:
We begin the file by creating a Todos class.
Its constructor () function takes no arguments, therefore we don't need to provide any values to instantiate an object for this class.
All we do when we initialize a Todos object is create a todos property that's an empty array.
The modules line allows other Node.js modules to require our Todos class.
Without explicitly exporting the class, the test file that we will create later would not be able to use it.
Let's add a function to return the array of todos we have stored.
Write in the following highlighted lines:
Our list () function returns a copy of the array that's used by the class.
It makes a copy of the array by using JavaScript's destructuring syntax.
We make a copy of the array so that changes the user makes to the array returned by list () does not affect the array used by the Todos object.
< $> note Note: JavaScript arrays are reference types.
This means that for any variable assignment to an array or function invocation with an array as a parameter, JavaScript refers to the original array that was created.
For example, if we have an array with three items called x, and create a new variable y such that y = x, y and x both refer to the same thing.
Any changes we make to the array with y impacts variable x and vice versa.
Now let's write the add () function, which adds a new TODO item:
Our add () function takes a string, and places it in a new JavaScript object's title property.
The new object also has a completed property, which is set to false by default.
We then add this new object to our array of TODOs.
Important functionality in a TODO manager is to mark items as completed.
For this implementation, we will loop through our todos array to find the TODO item the user is searching for.
If one is found, we'll mark it as completed.
If none is found, we'll throw an error.
Add the complete () function like this:
We now have a basic TODO manager that we can experiment with.
Next, let's manually test our code to see if the application is working.
Step 2 - Manually Testing the Code
In this step, we will run our code's functions and observe the output to ensure it matches our expectations.
This is called manual testing.
It's likely the most common testing methodology programmers apply.
Although we will automate our testing later with Mocha, we will first manually test our code to give a better sense of how manual testing differs from testing frameworks.
Let's add two TODO items to our app and mark one as complete.
You will see the > prompt in the REPL that tells us we can enter JavaScript code.
Type the following at the prompt:
With require (), we load the TODOs module into a Todos variable.
Recall that our module returns the Todos class by default.
Now, let's instantiate an object for that class.
In the REPL, add this line of code:
We can use the todos object to verify our implementation works.
Let's add our first TODO item:
So far we have not seen any output in our terminal.
Let's verify that we've stored our "run code" TODO item by getting a list of all our TODOs:
You will see this output in your REPL:
This is the expected result: We have one TODO item in our array of TODOs, and it's not completed by default.
Let's add another TODO item:
Mark the first TODO item as completed:
Our todos object will now be managing two items: "run code" and "test everything".
The "run code" TODO will be completed as well.
Let's confirm this by calling list () once again:
Now, exit the REPL with the following:
We've confirmed that our module behaves as we expect it to.
While we didn't put our code in a test file or use a testing library, we did test our code manually.
Unfortunately, this form of testing becomes time consuming to do every time we make a change.
Next, let's use automated testing in Node.js and see if we can solve this problem with the Mocha testing framework.
Step 3 - Writing Your First Test with Mocha and Assert
In the last step, we manually tested our application.
This will work for individual use cases, but as our module scales, this method becomes less viable.
As we test new features, we must be certain that the added functionality has not created problems in the old functionality.
We would like to test each feature over again for every change in the code, but doing this by hand would take a lot of effort and would be prone to error.
A more efficient practice would be to set up automated tests.
These are scripted tests written like any other code block.
We run our functions with defined inputs and inspect their effects to ensure they behave as we expect.
As our codebase grows, so will our automated tests.
When we write new tests alongside the features, we can verify the entire module still works - all without having to remember how to use each function every time.
In this tutorial, we're using the Mocha testing framework with the Node.js assert module.
Let's get some hands-on experience to see how they work together.
To begin, create a new file to store our test code:
Now use your preferred text editor to open the test file.
You can use nano like before:
In the first line of the text file, we will load the TODOs module like we did in the Node.js shell.
We will then load the assert module for when we write our tests.
The strict property of the assert module will allow us to use special equality tests that are recommended by Node.js and are good for future-proofing, since they account for more use cases.
Before we go into writing tests, let's discuss how Mocha organizes our code.
Tests structured in Mocha usually follow this template:
Notice two key functions: describe () and it ().
The describe () function is used to group similar tests.
It's not required for Mocha to run tests, but grouping tests make our test code easier to maintain.
It's recommended that you group your tests in a way that's easy for you to update similar ones together.
The it () contains our test code.
This is where we would interact with our module's functions and use the assert library.
Many it () functions can be defined in a describe () function.
Our goal in this section is to use Mocha and assert to automate our manual test.
We'll do this step-by-step, beginning with our describe block.
Add the following to your file after the module lines:
With this code block, we've created a grouping for our integrated tests.
Unit tests would test one function at a time.
Integration tests verify how well functions within or across modules work together.
When Mocha runs our test, all the tests within that describe block will run under the "integration test" group.
Let's add an it () function so we can begin testing our module's code:
Notice how descriptive we made the test's name.
If anyone runs our test, it will be immediately clear what's passing or failing.
A well-tested application is typically a well-documented application, and tests can sometimes be an effective kind of documentation.
For our first test, we will create a new Todos object and verify it has no items in it:
The first new line of code instantiated a new Todos object as we would do in the Node.js REPL or another module.
In the second new line, we use the assert module.
From the assert module we use the notStrictEqual () method.
This function takes two parameters: the value that we want to test (called the actual value) and the value we expect to get (called the expected value).
If both arguments are the same, notStrictEqual () throws an error to fail the test.
Save and exit from index.test.js.
The base case will be true as the length should be 0, which isn't 1. Let's confirm this by running Mocha.
To do this, we need to modify our package.json file.
Open your package.json file with your text editor:
Now, in your scripts property, change it so it looks like this:
We have just changed the behavior of npm's CLI test command.
When we run npm test, npm will review the command we just entered in package.json.
It will look for the Mocha library in our node _ modules folder and run the mocha command with our test file.
Save and exit package.json.
Let's see what happens when we run our test.
In your terminal, enter:
The command will produce the following output:
This output first shows us which group of tests it is about to run.
For every individual test within a group, the test case is indented.
We see our test name as we described it in the it () function.
The tick at the left side of the test case indicates that the test passed.
At the bottom, we get a summary of all our tests.
In our case, our one test is passing and was completed in 16ms (the time varies from computer to computer).
Our testing has started with success.
However, this current test case can allow for false-positives.
A false-positive is a test case that passes when it should fail.
We currently check that the length of the array is not equal to 1. Let's modify the test so that this condition holds true when it should not.
Add the following lines to index.test.js:
We added two TODO items.
Let's run the test to see what happens:
This passes as expected, as the length is greater than 1. However, it defeats the original purpose of having that first test.
The first test is meant to confirm that we start on a blank state.
A better test will confirm that in all cases.
Let's change the test so it only passes if we have absolutely no TODOs in store.
Make the following changes to index.test.js:
You changed notStrictEqual () to strictEqual (), a function that checks for equality between its actual and expected argument.
Strict equal will fail if our arguments are not exactly the same.
Save and exit, then run the test so we can see what happens:
This time, the output will show an error:
This text will be useful for us to debug why the test failed.
Notice that since the test failed there was no tick at the beginning of the test case.
Our test summary is no longer at the bottom of the output, but right after our list of test cases were displayed:
The remaining output provides us with data about our failing tests.
First, we see what test case has failed:
Then, we see why our test failed:
An AssertionError is thrown when strictEqual () fails.
We see that the expected value, 0, is different from the actual value, 2.
We then see the line in our test file where the code fails.
In this case, it's line 10.
Now, we've seen for ourselves that our test will fail if we expect incorrect values.
Let's change our test case back to its right value.
Then take out the todos.add lines so that your code looks like the following:
Run it once more to confirm that it passes without any potential false-positives:
We've now improved our test's resiliency quite a bit.
Let's move forward with our integration test.
The next step is to add a new TODO item to index.test.js:
After using the add () function, we confirm that we now have one TODO being managed by our todos object with strictEqual ().
Our next test confirms the data in the todos with deepStrictEqual ().
The deepStrictEqual () function recursively tests that our expected and actual objects have the same properties.
In this case, it tests that the arrays we expect both have a JavaScript object within them.
It then checks that their JavaScript objects have the same properties, that is, that both their title properties are "run code" and both their completed properties are false.
We then complete the remaining tests using these two equality checks as needed by adding the following highlighted lines:
Our test now mimics our manual test.
With these programmatic tests, we don't need to check the output continuously if our tests pass when we run them.
You typically want to test every aspect of use to make sure the code is tested properly.
Let's run our test with npm test once more to get this familiar output:
You've now set up an integrated test with the Mocha framework and the assert library.
Let's consider a situation where we've shared our module with some other developers and they're now giving us feedback.
A good portion of our users would like the complete () function to return an error if no TODOs were added as of yet.
Let's add this functionality in our complete () function.
Open index.js in your text editor:
Add the following to the function:
Now let's add a new test for this new feature.
We want to verify that if we call complete on a Todos object that has no items, it will return our special error.
Go back into index.test.js:
At the end of the file, add the following code:
We use describe () and it () like before.
Our test begins with creating a new todos object.
We then define the error we are expecting to receive when we call the complete () function.
Next, we use the throws () function of the assert module.
This function was created so we can verify the errors that are thrown in our code.
Its first argument is a function that contains the code that throws the error.
The second argument is the error we are expecting to receive.
In your terminal, run the tests with npm test once again and you will now see the following output:
This output highlights the benefit of why we do automated testing with Mocha and assert.
Because our tests are scripted, every time we run npm test, we verify that all our tests are passing.
We did not need to manually check if the other code is still working; we know that it is because the test we have still passed.
So far, our tests have verified the results of synchronous code.
Let's see how we would need to adapt our newfound testing habits to work with asynchronous code.
Step 4 - Testing Asynchronous Code
One of the features we want in our TODO module is a CSV export feature.
This will print all the TODOs we have in store along with the completed status to a file.
This requires that we use the fs module - a built-in Node.js module for working with the file system.
Writing to a file is an asynchronous operation.
There are many ways to write to a file in Node.js.
We can use callbacks, Promises, or the async / await keywords.
In this section, we'll look at how we write tests for those different methods.
Callbacks
A callback function is one used as an argument to an asynchronous function.
It is called when the asynchronous operation is completed.
Let's add a function to our Todos class called saveToFile ().
This function will build a string by looping through all our TODO items and writing that string to a file.
Open your index.js file:
In this file, add the following highlighted code:
We first have to import the fs module in our file.
Then we added our new saveToFile () function.
Our function takes a callback function that will be used once the file write operation is complete.
In that function, we create a fileContents variable that stores the entire string we want to be saved as a file.
It's initialized with the CSV's headers.
We then loop through each TODO item with the internal array's forEach () method.
As we iterate, we add the title and completed properties of the individual todos objects.
Finally, we use the fs module to write the file with the writeFile () function.
Our first argument is the file name: todos.csv.
The second is the contents of the file, in this case, our fileContents variable.
Our last argument is our callback function, which handles any file writing errors.
Let's now write a test for our saveToFile () function.
Our test will do two things: confirm that the file exists in the first place, and then verify that it has the right contents.
Open the index.test.js file:
let's begin by loading the fs module at the top of the file, as we'll use it to help test our results:
Now, at the end of the file let's add our new test case:
Like before, we use describe () to group our test separately from the others as it involves new functionality.
The it () function is slightly different from our other ones.
Usually, the callback function we use has no arguments.
This time, we have done as an argument.
We need this argument when testing functions with callbacks.
The done () callback function is used by Mocha to tell it when an asynchronous function is completed.
All callback functions being tested in Mocha must call the done () callback.
If not, Mocha would never know when the function was complete and would be stuck waiting for a signal.
Continuing, we create our Todos instance and add a single item to it. We then call the saveToFile () function, with a callback that captures a file writing error.
Note how our test for this function resides in the callback.
If our test code was outside the callback, it would fail as long as the code was called before the file writing completed.
In our callback function, we first check that our file exists:
The fs.existsSync () function returns true if the file path in its argument exists, false otherwise.
< $> note Note: The fs module's functions are asynchronous by default.
However, for key functions, they made synchronous counterparts.
This test is simpler by using synchronous functions, as we don't have to nest the asynchronous code to ensure it works.
In the fs module, synchronous functions usually end with "Sync" at the end of their names.
We then create a variable to store our expected value:
We use readFileSync () of the fs module to read the file synchronously:
We now provide readFileSync () with the right path for the file: todos.csv.
As readFileSync () returns a Buffer object, which stores binary data, we use its toString () method so we can compare its value with the string we expect to have saved.
Like before, we use the assert module's strictEqual to do a comparison:
We end our test by calling the done () callback, ensuring that Mocha knows to stop testing that case:
We provide the err object to done () so Mocha can fail the test in the case an error occurred.
Let's run this test with npm test like before.
Your console will display this output:
You've now tested your first asynchronous function with Mocha using callbacks.
But at the time of writing this tutorial, Promises are more prevalent than callbacks in new Node.js code, as explained in our How To Write Asynchronous Code in Node.js article.
Next, let's learn how we can test them with Mocha as well.
Promises
A Promise is a JavaScript object that will eventually return a value.
When a Promise is successful, it is resolved.
When it encounters an error, it is rejected.
Let's modify the saveToFile () function so that it uses Promises instead of callbacks.
Open up index.js:
First, we need to change how the fs module is loaded.
In your index.js file, change the require () statement at the top of the file to look like this:
We just imported the fs module that uses Promises instead of callbacks.
Now, we need to make some changes to saveToFile () so that it works with Promises instead.
In your text editor, make the following changes to the saveToFile () function to remove the callbacks:
The first difference is that our function no longer accepts any arguments.
With Promises we don't need a callback function.
The second change concerns how the file is written.
We now return the result of the writeFile () promise.
Save and close out of index.js.
Let's now adapt our test so that it works with Promises.
Open up index.test.js:
Change the saveToFile () test to this:
The first change we need to make is to remove the done () callback from its arguments.
If Mocha passes the done () argument, it needs to be called or it will throw an error like this:
When testing Promises, do not include the done () callback in it ().
To test our promise, we need to put our assertion code in the then () function.
Notice that we return this promise in the test, and we don't have a catch () function to catch when the Promise is rejected.
We return the promise so that any errors that are thrown in the then () function are bubbled up to the it () function.
If the errors are not bubbled up, Mocha will not fail the test case.
When testing Promises, you need to use return on the Promise being tested.
If not, you run the risk of getting a false-positive.
We also omit the catch () clause because Mocha can detect when a promise is rejected.
If rejected, it automatically fails the test.
Now that we have our test in place, save and exit the file, then run Mocha with npm test and to confirm we get a successful result:
We've changed our code and test to use Promises, and now we know for sure that it works.
But the most recent asynchronous patterns use async / await keywords so we don't have to create multiple then () functions to handle successful results.
Let's see how we can test with async / await.
async / await
The async / await keywords make working with Promises less verbose.
Once we define a function as asynchronous with the async keyword, we can get any future results in that function with the await keyword.
This way we can use Promises without having to use the then () or catch () functions.
We can simplify our saveToFile () test that's promise based with async / await.
In your text editor, make these minor edits to the saveToFile () test in index.test.js:
The first change is that the function used by the it () function now has the async keyword when it's defined.
This allows us to the use the await keyword inside its body.
The second change is found when we call saveToFile ().
The await keyword is used before it is called.
Now Node.js knows to wait until this function is resolved before continuing the test.
Our function code is easier to read now that we moved the code that was in the then () function to the it () function's body.
Running this code with npm test produces this output:
We can now test asynchronous functions using any of three asynchronous paradigms appropriately.
We have covered a lot of ground with testing synchronous and asynchronous code with Mocha.
Next, let's dive in a bit deeper to some other functionality that Mocha offers to improve our testing experience, particularly how hooks can change test environments.
Step 5 - Using Hooks to Improve Test Cases
Hooks are a useful feature of Mocha that allows us to configure the environment before and after a test.
We typically add hooks within a describe () function block, as they contain setup and teardown logic specific to some test cases.
Mocha provides four hooks that we can use in our tests:
before: This hook is run once before the first test begins.
beforeEach: This hook is run before every test case.
after: This hook is run once after the last test case is complete.
afterEach: This hook is run after every test case.
When we test a function or feature multiple times, hooks come in handy as they allow us to separate the test's setup code (like creating the todos object) from the test's assertion code.
To see the value of hooks, let's add more tests to our saveToFile () test block.
While we have confirmed that we can save our TODO items to a file, we only saved one item.
Furthermore, the item was not marked as completed.
Let's add more tests to be sure that the various aspects of our module works.
First, let's add a second test to confirm that our file is saved correctly when we have a completed a TODO item.
Open your index.test.js file in your text editor:
Change the last test to the following:
The test is similar to what we had before.
The key differences are that we call the complete () function before we call saveToFile (), and that our expectedFileContents now have true instead of false for the completed column's value.
Let's run our new test, and all the others, with npm test:
It works as expected.
There is, however, room for improvement.
They both have to instantiate a Todos object at the beginning of the test.
As we add more test cases, this quickly becomes repetitive and memory-wasteful.
Also, each time we run the test, it creates a file.
This can be mistaken for real output by someone less familiar with the module.
It would be nice if we cleaned up our output files after testing.
Let's make these improvements using test hooks.
We'll use the beforeEach () hook to set up our test fixture of TODO items.
A test fixture is any consistent state used in a test.
In our case, our test fixture is a new todos object that has one TODO item added to it already.
We will then use afterEach () to remove the file created by the test.
In index.test.js, make the following changes to your last test for saveToFile ():
Let's break down all the changes we've made.
We added a beforeEach () block to the test block:
These two lines of code create a new Todos object that will be available in each of our tests.
With Mocha, the this object in beforeEach () refers to the same this object in it (). this is the same for every code block inside the describe () block.
For more information on this, see our tutorial Understanding This, Bind, Call, and Apply in JavaScript.
This powerful context sharing is why we can quickly create test fixtures that work for both of our tests.
We then clean up our CSV file in the afterEach () function:
If our test failed, then it may not have created a file.
That's why we check if the file exists before we use the unlinkSync () function to delete it.
The remaining changes switch the reference from todos, which were previously created in the it () function, to this.todos which is available in the Mocha context.
We also deleted the lines that previously instantiated todos in the individual test cases.
Now, let's run this file to confirm our tests still work.
Enter npm test in your terminal to get:
The results are the same, and as a benefit, we have slightly reduced the setup time for new tests for the saveToFile () function and found a solution to the residual CSV file.
In this tutorial, you wrote a Node.js module to manage TODO items and tested the code manually using the Node.js REPL.
You then created a test file and used the Mocha framework to run automated tests.
With the assert module, you were able to verify that your code works.
You also tested synchronous and asynchronous functions with Mocha.
Finally, you created hooks with Mocha that make writing multiple related test cases much more readable and maintainable.
Equipped with this understanding, challenge yourself to write tests for new Node.js modules that you are creating.
Can you think about the inputs and outputs of your function and write your test before you write your code?
If you would like more information about the Mocha testing framework, check out the official Mocha documentation.
If you'd like to continue learning Node.js, you can return to the How To Code in Node.js series page.
A DigitalOcean Workshop Kit
3985
< $> note label Automating Server Setup with Ansible Workshop Kit Materials This workshop kit is designed to help a technical audience become familiar with configuration management concepts and how to use Ansible to automate server infrastructure setup.
The aim is to provide a complete set of resources for a speaker to host an event and deliver an introductory talk on Ansible.
It includes:
Slides and speaker notes including short demo videos and commands for running an optional live demo.
This talk runs for roughly 50 minutes.
A GitHub repository containing the demo app code and the necessary Ansible scripts to deploy that application to an Ubuntu server.
This tutorial, which walks a user through rolling out the Travellist demo Laravel application on a remote server.
This tutorial is intended to supplement the talk demo with additional detail and elucidation.
It also serves as a reference for readers seeking to deploy a Laravel application to a remote Ubuntu server using Ansible.
Configuration management tools such as Ansible are typically used to streamline the process of automating server setup by establishing standard procedures for new servers.
This has the benefit of reducing human error associated with manual setups.
This tutorial, designed to accompany the Slides and speaker notes for the Automating Server Setup with Ansible Workshop Kit, will show you how to set up an inventory file and execute a set of provisioning scripts to fully automate the process of setting up a remote LEMP server (Linux, (E) Nginx, MariaDB and PHP-FPM) on Ubuntu 18.04 and to deploy a demo Laravel application to this system.
< $> note Note: This material is intended to demonstrate how to use playbooks to automate server setup with Ansible.
Although our demo consists of a Laravel application running on a LEMP server, readers are encouraged to modify and adapt the included setup to suit their own needs.
Make sure the control node has a regular user with sudo permissions and a firewall enabled, as explained in our Initial Server Setup guide, and a set of valid SSH keys.
One or more Ansible Hosts: one or more remote Ubuntu 18.04 servers.
Each host must have the control node's public key added to its authorized _ keys file, as explained in Step 2 of the How to Set Up SSH Keys on Ubuntu 18.04 guide.
In case you are using DigitalOcean Droplets as nodes, you can use the control panel to add your public key to your Ansible hosts.
Step 1 - Cloning the Demo Repository
The first thing we need to do is clone the repository containing the Ansible provisioning scripts and the demo Laravel application that we'll deploy to the remote servers.
All the necessary files can be found at the do-community / ansible-laravel-demo Github repository.
After logging in to your Ansible control node as your sudo user, clone the repository and navigate to the directory created by the git command:
Now, you can run an ls command to inspect the contents of the cloned repository:
Here's an overview of each of these folders and files and what they are:
application /: This directory contains the demo Laravel application that is going to be deployed on the remote server by the end of the workshop.
group _ vars /: This directory holds variable files containing custom options for the application setup, such as database credentials and where to store the application files on the remote server.
roles /: This directory contains the different Ansible roles that handle the provisioning of an Ubuntu LEMP server.
inventory-example: This file can be used as a base to create a custom inventory for your infrastructure.
laravel-deploy.yml: This playbook will deploy the demo Laravel application to the remote server.
laravel-env.j2: This template is used by the laravel-deploy.yml playbook to set up the application environment file.
readme.md: This file contains general information about the provisioning contained in this repository.
server-setup.yml: This playbook will provision a LEMP server using the roles defined in the roles / directory.
Step 2 - Setting Up the Inventory File and Testing Connection to Nodes
We'll now create an inventory file to list the hosts we want to manage using Ansible.
First, copy the inventory-example file to a new file called hosts:
Now, use your text editor of choice to open the new inventory file and update it with your own servers.
The example inventory that comes with the workshop kit contains two Ansible groups: dev and production.
This is meant to demonstrate how to use group variables to customize deployment in multiple environments.
If you wish to test this setup with a single node, you can use either the dev or the production group and remove the other one from the inventory file.
< $> note Note: the ansible _ python _ interpreter variable defines the path to the Python executable on the remote host.
Here, we're telling Ansible to set this variable for all hosts in this inventory file.
If you are using nano, you can do that by hitting CTRL + X, then Y and ENTER to confirm.
Once you're done adjusting your inventory file, you can execute the ping Ansible module to test whether the control node is able to connect to the hosts:
Let's break down this command:
all: This option tells Ansible to run the following command on all hosts from the designated inventory file.
-i hosts: Specifies which inventory should be used.
When this option is not provided, Ansible will try to use the default inventory, which is typically located at / etc / ansible / hosts.
-m ping: This will execute the ping Ansible module, which will test connectivity to nodes and whether or not the Python executable can be found on the remote systems.
-u root: This option specifies which remote user should be used to connect to the nodes.
We're using the root account here as an example because this is typically the only account available on fresh new servers.
Other connection options might be necessary depending on your infrastructure provider and SSH configuration.
If your SSH connection to the nodes is properly set up, you'll get the following output:
The pong response means your control node is able to connect to your managed nodes, and that Ansible is able to execute Python commands on the remote hosts.
Step 3 - Setting Up Variable Files
Before running the playbooks that are included in this workshop kit, you'll first need to edit the variable file that contains settings such as the name of the remote user to create and the database credentials to set up with MariaDB.
Open the group _ vars / all file using your text editor of choice:
The variables that need your attention are:
remote _ user: The specified user will be created on the remote server and granted sudo privileges.
mysql _ root _ password: This variable defines the database root password for the MariaDB server.
Note that this should be a secure password of your own choosing.
mysql _ app _ db: The name of the database to create for the Laravel application.
You don't need to change this value, but you are free to do so if you wish.
This value will be used to set up the .env Laravel configuration file.
mysql _ app _ user: The name of the database user for the Laravel application.
Again, you are not required to change this value, but you are free to do so.
mysql _ app _ pass: The database password for the Laravel application.
This should be a secure password of your choosing.
http _ host: The domain name or IP address of the remote host.
Here, we're using an Ansible fact that contains the ipv4 address for the eth0 network interface.
In case you have domain names pointing to your remote hosts, you may want to create separate variable files for each of them, overwriting this value so that the Nginx configuration contains the correct hostname for each server.
When you are finished editing these values, save and close the file.
Creating additional variable files for multiple environments
If you've set up your inventory file with multiple nodes, you might want to create additional variable files to configure each node accordingly.
In our example inventory, we have created two distinct groups: dev and production.
To avoid having the same database credentials and other settings in both environments, we need to create a separate variable file to hold production values.
You might want to copy the default variable file and use it as base for your production values:
Because the all.yml file contains the default values that should be valid for all environments, you can remove all the variables that won't need changing from the new production.yml file.
The variables that you should update for each environment are highlighted here:
Notice that we've changed the app _ env value to prod and set the app _ debug value to false.
These are recommended Laravel settings for production environments.
Once you're finished customizing your production variables, save and close the file.
Encrypting variable files with Ansible Vault
If you plan on sharing your Ansible setup with other users, it is important to keep the database credentials and other sensitive data in your variable files safe.
This is possible with Ansible Vault, a feature that is included with Ansible by default.
Ansible Vault allows you to encrypt variable files so that only users with access to the vault password can view, edit or unencrypt these files.
The vault password is also necessary to run a playbook or a command that makes use of encrypted files.
To encrypt your production variable file, run:
You will be prompted to provide a vault password and confirm it. Once you're finished, if you check the contents of that file, you'll see that the data is now encrypted.
If you want to view the variable file without changing its contents, you can use the view command:
You will be prompted to provide the same password you defined when encrypting that file with ansible-vault.
After providing the password, the file's contents will appear in your terminal.
To exit the file view, type q.
To edit a file that was previously encrypted with Ansible Vault, use the edit vault command:
This command will prompt you to provide the vault password for that file.
Your default terminal editor will then be used to open the file for editing.
After making the desired changes, save and close the file, and it will be automatically encrypted again by Ansible Vault.
You have now finished setting up your variable files.
In the next step, we'll run the playbook to set up Nginx, PHP-FPM, and MariaDB (which, along with a Linux-based operating system like Ubuntu, form the LEMP stack) on your remote server (s).
Step 4 - Running the LEMP Playbook
Before deploying the demo Laravel app to the remote server (s), we need to set up a LEMP environment that will serve the application.
The server-setup.yml playbook includes the Ansible roles necessary to set this up.
To inspect its contents, run:
Here's an overview of all the roles included within this playbook:
setup: Contains the tasks necessary to create a new system user and grant them sudo privileges, as well as enabling the ufw firewall.
mariadb: Installs the MariaDB database server and creates the application database and user.
php: Installs php-fpm and PHP modules that are necessary in order to run a Laravel application.
nginx: Installs the Nginx web server and enables access on port 80.
composer: Installs Composer globally.
Notice that we've set up a few tags within each role.
This is to facilitate re-running only parts of this playbook, if necessary.
If you make changes to your Nginx template file, for instance, you might want to run only the Nginx role.
The following command will execute this playbook on all servers from your inventory file.
The --ask-vault-pass is only necessary in case you have used ansible-vault to encrypt variable files in the previous step:
Your node (s) are now ready to serve PHP applications using Nginx + PHP-FPM, with MariaDB as database server.
In the next step, we'll deploy the included demo Laravel app with the laravel-deploy.yml Ansible playbook.
Step 5 - Deploying the Laravel Application
Now that you have a working LEMP environment on your remote server (s), you can execute the laravel-deploy.yml playbook.
This playbook will execute the following tasks:
Create the application document root on the remote server, if it hasn't already been created.
Synchronize the local application folder to the remote server using the sync module.
Use the acl module to set permissions for the www-data user on the storage folder.
Set up the .env application file based on the laravel-env.j2 template.
Install application dependencies with Composer.
Generate application security key.
Set up a public link for the storage folder.
Run database migrations and seeders.
This playbook should be executed by a non-root user with sudo permissions.
This user should have been created when you executed the server-setup.yml playbook in the previous step, using the name defined by the remote _ user variable.
When you're ready, run the laravel-deploy.yml playbook with:
The --ask-vault-pass is only necessary in case you have used ansible-vault to encrypt variable files in the previous step.
When the execution is finished, you can access the demo application by pointing your browser to your node's domain name or IP address:
Laravel Travellist Demo
This tutorial demonstrates how to set up an Ansible inventory file and connect to remote nodes, and how to run Ansible playbooks to set up a LEMP server and deploy a Laravel demo application to it. This guide compliments the Automating Server Setup with Ansible Workshop Kit "s slides and speaker notes, and is accompanied by a demo GitHub repository containing all necessary files to follow up with the demo component of this workshop.
How To Create a URL Shortener with Django and GraphQL
3983
GraphQL is an API standard created and open-sourced by Facebook as an alternative to REST APIs.
As opposed to REST APIs, GraphQL uses a typed system to define its data structure, where all the information sent and received must be compliant to a pre-defined schema.
It also exposes a single endpoint for all communication instead of multiple URLs for different resources and solves the overfetching issue by returning only the data asked for by the client, thereby generating smaller and more concise responses.
In this tutorial you will create a backend for a URL shortener - a service that takes any URL and generates a shorter, more readable version - while diving into GraphQL concepts, like queries and mutations, and tools, like the GraphiQL interface.
You may already have used such services before, like bit.ly.
Since GraphQL is a language agnostic technology, it is implemented on top of various languages and frameworks.
Here, you will use the general purpose Python programming language, the Django web framework, and the Graphene-Django library as the GraphQL Python implementation with specific integrations for Django.
To continue with this tutorial, you'll need Python version 3.5 or higher installed on your development machine.
To install Python, follow our tutorial on How To Install and Set Up a Local Programming Environment for Python 3 for your OS.
Make sure to also create and start a virtual environment; to follow the lead of this tutorial, you can name your project directory < ^ > shorty < ^ >.
An entry-level knowledge of Django is desired, but not mandatory.
If you are curious, you can follow this Django Development series created by the DigitalOcean community.
Step 1 - Setting Up the Django Project
In this step, you will be installing all the necessary tools for the application and setting up your Django project.
Once you have created your project directory and started your virtual environment, as covered in the prerequisites, install the necessary packages using pip, the Python package manager.
This tutorial will install Django version 2.1.7 and Graphene-Django version 2.2.0 or higher:
You now have all the tools needed in your tool belt.
Next, you will create a Django project using the django-admin command.
A project is the default Django boilerplate - a set of folders and files with everything necessary to start the development of a web application.
In this case, you will call your project < ^ > shorty < ^ > and create it inside your current folder by specifying the. at the end:
After creating your project, you will run the Django migrations.
These files contain Python code generated by Django and are responsible for changing the application's structure according to the Django models.
Changes might include the creation of a table, for example.
By default, Django comes with its own set of migrations responsible for subsystems like Django Authentication, so it is necessary to execute them with the following command:
This command uses the Python interpreter to invoke a Django script called manage.py, responsible for managing different aspects of your project, like creating apps or running migrations.
Once Django's database is ready to go, start its local development server:
This command will take away the prompt in your terminal and start the server.
Visit the http: / / 127.0.0.1: 8000 page in your local browser.
You will see this page:
Django local server front page
To stop the server and return to your terminal, press CTRL + C.
Whenever you need to access the browser, make sure the preceding command is running.
Next, you will finish this step by enabling the Django-Graphene library in the project.
Django has the concept of app, a web application with a specific responsibility.
A project is composed of one or multiple apps.
For now, open the < ^ > shorty < ^ > / settings.py file in your text editor of choice.
This tutorial will be using vim:
The settings.py file manages all the settings in your project.
Inside it, search for the INSTALLED _ APPS entry and add the 'graphene _ django' line:
This addition tells Django that you will be using an app called graphene _ django, which you installed in Step 1.
At the bottom of the file, add the following variable:
This last variable points to your main Schema, which you will create later.
In GraphQL, a Schema contains all the object types, such as Resources, Queries, and Mutations.
Think of it as documentation representing all the data and functionality available in your system.
After the modifications, save and close the file.
Now you have configured the Django project.
In the next step, you will create a Django app and its Models.
Step 2 - Setting Up a Django App and Models
A Django platform is usually composed of one project and many applications or apps.
An app describes a set of features inside a project, and, if well-designed, can be reused across Django projects.
In this step, you will create an app called < ^ > shortener < ^ >, responsible for the actual URL shortening feature.
To create its basic skeleton, type the next command in your terminal:
Here you used the parameters startapp < ^ > app _ name < ^ >, instructing manage.py to create an app named shortener.
To finish the app creation, open the < ^ > shorty < ^ > / settings.py file
Add the app's name to the same INSTALLED _ APPS entry you modified before:
With your shortener added to < ^ > shorty < ^ > / settings.py, you can move on to creating the models for your project.
Models are one of the key features in Django.
They are used to represent a database in a "Pythonic" way, allowing you to manage, query, and store data using Python code.
Before opening the models.py file for changes, this tutorial will give an overview of the changes you will make.
Your model file - shortener / models.py - will contain the following content once you have replaced the existing code:
Here you will import the required packages needed by your code.
You will add the line from hashlib import md5 at the top to import the Python standard library that will be used to create a hash of the URL.
The from django.db import models line is a Django helper for creating models.
< $> warning Warning: This tutorial refers to hash as the result of a function that takes an input and always returns the same output.
This tutorial will be using the MD5 hash function for demonstration purposes.
Note that MD5 has collision issues and should be avoided in production.
Next, you will add a Model named URL with the following fields:
full _ url: the URL to be shortened.
url _ hash: a short hash representing the full URL.
clicks: how many times the short URL was accessed.
created _ at: the date and time at which the URL was created.
You will generate the url _ hash by applying the MD5 hash algorithm to the full _ url field and using just the first 10 characters returned during the Model's save () method, executed every time Django saves an entry to the database.
Additionally, URL shorteners usually track how many times a link was clicked.
You will achieve this by calling the method clicked () when the URL is visited by a user.
The operations mentioned will be added inside your URL model with this code:
Now that you've reviewed the code, open the shortener / models.py file:
Replace the code with the following content:
To apply these changes in the database, you will need to create the migrations by running the following command:
Then execute the migrations:
Now that you've set up the models, in the next step you will create the GraphQL endpoint and a Query.
Step 3 - Creating Queries
The REST architecture exposes different resources in different endpoints, each one containing a well-defined data structure.
For example, you may fetch a users' list at / api / users, always expecting the same fields.
GraphQL, on the other hand, has a single endpoint for all interactions, and uses Queries to access data. The main - and most valuable - difference is that you can use a Query to retrieve all your users within a single request.
Start by creating a Query to fetch all URLs.
You will need a couple of things:
A URL type, linked to your previously defined model.
A Query statement named urls.
A method to resolve your Query, meaning to fetch all URLs from the database and return them to the client.
Create a new file called shortener / schema.py:
Start by adding the Python import statements:
The first line imports the main graphene library, which contains the base GraphQL types, like List.
The DjangoObjectType is a helper to create a Schema definition from any Django model, and the third line imports your previously create URL model.
After that, create a new GraphQL type for the URL model by adding the following lines:
Finally, add these lines to create a Query type for the URL model:
This code creates a Query class with one field named urls, which is a list of the previously defined URLType.
When resolving the Query through the resolve _ urls method, you return all the URLs stored in the database.
The full shortener / schema.py file is shown here:
All the Queries must now be added to the main Schema.
Think of it as a holder for all your resources.
Create a new file in the < ^ > shorty < ^ > / schema.py path and open it with your editor:
Import the following Python packages by adding the following lines.
The first one, as already mentioned, contains the base GraphQL types.
The second line imports the previously created Schema file.
Next, add the main Query class.
It will hold, via inheritance, all the Queries and future operations created:
Lastly, create the schema variable:
The SCHEMA setting you defined in Step 2 points to the schema variable you've just created.
The full < ^ > shorty < ^ > / schema.py file is shown here:
Next, enable the GraphQL endpoint and the GraphiQL interface, which is a graphical web interface used to interact with the GraphQL system.
Open the < ^ > shorty < ^ > / urls.py file:
For learning purposes, delete the file contents and save it, so that you can start from scratch.
The first lines you will add are Python import statements:
The path function is used by Django to create an accessible URL for the GraphiQL interface.
Following it, you import the csrf _ exempt, which allows clients to send data to the server.
A complete explanation can be found in the Graphene Documentation.
In the last line, you imported the actual code responsible for the interface via GraphQLView.
Next, create a variable named urlpatterns.
This will stitch together all the code necessary to make the GraphiQL interface available in the graphql / path:
The full shortener / urls.py file is shown here:
Back in the terminal, run the python manage.py runserver command (if not running already):
Open your web browser at the http: / / localhost: 8000 / graphql address.
You will be presented with this screen:
GraphiQL interface
The GraphiQL is an interface where you can run GraphQL statements and see the results.
One feature is the Docs section on the top right.
Since everything in GraphQL is typed, you get free documentation about all your Types, Queries, Mutations, etc.
After exploring the page, insert your first Query on the main text area:
This content shows how a GraphQL Query is structured: First, you use the keyword query to tell the server that you only want some data back.
Next, you use the urls field defined in the shortener / schema.py file inside the Query class.
From that, you explicitly request all the fields defined in the URL model using camel case-style, which is the default for GraphQL.
Now, click on the play arrow button in the top left.
You will receive the following response, stating that you still have no URLs:
This shows that GraphQL is working.
In your terminal, press CTRL + C to stop your server.
You have accomplished a lot in this step, creating the GraphQL endpoint, making a Query to fetch all URLs, and enabling the GraphiQL interface.
Now, you will create Mutations to change the database.
Step 4 - Creating Mutations
The majority of applications have a way to change the database state by adding, updating, or deleting data. In GraphQL, these operations are called Mutations.
They look like Queries but use arguments to send data to the server.
To create your first Mutation, open shortener / schema.py:
At the end of the file, start by adding a new class named CreateURL:
This class inherits the graphene.Mutation helper to have the capabilities of a GraphQL Mutation.
It also has a property name url, defining the content returned by the server after the Mutation is completed.
In this case, it will be the URLType data structure.
Next, add a subclass named Arguments to the already defined class:
This defines what data will be accepted by the server.
Here, you are expecting a parameter named full _ url with a String content:
Now add the following lines to create the mutate method:
This mutate method does a lot of the work by receiving the data from the client and saving it to the database.
In the end, it returns the class itself containing the newly created item.
Lastly, create a Mutation class to hold all the Mutations for your app by adding these lines:
So far, you will only have one mutation named create _ url.
Close and save the file.
To finish adding the Mutation, change the < ^ > shorty < ^ > / schema.py file:
Alter the file to include the following highlighted code:
If you are not running the local server, start it:
Navigate to http: / / localhost: 8000 / graphql in your web browser.
Execute your first Mutation in the GraphiQL web interface by running the following statement:
You composed the Mutation with the createURL name, the fullUrl argument, and the data you want in the response defined inside the url field.
The output will contain the URL information you just created inside the GraphQL data field, as shown here:
With that, a URL was added to the database with its hashed version, as you can see in the urlHash field.
Try running the Query you created in the last Step to see its result:
The output will show the stored URL:
You can also try executing the same Query, but only asking for the fields you want.
Next, try it one more time with a different URL:
The system is now able to create short URLs and list them.
In the next step, you will enable users to access a URL by its short version, redirecting them to the correct page.
Step 5 - Creating the Access Endpoint
In this step, you will use Django Views - a method that takes a request and returns a response - to redirect anyone accessing the http: / / localhost: 8000 / < ^ > url _ hash < ^ > endpoint to its full URL.
Open the shortener / views.py file with your editor:
To start, import two packages by replacing the contents with the following lines:
These will be explained more thoroughly later on.
Next, you will create a Django View named root.
Add this code snippet responsible for the View at the end of your file:
This receives an argument called url _ hash from the URL requested by a user.
Inside the function, the first line tries to get the URL from the database using the url _ hash argument.
If not found, it returns the HTTP 404 error to the client, which means that the resource is missing.
Afterwards, it increments the clicked property of the URL entry, making sure to track how many times the URL is accessed.
At the end, it redirects the client to the requested URL.
The full shortener / views.py file is shown here:
Next, open < ^ > shorty < ^ > / urls.py:
Add the following highlighted code to enable the root View.
The root View will be accessible in the / path of your server, accepting a url _ hash as a string parameter.
If you are not running the local server, start it by executing the python manage.py runserver command.
To test your new addition, open your web browser and access the http: / / localhost: 8000 / 077880af78 URL.
Note that the last part of the URL is the hash created by the Mutation from Step 5. You will be redirected to the hash's URL page, in this case, the DigitalOcean Community website.
Now that you have the URL redirection working, you will make the application safer by implementing error handling when the Mutation is executed.
Step 6 - Implementing Error Handling
Handling errors is a best practice in all applications, since developers don't usually control what will be sent to the server.
In this case, you can try to foresee failures and minimize their impacts.
In a complex system such as GraphQL, a lot of things might go wrong, from the client asking for the wrong data to the server losing access to the database.
As a typed system, GraphQL can verify everything the client asks for and receives in an operation called Schema Validation.
You can see this in action by making a Query with a non-existing field.
Navigate to http: / / localhost: 8000 / graphql in your browser once more, and execute the next Query within the GraphiQL interface, with the iDontExist field:
Since there is no iDontExist field defined in your Query, GraphQL returns an error message:
This is important because, in the GraphQL typed system, the aim is to send and receive just the information already defined in the schema.
The current application accepts any arbitrary string in the full _ url field.
The problem is that if someone sends a poorly constructed URL, you would be redirecting the user to nowhere when trying the stored information.
In this case, you need to verify if the full _ url is well formatted before saving it to the database, and, if there's any error, raise the GraphQLError exception with a custom message.
Let's implement this functionality in two steps.
First, open the shortener / models.py file:
Add the highlighted lines in the import section:
The URLValidator is a Django helper to validate a URL String and the GraphQLError is used by Graphene to raise exceptions with a custom message.
Next, make sure to validate the URL received by the user before saving it to the database.
Enable this operation by adding the highlighted code in the shortener / models.py file:
First, this code instantiates the URLValidator in the validate variable.
Inside the try / except block, you validate () the URL received and raise a GraphQLError with the invalid url custom message if something went wrong.
The full shortener / models.py file is shown here:
If you are not running the local server, start it with the python manage.py runserver command.
Next, test your new error handling at http: / / localhost: 8000 / graphql.
Try to create a new URL with an invalid full _ url in the GraphiQL interface:
When sending an invalid URL, your exception will be raised with the custom message:
If you look in your terminal where the python manage.py runserver command is running, an error will appear:
A GraphQL endpoint will always fail with a HTTP 200 status code, which usually signifies success.
Remember that, even though GraphQL is built on top of HTTP, it doesn't use the concepts of HTTP status codes or HTTP methods as REST does.
With the error handling implemented, you can now put in place a mechanism to filter your Queries, minimizing the information returned by the server.
Step 7 - Implementing Filters
Imagine you've started using the URL shortener to add your own links.
After a while, there will be so many entries that finding the right one will become difficult.
You can solve this issue using filters.
Filtering is a common concept in REST APIs, where usually a Query Parameter with a field and value is appended to the URL.
As an example, to filter all the Users named jojo, you could use GET / api / users? name = jojo.
In GraphQL you will use Query Arguments as filters.
They create a nice and clean interface.
You can solve the "hard to find a URL" issue by allowing the client to filter URLs by name using the full _ url field.
To implement that, open the shortener / schema.py file in your favorite editor.
First, import the Q method in the highlighted line:
This will be used to filter your database query.
Next, rewrite the whole Query class with the following content:
The modifications you are making are:
Adding the url filter parameter inside the urls variable and resolve _ url method.
Inside the resolve _ urls, if a parameter named url is given, filtering the database results to return only URLs that contain the value given, using the Q (full _ url _ _ icontains = url) method.
Test your latest changes at http: / / localhost: 8000 / graphql.
In the GraphiQL interface, write the following statement.
It will filter all the URLs with the word community:
The output is only one entry since you just added one URL with the community string in it. If you added more URLs before, your output may vary.
Now you have the ability to search through your URLs.
However, with too many links, your clients might complain the URL list is returning more data than their apps can handle.
To solve this, you will implement pagination.
Step 8 - Implementing Pagination
Clients using your backend might complain that the response time is taking too long or that its size is too big if there are too many URL entries.
Even your database may struggle to put together a huge set of information.
To solve this issue, you can allow the client to specify how many items it wants within each request using a technique called pagination.
There's no default way to implement this feature.
Even in REST APIs, you might see it in HTTP headers or query parameters, with different names and behaviors.
In this application, you will implement pagination by enabling two more arguments to the URLs Query: first and skip. first will select the first variable number of elements and skip will specify how many elements should be skipped from the beginning.
For example, using first = = 10 and skip = = 5 gets the first 10 URLs, but skips 5 of them, returning just the remaining 5.
Implementing this solution is similar to adding a filter.
Open the shortener / schema.py file:
In the file, change the Query class by adding the two new parameters into the urls variable and resolve _ urls method, highlighted in the following code:
This code uses the newly created first and skip parameters inside the resolve _ urls method to filter the database query.
To test the pagination, issue the following Query in the GraphiQL interface at http: / / localhost: 8000 / graphql:
Your URL shortener will return the second URL created in your database:
This shows that the pagination feature works.
Feel free to play around by adding more URLs and testing different sets of first and skip.
The whole GraphQL ecosystem is growing every day, with an active community behind it. It has been proven production-ready by companies like GitHub and Facebook, and now you can apply this technology to your own projects.
In this tutorial you created a URL shortener service using GraphQL, Python, and Django, using concepts like Queries and Mutations.
But more than that, you now understand how to rely on these technologies to build web applications using the Django web framework.
You can explore more about GraphQL and the tools used here in the GraphQL website and the Graphene documentation websites.
Also, DigitalOcean has additional tutorials for Python and Django that you can use if you'd like to learn more about either.
How To Install and Use PostgreSQL on CentOS 8
4007
Relational database management systems are a key component of many websites and applications.
PostgreSQL, also known as Postgres, is a relational database management system that provides an implementation of Structured Query Language, better known as SQL.
It's used by many popular projects, both large and small, is standards-compliant, and has many advanced features like reliable transactions and concurrency without read locks.
By following this guide, you will install the latest version of PostgreSQL on a CentOS 8 server.
PostgreSQL is available from CentOS 8 "s default AppStream software repository, and there are multiple versions which you can install.
You can choose between these versions by enabling the appropriate collection of packages and dependencies that align with the version you want to install, with each collection referred to as a module stream.
In DNF, CentOS 8's default package manager, modules are special collections of RPM packages that together make up a larger application.
This is intended to make installing packages and their dependencies more intuitive for users.
List out the available streams for the postgresql module using the dnf command:
You can see in this output that there are three versions of PostgreSQL available from the AppStream repository: 9.6, 10, and 12. The stream that provides Postgres version 10 is the default, as indicated by the [d] following it. If you want to install that version you could just run sudo dnf install postgresql-server and move on to the next step.
However, even though version 10 is still maintained, this tutorial will install Postgres version 12, the latest release at the time of this writing.
To install PostgreSQL version 12, you must enable that version's module stream.
When you enable a module stream, you override the default stream and make all of the packages related to the enabled stream available on the system.
Note that only one stream of any given module can be enabled on a system at the same time.
To enable the module stream for Postgres version 12, run the following command:
When prompted, press y and then ENTER to confirm that you want to enable the stream:
After enabling the version 12 module stream, you can install the postgresql-server package to install PostgreSQL 12 and all of its dependencies:
When given the prompt, confirm the installation by pressing y then ENTER:
Now that the software is installed, you will perform some initialization steps to prepare a new database cluster for PostgreSQL.
Step 2 - Creating a New PostgreSQL Database Cluster
You have to create a new PostgreSQL database cluster before you can start creating tables and loading them with data. A database cluster is a collection of databases that are managed by a single server instance.
Creating a database cluster consists of creating the directories in which the database data will be placed, generating the shared catalog tables, and creating the template1 and postgres databases.
The template1 database is a template of sorts used to create new databases; everything that is stored in template1, even objects you add yourself, will be placed in new databases when they're created.
The postgres database is a default database designed for use by users, utilities, and third-party applications.
The Postgres package we installed in the previous step comes with a handy script called postgresql-setup which helps with low-level database cluster administration.
To create a database cluster, run the script using sudo and with the --initdb option:
Now start the PostgreSQL service using systemctl:
Then, use systemctl once more to enable the service to start up whenever the server boots:
Now that PostgreSQL is up and running, we will go over using roles to learn how Postgres works and how it is different from similar database management systems you may have used in the past.
Step 3 - Using PostgreSQL Roles and Databases
PostgreSQL uses a concept called roles to handle client authentication and authorization.
These are in some ways similar to regular Unix-style accounts, but Postgres does not distinguish between users and groups and instead prefers the more flexible term role.
In order to use PostgreSQL, you can log in to that account.
There are a few ways to use this account to access the PostgreSQL prompt.
This will bring you back to the postgres account "s Linux command prompt.
Now return to your original account with the following:
You can also run commands with the postgres account directly using sudo.
For instance, in the previous example, you were instructed to access the Postgres prompt by first switching to the postgres user and then running psql to open the Postgres prompt.
As an alternative, you could do this in one step by running the single command psql as the postgres user with sudo, like this:
This will log you directly into Postgres without the intermediary bash shell.
In this step, you used the postgres account to reach the psql prompt.
But many use cases require more than one Postgres role.
Read on to learn how to configure new roles.
Step 4 - Creating a New Role
The script will prompt you with some choices and, based on your responses, execute the necessary Postgres commands to create a user to your specifications.
For this tutorial, create a role named sammy and give it superuser privileges by entering y when prompted:
Check out the options by looking at the man page for createuser:
Your installation of Postgres now has a new role, but you have not yet added any databases.
Step 5 - Creating a New Database
This means that if the user you created in the last section is called sammy, that role will attempt to connect to a database which is also called sammy by default.
Now that you've created a new database, you will log in to it with your new role.
Step 6 - Opening a Postgres Prompt with the New Role
Once this new account is available, you can either switch over and then connect to the database by first typing:
This command will log you in automatically.
If you want your user to connect to a different database, you can do so by including the -d flag and specifying the database, like this:
This will show the following output:
Having connected to your database, you can now try out creating and deleting tables.
Step 7 - Creating and Deleting Tables
These commands give the table a name, and then define the columns as well as the column type and the max length of the field data. You can also optionally add table constraints for each column.
You "ve also given this column the constraint of PRIMARY KEY, which means that the values must be unique and not null.
This is a representation of the serial type that you gave your equip _ id column.
In this step, you created a sample table.
In the next step, you will try out adding, querying, and deleting entries in that table.
Step 8 - Adding, Querying, and Deleting Data in a Table
As an example, add a slide and a swing by calling the table you want to add to, naming the columns, and then providing data for each column, like this:
This is because it is automatically generated whenever a new row in the table is created.
Notice that your slide is no longer a part of the table.
Now that you've added and deleted entries in your table, you can try adding and deleting columns.
Step 9 - Adding and Deleting Columns from a Table
After creating a table, you can modify it to add or remove columns.
If you find that your work crew uses a separate tool to keep track of maintenance history, you can delete the column by typing:
Having now added and deleted columns, you can try updating existing data in the final step.
Step 10 - Updating Data in a Table
You can query for the swing record (this will match every swing in your table) and change its color to red:
You are now set up with PostgreSQL on your CentOS 8 server.
Learn about running queries in PostgreSQL
How To Install Node.js on CentOS 8
4006
In this guide, we will show you three different ways of getting Node.js installed on a CentOS 8 server:
using dnf to install the nodejs package from CentOS's default AppStream repository
installing nvm, the Node Version Manager, and using it to install and manage multiple versions of node
building and installing node from source
Most users should use dnf to install the built-in pre-packaged versions of Node.
If you're a developer or otherwise need to manage multiple installed versions of Node, use the nvm method.
Building from source is rarely necessary for most users.
Option 1 - Installing Node from the CentOS AppStream Repository
Node.js is available from CentOS 8's default AppStream software repository.
There are multiple versions available, and you can choose between them by enabling the appropriate module stream.
First list out the available streams for the nodejs module using the dnf command:
Two streams are available, 10 and 12. The < ^ > [d] < ^ > indicates that version 10 is the default stream.
If you'd prefer to install Node.js 12, switch module streams now:
You will be prompted to confirm your decision.
Afterwards the version 12 stream will be enabled and we can continue with the installation.
For more information on working with module streams, see the official CentOS AppStream documentation.
Install the nodejs package with dnf:
Again, dnf will ask you to confirm the actions it will take.
Press y then ENTER to do so, and the software will install.
Your --version output will be different if you installed Node.js 10 instead.
< $> note Note: both available versions of Node.js are long-term support releases, meaning they have a longer guaranteed window of maintenance.
See the official Node.js releases page for more lifecycle information.
Installing the nodejs package should also install the npm Node Package Manager utility as a dependency.
Verify that it was installed properly as well:
At this point you have successfully instlled Node.js and npm using the CentOS software repositories.
The next section will show how to use the Node Version Manager to do so.
Option 2 - Installing Node Using the Node Version Manager
To install NVM on your CentOS 8 machine, visit the project's GitHub page.
To use it, you must first source your .bash _ profile file:
< $> note Note: if you also have a version of Node installed through the CentOS software repositories, you may see a system - > v12.13.1 (or some other version number) line here.
You can always activate the system version of Node using nvm use system.
Option 3 - Installing Node from Source
Another way to install Node.js is to download the source code and compile it yourself.
To do so, use your web browser to navigate to the official Node.js download page, right-click on the Source Code link and click Copy Link Address or whichever similar option your browser gives you.
Back in your SSH session, first make sure you're in a directory you can write to.
We'll use the current user's home directory:
Then type curl, paste the link that you copied from the website, and follow it with | tar xz:
This will use the curl utility to download the source, then pipe it directly to the tar utility, which will extract it into the current directory.
Move into the newly created source directory:
There are a few packages that we need to download from the CentOS repositories in order to compile the code.
Use dnf to install these now:
You will be prompted to confirm the installation.
Type y then ENTER to do so.
Now, we can configure and compile the software:
The compilation will take quite a while (around 30 minutes on a four-core server).
We've used the -j4 option to run four parallel compilation processes.
You can omit this option or update the number based on the number of processor cores you have available.
When compilation is finished, you can install the software onto your system by typing:
To check that the installation was successful, ask Node to display its version number:
If you see the correct version number, then the installation was completed successfully.
By default Node also installs a compatible version of npm, so that should be available as well.
In this tutorial we've shown how to install Node.js using the CentOS AppStream software repository, using Node Version Manager, and by compiling from source.
If you'd like more information on programming in JavaScript, please read our related tutorial series:
How To Code in Javascript: a comprehensive overview of the JavaScript language, applicable to both the browser and Node.js
How To Code in Node.js: a series of exercises that teach the basics of using Node.js
< $> note Note: If your servers are running on DigitalOcean, you can optionally use DigitalOcean Cloud Firewalls instead of firewalld.
Making Servers Work
3995
Making Servers Work: A Practical Guide to System Administration eBook in EPUB format
Making Servers Work: A Practical Guide to System Administration eBook in PDF format < $>
This book highlights practical sysadmin skills, common architectures that you "ll encounter, and best practices that apply to automating and running systems at any scale, from one laptop or server to 1,000 or more.
It is intended to help orient you within the discipline, and hopefully encourages you to learn more about system administration.
Introductory Topics
LAMP and LEMP Technology Stacks
Securing your Servers
Automation with Ansible
Version Control and Continuous Integration
Feel free to pick topics in this book that interest you and explore them using these chapters as guides.
Working through this book will expose you to a wide variety of technologies, technical terms, and conceptual approaches to managing Linux servers.
You can work through each chapter or section at your own pace, and in any order that you choose.
For additional sysadmin resources to help you get started, and to participate in the DigitalOcean community of other developers and administrators, check out our growing library of tutorials, questions, and projects with the Getting Started tag.
How To Run Serverless Functions Using OpenFaaS on DigitalOcean Kubernetes
4025
Typically, hosting a software application on the internet requires infrastructure management, planning, and monitoring for a monolithic system.
Unlike this traditional approach, the serverless architecture (also known as function as a service, or FaaS) breaks down your application into functions.
These functions are stateless, self contained, event triggered, functionally complete entities that communicate via APIs that you manage, instead of the underlying hardware and explicit infrastructure provisioning.
Functions are scalable by design, portable, faster to set up and easier to test than ordinary apps.
For the serverless architecture to work in principle, it requires a platform agnostic way of packaging and orchestrating functions.
OpenFaaS is an open-source framework for implementing the serverless architecture on Kubernetes, using Docker containers for storing and running functions.
It allows any program to be packaged as a container and managed as a function via the command line or the integrated web UI.
OpenFaaS has excellent support for metrics and provides autoscaling for functions when demand increases.
In this tutorial, you will deploy OpenFaaS to your DigitalOcean Kubernetes cluster at your domain and secure it using free Let's Encrypt TLS certificates.
You'll also explore its web UI and deploy existing and new functions using the faas-cli, the official command line tool.
In the end, you'll have a flexible system for deploying serverless functions in place.
The cluster must have at least 8GB RAM and 4 CPU cores available for OpenFaaS (more will be required in case of heavier use).
Docker installed on your local machine.
Following Steps 1 and 2 for your distribution, see How To Install Docker.
An account at Docker Hub for storing Docker images you'll create during this tutorial.
faas-cli, the official CLI tool for managing OpenFaaS, installed on your local machine.
For instructions for multiple platforms, visit the official docs.
The Helm package manager installed on your local machine.
To do this, complete Step 1 and add the stable repo from Step 2 of the How To Install Software on Kubernetes Clusters with the Helm 3 Package Manager tutorial.
The Nginx Ingress Controller and Cert-Manager installed on your cluster using Helm in order to expose OpenFaaS using Ingress Resources.
A fully registered domain name to host OpenFaaS, pointed at the Load Balancer used by the Nginx Ingress.
This tutorial will use < ^ > openfaas.your _ domain < ^ > throughout.
< $> note Note: The domain name you use in this tutorial must differ from the one used in the "How To Set Up an Nginx Ingress on DigitalOcean Kubernetes" prerequisite tutorial.
Step 1 - Installing OpenFaaS using Helm
In this step, you will install OpenFaaS to your Kubernetes cluster using Helm and expose it at your domain.
As part of the Nginx Ingress Controller prerequisite, you created example Services and an Ingress.
You won't need them in this tutorial, so you can delete them by running the following commands:
Since you'll be deploying functions as Kubernetes objects, it's helpful to store them and OpenFaaS itself in separate namespaces in your cluster.
The OpenFaaS namespace will be called openfaas, and the functions namespace will be openfaas-fn.
Create them in your cluster by running the following command:
Next, you'll need to add the OpenFaaS Helm repository, which hosts the OpenFaaS chart.
Helm will display the following output:
Refresh Helm's chart cache:
Before installing OpenFaaS, you'll need to customize some chart parameters.
You'll store them on your local machine, in a file named values.yaml.
Create and open the file with your text editor:
First, you specify the namespace where functions will be stored by assigning openfaas-fn to the functionNamespace variable.
By setting generateBasicAuth to true, you order Helm to set up mandatory authentication when accessing the OpenFaaS web UI and to generate an admin username and password login combination for you.
Then, you enable Ingress creation and further configure it to use the Nginx Ingress Controller and serve the gateway OpenFaaS service at your domain.
Remember to replace < ^ > openfaas.your _ domain < ^ > with your desired domain from the prerequisites.
Finally, install OpenFaaS into the openfaas namespace with the customized values:
The output shows that the installation was successful.
Run the following command to reveal the password for the admin account:
The decoded password is written to the output and to a file called openfaas-password.txt at the same time using tee.
Note the output, which is your OpenFaaS password for the admin account.
You can watch OpenFaaS containers become available by running the following command:
When all listed deployments become ready, type CTRL + C to exit.
You can now navigate to the specified domain in your web browser.
Input admin as the username and the accompanying password when prompted.
You'll see the OpenFaaS web UI:
OpenFaaS - Empty Control Panel
You've successfully installed OpenFaaS and exposed its control panel at your domain.
Next, you'll secure it using free TLS certificates from Let's Encrypt.
Step 2 - Enabling TLS for Your Domain
In this step, you'll secure your exposed domain using Let's Encrypt certificates, provided by cert-manager.
To do this, you'll need to edit the ingress config in values.yaml.
Add the highlighted lines:
The tls block defines in what Secret the certificates for your sites (listed under hosts) will store their certificates, which the letsencrypt-prod ClusterIssuer issues.
Generally, the specified Secret must be different for every Ingress in your cluster.
Remember to replace < ^ > openfaas.your _ domain < ^ > with your desired domain, then save and close the file.
You "ll need to wait a few minutes for the Let" s Encrypt servers to issue a certificate for your domain.
In the meantime, you can track its progress by inspecting the output of the following command:
The end of the output will look similar to this:
When the last line of output reads Certificate issued successfully, you can exit by pressing CTRL + C. Refresh your domain in your browser to test.
You "ll see the padlock to the left of the address bar in your browser, signifying that your connection is secure.
You've secured your OpenFaaS domain using free TLS certificates from Let's Encrypt.
Now you'll use the web UI and manage functions from it.
Step 3 - Deploying Functions via the Web UI
In this section, you'll explore the OpenFaaS web UI and then deploy, manage, and invoke functions from it.
The OpenFaaS web UI has two main parts: on the left-hand side, a column where the deployed functions will be listed, and the central panel, where you'll see detailed info about a selected function and be able to interact with it.
To deploy a new function, click the Deploy New Function button underneath the OpenFaaS logo on the upper left.
You'll see a dialog asking you to choose a function:
OpenFaaS - Deploy a New Function dialog
The FROM STORE tab lists pre-made functions from the official OpenFaaS function store that you can deploy right away.
Each function is shown with a short description, and you can select the link icon on the right of a function to take a look at its source code.
To deploy a store function from this list, select it, and then click the DEPLOY button.
You can also supply your own function by switching to the CUSTOM tab:
OpenFaaS - Deploy a Custom Function
Here, you'd need to specify a Docker image of your function that is configured specifically for OpenFaaS and available at a Docker registry (such as Docker Hub).
In this step, you'll deploy a pre-made function from the OpenFaaS store, then in the next steps you'll create and deploy custom functions to Docker Hub.
You'll deploy the NodeInfo function, which returns information about the machine it's deployed on, such as CPU architecture, number of cores, total RAM memory available, and uptime (in seconds).
From the list of store functions, select NodeInfo and click DEPLOY.
It will soon show up in the list of deployed functions.
OpenFaaS - NodeInfo Deployed
Select it. In the central part of the screen, you'll see basic information about the deployed function.
OpenFaaS - Deployed Function Info
The status of the function updates in real time, and should quickly turn to Ready.
If it stays at Not Ready for longer periods of time, it's most likely that your cluster lacks the resources to accept a new pod.
You can follow How To Resize Droplets for information on how to fix this.
Once Ready, the deployed function is accessible at the shown URL.
To test it, you can navigate to the URL in your browser, or call it from the Invoke function panel located beneath the function info.
OpenFaaS - Invoke Deployed Function
You can select between Text, JSON, and Download to indicate the type of response you expect.
If you want the request to be a POST instead of GET, you can supply request data in the Request body field.
To call the nodeinfo function, click the INVOKE button.
OpenFaaS will craft and execute a HTTP request according to the selected options and fill in the response fields with received data.
OpenFaaS - nodeinfo Function Response
The response status is HTTP 200 OK, which means that the request was executed successfully.
The response body contains system information that the NodeInfo function collects, meaning that it's properly accessible and working correctly.
To delete a function, select it from the list and click the garbage can icon in the right upper corner of the page.
When prompted, click OK to confirm.
The function's status will turn to Not Ready (which means it's being removed from the cluster) and the function will soon vanish from the UI altogether.
In this step, you've used the OpenFaaS web UI, as well as deploy and manage functions from it. You'll now see how you can deploy and manage OpenFaaS functions using the command line.
Step 4 - Managing Functions Using the faas-cli
In this section, you'll configure the faas-cli to work with your cluster.
Then, you'll deploy and manage your existing functions through the command line.
To avoid having to specify your OpenFaaS domain every time you run the faas-cli, you'll store it in an environment variable called OPENFAAS _ URL, whose value the faas-cli will automatically pick up and use during execution.
Open .bash _ profile in your home directory for editing:
Remember to replace < ^ > openfaas.your _ domain < ^ > with your domain, then save and close the file.
To avoid having to log in again, manually evaluate the file:
Now, ensure that you have faas-cli installed on your local machine.
If you haven't yet installed it, do so by following the instructions outlined in the official docs.
Then, set up your login credentials by running the following command:
To deploy a function from the store, run the following command:
You can try deploying nodeinfo by running:
To list deployed functions, run faas list:
Your existing functions will be shown:
To get detailed info about a deployed function, use faas describe:
The output will be similar to:
You can invoke a function with faas invoke:
You can then provide a request body.
If you do, the method will be POST instead of GET.
When you are done with inputting data, or want the request to be GET, press CTRL + D. The faas-cli will then execute the inferred request and output the response, similarly to the web UI.
To delete a function, run faas remove:
Run faas list again to see that nodeinfo was removed:
In this step, you've deployed, listed, invoked, and removed functions in your cluster from the command line using the faas-cli.
In the next step, you'll create your own function and deploy it to your cluster.
Step 5 - Creating and Deploying a New Function
Now you'll create a sample Node.JS function using the faas-cli and deploy it to your cluster.
The resulting function you'll create will be packaged as a Docker container and published on Docker Hub.
To be able to publish containers, you'll need to log in by running the following command:
Enter your Docker Hub username and password when prompted to finish the login process.
You'll store the sample Node.JS function in a folder named sample-js-function.
Populate the directory with the template of a JS function by running the following command:
As written in the output, the code for the function itself is in the folder sample-js, while the OpenFaaS configuration for the function is in the file sample-js.yaml.
Under the sample-js directory (which resembles a regular Node.JS project) are two files, handler.js and package.json.
handler.js contains actual JS code that will return a response when the function is called.
The contents of the handler look like the following:
It exports a lambda function with two parameters, a context with request data and a callback that you can use to pass back response data, instead of just returning it.
Change the highlighted line as follows:
This OpenFaaS function will, when called, write Hello Sammy!
to the response.
Next, open the configuration file for editing:
It will look like the following:
For the provider, it specifies openfaas and a default gateway.
Then, it defines the sample-js function, specifies its language (node), its handler and the Docker image name, which you'll need to modify to include your Docker Hub account username, like so:
Then, build the Docker image, push it to Docker Hub, and deploy it on your cluster, all at the same time by running the following command:
There will be a lot of output (mainly from Docker), which will end like this:
Invoke your newly deployed function to make sure it's working:
Press CTRL + D. You'll see the following output:
This means that the function was packaged and deployed correctly.
You can remove the function by running:
You have now successfully created and deployed a custom Node.JS function on your OpenFaaS instance in your cluster.
You've deployed OpenFaaS on your DigitalOcean Kubernetes cluster and are ready to deploy and access both pre-made and custom functions.
Now, you are able to implement the Function as a Service architecture, which can increase resource utilization and bring performance improvements to your apps.
If you'd like to learn more about advanced OpenFaaS features, such as autoscaling for your deployed functions and monitoring their performance, visit the official docs.
How To Create a Web Server in Node.js with the HTTP Module
4024
When you view a webpage in your browser, you are making a request to another computer on the internet, which then provides you the webpage as a response.
That computer you are talking to via the internet is a web server.
A web server receives HTTP requests from a client, like your browser, and provides an HTTP response, like an HTML page or JSON from an API.
A lot of software is involved for a server to return a webpage.
This software generally falls into two categories: frontend and backend.
Front-end code is concerned with how the content is presented, such as the color of a navigation bar and the text styling.
Back-end code is concerned with how data is exchanged, processed, and stored.
Code that handles network requests from your browser or communicates with the database is primarily managed by back-end code.
Node.js allows developers to use JavaScript to write back-end code, even though traditionally it was used in the browser to write front-end code.
Having both the frontend and backend together like this reduces the effort it takes to make a web server, which is a major reason why Node.js is a popular choice for writing back-end code.
In this tutorial, you will learn how to build web servers using the http module that's included in Node.js.
You will build web servers that can return JSON data, CSV files, and HTML web pages.
Ensure that Node.js is installed on your development machine.
This tutorial uses Node.js version 10.19.0.
The Node.js platform supports creating web servers out of the box.
To get started, be sure you're familiar with the basics of Node.js.
You can get started by reviewing our guide on How To Write and Run Your First Program in Node.js.
We also make use of asynchronous programming for one of our sections.
If you're not familiar with asynchronous programming in Node.js or the fs module for interacting with files, you can learn more with our article on How To Write Asynchronous Code in Node.js.
Step 1 - Creating a Basic HTTP Server
Let's start by creating a server that returns plain text to the user.
This will cover the key concepts required to set up a server, which will provide the foundation necessary to return more complex data formats like JSON.
First, we need to set up an accessible coding environment to do our exercises, as well as the others in the article.
In the terminal, create a folder called < ^ > first-servers < ^ >:
Now, create the file that will house the code:
Open the file in a text editor.
We will use nano as it's available in the terminal:
We start by loading the http module that's standard with all Node.js installations.
Add the following line to < ^ > hello.js < ^ >:
The http module contains the function to create the server, which we will see later on.
If you would like to learn more about modules in Node.js, check out our How To Create a Node.js Module article.
Our next step will be to define two constants, the host and port that our server will be bound to:
As mentioned before, web servers accept requests from browsers and other clients.
We may interact with a web server by entering a domain name, which is translated to an IP address by a DNS server.
An IP address is a unique sequence of numbers that identify a machine on a network, like the internet.
For more information on domain name concepts, take a look at our An Introduction to DNS Terminology, Components, and Concepts article.
The value localhost is a special private address that computers use to refer to themselves.
It's typically the equivalent of the internal IP address 127.0.0.1 and it's only available to the local computer, not to any local networks we've joined or to the internet.
The port is a number that servers use as an endpoint or "door" to our IP address.
In our example, we will use port 8000 for our web server.
Ports 8080 and 8000 are typically used as default ports in development, and in most cases developers will use them rather than other ports for HTTP servers.
When we bind our server to this host and port, we will be able to reach our server by visiting http: / / localhost: 8000 in a local browser.
Let's add a special function, which in Node.js we call a request listener.
This function is meant to handle an incoming HTTP request and return an HTTP response.
This function must have two arguments, a request object and a response object.
The request object captures all the data of the HTTP request that's coming in.
The response object is used to return HTTP responses for the server.
We want our first server to return this message whenever someone accesses it: "My first server!".
Let's add that function next:
The function would usually be named based on what it does.
For example, if we created a request listener function to return a list of books, we would likely name it listBooks ().
Since this one is a sample case, we will use the generic name requestListener.
All request listener functions in Node.js accept two arguments: req and res (we can name them differently if we want).
The HTTP request the user sends is captured in a Request object, which corresponds to the first argument, req.
The HTTP response that we return to the user is formed by interacting with the Response object in second argument, res.
The first line res.writeHead (200); sets the HTTP status code of the response.
HTTP status codes indicate how well an HTTP request was handled by the server.
In this case, the status code 200 corresponds to "OK".
If you are interested in learning about the various HTTP codes that your web servers can return with the meaning they signify, our guide on How To Troubleshoot Common HTTP Error Codes is a good place to start.
The next line of the function, res.end (" My first server! ")
;, writes the HTTP response back to the client who requested it. This function returns any data the server has to return.
In this case, it's returning text data.
Finally, we can now create our server and make use of our request listener:
Save and exit nano by pressing CTRL + X.
In the first line, we create a new server object via the http module's createServer () function.
This server accepts HTTP requests and passes them on to our requestListener () function.
After we create our server, we must bind it to a network address.
We do that with the server.listen () method.
It accepts three arguments: port, host, and a callback function that fires when the server begins to listen.
All of these arguments are optional, but it is a good idea to explicitly state which port and host we want a web server to use.
When deploying web servers to different environments, knowing the port and host it is running on is required to set up load balancing or a DNS alias.
The callback function logs a message to our console so we can know when the server began listening to connections.
< $> note Note: Even though requestListener () does not use the req object, it must still be the first argument of the function.
With less than fifteen lines of code, we now have a web server.
Let's see it in action and test it end-to-end by running the program:
In the console, we will see this output:
Notice that the prompt disappears.
This is because a Node.js server is a long running process.
It only exits if it encounters an error that causes it to crash and quit, or if we stop the Node.js process running the server.
In a separate terminal window, we'll communicate with the server using cURL, a CLI tool to transfer data to and from a network.
Enter the command to make an HTTP GET request to our running server:
When we press ENTER, our terminal will show the following output:
We've now set up a server and got our first server response.
Let's break down what happened when we tested our server.
Using cURL, we sent a GET request to the server at http: / / localhost: 8000.
Our Node.js server listened to connections from that address.
The server passed that request to the requestListener () function.
The function returned text data with the status code 200. The server then sent that response back to cURL, which displayed the message in our terminal.
Before we continue, let's exit our running server by pressing CTRL + C.
This interrupts our server's execution, bringing us back to the command line prompt.
In most web sites we visit or APIs we use, the server responses are seldom in plain text.
We get HTML pages and JSON data as common response formats.
In the next step, we will learn how to return HTTP responses in common data formats we encounter in the web.
Step 2 - Returning Different Types of Content
The response we return from a web server can take a variety of formats.
JSON and HTML were mentioned before, and we can also return other text formats like XML and CSV.
Finally, web servers can return non-text data like PDFs, zipped files, audio, and video.
In this article, in addition to the plain text we just returned, you'll learn how to return the following types of data:
JSON
CSV
HTML
The three data types are all text-based, and are popular formats for delivering content on the web.
Many server-side development languages and tools have support for returning these different data types.
In the context of Node.js, we need to do two things:
Set the Content-Type header in our HTTP responses with the appropriate value.
Ensure that res.end () gets the data in the right format.
Let's see this in action with some examples.
The code we will be writing in this section and later ones have many similarities to the code we wrote previously.
Most changes exist within the requestListener () function.
Let's create files with this "template code" to make future sections easier to follow.
Create a new file called html.js.
This file will be used later to return HTML text in an HTTP response.
We'll put the template code here and copy it to the other servers that return various types.
In the terminal, enter the following:
Now open this file in a text editor:
Let's copy the "template code."
Enter this in nano:
Save and exit html.js with CTRL + X, then return to the terminal.
Now let's copy this file into two new files.
The first file will be to return CSV data in the HTTP response:
The second file will return a JSON response in the server:
The remaining files will be for later exercises:
We're now set up to continue our exercises.
Let's begin with returning JSON.
Serving JSON
JavaScript Object Notation, commonly referred to as JSON, is a text-based data exchange format.
As its name suggests, it is derived from JavaScript objects, but it is language independent, meaning it can be used by any programming language that can parse its syntax.
JSON is commonly used by APIs to accept and return data. Its popularity is due to lower data transfer size than previous data exchange standards like XML, as well as the tooling that exists that allow programs to parse them without excessive effort.
If you'd like to learn more about JSON, you can read our guide on How To Work with JSON in JavaScript.
Open the json.js file with nano:
We want to return a JSON response.
Let's modify the requestListener () function to return the appropriate header all JSON responses have by changing the highlighted lines like so:
The res.setHeader () method adds an HTTP header to the response.
HTTP headers are additional information that can be attached to a request or a response.
The res.setHeader () method takes two arguments: the header's name and its value.
The Content-Type header is used to indicate the format of the data, also known as media type, that's being sent with the request or response.
In this case our Content-Type is application / json.
Now, let's return JSON content to the user.
Modify json.js so it looks like this:
Like before, we tell the user that their request was successful by returning a status code of 200. This time in the response.end () call, our string argument contains valid JSON.
Save and exit json.js by pressing CTRL + X.
Now, let's run the server with the node command:
In another terminal, let's reach the server by using cURL:
As we press ENTER, we will see the following result:
We now have successfully returned a JSON response, just like many of the popular APIs we create apps with.
Be sure to exit the running server with CTRL + C so we can return to the standard terminal prompt.
Next, let's look at another popular format of returning data: CSV.
Serving CSV
The Comma Separated Values (CSV) file format is a text standard that's commonly used for providing tabular data. In most cases, each row is separated by a newline, and each item in the row is separated by a comma.
In our workspace, open the csv.js file with a text editor:
Let's add the following lines to our requestListener () function:
This time, our Content-Type indicates that a CSV file is being returned as the value is text / csv.
The second header we add is Content-Disposition.
This header tells the browser how to display the data, particularly in the browser or as a separate file.
When we return CSV responses, most modern browsers automatically download the file even if the Content-Disposition header is not set. However, when returning a CSV file we should still add this header as it allows us to set the name of the CSV file.
In this case, we signal to the browser that this CSV file is an attachment and should be downloaded.
We then tell the browser that the file's name is oceanpals.csv.
Let's write the CSV data in the HTTP response:
Like before we return a 200 / OK status with our response.
This time, our call to res.end () has a string that's a valid CSV.
The comma separates the value in each column and the new line character (\ n) separates the rows.
We have two rows, one for the table header and one for the data.
We'll test this server in the browser.
Save csv.js and exit the editor with CTRL + X.
Run the server with the Node.js command:
The console will show this:
If we go to http: / / localhost: 8000 in our browser, a CSV file will be downloaded.
Its file name will be oceanpals.csv.
Exit the running server with CTRL + C to return to the standard terminal prompt.
Having returned JSON and CSV, we've covered two cases that are popular for APIs.
Let's move on to how we return data for websites people view in a browser.
Serving HTML
HTML, HyperText Markup Language, is the most common format to use when we want users to interact with our server via a web browser.
It was created to structure web content.
Web browsers are built to display HTML content, as well as any styles we add with CSS, another front-end web technology that allows us to change the aesthetics of our websites.
Let's reopen html.js with our text editor:
Modify the requestListener () function to return the appropriate Content-Type header for an HTML response:
Now, let's return HTML content to the user.
Add the highlighted lines to html.js so it looks like this:
We first add the HTTP status code.
We then call response.end () with a string argument that contains valid HTML.
When we access our server in the browser, we will see an HTML page with one header tag containing This is HTML.
Let's save and exit by pressing CTRL + X.
We will see Server is running on http: / / localhost: 8000 when our program has started.
Now go into the browser and visit http: / / localhost: 8000.
Our page will look like this:
Image of HTML response returned from Node.js server
It's common for HTML to be written in a file, separate from the server-side code like our Node.js programs.
Next, let's see how we can return HTML responses from files.
Step 3 - Serving an HTML Page From a File
We can serve HTML as strings in Node.js to the user, but it's preferable that we load HTML files and serve their content.
This way, as the HTML file grows we don't have to maintain long strings in our Node.js code, keeping it more concise and allowing us to work on each aspect of our website independently.
This "separation of concerns" is common in many web development setups, so it's good to know how to load HTML files to support it in Node.js
To serve HTML files, we load the HTML file with the fs module and use its data when writing our HTTP response.
First, we'll create an HTML file that the web server will return.
Create a new HTML file:
Now open index.html in a text editor:
Our web page will be minimal.
It will have an orange background and will display some greeting text in the center.
Add this code to the file:
This single webpage shows two lines of text: Hello Again!
and This is served from a file.
The lines appear in the center of the page, one above each other.
The first line of text is displayed in a heading, meaning it would be large.
The second line of text will appear slightly smaller.
All the text will appear white and the webpage has an orange background.
While it's not the scope of this article or series, if you are interested in learning more about HTML, CSS, and other front-end web technologies, you can take a look at Mozilla's Getting Started with the Web guide.
That's all we need for the HTML, so save and exit the file with CTRL + X.
We can now move on to the server code.
For this exercise, we'll work on htmlFile.js.
Open it with the text editor:
As we have to read a file, let's begin by importing the fs module:
This module contains a readFile () function that we'll use to load the HTML file in place.
We import the promise variant in keeping with modern JavaScript best practices.
We use promises as its syntactically more succinct than callbacks, which we would have to use if we assigned fs to just require (' fs ').
To learn more about asynchronous programming best practices, you can read our How To Write Asynchronous Code in Node.js guide.
We want our HTML file to be read when a user requests our system.
Let's begin by modifying requestListener () to read the file:
We use the fs.readFile () method to load the file.
Its argument has _ _ dirname + "/ index.html".
The special variable _ _ dirname has the absolute path of where the Node.js code is being run.
We then append / index.html so we can load the HTML file we created earlier.
Now let's return the HTML page once it's loaded:
If the fs.readFile () promise successfully resolves, it will return its data. We use the then () method to handle this case.
The contents parameter contains the HTML file's data.
We first set the Content-Type header to text / html to tell the client that we are returning HTML data. We then write the status code to indicate the request was successful.
We finally send the client the HTML page we loaded, with the data in the contents variable.
The fs.readFile () method can fail at times, so we should handle this case when we get an error.
Add this to the requestListener () function:
Save the file and exit nano with CTRL + X.
When a promise encounters an error, it is rejected.
We handle that case with the catch () method.
It accepts the error that fs.readFile () returns, sets the status code to 500 signifying that an internal error was encountered, and returns the error to the user.
Run our server with the node command:
In the web browser, visit http: / / localhost: 8000.
Image of HTML page loaded from a file in Node.js
You have now returned an HTML page from the server to the user.
You can quit the running server with CTRL + C.
You will see the terminal prompt return when you do.
When writing code like this in production, you may not want to load an HTML page every time you get an HTTP request.
While this HTML page is roughly 800 bytes in size, more complex websites can be megabytes in size.
Large files can take a while to load.
If your site is expecting a lot of traffic, it may be best to load HTML files at startup and save their contents.
After they are loaded, you can set up the server and make it listen to requests on an address.
To demonstrate this method, let's see how we can rework our server to be more efficient and scalable.
Serving HTML Efficiently
Instead of loading the HTML for every request, in this step we will load it once at the beginning.
The request will return the data we loaded at startup.
In the terminal, re-open the Node.js script with a text editor:
Let's begin by adding a new variable before we create the requestListener () function:
When we run this program, this variable will hold the HTML file's contents.
Now, let's readjust the requestListener () function.
Instead of loading the file, it will now return the contents of indexFile:
Next, we shift the file reading logic from the requestListener () function to our server startup.
Make the following changes as we create the server:
The code that reads the file is similar to what we wrote in our first attempt.
However, when we successfully read the file we now save the contents to our global indexFile variable.
We then start the server with the listen () method.
The key thing is that the file is loaded before the server is run.
This way, the requestListener () function will be sure to return an HTML page, as indexFile is no longer an empty variable.
Our error handler has changed as well.
If the file can't be loaded, we capture the error and print it to our console.
We then exit the Node.js program with the exit () function without starting the server.
This way we can see why the file reading failed, address the problem, and then start the server again.
We've now created different web servers that return various types of data to a user.
So far, we have not used any request data to determine what should be returned.
We'll need to use request data when setting up different routes or paths in a Node.js server, so next let's see how they work together.
Step 4 - Managing Routes Using an HTTP Request Object
Most websites we visit or APIs we use usually have more than one endpoint so we can access various resources.
A good example would be a book management system, one that might be used in a library.
It would not only need to manage book data, but it would also manage author data for cataloguing and searching convenience.
Even though the data for books and authors are related, they are two different objects.
In these cases, software developers usually code each object on different endpoints as a way to indicate to the API user what kind of data they are interacting with.
Let's create a new server for a small library, which will return two different types of data. If the user goes to our server's address at / books, they will receive a list of books in JSON.
If they go to / authors, they will receive a list of author information in JSON.
So far, we have been returning the same response to every request we get. Let's illustrate this quickly.
Re-run our JSON response example:
In another terminal, let's do a cURL request like before:
Now let's try another curl command:
After pressing Enter, you will see the same result:
We have not built any special logic in our requestListener () function to handle a request whose URL contains / todos, so Node.js returns the same JSON message by default.
As we want to build a miniature library management server, we'll now separate the kind of data that's returned based on the endpoint the user accesses.
First, exit the running server with CTRL + C.
Now open routes.js in your text editor:
Let's begin by storing our JSON data in variables before the requestListener () function:
The books variable is a string that contains JSON for an array of book objects.
Each book has a title or name, an author, and the year it was published.
The authors variable is a string that contains the JSON for an array of author objects.
Each author has a name, a country of birth, and their year of birth.
Now that we have the data our responses will return, let's start modifying the requestListener () function to return them to the correct routes.
First, we'll ensure that every response from our server has the correct Content-Type header:
Now, we want to return the right JSON depending on the URL path the user visits.
Let's create a switch statement on the request's URL:
To get the URL path from a request object, we need to access its url property.
We can now add cases to the switch statement to return the appropriate JSON.
JavaScript's switch statement provides a way to control what code is run depending on the value of an object or JavaScript expression (for example, the result of mathematical operations).
If you need a lesson or reminder on how to use them, take a look at our guide on How To Use the Switch Statement in JavaScript.
Let's continue by adding a case for when the user wants to get our list of books:
We set our status code to 200 to indicate the request is fine and return the JSON containing the list of our books.
Now let's add another case for our authors:
Like before, the status code will be 200 as the request is fine.
This time we return the JSON containing the list of our authors.
We want to return an error if the user tries to go to any other path.
Let's add the default case to do this:
We use the default keyword in a switch statement to capture all other scenarios not captured by our previous cases.
We set the status code to 404 to indicate that the URL they were looking for was not found.
We then set a JSON object that contains an error message.
Let's test our server to see if it behaves as we expect.
In another terminal, let's first run a command to see if we get back our list of books:
Press Enter to see the following output:
So far so good.
Let's try the same for / authors.
Type the following command in the terminal:
You will see the following output when the command is complete:
Last, let's try an erroneous URL to ensure that requestListener () returns the error response:
Entering that command will display this message:
We've now created different avenues for users to get different data. We also added a default response that returns an HTTP error if the user enters a URL that we don't support.
In this tutorial, you've made a series of Node.js HTTP servers.
You first returned a basic textual response.
You then went on to return various types of data from our server: JSON, CSV, and HTML.
From there you were able to combine file loading with HTTP responses to return an HTML page from the server to the user, and to create an API that used information about the user's request to determine what data should be sent in its response.
You're now equipped to create web servers that can handle a variety of requests and responses.
With this knowledge, you can make a server that returns many HTML pages to the user at different endpoints.
You could also create your own API.
To learn about more HTTP web servers in Node.js, you can read the Node.js documentation on the http module.
If you "d like to continue learning Node.js, you can return to the How To Code in Node.js series page.
How To Create React Elements with JSX
4048
In this tutorial, you "ll learn how to describe elements with JSX.
JSX is an abstraction that allows you to write HTML-like syntax in your JavaScript code and will enable you to build React components that look like standard HTML markup.
JSX is the templating language of React elements, and is therefore the foundation for any markup that React will render into your application.
Since JSX enables you to also write JavaScript in your markup, you "ll be able to take advantage of JavaScript functions and methods, including array mapping and short-circuit evaluation for conditionals.
As part of the tutorial, you "ll capture click events on buttons directly in the markup and catch instances when the syntax does not match exactly to standard HTML, such as with CSS classes.
At the end of this tutorial, you'll have a working application that uses a variety of JSX features to display a list of elements that have a built-in click listener.
This is a common pattern in React applications that you will use often in the course of learning the framework.
You'll also be able to mix standard HTML elements along with JavaScript to see how React gives you the ability to create small, reusable pieces of code.
You will need a development environment running Node.js; this tutorial was tested on Node.js version 10.19.0 and npm version 6.13.4.
Step 1 - Adding Markup to a React Element
As mentioned earlier, React has a special markup language called JSX.
It is a mix of HTML and JavaScript syntax that looks something like this:
You will recognize some JavaScript functionality such as .filter and .map, as well as some standard HTML like < div >.
But there are other parts that look like both HTML and JavaScript, such as < Card > and className.
This is JSX, the special markup language that gives React components the feel of HTML with the power of JavaScript.
In this step, you "ll learn to add basic HTML-like syntax to an existing React element.
To start, you'll add standard HTML elements into a JavaScript function, then see the compiled code in a browser.
You'll also group elements so that React can compile them with minimal markup leaving clean HTML output.
To start, make a new project.
On your command line run the following script to install a fresh project using create-react-app:
After the project is finished, change into the directory:
In a new terminal tab or window, start the project using the Create React App start script.
The browser will autorefresh on changes, so leave this script running the whole time that you work:
You will get a running local server.
If the project did not open in a browser window, you can find it at http: / / localhost: 3000 /.
If you are running this from a remote server, the address will be http: / / < ^ > your _ IP _ address < ^ >: 3000.
Your browser will load with a React application included as part of Create React App.
You will be building a completely new set of custom components, so you'll need to start by clearing out some boilerplate code so that you can have an empty project.
To start open App.js in a text editor.
This is the root component that is injected into the page.
All components will start from here.
In a new terminal, move into the project folder and open src / App.js with the following command:
You will see a file like this:
Now, delete the line import logo from '. / logo.svg and everything after the return statement in the function.
Change it to return null.
The final code will look like this:
Save and exit the text editor.
Finally, delete the logo.
In the terminal window type the following command:
You won't be using this SVG file in your application, and you should remove unused files as you work.
It will better organize your code in the long run.
Now that these parts of your project are removed, you can move on to exploring the facets of JSX.
This markup language is compiled by React and eventually becomes the HTML you see on a web page.
Without going too deeply into the internals, React takes the JSX and creates a model of what your page will look like, then creates the necessary elements and adds them to the page.
What that means is that you can write what looks like HTML and expect that the rendered HTML will be similar.
However, there are a few catches.
First, if you look at the tab or window running your server, you'll see this:
That's the linter telling you that you aren't using the imported React code.
When you add the line import React from 'react' to your code, you are importing JavaScript code that converts the JSX to React code.
If there's no JSX, there's no need for the import.
Let's change that by adding a small amount of JSX.
Start by replacing null with a Hello, World example:
If you look at the terminal with the server running, the warning message will be gone.
If you visit your browser, you will see the message as an h1 element.
browser screen showing "Hello, World"
Next, below the < h1 > tag, add a paragraph tag that contains the string I am writing JSX.
The code will look like this:
Since the JSX spans multiple lines, you'll need to wrap the expression in parentheses.
When you do you'll see an error in the terminal running your server:
When you return JSX from a function or statement, you must return a single element.
That element may have nested children, but there must be a single top-level element.
In this case, you are returning two elements.
The fix is a small code change.
Surround the code with an empty tag.
An empty tag is an HTML element without any words.
It looks like this: < > < / >.
Go back to. / src / App.js in your editor and add the empty tag:
The empty tag creates a single element, but when the code is compiled, it is not added to the final markup.
This will keep your code clean while still giving React a single element.
< $> note Note: You could have also wrapped the code with a div instead of empty tags, as long as the code returns one element.
In this example, an empty tag has the advantage of not adding extra markup to the parsed output.
Save the code and exit the file.
Your browser will refresh and show the updated page with the paragraph element.
In addition, when the code is converted the empty tags are stripped out:
Browser showing markup and devtools showing markup without empty tags
You've now added some basic JSX to your component and learned how all JSX needs to be nested in a single component.
In the next step, you'll add some styling to your component.
Step 2 - Adding Styling to an Element with Attributes
In this step, you'll style the elements in your component to learn how HTML attributes work with JSX.
There are many styling options in React.
Some of them involve writing CSS in Javascript, others use preprocessors.
In this tutorial you'll work with imported CSS and CSS classes.
Now that you have your code, it's time to add some styling.
Open App.css in your text editor:
Since you are starting with new JSX, the current CSS refers to elements that no longer exist.
Since you don't need the CSS, you can delete it.
After deleting the code, you'll have an empty file.
Next, you will add in some styling to center the text.
In src / App.css, add the following code:
In this code block, you created a CSS class selector called .container and used that to center the content using display: flex.
The browser will update, but nothing will change.
Before you can see the change, you need to add the CSS class to your React component.
Open the component JavaScript code:
That means that webpack will pull in the code to make a final style sheet, but to apply the CSS to your elements, you need to add the classes.
First, in your text editor, change the empty tags, < >, to < div >.
In this code, you replaced the empty tags - < > - with div tags.
Empty tags are useful for grouping your code without adding any extra tags, but here you need to use a div because empty tags do not accept any HTML attributes.
Next, you need to add the class name.
This is where JSX will start to diverge from HTML.
If you wanted to add a class to a usual HTML element you would do it like this:
But since JSX is JavaScript, it has a few limitations.
One of the limitations is that JavaScript has reserved keywords.
That means you can't use certain words in any JavaScript code.
For example, you can't make a variable called null because that word is already reserved.
One of the reserved words is class.
React gets around this reserved word by changing it slightly.
Instead of adding the attribute class, you will add the attribute className.
As a rule, if an attribute is not working as expected, try adding the camel case version.
Another attribute that is slightly different is the for attribute that you'd use for labels.
There are a few other cases, but fortunately the list is fairly short.
< $> note Note: In React, attributes are often called props.
Props are pieces of data that you can pass to other custom components.
They look the same as attributes except that they do not match any HTML specs.
In this tutorial, we'll call them attributes since they are mainly used like standard HTML attributes.
This will distinguish them from props that do not behave like HTML attributes, which will be covered later in this series.
Now that you know how the class attribute is used in React, you can update your code to include the styles.
In your text editor, add className = "container" to your opening div tag:
When you do, the page will reload and the content will be centered.
Centered html elements in a browser.
The className attribute is unique in React.
You can add most HTML attributes to JSX without any change.
As an example, go back to your text editor and add an id of greeting to your < h1 > element.
It will look like standard HTML:
Save the page and reload the browser.
It will be the same.
So far, JSX looks like standard markup, but the advantage of JSX is that even though it looks like HTML, it has the power of JavaScript.
That means you can assign variables and reference them in your attributes.
To reference an attribute, wrap it with curly braces - {} - instead of quotes.
In your text editor, add the following highlighted lines to reference an attribute:
In this code, you created a variable above the return statement called greeting with the value of "greeting", then referenced the variable in the id attribute of your < h1 > tag.
The page will be the same, but with an id tag.
Page with id tag highlighted in the developer tools
So far you've worked with a few elements on their own, but you can also use JSX to add many HTML elements and nest them to create complex pages.
To demonstrate this, you'll make a page with a list of emoji.
These emoji will be wrapped with a < button > element.
When you click on the emoji, you'll get their CLDR Short Name.
To start, you'll need to add a few more elements to the page.
Open src / App.js in your text editor.
Keep it open during this step.
First, add a list of emojis by adding the following highlighted lines:
Here you created a < ul > tag to hold a list of emojis.
Each emoji is in a separate < li > element and is surrounded with a < button > element.
In the next step you'll add an event to this button.
You also surrounded the emoji with a < span > tag that has a few more attributes.
Each span has the role attribute set to the img role.
This will signal to accessibility software that the element is acting like an image.
In addition, each < span > also has an aria-label and an id attribute with the name of the emoji.
The aria-label will tell visitors with screen readers what is displayed.
You will use the id when writing events in the next step.
When you write code this way, you are using semantic elements, which will help keep the page accessible and easy to parse for screen readers.
Your browser will refresh and you will see this:
browser with emoji as a list
Now add a little styling.
Open the CSS code in your text editor:
Add the following highlighted code to remove the default background and border for the buttons while increasing the font size:
In this code, you used font-size, border, and other parameters to adjust the look of your buttons and change the font.
You also removed the list styles and added display: flex to the < ul > element to make it horizontal.
Save and close the CSS file.
list with default styles removed
You've now worked with several JSX elements that look like regular HTML.
You've added classes, ids, and aria tags, and have worked with data as strings and variables.
But React also uses attributes to define how your elements should respond to user events.
In the next step, you'll start to make the page interactive by adding events to the button.
Step 3 - Adding Events to Elements
In this step, you'll add events to elements using special attributes and capture a click event on a button element.
You "ll learn how to capture information from the event to dispatch another action or use other information in the scope of the file.
Now that you have a basic page with information, it's time to add a few events to it. There are many event handlers that you can add to HTML elements.
React gives you access to all of these.
Since your JavaScript code is coupled with your markup, you can quickly add the events while keeping your code well-organized.
To start, add the onclick event handler.
This lets you add some JavaScript code directly to your element rather than attaching an event listener:
Since this is JSX, you camelCased onclick, which means you added it as onClick.
This onClick attribute uses an anonymous function to retrieve information about the item that was clicked.
You added an anonymous arrow function that will get the event from the clicked button, and the event will have a target that is the < span > element.
The information you need is in the id attribute, which you can access with event.target.id.
You can trigger the alert with the alert () function.
In your browser, click on one of the emoji and you will get an alert with the name.
Alert for party popper
You can reduce a duplication by declaring the function once and passing it to each onClick action.
Since the function does not rely on anything other than inputs and outputs, you can declare it outside the main component function.
In other words, the function does not need to access the scope of the component.
The advantage to keeping them separate is that your component function is slightly shorter and you could move the function out to a separate file later if you wanted to.
In your text editor, create a function called displayEmojiName that takes the event and calls the alert () function with an id. Then pass the function to each onClick attribute:
In your browser, click on an emoji and you will see the same alert.
In this step, you added events to each element.
You also saw how JSX uses slightly different names for element events, and you started writing reusable code by taking the function and reusing it on several elements.
In the next step, you will write a reusable function that returns JSX elements rather than writing each element by hand. This will further reduce duplication.
Step 4 - Mapping Over Data to Create Elements
In this step, you'll move beyond using JSX as simple markup.
You'll learn to combine it with JavaScript to create dynamic markup that reduces code and improves readability.
You'll refactor your code into an array that you will loop over to create HTML elements.
JSX doesn't limit you to an HTML-like syntax.
It also gives you the ability to use JavaScript directly in your markup.
You tried this a little already by passing functions to attributes.
You also used variables to reuse data. Now it's time to create JSX directly from data using standard JavaScript code.
In your text editor, you will need to create an array of the emoji data in the src / App.js file.
Reopen the file if you have closed it:
Add an array that will contain objects that have the emoji and the emoji name.
Note that emojis need to be surrounded by quote marks.
Create this array above the App function:
Now that you have the data you can loop over it. To use JavaScript inside of JSX, you need to surround it with curly braces: {}.
This is the same as when you added functions to attributes.
To create React components, you'll need to convert the data to JSX elements.
To do this, you'll map over the data and return a JSX element.
There are a few things you'll need to keep in mind as you write the code.
First, a group of items needs to be surrounded by a container < div >.
Second, every item needs a special property called key.
The key needs to be a unique piece of data that React can use to keep track of the elements so it can know when to update the component.
The key will be stripped out of the compiled HTML, since it is for internal purposes only.
Whenever you are working with loops you will need to add a simple string as a key.
Here's a simplified example that maps a list of names into a containing < div >:
The resulting HTML would look like this:
Converting the emoji list will be similar.
The < ul > will be the container.
You'll map over data and return a < li > with a key of the emoji short name.
You will replace the hard-coded data in the < button > and < span > tags with information from the loop.
In your text editor, add the following:
In the code, you mapped over the emojis array in the < ul > tag and returned a < li >.
In each < li > you used the emoji name as the key prop.
The button will have the same function as normal.
In the < span > element, replace the aria-label and id with the name.
The content of the < span > tag should be the emoji.
Your window will refresh and you'll see the data. Notice that the key is not present in the generated HTML.
Browser with developer tools showing updated HTML without key props
Combining JSX with standard JavaScript gives you a lot of tools to create content dynamically, and you can use any standard JavaScript you want.
In this step, you replaced hard-coded JSX with an array and a loop to create HTML dynamically.
In the next step, you'll conditionally show information using short circuiting.
Step 5 - Conditionally Showing Elements with Short Circuiting
In this step, you "ll use short circuiting to conditionally show certain HTML elements.
This will let you create components that can hide or show HTML based on additional information giving your components flexibility to handle multiple situations.
There are times when you will need a component to show information in some cases and not others.
For example, you may only want to show an alert message for the user if certain cases are true, or you may want to display some account information for an admin that you wouldn't want a normal user to see.
To do this you will use short circuting.
This means that you will use a conditional, and if the first part is truthy, it will return the information in the second part.
Here's an example.
If you wanted to show a button only if the user was logged in, you would surround the element with curly braces and add the condition before.
In this example, you are using the & & operator, which returns the last value if everything is truthy.
Otherwise, it returns false, which will tell React to return no additional markup.
If isLoggedIn is truthy, React will display the button.
If isLoggedIn is falsy, it will not show the button.
To try this out, add the following highlighted lines:
In your text editor, you created a variable called displayAction with a value of false.
You then surrounded the < p > tag with curly braces.
At the start of the curly braces, you added displayAction & & to create the conditional.
Save the file and you will see the element disappear in your browser.
Crucially, it will also not appear in the generated HTML.
This is not the same as hiding an element with CSS.
It won't exist at all in the final markup.
Browser with developer tools showing no paragraph element
Right now the value of displayAction is hard-coded, but you can also store that value as a state or pass it as a prop from a parent component.
In this step, you learned how to conditionally show elements.
This gives you the ability to create components that are customizable based on other information.
At this point, you've created a custom application with JSX.
You've learned how to add HTML-like elements to your component, add styling to those elements, pass attributes to create semantic and accessible markup, and add events to the components.
You then mixed JavaScript into your JSX to reduce duplicate code and to conditionally show and hide elements.
This is the basis you need to make future components.
Using a combination of JavaScript and HTML, you can build dynamic components that are flexible and allow your application to grow and change.
If you'd like to learn more about React, check out our React topic page.
The CSS code is already imported with the line import '. / App.css'.
How To Install Git on CentOS 8
4084
Version control systems are an indispensable part of modern software development.
Versioning allows you to keep track of your software at the source level.
You can track changes, revert to previous stages, and branch to create alternate versions of files and directories.
One of the most popular version control systems currently available is Git.
Many projects "files are maintained in a Git repository, and sites like GitHub, GitLab, and Bitbucket help to facilitate software development project sharing and collaboration.
In this guide, we will go through how to install and configure Git on a CentOS 8 server.
We will cover how to install the software two different ways: via the built-in package manager and via source.
Each of these approaches has their own benefits depending on your specific needs.
You will need a CentOS 8 server with a non-root superuser account.
Installing Git with Default Packages
Our first option to install Git is via CentOS "s default packages.
This option is best for those who want to get up and running quickly with Git, those who prefer a widely-used stable version, or those who are not looking for the newest available options.
If you are looking for the most recently release, you should jump to the section on installing from source.
We will be using the open-source package manager tool DNF, which stands for Dandified YUM the next-generation version of the Yellowdog Updater, Modified (that is, yum).
DNF is a package manager that is now the default package manager for Red Hat based Linux systems like CentOS.
It will let you install, update, and remove software packages on your server.
First, use the apt package management tools to update your local package index.
The -y flag is used to alert the system that we are aware that we are making changes, preventing the terminal from prompting us to confirm.
With the update complete, you can install Git:
You can confirm that you have installed Git correctly by running the following command:
With Git successfully installed, you can now move on to the Setting Up Git section of this tutorial to complete your setup.
Installing Git from Source
A more flexible method of installing Git is to compile the software from source.
This takes longer and will not be maintained through your package manager, but it will allow you to download the latest release and will give you some control over the options you include if you wish to customize.
Before you begin, you need to install the software that Git depends on.
This is all available in the default repositories, so we can update our local package index and then install the packages.
After you have installed the necessary dependencies, create a temporary directory and move into it. This is where we will download our Git tarball.
From the Git project website, we can navigate to the Red Hat Linux distribution tarball list available at https: / / mirrors.edge.kernel.org / pub / software / scm / git / and download the version you would like.
At the time of writing, the most recent version is 2.26.0, so we will download that for demonstration purpposes.
We "ll use curl and output the file we download to git.tar.gz.
Unpack the compressed tarball file:
Next, move into the new Git directory:
Now, you can make the package and install it by typing these two commands:
With this complete, you can be sure that your install was successful by checking the version.
With Git successfully installed, you can now complete your setup.
Setting Up Git
Now that you have Git installed, you should configure it so that the generated commit messages will contain your correct information.
This can be achieved by using the git config command.
Specifically, we need to provide our name and email address because Git embeds this information into each commit we do.
We can go ahead and add this information by typing:
We can display all of the configuration items that have been set by typing:
The information you enter is stored in your Git configuration file, which you can optionally edit by hand with a text editor like this:
Press ESC then: q to exit the text editor.
There are many other options that you can set, but these are the two essential ones needed.
If you skip this step, you "ll likely see warnings when you commit to Git.
This makes more work for you because you will then have to revise the commits you have done with the corrected information.
You should now have Git installed and ready to use on your system.
To learn more about how to use Git, check out these articles and series:
How To Install Python 3 and Set Up a Programming Environment on CentOS 8
4083
A versatile programming language, Python can be used for many different programming projects.
Inspired by the British comedy group Monty Python, the development team behind Python wanted to make a language that was fun to use.
An increasingly popular language with many different applications, Python is a great choice for beginners and experienced developers alike.
This tutorial will guide you through installing Python 3 on a CentOS 8 cloud server and setting up a programming environment via the command line.
Step 1 - Preparing the System
Before we begin with the installation, let "s make sure to update the default system applications to ensure we have the latest versions available.
Let "s first make sure that our package manager is up to date by running this command:
Once everything is installed, our setup is in place and we can go on to install Python 3.
Step 2 - Installing and Setting Up Python 3
CentOS is derived from RHEL (Red Hat Enterprise Linux), which has stability as its primary focus.
Because of this, tested and stable versions of applications are what is most commonly found on the system and in downloadable packages, so using the CentOS package manager you will find earlier versions of Python than the most recent release.
When this process is complete, we can check to make sure that the installation was successful by checking for its version number with the python3 command:
With a version of Python 3 successfully installed, we will receive the following output:
Next, we "ll install the CentOS Development Tools, which are used to allow you to build and compile software from source code:
With that installed, we "ll go over how to set up Python development projects in the next section.
Step 3 - Setting Up a Virtual Environment
With Python installed and our system set up, we can go on to create our programming environment with venv.
Virtual environments enable you to have an isolated space on your computer for Python projects, ensuring that each of your projects can have its own set of dependencies that won "t disrupt any of your other projects.
Setting up a programming environment provides us with greater control over our Python projects, as well as over different packages and versions.
Each environment is essentially a directory or folder on your server that has a few scripts to set it up as an environment.
Choose which directory you would like to put your Python programming environments in, or create a new directory with mkdir, as in:
Once you are in the directory where you would like the environments to live, you can create an environment by running the following command.
You should use an environment name that makes sense for you, here we are calling it my _ env.
In this case the environment is < ^ > my _ env < ^ >, and this new directory contains a few items that we can display if we use the ls command in that directory:
Together, these files work to isolate your Python work from the broader context of your local machine, so that system files and project files don "t mix.
To use this environment, you need to activate it, which you can do by typing the following command that calls the activate script in the bin directory:
Your prompt will now be prefixed with the name of your environment, in this case it is called < ^ > my _ env < ^ >:
The Python package manager pip is already installed.
A tool for use with Python, we will use pip to install and manage programming packages we may want to use in our development projects.
You can install Python packages by typing:
So if you would like to install NumPy, you can do so with the command pip install numpy.
< $> note Note: Within the virtual environment, you can use the command python instead of python3, and pip instead of pip3.
If you use Python 3 or pip3 on your machine outside of an environment, you will need to use the python3 and pip3 commands exclusively.
Step 4 - Creating a "Hello, World!"
Program
Now that we have our virtual environment set up, let "s create the traditional" Hello, World! "
program to test our installation.
This will make sure that our environment is working and gives us the opportunity to become more familiar with Python if we aren "t already.
To do this, we "ll open up a command-line text editor such as vi and create a new file:
Once the text file opens up in our terminal window, we will have to type i to enter insert mode, and then we can write our first program:
Now press ESC to leave insert mode.
Next, type: x then ENTER to save and exit the file.
We are now ready to run our program:
The hello.py program that you just created should cause the terminal to produce the following output:
At this point you have a Python 3 programming environment set up on your CentOS 8 server and can begin a coding project!
With your machine ready for software development, you can continue to learn more about coding in Python by following along with our How To Code in Python series, or downloading the free HowTo Code in Python eBook.
To explore machine learning projects in particular, refer to our Python Machine Learning Projects eBook.
How To Run Multiple PHP Versions on One Server Using Apache and PHP-FPM on Debian 10
4049
One Debian 10 server with at least 1GB of RAM set up by following the Initial Server Setup with Debian 10, including a sudo non-root user and a firewall.
An Apache web server set up and configured by following How to Install the Apache Web Server on Debian 10.
A domain name configured to point to your Debian 10 server.
But to accomplish this, you will first need to add the sury php repository to your system.
First install several required packages including curl, wget, and gnupg2:
The above packages will allow you to access the sury php repository, and to do so securely. sury php is a third-party repository or PPA (personal package archive).
It offers PHP 7.4, 7.3, 7.2, 7.1, and 7.0 for the Debian operating system.
It also offers more up-to-date versions of PHP than the official Debian 10 repositories, and will allow you to install multiple versions of PHP on the same system.
Next, import the package's key:
Now add the sury php repository to your system:
Install php7.2, php7.2-fpm, php7.2-mysql, and libapache2-mod-php7.2.
And then verify the status of the php7.2-fpm service:
You now have a single Debian 10 server handling two websites with two different PHP versions.
How To Set Up and Configure an OpenVPN Server on CentOS 8
3993
In this tutorial, you will set up OpenVPN on a CentOS 8 server, and then configure it to be accessible from a client machine.
One CentOS 8 server with a sudo non-root user and a firewall enabled.
To set this up, you can follow our Initial Server Setup with CentOS 8 tutorial.
A separate CentOS 8 server set up as a private Certificate Authority (CA), which we will refer to as the CA Server throughout this guide.
After executing the steps from the Initial Server Setup Guide on this server, you can follow steps 1 to 3 of our guide on How To Set Up and Configure a Certificate Authority (CA) on CentOS 8 to accomplish that.
For this reason, this guide assumes that your CA is on a separate CentOS 8 server that also has a non-root user with sudo privileges and a basic firewall enabled.
With these prerequisites in place, you are ready to begin setting up and configuring an OpenVPN Server on CentOS 8.
See How to Set Up SSH Keys on CentOS 8 for instructions on how to perform either of these solutions.
However, OpenVPN and Easy-RSA are not available by default in CentOS 8, so you will need to enable the Extra Packages for Enterprise Linux (EPEL) repository.
Log in to your OpenVPN Server as the non-root sudo user that you created during the initial setup steps and run the following:
First you will cd into the easy-rsa directory, then you will create and edit the vars file with your preferred text editor.
These lines will ensure that your private keys and certificate requests are configured to use modern Elliptic Curve Cryptography (ECC) to generate keys and secure signatures for your clients and OpenVPN server.
Now log in to the CA server as the non-root user that owns the easy-rsa directory, where you created your PKI.
Import the certificate request using the easyrsa script:
Next, we want OpenVPN to run with no privileges once it has started, so we need to tell it to run with a user and group of nobody.
To enable this, find and uncomment the user nobody and group nobody lines by removing the; sign from the beginning of each line:
Then add the following line at the top of the file:
Assuming you followed the prerequisites at the start of this tutorial, you should already have firewalld installed and running on your server.
To allow OpenVPN through the firewall, you "ll need to know what your active firewalld zone is.
Find this with the following command:
If you do not see a trusted zone that lists the tun0 interface, run the following commands to add the VPN device to that zone:
Next, add the openvpn service to the list of services allowed by firewalld within your active zone, and then make that setting permanent by running the command again but with the --permanent option added:
To apply the changes on the firewall, run:
You can now check that the service was added correctly with the following command:
Next, we "ll add a masquerade rule to the firewall.
Masquerading allows your OpenVPN server to translate your OpenVPN clients "addresses into the server" s own public address, and then do the reverse with traffic that is sent back to clients.
This process is also known as Network Address Translation (NAT).
Add masquerade rules with the following commands:
You can check that the masquerade was added correctly with this command:
Next, you'll need to create the specific masquerade rule for your OpenVPN subnet only.
You can do this by first creating a shell variable (< ^ > DEVICE < ^ > in our example) which will represent the primary network interface used by your server, and then using that variable to permanently add the routing rule:
Be sure to reload firewalld so that all your changes take effect:
The commands with the --permanent flag will ensure that the rules will persist across reboots.
The firewall-cmd --reload command makes sure that all the outstanding changes to the firewall are applied.
To do so, follow the example in the prerequisite tutorial on How to Set Up and Configure a Certificate Authority on CentOS 8 under the Revoking a Certificate section.
Once the profile is added, you will see a screen like this:
You'll see real time stats of your connection and traffic being routed through your OpenVPN server:
The OpenVPN Android app connected to the VPN
How to Install and Configure Ansible on Ubuntu 18.04 Quickstart
4763
In this guide, we will discuss how to install and configure Ansible on an Ubuntu 18.04 server.
For a more detailed version of this tutorial, with more explanations of each step, please refer to How to Install and Configure Ansible on Ubuntu 18.04.
One Ansible Control Node: an Ubuntu 18.04 system where Ansible will be installed.
This can be a remote server or a local machine.
One or more Ansible Hosts: one or more Ubuntu 18.04 servers that are accessible from your Control Node via SSH.
Step 1 - Install Ansible
From your control node, run the following command to include the official project's PPA (personal package archive) in your system's list of sources:
Refresh your system's package index with:
Following this update, you can install the Ansible software with:
Step 2 - Set Up the Inventory File
To edit the contents of your default Ansible inventory, open the / etc / ansible / hosts file using your text editor of choice:
The default inventory file provided by the Ansible installation contains a number of examples that you can use as references for setting up your inventory.
The following example defines a group named [servers] with three different servers in it, each identified by a custom alias: server1, server2, and server3.
Be sure to replace the highlighted IPs with the IP addresses of your Ansible hosts.
The all: vars subgroup sets the ansible _ python _ interpreter host parameter that will be valid for all hosts in this inventory.
This parameter makes sure the remote server uses the / usr / bin / python3 Python 3 executable instead of / usr / bin / python (Python 2.7), which is not present on recent Ubuntu versions.
Don't forget to save and close the file when you're finished.
Step 3 - Test Connection
You can use the -u argument to specify the remote system user.
When not provided, Ansible will try to connect as your current system user on the control node.
From your Ansible control node, run:
You should get output similar to this:
If this is the first time you're connecting to these servers via SSH, you'll be asked to confirm the authenticity of the hosts you're connecting to via Ansible.
When prompted, type yes and then hit ENTER to confirm.
Once you get a "pong" reply back from a host, it means you're ready to run Ansible commands and playbooks on that server.
How to Install and Configure Ansible on Ubuntu 18.04
How to Use Ansible: a Reference Guide
Configuration Management 101: Writing Ansible Playbooks
How To Install and Configure Elasticsearch on Ubuntu 18.04
5242
An Ubuntu 18.04 server with 4GB RAM and 2 CPUs set up with a non-root sudo user.
You can achieve this by following the Initial Server Setup with Ubuntu 18.04
To configure Elasticsearch, we will edit its configuration files.
Elasticsearch has three configuration files:
elasticsearch.yml for configuring Elasticsearch, the main configuration file
jvm.options for configuring the Elasticsearch Java Virtual Machine (JVM) settings
log4j2.properties for configuring Elasticsearch logging
For this tutorial, we are interested in the elasticsearch.yml file, where most configuration options are stored.
This firewall should already be enabled if you followed the steps in the prerequisite Initial Server Setup with Ubuntu 18.04 tutorial.
If you have specified the rules correctly, the output should look like this:
Since the original release of Elasticsearch, Elastic has developed three additional tools - Logstash, Kabana, and Beats - to be used in conjunction with Elasticsearch as part of the Elastic Stack.
Used together, these tools allow you to search, analyze, and visualize logs generated from any source and in any format in a practice known as centralized logging.
To get started with the Elastic Stack on Ubuntu 18.04, please see our guide How To Install Elasticsearch, Logstash, and Kibana (Elastic Stack) on Ubuntu 18.04.
How To Install Linux, Apache, MariaDB, PHP (LAMP) Stack on CentOS 8 Quickstart
4762
In this tutorial, you'll install a LAMP stack on a CentOS 8 server.
Although MySQL is available from the default repositories in CentOS 8, this guide will walk through the process of setting up a LAMP stack with MariaDB as the database management system.
For a more detailed version of this tutorial, with more explanations of each step, please refer to How To Install Linux, Apache, MySQL, PHP (LAMP) Stack on CentOS 8.
Step 4 - Test PHP with Apache
The following command will change the ownership of the default Apache document root to a user and group called < ^ > sammy < ^ >:
First, you might want to install nano, a more user-friendly text editor, since that doesn't come installed with CentOS 8 by default:
To test whether our web server can correctly display content generated by a PHP script, go to your browser and access your server hostname or IP address, followed by / info.php:
How To Install Linux, Apache, MySQL, PHP (LAMP) Stack on CentOS 8
How To Install MySQL on Ubuntu 20.04 Quickstart
5244
MySQL is an open-source database management system, commonly installed as part of the popular LAMP (Linux, Apache, MySQL, PHP / Python / Perl) stack.
It implements the relational model and uses Structured Query Language (better known as SQL) to manage its data.
This quickstart tutorial will explain how to install MySQL version 8 on an Ubuntu 20.04 server.
One Ubuntu 20.04 server with a non-root administrative user and a firewall configured with UFW.
To set up, follow our initial server setup guide for Ubuntu 20.04.
To install MySQL, first update your server's package index if you've not done so recently:
Then install the mysql-server package:
Step 2 - Configuring MySQL
Run MySQL's included security script with sudo:
This will take you through a series of prompts where you can make some changes to your MySQL installation "s security options.
If you elect to set up the Validate Password Plugin, the script will ask you to choose a password validation level, with the weakest being 0 and the strongest being 2:
The next prompt will be to enter and confirm a password for the MySQL root user:
If you're satisfied with your password, enter Y to continue the script:
In order to use a password to connect to MySQL as root, you will need to switch its authentication method from the default authentication plugin auth _ socket to another one, such as caching _ sha2 _ password or mysql _ native _ password.
From there, run an ALTER USER statement to change which authentication plugin it uses and set a new password.
Be sure to change < ^ > password < ^ > to a strong password of your choosing, and be aware that this command will change the root password you set in Step 2:
< $> note Note: caching _ sha2 _ password is MySQL's preferred authentication plugin, as it provides more secure password encryption than the older, but still widely used, mysql _ native _ password.
However, many PHP applications - phpMyAdmin, for example - don't work reliably with caching _ sha2 _ password.
If you plan to use this database with a PHP application, you may want to set root to authenticate with mysql _ native _ password instead:
Alternatively, you could connect to MySQL with a dedicated user instead of root.
To create such a user, open up the MySQL shell once again:
< $> note Note: If you have password authentication enabled for root, as described in the preceding paragraphs, you will need to run the following instead:
From there, create a new user and give it a strong password:
Then, grant your new user the appropriate privileges.
For example, you could grant the user privileges to all tables within the database, as well as the power to add, change, and remove user privileges, with this command:
Following this, you can exit the MySQL shell:
You now have a basic MySQL setup installed on your server.
Here are a few examples of next steps you can take:
Set up a LAMP stack
How To Install MySQL on Ubuntu 20.04
5240
A previous version of this tutorial was written by Hazel Virdó
This tutorial will go over how to install MySQL version 8 on an Ubuntu 20.04 server.
By completing it, you will have a working relational database that you can use to build your next website or application.
On Ubuntu 20.04, you can install MySQL using the APT package repository.
At the time of this writing, the version of MySQL available in the default Ubuntu repository is version < ^ > 8.0.19 < ^ >.
To install it, update the package index on your server if you've not done so recently:
This will install MySQL, but will not prompt you to set a password or make any other configuration changes.
Because this leaves your installation of MySQL insecure, we will address this next.
For fresh installations of MySQL, you'll want to run the DBMS's included security script.
This script changes some of the less secure default options for things like remote root logins and sample users.
Run the security script with sudo:
Note that even though you "ve set a password for the root MySQL user, this user is not configured to authenticate with a password when connecting to the MySQL shell.
If you "d like, you can adjust this setting by following Step 3.
In Ubuntu systems running MySQL 5.7 (and later versions), the root MySQL user is set to authenticate using the auth _ socket plugin by default rather than with a password.
In order to use a password to connect to MySQL as root, you will need to switch its authentication method from auth _ socket to another plugin, such as caching _ sha2 _ password or mysql _ native _ password.
To configure the root account to authenticate with a password, run an ALTER USER statement to change which authentication plugin it uses and set a new password.
< $> note Note: The previous ALTER USER statement sets the root MySQL user to authenticate with the caching _ sha2 _ password plugin.
Per the official MySQL documentation, caching _ sha2 _ password is MySQL's preferred authentication plugin, as it provides more secure password encryption than the older, but still widely used, mysql _ native _ password.
You can see in this example output that the root MySQL user now authenticates using caching _ sha2 _ password.
Alternatively, some may find that it better suits their workflow to connect to MySQL with a dedicated user.
< $> note Note: If you have password authentication enabled for root, as described in the preceding paragraphs, you will need to use a different command to access the MySQL shell.
The following will run your MySQL client with regular user privileges, and you will only gain administrator privileges within the database by authenticating:
Note that, at this point, you do not need to run the FLUSH PRIVILEGES command again.
This command is only needed when you modify the grant tables using statements like INSERT, UPDATE, or DELETE.
Because you created a new user, instead of modifying an existing one, FLUSH PRIVILEGES is unnecessary here.
Following this, exit the MySQL shell:
Finally, let's test the MySQL installation.
Step 4 - Testing MySQL
Regardless of how you installed it, MySQL should have started running automatically.
If MySQL isn't running, you can start it with sudo systemctl start mysql.
For example, this command says to connect to MySQL as root (-u root), prompt for a password (-p), and return the version.
This means MySQL is up and running.
How To Make a Web Application Using Flask in Python 3
4100
Flask is a small and lightweight Python web framework that provides useful tools and features that make creating web applications in Python easier.
It gives developers flexibility and is a more accessible framework for new developers since you can build a web application quickly using only a single Python file.
Flask is also extensible and doesn't force a particular directory structure or require complicated boilerplate code before getting started.
As part of this tutorial, you'll use the Bootstrap toolkit to style your application so it is more visually appealing.
Bootstrap will help you incorporate responsive web pages in your web application so that it also works well on mobile browsers without writing your own HTML, CSS, and JavaScript code to achieve these goals.
The toolkit will allow you to focus on learning how Flask works.
Flask uses the Jinja template engine to dynamically build HTML pages using familiar Python concepts such as variables, loops, lists, and so on.
You'll use these templates as part of this project.
In this tutorial, you'll build a small web blog using Flask and SQLite in Python 3. Users of the application can view all the posts in your database and click on the title of a post to view its contents with the ability to add a new post to the database and edit or delete an existing post.
Before you start following this guide, you will need:
A local Python 3 programming environment, follow the tutorial for your distribution in How To Install and Set Up a Local Programming Environment for Python 3 series for your local machine.
In this tutorial we'll call our project directory flask _ blog.
An understanding of Python 3 concepts, such as data types, conditional statements, for loops, functions, and other such concepts.
If you are not familiar with Python, check out our How To Code in Python 3 series.
Step 1 - Installing Flask
In this step, you'll activate your Python environment and install Flask using the pip package installer.
If you haven't already activated your programming environment, make sure you're in your project directory (< ^ > flask _ blog < ^ >) and use the following command to activate the environment:
Once your programming environment is activated, your prompt will now have an < ^ > env < ^ > prefix that may look as follows:
This prefix is an indication that the environment env is currently active, which might have another name depending on how you named it during creation.
< $> note Note: You can use Git, a version control system, to effectively manage and track the development process for your project.
To learn how to use Git, you might want to check out our Introduction to Git Installation Usage and Branches article.
If you are using Git, it is a good idea to ignore the newly created env directory in your .gitignore file to avoid tracking files not related to the project.
Now you'll install Python packages and isolate your project code away from the main Python system installation.
You'll do this using pip and python.
To install Flask, run the following command:
Once the installation is complete, run the following command to confirm the installation:
You use the python command line interface with the option -c to execute Python code.
Next you import the flask package with import flask; then print the Flask version, which is provided via the flask. _ _ version _ _ variable.
The output will be a version number similar to the following:
You've created the project folder, a virtual environment, and installed Flask.
You're now ready to move on to setting up your base application.
Step 2 - Creating a Base Application
Now that you have your programming environment set up, you'll start using Flask.
In this step, you'll make a small web application inside a Python file and run it to start the server, which will display some information on the browser.
In your flask _ blog directory, open a file named hello.py for editing, use nano or your favorite text editor:
This hello.py file will serve as a minimal example of how to handle HTTP requests.
Inside it, you'll import the Flask object, and create a function that returns an HTTP response.
Write the following code inside hello.py:
In the preceding code block, you first import the Flask object from the flask package.
You then use it to create your Flask application instance with the name app. You pass the special variable _ _ name _ _ that holds the name of the current Python module.
It's used to tell the instance where it's located - you need this because Flask sets up some paths behind the scenes.
Once you create the app instance, you use it to handle incoming web requests and send responses to the user. @ app.route is a decorator that turns a regular Python function into a Flask view function, which converts the function's return value into an HTTP response to be displayed by an HTTP client, such as a web browser.
You pass the value '/' to @ app.route () to signify that this function will respond to web requests for the URL /, which is the main URL.
The hello () view function returns the string 'Hello, World!'
as a response.
To run your web application, you'll first tell Flask where to find the application (the hello.py file in your case) with the FLASK _ APP environment variable:
Then run it in development mode with the FLASK _ ENV environment variable:
Lastly, run the application using the flask run command:
Once the application is running the output will be something like this:
The preceding output has several pieces of information, such as:
The name of the application you're running.
The environment in which the application is being run.
Debug mode: on signifies that the Flask debugger is running.
This is useful when developing because it gives us detailed error messages when things go wrong, which makes troubleshooting easier.
The application is running locally on the URL http: / / 127.0.0.1: 5000 /, 127.0.0.1 is the IP that represents your machine's localhost and: 5000 is the port number.
Open a browser and type in the URL http: / / 127.0.0.1: 5000 /, you will receive the string Hello, World!
as a response, this confirms that your application is successfully running.
< $> warning Warning Flask uses a simple web server to serve our application in a development environment, which also means that the Flask debugger is running to make catching errors easier.
This development server should not be used in a production deployment.
See the Deployment Options page on the Flask documentation for more information, you can also check out this Flask deployment tutorial.
You can now leave the development server running in the terminal and open another terminal window.
Move into the project folder where hello.py is located, activate the virtual environment, set the environment variables FLASK _ ENV and FLASK _ APP, and continue to the next steps.
(These commands are listed earlier in this step.)
< $> note Note: When opening a new terminal, it is important to remember activating the virtual environment and setting the environment variables FLASK _ ENV and FLASK _ APP.
While a Flask application's development server is already running, it is not possible to run another Flask application with the same flask run command.
This is because flask run uses the port number 5000 by default, and once it is taken, it becomes unavailable to run another application on so you would receive an error similar to the following:
To solve this problem, either stop the server that's currently running via CTRL + C, then run flask run again, or if you want to run both at the same time, you can pass a different port number to the -p argument, for example, to run another application on port 5001 use the following command:
You now have a small Flask web application.
You've run your application and displayed information on the web browser.
Next, you'll use HTML files in your application.
Step 3 - Using HTML templates
Currently your application only displays a simple message without any HTML.
Web applications mainly use HTML to display information for the visitor, so you'll now work on incorporating HTML files in your app, which can be displayed on the web browser.
Flask provides a render _ template () helper function that allows use of the Jinja template engine.
This will make managing HTML much easier by writing your HTML code in .html files as well as using logic in your HTML code.
You'll use these HTML files, (templates) to build all of your application pages, such as the main page where you'll display the current blog posts, the page of the blog post, the page where the user can add a new post, and so on.
In this step, you'll create your main Flask application in a new file.
First, in your flask _ blog directory, use nano or your favorite editor to create and edit your app.py file.
This will hold all the code you'll use to create the blogging application:
In this new file, you'll import the Flask object to create a Flask application instance as you previously did.
You'll also import the render _ template () helper function that lets you render HTML template files that exist in the templates folder you're about to create.
The file will have a single view function that will be responsible for handling requests to the main / route.
The index () view function returns the result of calling render _ template () with index.html as an argument, this tells render _ template () to look for a file called index.html in the templates folder.
Both the folder and the file do not yet exist, you will get an error if you were to run the application at this point.
You'll run it nonetheless so you're familiar with this commonly encountered exception.
You'll then fix it by creating the needed folder and file.
Stop the development server in your other terminal that runs the hello application with CTRL + C.
Before you run the application, make sure you correctly specify the value for the FLASK _ APP environment variable, since you're no longer using the application hello:
Opening the URL http: / / 127.0.0.1: 5000 / in your browser will result in the debugger page informing you that the index.html template was not found.
The main line in the code that was responsible for this error will be highlighted.
In this case, it is the line return render _ template (' index.html ').
If you click this line, the debugger will reveal more code so that you have more context to help you solve the problem.
The Flask Debugger
To fix this error, create a directory called templates inside your flask _ blog directory.
Then inside it, open a file called index.html for editing:
Next, add the following HTML code inside index.html:
Save the file and use your browser to navigate to http: / / 127.0.0.1: 5000 / again, or refresh the page.
This time the browser should display the text Welcome to FlaskBlog in an < h1 > tag.
In addition to the templates folder, Flask web applications also typically have a static folder for hosting static files, such as CSS files, JavaScript files, and images the application uses.
You can create a style.css style sheet file to add CSS to your application.
First, create a directory called static inside your main flask _ blog directory:
Then create another directory called css inside the static directory to host .css files.
This is typically done to organize static files in dedicated folders, as such, JavaScript files typically live inside a directory called js, images are put in a directory called images (or img), and so on.
The following command will create the css directory inside the static directory:
Then open a style.css file inside the css directory for editing:
Add the following CSS rule to your style.css file:
The CSS code will add a border, change the color to brown, center the text, and add a little padding to < h1 > tags.
Next, open the index.html template file for editing:
You'll add a link to the style.css file inside the < head > section of the index.html template file:
Here you use the url _ for () helper function to generate the appropriate location of the file.
The first argument specifies that you're linking to a static file and the second argument is the path of the file inside the static directory.
Upon refreshing the index page of your application, you will notice that the text Welcome to FlaskBlog is now in brown, centered, and enclosed inside a border.
You can use the CSS language to style the application and make it more appealing using your own design.
However, if you're not a web designer, or if you aren't familiar with CSS, then you can use the Bootstrap toolkit, which provides easy-to-use components for styling your application.
In this project, we'll use Bootstrap.
You might have guessed that making another HTML template would mean repeating most of the HTML code you already wrote in the index.html template.
You can avoid unnecessary code repetition with the help of a base template file, which all of your HTML files will inherit from.
See Template Inheritance in Jinja for more information.
To make a base template, first create a file called base.html inside your templates directory:
Type the following code in your base.html template:
Most of the code in the preceding block is standard HTML and code required for Bootstrap.
The < meta > tags provide information for the web browser, the < link > tag links the Bootstrap CSS files, and the < script > tags are links to JavaScript code that allows some additional Bootstrap features, check out the Bootstrap documentation for more.
However, the following highlighted parts are specific to the Jinja template engine:
< ^ > {% block title%} {% endblock%} < ^ >: A block that serves as a placeholder for a title, you'll later use it in other templates to give a custom title for each page in your application without rewriting the entire < head > section each time.
< ^ > {{url _ for (' index ')}} < ^ >: A function call that will return the URL for the index () view function.
This is different from the past url _ for () call you used to link a static CSS file, because it only takes one argument, which is the view function's name, and links to the route associated with the function instead of a static file.
< ^ > {% block content%} {% endblock%} < ^ >: Another block that will be replaced by content depending on the child template (templates that inherit from base.html) that will override it.
Now that you have a base template, you can take advantage of it using inheritance.
Open the index.html file:
Then replace its contents with the following:
In this new version of the index.html template, you use the {% extends%} tag to inherit from the base.html template.
You then extend it via replacing the content block in the base template with what is inside the content block in the preceding code block.
This content block contains an < h1 > tag with the text Welcome to FlaskBlog inside a title block, which in turn replaces the original title block in the base.html template with the text Welcome to FlaskBlog.
This way, you can avoid repeating the same text twice, as it works both as a title for the page and a heading that appears below the navigation bar inherited from the base template.
Template inheritance also gives you the ability to reuse the HTML code you have in other templates (base.html in this case) without having to repeat it each time it is needed.
Save and close the file and refresh the index page on your browser.
You'll see your page with a navigation bar and styled title.
Index Page with Bootstrap
You've used HTML templates and static files in Flask.
You also used Bootstrap to start refining the look of your page and a base template to avoid code repetition.
In the next step, you'll set up a database that will store your application data.
Step 4 - Setting up the Database
In this step, you'll set up a database to store data, that is, the blog posts for your application.
You'll also populate the database with a few example entries.
You'll use a SQLite database file to store your data because the sqlite3 module, which we will use to interact with the database, is readily available in the standard Python library.
For more information about SQLite, check out this tutorial.
First, because data in SQLite is stored in tables and columns, and since your data mainly consists of blog posts, you first need to create a table called posts with the necessary columns.
You'll create a .sql file that contains SQL commands to create the posts table with a few columns.
You'll then use this file to create the database.
Open a file called schema.sql inside your flask _ blog directory:
Type the following SQL commands inside this file:
The first SQL command is DROP TABLE IF EXISTS posts;, this deletes any already existing tables named posts so you don't get confusing behavior.
Note that this will delete all of the content you have in the database whenever you use these SQL commands, so ensure you don't write any important content in the web application until you finish this tutorial and experiment with the final result.
Next, CREATE TABLE posts is used to create the posts table with the following columns:
id: An integer that represents a primary key, this will get assigned a unique value by the database for each entry (that is a blog post).
created: The time the blog post was created at.
NOT NULL signifies that this column should not be empty and the DEFAULT value is the CURRENT _ TIMESTAMP value, which is the time at which the post was added to the database.
Just like id, you don't need to specify a value for this column, as it will be automatically filled in.
title: The post title.
content: The post content.
Now that you have a SQL schema in the schema.sql file, you'll use it to create the database using a Python file that will generate an SQLite .db database file.
Open a file named init _ db.py inside the flask _ blog directory using your preferred editor:
And then add the following code.
You first import the sqlite3 module and then open a connection to a database file named database.db, which will be created once you run the Python file.
Then you use the open () function to open the schema.sql file.
Next you execute its contents using the executescript () method that executes multiple SQL statements at once, which will create the posts table.
You create a Cursor object that allows you to use its execute () method to execute two INSERT SQL statements to add two blog posts to your posts table.
Finally, you commit the changes and close the connection.
Save and close the file and then run it in the terminal using the python command:
Once the file finishes execution, a new file called database.db will appear in your flask _ blog directory.
This means you've successfully set up your database.
In the next step, you'll retrieve the posts you inserted into your database and display them in your application's homepage.
Step 5 - Displaying All Posts
Now that you've set up your database, you can now modify the index () view function to display all the posts you have in your database.
Open the app.py file to make the following modifications:
For your first modification, you'll import the sqlite3 module at the top of the file:
Next, you'll create a function that creates a database connection and return it. Add it directly after the imports:
This get _ db _ connection () function opens a connection to the database.db database file, and then sets the row _ factory attribute to sqlite3.
Row so you can have name-based access to columns.
This means that the database connection will return rows that behave like regular Python dictionaries.
Lastly, the function returns the conn connection object you'll be using to access the database.
After defining the get _ db _ connection () function, modify the index () function to look like the following:
In this new version of the index () function, you first open a database connection using the get _ db _ connection () function you defined earlier.
Then you execute an SQL query to select all entries from the posts table.
You implement the fetchall () method to fetch all the rows of the query result, this will return a list of the posts you inserted into the database in the previous step.
You close the database connection using the close () method and return the result of rendering the index.html template.
You also pass the posts object as an argument, which contains the results you got from the database, this will allow you to access the blog posts in the index.html template.
With these modifications in place, save and close the app.py file.
Now that you've passed the posts you fetched from the database to the index.html template, you can use a for loop to display each post on your index page.
Then, modify it to look as follows:
Here, the syntax {% for post in posts%} is a Jinja for loop, which is similar to a Python for loop except that it has to be later closed with the {% endfor%} syntax.
You use this syntax to loop over each item in the posts list that was passed by the index () function in the line return render _ template (' index.html ', posts = posts).
Inside this for loop, you display the post title in an < h2 > heading inside an < a > tag (you'll later use this tag to link to each post individually).
You display the title using a literal variable delimiter ({{...
}}).
Remember that post will be a dictionary-like object, so you can access the post title with post ['title'].
You also display the post creation date using the same method.
Once you are done editing the file, save and close it. Then navigate to the index page in your browser.
You'll see the two posts you added to the database on your page.
Index Page with Posts Displayed
Now that you've modified the index () view function to display all the posts you have in the database on your application's homepage, you'll move on to display each post in a single page and allow users to link to each individual post.
Step 6 - Displaying a Single Post
In this step, you'll create a new Flask route with a view function and a new HTML template to display an individual blog post by its ID.
By the end of this step, the URL http: / / 127.0.0.1: 5000 / 1 will be a page that displays the first post (because it has the ID 1).
The http: / / 127.0.0.1: 5000 / < ^ > ID < ^ > URL will display the post with the associated < ^ > ID < ^ > number if it exists.
Open app.py for editing:
Since you'll need to get a blog post by its ID from the database in multiple locations later in this project, you'll create a standalone function called get _ post ().
You can call it by passing it an ID and receive back the blog post associated with the provided ID, or make Flask respond with a 404 Not Found message if the blog post does not exist.
To respond with a 404 page, you need to import the abort () function from the Werkzeug library, which was installed along with Flask, at the top of the file:
Then, add the get _ post () function right after the get _ db _ connection () function you created in the previous step:
This new function has a post _ id argument that determines what blog post to return.
Inside the function, you use the get _ db _ connection () function to open a database connection and execute a SQL query to get the blog post associated with the given post _ id value.
You add the fetchone () method to get the result and store it in the post variable then close the connection.
If the post variable has the value None, meaning no result was found in the database, you use the abort () function you imported earlier to respond with a 404 error code and the function will finish execution.
If however, a post was found, you return the value of the post variable.
Next, add the following view function at the end of the app.py file:
In this new view function, you add a variable rule < int: post _ id > to specify that the part after the slash (/) is a positive integer (marked with the int converter) that you need to access in your view function.
Flask recognizes this and passes its value to the post _ id keyword argument of your post () view function.
You then use the get _ post () function to get the blog post associated with the specified ID and store the result in the post variable, which you pass to a post.html template that you'll soon create.
Save the app.py file and open a new post.html template file for editing:
Type the following code in this new post.html file.
This will be similar to the index.html file, except that it will only display a single post, in addition to also displaying the contents of the post:
You add the title block that you defined in the base.html template to make the title of the page reflect the post title that is displayed in an < h2 > heading at the same time.
You can now navigate to the following URLs to see the two posts you have in your database, along with a page that tells the user that the requested blog post was not found (since there is no post with an ID number of 3 so far):
Going back to the index page, you'll make each post title link to its respective page.
You'll do this using the url _ for () function.
First, open the index.html template for editing:
Then change the value of the href attribute from # to {{url _ for (' post ', post _ id = post [' id '])}} so that the for loop will look exactly as follows:
Here, you pass' post 'to the url _ for () function as a first argument.
This is the name of the post () view function and since it accepts a post _ id argument, you give it the value post ['id'].
The url _ for () function will return the proper URL for each post based on its ID.
The links on the index page will now function as expected.
With this, you've now finished building the part of the application responsible for displaying the blog posts in your database.
Next, you'll add the ability to create, edit, and delete blog posts to your application.
Step 7 - Modifying Posts
Now that you've finished displaying the blog posts that are present in the database on the web application, you need to allow the users of your application to write new blog posts and add them to the database, edit the existing ones, and delete unnecessary blog posts.
Creating a New Post
Up to this point, you have an application that displays the posts in your database but provides no way of adding a new post unless you directly connect to the SQLite database and add one manually.
In this section, you'll create a page on which you will be able to create a post by providing its title and content.
Open the app.py file for editing:
First, you'll import the following from the Flask framework:
The global request object to access incoming request data that will be submitted via an HTML form.
The url _ for () function to generate URLs.
The flash () function to flash a message when a request is processed.
The redirect () function to redirect the client to a different location.
Add the imports to your file like the following:
The flash () function stores flashed messages in the client "s browser session, which requires setting a secret key.
This secret key is used to secure sessions, which allow Flask to remember information from one request to another, such as moving from the new post page to the index page.
The user can access the information stored in the session, but cannot modify it unless they have the secret key, so you must never allow anyone to access your secret key.
See the Flask documentation for sessions for more information.
To set a secret key, you'll add a SECRET _ KEY configuration to your application via the app.config object.
Add it directly following the app definition before defining the index () view function:
Remember that the secret key should be a long random string.
After setting a secret key, you'll create a view function that will render a template that displays a form you can fill in to create a new blog post.
Add this new function at the bottom of the file:
This creates a / create route that accepts both GET and POST requests.
GET requests are accepted by default.
To also accept POST requests, which are sent by the browser when submitting forms, you'll pass a tuple with the accepted types of requests to the methods argument of the @ app.route () decorator.
To create the template, open a file called create.html inside your templates folder:
Add the following code inside this new file:
Most of this code is standard HTML.
It will display an input box for the post title, a text area for the post content, and a button to submit the form.
The value of the post title input is {{request.form ['title']}} and the text area has the value {{request.form ['content']}}, this is done so that the data you enter does not get lost if something goes wrong.
For example, if you write a long post and you forget to give it a title, a message will be displayed informing you that the title is required.
This will happen without losing the post you wrote since it will be stored in the request global object that you have access to in your templates.
Now, with the development server running, use your browser to navigate to the / create route:
You will see a Create a New Post page with a box for a title and content.
Create a New Post Page
This form submits a POST request to your create () view function.
However, there is no code to handle a POST request in the function yet, so nothing happens after filling in the form and submitting it.
You'll handle the incoming POST request when a form is submitted.
You'll do this inside the create () view function.
You can separately handle the POST request by checking the value of request.method.
When its value is set to 'POST' it means the request is a POST request, you'll then proceed to extract submitted data, validate it, and insert it into your database.
Modify the create () view function to look exactly as follows:
In the if statement you ensure that the code following it is only executed when the request is a POST request via the comparison request.method = = 'POST'.
You then extract the submitted title and content from the request.form object that gives you access to the form data in the request.
If the title is not provided, the condition if not title would be fulfilled, displaying a message to the user informing them that the title is required.
If, on the other hand, the title is provided, you open a connection with the get _ db _ connection () function and insert the title and the content you received into the posts table.
You then commit the changes to the database and close the connection.
After adding the blog post to the database, you redirect the client to the index page using the redirect () function passing it the URL generated by the url _ for () function with the value 'index' as an argument.
Now, navigate to the / create route using your web browser:
Fill in the form with a title of your choice and some content.
Once you submit the form, you will see the new post listed on the index page.
Lastly, you'll display flashed messages and add a link to the navigation bar in the base.html template to have easy access to this new page.
Open the template file:
Edit the file by adding a new < li > tag following the About link inside the < nav > tag.
Then add a new for loop directly above the content block to display the flashed messages below the navigation bar.
These messages are available in the special get _ flashed _ messages () function Flask provides:
The navigation bar will now have a New Post item that links to the / create route.
Editing a Post
For a blog to be up to date, you'll need to be able to edit your existing posts.
This section will guide you through creating a new page in your application to simplify the process of editing a post.
First, you'll add a new route to the app.py file.
Its view function will receive the ID of the post that needs to be edited, the URL will be in the format / < ^ > post _ id < ^ > / edit with the < ^ > post _ id < ^ > variable being the ID of the post.
Next, add the following edit () view function at the end of the file.
Editing an existing post is similar to creating a new one, so this view function will be similar to the create () view function:
The post you edit is determined by the URL and Flask will pass the ID number to the edit () function via the id argument.
You add this value to the get _ post () function to fetch the post associated with the provided ID from the database.
The new data will come in a POST request, which is handled inside the if request.method = = 'POST' condition.
Just like when you create a new post, you first extract the data from the request.form object then flash a message if the title has an empty value, otherwise, you open a database connection.
Then you update the posts table by setting a new title and new content where the ID of the post in the database is equal to the ID that was in the URL.
In the case of a GET request, you render an edit.html template passing in the post variable that holds the returned value of the get _ post () function.
You'll use this to display the existing title and content on the edit page.
Save and close the file, then create a new edit.html template:
Write the following code inside this new file:
This code follows the same pattern except for the {{request.form ['title'] or post ['title']}} and {{request.form ['content'] or post ['content']}} syntax.
This displays the data stored in the request if it exists, otherwise it displays the data from the post variable that was passed to the template containing current database data.
Now, navigate to the following URL to edit the first post:
You will see an Edit "First Post" page.
Edit a Post Page
Edit the post and submit the form, then make sure the post was updated.
You now need to add a link that points to the edit page for each post on the index page.
Open the index.html template file:
Edit the file to look exactly like the following:
Here you add an < a > tag to link to the edit () view function, passing in the post ['id'] value to link to the edit page of each post with the Edit link.
Deleting a Post
Sometimes a post no longer needs to be publicly available, which is why the functionality of deleting a post is crucial.
In this step you will add the delete functionality to your application.
First, you'll add a new / < ^ > ID < ^ > / delete route that accepts POST requests, similar to the edit () view function.
Your new delete () view function will receive the ID of the post to be deleted from the URL.
Open the app.py file:
Add the following view function at the bottom of the file:
This view function only accepts POST requests.
This means that navigating to the / < ^ > ID < ^ > / delete route on your browser will return an error because web browsers default to GET requests.
However you can access this route via a form that sends a POST request passing in the ID of the post you want to delete.
The function will receive the ID value and use it to get the post from the database with the get _ post () function.
Then you open a database connection and execute a DELETE FROM SQL command to delete the post.
You commit the change to the database and close the connection while flashing a message to inform the user that the post was successfully deleted and redirect them to the index page.
Note that you don't render a template file, this is because you'll just add a Delete button to the edit page.
Open the edit.html template file:
Then add the following < form > tag after the < hr > tag and directly before the {% endblock%} line:
You use the confirm () method to display a confirmation message before submitting the request.
Now navigate again to the edit page of a blog post and try deleting it:
At the end of this step, the source code of your project will look like the code on this page.
With this, the users of your application can now write new blog posts and add them to the database, edit, and delete existing posts.
This tutorial introduced essential concepts of the Flask Python framework.
You learned how to make a small web application, run it in a development server, and allow the user to provide custom data via URL parameters and web forms.
You also used the Jinja template engine to reuse HTML files and use logic in them.
At the end of this tutorial, you now have a fully functioning web blog that interacts with an SQLite database to create, display, edit, and delete blog posts using the Python language and SQL queries.
You can further develop this application by adding user authentication so that only registered users can create and modify blog posts, you may also add comments and tags for each blog post, and add file uploads to give users the ability to include images in the post.
See the Flask documentation for more information.
Flask has many community-made Flask extensions.
The following is a list of extensions you might consider using to make your development process easier:
Flask-Login: manages the user session and handles logging in and logging out and remembering logged-in users.
Flask-SQLAlchemy: simplifies using Flask with SQLAlchemy, a Python SQL toolkit and Object Relational Mapper for interacting with SQL databases.
Flask-Mail: helps with the task of sending email messages in your Flask application.
How To Monitor Server Health with Checkmk on Ubuntu 18.04
4102
As a systems administrator, it's a best practice to know the current state of your infrastructure and services.
Ideally, you want to notice failing disks or application downtimes before your users do.
Monitoring tools like Checkmk can help administrators detect these issues and maintain healthy servers.
Generally, monitoring software can track your servers' hardware, uptime, and service statuses, and it can raise alerts when something goes wrong.
In a very basic scenario, a monitoring system would alert you if any services go down.
In a more robust one, the notifications would come shortly after any suspicious signs arose, such as increased memory usage or an abnormal amount of TCP connections.
There are many monitoring solutions available offering varying degrees of complexity and feature sets, both free and commercial.
In many cases, the installation, configuration, and management of these tools is difficult and time-consuming.
Checkmk, however, is a monitoring solution that is both robust and simpler to install.
It is a self-contained software bundle that combines Nagios (a popular and open-source alerting service) with add-ons for gathering, monitoring, and graphing data. It also comes with Checkmk's web interface - a comprehensive tool that addresses many of Nagios's shortcomings.
It offers a user-friendly dashboard, a full-featured notification system, and a repository of easy-to-install monitoring agents for many Linux distributions.
If it weren't for Checkmk's web interface, we would have to use different views for different tasks and it wouldn't be possible to configure all these features without resorting to extensive file modifications.
In this guide we will set up Checkmk on an Ubuntu 18.04 server and monitor two separate hosts.
We will monitor the Ubuntu server itself as well as a separate CentOS 7 server, but we could use the same approach to add any number of additional hosts to our monitoring configuration.
One Ubuntu 18.04 server with a regular, non-root user with sudo privileges.
You can learn how to prepare your server by following this initial server setup tutorial.
One CentOS 7 server with a regular, non-root user with sudo privileges.
Step 1 - Installing Checkmk on Ubuntu
In order to use our monitoring site, we first must install Checkmk on the Ubuntu server.
This will give us all the tools we need.
Checkmk provides official ready-to-use Ubuntu package files that we can use to install the software bundle.
First, let's update the packages list so that we have the most recent version of the repository listings:
To browse the packages we can go to the package listing site.
Ubuntu 18.04, among others, can be selected in the page menu.
Now download the package:
Then install the newly downloaded package:
This command will install the Checkmk package along with all necessary dependencies, including the Apache web server that is used to provide web access to the monitoring interface.
After the installation completes, we now can access the omd command.
Try it out:
This omd command will output the following:
The omd command can manage all Checkmk instances on our server.
It can start and stop all the monitoring services at once, and we will use it to create our Checkmk instance.
First, however, we have to update our firewall settings to allow outside access to the default web ports.
Step 2 - Adjusting the Firewall Settings
Before we'll be able to work with Checkmk, it's necessary to allow outside access to the web server in the firewall configuration.
Assuming that you followed the firewall configuration steps in the prerequisites, you'll have a UFW firewall set up to restrict access to your server.
During installation, Apache registers itself with UFW to provide an easy way to enable or disable access to Apache through the firewall.
To allow access to Apache, use the following command:
Now verify the changes:
You'll see that Apache is listed among the allowed services:
This will allow us to access the Checkmk web interface.
In the next step, we'll create the first Checkmk monitoring instance.
Step 3 - Creating a Checkmk Monitoring Instance
Checkmk uses the concept of instances, or individual installations, to isolate multiple Checkmk copies on a server.
In most cases, only one copy of Checkmk is enough and that's how we will configure the software in this guide.
First we must give our new instance a name, and we will use < ^ > monitoring < ^ > throughout this text.
To create the instance, type:
The omd tool will set up everything for us automatically.
The command output will look similar to the following:
In this output the URL address, default username, and password for accessing our monitoring interface are highlighted.
The instance is now created, but it still needs to be started.
To start the instance, type:
Now all the necessary tools and services will be started at once.
At the end we we'll see an output verifying that all our services have started successfully:
The instance is up and running.
To access the Checkmk instance, open http: / / < ^ > your _ ubuntu _ server _ ip < ^ > / monitoring / in the web browser.
You will be prompted for a password.
Use the default credentials printed beforehand on the screen; we will change these defaults later on.
The Checkmk screen opens with a dashboard, which shows all our services and server statuses in lists, and it uses practical graphs resembling the Earth.
Straight after installation these are empty, but we will shortly make it display statuses for our services and systems.
Blank Checkmk dashboard
In the next step, we will change the default password to secure the site using this interface.
Step 4 - Changing Your Administrative Password
During installation, Checkmk generates a random password for the cmkadmin administrative user.
This password is meant to be changed upon installation, and as such it is often short and not very secure.
We can change this via the web interface.
First, open the Users page from the WATO - Configuration menu on the left.
The list will show all users that currently have access to the Checkmk site.
On a fresh installation it will list only two users.
The first one, automation, is intended for use with automated tools; the second is the cmkadmin user we used to log in to the site.
List of Checkmk users
Click on the pencil icon next to the cmkadmin user to change its details, including the password.
Edit form for Checkmk admin user
Update the password, add an admin email, and make any other desired changes.
After saving the changes we will be asked to log in again using our new credentials.
Do so and return to the dashboard, where there is one more thing we must do to fully apply our new configuration.
Once again open the Users page from the WATO - Configuration menu on the left.
The orange button in the top left corner labeled as 1 Change tells us that we have made some changes to the configuration of Checkmk, and that we need to save and activate them.
This will happen every time we change the configuration of our monitoring system, not only after editing a user's credentials.
To save and activate pending changes we have to click on this button and agree to activate the listed changes using the Activate affected option on the following screen.
List of Checkmk users after modifications Activate configuration changes confirmation screen Successfully activated configuration changes
After activating the changes the new user's data is written to the configuration files and it will be used by all the system's components.
Checkmk automatically takes care of notifying individual monitoring system components, reloading them when necessary, and managing all the needed configuration files.
The Checkmk installation is now ready for use.
In the next step, we will add the first host to our monitoring system.
Step 5 - Monitoring the First Host
We are now ready to monitor the first host.
To accomplish this, we will first install check-mk-agent on the Ubuntu server.
Then, we'll restrict access to the monitoring data using xinetd.
The components installed with Checkmk are responsible for receiving, storing, and presenting monitoring information.
They do not provide the information itself.
To gather the actual data, we will use Checkmk agent.
Designed specifically for the job, Checkmk agent is capable of monitoring all vital system components at once and reporting that information back to the Checkmk instance.
Installing the agent
The first host we will monitor will be < ^ > your _ ubuntu _ server < ^ > - the server on which we have installed the Checkmk instance itself.
To begin, we must install the Checkmk agent.
Packages for all major distributions, including Ubuntu, are available directly from the web interface.
Open the Monitoring Agents page from the WATO - Configuration menu on the left.
You will see the available agent downloads with the most popular packages under the first section labeled Packaged agents.
List of available packaged monitoring agents
The package check-mk-agent _ 1.6.0p8-1 _ all. < ^ > deb < ^ > is the one suited for Debian based distributions, including Ubuntu.
Copy the download link for that package from the web browser and use that address to download the package.
After downloading, install the package:
Now verify that the agent has been successfully installed:
The command will output a very long text that looks like gibberish but combines all vital information about the system in one place.
It is the output from this command that Checkmk uses to gather status data from monitored hosts.
Now, we'll restrict access to the monitoring data with xinetd.
Restricting Access to Monitoring Data Using xinetd
By default, the data from check _ mk _ agent is served using xinetd, a mechanism that outputs data on a certain network port upon accessing it. This means that we can access the check _ mk _ agent by using telnet to port 6556 (the default port for Checkmk) from any other computer on the internet unless our firewall configuration disallows it.
It is not a good security policy to publish vital information about servers to anyone on the internet.
We should allow only hosts that run Checkmk and are under our supervision to access this data, so that only our monitoring system can gather it.
If you have followed the initial server setup tutorial including the steps about setting up a firewall, then access to Checkmk agent is by default blocked.
It is, however, a good practice to enforce these access restrictions directly in the service configuration and not rely only on the firewall to guard it.
To restrict access to the agent data, we have to edit the configuration file at / etc / xinetd.d / check _ mk.
Open the configuration file in your favorite editor.
To use nano, type:
The only _ from setting is responsible for restricting access to certain IP addresses.
Because we are now working on monitoring the same server that Checkmk is running on, it is ok to allow only localhost to connect.
Uncomment and update the configuration setting to:
The xinetd daemon has to be restarted for changes to take place.
Do so now:
Now our agent is up and running and restricted to accept only local connections.
We can proceed to configure monitoring for that host using Checkmk.
Configuring Host in Checkmk Web Interface
First, to add a new host to monitor we have to go to the Hosts menu in the WATO - Configuration menu on the left.
From here click Create new host.
We will be asked for some information about the host.
Creating a new host in Checkmk
The Hostname is the familiar name that Checkmk will use for the monitoring.
It may be a fully-qualified domain name, but it is not necessary.
In this example, we will name the host monitoring, just like the name of the Checkmk instance itself.
Because monitoring is not resolvable to our IP address, we also have to provide the IP address of our server.
And since we are monitoring the local host, the IP will simply be 127.0.0.1.
Check the IPv4 Address box to enable the manual IP input and enter the value in the text field.
The default configuration of the Data Sources section relies on Checkmk agent to provide monitoring data, which is fine.
The Networking Segment setting is used to denote hosts on remote networks, which are characterized by a higher expected latency that is not a sign of malfunction.
Since this is a local host, the default setting is fine as well.
To save the host and configure which services will be monitored, click the Save & go to services button.
List of available services to monitor
Checkmk will do an automatic inventory.
That means it will gather the output from the agent and decipher it to know what kinds of services it can monitor.
All available services for monitoring will be on the list, including CPU load, memory usage, and free space on disks.
To enable monitoring of all discovered services, we have to click the Monitor button under the Undecided services (currently not monitored) section.
This will refresh the page, but now all services will be listed under the Monitored services section, informing us that they are indeed being monitored.
As was the case when changing our user password, these new changes must be saved and activated before they go live.
Press the 2 changes button and accept the changes using the Activate affected button.
After that, the host monitoring will be up and running.
Now you are ready to work with your server data. Take a look at the main dashboard using the Overview / Main Overview menu item on the left.
Working with Monitoring Data
Now let's take a look at the main dashboard using the Overview / Main Overview menu item on the left:
Monitoring dashboard with all services healthy
The Earth sphere is now fully green and the table says that one host is up with no problems.
We can see the full host list, which now consists of a single host, in the Hosts / All hosts view (using the menu on the left).
List of hosts with all services healthy
There we will see how many services are in good health (shown in green), how many are failing, and how many are pending to be checked.
After clicking on the hostname we will be able to see the list of all services with their full statuses and their Perf-O-Meters.
Perf-O-Meter shows the performance of a single service relative to what Checkmk considers to be good health.
Details of a host service status
All services that return graphable data display a graph icon next to their name.
We can use that icon to get access to graphs associated with the service.
Since the host monitoring is fresh, there is almost nothing on the graphs - but after some time the graphs will provide valuable information on how our service performance changes over time.
Graphs depicting CPU load on the server
When any of these services fails or recovers, the information will be shown on the dashboard.
For failing services a red error will be shown, and the problem will also be visible on the Earth graph.
Dashboard with one host having problems
After recovery, everything will be shown in green as working properly, but the event log on the right will contain information about past failures.
Dashboard with one host recovered after problems
Now that we have explored the dashboard a little, let's add a second host to our monitoring instance.
Step 6 - Monitoring a Second CentOS Host
Monitoring gets really useful when you have multiple hosts.
We will now add a second server to our Checkmk instance, this time running CentOS 7.
As with our Ubuntu server, installing Checkmk agent is necessary to gather monitoring data on CentOS.
This time, however, we will need an rpm package from the Monitoring Agents page in the web interface, called check-mk-agent-1.6.0p8-1.noarch. < ^ > rpm < ^ >.
First, however, we must install xinetd, which by default is not available on the CentOS installation.
Xinetd, we will remember, is a daemon that is responsible for making the monitoring data provided by check _ mk _ agent available over the network.
On your CentOS server, first install xinetd:
Now we can download and install the monitoring agent package needed for our CentOS server:
Just like before, we can verify that the agent is working properly by executing check _ mk _ agent:
The output will be similar to that from the Ubuntu server.
Now we will restrict access to the agent.
This time we will not be monitoring a local host, so xinetd must allow connections coming from the Ubuntu server, where Checkmk is installed, to gather the data. To allow that, first open your configuration file:
Here you will see the configuration for your check _ mk service, specifying how Checkmk agent can be accessed through the xinetd daemon.
Find the following two commented lines:
Now uncomment the second line and replace the local IP addresses with < ^ > your _ ubuntu _ server _ ip < ^ >:
Save and exit the file by typing: x and then ENTER.
Restart the xinetd service using:
We can now proceed to configure Checkmk to monitor our CentOS 7 host.
Configuring the New Host in Checkmk
To add additional hosts to Checkmk, we use the Hosts menu just like before.
This time we will name the host centos, configure its IP address, and choose WAN (high-latency) under the Networking Segment select box, since the host is on another network.
If we skipped this and left it as local, Checkmk would soon alert us that the host is down, since it would expect it to respond to agent queries much quicker than is possible over the internet.
Creating second host configuration screen
Click Save & go to services, which will show services available for monitoring on the CentOS server.
The list will be very similar to the one from the first host.
Again, this time we also must click Monitor and then activate the changes using the orange button on the top left corner.
After activating the changes, we can verify that the host is monitored on the All hosts page.
Go there.
Two hosts, monitoring and centos, will now be visible.
List of hosts with two hosts being monitored
You are now monitoring an Ubuntu server and a CentOS server with Checkmk.
It is possible to monitor even more hosts.
In fact, there is no upper limit other than server performance, which should not be a problem until your hosts number in the hundreds.
Moreover, the procedure is the same for any other host.
Checkmk agents in deb and rpm packages work on Ubuntu, CentOS, and the majority of other Linux distributions.
In this guide we set up two servers with two different Linux distributions: Ubuntu and CentOS.
We then installed and configured Checkmk to monitor both servers, and explored Checkmk's powerful web interface.
Checkmk allows for the easy setup of a complete and versatile monitoring system, which packs all the hard work of manual configuration into an easy-to-use web interface full of options and features.
With these tools it is possible to monitor multiple hosts; set up email, SMS, or push notifications for problems; set up additional checks for more services; monitor accessibility and performance, and so on.
To learn more about Checkmk, make sure to visit the official documentation.
How To Run Multiple PHP Versions on One Server Using Apache and PHP-FPM on CentOS 7
5237
One CentOS 7 server with at least 1GB of RAM set up by following the Initial Server Setup with CentOS 7, including a sudo non-root user and a firewall.
An Apache web server set up and configured by following How to Install the Apache Web Server on CentOS 7.
A domain name configured to point to your CentOS 7 server.
With the prerequisites completed, you will now install PHP versions 7.0 and 7.2.
The SCL (Software Collections) repository maintains numerous versions of the PHP stack for the CentOS 7 system.
If you require the absolute newest version of PHP and it is not available on SCL, check the remi PPA (personal package archive) instead.
Begin by installing the SCL repository to your system:
First let's discover what versions of PHP 7 are available on SCL:
You'll see an output like this:
You will note that the newest version, PHP 7.3, is also available.
For our examples, however, we will install versions 7.0 and 7.2.
Lets begin with the older version.
Install rh-php70 and rh-php70-php-fpm:
rh-php70 is a metapackage that runs PHP applications.
rh-php70-php-fpm provides the Fast Process Manager interpreter that runs as a daemon and receives Fast / CGI requests.
Install rh-php72 and rh-php72-php-fpm.
Next, run the following commands to start using both software collections:
By default, both PHP versions are listening on port 9000.
But in this tutorial, we want to run two versions simultaneously.
Therefore, let's designate two new ports:
To accomplish this, you can open / etc / opt / rh / rh-php70 / php-fpm.d / www.conf in your favorite text editor and change every appearance of 9000 to 9002.
Then save and close the file and repeat the process for / etc / opt / rh / rh-php72 / php-fpm.d / www.conf, only now substitute 9000 with 9003.
Alternately, you can use these two sed commands to make the replacements:
You have now designated a dedicated port for each of your PHP services.
Before these modifications will work, however, you must add the ports to your SELinux configuration.
SELinux is short for Security Enhanced Linux, and it is enabled by default on CentOS 7. You must add your new ports of 9002 and 9003 to your SELinux database and assign them to your httpd services, or your applications will not run.
Use the semanage command to perform this task:
The -a flag specifies that you are adding an object to the database.
The -t flag specifies the type of object, which in this case is http _ port _ t.
And the -p flag designates the tcp protocol.
You can learn more about SELinux and the semanage command in this tutorial, or by visiting the official SELinux documentation.
Now you are ready to start and enable your PHP services.
Begin with your rh-php70-php-fpm service and enable it to start at boot:
Next, verify the status of your rh-php70-php-fpm service:
Repeating this process, start the rh-php72-php-fpm service and enable it to start at boot:
By default, the Apache webserver runs as an apache user and an apache group.
So / var / www / and all of its files and subdirectories should also be owned by them.
Run the following commands to verify the correct ownership and permissions of your website root directories:
The chown command changes the ownership of your two website directories to the apache user and the apache group.
The chmod command changes the permissions associated with that user and group, as well as others.
You'll create two new virtual host configuration files inside the directory / etc / httpd / conf.d /.
Here you will direct Apache to render content using PHP 7.0:
Make sure the website directory path, server name, port, and PHP version match your setup:
For DocumentRoot you are specifying the path of your website root directory.
For ServerAdmin you are adding an email that the < ^ > your _ domain < ^ > site administrator can access.
For ServerName you are adding the url for your first subdomain.
For SetHandler you are specifying port < ^ > 9002 < ^ >.
The remaining directives also configure your service to deploy PHP 7.0.
You will specify this subdomain to deploy PHP 7.2:
Again, make sure the website directory path, server name, port, and PHP version match your unique information:
You'll see an output printing Syntax OK:
Because they contain sensitive information about your server and are accessible to unauthorized users, they pose a security vulnerability.
Remove the files:
You now have a single CentOS 7 server handling two websites with two different PHP versions.
From here you might consider exploring PHP-FPM's more advanced features, like its adaptive spawning process or how it can log sdtout and stderr Alternatively, you could now secure your websites.
How To Install Drupal with Docker Compose
5299
The author selected United Nations Foundation to receive a donation as part of the Write for DOnations program.
The original WordPress version of this tutorial was written by Kathleen Juell.
Drupal is a content management system (CMS) written in PHP and distributed under the open-source GNU General Public License.
People and organizations around the world use Drupal to power government sites, personal blogs, businesses, and more.
What makes Drupal unique from other CMS frameworks is its growing community and a set of features that include secure processes, reliable performance, modularity, and flexibility to adapt.
Drupal requires installing the LAMP (Linux, Apache, MySQL, and PHP) or LEMP (Linux, Nginx, MySQL, and PHP) stack, but installing individual components is a time-consuming task.
We can use tools like Docker and Docker Compose to simplify the process of installing Drupal.
This tutorial will use Docker images for installing individual components within the Docker containers.
By using Docker Compose, we can define and manage multiple containers for the database, application, and the networking / communication between them.
In this tutorial, we will install Drupal using Docker Compose so that we can take advantage of containerization and deploy our Drupal website on servers.
We will be running containers for a MySQL database, Nginx webserver, and Drupal.
We will also secure our installation by obtaining TLS / SSL certificates with Let "s Encrypt for the domain we want to associate with our site.
Finally, we will set up a cron job to renew our certificates so that our domain remains secure.
A server running Ubuntu 18.04, along with a non-root user with sudo privileges and an active firewall.
This tutorial has been tested on version 19.03.8.
This tutorial has been tested on version 1.21.2.
You can follow this introduction to DigitalOcean DNS for details on how to add them to a DigitalOcean account, if that "s what you" re using:
Step 1 - Defining the Web Server Configuration
Before running any containers, we need to define the configuration for our Nginx web server.
Our configuration file will include some Drupal-specific location blocks, along with a location block to direct Let "s Encrypt verification requests to the Certbot client for automated certificate renewals.
First, let "s create a project directory for our Drupal setup named drupal:
Move into the newly created directory:
Now we can make a directory for our configuration file:
Open the file with nano or your favorite text editor:
In this file, we will add a server block with directives for our server name and document root, and location blocks to direct the Certbot client "s request for certificates, PHP processing, and static asset requests.
Add the following code into the file.
Be sure to replace < ^ > your _ domain < ^ > with your own domain name:
Our server block includes the following information:
Directives:
listen: This tells Nginx to listen on port 80, which will allow us to use Certbot "s webroot plugin for our certificate requests.
Note that we are not including port 443 yet - we will update our configuration to include SSL once we have successfully obtained our certificates.
server _ name: This defines our server name and the server block that should be used for requests to our server.
Be sure to replace < ^ > your _ domain < ^ > in this line with your own domain name.
index: The index directive defines the files that will be used as indexes when processing requests to our server.
We "ve modified the default order of priority here, moving index.php in front of index.html so that Nginx prioritizes files called index.php when possible.
root: Our root directive names the root directory for requests to our server.
This directory, / var / www / html, is created as a mount point at build time by instructions in our Drupal Dockerfile.
These Dockerfile instructions also ensure that the files from the Drupal release are mounted to this volume.
rewrite: If the specified regular expression (< ^ > ^ / core / authorize.php / core / authorize.php (. *) $< ^ >) matches a request URI, the URI is changed as specified in the replacement string (< ^ > / core / authorize.php $1 < ^ >).
Location Blocks:
location ~ / .well-known / acme-challenge: This location block will handle requests to the .well-known directory, where Certbot will place a temporary file to validate that the DNS for our domain resolves to our server.
With this configuration in place, we will be able to use Certbot "s webroot plugin to obtain certificates for our domain.
location /: In this location block, we "ll use a try _ files directive to check for files that match individual URI requests.
Instead of returning a 404 Not Found status as a default, however, we "ll pass control to Drupal" s index.php file with the request arguments.
location ~\ .php $: This location block will handle PHP processing and proxy these requests to our drupal container.
Because our Drupal Docker image will be based on the php: fpm image, we will also include configuration options that are specific to the FastCGI protocol in this block.
Nginx requires an independent PHP processor for PHP requests: in our case, these requests will be handled by the php-fpm processor that "s included with the php: fpm image.
Additionally, this location block includes FastCGI-specific directives, variables, and options that will proxy requests to the Drupal application running in our Drupal container, set the preferred index for the parsed request URI, and parse URI requests.
location ~ /\ .ht: This block will handle .htaccess files since Nginx won "t serve them.
The deny _ all directive ensures that .htaccess files will never be served to users.
location = / favicon.ico, location = / robots.txt: These blocks ensure that requests to / favicon.ico and / robots.txt will not be logged.
location ~ *\. (css | gif | ico | jpeg | jpg | js | png) $: This block turns off logging for static asset requests and ensures that these assets are highly cacheable, as they are typically expensive to serve.
For more information about FastCGI proxying, see Understanding and Implementing FastCGI Proxying in Nginx.
For information about server and location blocks, see Understanding Nginx Server and Location Block Selection Algorithms.
With your Nginx configuration in place, you can move on to creating environment variables to pass to your application and database containers at runtime.
Step 2 - Defining Environment Variables
Our Drupal application needs a database (MySQL, PostgresSQL, etc.) for saving information related to the site.
The Drupal container will need access to certain environment variables at runtime in order to access the database (MySQL) container.
These variables contain the sensitive information like the credentials of the database, so we can't expose them directly in the Docker Compose file - the main file that contains information about how our containers will run.
It is always recommended to set the sensitive values in the .env file and restrict its circulation.
This will prevent these values from copying over to our project repositories and being exposed publicly.
In the main project directory, ~ / drupal, create and open a file called .env:
Add the following variables to the .env file, replacing the highlighted sections with the credentials you want to use:
We have now added the password for the MySQL root administrative account, as well as our preferred username and password for our application database.
Our .env file contains sensitive information so it is always recommended to include it in a project's .gitignore and .dockerignore files so that it won't be added in our Git repositories and Docker images.
If you plan to work with Git for version control, initialize your current working directory as a repository with git init:
Open .gitignore file:
Similarly, open the .dockerignore file:
Now that we have taken measures to safeguard our credentials as environment variables, let's move to our next step of defining our services in a docker-compose.yml file.
Step 3 - Defining Services with Docker Compose
Docker Compose is a tool for defining and running multi-container Docker applications.
We define a YAML file to configure our application "s services.
A service in Docker Compose is a running container, and Compose allows us to link these services together with shared volumes and networks.
We will create different containers for our Drupal application, database, and web server.
Along with these, we will also create a container to run Certbot in order to obtain certificates for our web server.
Create a docker-compose.yml file:
Add the following code to define the Compose file version and mysql database service:
Let's go through these one-by-one with all the configuration options of the mysql service:
image: This specifies the image that will be used / pulled for creating the container.
It is always recommended to use the image with the proper version tag excluding the latest tag to avoid future conflicts.
Read more on Dockerfile best practices from the Docker documentation.
container _ name: To define the name of the container.
command: This is used to override the default command (CMD instruction) in the image.
MySQL has supported different authentication plugins, but mysql _ native _ password is the traditional method to authenticate.
Since PHP, and hence Drupal, won't support the newer MySQL authentication, we need to set the --default-authentication-plugin = mysql _ native _ password as the default authentication mechanism.
restart: This is used to define the container restart policy.
The unless-stopped policy restarts a container unless it is stopped manually.
env _ file: This adds the environment variables from a file.
In our case, it will read the environment variables from the .env file defined in the previous step.
volumes: This mounts host paths or named volumes, specified as sub-options to a service.
We are mounting a named volume called db-data to the / var / lib / mysql directory on the container, where MySQL by default will write its data files.
networks: This defines the internal network that our application service will join.
We will define the networks at the end of the file.
We have defined our mysql service definition, so now let's add the definition of the drupal application service to the end of the file:
In this service definition, we are naming our container and defining a restart policy, as we did with the mysql service.
We "re also adding some options specific to this container:
image: Here, we are using the 8.7.8-fpm-alpine Drupal image.
This image has the php-fpm processor that our Nginx web server requires to handle PHP processing.
Moreover we are using the alpine image, derived from the Alpine Linux project, which will reduce the size of the overall image and is recommended in the Dockerfile best practices.
Drupal has more versions of images, so check them out on Dockerhub.
depends _ on: This is used to express dependency between services.
Defining the mysql service as the dependency to our drupal container will ensure that our drupal container will be created after the mysql container and enable our application to start smoothly.
networks: Here, we have added this container to the external network along with the internal network.
This will ensure that our mysql service is accessible only from the drupal container through the internal network while keeping this container accessible to other containers through the external network.
volumes: We are mounting a named volume called drupal-data to the / var / www / html mountpoint created by the Drupal image.
Using a named volume in this way will allow us to share our application code with other containers.
Next, let's add the Nginx service definition after the drupal service definition:
Again, we "re naming our container and making it dependent on the Drupal container in order of starting.
We "re also using an alpine image - the 1.17.4-alpine Nginx image.
This service definition also includes the following options:
ports: This exposes port 80 to enable the configuration options we defined in our nginx.conf file in Step 1.
volumes: Here, we are defining both the named volume and host path:
drupal-data: / var / www / html: This will mount our Drupal application code to the / var / www / html directory, which we set as the root in our Nginx server block.
. / nginx-conf: / etc / nginx / conf.d: This will mount the Nginx configuration directory on the host to the relevant directory on the container, ensuring that any changes we make to files on the host will be reflected in the container.
certbot-etc: / etc / letsencrypt: This will mount the relevant Let "s Encrypt certificates and keys for our domain to the appropriate directory on the container.
networks: We have defined the external network only to let this container communicate with the drupal container and not with the mysql container.
Finally, we will add our last service definition for the certbot service.
Be sure to replace < ^ > sammy @ your _ domain < ^ > and < ^ > your _ domain < ^ > with your own email and domain name:
This definition tells Compose to pull the certbot / certbot image from Docker Hub.
It also uses named volumes to share resources with the Nginx container, including the domain certificates and key in certbot-etc and the application code in drupal-data.
We have also used depends _ on to make sure that the certbot container will be started after the webserver service is running.
We haven't specified any networks here because this container won't communicate to any services over the network.
It is only adding the domain certificates and key, which we have mounted using the named volumes.
We have also included the command option that specifies a subcommand to run with the container "s default certbot command.
The Certbot client supports plugins for obtaining and installing certificates.
We are using the webroot plugin to obtain a certificate by including certonly and --webroot on the command line.
Read more about the plugin and additional commands from the official Certbot Documentation.
After the certbot service definition, add the network and volume definitions:
The top-level networks key lets us specify networks to be created. networks allows communication across the services / containers on all the ports since they are on the same Docker daemon host.
We have defined two networks, internal and external, to secure the communication of the webserver, drupal, and mysql services.
The volumes key is used to define the named volumes drupal-data, db-data, and certbot-etc.
When Docker creates volumes, the contents of the volume are stored in a directory on the host filesystem, / var / lib / docker / volumes /, that "s managed by Docker.
The contents of each volume then get mounted from this directory to any container that uses the volume.
In this way, it "s possible to share code and data between containers.
The finished docker-compose.yml file will look like this:
We are done with defining our services.
Next, let's start the container and test our certificate requests.
Step 4 - Obtaining SSL Certificates and Credentials
We can start our containers with the docker-compose up command, which will create and run our containers in the order we have specified.
If our domain requests are successful, we will see the correct exit status in our output and the right certificates mounted in the / etc / letsencrypt / live folder on the web server container.
To run the containers in the background, use the docker-compose up command with the -d flag:
You will see similar output confirming that your services have been created:
Check the status of the services using the docker-compose ps command:
We will see the mysql, drupal, and webserver services with a State of Up, while certbot will be exited with a 0 status message:
If you see anything other than Up in the State column for the mysql, drupal, or webserver services, or an exit status other than 0 for the certbot container, be sure to check the service logs with the docker-compose logs command:
We can now check that our certificates mounted on the webserver container using the docker-compose exec command:
Now that everything runs successfully, we can edit our certbot service definition to remove the --staging flag.
Open the docker-compose.yml file, go to the certbot service definition, and replace the --staging flag in the command option with the --force-renewal flag, which will tell Certbot that you want to request a new certificate with the same domains as an existing certificate.
The updated certbot definition will look like this:
We need to run docker-compose up again to recreate the certbot container.
We will also include the --no-deps option to tell Compose that it can skip starting the webserver service, since it is already running:
We will see output indicating that our certificate request was successful:
Now that we have successfully generated our certificates, we can update our Nginx Configuration to include SSL.
Step 5 - Modifying the Web Server Configuration and Service Definition
After installing SSL certificates in Nginx, we will need to redirect all the HTTP requests to HTTPS.
We will also have to specify our SSL certificate and key locations and add security parameters and headers.
Since you are going to recreate the webserver service to include these additions, you can stop it now:
Next, let's remove the Nginx configuration file we created earlier:
Open another version of the file:
Add the following code to the file to redirect HTTP to HTTPS and to add SSL credentials, protocols, and security headers.
Remember to replace < ^ > your _ domain < ^ > with your own domain:
The HTTP server block specifies the webroot plugin for Certbot renewal requests to the .well-known / acme-challenge directory.
It also includes a rewrite directive that directs HTTP requests to the root directory to HTTPS.
The HTTPS server block enables ssl and http2.
To read more about how HTTP / 2 iterates on HTTP protocols and the benefits it can have for website performance, please see the introduction to How To Set Up Nginx with HTTP / 2 Support on Ubuntu 18.04.
These blocks enable SSL, as we have included our SSL certificate and key locations along with the recommended headers.
These headers will enable us to get an A rating on the SSL Labs and Security Headers server test sites.
Our root and index directives are also located in this block, as are the rest of the Drupal-specific location blocks discussed in Step 1.
Save and close the updated Nginx configuration file.
Before recreating the webserver container, we will need to add a 443 port mapping to our webserver service definition as we have enabled SSL certificates.
Make the following changes in the webserver service definition:
After enabling the SSL certificates, our docker-compose.yml will look like this:
Let's recreate the webserver service with our updated configuration:
Check the services with docker-compose ps:
We will see the mysql, drupal, and webserver services as Up while certbot will be exited with a 0 status message:
Now, all our services are running and we are good to move forward with installing Drupal through the web interface.
Step 6 - Completing the Installation Through the Web Interface
Let's complete the installation through Drupal's web interface.
In a web browser, navigate to the server "s domain.
Remember to substitute your _ domain here with your own domain name:
Choose language page on Drupal web interface
Click Save and continue.
We will land on the Installation profile page.
Drupal has multiple profiles, so select the Standard profile and click on Save and continue.
Choose profile page on Drupal web interface
After selecting the profile, we will move forward to Database configuration page.
Select the Database type as MySQL, MariaDB, Percona Server, or equivalent and enter the values of Database name, username, and password from the values corresponding to MYSQL _ DATABASE, MYSQL _ USER, and MYSQL _ PASSWORD respectively defined in the .env file in Step 2. Click on Advanced Options and set the value of Host to the name of the mysql service container.
Set up database page on Drupal web interface
After configuring the database, it will start installing Drupal default modules and themes:
Install site page on Drupal web interface
Once the site is installed, we will land on the Drupal site setup page for configuring the site name, email, username, password, and regional settings.
Fill in the information and click on Save and continue:
Configure site page on Drupal web interface
After clicking Save and continue, we can see the Welcome to Drupal page, which shows that our Drupal site is up and running successfully.
Welcome to Drupal page on Drupal web interface
Now that our Drupal installation is complete, we need to ensure that our SSL certificates will renew automatically.
Step 7 - Renewing Certificates
Let "s Encrypt certificates are valid for 90 days, so we need to set up an automated renewal process to ensure that they do not lapse.
One way to do this is to create a job with the cron scheduling utility.
In this case, we will create a cron job to periodically run a script that will renew our certificates and reload our Nginx configuration.
Let's create the ssl _ renew.sh file to renew our certificates:
Add the following code.
Remember to replace the directory name with your own non-root user:
This script changes to the ~ / drupal project directory and runs the following docker-compose commands.
docker-compose run: This will start a certbot container and override the command provided in our certbot service definition.
Instead of using the certonly subcommand, we "re using the renew subcommand here, which will renew certificates that are close to expiring.
We "ve included the --dry-run option here to test our script.
docker-compose kill: This will send a SIGHUP signal to the webserver container to reload the Nginx configuration.
Close the file and make it executable by running the following command:
Next, open the root crontab file to run the renewal script at a specified interval:
If this is your first time editing this file, you will be asked to choose a text editor to open the file with:
At the end of the file, add the following line, replacing sammy with your username:
This will set the job interval to every five minutes, so we can test whether or not our renewal request has worked as intended.
We have also created a log file, cron.log, to record relevant output from the job.
After five minutes, use the tail command to check cron.log to see whether or not the renewal request has succeeded:
You will see output confirming a successful renewal:
Press CTRL + C to quit the tail process.
We can now modify the crontab file to run the script every 2nd day of the week at 2 AM.
Change the final line of the crontab to the following:
Now, let's remove the --dry-run option from the ssl _ renew.sh script.
First, open it up:
Then change the contents to the following:
Our cron job will now take care of our SSL certificates expiry by renewing them when they are eligible.
In this tutorial, we have used Docker Compose to create a Drupal installation with an Nginx web server.
As part of this workflow, we obtained TLS / SSL certificates for the domain we wanted associated with our Drupal site and created a cron job to renew these certificates when necessary.
If you would like to learn more about Docker, check out our Docker topic page.
How To Install Linux, Nginx, MySQL, PHP (LEMP stack) on Ubuntu 20.04
5389
This is an acronym that describes a Linux operating system, with an Nginx (pronounced like "Engine-X ") web server.
The backend data is stored in the MySQL database and the dynamic processing is handled by PHP.
This guide demonstrates how to install a LEMP stack on an Ubuntu 20.04 server.
The Ubuntu operating system takes care of the first requirement.
We will describe how to get the rest of the components up and running.
In order to complete this tutorial, you will need access to an Ubuntu 20.04 server as a regular, non-root sudo user, and a firewall enabled on your server.
To set this up, you can follow our initial server setup guide for Ubuntu 20.04.
Step 1 - Installing the Nginx Web Server
We'll use the apt package manager to obtain this software.
Since this is our first time using apt for this session, start off by updating your server "s package index.
Following that, you can use apt install to get Nginx installed:
When prompted, enter Y to confirm that you want to install Nginx.
Once the installation is finished, the Nginx web server will be active and running on your Ubuntu 20.04 server.
If you have the ufw firewall enabled, as recommended in our initial server setup guide, you will need to allow connections to Nginx.
Nginx registers a few different UFW application profiles upon installation.
To check which UFW profiles are available, run:
It is recommended that you enable the most restrictive profile that will still allow the traffic you need.
Since you haven't configured SSL for your server in this guide, you will only need to allow regular HTTP traffic on port 80.
Enable this by typing:
You can verify the change by running:
This command "s output will show that HTTP traffic is now allowed:
With the new firewall rule added, you can test if the server is up and running by accessing your server's domain name or public IP address in your web browser.
If you do not have a domain name pointed at your server and you do not know your server's public IP address, you can find it by running the following command:
Nginx default page
If you see this page, it means you have successfully installed Nginx and enabled HTTP traffic for your web server.
Next, we'll install PHP, the final component in the LEMP stack.
You have Nginx installed to serve your content and MySQL installed to store and manage your data. Now you can install PHP to process code and generate dynamic content for the web server.
While Apache embeds the PHP interpreter in each request, Nginx requires an external program to handle PHP processing and act as a bridge between the PHP interpreter itself and the web server.
Additionally, you "ll need php-mysql, a PHP module that allows PHP to communicate with MySQL-based databases.
When prompted, type Y and ENTER to confirm installation.
You now have your PHP components installed.
Next, you'll configure Nginx to use them.
Step 4 - Configuring Nginx to Use the PHP Processor
When using the Nginx web server, we can create server blocks (similar to virtual hosts in Apache) to encapsulate configuration details and host more than one domain on a single server.
In this guide, we'll use your _ domain as an example domain name.
On Ubuntu 20.04, Nginx has one server block enabled by default and is configured to serve documents out of a directory at / var / www / html.
Instead of modifying / var / www / html, we'll create a directory structure within / var / www for the your _ domain website, leaving / var / www / html in place as the default directory to be served if a client request doesn't match any other sites.
Create the root web directory for your _ domain as follows:
Then, open a new configuration file in Nginx's sites-available directory using your preferred command-line editor.
Here "s what each of these directives and location blocks do:
listen - Defines what port Nginx will listen on.
In this case, it will listen on port 80, the default port for HTTP.
root - Defines the document root where the files served by this website are stored.
index - Defines in which order Nginx will prioritize index files for this website.
It is a common practice to list index.html files with a higher precedence than index.php files to allow for quickly setting up a maintenance landing page in PHP applications.
You can adjust these settings to better suit your application needs.
server _ name - Defines which domain names and / or IP addresses this server block should respond for.
Point this directive to your server's domain name or public IP address.
location / - The first location block includes a try _ files directive, which checks for the existence of files or directories matching a URI request.
If Nginx cannot find the appropriate resource, it will return a 404 error.
location ~\ .php $- This location block handles the actual PHP processing by pointing Nginx to the fastcgi-php.conf configuration file and the php7.4-fpm.sock file, which declares what socket is associated with php-fpm.
location ~ /\ .ht - The last location block deals with .htaccess files, which Nginx does not process.
By adding the deny all directive, if any .htaccess files happen to find their way into the document root, they will not be served to visitors.
When you're done editing, save and close the file.
If you're using nano, you can do so by typing CTRL + X and then y and ENTER to confirm.
Activate your configuration by linking to the config file from Nginx's sites-enabled directory:
This will tell Nginx to use the configuration next time it is reloaded.
You can test your configuration for syntax errors by typing:
If any errors are reported, go back to your configuration file to review its contents before continuing.
When you are ready, reload Nginx to apply the changes:
Create an index.html file in that location so that we can test that your new server block works as expected:
Now go to your browser and access your server's domain name or IP address, as listed within the server _ name directive in your server block configuration file:
If you see this page, it means your Nginx server block is working as expected.
Your LEMP stack is now fully configured.
In the next step, we'll create a PHP script to test that Nginx is in fact able to handle .php files within your newly configured website.
Step 5 -Testing PHP with Nginx
Your LEMP stack should now be completely set up.
You can test it to validate that Nginx can correctly hand .php files off to your PHP processor.
You can do this by creating a test PHP file in your document root.
Open a new file called info.php within your document root in your text editor:
Type or paste the following lines into the new file.
This is valid PHP code that will return information about your server:
When you are finished, save and close the file by typing CTRL + X and then y and ENTER to confirm.
You can now access this page in your web browser by visiting the domain name or public IP address you've set up in your Nginx configuration file, followed by / info.php:
You will see a web page containing detailed information about your server:
PHPInfo Ubuntu 20.04
You can now access this page in your web browser by visiting the domain name or public IP address configured for your website, followed by / todo _ list.php:
In this guide, we've built a flexible foundation for serving PHP websites and applications to your visitors, using Nginx as web server and MySQL as database system.
There are a number of next steps you could take from here.
For example, you should ensure that connections to your server are secured.
To this end, you could secure your Nginx installation with Let "s Encrypt.
By following this guide, you will acquire a free TLS / SSL certificate for your server, allowing it to serve content over HTTPS.
How To Install Nginx on Ubuntu 20.04
5298
Nginx is one of the most popular web servers in the world and is responsible for hosting some of the largest and highest-traffic sites on the internet.
It is a lightweight choice that can be used as either a web server or reverse proxy.
In this guide, we'll discuss how to install Nginx on your Ubuntu 20.04 server, adjust the firewall, manage the Nginx process, and set up server blocks for hosting more than one domain from a single server.
Before you begin this guide, you should have a regular, non-root user with sudo privileges configured on your server.
You can learn how to configure a regular user account by following our Initial server setup guide for Ubuntu 20.04.
Step 1 - Installing Nginx
Because Nginx is available in Ubuntu's default repositories, it is possible to install it from these repositories using the apt packaging system.
Since this is our first interaction with the apt packaging system in this session, we will update our local package index so that we have access to the most recent package listings.
Afterwards, we can install nginx:
After accepting the procedure, apt will install Nginx and any required dependencies to your server.
Step 2 - Adjusting the Firewall
Before testing Nginx, the firewall software needs to be adjusted to allow access to the service.
Nginx registers itself as a service with ufw upon installation, making it straightforward to allow Nginx access.
List the application configurations that ufw knows how to work with by typing:
You should get a listing of the application profiles:
As demonstrated by the output, there are three profiles available for Nginx:
Nginx Full: This profile opens both port 80 (normal, unencrypted web traffic) and port 443 (TLS / SSL encrypted traffic)
Nginx HTTP: This profile opens only port 80 (normal, unencrypted web traffic)
Nginx HTTPS: This profile opens only port 443 (TLS / SSL encrypted traffic)
It is recommended that you enable the most restrictive profile that will still allow the traffic you've configured.
Right now, we will only need to allow traffic on port 80.
You can enable this by typing:
You can verify the change by typing:
The output will indicated which HTTP traffic is allowed:
Step 3 - Checking your Web Server
At the end of the installation process, Ubuntu 20.04 starts Nginx.
The web server should already be up and running.
We can check with the systemd init system to make sure the service is running by typing:
As confirmed by this out, the service has started successfully.
However, the best way to test this is to actually request a page from Nginx.
You can access the default Nginx landing page to confirm that the software is running properly by navigating to your server's IP address.
If you do not know your server's IP address, you can find it by using the icanhazip.com tool, which will give you your public IP address as received from another location on the internet:
You should receive the default Nginx landing page:
If you are on this page, your server is running correctly and is ready to be managed.
Now that you have your web server up and running, let's review some basic management commands.
If you are only making configuration changes, Nginx can often reload without dropping connections.
To re-enable the service to start up at boot, you can type:
You have now learned basic management commands and should be ready to configure the site to host more than one domain.
Step 5 - Setting Up Server Blocks (Recommended)
When using the Nginx web server, server blocks (similar to virtual hosts in Apache) can be used to encapsulate configuration details and host more than one domain from a single server.
Nginx on Ubuntu 20.04 has one server block enabled by default that is configured to serve documents out of a directory at / var / www / html.
The permissions of your web roots should be correct if you haven't modified your umask value, which sets default file permissions.
To ensure that your permissions are correct and allow the owner to read, write, and execute the files while granting only read and execute permissions to groups and others, you can input the following command:
Save and close the file by typing CTRL and X then Y and ENTER when you are finished.
In order for Nginx to serve this content, it's necessary to create a server block with the correct directives.
Instead of modifying the default configuration file directly, let "s make a new one at / etc / nginx / sites-available / < ^ > your _ domain < ^ >:
Notice that we "ve updated the root configuration to our new directory, and the server _ name to our domain name.
Next, let's enable the file by creating a link from it to the sites-enabled directory, which Nginx reads from during startup:
Two server blocks are now enabled and configured to respond to requests based on their listen and server _ name directives (you can read more about how Nginx processes these directives here):
your _ domain: Will respond to requests for your _ domain and www.your _ domain.
default: Will respond to any requests on port 80 that do not match the other two blocks.
To avoid a possible hash bucket memory problem that can arise from adding additional server names, it is necessary to adjust a single value in the / etc / nginx / nginx.conf file.
Find the server _ names _ hash _ bucket _ size directive and remove the # symbol to uncomment the line.
If you are using nano, you can quickly search for words in the file by pressing CTRL and w.
Next, test to make sure that there are no syntax errors in any of your Nginx files:
If there aren't any problems, restart Nginx to enable your changes:
Nginx should now be serving your domain name.
Nginx first server block
Step 6 - Getting Familiar with Important Nginx Files and Directories
Now that you know how to manage the Nginx service itself, you should take a few minutes to familiarize yourself with a few important directories and files.
/ var / www / html: The actual web content, which by default only consists of the default Nginx page you saw earlier, is served out of the / var / www / html directory.
/ etc / nginx / sites-available /: The directory where per-site server blocks can be stored.
Nginx will not use the configuration files found in this directory unless they are linked to the sites-enabled directory.
Typically, all server block configuration is done in this directory, and then enabled by linking to the other directory.
/ etc / nginx / sites-enabled /: The directory where enabled per-site server blocks are stored.
Typically, these are created by linking to configuration files found in the sites-available directory.
/ etc / nginx / snippets: This directory contains configuration fragments that can be included elsewhere in the Nginx configuration.
Potentially repeatable configuration segments are good candidates for refactoring into snippets.
If you "d like to build out a more complete application stack, check out the article How To Install Linux, Nginx, MySQL, PHP (LEMP stack) on Ubuntu 20.04.
How To Install the Apache Web Server on Ubuntu 20.04
5354
Apache is available within Ubuntu's default software repositories, making it possible to install it using conventional package management tools.
Let's begin by updating the local package index to reflect the latest upstream changes:
Then, install the apache2 package:
After confirming the installation, apt will install Apache and all required dependencies.
Before testing Apache, it's necessary to modify the firewall settings to allow outside access to the default web ports.
Assuming that you followed the instructions in the prerequisites, you should have a UFW firewall configured to restrict access to your server.
During installation, Apache registers itself with UFW to provide a few application profiles that can be used to enable or disable access to Apache through the firewall.
List the ufw application profiles by typing:
You will receive a list of the application profiles:
As indicated by the output, there are three profiles available for Apache:
Apache: This profile opens only port 80 (normal, unencrypted web traffic)
Since we haven't configured SSL for our server yet in this guide, we will only need to allow traffic on port 80:
The output will provide a list of allowed HTTP traffic:
As indicated by the output, the profile has been activated to allow access to the Apache web server.
At the end of the installation process, Ubuntu 20.04 starts Apache.
As confirmed by this output, the service has started successfully.
Try typing this at your server's command prompt:
You will get back a few addresses separated by spaces.
Another option is to use the Icanhazip tool, which should give you your public IP address as read from another location on the internet:
You should see the default Ubuntu 20.04 Apache web page:
Step 4 - Managing the Apache Process
Now that you have your web server up and running, let's go over some basic management commands using systemctl.
Apache should now start automatically when the server boots again.
Step 5 - Setting Up Virtual Hosts (Recommended)
Instead of modifying / var / www / html, let's create a directory structure within / var / www for a your _ domain site, leaving / var / www / html in place as the default directory to be served if a client request doesn't match any other sites.
The permissions of your web roots should be correct if you haven "t modified your umask value, which sets default file permissions.
Step 6 - Getting Familiar with Important Apache Files and Directories
Now that you know how to manage the Apache service itself, you should take a few minutes to familiarize yourself with a few important directories and files.
/ var / www / html: The actual web content, which by default only consists of the default Apache page you saw earlier, is served out of the / var / www / html directory.
This can be changed by altering Apache configuration files.
/ etc / apache2: The Apache configuration directory.
All of the Apache configuration files reside here.
/ etc / apache2 / apache2.conf: The main Apache configuration file.
This can be modified to make changes to the Apache global configuration.
This file is responsible for loading many of the other files in the configuration directory.
/ etc / apache2 / ports.conf: This file specifies the ports that Apache will listen on.
By default, Apache listens on port 80 and additionally listens on port 443 when a module providing SSL capabilities is enabled.
/ etc / apache2 / sites-available /: The directory where per-site virtual hosts can be stored.
Apache will not use the configuration files found in this directory unless they are linked to the sites-enabled directory.
Typically, all server block configuration is done in this directory, and then enabled by linking to the other directory with the a2ensite command.
/ etc / apache2 / sites-enabled /: The directory where enabled per-site virtual hosts are stored.
Typically, these are created by linking to configuration files found in the sites-available directory with the a2ensite.
Apache reads the configuration files and links found in this directory when it starts or reloads to compile a complete configuration.
/ etc / apache2 / conf-available /, / etc / apache2 / conf-enabled /: These directories have the same relationship as the sites-available and sites-enabled directories, but are used to store configuration fragments that do not belong in a virtual host.
Files in the conf-available directory can be enabled with the a2enconf command and disabled with the a2disconf command.
/ etc / apache2 / mods-available /, / etc / apache2 / mods-enabled /: These directories contain the available and enabled modules, respectively.
Files ending in .load contain fragments to load specific modules, while files ending in .conf contain the configuration for those modules.
Modules can be enabled and disabled using the a2enmod and a2dismod command.
/ var / log / apache2 / access.log: By default, every request to your web server is recorded in this log file unless Apache is configured to do otherwise.
/ var / log / apache2 / error.log: By default, all errors are recorded in this file.
The LogLevel directive in the Apache configuration specifies how much detail the error logs will contain.
If you'd like to build out a more complete application stack, you can read this article on how to configure a LAMP stack on Ubuntu 20.04
How To Monitor BGP Announcements and Routes Using BGPalerter on Ubuntu 18.04
5236
BGP (Border Gateway Protocol) is one of the core protocols responsible for routing packets across the internet, so when it goes wrong, significant outages can occur.
For example, in 2019, a small ISP made a BGP misconfiguration that unfortunately propagated upstream and took large parts of Cloudflare and AWS offline for over an hour.
Also, a year earlier, a BGP hijack took place in order to intercept traffic to a well-known cryptocurrency wallet provider and steal the funds of unsuspecting customers.
BGPalerter is an open-source BGP network monitoring tool that can provide real-time alerts on BGP activity, including route visibility and new route announcements, as well as potentially nefarious activity such as route hijacks or route leaks.
BGPalerter automatically ingests publicly-available network routing information, meaning that it does not have to have any level of privileged access or integration into the network (s) that you wish to monitor.
< $> note Note: BGPalerter automatically ingests publicly available network routing information, meaning that it does not have to have any level of privileged access or integration into the network (s) that you wish to monitor.
All monitoring is fully compliant with the Computer Misuse Act, Computer Fraud and Abuse Act, and other similar laws.
However, it is recommended to responsibly disclose any relevant findings to the affected network operator.
In this tutorial, you'll install and configure BGPalerter to monitor your important networks for potentially suspicious activity.
One or more networks or devices that you wish to monitor, for example:
A server that you maintain
Your company network
Your local ISP
For each device or network you'll need to identify either the individual IP address, IP address range, or Autonomous System number that it is part of.
This is covered in Step 1.
Step 1 - Identifying the Networks to Monitor
In this step, you will identify the relevant details of the networks that you want to monitor.
BGPalerter can monitor based on individual IP addresses or network prefixes.
It can also monitor entire networks based on their Autonomous System (AS) number, which is a globally unique identifier for a network owned by a particular administrative entity.
In order to find this information, you can use the IP-to-ASN WHOIS lookup service provided by threat intelligence service Team Cymru.
This is a custom WHOIS server designed for looking up IP address and network routing information.
If you don't already have whois installed, you can install it using the following commands:
Once you've confirmed that whois is installed, begin by performing a lookup for the IP address of your own server, using the -h argument to specify a custom server:
This will output a result similar to the following, which shows the AS name and number that your server is a part of.
This will usually be the AS of your server hosting provider, for example, DigitalOcean.
Next, you can perform a lookup to identify the network prefix / range that your server is a part of.
You do this by adding the -p argument to your request:
The output will be very similar to the previous command, but will now show the IP address prefix that the IP address of your server belongs to:
Finally, you can look up further details of the AS that your server is a part of, including the geographic region and allocation date.
Substitute in the AS number that you identified using the previous commands.
You use the -v argument to enable verbose output, which ensures that all relevant details are shown:
The output will show further information about the AS:
You've identified key details about the network (s) that you wish to monitor.
Keep a note of these details somewhere, as you'll need them later on.
Next, you'll begin the setup of BGPalerter.
Step 2 - Creating a Non-Privileged User for BGPalerter
In this step, you will create a new non-privileged user account for BGPalerter, as the program doesn't need to run with sudo / root privileges.
Firstly, create a new user with a disabled password:
You do not need to set up a password or SSH keys, as you'll use this user only as a service account for running / maintaining BGPalerter.
Log in to the new user using su:
You'll now be logged in as the new user:
Use the cd command to move to the home directory of your new user:
You've created a new non-privileged user for BGPalerter.
Next, you will install and configure BGPalerter on your system.
Step 3 - Installing and Configuring BGPalerter
In this step, you will install and configure BGPalerter.
Make sure that you're still logged in as your new non-privileged user.
Firstly, you need to identify the latest release of BGPalerter, in order to ensure that you download the most up-to-date version.
Browse to the BGPalerter Releases page and take a copy of the download link for the most recent Linux x64 release.
You can now download a copy of BGPalerter using wget, making sure to substitute in the correct download link:
Once the file has finished downloading, mark it as executable:
Next, check that BGPalerter has been downloaded and installed successfully by checking the version number:
This will output the current version number:
Before you can run BGPalerter properly, you'll need to define the networks that you wish to monitor within a configuration file.
Create and open the prefixes.yml file in your favourite text editor:
In this config file, you'll specify each of the individual IP addresses, IP address ranges, and AS numbers that you want to monitor.
Add the following example and adjust the configuration values as required by using the network information that you identified in Step 1:
You can monitor as many IP address ranges or AS numbers as you want.
To monitor individual IP addresses, represent them using / 32 for IPv4, and / 128 for IPv6.
The ignoreMorespecifics value is used to control whether BGPalerter should ignore activity for routes that are more specific (smaller) than the one that you're monitoring.
For example, if you're monitoring a / 20 and a routing change is detected for a / 24 within it, this is considered to be more specific.
In most cases, you don't want to ignore these, however if you are monitoring a large network with multiple delegated customer prefixes, this may help to reduce background noise.
You can now run BGPalerter for the first time in order to begin monitoring your networks:
If BGPalerter starts successfully, you'll see output similar to the following.
Note that it can sometimes take a few minutes for the monitoring to begin:
BGPalerter will continue to run until you stop it using Ctrl + C.
In the next step, you will interpret some of the alerts that BGPalerter can generate.
Step 4 - Interpreting BGPalerter Alerts
In this step, you will review some example BGPalerter alerts.
BGPalerter will output alerts to the main output feed, and also optionally to any additional reporting endpoints that can be configured within config.yml, as described in the BGPalerter documentation.
By default, BGPalerter monitors and alerts on the following:
Route hijacks: occur when an AS announces a prefix that it is not permitted to, causing traffic to be erroneously routed.
This could be either a deliberate attack, or an accidental configuration error.
Loss of route visibility: A route is considered visible when a majority of BGP routers on the internet are able to reliably route to it. Loss of visibility refers to your network potentially being unavailable, for example if your BGP peering has stopped working.
New sub-prefix announcements: is when an AS begins announcing a prefix that is smaller that what is anticipated.
This could be indicative of an intended configuration change, an accidental misconfiguration, or in some cases an attack.
Activity within your AS: will usually refer to new route announcements.
A route is considered "new" if BGPalerter doesn't yet know about it.
Following are some example alerts, along with a short description of their meaning:
This alert shows evidence of a route hijack, where AS64496 has announced 203.0.113.0 / 24 when it is expected that this route would be announced by AS65540.
This is a strong indicator of a misconfiguration leading to a route leak, or a deliberate hijack by an attacker.
This alert shows that the 203.0.113.0 / 24 network is no longer visible.
This could be because of an upstream routing issue, or a router has suffered a power failure.
This alert shows that a more-specific prefix has been announced where it is not anticipated, for example by announcing a / 25 when only a / 24 is expected.
This is most likely a misconfiguration, however in some cases could be evidence of a route hijack.
Finally, this alert shows that AS64496 has announced a prefix that BGPalerter does not yet know about.
This could be because your are legitimately announcing a new prefix, or it could be indicative of a misconfiguration resulting in you accidentally announcing a prefix owned by someone else.
In this step, you reviewed some example BGPalerter alerts.
Next, you'll configure BGPalerter to run automatically at boot.
Step 5 - Starting BGPalerter at Boot
In this final step, you'll configure BGPalerter to run at boot.
Ensure that you're still logged in as your new non-privileged user, and then open the crontab editor:
Next, add the following entry to the bottom of the crontab file:
Every time your system boots, this will create a detached screen session called 'bgpalerter', and start BGPalerter within it.
Save and exit the crontab editor.
You may now wish to reboot your system in order to make sure that BGPalerter correctly starts at boot.
You'll first need to log out of your BGPalerter user:
Then proceed with a normal system reboot:
Once your system has rebooted, log back in to your server and use su to access your BGPalerter user again:
You can then attach to the session at any time in order to view the output from BGPalerter:
In this article you set up BGPalerter and used it to monitor networks for BGP routing changes.
If you wish to make BGPalerter more user-friendly, you can configure it to send alerts to a Slack channel via a webhook:
Configure Slack Reporting for BGPalerter
If you wish to learn more about BGP itself, but do not have access to a production BGP environment, you may enjoy using DN42 to experiment with BGP in a safe, isolated environment:
Decentralized Network 42
How To Set Up and Configure a Certificate Authority (CA) On Ubuntu 20.04
5387
In this guide, we'll learn how to set up a private Certificate Authority on an Ubuntu 20.04 server, and how to generate and sign a testing certificate using your new CA.
To complete this tutorial, you will need access to an Ubuntu 20.04 server to host your CA server.
You can follow our Ubuntu 20.04 initial server setup guide to set up a user with appropriate permissions.
If you choose to complete those practice steps, you will need a second Ubuntu 20.04 server or you can also use your own local Linux computer running Ubuntu or Debian, or distributions derived from either of those.
On Ubuntu and Debian based systems, run the following commands as your non-root user to import the certificate:
The following steps will be run on your second Ubuntu or Debian system, or distribution that is derived from either of those.
In this tutorial you created a private Certificate Authority using the Easy-RSA package on a standalone Ubuntu 20.04 server.
How To Use Go with MongoDB Using the MongoDB Go Driver
5369
After relying on community developed solutions for many years, MongoDB announced that they were working on an official driver for Go.
In March 2019, this new driver reached a production-ready status with the release of v1.0.0 and has been updated continually since then.
Like the other official MongoDB drivers, the Go driver is idiomatic to the Go programming language and provides an easy way to use MongoDB as the database solution for a Go program.
It is fully integrated with the MongoDB API, and exposes all of the query, indexing, and aggregation features of the API, along with other advanced features.
Unlike third-party libraries, it will be fully supported by MongoDB engineers so you can be assured of its continued development and maintenance.
In this tutorial you'll get started with using the official MongoDB Go Driver.
You'll install the driver, connect to a MongoDB database, and perform several CRUD operations.
In the process, you'll create a task manager program for managing tasks through the command line.
Go installed on your machine and a Go workspace configured following How To Install Go and Set Up a Local Programming Environment.
In this tutorial, the project will be named tasker.
You'll need Go v1.11 or higher installed on your machine with Go Modules enabled.
MongoDB installed for your operating system following How To Install MongoDB.
MongoDB 2.6 or higher is the minimum version supported by the MongoDB Go driver.
If you're using Go v1.11 or 1.12, ensure Go Modules is enabled by setting the GO111MODULE environment variable to on as shown following:
For more information on implementing environment variables, read this tutorial on How To Read and Set Environmental and Shell Variables.
The commands and code shown in this guide were tested with Go v1.14.1 and MongoDB v3.6.3.
Step 1 - Installing the MongoDB Go Driver
In this step, you'll install the Go Driver package for MongoDB and import it into your project.
You'll also connect to your MongoDB database and check the status of the connection.
Go ahead and create a new directory for this tutorial in your filesystem:
Once your project directory is set up, change into it with the following command:
Next, initialize the Go project with a go.mod file.
This file defines project requirements and locks dependencies to their correct versions:
If your project directory is outside the $GOPATH, you need to specify the import path for your module as follows:
At this point, your go.mod file will look like this:
Add the MongoDB Go Driver as a dependency for your project using following command:
Next, create a main.go file in your project root and open it in your text editor:
To get started with the driver, import the following packages into your main.go file:
Here you add the mongo and options packages, which the MongoDB Go driver provides.
Next, following your imports, create a new MongoDB client and connect to your running MongoDB server:
mongo.Connect () accepts a Context and a options.ClientOptions object, which is used to set the connection string and other driver settings.
You can visit the options package documentation to see what configuration options are available.
Context is like a timeout or deadline that indicates when an operation should stop running and return.
It helps to prevent performance degradation on production systems when specific operations are running slow.
In this code, you're passing context.TODO () to indicate that you're not sure what context to use right now, but you plan to add one in the future.
Next, let's ensure that your MongoDB server was found and connected to successfully using the Ping method.
Add the following code inside the init function:
If there are any errors while connecting to the database, the program should crash while you try to fix the problem as there's no point keeping the program running without an active database connection.
Add the following code to create a database:
You create a tasker database and a task collection to store the tasks you'll be creating.
You also set up collection as a package-level variable so you can reuse the database connection throughout the package.
The full main.go at this point is as follows:
You've set up your program to connect to your MongoDB server using the Go driver.
In the next step, you'll proceed with the creation of your task manager program.
Step 2 - Creating a CLI Program
In this step, you'll install the well-known cli package to aid with the development of your task manager program.
It provides an interface that you can take advantage of to rapidly create modern command line tools.
For example, this package gives the ability to define subcommands for your program for a more git-like command line experience.
Run the following command to add the package as a dependency:
Next, open up your main.go file again:
Add the following highlighted code to your main.go file:
You import the cli package as mentioned.
You also import the os package, which you'll use to pass command line arguments to your program:
Add the following code after your init function to create your CLI program and cause your code to compile:
This snippet creates a CLI program called tasker and adds a short usage description that will be printed out when you run the program.
The Commands slice is where you'll add commands for your program.
The Run command parses the arguments slice to the appropriate command.
Here's the command you need to build and run the program:
The program runs and shows help text, which is handy for learning about what the program can do, and how to use it.
In the next steps, you'll improve the utility of your program by adding subcommands to help manage your tasks in MongoDB.
Step 3 - Creating a Task
In this step, you'll add a subcommand to your CLI program using the cli package.
At the end of this section, you'll be able to add a new task to your MongoDB database by using a new add command in your CLI program.
Begin by opening up your main.go file:
Next, import the go.mongodb.org / mongo-driver / bson / primitive, time, and errors packages:
Then create a new struct to represent a single task in the database and insert it immediately preceding the main function:
You use the primitive package to set the type of the ID of each task since MongoDB uses ObjectIDs for the _ id field by default.
Another default behavior of MongoDB is that the lowercase field name is used as the key for each exported field when it is being serialized, but this can be changed using bson struct tags.
Next, create a function that receives an instance of Task and saves it in the database.
Add this snippet following the main function:
The collection.InsertOne () method inserts the provided task in the database collection and returns the ID of the document that was inserted.
Since you don't need this ID, you discard it by assigning to the underscore operator.
The next step is to add a new command to your task manager program for creating new tasks.
Let's call it add:
Every new command that is added to your CLI program is placed inside the Commands slice.
Each one consists of a name, usage description, and action.
This is the code that will run upon command execution.
In this code, you collect the first argument to add and use it to set the Text property of a new Task instance while assigning the appropriate defaults for the other properties.
The new task is subsequently passed on to createTask, which inserts the task into the database and returns nil if all goes well causing the command to exit.
Test it out by adding a few tasks using the add command.
If successful, you'll see no errors printed to your screen:
Now that you can add tasks successfully, let's implement a way to display all the tasks that you've added to the database.
Step 4 - Listing all Tasks
Listing the documents in a collection can be done using the collection.Find () method, which expects a filter as well as a pointer to a value into which the result can be decoded.
Its return value is a Cursor, which provides a stream of documents that can be iterated over and decoded one at a time.
The Cursor is then closed once it has been exhausted.
Open your main.go file:
Make sure to import the bson package:
Then create the following functions immediately after createTask:
BSON (Binary-encoded JSON) is how documents are represented in a MongoDB database and the bson package is what helps us work with BSON objects in Go.
The bson.D type used in the getAll () function represents a BSON document and it's used where the order of the properties matter.
By passing bson.D {{}} as your filter to filterTasks (), you're indicating that you want to match all the documents in the collection.
In the filterTasks () function, you iterate over the Cursor returned by the collection.Find () method and decode each document into an instance of Task.
Each Task is then appended to the slice of tasks created at the start of the function.
Once the Cursor is exhausted, it is closed and the tasks slice is returned.
Before you create a command for listing all tasks, let's create a helper function that takes a slice of tasks and prints to the standard output.
You'll be using the color package to colorize the output.
Before you can use the this package, install it with:
And import it into your main.go file along with the fmt package:
Next, create a new printTasks function following your main function:
This printTasks function takes a slice of tasks, iterates over each one, and prints it out to the standard output using the green color to indicate completed tasks, and yellow for incomplete tasks.
Go ahead and add the following highlighted lines to create a new all command to the Commands slice.
This command will print all added tasks to the standard output:
The all command retrieves all the tasks present in the database and prints them to the standard output.
If no tasks are present, a prompt to add a new task is printed instead.
Build and run your program with the all command:
It will list all the tasks that you've added so far:
Now that you can view all the tasks in the database, let's add the ability to mark a task as completed in the next step.
Step 5 - Completing a Task
In this step, you'll create a new subcommand called done that will allow you to mark an existing task in the database as completed.
To mark a task as completed, you can use the collection.FindOneAndUpdate () method.
It allows you to locate a document in a collection and update some or all of its properties.
This method requires a filter to locate the document and an update document to describe the operation.
Both of these are built using bson.D types.
Start by opening up your main.go file:
Insert the following snippet following your filterTasks function:
The function matches the first document where the text property is equal to the text parameter.
The update document specifies that the completed property be set to true.
If there's an error in the FindOneAndUpdate () operation, it will be returned by completeTask ().
Otherwise nil is returned.
Next, let's add a new done command to your CLI program that marks a task as completed:
You use the argument passed to the done command to find the first document whose text property matches.
If found, the completed property on the document is set to true.
Then run your program with the done command:
If you use the all command again, you will notice that the task that was marked as completed is now printed with green.
Screenshot of terminal output after completing a task
Sometimes, you only want to view tasks that have not yet been done.
We'll add that feature next.
Step 6 - Displaying Pending Tasks Only
In this step, you'll incorporate code to retrieve pending tasks from the database using the MongoDB driver.
Pending tasks are those whose completed property is set to false.
Let's add a new function that retrieves tasks that have not been completed yet.
Then add this snippet following the completeTask function:
You create a filter using the bson and primitive packages from the MongoDB driver, which will match documents whose completed property is set to false.
The slice of pending tasks is then returned to the caller.
Instead of creating a new command to list pending tasks, let's make it the default action when running the program without any commands.
You can do this by adding an Action property to the program as follows:
The Action property performs a default action when the program is executed without any subcommands.
This is where logic for listing pending tasks is placed.
The getPending () function is called and the resulting tasks are printed to the standard output using printTasks ().
If there are no pending tasks, a prompt is displayed instead, encouraging the user to add a new task using the add command.
Running the program now without adding any commands will list all pending tasks in the database:
Now that you can list incomplete tasks, let's add another command that allows you to view completed tasks only.
Step 7 - Displaying Finished Tasks
In this step, you'll add a new finished subcommand that fetches completed tasks from the database and displays them on the screen.
This involves filtering and returning tasks whose completed property is set to true.
Then add in the following code at the end of your file:
Similar to the getPending () function, you've added a getFinished () function that returns a slice of completed tasks.
In this case, the filter has the completed property set to true so only the documents that match this condition will be returned.
Next, create a finished command that prints all completed tasks:
The finished command retrieves tasks whose completed property is set to true via the getFinished () function created here.
It then passes it to the printTasks function so that they are printed to the standard output.
In the final step, you'll give users the option to delete tasks from the database.
Step 8 - Deleting a Task
In this step, you'll add a new delete subcommand to allow users to delete a task from the database.
To delete a single task, you'll use the collection.DeleteOne () method from the MongoDB driver.
It also relies on a filter to match the document to delete.
Open your main.go file once more:
Add this deleteTask function to delete tasks from the database straight after your getFinished function:
This deleteTask method takes a string argument that represents the task item to be deleted.
A filter is constructed to match the task item whose text property is set to the string argument.
You pass the filter to the DeleteOne () method that matches the item in the collection and deletes it.
You can check the DeletedCount property on the result from the DeleteOne method to confirm if any documents were deleted.
If the filter is unable to match a document to be deleted, the DeletedCount will be zero and you can return an error in that case.
Now add a new rm command as highlighted:
Just as with all the other subcommands added previously, the rm command uses its first argument to match a task in the database and deletes it.
You can list pending tasks by running your program without passing any subcommands:
Running the rm subcommand on the "Read a book" task will delete it from the database:
If you list all pending tasks again, you'll notice that the "Read a book" task does not appear anymore and a prompt to add a new task is shown instead:
In this step you added a function to delete tasks from the database.
You've successfully created a task manager command line program and learned the fundamentals of using the MongoDB Go driver in the process.
Be sure to check out the full documentation for the MongoDB Go Driver at GoDoc to learn more about the features that using the driver provides.
The documentation that describes using aggregations or transactions may be of particular interest to you.
The final code for this tutorial can be viewed in this GitHub repo.
How To Add Unit Testing to Your Django Project
5467
It is nearly impossible to build websites that work perfectly the first time without errors.
For that reason, you need to test your web application to find these errors and work on them proactively.
In order to improve the efficiency of tests, it is common to break down testing into units that test specific functionalities of the web application.
This practice is called unit testing.
It makes it easier to detect errors because the tests focus on small parts (units) of your project independently from other parts.
Testing a website can be a complex task to undertake because it is made up of several layers of logic like handling HTTP requests, form validation, and rendering templates.
However Django provides a set of tools that makes testing your web application seamless.
In Django, the preferred way to write tests is to use the Python unittest module, although it is possible to use other testing frameworks.
In this tutorial, you will set up a test suite in your Django project and write unit tests for the models and views in your application.
You will run these tests, analyze their results, and learn how to find the causes of failing tests.
Django installed on your server with a programming environment set up.
To do this, you can follow one of our How To Install the Django Web Framework and Set Up a Programming Environment tutorials.
A Django project created with models and views.
In this tutorial, we have followed the project from our Django Development tutorial series.
Step 1 - Adding a Test Suite to Your Django Application
A test suite in Django is a collection of all the test cases in all the apps in your project.
To make it possible for the Django testing utility to discover the test cases you have, you write the test cases in scripts whose names begin with test.
In this step, you'll create the directory structure and files for your test suite, and create an empty test case in it.
If you followed the Django Development tutorial series, you'll have a Django app called < ^ > blogsite < ^ >.
Let's create a folder to hold all our testing scripts.
First, activate the virtual environment:
Then navigate to the < ^ > blogsite < ^ > app directory, the folder that contains the models.py and views.py files, and then create a new folder called tests:
Next, you'll turn this folder into a Python package, so add an _ _ init _ _ .py file:
You'll now add a file for testing your models and another for testing your views:
Finally, you will create an empty test case in test _ models.py.
You will need to import the Django TestCase class and make it a super class of your own test case class.
Later on, you will add methods to this test case to test the logic in your models.
Open the file test _ models.py:
You've now successfully added a test suite to the blogsite app. Next, you will fill out the details of the empty model test case you created here.
Step 2 - Testing Your Python Code
In this step, you will test the logic of the code written in the models.py file.
In particular, you will be testing the save method of the Post model to ensure it creates the correct slug of a post's title when called.
Let's begin by looking at the code you already have in your models.py file for the save method of the Post model:
We can see that it checks whether the post about to be saved has a slug value, and if not, calls slugify to create a slug value for it. This is the type of logic you might want to test to ensure that slugs are actually created when saving a post.
Close the file.
To test this, go back to test _ models.py:
Then update it to the following, adding in the highlighted portions:
This new method test _ post _ has _ slug creates a new post with the title "My first post" and then gives the post an author and saves the post.
After this, using the assertEqual method from the Python unittest module, it checks whether the slug for the post is correct.
The assertEqual method checks whether the two arguments passed to it are equal as determined by the "= =" operator and raises an error if they are not.
Save and exit test _ models.py.
This is an example of what can be tested.
The more logic you add to your project, the more there is to test.
If you add more logic to the save method or create new methods for the Post model, you would want to add more tests here.
You can add them to the test _ post _ has _ slug method or create new test methods, but their names must begin with test.
You have successfully created a test case for the Post model where you asserted that slugs are correctly created after saving.
In the next step, you will write a test case to test views.
Step 3 - Using Django's Test Client
In this step, you will write a test case that tests a view using the Django test client.
The test client is a Python class that acts as a dummy web browser, allowing you to test your views and interact with your Django application the same way a user would.
You can access the test client by referring to self.client in your test methods.
For example, let us create a test case in test _ views.py.
First, open the test _ views.py file:
The ViewsTestCase contains a test _ index _ loads _ properly method that uses the Django test client to visit the index page of the website (http: / / < ^ > your _ server _ ip < ^ >: 8000, where < ^ > your _ server _ ip < ^ > is the IP address of the server you are using).
Then the test method checks whether the response has a status code of 200, which means the page responded without any errors.
As a result you can be sure that when the user visits, it will respond without errors too.
Apart from the status code, you can read about other properties of the test client response you can test in the Django Documentation Testing Responses page.
In this step, you created a test case for testing that the view rendering the index page works without errors.
There are now two test cases in your test suite.
In the next step you will run them to see their results.
Step 4 - Running Your Tests
Now that you have finished building a suite of tests for the project, it is time to execute these tests and see their results.
To run the tests, navigate to the < ^ > blog < ^ > folder (containing the application's manage.py file):
Then run them with:
You'll see output similar to the following in your terminal:
In this output, there are two dots.., each of which represents a passed test case.
Now you'll modify test _ views.py to trigger a failing test.
First open the file with:
Then change the highlighted code to:
Here you have changed the status code from 200 to 404. Now run the test again from your directory with manage.py:
You see that there is a descriptive failure message that tells you the script, test case, and method that failed.
It also tells you the cause of the failure, the status code not being equal to 404 in this case, with the message AssertionError: 200!
= 404. The AssertionError here is raised at the highlighted line of code in the test _ views.py file:
It tells you that the assertion is false, that is, the response status code (200) is not what was expected (404).
Preceding the failure message, you can see that the two dots.. have now changed to.
F, which tells you that the first test case passed while the second didn't.
In this tutorial, you created a test suite in your Django project, added test cases to test model and view logic, learned how to run tests, and analyzed the test output.
As a next step, you can create new test scripts for Python code not in models.py and views.py.
Following are some articles that may prove helpful when building and testing websites with Django:
The Django Unit Tests documentation
The Scaling Django tutorial series
You can also check out our Django topic page for further tutorials and projects.
How To Create a Minecraft Server on Ubuntu 18.04
5474
Minecraft is a popular sandbox video game.
Originally released in 2009, it allows players to build, explore, craft, and survive in a block 3D generated world.
As of late 2019, it was the second best-selling video game of all time.
In this tutorial, you will create your own Minecraft server so that you and your friends can play together.
Specifically, you will install the necessary software packages to run Minecraft, configure the server to run, and then deploy the game.
Alternately, you can explore DigitalOcean's One-Click Minecraft: Java Edition Server as another installation path.
< $> note This tutorial uses the Java version of Minecraft.
If you purchased your version of Minecraft through the Microsoft App Store, you will be unable to connect to this server.
Most versions of Minecraft purchased on gaming consoles such as the PlayStation 4, Xbox One, or Nintendo Switch are also the Microsoft version of Minecraft.
These consoles are also unable to connect to the server built in this tutorial.
You can obtain the Java version of Minecraft here.
A server with a fresh installation of Ubuntu 18.04, a non-root user with sudo privileges, and SSH enabled.
You can follow this guide to initialize your server and complete these steps.
Minecraft can be resource-intensive, so keep that in mind when selecting your server size.
If you are using DigitalOcean and need more resources, you can always resize your Droplet to add more CPUs and RAM.
A copy of Minecraft Java Edition installed on a local Mac, Windows, or Linux machine.
Step 1 - Installing the Necessary Software Packages
With your server initialized, your first step is to install Java; you'll need it to run Minecraft.
Next, install the OpenJDK version 8 of Java, specifically the headless JRE.
This is a minimal version of Java that removes the support for GUI applications.
This makes it ideal for running Java applications on a server:
You also need to use a software called screen to create detachable server sessions. screen allows you to create a terminal session and detach from it, leaving the process started on it running.
This is important because if you were to start your server and then close your terminal, this would kill the session and stop your server.
Install screen now:
Now that you have Java installed, you will download the Minecraft server from the Minecraft website.
Step 2 - Downloading the Latest Version of Minecraft
Now you need to download the current version of the Minecraft server.
You can do this by navigating to Minecraft's Website and copying the link that says Download minecraft _ server.
< ^ > X.X.X < ^ > .jar, where the X's are the latest version of the server.
You can now use wget and the copied link to download the server:
If you intend to upgrade your Minecraft server, or if you want to run different versions of Minecraft, rename the downloaded server.jar to minecraft _ server _ < ^ > 1.15.2 < ^ > .jar, matching the highlighted version numbers to whatever version you just downloaded:
If you want to download an older version of Minecraft, you can find them archived at mcversions.net.
But this tutorial will focus on the current latest release.
Now that you have your download let's start configuring your Minecraft server.
Step 3 - Configuring and Running the Minecraft Server
Now that you have the Minecraft jar downloaded, you are ready to run it.
First, start a screen session by running the screen command:
Once you have read the banner that has appeared, press the SPACE bar. screen will present you with a terminal session like normal.
This session is now detachable, which means that you'll be able to start a command here and leave it running.
You can now perform your initial configuration.
Do not be alarmed when this next command throws an error.
Minecraft has designed its installation this way so that users must first consent to the company's licensing agreement.
You will do this next:
Before examining this command's output, let's take a closer look at all these command-line arguments, which are tuning your server:
Xms1024M - This configures the server to start running with 1024MB or 1GB of RAM running.
You can raise this limit if you want your server to start with more RAM.
Both M for megabytes and G for gigabytes are supported options.
For example: Xms2G will start the server with 2 gigabytes of RAM.
Xmx1024M - This configures the server to use, at most, 1024M of RAM.
You can raise this limit if you want your server to run at a larger size, allow for more players, or if you feel that your server is running slowly.
jar - This flag specifies which server jar file to run.
nogui - This tells the server not to launch a GUI since this is a server, and you don't have a graphical user interface.
The first time you run this command, which normally starts your server, it will instead generate the following error:
These errors were generated because the server could not find two necessary files required for execution: the EULA (End User License Agreement), found in eula.txt, and the configuration file server.properties.
Fortunately, since the server was unable to find these files, it created them in your current working directory.
First, open eula.txt in nano or your favorite text editor:
Inside this file, you will see a link to the Minecraft EULA.
Copy the URL:
Open the URL in your web browser and read the agreement.
Then return to your text editor and find the last line in eula.txt.
Here, change eula = false to eula = < ^ > true < ^ >.
Now save and close the file.
Now that you've accepted the EULA, it is time to configure the server to your specifications.
In your current working directory, you will also find the newly created server.properties file.
This file contains all of the configuration options for your Minecraft server.
You can find a detailed list of all server properties on the Official Minecraft Wiki.
You will modify this file with your preferred settings before starting your server.
This tutorial will cover the fundamental properties:
Your file will appear like this:
Let's take a closer look at some of the most important properties in this list:
difficulty (default < ^ > easy < ^ >) - This sets the difficulty of the game, such as how much damage is dealt and how the elements affect your player.
The options are peaceful, < ^ > easy < ^ >, normal, and hard.
gamemode (default < ^ > survival < ^ >) - This sets the gameplay mode.
The options are < ^ > survival < ^ >, creative, adventure, and spectator.
level-name (default < ^ > world < ^ >) - This sets the name of your server that will appear in the client.
Characters such as the apostrophe may need to be escaped with a backslash.
motd (default < ^ > A Minecraft Server < ^ >) - The message that is displayed in the server list of the Minecraft client.
pvp (default < ^ > true < ^ >) - Enables Player versus Player combat.
If set to true, players will be able to engage in combat and damage each other.
Once you have set the options that you want, save and close the file.
Now that you have changed EULA to < ^ > true < ^ > and configured your settings, you can successfully start your server.
Like last time, let's start your server with 1024M of RAM.
Only now, let's also grant Minecraft the ability to use up to 4G of RAM if it needs it. Remember, you are welcome to adjust this number to fit your server limitations or user needs:
Give the initialization a few moments.
Soon your new Minecraft server will start producing an output similar to this:
Once the server is up and running, you will see the following output:
Your server is now running, and you have been dropped into the server administrator control panel.
Now type help:
An output like this will appear:
From this terminal you can execute administrator commands and control your Minecraft server.
Now let's use screen to keep your new server running, even after you log out. Then you can connect to your Minecraft client and start a new game.
Step 4 - Keeping the Server Running
Now that you have your server up, you want it to remain running even after you disconnect from your SSH session.
Since you used screen earlier, you can detach from this session by pressing Ctrl + A + D. Now you're back in your original shell.
Run this command to see all of your screen sessions:
You'll get an output with the ID of your session, which you'll need to resume that session:
To resume your session, pass the -r flag to the screen command and then enter your session ID:
When you are ready to log out of your server, be sure to detach from the session with Ctrl + A + D and then log out.
Step 5 - Connecting to Your Server from the Minecraft Client
Now that your server is up and running, let's connect to it through the Minecraft client.
Then you can play!
Launch your copy of Minecraft Java Edition and select Multiplayer in the menu.
Select Multiplayer in the menu
Next, you will need to add a server to connect to, so click on the Add Server button.
Click the Add Server button
In the Edit Server Info screen that shows up, give your server a name and type in the IP address of your server.
This is the same IP address that you used to connect through SSH.
Name your server and type in the IP address
Once you have entered your server name and IP address, you'll be taken back to the Multiplayer screen where your server will now be listed.
Select your server and click Join Server
From now on, your server will always appear in this list.
Select it and click Join Server.
Enjoy the game!
You are in your server and ready to play!
You now have a Minecraft server running on Ubuntu 18.04 for you and all of your friends to play on!
Have fun exploring, crafting, and surviving in a crude 3D world.
And remember: watch out for griefers.
How To Customize React Components with Props
5468
In this tutorial, you'll create custom components by passing props to your component.
Props are arguments that you provide to a JSX element.
They look like standard HTML props, but they aren't predefined and can have many different JavaScript data types including numbers, strings, functions, arrays, and even other React components.
Your custom components can use props to display data or use the data to make the components interactive.
Props are a key part of creating components that are adaptable to different situations, and learning about them will give you the tools to develop custom components that can handle unique situations.
After adding props to your component, you will use PropTypes to define the type of data you expect a component to receive.
PropTypes are a simple type system to check that data matches the expected types during runtime.
They serve as both documentation and an error checker that will help keep your application predictable as it scales.
By the end of the tutorial, you'll use a variety of props to build a small application that will take an array of animal data and display the information, including the name, scientific name, size, diet, and additional information.
< $> note Note: The first step sets up a blank project on which you will build the tutorial exercise.
If you already have a working project and want to go directly to working with props, start with Step 2. < $>
In following this tutorial, you will use Create React App.
This tutorial also assumes a knowledge of React components, which you can learn about in our How To Create Custom Components in React tutorial.
Step 1 - Creating an Empty Project
In this step, you'll create a new project using Create React App.
Then you will delete the sample project and related files that are installed when you bootstrap the project.
Finally, you will create a simple file structure to organize your components.
In your command line, run the following script to install a fresh project using create-react-app:
If the project did not open in a browser window, you can open it by navigating to http: / / localhost: 3000 /.
If you are running this from a remote server, the address will be http: / / < ^ > your _ domain < ^ >: 3000.
Your browser will load with a simple React application included as part of Create React App:
You will be building a completely new set of custom components.
You'll start by clearing out some boilerplate code so that you can have an empty project.
To start, open src / App.js in a text editor.
You can find more information about App.js at How To Set Up a React Project with Create React App.
Open src / App.js with the following command:
Delete the line import logo from '. / logo.svg ';.
Then replace everything in the return statement to return a set of empty tags: < > < / >.
This will give you a validate page that returns nothing.
You won't be using it in your application and you should remove unused files as you work.
It will save you from confusion in the future.
If you look at your browser, you will see a blank screen.
blank screen in chrome
Now that you have cleared out the sample Create React App project, create a simple file structure.
This will help you keep your components isolated and independent.
Create a directory called components in the src directory.
This will hold all of your custom components.
Each component will have its own directory to store the component file along with the styles, images if there are any, and tests.
Create a directory for App:
Move all of the App files into that directory.
Use the wildcard, *, to select any files that start with App. regardless of file extension.
Then use the mv command to put them into the new directory.
Finally, update the relative import path in index.js, which is the root component that bootstraps the whole process.
The import statement needs to point to the App.js file in the App directory, so make the following highlighted change:
Now that the project is set up, you can create your first component.
Step 2 - Building Dynamic Components with Props
In this step, you will create a component that will change based on the input information called props.
Props are the arguments you pass to a function or class, but since your components are transformed into HTML-like objects with JSX, you will pass the props like they are HTML attributes.
Unlike HTML elements, you can pass many different data types, from strings, to arrays, to objects, and even functions.
Here you will create a component that will display information about animals.
This component will take the name and scientific name of the animal as strings, the size as an integer, the diet as an array of strings, and additional information as an object.
You'll pass the information to the new component as props and consume that information in your component.
By the end of this step, you'll have a custom component that will consume different props.
You'll also reuse the component to display an array of data using a common component.
Adding Data
First, you need some sample data. Create a file in the src / App directory called data.
Open the new file in your text editor:
Next, add an array of objects you will use as sample data:
The array of objects contains a variety of data and will give you an opportunity to try a variety of props.
Each object is a separate animal with the name of the animal, the scientific name, size, diet, and an optional field called additional, which will contain links or notes.
In this code, you also exported the array as the default.
Creating Components
Next, create a placeholder component called AnimalCard.
This component will eventually take props and display the data.
First, make a directory in src / components called AnimalCard then touch a file called src / components / AnimalCard / AnimalCard.js and a CSS file called src / components / AnimalCard / AnimalCard.css.
Open AnimalCard.js in your text editor:
Add a basic component that imports the CSS and returns an < h2 > tag.
Now you need to import the data and component into your base App component.
Open src / components / App / App.js:
Import the data and the component, then loop over the data returning the component for each item in the array:
Here, you use the .map () array method to iterate over the data. In addition to adding this loop, you also have a wrapping div with a class that you will use for styling and an < h1 > tag to label your project.
When you save, the browser will reload and you'll see a label for each card.
React project in the browser without styling
Next, add some styling to line up the items.
Open App.css:
Replace the contents with the following to arrange the elements:
This will use flexbox to rearrange the data so it will line up.
The padding gives some space in the browser window. justify-content will spread out the extra space between elements, and .wrapper h1 will give the Animal label the full width.
When you do, the browser will refresh and you'll see some data spaced out.
React project in the browser with data spaced out
Adding Props
Now that you have your components set up, you can add your first prop.
When you looped over your data, you had access to each object in the data array and the items it contained.
You will add each piece of the data to a separate prop that you will then use in your AnimalCard component.
Add a prop of name to AnimalCard.
The name prop looks like a standard HTML attribute, but instead of a string, you'll pass the name property from the animal object in curly braces.
Now that you've passed one prop to the new component, you need to use it. Open AnimalCard.js:
All props that you pass into the component are collected into an object that will be the first argument of your function.
Destructure the object to pull out individual props:
Note that you do not need to destructure a prop to use it, but that this is a useful method for dealing with the sample data in this tutorial.
After you destructure the object, you can use the individual pieces of data. In this case, you'll use the title in an < h2 > tag, surrounding the value with curly braces so that React will know to evaluate it as JavaScript.
You can also use a property on the prop object using dot notation.
As an example, you could create an < h2 > element like this: < h2 > {props.title} < / h2 >.
The advantage of destructring is that you can collect unused props and use the object rest operator.
When you do, the browser will reload and you'll see the specific name for each animal instead of a placeholder.
React projects with animal names rendered
The name property is a string, but props can be any data type that you could pass to a JavaScript function.
To see this at work, add the rest of the data.
Open the App.js file:
Add a prop for each of the following: scientificName, size, diet, and additional.
These include strings, integers, arrays, and objects.
Since you are creating an object, you can add them in any order you want.
Alphabetizing makes it easier to skim a list of props especially in a larger list.
You also can add them on the same line, but separating to one per line keeps things readable.
Open AnimalCard.js.
This time, destructure the props in the function parameter list and use the data in the component:
After pulling out the data, you can add the scientificName and size into heading tags, but you'll need to convert the array into a string so that React can display it on the page.
You can do that with join (', '), which will create a comma separated list.
When you do, the browser will refresh and you'll see the structured data.
React project with animals with full data
You could create a similar list with the additional object, but instead add a function to alert the user with the data. This will give you the chance to pass functions as props and then use data inside a component when you call a function.
Create a function called showAdditionalData that will convert the object to a string and display it as an alert.
The function showAdditional converts the object to an array of pairs where the first item is the key and the second is the value.
It then maps over the data converting the key-pair to a string.
Then it joins them with a line break -\ n - before passing the complete string to the alert function.
Since JavaScript can accept functions as arguments, React can also accept functions as props.
You can therefore pass showAdditional to AnimalCard as a prop called showAdditional.
Open AnimalCard:
Pull the showAdditional function from the props object, then create a < button > with an onClick event that calls the function with the additional object:
When you do, the browser will refresh and you'll see a button after each card.
When you click the button, you'll get an alert with the additional data.
Alert with information
If you try clicking More Info for the Lion, you will get an error.
That's because there is no additional data for the lion.
You'll see how to fix that in Step 3.
Finally, add some styling to the music card.
Add a className of animal-wrapper to the div in AnimalCard:
Open AnimalCard.css:
Add CSS to give the cards and the button a small border and padding:
This CSS will add a slight border to the card and replace the default button styling with a border and padding. cursor: pointer will change the cursor when you hover over the button.
When you do the browser will refresh and you'll see the data in individual cards.
React project with styled animal cards
At this point, you've created two custom components.
You've passed data to the second component from the first component using props.
The props included a variety of data, such as strings, integers, arrays, objects, and functions.
In your second component, you used the props to create a dynamic component using JSX.
In the next step, you'll use a type system called prop-types to specify the structure your component expects to see, which will create predictability in your app and prevent bugs.
Step 3 - Creating Predictable Props with PropTypes and defaultProps
In this step, you'll add a light type system to your components with PropTypes.
PropTypes act like other type systems by explicitly defining the type of data you expect to receive for a certain prop.
They also give you the chance to define default data in cases where the prop is not always required.
Unlike most type systems, PropTypes is a runtime check, so if the props do not match the type the code will still compile, but will also display a console error.
By the end of this step, you'll add predictability to your custom component by defining the type for each prop.
This will ensure that the next person to work on the component will have a clear idea of the structure of the data the component will need.
The prop-types package is included as part of the Create React App installation, so to use it, all you have to do is import it into your component.
Open up AnimalCard.js:
Then import PropTypes from prop-types:
Add PropTypes directly to the component function.
In JavaScript, functions are objects, which means you can add properties using dot syntax.
Add the following PropTypes to AnimalCard.js:
As you can see, there are many different PropTypes.
This is only a small sample; see the official React documentation to see the others you can use.
Let's start with the name prop.
Here, you are specifying that name must be a string.
The property scientificName is the same. size is a number, which can include both floats such as 1.5 and integers such as 6. showAdditional is a function (func).
diet, on the other hand, is a little different.
In this case, you are specifying that diet will be an array, but you also need to specify what this array will contain.
In this case, the array will only contain strings.
If you want to mix types, you can use another prop called oneOfType, which takes an array of valid PropTypes.
You can use oneOfType anywhere, so if you wanted size to be either a number or a string you could change it to this:
The prop additional is also a little more complex.
In this case, you are specifying an object, but to be a little more clear, you are stating what you want the object to contain.
To do that, you use PropTypes.shape, which takes an object with additional fields that will need their own PropTypes.
In this case, link and notes are both PropTypes.string.
Currently, all of the data is well-formed and matches the props.
To see what happens if the PropTypes don't match, open up your data:
Change the size to a string on the first item:
When you do the browser will refresh and you'll see an error in the console.
Browser with type error
Unlike other type systems such as TypeScript, PropTypes will not give you a warning at build time, and as long as there are no code errors, it will still compile.
This means that you could accidentally publish code with prop errors.
Change the data back to the correct type:
Every prop except for additional has the isRequired property.
That means, that they are required.
If you don't include a required prop, the code will still compile, but you'll see a runtime error in the console.
If a prop isn't required, you can add a default value.
It's good practice to always add a default value to prevent runtime errors if a prop is not required.
For example, in the AnimalCard component, you are calling a function with the additional data. If it's not there, the function will try and modify an object that doesn't exist and the application will crash.
To prevent this problem, add a defaultProp for additional:
You add the defaultProps to the function using dot syntax just as you did with propTypes, then you add a default value that the component should use if the prop is undefined.
In this case, you are matching the shape of additional, including a message that the there is no additional information.
When you do, the browser will refresh.
After it refreshes, click on the More Info button for Lion.
It has no additional field in the data so the prop is undefined.
But AnimalCard will substitute in the default prop.
Browser with default message in the alert
Now your props are well-documented and are either required or have a default to ensure predictable code.
This will help future developers (including yourself) understand what props a component needs.
It will make it easier to swap and reuse your components by giving you full information about how the component will use the data it is receiving.
In this tutorial, you have created several components that use props to display information from a parent.
Props give you the flexibility to begin to break larger components into smaller, more focused pieces.
Now that you no longer have your data tightly coupled with your display information, you have the ability to make choices about how to segment your application.
Props are a crucial tool in building complex applications, giving the opportunity to create components that can adapt to the data they receive.
With PropTypes, you are creating predictable and readable components that will give a team the ability to reuse each other's work to create a flexible and stable code base.
How To Install and Secure phpMyAdmin on Ubuntu 20.04
5417
While many users need the functionality of a database management system like MySQL, they may not feel comfortable interacting with the system solely from the MySQL prompt.
phpMyAdmin was created so that users can interact with MySQL through a web interface.
In this guide, we'll discuss how to install and secure phpMyAdmin so that you can safely use it to manage your databases on an Ubuntu 20.04 system.
In order to complete this guide, you will need:
An Ubuntu 20.04 server.
This server should have a non-root user with administrative privileges and a firewall configured with ufw.
A LAMP (Linux, Apache, MySQL, and PHP) stack installed on your Ubuntu 20.04 server.
If this is not completed yet, you can follow this guide on installing a LAMP stack on Ubuntu 20.04.
Additionally, there are important security considerations when using software like phpMyAdmin, since it:
Communicates directly with your MySQL installation
Handles authentication using MySQL credentials
Executes and returns results for arbitrary SQL queries
For these reasons, and because it is a widely-deployed PHP application which is frequently targeted for attack, you should never run phpMyAdmin on remote systems over a plain HTTP connection.
If you do not have an existing domain configured with an SSL / TLS certificate, you can follow this guide on securing Apache with Let's Encrypt on Ubuntu 20.04.
This will require you to register a domain name, create DNS records for your server, and set up an Apache Virtual Host.
Step 1 - Installing phpMyAdmin
You can use APT to install phpMyAdmin from the default Ubuntu repositories.
As your non-root sudo user, update your server "s package index:
Following that you can install the phpmyadmin package.
Along with this package, the official documentation also recommends that you install a few PHP extensions onto your server to enable certain functionalities and improve performance.
If you followed the prerequisite LAMP stack tutorial, several of these modules will have been installed along with the php package.
However, it's recommended that you also install these packages:
php-mbstring: A module for managing non-ASCII strings and convert strings to different encodings
php-zip: This extension supports uploading .zip files to phpMyAdmin
php-gd: Enables support for the GD Graphics Library
php-json: Provides PHP with support for JSON serialization
php-curl: Allows PHP to interact with different kinds of servers using different protocols
Run the following command to install these packages onto your system.
Please note, though, that the installation process requires you to make some choices to configure phpMyAdmin correctly.
We'll walk through these options shortly:
Here are the options you should choose when prompted in order to configure your installation correctly:
For the server selection, choose apache2 < $> warning Warning: When the prompt appears, "apache2" is highlighted, but not selected.
If you do not hit SPACE to select Apache, the installer will not move the necessary files during installation.
Hit SPACE, TAB, and then ENTER to select Apache.
Select Yes when asked whether to use dbconfig-common to set up the database
You will then be asked to choose and confirm a MySQL application password for phpMyAdmin
< $> note Note: Assuming you installed MySQL by following Step 2 of the prerequisite LAMP stack tutorial, you may have decided to enable the Validate Password plugin.
As of this writing, enabling this component will trigger an error when you attempt to set a password for the phpmyadmin user:
phpMyAdmin password validation error
To resolve this, select the abort option to stop the installation process.
Then, open up your MySQL prompt:
Or, if you enabled password authentication for the root MySQL user, run this command and then enter your password when prompted:
From the prompt, run the following command to disable the Validate Password component.
Note that this won't actually uninstall it, but just stop the component from being loaded on your MySQL server:
Following that, you can close the MySQL client:
Then try installing the phpmyadmin package again and it will work as expected:
Once phpMyAdmin is installed, you can open the MySQL prompt once again with sudo mysql or mysql -u root -p and then run the following command to re-enable the Validate Password component:
The installation process adds the phpMyAdmin Apache configuration file into the / etc / apache2 / conf-enabled / directory, where it is read automatically.
To finish configuring Apache and PHP to work with phpMyAdmin, the only remaining task in this section of the tutorial is to is explicitly enable the mbstring PHP extension, which you can do by typing:
Afterwards, restart Apache for your changes to be recognized:
phpMyAdmin is now installed and configured to work with Apache.
However, before you can log in and begin interacting with your MySQL databases, you will need to ensure that your MySQL users have the privileges required for interacting with the program.
Step 2 - Adjusting User Authentication and Privileges
When you installed phpMyAdmin onto your server, it automatically created a database user called phpmyadmin which performs certain underlying processes for the program.
Rather than logging in as this user with the administrative password you set during installation, it "s recommended that you log in as either your root MySQL user or as a user dedicated to managing databases through the phpMyAdmin interface.
Configuring Password Access for the MySQL Root Account
This allows for some greater security and usability in many cases, but it can also complicate things when you need to allow an external program - like phpMyAdmin - to access the user.
In order to log in to phpMyAdmin as your root MySQL user, you will need to switch its authentication method from auth _ socket to one that makes use of a password, if you haven "t already done so.
However, some versions of PHP don't work reliably with caching _ sha2 _ password.
PHP has reported that this issue was fixed as of PHP 7.4, but if you encounter an error when trying to log in to phpMyAdmin later on, you may want to set root to authenticate with mysql _ native _ password instead:
Then, check the authentication methods employed by each of your users again to confirm that root no longer authenticates using the auth _ socket plugin:
You can see from this output that the root user will authenticate using a password.
You can now log in to the phpMyAdmin interface as your root user with the password you "ve set for it here.
Configuring Password Access for a Dedicated MySQL User
Alternatively, some may find that it better suits their workflow to connect to phpMyAdmin with a dedicated user.
To do this, open up the MySQL shell once again:
If you have password authentication enabled for your root user, as described in the previous section, you will need to run the following command and enter your password when prompted in order to connect:
< $> note Note: Again, depending on what version of PHP you have installed, you may want to set your new user to authenticate with mysql _ native _ password instead of caching _ sha2 _ password:
Following that, exit the MySQL shell:
You can now access the web interface by visiting your server's domain name or public IP address followed by / phpmyadmin:
phpMyAdmin login screen
Log in to the interface, either as root or with the new username and password you just configured.
When you log in, you'll see the user interface, which will look something like this:
phpMyAdmin user interface
Now that you "re able to connect and interact with phpMyAdmin, all that" s left to do is harden your system's security to protect it from attackers.
Step 3 - Securing Your phpMyAdmin Instance
Because of its ubiquity, phpMyAdmin is a popular target for attackers, and you should take extra care to prevent unauthorized access.
To do this, you must first enable the use of .htaccess file overrides by editing your phpMyAdmin installation's Apache configuration file.
Use your preferred text editor to edit the phpmyadmin.conf file that has been placed in your Apache configuration directory.
Add an AllowOverride All directive within the < Directory / usr / share / phpmyadmin > section of the configuration file, like this:
When you have added this line, save and close the file.
To implement the changes you made, restart Apache:
Now that you have enabled the use of .htaccess files for your application, you need to create one to actually implement some security.
In order for this to be successful, the file must be created within the application directory.
You can create the necessary file and open it in your text editor with root privileges by typing:
Within this file, enter the following information:
Here is what each of these lines mean:
AuthType Basic: This line specifies the authentication type that you are implementing.
This type will implement password authentication using a password file.
AuthName: This sets the message for the authentication dialog box.
You should keep this generic so that unauthorized users won't gain any information about what is being protected.
AuthUserFile: This sets the location of the password file that will be used for authentication.
This should be outside of the directories that are being served.
We will create this file shortly.
Require valid-user: This specifies that only authenticated users should be given access to this resource.
This is what actually stops unauthorized users from entering.
The location that you selected for your password file was / etc / phpmyadmin / .htpasswd.
You can now create this file and pass it an initial user with the htpasswd utility:
You will be prompted to select and confirm a password for the user you are creating.
Afterwards, the file is created with the hashed password that you entered.
If you want to enter an additional user, you need to do so without the -c flag, like this:
Now, when you access your phpMyAdmin subdirectory, you will be prompted for the additional account name and password that you just configured:
phpMyAdmin apache password
After entering the Apache authentication, you'll be taken to the regular phpMyAdmin authentication page to enter your MySQL credentials.
By adding an extra set of non-MySQL credentials, you're providing your database with an additional layer of security.
This is desirable, since phpMyAdmin has been vulnerable to security threats in the past.
You should now have phpMyAdmin configured and ready to use on your Ubuntu 20.04 server.
Using this interface, you can create databases, users, and tables, as well as perform the usual operations like deleting and modifying structures and data.
How to Install and Use Composer on Ubuntu 20.04
5462
Composer is a popular dependency management tool for PHP, created mainly to facilitate installation and updates for project dependencies.
It will check which other packages a specific project depends on and install them for you, using the appropriate versions according to the project requirements.
Composer is also commonly used to bootstrap new projects based on popular PHP frameworks, such as Symfony and Laravel.
In this tutorial, you'll install and get started with Composer on an Ubuntu 20.04 system.
In order to follow this guide, you will need access to an Ubuntu 20.04 server as a non-root sudo user, and a firewall enabled on your server.
Step 1 - Installing PHP and Additional Dependencies
In addition to dependencies that should be already included within your Ubuntu 20.04 system, such as git and curl, Composer requires php-cli in order to execute PHP scripts in the command line, and unzip to extract zipped archives.
We'll install these dependencies now.
First, update the package manager cache by running:
Next, run the following command to install the required packages:
You will be prompted to confirm installation by typing Y and then ENTER.
Once the prerequisites are installed, you can proceed to installing Composer.
Step 2 - Downloading and Installing Composer
Composer provides an installer script written in PHP.
We'll download it, verify that it's not corrupted, and then use it to install Composer.
Make sure you're in your home directory, then retrieve the installer using curl:
Next, we'll verify that the downloaded installer matches the SHA-384 hash for the latest installer found on the Composer Public Keys / Signatures page.
To facilitate the verification step, you can use the following command to programmatically obtain the latest hash from the Composer page and store it in a shell variable:
If you want to verify the obtained value, you can run:
Now execute the following PHP code, as provided in the Composer download page, to verify that the installation script is safe to run:
If the output says Installer corrupt, you'll need to download the installation script again and double check that you're using the correct hash.
Then, repeat the verification process.
When you have a verified installer, you can continue.
To install composer globally, use the following command which will download and install Composer as a system-wide command named composer, under / usr / local / bin:
To test your installation, run:
This verifies that Composer was successfully installed on your system and is available system-wide.
< $> note Note: If you prefer to have separate Composer executables for each project you host on this server, you can install it locally, on a per-project basis.
This method is also useful when your system user doesn't have permission to install software system-wide.
To do this, use the command php composer-setup.php.
This will generate a composer.phar file in your current directory, which can be executed with php composer.phar.
Now let's look at using Composer to manage dependencies.
Step 3 - Using Composer in a PHP Project
PHP projects often depend on external libraries, and managing those dependencies and their versions can be tricky.
Composer solves that problem by keeping track of project versions and dependencies, while also facilitating the process of finding, installing, and updating packages that are required by a project.
In order to use Composer in your project, you'll need a composer.json file.
The composer.json file tells Composer which dependencies it needs to download for your project, and which versions of each package are allowed to be installed.
This is extremely important to keep your project consistent and avoid installing unstable versions that could potentially cause backwards compatibility issues.
You don't need to create this file manually - it's common to run into syntax errors when you do so.
Composer offers an interactive way to create a new composer.json file based on the user's input, which is a good choice if you plan on sharing your project later as a public package on Packagist.
Composer also auto-generates a barebones composer.json file when you run a composer require command to include a dependency in a newly created project.
The process of using Composer to install a package as dependency in a project involves the following steps:
Identify what kind of library the application needs.
Research a suitable open source library on Packagist.org, the official package repository for Composer.
Choose the package you want to depend on.
Run composer require to include the dependency in the composer.json file and install the package.
Let's try this out with a demo application.
The goal of this application is to transform a given sentence into a URL-friendly string - a slug.
This is commonly used to convert page titles to URL paths (like the final portion of the URL for this tutorial).
Let's start by creating a directory for our project.
We'll call it slugify:
Although not required, you could now run a composer init command to create a detailed composer.json file for your project.
Because our project's only objective is to demonstrate how to install dependencies with Composer, we'll use a simpler composer.json file that will be auto-generated when we require our first package.
Now it's time to search Packagist.org for a package that can help us generate slugs.
If you search for the term "slug" on Packagist, you'll get a result similar to this:
Packagist Search Results for the term "slug"
You'll see two numbers on the right side of each package in the list.
The number on the top represents how many times the package was installed via Composer, and the number on the bottom shows how many times a package was starred on GitHub.
Generally speaking, packages with more installations and more stars tend to be more stable, since so many people are using them.
It's also important to check the package description for relevance to make sure it's what you need.
We need a string-to-slug converter.
From the search results, the package cocur / slugify, which appears as the first result in that page, seems to be a good match, with a reasonable amount of installations and stars.
Packages on Packagist have a vendor name and a package name.
Each package has a unique identifier (a namespace) in the same format GitHub uses for its repositories: < ^ > vendor < ^ > / < ^ > package < ^ >.
The library we want to install uses the namespace cocur / slugify.
You need a package's namespace in order to require it in your project.
Now that you know exactly which package you want to install, you can run composer require to include it as a dependency and also generate the composer.json file for your project.
One thing that is important to notice when requiring packages is that Composer tracks both application-level dependencies as well as system-level dependencies.
System-level dependencies are important to indicate which PHP modules a package relies on.
In the case of the cocur / slugify package, it requires a PHP module that we haven't installed yet.
When a required package relies on a system library that is currently not installed on your server, you will get an error telling which requirement is missing:
To solve the system dependency problem, we can search for the missing package using apt search:
After locating the correct package name, you can use apt once again to install the system dependency:
Once the installation is finished, you can run the composer require command again:
As you can see from the output, Composer automatically decided which version of the package to use.
If you check your project's directory now, it will contain two new files: composer.json and composer.lock, and a vendor directory:
The composer.lock file is used to store information about which versions of each package are installed, and ensure the same versions are used if someone else clones your project and installs its dependencies.
The vendor directory is where the project dependencies are located.
The vendor folder shouldn't be committed into version control - you only need to include the composer.json and composer.lock files.
< $> note When installing a project that already contains a composer.json file, run composer install in order to download the project's dependencies.
Let's take a quick look at version constraints.
If you check the contents of your composer.json file, you'll see something like this:
You might notice the special character ^ before the version number in composer.json.
Composer supports several different constraints and formats for defining the required package version, in order to provide flexibility while also keeping your project stable.
The caret (^) operator used by the auto-generated composer.json file is the recommended operator for maximum interoperability, following semantic versioning.
In this case, it defines 4.0 as the minimum compatible version, and allows updates to any future version below 5.0.
Generally speaking, you won't need to tamper with version constraints in your composer.json file.
However, some situations might require that you manually edit the constraints-for instance, when a major new version of your required library is released and you want to upgrade, or when the library you want to use doesn't follow semantic versioning.
Here are some examples to give you a better understanding of how Composer version constraints work:
Constraint
Meaning
Example Versions Allowed
^ 1.0
> = 1.0 < 2.0
1.0, 1.2.3, 1.9.9
^ 1.1.0
> = 1.1.0 < 2.0
1.1.0, 1.5.6, 1.9.9
~ 1.0
> = 1.0 < 2.0.0
1.0, 1.4.1, 1.9.9
~ 1.0.0
> = 1.0.0 < 1.1
1.0.0, 1.0.4, 1.0.9
1.2.1
1. *
1.0.0, 1.4.5, 1.9.9
1.2.
*
> = 1.2 < 1.3
1.2.0, 1.2.3, 1.2.9
For a more in-depth view of Composer version constraints, see the official documentation.
Next, let's look at how to load dependencies automatically with Composer.
Step 4 - Including the Autoload Script
Since PHP itself doesn't automatically load classes, Composer provides an autoload script that you can include in your project to get autoloading working for your project.
This file is automatically generated by Composer when you add your first dependency.
The only thing you need to do is include the vendor / autoload.php file in your PHP scripts before any class instantiation.
Let's try it out in our demo application.
Open a new file called test.php in your text editor:
Add the following code which brings in the vendor / autoload.php file, loads the cocur / slugify dependency, and uses it to create a slug:
Now run the script:
This produces the output hello-world-this-is-a-long-sentence-and-i-need-to-make-a-slug-from-it.
Dependencies need updates when new versions come out, so let's look at how to handle that.
Step 5 - Updating Project Dependencies
Whenever you want to update your project dependencies to more recent versions, run the update command:
This will check for newer versions of the libraries you required in your project.
If a newer version is found and it's compatible with the version constraint defined in the composer.json file, Composer will replace the previous version installed.
The composer.lock file will be updated to reflect these changes.
You can also update one or more specific libraries by specifying them like this:
Be sure to check in your composer.json and composer.lock files within your version control system after you update your dependencies so that others can install these newer versions too.
Composer is a powerful tool that can greatly facilitate the work of managing dependencies in PHP projects.
It provides a reliable way of discovering, installing, and updating PHP packages that a project depends on.
In this guide, we saw how to install Composer, how to include new dependencies in a project, and how to update these dependencies once new versions are available.
How To Install Django and Set Up a Development Environment on Ubuntu 20.04
5478
Django is a free and open-source web framework written in Python with its core principles being scalability, re-usability and rapid development.
It is also known for its framework-level consistency and loose coupling, allowing for individual components to be independent of one another.
In this tutorial, we will set up a Django environment for development purposes on an Ubuntu 20.04 server.
For a live website, you will have additional considerations, including connecting to a database, setting up a domain name, and adding layers of security.
We have a variety of tutorials on Django that can help support you as you build under our Django tag.
A non-root user account with sudo privileges, which you can achieve by following and completing the initial server setup for Ubuntu 20.04 tutorial.
Python 3 set up with a virtual programming environment.
You can get this set up via our Python 3 installation guide.
Step 1 - Installing Django
There are several ways to install Django, the Python package manager pip within a virtual environment.
While in the server "s home directory, we" ll create the directory that will contain our Django application.
Run the following command to create a directory called < ^ > django-apps < ^ >, or another name of your choice.
Then navigate to the directory.
While inside the < ^ > django-apps < ^ > directory, create your virtual environment.
We "ll call it the generic < ^ > env < ^ >, but you should use a name that is meaningful for you and your project.
Now, activate the virtual environment with the following command:
You "ll know it" s activated once the prefix is changed to < ^ > (env) < ^ >, which will look similar to the following, depending on what directory you are in:
Within the environment, install the Django package using pip.
Installing Django allows us to create and run Django applications.
Once installed, verify your Django installation by running a version check:
This, or something similar, will be the resulting output:
With Django installed on your server, we can move on to creating a test project to make sure everything is working correctly.
We "ll be creating a skeleton web application.
If you followed our initial server setup tutorial or have a firewall running on your server, we "ll need to open the port we" ll be using in our server "s firewall.
For the UFW firewall, you can open the port with the following command:
If you "re using DigitalOcean Firewalls, you can select HTTP from the inbound rules.
You can read more about DigitalOcean Firewalls and creating rules for them by modifying the inbound rules.
Step 3 - Starting the Project
We now can generate an application using django-admin, a command line utility for administration tasks in Python.
Then we can use the startproject command to create the project directory structure for our test website.
While in the django-apps directory, run the following command:
< $> note Note: Running the django-admin startproject < ^ > < projectname > < ^ > command will name both project directory and project package the < ^ > < projectname > < ^ > and create the project in the directory in which the command was run.
If the optional < ^ > < destination > < ^ > parameter is provided, Django will use the provided destination directory as the project directory, and create manage.py and the project package within it. < $>
Now we can look to see what project files were just created.
Navigate to the testsite directory then list the contents of that directory to see what files were created:
You will notice output that shows this directory contains a file named manage.py and a folder named testsite.
The manage.py file is similar to django-admin and puts the project "s package on sys.path.
This also sets the DJANGO _ SETTINGS _ MODULE environment variable to point to your project "s settings.py file.
You can view the manage.py script in your terminal by running the less command like so:
When you "re finished reading the script, press q, to quit viewing the file.
Now navigate to the testsite directory to view the other files that were created:
Then run the following command to list the contents of the directory:
You will see four files:
Let "s go over what each of these files are:
_ _ init _ _ .py acts as the entry point for your Python project.
asgi.py contains the configuration for the optional deployment to the Asynchronous Server Gateway Interface or ASGI, which provides a standard for apps that are either synchronous and asynchronous, and is considered to be a successor of WSGI (see below).
settings.py describes the configuration of your Django installation and lets Django know which settings are available.
urls.py contains a urlpatterns list, that routes and maps URLs to their views.
wsgi.py contains the configuration for the Web Server Gateway Interface or WSGI, which provides a standard for synchronous Python apps.
< $> note Note: Although default files are generated, you still have the ability to tweak the asgi.py or wsgi.py files at any time to fit your deployment needs.
Step 4 - Configuring Django
Now we can start the server and view the website on a designated host and port by running the runserver command.
We "ll need to add your server ip address to the list of ALLOWED _ HOSTS in the settings.py file located in ~ / test _ django _ app / testsite / testsite /.
As stated in the Django docs, the ALLOWED _ HOSTS variable contains "a list of strings representing the host / domain names that this Django site can serve.
This is a security measure to prevent HTTP Host header attacks, which are possible even under many seemingly-safe web server configurations. "
You can use your favorite text editor to add your IP address.
For example, if you're using nano, run the following command:
Once you run the command, you "ll want to navigate to the Allowed Hosts Section of the document and add your server" s IP address inside the square brackets within single or double quotes.
You can save the change and exit nano by holding down the CTRL + x keys and then pressing the y key.
Next, we'll go on to access our web app via a browser.
Step 5 - Accessing the Django Web App
With our configuration completed, be sure to navigate back to the directory where manage.py is located:
Now, run the following command replacing the < ^ > your-server-ip < ^ > text with the IP of your server:
Finally, you can navigate to the below link to see what your skeleton website looks like, again replacing the highlighted text with your server "s actual IP:
Once the page loads, you "ll see the following:
Django Default Page
This confirms that Django was properly installed and our test project is working correctly.
To access the admin interface, add / admin / to the end of your URL:
This will take you to a login screen:
If you enter the admin username and password that you just created, you will have access to the main admin section of the site:
Django admin page
For more information about working with the Django admin interface, please see "How To Enable and Connect the Django Admin Interface."
When you are done with testing your app, you can press CTRL + C to stop the runserver command.
This will return you to your programming environment.
When you are ready to leave your Python environment, you can run the deactivate command:
Deactivating your programming environment will put you back to the terminal command prompt.
In this tutorial you have successfully installed Django and set up a development environment to begin working on your Django app.
You now have the foundation needed to get started building Django web applications.
How To Install Nginx on Ubuntu 20.04 Quickstart
5466
In this guide, we'll explain how to install Nginx on your Ubuntu 20.04 server.
For a more detailed version of this tutorial, please refer to How To Install Nginx on Ubuntu 20.04.
Because Nginx is available in Ubuntu's default repositories, you can install it using the apt packaging system.
Install Nginx:
If you followed the prerequisite server setup tutorial, then you have the UFW firewall enabled.
Check the available ufw application profiles with the following command:
Let's enable the most restrictive profile that will still allow the traffic you've configured, permitting traffic on port 80:
Access the default Nginx landing page to confirm that the software is running properly through your IP address:
Step 4 - Setting Up Server Blocks (Recommended)
Create the directory for < ^ > your _ domain < ^ >, using the -p flag to create any necessary parent directories:
The permissions of your web roots should be correct if you haven't modified your umask value, but you can make sure by typing:
Make a new server block at / etc / nginx / sites-available / < ^ > your _ domain < ^ >:
Enable the file by creating a link from it to the sites-enabled directory:
Two server blocks are now enabled and configured to respond to requests based on their listen and server _ name directives:
Find the server _ names _ hash _ bucket _ size directive and remove the # symbol to uncomment the line:
Test for syntax errors:
Restart Nginx to enable your changes:
If you'd like to build out a more complete application stack, check out this article on how to configure a LEMP stack on Ubuntu 20.04.
How To Install the Anaconda Python Distribution on Ubuntu 20.04
5470
Anaconda is an open-source package manager, environment manager, and distribution of the Python and R programming languages.
It is commonly used for data science, machine learning, large-scale data processing, scientific computing, and predictive analytics.
Offering a collection of over 1,000 data science packages, Anaconda is available in both free and paid enterprise versions.
The Anaconda distribution ships with the conda command-line utility.
You can learn more about Anaconda and conda by reading the official Anaconda Documentation.
This tutorial will guide you through installing the Python 3 version of Anaconda on an Ubuntu 20.04 server.
Before you begin with this guide, you should have a non-root user with sudo privileges set up on your server.
You can achieve this prerequisite by completing our Ubuntu 20.04 initial server setup guide.
Installing Anaconda
The best way to install Anaconda is to download the latest Anaconda installer bash script, verify it, and then run it.
Find the latest version of Anaconda for Python 3 at the Anaconda Downloads page.
At the time of writing, the latest version is 2020.02, but you should use a later stable version if it is available.
Next, change to the / tmp directory on your server.
This is a good directory to download ephemeral items, like the Anaconda bash script, which we won't need after running it.
Use curl to download the link that you copied from the Anaconda website.
We "ll output this to a file called anaconda.sh for quicker use.
We can now verify the data integrity of the installer with cryptographic hash verification through the SHA-256 checksum.
We "ll use the sha256sum command along with the filename of the script:
You "ll receive output that looks similar to this:
You should check the output against the hashes available at the Anaconda with Python 3 on 64-bit Linux page for your appropriate Anaconda version.
As long as your output matches the hash displayed in the sha2561 row, you "re good to go.
Now we can run the script:
Press ENTER to continue and then press ENTER to read through the license.
Once you "re done reading the license, you" ll be prompted to approve the license terms:
As long as you agree, type yes.
At this point, you "ll be prompted to choose the location of the installation.
You can press ENTER to accept the default location, or specify a different location to modify it.
The installation process will continue.
Note that it may take some time.
Type yes so that you can initialize Anaconda3.
You "ll receive some output that states changes made in various directories.
One of the lines you receive will thank you for installing Anaconda.
You can now activate the installation by sourcing the ~ / .bashrc file:
Once you have done that, you "ll be placed into the default base programming environment of Anaconda, and your command prompt will change to be the following:
Although Anaconda ships with this default base programming environment, you should create separate environments for your programs and to keep them isolated from each other.
You can further verify your install by making use of the conda command, for example with list:
You "ll receive output of all the packages you have available through the Anaconda installation:
Now that Anaconda is installed, we can go on to setting up Anaconda environments.
Setting Up Anaconda Environments
Anaconda virtual environments allow you to keep projects organized by Python versions and packages needed.
For each Anaconda environment you set up, you can specify which version of Python to use and can keep all of your related programming files together within that directory.
First, we can check to see which versions of Python are available for us to use:
You "ll receive output with the different versions of Python that you can target, including both Python 3 and Python 2 versions.
Since we are using the Anaconda with Python 3 in this tutorial, you will have access only to the Python 3 versions of packages.
Let "s create an environment using the most recent version of Python 3. We can achieve this by assigning version 3 to the python argument.
We "ll call the environment < ^ > my _ env < ^ >, but you" ll likely want to use a more descriptive name for your environment especially if you are using environments to access more than one version of Python.
We "ll receive output with information about what is downloaded and which packages will be installed, and then be prompted to proceed with y or n. As long as you agree, type y.
The conda utility will now fetch the packages for the environment and let you know when it "s complete.
You can activate your new environment by typing the following:
With your environment activated, your command prompt prefix will reflect that you are no longer in the base environment, but in the new one that you just created.
Within the environment, you can verify that you "re using the version of Python that you had intended to use:
When you "re ready to deactivate your Anaconda environment, you can do so by typing:
Note that you can replace the word source with. to achieve the same results.
To target a more specific version of Python, you can pass a specific version to the python argument, like 3.5, for example:
You can inspect all of the environments you have set up with this command:
The asterisk indicates the current active environment.
Each environment you create with conda create will come with several default packages:
_ libgcc _ mutex
ca-certificates
certifi
libedit
libffi
libgcc-ng
libstdcxx-ng
ncurses
openssl
pip
python
readline
setuptools
sqlite
tk
wheel
xz
zlib
You can add additional packages, such as numpy for example, with the following command:
If you know you would like a numpy environment upon creation, you can target it in your conda create command:
If you are no longer working on a specific project and have no further need for the associated environment, you can remove it. To do so, type the following:
Now, when you type the conda info --envs command, the environment that you removed will no longer be listed.
Updating Anaconda
You should regularly ensure that Anaconda is up-to-date so that you are working with all the latest package releases.
To do this, you should first update the conda utility:
When prompted to do so, type y to proceed with the update.
Once the update of conda is complete, you can update the Anaconda distribution:
Again, when prompted to do so, type y to proceed.
This will ensure that you are using the latest releases of conda and Anaconda.
Uninstalling Anaconda
If you are no longer using Anaconda and find that you need to uninstall it, you should start with the anaconda-clean module, which will remove configuration files for when you uninstall Anaconda.
Type y when prompted to do so.
Once it is installed, you can run the following command.
You will be prompted to answer y before deleting each one.
If you would prefer not to be prompted, add --yes to the end of your command:
This will also create a backup folder called .anaconda _ backup in your home directory:
You can now remove your entire Anaconda directory by entering the following command:
Finally, you can remove the PATH line from your .bashrc file that Anaconda added.
To do so, first open a text editor such as nano:
Then scroll down to the end of the file (if this is a recent install) or type CTRL + W to search for Anaconda.
Delete or comment out this Anaconda block:
When you "re done editing the file, type CTRL + X to exit and y to save changes.
Anaconda is now removed from your server.
If you did not deactivate the base programming environment, you can exit and re-enter the server to remove it.
This tutorial walked you through the installation of Anaconda, working with the conda command-line utility, setting up environments, updating Anaconda, and deleting Anaconda if you no longer need it.
You can use Anaconda to help you manage workloads for data science, scientific computing, analytics, and large-scale data processing.
From here, you can check out our tutorials on data analysis and machine learning to learn more about various tools available to use and projects that you can do.
We also have a free machine learning ebook available for download, Python Machine Learning Projects.
How To Set Up a Firewall with UFW on Ubuntu 20.04
5465
UFW, or Uncomplicated Firewall, is a simplified firewall management interface that hides the complexity of lower-level packet filtering technologies such as iptables and nftables.
If you're looking to get started securing your network, and you're not sure which tool to use, UFW may be the right choice for you.
This tutorial will show you how to set up a firewall with UFW on Ubuntu 20.04.
One Ubuntu 20.04 server with a sudo non-root user, which you can set up by following our Initial Server Setup with Ubuntu 20.04 tutorial.
UFW is installed by default on Ubuntu.
If it has been uninstalled for some reason, you can install it with sudo apt install ufw.
Step 1 - Using IPv6 with UFW (Optional)
This tutorial is written with IPv4 in mind, but will work for IPv6 as well as long as you enable it. If your Ubuntu server has IPv6 enabled, ensure that UFW is configured to support IPv6 so that it will manage firewall rules for IPv6 in addition to IPv4.
To do this, open the UFW configuration with nano or your favorite editor.
Then make sure the value of IPV6 is yes.
It should look like this:
Now, when UFW is enabled, it will be configured to write both IPv4 and IPv6 firewall rules.
However, before enabling UFW, we will want to ensure that your firewall is configured to allow you to connect via SSH.
Let's start with setting the default policies.
Step 2 - Setting Up Default Policies
If you're just getting started with your firewall, the first rules to define are your default policies.
These rules control how to handle traffic that does not explicitly match any other rules.
By default, UFW is set to deny all incoming connections and allow all outgoing connections.
This means anyone trying to reach your server would not be able to connect, while any application within the server would be able to reach the outside world.
Let's set your UFW rules back to the defaults so we can be sure that you'll be able to follow along with this tutorial.
To set the defaults used by UFW, use these commands:
These commands set the defaults to deny incoming and allow outgoing connections.
These firewall defaults alone might suffice for a personal computer, but servers typically need to respond to incoming requests from outside users.
We'll look into that next.
Step 3 - Allowing SSH Connections
If we enabled our UFW firewall now, it would deny all incoming connections.
This means that we will need to create rules that explicitly allow legitimate incoming connections - SSH or HTTP connections, for example - if we want our server to respond to those types of requests.
If you're using a cloud server, you will probably want to allow incoming SSH connections so you can connect to and manage your server.
To configure your server to allow incoming SSH connections, you can use this command:
This will create firewall rules that will allow all connections on port 22, which is the port that the SSH daemon listens on by default.
UFW knows what port allow ssh means because it's listed as a service in the / etc / services file.
However, we can actually write the equivalent rule by specifying the port instead of the service name.
For example, this command works the same as the one above:
If you configured your SSH daemon to use a different port, you will have to specify the appropriate port. For example, if your SSH server is listening on port 2222, you can use this command to allow connections on that port:
Now that your firewall is configured to allow incoming SSH connections, we can enable it.
Step 4 - Enabling UFW
To enable UFW, use this command:
You will receive a warning that says the command may disrupt existing SSH connections.
We already set up a firewall rule that allows SSH connections, so it should be fine to continue.
Respond to the prompt with y and hit ENTER.
The firewall is now active.
Run the sudo ufw status verbose command to see the rules that are set. The rest of this tutorial covers how to use UFW in more detail, like allowing or denying different kinds of connections.
Step 5 - Allowing Other Connections
At this point, you should allow all of the other connections that your server needs to respond to.
The connections that you should allow depends on your specific needs.
Luckily, you already know how to write rules that allow connections based on a service name or port; we already did this for SSH on port 22. You can also do this for:
HTTP on port 80, which is what unencrypted web servers use, using sudo ufw allow http or sudo ufw allow 80
HTTPS on port 443, which is what encrypted web servers use, using sudo ufw allow https or sudo ufw allow 443
There are several others ways to allow other connections, aside from specifying a port or known service.
Specific Port Ranges
You can specify port ranges with UFW.
Some applications use multiple ports, instead of a single port.
For example, to allow X11 connections, which use ports 6000-6007, use these commands:
When specifying port ranges with UFW, you must specify the protocol (tcp or udp) that the rules should apply to.
We haven't mentioned this before because not specifying the protocol automatically allows both protocols, which is OK in most cases.
Specific IP Addresses
When working with UFW, you can also specify IP addresses.
For example, if you want to allow connections from a specific IP address, such as a work or home IP address of 203.0.113.4, you need to specify from, then the IP address:
You can also specify a specific port that the IP address is allowed to connect to by adding to any port followed by the port number.
For example, If you want to allow 203.0.113.4 to connect to port 22 (SSH), use this command:
Subnets
If you want to allow a subnet of IP addresses, you can do so using CIDR notation to specify a netmask.
For example, if you want to allow all of the IP addresses ranging from 203.0.113.1 to 203.0.113.254 you could use this command:
Likewise, you may also specify the destination port that the subnet 203.0.113.0 / 24 is allowed to connect to.
Again, we'll use port 22 (SSH) as an example:
Connections to a Specific Network Interface
If you want to create a firewall rule that only applies to a specific network interface, you can do so by specifying "allow in on" followed by the name of the network interface.
You may want to look up your network interfaces before continuing.
The highlighted output indicates the network interface names.
They are typically named something like eth0 or enp3s2.
So, if your server has a public network interface called eth0, you could allow HTTP traffic (port 80) to it with this command:
Doing so would allow your server to receive HTTP requests from the public internet.
Or, if you want your MySQL database server (port 3306) to listen for connections on the private network interface eth1, for example, you could use this command:
This would allow other servers on your private network to connect to your MySQL database.
Step 6 - Denying Connections
If you haven't changed the default policy for incoming connections, UFW is configured to deny all incoming connections.
Generally, this simplifies the process of creating a secure firewall policy by requiring you to create rules that explicitly allow specific ports and IP addresses through.
However, sometimes you will want to deny specific connections based on the source IP address or subnet, perhaps because you know that your server is being attacked from there.
Also, if you want to change your default incoming policy to allow (which is not recommended), you would need to create deny rules for any services or IP addresses that you don't want to allow connections for.
To write deny rules, you can use the commands described above, replacing allow with deny.
For example, to deny HTTP connections, you could use this command:
Or if you want to deny all connections from 203.0.113.4 you could use this command:
Now let's take a look at how to delete rules.
Step 7 - Deleting Rules
Knowing how to delete firewall rules is just as important as knowing how to create them.
There are two different ways to specify which rules to delete: by rule number or by the actual rule (similar to how the rules were specified when they were created).
We'll start with the delete by rule number method because it is easier.
By Rule Number
If you're using the rule number to delete firewall rules, the first thing you'll want to do is get a list of your firewall rules.
The UFW status command has an option to display numbers next to each rule, as demonstrated here:
If we decide that we want to delete rule 2, the one that allows port 80 (HTTP) connections, we can specify it in a UFW delete command like this:
This would show a confirmation prompt then delete rule 2, which allows HTTP connections.
Note that if you have IPv6 enabled, you would want to delete the corresponding IPv6 rule as well.
By Actual Rule
The alternative to rule numbers is to specify the actual rule to delete.
For example, if you want to remove the allow http rule, you could write it like this:
You could also specify the rule by allow 80, instead of by service name:
This method will delete both IPv4 and IPv6 rules, if they exist.
Step 8 - Checking UFW Status and Rules
At any time, you can check the status of UFW with this command:
If UFW is disabled, which it is by default, you'll see something like this:
If UFW is active, which it should be if you followed Step 3, the output will say that it's active and it will list any rules that are set. For example, if the firewall is set to allow SSH (port 22) connections from anywhere, the output might look something like this:
Use the status command if you want to check how UFW has configured the firewall.
Step 9 - Disabling or Resetting UFW (optional)
If you decide you don't want to use UFW, you can disable it with this command:
Any rules that you created with UFW will no longer be active.
You can always run sudo ufw enable if you need to activate it later.
If you already have UFW rules configured but you decide that you want to start over, you can use the reset command:
This will disable UFW and delete any rules that were previously defined.
Keep in mind that the default policies won't change to their original settings, if you modified them at any point.
This should give you a fresh start with UFW.
Your firewall is now configured to allow (at least) SSH connections.
Be sure to allow any other incoming connections that your server needs, while limiting any unnecessary connections, so your server will be functional and secure.
To learn about more common UFW configurations, check out the UFW Essentials: Common Firewall Rules and Commands tutorial.
Step 1 - Installing the Necessary Software Packages and Configure the Firewall
Now that you have the packages installed we need to enable the firewall to allow traffic to come in to our Minecraft server.
In the initial server setup that you performed you only allowed traffic from SSH.
Now you need to allow for traffic to come in via port 25565, which is the default port that Minecraft uses to allow connections.
Add the necessary firewall rule by running the following command:
Now that you have Java installed and your firewall properly configured, you will download the Minecraft server from the Minecraft website.
How To Create a Minecraft Server on Ubuntu 20.04
5502
A server with a fresh installation of Ubuntu 20.04, a non-root user with sudo privileges, and SSH enabled.
You now have a Minecraft server running on Ubuntu 20.04 for you and all of your friends to play on!
How to Install and Configure VNC on Ubuntu 20.04 Quickstart
5510
In this quickstart guide, you'll set up a VNC server with TightVNC on an Ubuntu 20.04 server and connect to it securely through an SSH tunnel.
Then, you'll use a VNC client program on your local machine to interact with your server through a graphical desktop environment.
A local computer with a VNC client installed.
The VNC client you use must support connections over SSH tunnels:
After connecting to your server with SSH, update your list of packages:
Then install Xfce along with the xfce4-goodies package, which contains a few enhancements for the desktop environment:
Next, run the vncpasswd command to set a VNC access password and create the initial configuration files:
The password must be between six and eight characters long; passwords more than 8 characters will be truncated automatically.
Once you verify the password you'll have the option to create a view-only password, but this isn't required.
If you ever want to change your password or add a view-only password, re-run the vncpasswd command.
The commands that the VNC server runs at startup are located in a configuration file called xstartup in the .vnc folder under your home directory.
In this step, we'll create a custom xstartup script which will tell the VNC server to connect to the Xfce desktop.
Create a new xstartup file and open it in a text editor, such as nano:
Add the following lines to the new file:
Following the shebang, the first command in the file, xrdb $HOME /.
Xresources file.
The second command tells the server to launch Xfce.
Then make the file executable:
And start the VNC server with the vncserver command:
This command includes the -localhost option, which binds the VNC server to your server's loopback interface.
This will cause VNC to only allow connections that originate from the server on which it's installed.
Here, you can see that the command launches a default server instance on port 5901.
Step 3 - Connecting to the VNC Desktop Securely
To securely connect to your server, you'll establish an SSH tunnel and then tell your VNC client to connect using that tunnel rather than making a direct connection.
You can do this via the terminal on Linux or macOS with the following ssh command:
The local port can be any port that isn't already blocked by another program or process, though we use < ^ > 59000 < ^ > in this example.
Also, make sure to change < ^ > sammy < ^ > to your Ubuntu user's username and < ^ > your _ server _ ip < ^ > to reflect your server's IP address.
If you are using PuTTY to connect to your server, you can create an SSH tunnel by right-clicking on the top bar of the terminal window, and then clicking the Change Settings... option:
Right-click on top bar to reveal Change Settings option
Find the Connection branch in the tree menu on the left-hand side of the PuTTY Reconfiguration window.
Expand the SSH branch and click on Tunnels.
On the Options controlling SSH port forwarding screen, enter 59000 as the Source Port and localhost: 5901 as the Destination, like this:
Example PuTTY SSH tunnel configuration
Then click the Add button, and then the Apply button to implement the tunnel.
Once the tunnel is running, use a VNC client to connect to localhost: 59000.
VNC connection to Ubuntu 20.04 server with the Xfce desktop environment
File Manager via VNC connection to Ubuntu 20.04
Press CTRL + C in your local terminal to stop the SSH tunnel and return to your prompt.
By setting up the VNC server to run as a systemd service you can use systemd's management commands start, stop, and restart the server, as well as enable it to start running whenever the server boots up.
First, create a new systemd unit file called / etc / systemd / system / vncserver @ .service:
The @ symbol at the end of the name will let us pass in an argument you can use in the service configuration.
You'll use this to specify the VNC display port you want to use when you manage the service.
Add the following lines to the file, making sure to change the value of User, Group, WorkingDirectory, and the username in the value of PIDFILE to match your username:
Next, make the system aware of the new unit file:
Enable the unit file:
The 1 following the @ sign signifies which display number the service should appear over, in this case the default: 1 as was discussed in Step 2.
Stop the current instance of the VNC server if it's still running:
Then start it as you would start any other systemd service:
See our tutorial on How To Use Systemctl to Manage Systemd Services and Units for more information on systemctl.
To reconnect, start your SSH tunnel again:
Then make a new connection using your VNC client software to localhost: < ^ > 59000 < ^ > to connect to your server.
You now have a secured VNC server up and running on your Ubuntu 20.04 server.
Now you'll be able to manage your files, software, and settings with a user-friendly graphical interface, and you'll be able to run graphical software like web browsers remotely.
How to Install and Configure VNC on Ubuntu 20.04
5477
To set this up, follow our initial server setup guide for Ubuntu 20.04.
By default, an Ubuntu 20.04 server does not come with a graphical desktop environment or a VNC server installed, so you'll begin by installing those.
You have many options when it comes to which VNC server and desktop environment you choose.
In this tutorial, you will install packages for the latest Xfce desktop environment and the TightVNC package available from the official Ubuntu repository.
Both Xfce and TightVNC are known for being lightweight and fast, which will help ensure that the VNC connection will be smooth and stable even on slower internet connections.
Now install Xfce along with the xfce4-goodies package, which contains a few enhancements for the desktop environment:
During installation, you may be prompted to choose a default display manager for Xfce.
A display manager is a program that allows you to select and log in to a desktop environment through a graphical interface.
You'll only be using Xfce when you connect with a VNC client, and in these Xfce sessions you'll already be logged in as your non-root Ubuntu user.
So for the purposes of this tutorial, your choice of display manager isn't pertinent.
Select either one and press ENTER.
Next, run the vncserver command to set a VNC access password, create the initial configuration files, and start a VNC server instance:
Additionally, it launches a default server instance on port 5901.
VNC can launch multiple instances on other display ports, with: 2 referring to port 5902,: 3 referring to 5903, and so on:
Note that if you ever want to change your password or add a view-only password, you can do so with the vncpasswd command:
At this point, the VNC server is installed and running.
Now let's configure it to launch Xfce and give us access to the server through a graphical interface.
Specifically, VNC needs to know which graphical desktop environment it should connect to.
The startup script was created when you ran the vncserver command in the previous step, but you'll create your own to launch the Xfce desktop.
Because you are going to be changing how the VNC server is configured, first stop the VNC server instance that is running on port 5901 with the following command:
Then add the following lines to the file:
The first line is a shebang.
In executable plain-text files on * nix platforms, a shebang tells the system what interpreter to pass that file to for execution.
In this case, you're passing the file to the Bash interpreter.
This will allow each successive line to be executed as commands, in order.
Whenever you start or restart the VNC server, these commands will execute automatically.
To ensure that the VNC server will be able to use this new startup file properly, you'll need to make it executable:
Then restart the VNC server:
Notice that this time the command includes the -localhost option, which binds the VNC server to your server's loopback interface.
In the next step, you'll establish an SSH tunnel between your local machine and your server, essentially tricking VNC into thinking that the connection from your local machine originated on your server.
This strategy will add an extra layer of security around VNC, as the only users who will be able to access it are those that already have SSH access to your server.
With the configuration in place, you're ready to connect to the VNC server from your local machine.
Here's what this ssh command's options mean:
-L < ^ > 59000 < ^ >: localhost: < ^ > 5901 < ^ >: The -L switch specifies that the given port on the local computer (59000) is to be forwarded to the given host and port on the destination server (localhost: 5901, meaning port 5901 on the destination server, defined as < ^ > your _ server _ ip < ^ >).
Note that the local port you specify is somewhat arbitrary; as long as the port isn't already bound to another service, you can use it as the forwarding port for your tunnel.
-C: This flag enables compression which can help minimize resource consumption and speed things up.
-N: This option tells ssh that you don't want to execute any remote commands.
This setting is useful when you just want to forward ports.
-l < ^ > sammy < ^ > < ^ > your _ server _ ip < ^ >: The -l switch let's you specify the user you want to log in as once you connect to the server.
Make sure to replace < ^ > sammy < ^ > and < ^ > your _ server _ ip < ^ > with the name of your non-root user and your server's IP address.
< $> note Note: This command establishes an SSH tunnel that forwards information from port 5901 on your VNC server to port 59000 on your local machine via port 22 on each machine, the default port for SSH.
Assuming you followed the prerequisite Initial Server Setup guide for Ubuntu 20.04, you will have added a UFW rule to allow connections to your server over OpenSSH.
This is more secure than simply opening up your server's firewall to allow connections to port 5901, as that would allow anyone to access your server over VNC.
By connecting over an SSH tunnel, you're limiting VNC access to machines that already have SSH access to the server.
Now you can configure your VNC server to run as a systemd service.
By setting up the VNC server to run as a systemd service you can start, stop, and restart it as needed, like any other service.
You can also use systemd's management commands to ensure that VNC starts when your server boots up.
First, create a new unit file called / etc / systemd / system / vncserver @ .service:
Also, note that the ExecStart command again includes the -localhost option.
Your VNC server is now ready to use whenever your server boots up, and you can manage it with systemctl commands like any other systemd service.
However, there won't be any difference on the client side.
Then make a new connection using your VNC client software to localhost: 59000 to connect to your server.
Finally, let's create an administrative user so that you can use the Djano admin interface.
Let's do this with the createsuperuser command:
You will be prompted for a username, an email address, and a password for your user.
How To Install Linux, Nginx, MySQL, PHP (LEMP stack) on Ubuntu 20.04 Quickstart
5608
In this quickstart guide, we'll install a LEMP stack on an Ubuntu 20.04 server.
For a more detailed version of this tutorial, with more explanations of each step, please refer to How To Install Linux, Nginx, MySQL, PHP (LEMP stack) on Ubuntu 20.0
Update your package manager cache and then install Nginx with:
Please refer to step 6 of our detailed LEMP on Ubuntu 20.04 guide to learn how to do that.
Step 4 - Configure Nginx for PHP
Step 5 - Test PHP with Nginx
We'll now create a PHP test script to confirm that Nginx is able to handle and process requests for PHP files.
How To Install MariaDB on Ubuntu 20.04
5512
This tutorial will explain how to install MariaDB on an Ubuntu 20.04 server and verify that it is running and has a safe initial configuration.
As of this writing, Ubuntu 20.04's default APT repositories include MariaDB version < ^ > 10.3 < ^ >.
Later, we will cover how to set up an additional administrative account for password access if socket authentication is not appropriate for your use case.
With that, you've finished MariaDB's initial security configuration.
The next step is an optional one, though you should follow it if you prefer to authenticate to your MariaDB server with a password.
To this end, we will create a new account called admin with the same capabilities as the root account, but configured for password authentication.
Open up the MariaDB prompt from your terminal:
For example, this command says to connect to MariaDB as root using the Unix socket and return the version:
You also had the option to create a new administrative user that uses password authentication before testing the MariaDB server's functionality.
How To Install Webmin on Ubuntu 20.04
5506
Webmin is a modern web control panel that allows you to administer your Linux server through a browser-based interface.
With Webmin, you can manage user accounts, configure DNS settings, and change settings for common packages on the fly.
In this tutorial, you'll install and configure Webmin on your server and secure access to the interface with a valid certificate from Let's Encrypt.
You'll then use Webmin to add new user accounts, and update all packages on your server from the dashboard.
This server should have a non-root user with sudo privileges and a UFW firewall configured.
Apache installed by following our tutorial on How To Install the Apache Web Server on Ubuntu 20.04.
As you follow this prerequisite guide, be sure to configure a virtual host.
A Fully-Qualified Domain Name (FQDN), with a DNS A record pointing to the IP address of your server.
To configure this, follow these instructions on DNS hosting on DigitalOcean.
Step 1 - Installing Webmin
First, we need to add the Webmin repository so that we can install and update Webmin using our package manager.
We do this by adding the repository to the / etc / apt / sources.list file.
Open the file in your preferred editor.
Then add this line to the bottom of the file to add the new repository:
If you used nano, do so by pressing CTRL + X, Y, then ENTER.
Next, you'll add the Webmin PGP key so that your system will trust the new repository.
In order to do that, though, you must install the gnupg1 package, which is GNU's tool for secure communication and data storage.
Update your server's package index if you've not done so recently:
Following that, download the Webmin PGP key with wget and add it to your system's list of keys:
Next, update the list of packages again in order to include the now-trusted Webmin repository:
Then install Webmin:
Once the installation finishes, you'll be presented with the following output:
< $> note Note: If you installed and enabled ufw during the prerequisite step, you will need to run the following command in order to allow Webmin through the firewall:
For extra security, you may want to configure your firewall to only allow access to this port from certain IP ranges.
Let's secure access to Webmin by adding a valid certificate.
Step 2 - Adding a Valid Certificate with Let's Encrypt
Webmin is already configured to use HTTPS, but it uses a self-signed, untrusted certificate.
Let's replace it with a valid certificate from Let's Encrypt.
Navigate to https: / / < ^ > your _ domain < ^ >: 10000 in your web browser, replacing < ^ > your _ domain < ^ > with the domain name pointing to your server's IP address.
< $> note Note: When logging in for the first time, you will see an "Invalid SSL" warning.
This warning may say something different depending on your browser, but the reason for it is that the server has generated a self-signed certificate.
Allow the exception and proceed to your domain so you can replace the self-signed certificate with one from Let's Encrypt.
You'll be presented with a login screen.
Sign in with the non-root user you created while fulfilling the prerequisites for this tutorial.
Once you log in, the first screen you will see is the Webmin dashboard.
Before you can apply a valid certificate, you have to set the server's hostname.
Look for the System hostname field and click on the link to the right, as shown in the following figure:
Image showing where the link is on the Webmin dashboard
This will take you to the Hostname and DNS Client page.
Locate the Hostname field, and enter your Fully-Qualified Domain Name into the field.
Then click the Save button at the bottom of the page to apply the setting.
After you've set your hostname, click on the Webmin dropdown menu in the left-hand navigation bar, and then click on Webmin Configuration.
From the Webmin Configuration page, select SSL Encryption from the list of icons, and then click on the Let's Encrypt tab. You'll see a screen like the following figure:
Image showing the Let's Encrypt tab of the SSL Encryption section
On this page, you'll tell Webmin how to obtain and renew your certificate.
Let's Encrypt certificates expire after 3 months, but you can instruct Webmin to automatically attempt to renew the Let's Encrypt certificate every month.
Let's Encrypt looks for a verification file on the server, so we'll configure Webmin to place the verification file inside the folder / var / www / < ^ > your _ domain < ^ >, which is the folder that the Apache web server you configured in the prerequisites uses.
Follow these steps to set up your certificate:
Fill in Hostnames for certificate with your FQDN.
For Website root directory for validation file, select the Other Directory button and enter your website's document root.
Assuming you followed the prerequisite Apache tutorial this will be / var / www / < ^ > your _ domain < ^ >.
For Months between automatic renewal section, deselect the Only renew manually option by typing 1 into the input box, and select the radio button to the left of the input box.
Click the Request Certificate button.
After a few seconds, you will see a confirmation screen.
To use the new certificate, click the Return to Webmin configuration button on the confirmation screen.
From that page, scroll down and click the Restart Webmin button.
Wait around 30 seconds, and then reload the page and log in again.
Your browser should now indicate that the certificate is valid.
Step 3 - Using Webmin
You've now set up a secured working instance of Webmin.
Let's look at how to use it.
Webmin has many different modules that can control everything from the BIND DNS Server to adding users to the system.
Let's look at how to create a new user, and then explore how to update your system's packages using Webmin.
Managing Users and Groups
Let's explore how to manage the users and groups on your server.
First, click the System dropdown menu in the left-hand sidebar, and then click the link for Users and Groups.
From here, you can add and manage users and groups.
Let's create a new user called deploy which you can use to host web applications.
When creating a user, you can set options for password expiry, the user's shell, and whether or not they are allowed a home directory.
To add a user, click Create a new user, which is located at the top of the users table.
This displays the Create User screen, where you can supply the username, password, groups and other options.
Follow these instructions to create the user:
Fill in Username with deploy.
Select Automatic for User ID.
Fill in Real Name with a descriptive name like Deployment user.
For Home Directory, select Automatic.
For Shell, select / bin / bash from the dropdown list.
For Password, select Normal Password and type in a password of your choice.
Jump down to Primary Group and select New group with same name as user.
For Secondary Group, select sudo from the All groups list.
This should automatically be added to the In groups list, but if it isn't press the - > button to add it.
After making those selections, press Create.
This will create the deploy user in short order.
Next, let's look at how to install updates to our system.
Updating Packages
Webmin lets you update all of your packages through its user interface.
To update all of your packages, first, click the Dashboard button above the left-hand sidebar, and then locate the Package updates field.
If there are updates available, you'll see a link that states the number of available updates.
Click this link, and then press Update selected packages to start the update.
You may be asked to reboot the server, which you can also do through the Webmin interface.
You now have a secured working instance of Webmin and you've used the interface to create a user and update packages.
Webmin gives you access to many things you'd normally need to access through the console, and it organizes them in an intuitive way.
For example, if you have Apache installed, you would find the configuration tab for it under Servers, and then Apache.
Explore the interface, or read the Official Webmin wiki to learn more about managing your system with Webmin.
5501
Python 3 has a number of built-in data structures, including tuples, dictionaries, and lists.
Data structures provide us with a way to organize and store data. The collections module helps us populate and manipulate data structures efficiently.
In this tutorial, we "ll go through three classes in the collections module to help you work with tuples, dictionaries, and lists.
We "ll use namedtuples to create tuples with named fields, defaultdict to concisely group information in dictionaries, and deque to efficiently add elements to either side of a list-like object.
For this tutorial, we "ll be working primarily with an inventory of fish that we need to modify as fish are added to or removed from a fictional aquarium.
To get the most out of this tutorial, it is recommended to have some familiarity with the tuple, dictionary, and list data types, both with their syntax, and how to retrieve data from them.
You can review these tutorials for the necessary background information:
Understanding Tuples in Python 3
Understanding Dictionaries in Python 3
Understanding Lists in Python 3
Adding Named Fields to Tuples
Python tuples are an immutable, or unchangeable, ordered sequence of elements.
Tuples are frequently used to represent columnar data; for example, lines from a CSV file or rows from a SQL database.
An aquarium might keep track of its inventory of fish as a series of tuples.
An individual fish tuple:
This tuple is composed of three string elements.
While useful in some ways, this tuple does not clearly indicate what each of its fields represents.
In actuality, element 0 is a name, element 1 is a species, and element 2 is the holding tank.
Explanation of fish tuple fields:
name
species
tank
Sammy
shark
tank-a
This table makes it clear that each of the tuple's three elements has a clear meaning.
namedtuple from the collections module lets you add explicit names to each element of a tuple to make these meanings clear in your Python program.
Let's use namedtuple to generate a class that clearly names each element of the fish tuple:
from collections import namedtuple gives your Python program access to the namedtuple factory function.
The namedtuple () function call returns a class that is bound to the name Fish.
The namedtuple () function has two arguments: the desired name of our new class "Fish" and a list of named elements ["name", "species", "tank"].
We can use the Fish class to represent the fish tuple from earlier:
If we run this code, we'll see the following output:
sammy is instantiated using the Fish class. sammy is a tuple with three clearly named elements.
sammy's fields can be accessed by their name or with a traditional tuple index:
If we run these two print calls, we'll see the following output:
Accessing .species returns the same value as accessing the second element of sammy using [1].
Using namedtuple from the collections module makes your program more readable while maintaining the important properties of a tuple (that they're immutable and ordered).
In addition, the namedtuple factory function adds several extra methods to instances of Fish.
Use. _ asdict () to convert an instance to a dictionary:
If we run print, you'll see output like the following:
Calling .asdict () on sammy returns a dictionary mapping each of the three field names to their corresponding values.
Python versions older than 3.8 might output this line slightly differently.
You might, for example, see an OrderedDict instead of the plain dictionary shown here.
< $> note Note: In Python, methods with leading underscores are usually considered "private."
Additional methods provided by namedtuple (like _ asdict (),. _ make (),. _ replace (), etc.), however, are public.
Collecting Data in a Dictionary
It is often useful to collect data in Python dictionaries. defaultdict from the collections module can help us assemble information in dictionaries quickly and concisely.
defaultdict never raises a KeyError.
If a key isn "t present, defaultdict just inserts and returns a placeholder value instead:
If we run this code, we'll see output like the following:
defaultdict inserts and returns a placeholder value instead of throwing a KeyError.
In this case we specified the placeholder value as a list.
Regular dictionaries, in contrast, will throw a KeyError on missing keys:
The regular dictionary my _ regular _ dict raises a KeyError when we try to access a key that is not present.
defaultdict behaves differently than a regular dictionary.
Instead of raising a KeyError on a missing key, defaultdict calls the placeholder value with no arguments to create a new object.
In this case list () to create an empty list.
Continuing with our fictional aquarium example, let's say we have a list of fish tuples representing an aquarium's inventory:
Three fish exist in the aquarium - their name, species, and holding tank are noted in these three tuples.
Our goal is to organize our inventory by tank - we want to know the list of fish present in each tank.
In other words, we want a dictionary that maps "tank-a" to ["Jamie", "Mary"] and "tank-b" to ["Jamie"].
We can use defaultdict to group fish by tank:
Running this code, we'll see the following output:
fish _ names _ by _ tank is declared as a defaultdict that defaults to inserting list () instead of throwing a KeyError.
Since this guarantees that every key in fish _ names _ by _ tank will point to a list, we can freely call .append () to add names to each tank's list.
defaultdict helps you here because it reduces the chance of unexpected KeyErrors.
Reducing the unexpected KeyErrors means your program can be written more clearly and with fewer lines.
More concretely, the defaultdict idiom lets you avoid manually instantiating an empty list for every tank.
Without defaultdict, the for loop body might have looked more like this:
Using just a regular dictionary (instead of a defaultdict) means that the for loop body always has to check for the existence of the given tank in fish _ names _ by _ tank.
Only after we've verified that tank is already present in fish _ names _ by _ tank, or has just been initialized with a [], can we append the fish name.
defaultdict can help cut down on boilerplate code when filling up dictionaries because it never raises a KeyError.
Using deque to Efficiently Add Elements to Either Side of a Collection
Python lists are a mutable, or changeable, ordered sequence of elements.
Python can append to lists in constant time (the length of the list has no effect on the time it takes to append), but inserting at the beginning of a list can be slower - the time it takes increases as the list gets bigger.
In terms of Big O notation, appending to a list is a constant time O (1) operation.
Inserting at the beginning of a list, in contrast, is slower with O (n) performance.
< $> note Note: Software engineers often measure the performance of procedures using something called "Big O" notation.
When the size of an input has no effect on the time it takes to perform a procedure, it is said to run in constant time or O (1) (" Big O of 1 ").
As you learned above, Python can append to lists with constant time performance, otherwise known as O (1).
Sometimes, the size of an input directly affects the amount of time it takes to run a procedure.
Inserting at the beginning of a Python list, for example, runs slower the more elements there are in the list.
Big O notation uses the letter n to represent the size of the input.
This means that adding items to the beginning of a Python list runs in "linear time" or O (n) (" Big O of n ").
In general, O (1) procedures are faster than O (n) procedures.
We can insert at the beginning of a Python list:
If we run the following, we will see output like the following:
The .insert (index, object) method on list allows us to insert "Alice" at the beginning of favorite _ fish _ list.
Notably, though, inserting at the beginning of a list has O (n) performance.
As the length of favorite _ fish _ list grows, the time to insert a fish at the beginning of the list will grow proportionally and take longer and longer.
deque (pronounced "deck ") from the collections module is a list-like object that allows us to insert items at the beginning or end of a sequence with constant time (O (1)) performance.
Insert an item at the beginning of a deque:
We can instantiate a deque using a preexisting collection of elements, in this case a list of three favorite fish names.
Calling favorite _ fish _ deque's appendleft method allows us to insert an item at the beginning of our collection with O (1) performance.
O (1) performance means that the time it takes to add an item to the beginning of favorite _ fish _ deque will not grow even if favorite _ fish _ deque has thousands or millions of elements.
< $> note Note: Although deque adds entries to the beginning of a sequence more efficiently than a list, deque does not perform all of its operations more efficiently than a list.
For example, accessing a random item in a deque has O (n) performance, but accessing a random item in a list has O (1) performance.
Use deque when it is important to insert or remove elements from either side of your collection quickly.
A full comparison of time performance is available on Python's wiki.
The collections module is a powerful part of the Python standard library that lets you work with data concisely and efficiently.
This tutorial covered three of the classes provided by the collections module including namedtuple, defaultdict, and deque.
From here, you can use the collection module's documentation to learn more about other available classes and utilities.
To learn more about Python in general, you can read our How To Code in Python 3 tutorial series.
Understanding Destructuring, Rest Parameters, and Spread Syntax in JavaScript
5511
Many new features for working with arrays and objects have been made available to the JavaScript language since the 2015 Edition of the ECMAScript specification.
A few of the notable ones that you will learn in this article are destructuring, rest parameters, and spread syntax.
These features provide more direct ways of accessing the members of an array or an object, and can make working with these data structures quicker and more succinct.
Many other languages do not have corresponding syntax for destructuring, rest parameters, and spread, so these features may have a learning curve both for new JavaScript developers and those coming from another language.
In this article, you will learn how to destructure objects and arrays, how to use the spread operator to unpack objects and arrays, and how to use rest parameters in function calls.
Destructuring
Destructuring assignment is a syntax that allows you to assign object properties or array items as variables.
This can greatly reduce the lines of code necessary to manipulate data in these structures.
There are two types of destructuring: Object destructuring and Array destructuring.
Object Destructuring
Object destructuring allows you to create new variables using an object property as the value.
Consider this example, an object that represents a note with an id, title, and date:
Traditionally, if you wanted to create a new variable for each property, you would have to assign each variable individually, with a lot of repetition:
With object destructuring, this can all be done in one line.
By surrounding each variable in curly brackets {}, JavaScript will create new variables from each property with the same name:
Now, console.log () the new variables:
You will get the original property values as output:
< $> note Note: Destructuring an object does not modify the original object.
You could still call the original note with all its entries intact.
The default assignment for object destructuring creates new variables with the same name as the object property.
If you do not want the new variable to have the same name as the property name, you also have the option of renaming the new variable by using a colon (:) to decide a new name, as seen with noteId in the following:
Log the new variable noteId to the console:
You can also destructure nested object values.
For example, update the note object to have a nested author object:
Now you can destructure note, then destructure once again to create variables from the author properties:
Next, log the new variables firstName and lastName using template literals:
Note that in this example, though you have access to the contents of the author object, the author object itself is not accessible.
In order to access an object as well as its nested values, you would have to declare them separately:
This code will output the author object:
Destructuring an object is not only useful for reducing the amount of code that you have to write; it also allows you to target your access to the properties you care about.
Finally, destructuring can be used to access the object properties of primitive values.
For example, String is a global object for strings, and has a length property:
This will find the inherent length property of a string and set it equal to the length variable.
Log length to see if this worked:
The string A string was implicitly converted into an object here to retrieve the length property.
Array Destructuring
Array destructuring allows you to create new variables using an array item as a value.
Consider this example, an array with the various parts of a date:
Arrays in JavaScript are guaranteed to preserve their order, so in this case the first index will always be a year, the second will be the month, and so on.
Knowing this, you can create variables from the items in the array:
But doing this manually can take up a lot of space in your code.
With array destructuring, you can unpack the values from the array in order and assign them to their own variables, like so:
Now log the new variables:
Values can be skipped by leaving the destructuring syntax blank between commas:
Running this will give the value of year and day:
Nested arrays can also be destructured.
First, create a nested array:
Then destructure that array and log the new variables:
Destructuring syntax can be applied to destructure the parameters in a function.
To test this out, you will destructure the keys and values out of Object.entries ().
First, declare the note object:
Given this object, you could list the key-value pairs by destructuring arguments as they are passed to the forEach () method:
Or you could accomplish the same thing using a for loop:
Either way, you will receive the following:
Object destructuring and array destructuring can be combined in a single destructuring assignment.
Default parameters can also be used with destructuring, as seen in this example that sets the default date to new Date ().
Then destructure the object, while also setting a new date variable with the default of new Date ():
console.log (date) will then give output similar to the following:
As shown in this section, the destructuring assignment syntax adds a lot of flexibility to JavaScript and allows you to write more succinct code.
In the next section, you will see how spread syntax can be used to expand data structures into their constituent data entries.
Spread
Spread syntax (...) is another helpful addition to JavaScript for working with arrays, objects, and function calls.
Spread allows objects and iterables (such as arrays) to be unpacked, or expanded, which can be used to make shallow copies of data structures to increase the ease of data manipulation.
Spread with Arrays
Spread can simplify common tasks with arrays.
For example, let's say you have two arrays and want to combine them:
Originally you would use concat () to concatenate the two arrays:
Now you can also use spread to unpack the arrays into a new array:
Running this would give the following:
This can be particularly helpful with immutability.
For example, you might be working with an app that has users stored in an array of objects:
You could use push to modify the existing array and add a new user, which would be the mutable option:
But this changes the user array, which we might want to preserve.
Spread allows you to create a new array from the existing one and add a new item to the end:
Now the new array, updatedUsers, has the new user, but the original users array remains unchanged:
Creating copies of data instead of changing existing data can help prevent unexpected changes.
In JavaScript, when you create an object or array and assign it to another variable, you are not actually creating a new object - you are passing a reference.
Take this example, in which an array is created and assigned to another variable:
Removing the last item of the second Array will modify the first one:
Spread allows you to make a shallow copy of an array or object, meaning that any top level properties will be cloned, but nested objects will still be passed by reference.
For simple arrays or objects, a shallow copy may be all you need.
If you write the same example code but copy the array with spread, the original array will no longer be modified:
The following will be logged to the console:
Spread can also be used to convert a set, or any other iterable to an Array.
Create a new set and add some entries to it:
Next, use the spread operator with set and log the results:
This can also be useful for creating an array from a string:
This will give an array with each character as an item in the array:
Spread with Objects
When working with objects, spread can be used to copy and update objects.
Originally, Object.assign () was used to copy an object:
The secondObject will now be a clone of the originalObject.
This is simplified with the spread syntax - you can shallow copy an object by spreading it into a new one:
Just like with arrays, this will only create a shallow copy, and nested objects will still be passed by reference.
Adding or modifying properties on an existing object in an immutable fashion is simplified with spread.
In this example, the isLoggedIn property is added to the user object:
One important thing to note with updating objects via spread is that any nested object will have to be spread as well.
For example, let's say that in the user object there is a nested organization object:
If you tried to add a new item to organization, it would overwrite the existing fields:
This would result in the following:
If mutability is not an issue, the field could be updated directly:
But since we are seeking an immutable solution, we can spread the inner object to retain the existing properties:
Spread with Function Calls
Spread can also be used with arguments in function calls.
As an example, here is a multiply function that takes three parameters and multiplies them:
Normally, you would pass three values individually as arguments to the function call, like so:
However, if all the values you want to pass to the function already exist in an array, the spread syntax allows you to use each item in an array as an argument:
< $> note Note: Without spread, this can be accomplished by using apply ():
Now that you have seen how spread can shorten your code, you can take a look at a different use of the... syntax: rest parameters.
Rest Parameters
The last feature you will learn in this article is the rest parameter syntax.
The syntax appears the same as spread (...) but has the opposite effect.
Instead of unpacking an array or object into individual values, the rest syntax will create an array of an indefinite number of arguments.
In the function restTest for example, if we wanted args to be an array composed of an indefinite number of arguments, we could have the following:
All the arguments passed to the restTest function are now available in the args array:
Rest syntax can be used as the only parameter or as the last parameter in the list.
If used as the only parameter, it will gather all arguments, but if it's at the end of a list, it will gather every argument that is remaining, as seen in this example:
This will take the first two arguments individually, then group the rest into an array:
In older code, the arguments variable could be used to gather all the arguments passed through to a function:
This would give the following output:
However, this has a few disadvantages.
First, the arguments variable cannot be used with arrow functions.
This would yield an error:
Additionally, arguments is not a true array and cannot use methods like map and filter without first being converted to an array.
It also will collect all arguments passed instead of just the rest of the arguments, as seen in the restTest (one, two,... args) example.
Rest can be used when destructuring arrays as well:
Rest can also be used when destructuring objects:
Giving the following output:
In this way, rest syntax provides efficient methods for gathering an indeterminate amount of items.
In this article, you learned about destructuring, spread syntax, and rest parameters.
In summary:
Destructuring is used to create varibles from array items or object properties.
Spread syntax is used to unpack iterables such as arrays, objects, and function calls.
Rest parameter syntax will create an array from an indefinite number of values.
Destructuring, rest parameters, and spread syntax are useful features in JavaScript that help keep your code succinct and clean.
If you would like to see destructuring in action, take a look at How To Customize React Components with Props, which uses this syntax to destructure data and pass it to custom front-end components.
If you'd like to learn more about JavaScript, return to our How To Code in JavaScript series page.
How To Install and Set Up Laravel with Docker Compose on Ubuntu 20.04
5832
Docker installed on your server, following Steps 1 and 2 of How To Install and Use Docker on Ubuntu 20.04.
Docker Compose installed on your server, following Step 1 of How To Install and Use Docker Compose on Ubuntu 20.04.
< $> note Note: In case you are running this demo on your local machine, use http: / / localhost: 8000 to access the application from your browser.
How To Remotely Access GUI Applications Using Docker and Caddy on Ubuntu 18.04
5806
Even with the growing popularity of cloud services, the need for running native applications still exists.
By using noVNC and TigerVNC, you can run native applications inside a Docker container and access them remotely using a web browser.
Additionally, you can run your application on a server with more system resources than you might have available locally, which can provide increased flexibility when running large applications.
In this tutorial, you'll containerize Mozilla Thunderbird, an email client, using Docker.
Afterward, you'll secure it and provide remote access using the Caddy web server.
When you're finished, you'll be able to access Thunderbird from any device using just a web browser.
Optionally, you'll also be able to locally access the files from it using WebDAV.
You'll also have a fully self-contained Docker image that you can run anywhere.
One Ubuntu 18.04 server with at least 2GB RAM and 4GB disk space.
A non-root user with sudo privileges.
Docker set up on your server.
You can follow the How To Install and Use Docker on Ubuntu 18.04.
Step 1 & mdash; Creating the supervisord Configuration
Now that your server is running and Docker is installed, you are ready to begin configuring your application's container.
Since your container consists of multiple components, you need to use a process manager to launch and monitor them.
Here, you'll be using supervisord. supervisord is a process manager written in Python that is often used to orchestrate complex containers.
First, create and enter a directory called thunderbird for your container:
Now create and open a file called supervisord.conf using nano or your preferred editor:
Now add this first block of code into supervisord.conf, which will define the global options for supervisord:
In this block, you are configuring supervisord itself.
You need to set nodaemon to true because it will be running inside of a Docker container as the entrypoint.
Therefore, you want it to remain running in the foreground.
You also are setting pidfile to a path accessible by a non-root user (more on this later), and logfile to stdout so you can see the logs.
Next, add another small block of code to supervisord.conf.
This block starts TigerVNC, which is a combined VNC / X11 server:
In this block, you are setting up the X11 server.
X11 is a display server protocol, which is what allows GUI applications to run.
Note that in the future it will be replaced with Wayland, but remote access is still in development.
For this container, you are using TigerVNC and its built-in VNC server.
This has a number of advantages over using a separate X11 and VNC server:
Faster response time, as the GUI drawing is done directly to the VNC server rather than being done to an intermediary framebuffer (the memory which stores the contents of the screen).
Automatic screen resizing, which allows the remote application to automatically resize to fit the client (in this case, your web browser window).
If you wish, you can change the argument for the -desktop option from Thunderbird to something else of your choosing.
The server will display your choice as the title of the webpage used to access your application.
Now, let's add a third block of code to supervisord.conf to start easy-novnc:
In this block, you are setting up easy-novnc, a standalone server which provides a wrapper around noVNC.
This server performs two roles.
First, it provides a simple connection page which allows you to configure options for the connection, and allows you to set default ones.
Second, it proxies VNC over WebSocket, which allows it to be accessed through an ordinary web browser.
Usually, resizing is done on the client side (i.e. image scaling), but you are using the resize = remote option to take full advantage of TigerVNC's remote resolution adjustments.
This also provides lower latency on slower devices, such as lower-end Chromebooks:
< $> note Note: This tutorial uses easy-novnc.
If you wish, you can use websockify and a separate web server instead.
The advantage of easy-novnc is that the memory usage and startup time is significantly lower and that it's self-contained. easy-novnc also provides a cleaner connection page than the default noVNC one and allows setting default options that are helpful for this setup (such as resize = remote).
Now add the following block to your configuration to start OpenBox, the window manager:
In this block, you are setting up OpenBox, a lightweight X11 window manager.
You could skip this step, but without it, you wouldn't have title bars or be able to resize windows.
Finally, let's add the last block to supervisord.conf, which will start the main application:
In this final block, you are setting priority to 1 to ensure that Thunderbird launches after TigerVNC, or it would encounter a race-condition and randomly fail to start.
We also set autorestart = true to automatically reopen the application if it mistakenly closes.
The DISPLAY environment variable tells the application to display on the VNC server you set up earlier.
Here is what your completed supervisord.conf will look like:
If you want to containerize a different application, replace / usr / bin / thunderbird with the path to your application's executable.
Otherwise, you are now ready to configure your GUI's main menu.
Step 2 & mdash; Setting Up the OpenBox Menu
Now that your process manager is configured, let's set up the OpenBox menu.
This menu allows us to launch applications inside the container.
We will also include a terminal and process monitor for debugging if required.
Inside your application's directory, use nano or your favorite text editor to create and open a new file called menu.xml:
Now add the following code to menu.xml:
This XML file contains the menu items that will appear when you right-click on the desktop.
Each item consists of a label and an action.
If you want to containerize a different application, replace / usr / bin / thunderbird with the path to your application's executable and change the label of the item.
Step 3 & mdash; Creating the Dockerfile
Now that OpenBox is configured, you'll be creating the Dockerfile, which ties everything together.
Create a Dockerfile in your container's directory:
To begin, let's add some code to build easy-novnc:
In the first stage, you are building easy-novnc.
This is done in a separate stage for simplicity and to save space & mdash; you don't need the entire Go toolchain in your final image.
Note the @ v1.1.0 in the build command.
This ensures that the result is deterministic, which is important because Docker caches the result of each step.
If you had not specified an explicit version, Docker would reference the latest version of easy-novnc at the time the image was first built.
In addition, you want to ensure that you download a specific version of easy-novnc, in case breaking changes are made to the CLI interface.
Now let's create the second stage, which will become the final image.
Here you will be using Debian 10 (buster) as the base image.
Note that since this is running in a container, it will work regardless of the distribution you are running on your server.
Next, add the following block to your Dockerfile:
In this instruction, you are installing Debian 10 as your base image and then installing the bare minimum required to run GUI applications in your container.
Note that you run apt-get update as part of the same instruction to prevent caching issues from Docker.
To save space, you are also removing the package lists downloaded afterward (the cached packages themselves are removed by default).
You are also creating / usr / share / desktop-directories because some applications depend on the directory existing.
Let's add another small block of code:
In this instruction, you are installing some useful general-purpose utilities and packages.
Of particular interest here are xdg-utils (which provides the base commands used by desktop applications on Linux) and ca-certificates (which installs the root certificates to allow us to access HTTPS sites).
Now, we can add the instructions for the main application:
As before, here we are installing the application.
If you are containerizing a different application, you can replace these commands with the ones required to install your specific app. Some applications will require a bit more work to run inside Docker.
For example, if you are installing an app that uses Chrome, Chromium, or QtWebEngine, you'll need to use the command line argument --no-sandbox because it won't be supported within Docker.
Next, let's start adding the instructions to add the last few files to the container:
Here you are adding the configuration files you created earlier to the image and copying the easy-novnc binary from the first stage.
This next code block creates the data directory and adds a dedicated user for your app. This is important because some applications refuse to run as root.
It's also good practice not to run applications as root, even in a container.
To ensure a consistent UID / GID for the files, you are explicitly setting both to 1000.
You are also mounting a volume on the data directory to ensure it persists between restarts.
Finally, let's add the instructions to launch everything:
By setting the default command to supervisord, the manager will launch the processes required to run your application.
In this case, you are using CMD rather than ENTRYPOINT.
In most cases, it wouldn't make a difference, but using CMD is better-suited for this purpose for a few reasons.
First, supervisord doesn't take any arguments that would be relevant to us, and if you provide arguments to the container, they replace CMD and are appended to ENTRYPOINT.
Second, using CMD allows us to provide an entirely different command (which will be executed by / bin / sh -c) when passing arguments to the container, which makes debugging easier.
And lastly, you need to run chown as root before starting supervisord to prevent permission issues on the data volume and to allow the child processes to open stdout.
This also means you need to use gosu instead of the USER instruction to switch the user.
Here is what your completed Dockerfile will look like:
Save and close your Dockerfile.
Now we are ready to build and run our container, and then access Thunderbird & mdash; a GUI application.
Step 4 & mdash; Building and Running the Container
The next step is to build your container and set it to run at startup.
You'll also set up a volume to preserve the application data between restarts and updates.
First build your container.
Make sure to run these commands in the ~ / thunderbird directory:
Now create a new network that will be shared between the app's containers:
Then create a volume to store the application data:
Finally, run it and set it to restart automatically:
Note that if you want, you can replace the < ^ > thunderbird-app < ^ > after the --name option with a different name.
Whatever you have chosen, your application is now containerized and running.
Now let's use the Caddy web server to secure it and remotely connect to it.
Step 5 & mdash; Setting up Caddy
In this step, you'll set up the Caddy web server to provide authentication and, optionally, remote file access over WebDAV.
For simplicity, and to allow you to use it with your existing reverse proxy, you'll run it in another container.
Create a new directory and then move inside it:
Now create a new Dockerfile using nano or your preferred editor:
Then add the following directives:
This Dockerfile builds Caddy with the WebDAV plugin enabled, and then launches it on port 8080 with the Caddyfile at / etc / Caddyfile.
Next you will configure the Caddy web server.
Create a file named Caddyfile in the directory you just created:
Now add the following code block to your Caddyfile:
This Caddyfile proxies the root directory to the thunderbird-app container you created in Step 4 (Docker resolves it into the correct IP).
It will also serve a read-only web-based file browser on / files and run a WebDAV server on / webdav which you can mount locally to access your files.
The username and password are read from the environment variables APP _ USERNAME and APP _ PASSWORD _ HASH.
Now build the container:
Caddy v.2 requires you to hash your desired password.
Run the following command and remember to replace < ^ > mypass < ^ > with a strong password of your choosing:
This command will output a string of characters.
Copy this to your clipboard in preparation of running the next command.
Now you are ready to run the container.
Make sure to replace < ^ > myuser < ^ > with a username of your choosing, and replace < ^ > mypass-hash < ^ > with the output of the command you ran in the previous step.
You can also change the port (< ^ > 8080 < ^ > here) to access your server on a different port:
We are now ready to access and test our application.
Step 6 & mdash; Testing and Managing the Application
Let's access your application and ensure that it's working.
First, open http: / / < ^ > your _ server _ ip < ^ >: < ^ > 8080 < ^ > in a web browser, log in with the credentials you chose earlier, and click Connect.
NoVNC connect page
You should now be able to interact with the application, and it should automatically resize to fit your browser window.
Thunderbird main menu
If you right-click on the black desktop, you should see a menu that allows you to access a terminal.
If you middle-click, you should see a list of windows.
NoVNC right click
Now open http: / / < ^ > your _ server _ ip < ^ >: < ^ > 8080 < ^ > / files / in a web browser.
You should be able to access your files.
NoVNC file access webdav
Optionally, you can try mounting http: / / < ^ > your _ server _ ip < ^ >: < ^ > 8080 < ^ > / webdav / in a WebDAV client.
You should be able to access and modify your files directly.
If you use the Map network drive option in Windows Explorer, you will either need to use a reverse proxy to add HTTPS or set HKLM\ SYSTEM\ CurrentControlSet\ Services\ WebClient\ Parameters\ BasicAuthLevel to DWORD: 2.
In either case, your native GUI application is now ready for remote use.
You have now successfully set up a Docker container for Thunderbird and then, using Caddy, you've configured access to it through a web browser.
Should you ever need to upgrade your app, stop the containers, run docker rm thunderbird-app thunderbird-web, re-build the images, and then re-run the docker run commands from the previous steps above.
Your data will still be preserved since it is stored in a volume.
If you want to learn more about basic Docker commands, you can read this tutorial or this cheatsheet.
For longer-term use, you may also want to consider enabling HTTPS (this requires a domain) for additional security.
Additionally, if you're deploying more than one application, you may want to use Docker Compose or Kubernetes instead of starting each container manually.
And remember, this tutorial can serve as a base for running any other Linux application on your server, including:
Wine, a compatibility layer for running Windows applications on Linux.
GIMP, an open-source image editor.
Cutter, an open-source reverse engineering platform.
This last option demonstrates the great potential of containerizing and remotely accessing GUI applications.
With this setup, you can now use a server with considerably more computing power than you might have locally to run resource-intensive tools like Cutter.
How To Remotely Access GUI Applications Using Docker and Caddy on Ubuntu 20.04
5835
One Ubuntu 20.04 server with at least 2GB RAM and 4GB disk space.
You can follow the How To Install and Use Docker on Ubuntu 20.04.
How To Create a Redundant Storage Pool Using GlusterFS on Ubuntu 20.04
6028
An earlier version of this tutorial was written by Justin Ellingwood.
When storing any critical data, having a single point of failure is very risky.
While many databases and other software allow you to spread data out in the context of a single application, other systems can operate on the filesystem level to ensure that data is copied to another location whenever it's written to disk.
GlusterFS is a network-attached storage filesystem that allows you to pool storage resources of multiple machines.
In turn, this lets you treat multiple storage devices that are distributed among many computers as a single, more powerful unit.
GlusterFS also gives you the freedom to create different kinds of storage configurations, many of which are functionally similar to RAID levels.
For instance, you can stripe data across different nodes in the cluster, or you can implement redundancy for better data availability.
In this guide, you will create a redundant clustered storage array, also known as a distributed file system or, as it's referred to in the GlusterFS documentation, a Trusted Storage Pool.
This will provide functionality similar to a mirrored RAID configuration over the network: each independent server will contain its own copy of the data, allowing your applications to access either copy, thereby helping distribute your read load.
This redundant GlusterFS cluster will consist of two Ubuntu 20.04 servers.
This will act similar to an NAS server with mirrored RAID.
You'll then access the cluster from a third Ubuntu 20.04 server configured to function as a GlusterFS client.
A Note About Running GlusterFS Securely
When you add data to a GlusterFS volume, that data gets synced to every machine in the storage pool where the volume is hosted.
This traffic between nodes isn't encrypted by default, meaning there's a risk it could be intercepted by malicious actors.
For this reason, if you're going to use GlusterFS in production, it's recommended that you run it on an isolated network.
For example, you could set it up to run in a Virtual Private Cloud (VPC) or with a VPN running between each of the nodes.
< $> info If you plan to deploy GlusterFS on DigitalOcean, you can set it up in an isolated network by adding your server infrastructure to a DigitalOcean Virtual Private Cloud.
For details on how to set this up, see our VPC product documentation.
To follow this tutorial, you will need three servers running Ubuntu 20.04.
Each server should have a non-root user with administrative privileges, and a firewall configured with UFW.
< $> note Note: As mentioned in the Goals section, this tutorial will walk you through configuring two of your Ubuntu servers to act as servers in your storage pool and the remaining one to act as a client which you'll use to access these nodes.
For clarity, this tutorial will refer to these machines with the following hostnames:
Role in Storage Pool
gluster0
Server
gluster1
gluster2
Client
Commands that should be run on either gluster0 or gluster1 will have blue and red backgrounds, respectively:
Commands that should only be run on the client (gluster2) will have a green background:
Commands that can or should be run on more than one machine will have a gray background:
Step 1 - Configuring DNS Resolution on Each Machine
Setting up some kind of hostname resolution between each computer can help with managing your Gluster storage pool.
This way, whenever you have to reference one of your machines in a gluster command later in this tutorial, you can do so with an easy-to-remember domain name or even a nickname instead of their respective IP addresses.
If you do not have a spare domain name, or if you just want to set up something quickly, you can instead edit the / etc / hosts file on each computer.
This is a special file on Linux machines where you can statically configure the system to resolve any hostnames contained in the file to static IP addresses.
< $> note Note: If you'd like to configure your servers to authenticate with a domain that you own, you'll first need to obtain a domain name from a domain registrar - like Namecheap or Enom - and configure the appropriate DNS records.
Once you've configured an A record for each server, you can jump ahead to Step 2. As you follow this guide, make sure that you replace gluster < ^ > N < ^ > .example.com and gluster < ^ > N < ^ > with the domain name that resolves to the respective server referenced in the example command.
< $> info If you obtained your infrastructure from DigitalOcean, you could add your domain name to DigitalOcean then set up a unique A record for each of your servers.
Using your preferred text editor, open this file with root privileges on each of your machines.
By default, the file will look something like this with comments removed:
On one of your Ubuntu servers, add each server's IP address followed by any names you wish to use to reference them in commands below the local host definition.
In the following example, each server is given a long hostname that aligns with gluster < ^ > N < ^ > .example.com and a short one that aligns with gluster < ^ > N < ^ >.
You can change the gluster < ^ > N < ^ > .example.com and gluster < ^ > N < ^ > portions of each line to whatever name - or names separated by single spaces - you would like to use to access each server.
Note, though, that this tutorial will use these examples throughout:
< $> note Note: If your servers are part of a Virtual Private Cloud infrastructure pool, you should use each server's private IP address in the / etc / hosts file rather than their public IPs.
When you are finished adding these new lines to the / etc / hosts file of one machine, copy and add them to the / etc / hosts files on your other machines.
Each / etc / hosts file should contain the same lines, linking your servers' IP addresses to the names you've selected.
If you used nano, do so by pressing CTRL + X, Y, and then ENTER.
Now that you've configured hostname resolution between each of your servers, it will be easier to run commands later on as you set up a storage pool and volume.
Next, you'll go through another step that must be completed on each of your servers.
Namely, you'll add the Gluster project's official personal package archive (PPA) to each of your three Ubuntu servers to ensure that you can install the latest version of GlusterFS.
Step 2 - Setting Up Software Sources on Each Machine
Although the default Ubuntu 20.04 APT repositories do contain GlusterFS packages, at the time of this writing they are not the most recent versions.
One way to install the latest stable version of GlusterFS (version < ^ > 7.6 < ^ > as of this writing) is to add the Gluster project's official PPA to each of your three Ubuntu servers.
Add the PPA for the GlusterFS packages by running the following command on each server:
Press ENTER when prompted to confirm that you actually want to add the PPA.
After adding the PPA, refresh each server's local package index.
This will make each server aware of the new packages available:
After adding the Gluster project's official PPA to each server and updating the local package index, you're ready to install the necessary GlusterFS packages.
However, because two of your three machines will act as Gluster servers and the other will act as a client, there are two separate installation and configuration procedures.
First, you'll install and set up the server components.
Step 3 - Installing Server Components and Creating a Trusted Storage Pool
A storage pool is any amount of storage capacity aggregated from more than one storage resource.
In this step, you will configure two of your servers - gluster0 and gluster1 - as the cluster components.
On both gluster0 and gluster1, install the GlusterFS server package by typing:
When prompted, press Y and then ENTER to confirm the installation.
The installation process automatically configures GlusterFS to run as a systemd service.
However, it doesn't automatically start the service or enable it to run at boot.
To start glusterd, the GlusterFS service, run the following systemctl start command on both gluster0 and gluster1:
Then run the following command on both servers.
This will enable the service to start whenever the server boots up:
Following that, you can check the service's status on either or both servers:
If the service is up and running, you'll receive output like this:
Assuming you followed the prerequisite initial server setup guide, you will have set up a firewall with UFW on each of your machines.
Because of this, you'll need to open up the firewall on each node before you can establish communications between them and create a storage pool.
The Gluster daemon uses port 24007, so you'll need to allow each node access to that port through the firewall of each other node in your storage pool.
To do so, run the following command on gluster0.
Remember to change < ^ > gluster1 _ ip _ address < ^ > to gluster1's IP address:
And run the following command on gluster1.
Again, be sure to change < ^ > gluster0 _ ip _ address < ^ > to gluster0's IP address:
You'll also need to allow your client machine (gluster2) access to this port. Otherwise, you'll run into issues later on when you try to mount the volume.
Run the following command on both gluster0 and gluster1 to open up this port to your client machine:
Then, to ensure that no other machines are able to access Gluster's port on either server, add the following blanket deny rule to both gluster0 and gluster1:
You're now ready to establish communication between gluster0 and gluster1.
To do so, you'll need to run the gluster peer probe command on one of your nodes.
It doesn't matter which node you use, but the following example shows the command being run on gluster0:
Essentially, this command tells gluster0 to trust gluster1 and register it as part of its storage pool.
If the probe is successful, it will return the following output:
You can check that the nodes are communicating at any time by running the gluster peer status command on either one.
In this example, it's run on gluster1:
If you run this command from gluster1, it will show output like this:
At this point, your two servers are communicating and ready to create storage volumes with each other.
Step 4 - Creating a Storage Volume
Recall that the primary goal of this tutorial is to create a redundant storage pool.
To this end you'll set up a volume with replica functionality, allowing you to keep multiple copies of your data and prevent your cluster from having a single point of failure.
To create a volume, you'll use the gluster volume create command with this general syntax:
Here's what this gluster volume create command's arguments and options mean:
< ^ > volume _ name < ^ >: This is the name you'll use to refer to the volume after it's created.
The following example command creates a volume named volume1.
replica < ^ > number _ of _ servers < ^ >: Following the volume name, you can define what type of volume you want to create.
Recall that the goal of this tutorial is to create a redundant storage pool, so we'll use the replica volume type.
This requires an argument indicating how many servers the volume's data will be replicated to (2 in the case of this tutorial).
< ^ > domain1.com: /... < ^ > and < ^ > domain2.com: /... < ^ >: These define the machines and directory location of the bricks - GlusterFS's term for its basic unit of storage, which includes any directories on any machines that serve as a part or a copy of a larger volume - that will make up volume1.
The following example will create a directory named gluster-storage in the root directory of both servers.
force: This option will override any warnings or options that would otherwise come up and halt the volume's creation.
Following the conventions established earlier in this tutorial, you can run this command to create a volume.
Note that you can run it from either gluster0 or gluster1:
If the volume was created successfully, you'll receive the following output:
At this point, your volume has been created, but it's not yet active.
You can start the volume and make it available for use by running the following command, again from either of your Gluster servers:
You'll receive this output if the volume was started correctly:
Next, check that the volume is online.
Run the following command from either one of your nodes:
This will return output similar to this:
Based on this output, the bricks on both servers are online.
As a final step to configuring your volume, you'll need to open up the firewall on both servers so your client machine will be able to connect to and mount the volume.
According to the previous command's sample output, volume1 is running on port 49152 on both machines.
This is GlusterFS's default port for its initial volume, and the next volume you create will use port 49153, then 49154, and so on.
Run the following command on both gluster0 and gluster1 to allow gluster2 access to this port through each one's respective firewall:
Then, for an added layer of security, add another blanket deny rule for the volume's port on both gluster0 and gluster1.
This will ensure that no machines other than your client can access the volume on either server:
Now that your volume is up and running, you can set up your client machine and begin using it remotely.
Step 5 - Installing and Configuring Client Components
Your volume is now configured and available for use by your client machine.
Before you begin though, you need to install the glusterfs-client package from the PPA you set up in Step 1 on your client machine.
This package's dependencies include some of GlusterFS's common libraries and translator modules and the FUSE tools required for it to work.
Run the following command on gluster2:
You will mount your remote storage volume on your client computer shortly.
Before you can do that, though, you need to create a mount point.
Traditionally, this is in the / mnt directory, but anywhere convenient can be used.
For simplicity's sake, create a directory named / storage-pool on your client machine to serve as the mount point.
This directory name starts with a forward slash (/) which places it in the root directory, so you'll need to create it with sudo privileges:
Now you can mount the remote volume.
Before that, though, take a look at the syntax of the mount command you'll use to do so:
mount is a utility found in many Unix-like operating systems.
It's used to mount filesystems - anything from external storage devices, like SD cards or USB sticks, to network-attached storage as in the case of this tutorial - to directories on the machine's existing filesystem.
The mount command syntax you'll use includes the -t option, which requires three arguments: the type of filesystem to be mounted, the device where the filesystem to mount can be found, and the directory on the client where you'll mount the volume.
Notice that in this example syntax, the device argument points to a hostname followed by a colon and then the volume's name.
GlusterFS abstracts the actual storage directories on each host, meaning that this command doesn't look to mount the / gluster-storage directory, but instead the volume1 volume.
Also notice that you only have to specify one member of the storage cluster.
This can be either node, since the GlusterFS service treats them as one machine.
Run the following command on your client machine (gluster2) to mount the volume to the / storage-pool directory you created:
Following that, run the df command.
This will display the amount of available disk space for file systems to which the user invoking it has access:
This command will show that the GlusterFS volume is mounted at the correct location:
Now, you can move on to testing that any data you write to the volume on your client gets replicated to your server nodes as expected.
Step 6 - Testing Redundancy Features
Now that you've set up your client to use your storage pool and volume, you can test its functionality.
On your client machine (gluster2), navigate to the mount point that you defined in the previous step:
Then create a few test files.
The following command creates ten separate empty files in your storage pool:
If you examine the storage directories you defined earlier on each storage host, you'll discover that all of these files are present on each system.
On gluster0:
Likewise, on gluster1:
As these outputs show, the test files that you added to the client were also written to both of your nodes.
If there is ever a point when one of the nodes in your storage cluster is down, it could fall out of sync with the storage pool if any changes are made to the filesystem.
Doing a read operation on the client mount point after the node comes back online will alert the node to get any missing files:
Now that you've verified that your storage volume is mounted correctly and can replicate data to both machines in the cluster, you can lock down access to the storage pool.
Step 7 - Restricting Redundancy Features
At this point, any computer can connect to your storage volume without any restrictions.
You can change this by setting the auth.allow option, which defines the IP addresses of whatever clients should have access to the volume.
If you're using the / etc / hosts configuration, the names you've set for each server will not route correctly.
You must use a static IP address instead.
On the other hand, if you're using DNS records, the domain name you've configured will work here.
On either one of your storage nodes (gluster0 or gluster1), run the following command:
If the command completes successfully, it will return this output:
If you need to remove the restriction at any point, you can type:
This will allow connections from any machine again.
This is insecure, but can be useful for debugging issues.
If you have multiple clients, you can specify their IP addresses or domain names at the same time (depending whether you are using / etc / hosts or DNS hostname resolution), separated by commas:
Your storage pool is now configured, secured, and ready for use.
Next you "ll learn a few commands that will help you get information about the status of your storage pool.
Step 8 - Getting Information About your Storage Pool with GlusterFS Commands
When you begin changing some of the settings for your GlusterFS storage, you might get confused about what options you have available, which volumes are live, and which nodes are associated with each volume.
There are a number of different commands that are available on your nodes to retrieve this data and interact with your storage pool.
If you want information about each of your volumes, run the gluster volume info command:
Similarly, to get information about any peers that this node is connected to, you can type:
If you want detailed information about how each node is performing, you can profile a volume by typing:
When this command is complete, you can obtain the information that was gathered by typing:
As shown previously, for a list of all of the GlusterFS associated components running on each of your nodes, run the gluster volume status command:
If you are going to be administering your GlusterFS storage volumes, it may be a good idea to drop into the GlusterFS console.
This will allow you to interact with your GlusterFS environment without needing to type sudo gluster before everything:
This will give you a prompt where you can type your commands. help is a good one to get yourself oriented:
When you are finished, run exit to exit the Gluster console:
With that, you're ready to begin integrating GlusterFS with your next application.
By completing this tutorial, you have a redundant storage system that will allow you to write to two separate servers simultaneously.
This can be useful for a number of applications and can ensure that your data is available even when one server goes down.
How To Install and Secure Grafana on Ubuntu 20.04
5741
Grafana is an open-source data visualization and monitoring tool that integrates with complex data from sources like Prometheus, InfluxDB, Graphite, and ElasticSearch.
Grafana lets you create alerts, notifications, and ad-hoc filters for your data while also making collaboration with your teammates easier through built-in sharing features.
In this tutorial, you will install Grafana and secure it with an SSL certificate and an Nginx reverse proxy.
Once you have set up Grafana, you'll have the option to configure user authentication through GitHub, allowing you to better organize your team permissions.
One Ubuntu 20.04 server set up by following the Initial Server Setup Guide for Ubuntu 20.04, including a non-root user with sudo privileges and a firewall configured with ufw.
This tutorial uses < ^ > your _ domain < ^ > throughout.
The following DNS records set up for your server.
You can follow How To Set Up a Host Name with DigitalOcean for details on how to add them.
Nginx set up by following the How To Install Nginx on Ubuntu 20.04 tutorial, including a server block for your domain.
An Nginx server block with Let's Encrypt configured, which you can set up by following How To Secure Nginx with Let's Encrypt on Ubuntu 20.04.
Optionally, to set up GitHub authentication, you'll need a GitHub account associated with an organization.
Step 1 - Installing Grafana
In this first step, you will install Grafana onto your Ubuntu 20.04 server.
You can install Grafana either by downloading it directly from its official website or by going through an APT repository.
Because an APT repository makes it easier to install and manage Grafana's updates, you'll use that method in this tutorial.
Download the Grafana GPG key with wget, then pipe the output to apt-key.
This will add the key to your APT installation's list of trusted keys, which will allow you to download and verify the GPG-signed Grafana package:
In this command, the option -q turns off the status update message for wget, and -O outputs the file that you downloaded to the terminal.
These two options ensure that only the contents of the downloaded file are pipelined to apt-key.
Next, add the Grafana repository to your APT sources:
Refresh your APT cache to update your package lists:
You can now proceed with the installation:
Once Grafana is installed, use systemctl to start the Grafana server:
Next, verify that Grafana is running by checking the service's status:
This output contains information about Grafana's process, including its status, Main Process Identifier (PID), and more. < ^ > active (running) < ^ > shows that the process is running correctly.
Lastly, enable the service to automatically start Grafana on boot:
This confirms that systemd has created the necessary symbolic links to autostart Grafana.
Grafana is now installed and ready for use.
Next, you wil secure your connection to Grafana with a reverse proxy and SSL certificate.
Step 2 - Setting Up the Reverse Proxy
Using an SSL certificate will ensure that your data is secure by encrypting the connection to and from Grafana.
But, to make use of this connection, you'll first need to reconfigure Nginx as a reverse proxy for Grafana.
Open the Nginx configuration file you created when you set up the Nginx server block with Let's Encrypt in the Prerequisites.
You can use any text editor, but for this tutorial we'll use nano:
Locate the following block:
Because you already configured Nginx to communicate over SSL and because all web traffic to your server already passes through Nginx, you just need to tell Nginx to forward all requests to Grafana, which runs on port 3000 by default.
Delete the existing try _ files line in this location block and replace it with the following proxy _ pass option:
This will map the proxy to the appropriate port. Once you're done, save and close the file by pressing CTRL + X, Y, and then ENTER if you're using nano.
Now, test the new settings to make sure everything is configured correctly:
Finally, activate the changes by reloading Nginx:
You can now access the default Grafana login screen by pointing your web browser to https: / / < ^ > your _ domain < ^ >.
If you're unable to reach Grafana, verify that your firewall is set to allow traffic on port 443 and then re-trace the previous instructions.
With the connection to Grafana encrypted, you can now implement additional security measures, starting with changing Grafana's default administrative credentials.
Step 3 - Updating Credentials
Because every Grafana installation uses the same administrative credentials by default, it is a best practice to change your login information as soon as possible.
In this step, you'll update the credentials to improve security.
Start by navigating to https: / / < ^ > your _ domain < ^ > from your web browser.
This will bring up the default login screen where you'll see the Grafana logo, a form asking you to enter an Email or username and Password, a Log in button, and a Forgot your password?
Grafana Login
Enter admin into both the Email or username and Password fields and then click on the Log in button.
On the next screen, you'll be asked to make your account more secure by changing the default password:
Change Password
Enter the password you'd like to start using into the New password and Confirm new password fields.
From here, you can click Submit to save the new information or press Skip to skip this step.
If you skip, you will be prompted to change the password next time you log in.
In order to increase the security of your Grafana setup, click Submit.
You'll go to the Welcome to Grafana dashboard:
Home Dashboard
You've now secured your account by changing the default credentials.
Next, you will make changes to your Grafana configuration so that nobody can create a new Grafana account without your permission.
Step 4 - Disabling Grafana Registrations and Anonymous Access
Grafana provides options that allow visitors to create user accounts for themselves and preview dashboards without registering.
When Grafana isn't accessible via the internet or when it's working with publicly available data like service statuses, you may want to allow these features.
However, when using Grafana online to work with sensitive data, anonymous access could be a security problem.
To fix this problem, make some changes to your Grafana configuration.
Start by opening Grafana's main configuration file for editing:
Locate the following allow _ sign _ up directive under the [users] heading:
Enabling this directive with true adds a Sign Up button to the login screen, allowing users to register themselves and access Grafana.
Disabling this directive with false removes the Sign Up button and strengthens Grafana's security and privacy.
Uncomment this directive by removing the; at the beginning of the line and then setting the option to false:
Next, locate the following enabled directive under the [auth.anonymous] heading:
Setting enabled to true gives non-registered users access to your dashboards; setting this option to false limits dashboard access to registered users only.
Uncomment this directive by removing the; at the beginning of the line and then setting the option to false.
To activate the changes, restart Grafana:
Verify that everything is working by checking Grafana's service status:
Like before, the output will report that Grafana is active (running).
Now, point your web browser to https: / / < ^ > your _ domain < ^ >.
To return to the Sign Up screen, bring your cursor to your avatar in the lower left of the screen and click on the Sign out option that appears.
Once you have signed out, verify that there is no Sign Up button and that you can't sign in without entering login credentials.
At this point, Grafana is fully configured and ready for use.
Next, you can simplify the login process for your organization by authenticating through GitHub.
(Optional) Step 5 - Setting Up a GitHub OAuth App
For an alternative approach to signing in, you can configure Grafana to authenticate through GitHub, which provides login access to all members of authorized GitHub organizations.
This can be particularly useful when you want to allow multiple developers to collaborate and access metrics without having to create Grafana-specific credentials.
Start by logging in to a GitHub account associated with your organization and then navigate to your GitHub profile page at https: / / github.com / settings / profile.
Switch settings context by clicking on your name on the left side of the screen then selecting your organization in the dropdown menu.
This will switch context from Personal settings to Organization settings.
On the next screen, you'll see your Organization profile where you can change settings like your Organization display name, organization Email, and organization URL.
Because Grafana uses OAuth & mdash; an open standard for granting remote third parties access to local resources & mdash; to authenticate users through GitHub, you'll need to create a new OAuth application within GitHub.
Click the OAuth Apps link under Developer settings on the lower left-hand side of the screen.
If you don't already have any OAuth applications associated with your organization on GitHub, you'll be told there are No Organization Owned Applications.
Otherwise, you'll see a list of the OAuth applications already connected to your account.
Click the New OAuth App button to continue.
On the next screen, fill in the following details about your Grafana installation:
Application name - This helps you distinguish your different OAuth applications from one another.
Homepage URL - This tells GitHub where to find Grafana.
Type https: / / < ^ > your _ domain < ^ > into this field, replacing < ^ > your _ domain < ^ > with your domain.
Application Description - This provides a description of your OAuth application's purpose.
Application callback URL - This is the address where users will be sent once successfully authenticated.
For Grafana, this field must be set to https: / / < ^ > your _ domain < ^ > / login / github.
Keep in mind that Grafana users logging in through GitHub will see the values you entered in the first three preceding fields, so be sure to enter something meaningful and appropriate.
When completed, the form will look something like:
GitHub Register OAuth Application
Click the green Register application button.
You will now be redirected to a page containing the Client ID and Client Secret associated with your new OAuth application.
Make note of both values, because you will need to add them to Grafana's main configuration file to complete the setup.
< $> warning Warning: Make sure to keep your Client ID and Client Secret in a secure and non-public location, because they could be used as the basis of an attack.
With your GitHub OAuth application created, you're now ready to reconfigure Grafana to use GitHub for authentication.
(Optional) Step 6 - Configuring Grafana as a GitHub OAuth App
To complete GitHub authentication for your Grafana setup, you will now make some changes to your Grafana configuration files.
To begin, open the main Grafana configuration file.
Locate the [auth.github] heading, and uncomment this section by removing the; at the beginning of every line except; allowed _ domains = and; team _ ids =, which will not be changed in this tutorial.
Next, make the following changes:
Set enabled and allow _ sign _ up to true.
This will enable GitHub Authentication and permit members of the allowed organization to create accounts themselves.
Note that this setting is different from the allow _ sign _ up property under [users] that you changed in Step 4.
Set client _ id and client _ secret to the values you got while creating your GitHub OAuth application.
Set allowed _ organizations to the name of your organization to ensure that only members of your organization can sign up and log in to Grafana.
The complete configuration will look like this:
You've now told Grafana everything it needs to know about GitHub.
To complete the setup, you'll need to enable redirects behind a reverse proxy.
This is done by setting a root _ url value under the [server] heading.
Save your configuration and close the file.
Then, restart Grafana to activate the changes:
Lastly, verify that the service is up and running.
The output will indicate that the service is active (running).
Now, test your new authentication system by navigating to https: / / < ^ > your _ domain < ^ >.
If you are already logged in to Grafana, hover your mouse over the avatar log in the lower left-hand corner of the screen, and click on Sign out in the secondary menu that appears next to your name.
On the login page, you'll see a new section under the original Log in button that includes a Sign in with GitHub button with the GitHub logo.
Grafana Login page with GitHub
Click on the Sign in with GitHub button to be redirected to GitHub, where you'll sign in to your GitHub account and confirm your intention to Authorize Grafana.
Click the green Authorize < ^ > your\ _ github\ _ organization < ^ > button.
< $> note Note: Make sure your GitHub account is a member of your approved organization and your Grafana email address matches your GitHub email address.
If you try to authenticate with a GitHub account that isn't a member of your approved organization, you'll get a Login Failed message telling you User not a member of one of the required organizations.
You will now be logged in with your existing Grafana account.
If a Grafana account doesn't already exist for the user you logged in as, Grafana will create a new user account with Viewer permissions, ensuring that new users can only use existing dashboards.
To change the default permissions for new users, open the main Grafana configuration file for editing.
Locate the auto _ assign _ org _ role directive under the [users] heading, and uncomment the setting by removing the; at the beginning of the line.
Set the directive to one of the following values:
Viewer - can only use existing dashboards
Editor - can use, modify, and add dashboards
Admin - has permission to do everything
This tutorial will set the auto-assign to Viewer:
Once you've saved your changes, close the file and restart Grafana:
Check the service's status:
Like before, the status will read active (running).
At this point, you have fully configured Grafana to allow members of your GitHub organization to register and use your Grafana installation.
In this tutorial you installed, configured, and secured Grafana, and you also learned how to permit members of your organization to authenticate through GitHub.
To extend your current Grafana installation, see the list of official and community-built dashboards and plugins.
To learn more about using Grafana in general, see the official Grafana documentation, or check out our other monitoring tutorials.
How To Install Docker Compose on Ubuntu 20.04 Quickstart
5840
In this quickstart guide, we'll install Docker Compose on an Ubuntu 20.04 server.
For a more detailed version of this tutorial, with more explanations of each step, please refer to How To Install and Use Docker Compose on Ubuntu 20.04.
To follow this guide, you'll need access to an Ubuntu 20.04 server or local machine as a sudo user, and Docker installed on this system.
Step 1 - Download Docker Compose
Start by confirming the most recent Docker Compose release available in their releases page.
Run the following command to download Docker Compose and make this software globally accessible on your system as docker-compose:
Step 2 - Set Up Executable Permissions
How To Install and Use Docker on Ubuntu 20.04
How To Install Git on Ubuntu 20.04
5789
Version control systems like Git are essential to modern software development best practices.
Many software projects "files are maintained in Git repositories, and platforms like GitHub, GitLab, and Bitbucket help to facilitate software development project sharing and collaboration.
In this guide, we will go through how to install and configure Git on an Ubuntu 20.04 server.
We will cover how to install the software two different ways: via the built-in package manager, and via source.
Each of these approaches come with their own benefits depending on your specific needs.
You will need an Ubuntu 20.04 server with a non-root superuser account.
The option of installing with default packages is best if you want to get up and running quickly with Git, if you prefer a widely-used stable version, or if you are not looking for the newest available functionalities.
If you are looking for the most recent release, you should jump to the section on installing from source.
Git is likely already installed in your Ubuntu 20.04 server.
You can confirm this is the case on your server with the following command:
If you receive output similar to the following, then Git is already installed.
If this is the case for you, then you can move onto setting up Git, or you can read the next section on how to install from source if you need a more up-to-date version.
However, if you did not get output of a Git version number, you can install it with the Ubuntu default package manager APT.
You can confirm that you have installed Git correctly by running the following command and checking that you receive relevant output.
If you "re looking for a more flexible method of installing Git, you may want to compile the software from source, which we will go over in this section.
This takes longer and will not be maintained through your package manager, but it will allow you to download the latest release and will give you greater control over the options you include if you wish to make customizations.
Verify the version of Git currently installed on the server:
If Git is installed, you "ll receive output similar to the following:
This is all available in the default repositories, so we can update our local package index and then install the relevant packages.
From the Git project website, we can navigate to the tarball list available at https: / / mirrors.edge.kernel.org / pub / software / scm / git / and download the version you would like.
At the time of writing, the most recent version is 2.26.2, so we will download that for demonstration purposes.
Now, replace the shell process so that the version of Git we just installed will be used:
After you are satisfied with your Git version, you should configure Git so that the generated commit messages you make will contain your correct information and support you as you build your software project.
Configuration can be achieved by using the git config command.
The information you enter is stored in your Git configuration file, which you can optionally edit by hand with a text editor of your choice like this (we "ll use nano):
Press CTRL and X, then Y then ENTER to exit the text editor.
How To Install Jenkins on Ubuntu 20.04
5827
When faced with repetitive technical tasks, finding automation solutions that work can be a chore.
With Jenkins, an open-source automation server, you can efficiently manage tasks from building to deploying software.
Jenkins is Java-based, installed from Ubuntu packages or by downloading and running its web application archive (WAR) file - a collection of files that make up a complete web application to run on a server.
In this tutorial we "ll install Jenkins on Ubuntu 20.04, start the development server and create an administrative user to get you started in exploring what Jenkins can do.
While you "ll have a development-level server ready for use at the conclusion of this tutorial, to secure this installation for production, follow the guide How to Configure Jenkins with SSL Using an Nginx Reverse Proxy on Ubuntu 18.04.
One Ubuntu 20.04 server configured with a non-root sudo user and firewall by following the Ubuntu 20.04 initial server setup guide.
We recommend starting with at least 1 GB of RAM.
Visit Jenkins "s" Hardware Recommendations "for guidance in planning the capacity of a production-level Jenkins installation.
Oracle JDK 11 installed, following our guidelines on installing specific versions of OpenJDK on Ubuntu 20.04.
Step 1 - Installing Jenkins
The version of Jenkins included with the default Ubuntu packages is often behind the latest available version from the project itself.
To ensure you have the latest fixes and features, use the project-maintained packages to install Jenkins.
First, add the repository key to the system:
After the key is added the system will return with OK.
Next, let "s append the Debian package repository address to the server's sources.list:
After both commands have been entered, we "ll run update so that apt will use the new repository.
Finally, we "ll install Jenkins and its dependencies.
Now that Jenkins and its dependencies are in place, we'll start the Jenkins server.
Step 2 - Starting Jenkins
Let's start Jenkins by using systemctl:
Since systemctl doesn't display status output, we "ll use the status command to verify that Jenkins started successfully:
If everything went well, the beginning of the status output shows that the service is active and configured to start at boot:
Now that Jenkins is up and running, let's adjust our firewall rules so that we can reach it from a web browser to complete the initial setup.
Step 3 - Opening the Firewall
To set up a UFW firewall, visit Initial Server Setup with Ubuntu 20.04, Step 4- Setting up a Basic Firewall.
By default, Jenkins runs on port 8080.
We "ll open that port using ufw:
< $> note Note: If the firewall is inactive, the following commands will allow OpenSSH and enable the firewall:
Check ufw's status to confirm the new rules:
You'll notice that traffic is allowed to port 8080 from anywhere:
With Jenkins installed and our firewall configured, we can complete the installation stage and dive into Jenkins setup.
Step 4 - Setting Up Jenkins
To set up your installation, visit Jenkins on its default port, 8080, using your server domain name or IP address: http: / / < ^ > your _ server _ ip _ or _ domain < ^ >: 8080
You should receive the Unlock Jenkins screen, which displays the location of the initial password:
Unlock Jenkins screen
In the terminal window, use the cat command to display the password:
Copy the 32-character alphanumeric password from the terminal and paste it into the Administrator password field, then click Continue.
The next screen presents the option of installing suggested plugins or selecting specific plugins:
Customize Jenkins Screen
We'll click the Install suggested plugins option, which will immediately begin the installation process.
Jenkins Getting Started Install Plugins Screen
When the installation is complete, you "ll be prompted to set up the first administrative user.
It's possible to skip this step and continue as admin using the initial password we used above, but we'll take a moment to create the user.
< $> note Note: The default Jenkins server is NOT encrypted, so the data submitted with this form is not protected.
Refer to How to Configure Jenkins with SSL Using an Nginx Reverse Proxy on Ubuntu 20.04 to protect user credentials and information about builds that are transmitted via the web interface.
Jenkins Create First Admin User Screen
Enter the name and password for your user:
Jenkins Create User
You'll receive an Instance Configuration page that will ask you to confirm the preferred URL for your Jenkins instance.
Confirm either the domain name for your server or your server's IP address:
Jenkins Instance Configuration
After confirming the appropriate information, click Save and Finish.
You'll receive a confirmation page confirming that "Jenkins is Ready! ":
Jenkins is ready screen
Click Start using Jenkins to visit the main Jenkins dashboard:
Welcome to Jenkins Screen
At this point, you have completed a successful installation of Jenkins.
In this tutorial, you installed Jenkins using the project-provided packages, started the server, opened the firewall, and created an administrative user.
At this point, you can start exploring Jenkins.
When you've completed your exploration, follow the guide How to Configure Jenkins with SSL Using an Nginx Reverse Proxy on Ubuntu 20.04 to protect your passwords, as well as any sensitive system or product information that will be sent between your machine and the server in plain text to continue using Jenkins.
To learn more about what you can do using Jenkins, check out other tutorials on the subject:
How to Build Android Apps with Jenkins
How To Set Up Continuous Integration Pipelines in Jenkins on Ubuntu 16.04
How To Install WordPress on Ubuntu 20.04 with a LAMP Stack
5469
WordPress is an extremely popular open-source technology for making websites and blogs on the internet today.
Used by 63% of all websites that use a content management system (CMS), WordPress sites represent 36% of all websites that are currently online.
There are many different approaches to getting access to WordPress and some setup processes are more complex than others.
This tutorial is intended for those who desire to install and administer a Wordpress instance on an unmanaged cloud server via the command line.
Though this approach requires more steps than a ready-made WordPress installation, it offers administrators greater control over their WordPress environment.
< $> info If you are looking to access a ready-made WordPress installation, DigitalOcean Marketplace offers a one-click app to get you started with WordPress through installation when spinning up your server.
Depending on your needs and goals, you may find other options that are more suitable.
As open-source software, WordPress can be freely downloaded and installed, but to be available on the web, you will likely need to purchase cloud infrastructure and a domain name.
Continue following this guide if you are interested in working through the server-side installation and set up of a WordPress site.
This tutorial will be using a LAMP (Linux, Apache, MySQL, and PHP) stack, which is one option for a server architecture that supports WordPress by providing the Linux operating system, Apache web server, MySQL database, and PHP programming language.
We "ll install and set up WordPress via LAMP on a Linux Ubuntu 20.04 server.
In order to complete this tutorial, you will need access to an Ubuntu 20.04 server and will need to complete these steps before beginning this guide:
Set up your server by following our Ubuntu 20.04 initial server setup guide, and ensure you have a non-root sudo user.
Install a LAMP stack by following our LAMP guide to install and configure this software.
Secure your site: WordPress takes in user input and stores user data, so it is important for it to have a layer of security.
TLS / SSL is the technology that allows you to encrypt the traffic from your site so that your and your users "connection is secure.
Here are two options available to you to meet this requirement:
If you have a domain name... you can secure your site with Let "s Encrypt, which provides free, trusted certificates.
Step 1 - Creating a MySQL Database and User for WordPress
To get started, log into the MySQL root (administrative) account by issuing this command (note that this is not the root user of your server):
< $> note Note: If you cannot access your MySQL database via root, as a sudo user you can update your root user "s password by logging into the database like so:
Once you receive the MySQL prompt, you can update the root user "s password.
Here, replace < ^ > new _ password < ^ > with a strong password of your choosing.
You may now type EXIT; and can log back into the database via password with the following command:
Within the database, we can create an exclusive database for WordPress to control.
You can call this whatever you would like, but we will be using the name wordpress in this guide.
Next, we are going to create a separate MySQL user account that we will use exclusively to operate our new database.
Creating specific databases and accounts can support us from a management and security standpoint.
We will use the name wordpressuser in this guide, but feel free to use whatever name is relevant for you.
Remember to choose a strong password here for your database user where we have < ^ > password < ^ >:
Next, let the database know that our wordpressuser should have complete access to the database we set up:
In the next step, we "ll lay some foundations for WordPress plugins by downloading PHP extensions for our server.
This will lay the groundwork for installing additional plugins into our WordPress site.
We will need to restart Apache to load these new extensions, we "ll be doing more configurations on Apache in the next section, so you can wait until then, or restart now to complete the PHP extension process.
Step 3 - Adjusting Apache's Configuration to Allow for .htaccess Overrides and Rewrites
In this guide, we'll use / etc / apache2 / sites-available / < ^ > wordpress < ^ > .conf as an example here, but you should substitute the path to your configuration file where appropriate.
Additionally, we will use / var / www / < ^ > wordpress < ^ > as the root directory of our WordPress install.
If you followed our LAMP tutorial, it may be your domain name instead of wordpress in both of these instances.
This is fine to use if you "re only going to host one website on this server.
With our paths identified, we can move onto working with .htaccess so that Apache can handle configuration changes on a per-directory basis.
WordPress and many WordPress plugins use these files extensively for in-directory tweaks to the web server "s behavior.
Open the Apache configuration file for your website with a text editor like nano.
In nano, you can do this by pressing CTRL and X together, then Y, then ENTER.
This allows you to have more human-readable permalinks to your posts, like the following two examples:
The a2enmod command calls a script that enables the specified module within the Apache configuration.
Before we implement the changes we "ve made, check to make sure we haven" t made any syntax errors by running the following test.
You may receive output like the following:
This is just a message, however, and doesn "t affect the functionality of your site.
Restart Apache to implement the changes.
Make sure to restart now even if you have restarted earlier in this tutorial.
Step 4 - Downloading WordPress
Change into a writable directory (we recommend a temporary one like / tmp) and download the compressed release.
We "ll also copy over the sample configuration file to the filename that WordPress reads:
We can also create the upgrade directory, so that WordPress won "t run into permissions issues when trying to do this on its own following an update to its software:
Ensure that you replace the / var / www / < ^ > wordpress < ^ > directory with the directory you have set up on your server.
Step 5 - Configuring the WordPress Directory
An important step that we need to accomplish is setting up reasonable file permissions and ownership.
We "ll start by giving ownership of all the files to the www-data user and group.
This is the user that the Apache web server runs as, and Apache will need to be able to read and write WordPress files in order to serve the website and perform automatic updates.
Update the ownership with the chown command which allows you to modify file ownership.
Be sure to point to your server "s relevant directory.
These permissions should get you working effectively with WordPress, but note that some plugins and procedures may require additional tweaks.
When we open the file, our first task will be to adjust some secret keys to provide a level of security for our installation.
These are only used internally, so it won "t hurt usability to have complex, secure values here.
You will get back unique values that resemble output similar to the block below.
Do NOT copy the values below!
Find the section that contains the example values for those settings.
Next, we are going to modify some of the database connection settings at the beginning of the file.
You need to adjust the database name, the database user, and the associated password that you configured within MySQL.
Since we "ve given the web server permission to write where it needs to, we can explicitly set the filesystem method to" direct ".
In your web browser, navigate to your server "s domain name or public IP address:
Select a name for your WordPress site and choose a username.
It is recommended to choose something unique and avoid common usernames like "admin" for security purposes.
At this point, you can begin to design your WordPress website!
Congratulations, WordPress is now installed and is ready to be used!
At this point you may want to start doing the following:
Choose your permalinks setting for WordPress posts, which can be found in Settings > Permalinks.
Select a new theme in Appearance > Themes.
Install new plugins to increase your site "s functionality under Plugins > Add New.
If you are going to collaborate with others, you may also wish to add additional users at this time under Users > Add New.
You can find additional resources for alternate ways to install WordPress, learn how to install WordPress on different server distributions, automate your WordPress installations, and scale your WordPress sites by checking out our WordPress Community tag.
How To Set Up a Node.js Application for Production on Ubuntu 20.04
6051
Node.js is an open-source JavaScript runtime environment for building server-side and networking applications.
The platform runs on Linux, macOS, FreeBSD, and Windows.
Though you can run Node.js applications at the command line, this tutorial will focus on running them as a service.
This means that they will restart on reboot or failure and are safe for use in a production environment.
In this tutorial, you will set up a production-ready Node.js environment on a single Ubuntu 20.04 server.
This server will run a Node.js application managed by PM2, and provide users with secure access to the application through an Nginx reverse proxy.
The Nginx server will offer HTTPS using a free certificate provided by Let's Encrypt.
This guide assumes that you have the following:
An Ubuntu 20.04 server setup, as described in the initial server setup guide for Ubuntu 20.04.
You should have a non-root user with sudo privileges and an active firewall.
A domain name pointed at your server's public IP.
This tutorial will use the domain name example.com throughout.
Nginx installed, as covered in How To Install Nginx on Ubuntu 20.04.
Nginx configured with SSL using Let's Encrypt certificates.
How To Secure Nginx with Let's Encrypt on Ubuntu 20.04 will walk you through the process.
When you've completed the prerequisites, you will have a server serving your domain's default placeholder page at https: / / < ^ > example.com < ^ > /.
Step 1 - Installing Node.js
Let's begin by installing the latest LTS release of Node.js, using the NodeSource package archives.
First, install the NodeSource PPA in order to get access to its contents.
Make sure you're in your home directory, and use curl to retrieve the installation script for the most recent LTS version of Node.js from its archives.
You can inspect the contents of this script with nano or your preferred text editor:
When you're done inspecting the script, run it under sudo:
After running the setup script from Nodesource, you can install the Node.js package:
< $> note Note: When installing from the NodeSource PPA, the Node.js executable is called nodejs, rather than node.
With the Node.js runtime installed, let's move on to writing a Node.js application.
Step 2 - Creating a Node.js Application
Let's write a Hello World application that returns "Hello World" to any HTTP requests.
This sample application will help you get Node.js set up.
You can replace it with your own application - just make sure that you modify your application to listen on the appropriate IP addresses and ports.
First, let's create a sample application called hello.js:
This Node.js application listens on the specified address (localhost) and port (3000), and returns "Hello World!"
with a 200 HTTP success code.
Since we're listening on localhost, remote clients won't be able to connect to our application.
To test your application, type:
< $> note Note: Running a Node.js application in this manner will block additional commands until the application is killed by pressing CTRL + C.
To test the application, open another terminal session on your server, and connect to localhost with curl:
If you get the following output, the application is working properly and listening on the correct address and port:
If you do not get the expected output, make sure that your Node.js application is running and configured to listen on the proper address and port.
Once you're sure it's working, kill the application (if you haven't already) by pressing CTRL + C.
Step 3 - Installing PM2
Next let's install PM2, a process manager for Node.js applications.
PM2 makes it possible to daemonize applications so that they will run in the background as a service.
Use npm to install the latest version of PM2 on your server:
The -g option tells npm to install the module globally, so that it's available system-wide.
Let's first use the pm2 start command to run your application, hello.js, in the background:
This also adds your application to PM2's process list, which is outputted every time you start an application:
As indicated above, PM2 automatically assigns an App name (based on the filename, without the .js extension) and a PM2 id. PM2 also maintains other information, such as the PID of the process, its current status, and memory usage.
Applications that are running under PM2 will be restarted automatically if the application crashes or is killed, but we can take an additional step to get the application to launch on system startup using the startup subcommand.
This subcommand generates and configures a startup script to launch PM2 and its managed processes on server boots:
The last line of the resulting output will include a command to run with superuser privileges in order to set PM2 to start on boot:
Run the command from the output, with your username in place of < ^ > sammy < ^ >:
As an additional step, we can save the PM2 process list and corresponding environments:
You have now created a systemd unit that runs pm2 for your user on boot.
This pm2 instance, in turn, runs hello.js.
Start the service with systemctl:
If at this point you encounter an error, you may need to reboot, which you can achieve with sudo reboot.
Check the status of the systemd unit:
For a detailed overview of systemd, please review Systemd Essentials: Working with Services, Units, and the Journal.
In addition to those we have covered, PM2 provides many subcommands that allow you to manage or look up information about your applications.
Stop an application with this command (specify the PM2 App name or id):
Restart an application:
List the applications currently managed by PM2:
Get information about a specific application using its App name:
The PM2 process monitor can be pulled up with the monit subcommand.
This displays the application status, CPU, and memory usage:
Note that running pm2 without any arguments will also display a help page with example usage.
Now that your Node.js application is running and managed by PM2, let's set up the reverse proxy.
Step 4 - Setting Up Nginx as a Reverse Proxy Server
Your application is running and listening on localhost, but you need to set up a way for your users to access it. We will set up the Nginx web server as a reverse proxy for this purpose.
In the prerequisite tutorial, you set up your Nginx configuration in the / etc / nginx / sites-available / < ^ > example.com < ^ > file.
Within the server block, you should have an existing location / block.
Replace the contents of that block with the following configuration.
If your application is set to listen on a different port, update the highlighted portion to the correct port number:
This configures the server to respond to requests at its root.
Assuming our server is available at < ^ > example.com < ^ >, accessing https: / / < ^ > example.com < ^ > / via a web browser would send the request to hello.js, listening on port 3000 at localhost.
You can add additional location blocks to the same server block to provide access to other applications on the same server.
For example, if you were also running another Node.js application on port 3001, you could add this location block to allow access to it via https: / / < ^ > example.com < ^ > / < ^ > app2 < ^ >:
Once you are done adding the location blocks for your applications, save the file and exit your editor.
Make sure you didn't introduce any syntax errors by typing:
Restart Nginx:
Assuming that your Node.js application is running, and your application and Nginx configurations are correct, you should now be able to access your application via the Nginx reverse proxy.
Try it out by accessing your server's URL (its public IP address or domain name).
You now have your Node.js application running behind an Nginx reverse proxy on an Ubuntu 20.04 server.
This reverse proxy setup is flexible enough to provide your users access to other applications or static web content that you want to share.
How To Trick a Neural Network in Python 3
6037
The author selected Dev Color to receive a donation as part of the Write for DOnations program.
Could a neural network for animal classification be fooled?
Fooling an animal classifier may have few consequences, but what if our face authenticator could be fooled?
Or our self-driving car prototype's software?
Fortunately, legions of engineers and research stand between a prototype computer-vision model and production-quality models on our mobile devices or cars.
Still, these risks have significant implications and are important to consider as a machine-learning practitioner.
In this tutorial, you will try "fooling" or tricking an animal classifier.
As you work through the tutorial, you'll use OpenCV, a computer-vision library, and PyTorch, a deep learning library.
You will cover the following topics in the associated field of adversarial machine learning:
Create a targeted adversarial example.
Pick an image, say, of a dog.
Pick a target class, say, a cat.
Your goal is to trick the neural network into believing the pictured dog is a cat.
Create an adversarial defense.
In short, protect your neural network against these tricky images, without knowing what the trick is.
By the end of the tutorial, you will have a tool for tricking neural networks and an understanding of how to defend against tricks.
It is recommended that you review Build an Emotion-Based Dog Filter; this tutorial is not explicitly used but introduces the notion of classification.
You'll call your workspace AdversarialML:
Navigate to the AdversarialML directory:
Make a directory to hold all your assets:
Now install prepackaged binaries for OpenCV and numpy, which are libraries for computer vision and linear algebra, respectively.
With the dependencies installed, let's run an animal classifier called ResNet18, which we describe next.
Step 2 - Running a Pretrained Animal Classifier
The torchvision library, the official computer vision library for PyTorch, contains pretrained versions of commonly used computer vision neural networks.
These neural networks are all trained on ImageNet 2012, a dataset of 1.2 million training images with 1000 classes.
These classes include vehicles, places, and most importantly, animals.
In this step, you will run one of these pretrained neural networks, called ResNet18.
We will refer to ResNet18 trained on ImageNet as an "animal classifier".
< $> note What is ResNet18?
ResNet18 is the smallest neural network in a family of neural networks called residual neural networks, developed by MSR (He et al.).
In short, He found that a neural network (denoted as a function f, with input x, and output f (x)) would perform better with a "residual connection" x + f (x).
This residual connection is used prolifically in state-of-the-art neural networks, even today.
For example, FBNetV2, FBNetV3.
Download this image of a dog with the following command:
Image of corgi running near pond
Then, download a JSON file to convert neural network output to a human-readable class name:
Next, create a script to run your pretrained model on the dog image.
Create a new file called step _ 2 _ pretrained.py:
First, add the Python boilerplate by importing the necessary packages and declaring a main function:
Next, load the mapping from neural network output to human-readable class names.
Add this directly after your import statements and before your main function:
Create an image transformation function that will ensure your input image firstly has the correct dimensions, and secondly is normalized correctly.
Add the following function directly after the last:
In get _ image _ transform, you define a number of different transformations to apply to the images that are passed to your neural network:
transforms.Resize (224): Resizes the smaller side of the image to 224. For example, if your image is 448 x 672, this operation would downsample the image to 224 x 336.
transforms.CenterCrop (224): Takes a crop from the center of the image, of size 224 x 224.
transforms.ToTensor (): Converts the image into a PyTorch tensor.
All PyTorch models require PyTorch tensors as input.
transforms.Normalize (mean =..., std =...): Standardizes your input by subtracting the mean, then dividing by the standard deviation.
This is described more precisely in the torchvision documentation.
Add a utility to predict the animal class, given the image.
This method uses both the previous utilities to perform animal classification:
Here the predict function classifies the provided image using a pretrained neural network:
models.resnet18 (pretrained = True): Loads a pretrained neural network called ResNet18.
model.eval (): Modifies the model in-place to run in 'evaluation' mode.
The only other mode is' training 'mode, but training mode isn't needed, as you aren't training the model (that is, updating the model's parameters) in this tutorial.
out = model (image): Runs the neural network on the provided, transformed image.
_, pred = torch.max (out, 1): The neural network outputs one probability for each possible class.
This step computes the index of the class with the highest probability.
For example, if out = [0.4, 0.1, 0.2], then pred = 0.
idx _ to _ label = get _ idx _ to _ label (): Obtains a mapping from class index to human-readable class names.
For example, the mapping could be {0: cat, 1: dog, 2: fish}.
cls = idx _ to _ label [str (int (pred))]: Convert the predicted class index to a class name.
The examples provided in the last two bullet points would yield cls = idx _ to _ label [0] = 'cat'.
Next, following the last function, add a utility to load images:
This will load an image from the path provided in the first argument to the script. transform (image) [None] applies the sequence of image transformations defined in the previous lines.
Finally, populate your main function with the following, to load your image and classify the animal in the image:
Double check that your file matches our final step 2 script at step _ 2 _ pretrained.py on GitHub.
Save and exit your script, and run the animal classifier:
This will produce the following output, showing your animal classifier works as expected:
That concludes running inference with your pretrained model.
Next, you will see an adversarial example in action by tricking a neural network with impercetible differences in the image.
Step 3 - Trying an Adversarial Example
Now, you will synthesize an adversarial example, and test the neural network on that example.
For this tutorial, you will build adversarial examples of the form x + r, where x is the original image and r is some "perturbation".
You will eventually create the perturbation r yourself, but in this step, you will download one we created for you beforehand.
Start by downloading the perturbation r:
Now composite the picture with the perturbation.
Create a new file called step _ 3 _ adversarial.py:
In this file, you will perform the following three-step process, to produce an adversarial example:
Transform an image
Apply the perturbation r
Inverse transform the perturbed image
At the end of step 3, you will have an adversarial image.
First, import the necessary packages and declare a main function:
Next, create an "image transformation" that inverts the earlier image transformation.
Place this after your imports, before the main function:
As before, the transforms.Normalize operation subtracts the mean and divides by the standard deviation (that is, for the original image x, y = transforms.Normalize (mean = u, std = o) = (x - u) / o).
You do some algebra and define a new operation that reverses this normalize function (transforms.Normalize (mean = -u / o, std = 1 / o) = (y - -u / o) / 1 / o = (y + u / o) o = yo + u = x).
As part of the inverse transformation, add a method that transforms a PyTorch tensor back to a PIL image.
Add this following the last function:
tensor.data.numpy () converts the PyTorch tensor into a NumPy array. .transpose (1, 2, 0) rearranges (channels, width, height) into (height, width, channels).
This NumPy array is approximately in the range (0, 1).
Finally, multiply by 255 to ensure the image is now in the range (0, 255).
np.clip ensures that all values in the image are between (0, 255).
x.astype (np.uint8) ensures all image values are integers.
Finally, Image.fromarray (...) creates a PIL image object from the NumPy array.
Then, use these utilities to create the adversarial example with the following:
This function generates the adversarial example as described at the start of the section:
y = x + r. Take your perturbation r and add it to the original image x.
get _ inverse _ transform: Obtain and apply the reverse image transformation you defined several lines earlier.
tensor _ to _ image: Finally, convert the PyTorch tensor back to an image object.
Finally, modify your main function to load the image, load the adversarial perturbation r, apply the perturbation, save the adversarial example to disk, and run prediction on the adversarial example:
Your completed file should match step _ 3 _ adversarial.py on GitHub.
Save the file, exit the editor, and launch your script with:
You've now created an adversarial example: tricking the neural network into thinking a corgi is a goldfish.
In the next step, you will actually create the perturbation r that you used here.
Step 4 - Understanding an Adversarial Example
For a primer on classification, see "How to Build an Emotion-Based Dog Filter".
Taking a step back, recall that your classification model outputs a probability for each class.
During inference, the model predicts the class with the highest probability.
During training, you update the model parameters t to maximize the probability of the correct class y, given your data x.
However, to generate adversarial examples, you now modify your goal.
Instead of finding a class, your goal is now to find a new image, x. Take any class other than the correct one.
Let us call this new class w. Your new objective is to maximize the probability of the wrong class.
Note that the neural network weights t are missing from the above expression.
This is because you now assume the role of the adversary: Someone else has trained and deployed a model.
You are only allowed to create adversarial inputs and are not allowed to modify the deployed model.
To generate the adversarial example x, you can run "training", except instead of updating the neural network weights, you update the input image with the new objective.
As a reminder, for this tutorial, you assume that the adversarial example is an affine transformation of x. In other words, your adversarial example takes the form x + r for some r. In the next step, you will write a script to generate this r.
Step 5 - Creating an Adversarial Example
In this step, you will learn a perturbation r, so that your corgi is misclassified as a goldfish.
Create a new file called step _ 5 _ perturb.py:
Import the necessary packages and declare a main function:
Directly following your imports and before the main function, define two constants:
The first constant TARGET _ LABEL is the class to misclassify the corgi as.
In this case, index 1 corresponds to "goldfish".
The second constant EPSILON is the maximum amount of perturbation allowed for each image value.
This limit is introduced so that the image is imperceptibly altered.
Following your two constants, add a helper function to define a neural network and the perturbation parameter r:
model.resnet18 (pretrained = True) loads a pretrained neural network called ResNet18, like before.
Also like before, you set the model to evaluation mode using .eval.
nn.Parameter (...) defines a new perturbation r, the size of the input image.
The input image is also of size (1, 3, 224, 224).
The requires _ grad = True keyword argument ensures that you can update this perturbation r in later lines, in this file.
Next, begin modifying your main function.
Start by loading the model net, loading the inputs x, and defining the label label:
Next, define both the criterion and the optimizer in your main function.
The former tells PyTorch what the objective is - that is, what loss to minimize.
The latter tells PyTorch how to train your parameter r:
Directly following, add the main training loop for your parameter r:
On each iteration of this training loop, you:
r.data.clamp _ (...): Ensure the parameter r is small, within EPSILON of 0.
optimizer.zero _ grad (): Clear any gradients you computed in the previous iteration.
model (x + r): Run inference on the modified image x + r.
Compute the loss.
Compute the gradient loss.backward.
Take a gradient descent step optimizer.step.
Compute the prediction pred.
Finally, report the loss and predicted class print (...).
Next, save the final perturbation r:
Directly following, still in the main function, save the perturbed image:
Finally, run prediction on both the original image and the adversarial example:
Double check your script matches step _ 5 _ perturb.py on GitHub.
Save, exit, and run the script:
Your script will output the following.
The last two lines indicate you have now completed construction of an adversarial example from scratch.
Your neural network now classifies a perfectly reasonable corgi image as a goldfish.
You've now shown that neural networks can be fooled easily - what's more, the lack of robustness to adversarial examples has significant consequences.
A natural next question is this: How can you combat adversarial examples?
A good amount of research has been conducted by various organizations, including OpenAI.
In the next section, you'll run a defense to thwart this adversarial example.
Step 6 - Defending Against Adversarial Examples
In this step, you will implement a defense against adversarial examples.
The idea is the following: You are now the owner of the animal classifier being deployed to production.
You don't know what adversarial examples may be generated, but you can modify the image or the model to protect against attacks.
Before you defend, you should see for yourself how imperceptible the image manipulation is.
Open both of the following images:
assets / dog.jpg
outputs / adversarial.png
Here, you show both side by side.
Your original image will have a different aspect ratio.
Can you tell which is the adversarial example?
(left) Corgi as goldfish, adversarial, (right) Corgi as itself, not adversarial
Notice that the new image looks identical to the original.
As it turns out, the left image is your adversarial image.
To be certain, download the image and run your evaluation script:
This will output the goldfish class, to prove its adversarial nature:
You will run a fairly naive, but effective, defense: Compress the image by writing to a lossy JPEG format.
Open the Python interactive prompt:
Then, load the adversarial image as PNG, and save it back as a JPEG.
Type CTRL + D to leave the Python interactive prompt.
Next, run inference with your model on the compressed adversarial example:
This will now output the corgi class, proving the efficacy of your naive defense.
You've now completed your very first adversarial defense.
This is what makes an effective defense.
There are also many other forms of defense, many of which involve retraining the neural network.
However, these retraining procedures are a topic of their own and beyond the scope of this tutorial.
With that, this concludes your guide into adversarial machine learning.
To understand the implications of your work in this tutorial, revisit the two images side-by-side - the original and the adversarial example.
Despite the fact that both images look identical to the human eye, the first has been manipulated to fool your model.
Both images clearly feature a corgi, and yet the model is entirely confident that the second model contains a goldfish.
This should concern you and, as you wrap up this tutorial, keep in mind the fragility of your model.
Just by applying a simple transformation, you can fool it. These are real, plausible dangers that evade even cutting-edge research.
Research beyond machine-learning security is just as susceptible to these flaws, and, as a practitioner, it is up to you to apply machine learning safely.
For more readings, check out the following links:
Adversarial Machine Learning tutorial from NeurIPS 2018 Conference.
Related blog posts from OpenAI on adversarial examples and robustness to adversarial attacks.
For more machine learning content and tutorials, you can visit our Machine Learning Topic page.
How To Use the Python Filter Function
6038
The Python built-in filter () function can be used to create a new iterator from an existing iterable (like a list or dictionary) that will efficiently filter out elements using a function that we provide.
An iterable is a Python object that can be "iterated over", that is, it will return items in a sequence such that we can use it in a for loop.
The basic syntax for the filter () function is:
This will return a filter object, which is an iterable.
We can use a function like list () to make a list of all the items returned in a filter object.
The filter () function provides a way of filtering values that can often be more efficient than a list comprehension, especially when we're starting to work with larger data sets.
For example, a list comprehension will make a new list, which will increase the run time for that processing.
This means that after our list comprehension has completed its expression, we'll have two lists in memory.
However, filter () will make a simple object that holds a reference to the original list, the provided function, and an index of where to go in the original list, which will take up less memory.
In this tutorial, we'll review four different ways of using filter (): with two different iterable structures, with a lambda function, and with no defined function.
Using filter () with a Function
The first argument to filter () is a function, which we use to decide whether to include or filter out each item.
The function is called once for every item in the iterable passed as the second argument and each time it returns False, the value is dropped.
As this argument is a function, we can either pass a normal function or we can make use of lambda functions, particularly when the expression is less complex.
Following is the syntax of a lambda with filter ():
With a list, like the following, we can incorporate a lambda function with an expression against which we want to evaluate each item from the list:
To filter this list to find the names of our aquarium creatures that start with a vowel, we can run the following lambda function:
Here we declare an item in our list as x. Then we set our expression to access the first character of each string (or character "zero "), so x [0].
Lowering the case of each of the names ensures this will match letters to the string in our expression, 'aeiou'.
Finally we pass the iterable creature _ names.
Like in the previous section we apply list () to the result in order to create a list from the iterator filter () returns.
The output will be the following:
This same result can be achieved using a function we define:
Our function names _ vowels defines the expression that we will implement to filter creature _ names.
Again, the output would be as follows:
Overall, lambda functions achieve the same result with filter () as when we use a regular function.
The necessity to define a regular function grows as the complexity of expressions for filtering our data increases, which is likely to promote better readability in our code.
Using None with filter ()
We can pass None as the first argument to filter () to have the returned iterator filter out any value that Python considers "falsy".
Generally, Python considers anything with a length of 0 (such as an empty list or empty string) or numerically equivalent to 0 as false, thus the use of the term "falsy."
In the following case we want to filter our list to only show the tank numbers at our aquarium:
In this code we have a list containing integers, empty sequences, and a boolean value.
We use the filter () function with None and pass in the aquarium _ tanks list as our iterable.
Since we have passed None as the first argument, we will check if the items in our list are considered false.
Then we wrap filtered _ tanks in a list () function so that it returns a list for filtered _ tanks when we print.
Here we see the output shows only the integers.
All the items that evaluated to False, that are equivalent to 0 in length, were removed by filter ():
< $> note Note: If we don't use list () and print filtered _ tanks we would receive a filter object something like this: < filter object at 0x7fafd5903240 >.
The filter object is an iterable, so we could loop over it with for or we can use list () to turn it into a list, which we're doing here because it's a good way to review the results.
With None we have used filter () to quickly remove items from our list that were considered false.
Using filter () with a List of Dictionaries
When we have a more complex data structure, we can still use filter () to evaluate each of the items.
For example, if we have a list of dictionaries, not only do we want to iterate over each item in the list - one of the dictionaries - but we may also want to iterate over each key: value pair in a dictionary in order to evaluate all the data.
As an example, let's say we have a list of each creature in our aquarium along with different details about each of them:
We want to filter this data by a search string we give to the function.
To have filter () access each dictionary and each item in the dictionaries, we construct a nested function, like the following:
We define a filter _ set () function that takes aquarium _ creatures and search _ string as parameters.
In filter _ set () we pass our iterator _ func () as the function to filter ().
The filter _ set () function will return the iterator resulting from filter ().
The iterator _ func () takes x as an argument, which represents an item in our list (that is, a single dictionary).
Next the for loop accesses the values in each key: value pair in our dictionaries and then uses a conditional statement to check whether the search _ string is in v, representing a value.
Like in our previous examples, if the expression evaluates to True the function adds the item to the filter object.
This will return once the filter _ set () function has completed.
We position return False outside of our loop so that it checks every item in each dictionary, instead of returning after checking the first dictionary alone.
We call filter _ set () with our list of dictionaries and the search string we want to find matches for:
Once the function completes we have our filter object stored in the filtered _ records variable, which we turn into a list and print:
We'll see the following output from this program:
We've filtered the list of dictionaries with the search string 2. We can see that the three dictionaries that included a tank number with 2 have been returned.
Using our own nested function allowed us to access every item and efficiently check each against the search string.
In this tutorial, we've learned the different ways of using the filter () function in Python.
Now you can use filter () with your own function, a lambda function, or with None to filter for items in varying complexities of data structures.
Although in this tutorial we printed the results from filter () immediately in list format, it is likely in our programs we would use the returned filter () object and further manipulate the data.
If you would like to learn more Python, check out our How To Code in Python 3 series and our Python topic page.
How To Host a Website Using Cloudflare and Nginx on Ubuntu 18.04
6062
Cloudflare is a service that sits between the visitor and the website owner's server, acting as a reverse proxy for websites.
Cloudflare provides a Content Delivery Network (CDN), as well as DDoS mitigation and distributed domain name server services.
Nginx is a popular web server responsible for hosting some of the largest and highest-traffic sites on the internet.
It's common for organizations to serve websites with Nginx and use Cloudflare as a CDN and DNS provider.
In this tutorial you will secure your website served by Nginx with an Origin CA certificate from Cloudflare and then configure Nginx to use authenticated pull requests.
The advantages of using this setup are that you benefit from Cloudflare's CDN and fast DNS resolution while ensuring that all connections pass through Cloudflare.
This prevents any malicious requests from reaching your server.
You can follow our guide on how to install Nginx on Ubuntu 18.04.
A Cloudflare account.
A registered domain added to your Cloudflare account that points to your Nginx server.
Our guide on how to mitigate DDoS attacks against your website with Cloudflare can help you set this up.
Our introduction to DNS terminology, components, and concepts can also provide assistance.
An Nginx Server Block configured for your domain, which you can do by following Step 5 of How To Install Nginx on Ubuntu 18.04.
Step 1 - Generating an Origin CA TLS Certificate
The Cloudflare Origin CA lets you generate a free TLS certificate signed by Cloudflare to install on your Nginx server.
By using the Cloudflare generated TLS certificate you can secure the connection between Cloudflare's servers and your Nginx server.
To generate a certificate with Origin CA, log in to your Clouflare account in a web browser.
Select the domain that you want to secure and navigate to the SSL / TLS section of your Cloudflare dashboard.
From there, navigate to the Origin Server tab and click on the Create Certificate button:
Create certificate option in the Cloudflare dashboard
Leave the default option of Let Cloudflare generate a private key and a CSR selected.
Origin CA GUI options
Click Next and you will see a dialog with the Origin Certificate and Private key.
You need to transfer both the origin certificate and private key from Cloudflare to your server.
For security reasons, the Private Key information will not be displayed again, so copy the key to your server before clicking Ok.
Dialog showing the origin certificate and private key
We'll use the / etc / ssl directory on the server to hold the origin certificate and the private key files.
The folder already exists on the server.
First, copy the contents of the Origin Certificate displayed in the dialog box in your browser.
Then, on your server, open / etc / ssl / cert.pem in your preferred text editor:
Add the certificate contents into the file.
Then save and exit the editor.
Then return to your browser and copy the contents of the Private key.
Open the file / etc / ssl / key.pem for editing:
Paste the private key into the file, save the file, and exit the editor.
< $> note Note: Sometimes, when you copy the certificate and key from the Cloudflare dashboard and paste it into the relevant files on the server, blank lines are inserted.
Nginx will treat such certificates and keys as invalid, so ensure that there are no blank lines in your files.
< $> warning Warning: Cloudflare's Origin CA Certificate is only trusted by Cloudflare and therefore should only be used by origin servers that are actively connected to Cloudflare.
If at any point you pause or disable Cloudflare, your Origin CA certificate will throw an untrusted certificate error.
Now that you copied the key and certificate files to your server, you need to update the Nginx configuration to use them.
Step 2 - Installing the Origin CA Certificate in Nginx
In the previous section, you generated an origin certificate and private key using Cloudlfare's dashboard and saved the files to your server.
Now you'll update the Nginx configuration for your site to use the origin certificate and private key to secure the connection between Cloudflare's servers and your server.
First, make sure that UFW will allow HTTPS traffic.
Enable Nginx Full, which will open both port 80 (HTTP) and port 443 (HTTPS):
Now reload UFW:
Finally, check that your new rules are allowed and that UFW is active:
Now you are ready to adjust your Nginx server block.
Nginx creates a default server block during installation.
Remove it if it still exists, as you've already configured a custom server block for your domain:
Next, open the Nginx configuration file for your domain:
The file should look like this:
We'll modify the Nginx configuration file to do the following:
Listen on port 80 and redirect all requests to use https.
Listen on port 443 and use the origin certificate and private key that you added in the previous section.
Modify the file so it looks like the following:
Next, test to make sure that there are no syntax errors in any of your Nginx configuration files:
If no problems were found, restart Nginx to enable your changes:
Now go to the Cloudflare dashboard's SSL / TLS section, navigate to the Overview tab, and change SSL / TLS encryption mode to Full (strict).
This informs Cloudflare to always encrypt the connection between Cloudflare and your origin Nginx server.
Enable Full (strict) SSL mode in the Cloudflare Dashboard
Now visit your website at https: / / < ^ > your _ domain < ^ > to verify that it's set up properly.
You'll see your home page displayed, and the browser will report that the site is secure.
In the next section, you will set up Authenticated Origin Pulls to verify that your origin server is indeed talking to Cloudflare and not some other server.
By doing so, Nginx will be configured to only accept requests that use a valid client certificate from Cloudflare; all requests that have not passed through Cloudflare will be dropped.
Step 3 - Setting Up Authenticated Origin Pulls
The Origin CA certificate will help Cloudflare verify that it is talking to the correct origin server.
This step will use TLS Client Authentication to verify that your origin Nginx server is talking to Cloudflare.
In a client-authenticated TLS handshake, both sides provide a certificate to be verified.
The origin server is configured to only accept requests that use a valid client certificate from Cloudflare.
Requests which have not passed through Cloudflare will be dropped as they will not have Cloudflare's certificate.
This means that attackers cannot circumvent Cloudflare's security measures and directly connect to your Nginx server.
Cloudflare presents certificates signed by a CA with the following certificate:
You can also download the certificate directly from Cloudflare here.
Copy this certificate.
Then create the file / etc / ssl / cloudflare.crt file to hold Cloudflare's certificate:
Add the certificate to the file.
Now update your Nginx configuration to use TLS Authenticated Origin Pulls.
Open the configuration file for your domain:
Add the ssl _ client _ certificate and ssl _ verify _ client directives as shown in the following example:
Next, test to make sure that there are no syntax errors in your Nginx configuration:
Finally, to enable Authenticated Pulls, open the SSL / TLS section in the Cloudflare dashboard, navigate to the Origin Server tab and toggle the Authenticated Origin Pulls option.
Enable Authenticated Origin Pulls
Now visit your website at https: / / < ^ > your _ domain < ^ > to verify that it was set up properly.
As before, you'll see your home page displayed.
To verify that your server will only accept requests signed by Cloudflare's CA, toggle the Authenticated Origin Pulls option to disable it and then reload your website.
You should get the following error message:
Error message
Your origin server raises an error if a request is not signed by Cloudflare's CA.
< $> note Note: Most browsers will cache requests, so to see the above change you can use Incognito / Private browsing mode in your browser.
To prevent Cloudflare from caching requests while you set up your website, navigate to Overview in the Cloudflare dashboard and toggle Development Mode.
Now that you know it works properly, return to the SSL / TLS section in the Cloudflare dashboard, navigate to the Origin Server tab and toggle the Authenticated Origin Pulls option again to enable it.
In this tutorial you secured your Nginx-powered website by encrypting traffic between Cloudflare and the Nginx server using an Origin CA certificate from Cloudflare.
You then set up Authenticated Origin Pulls on the Nginx server to ensure that it only accepts requests from Cloudflare's servers, preventing anyone else from directly connecting to the Nginx server.
6069
One Ubuntu 20.04 server set up by following the Ubuntu 20.04 initial server setup guide, including a sudo non-root user and a firewall.
You can follow our guide on how to install Nginx on Ubuntu 20.04.
An Nginx Server Block configured for your domain, which you can do by following Step 5 of How To Install Nginx on Ubuntu 20.04.
You'll modify the Nginx configuration file to do the following:
If you found no problems, restart Nginx to enable your changes:
Next, test Nginx to make sure that there are no syntax errors in your Nginx configuration:
Your origin server raises an error if Cloudflare's CA does not sign a request.
How To Install an ERPNext Stack on Ubuntu 20.04
6076
The author selected Software in the Public Interest to receive a donation as part of the Write for DOnations program.
ERPNext is an Enterprise Resource Planning (ERP) suite that leverages the power and flexibility of open-source technologies.
It excels at managing core business processes such as finance, sales, human resources, manufacturing, purchases, services, helpdesk needs, and more.
Among the benefits of implementing a system like ERPNext are:
Better productivity by automating repetitive business processes
Improved IT efficiency by sharing a database for all departments within the company
Better decision-making thanks to an integral vision of how business units relate to each other
ERPNext is based on Frappe, a full-stack web application framework written in Python that takes full advantage of the Node / JavaScript runtime environment and uses MariaDB as its database backend.
One of the many advantages of Frappe-based applications, like ERPNext, is the bench command-line utility.
The bench CLI saves administrators time by automating tasks such as installing, updating, configuring, and managing multiple Frappe / ERPNext sites.
In this tutorial you will install and configure an ERPNext stack on one server running Ubuntu 20.04.
This will allow you to configure your stack for various development or production environments depending on your needs, and it will prepare you to build a more complex, fault-tolerant architecture.
One Ubuntu 20.04 server with at least 4 GB of RAM and a non-root sudo user.
You can set up your server and user by following our Ubuntu 20.04 initial server setup guide.
< $> note Note: When choosing your server's specifications, keep in mind that ERP systems are resource-intensive.
This guide calls for one server with 4 GB of RAM, which is sufficient for basic use cases, but specific hardware requirements may vary depending on the number of users as well as your business size.
A fully registered domain name with an A record pointed to your server.
If you are using a DigitalOcean Droplet then you can follow this guide to properly set up your DNS.
Step 1 & mdash; Configuring the Firewall
Although configuring a firewall for development is optional, for production it is a mandatory security practice.
You will need to open the following ports on your ERPNext server:
80 / tcp and 443 / tcp for HTTP and HTTPS respectively
3306 / tcp for MariaDB connection (recommended only if you need remote access to database)
143 / tcp and 25 / tcp for IMAP and STMP respectively
22 / tcp for SSH (if you have not already enabled OpenSSH in your UFW settings)
8000 / tcp for testing your platform before deploying to production
To open multiple ports at once you can use the following command:
Alternatively, you can allow connections from specific IP addresses on specific ports using this command:
After opening all necessary ports enable the firewall:
Now confirm the status of your firewall:
UFW will output a list of your enabled rules.
Make sure ERPNext's necessary ports are open:
For more information regarding UFW's configuration, consult our guide on how to set up a firewall with UFW on Ubuntu 20.04.
Setting up a proper firewall is the first of two preliminary steps.
Now you will configure keyboard mapping and character encoding on your server.
Step 2 & mdash; Configuring Locales
It's highly recommended that you configure keyboard mapping for the console as well as the language and the character encoding on your host.
This is necessary to avoid possible issues during the ERPNext 12 installation process.
Take note that this configuration has nothing to do with the UI language on your actual ERPNext platform, but with the system locale configuration.
First, update your server:
Now configure keymap, language, and character encoding:
The localectl utility is used by Ubuntu 20.04 and other Linux distributions to control and change system-wide locale and keyboard layout settings before the user logs in, which is exactly what ERPNext 12 requires.
You will also need to add the following lines to your / etc / environment file.
Use nano or your preferred text editor to open the file:
Now add the following content:
Reboot your server to apply all changes:
Give your server a few minutes to reboot and then use ssh to reenter your instance.
You are now ready to install your database.
Step 3 & mdash; Installing MariaDB
Now you will add MariaDB to your server stack.
ERPNext 12 requires MariaDB 10.2 + for proper operation.
Because Ubuntu 20.04 includes MariaDB 10.3 in its official repositories you can install this version using the apt command:
Alternatively, if prefer a newer MariaDB version, you can follow Step 3 of our guide on how to install an ERPNext Stack on Ubuntu 18.04.
This will guide you through MariaDB's online repository wizard, which will help you install the newest version & mdash; MariaDB 10.5.
After installing mariadb-server, install the following packages:
ERPNext 12 is a Python application and thus it requires the python3-mysqldb library for database management. libmysqlclient-dev is required to access certain MariaDB developer features.
Next, add an extra layer of security to the MariaDB server by running the mysql _ secure _ installation script:
The mysql _ secure _ installation script will prompt you with several questions:
The first prompt will ask you about the root password, but since there is no password configured yet, press ENTER.
Next, when asked about changing the MariaDB root password, answer N. Using the default password along with Unix authentication is the recommended setup for Ubuntu-based systems because the root account is closely related to automated system maintenance tasks.
The remaining questions have to do with removing the anonymous database user, restricting the root account to log in remotely on localhost, removing the test database, and reloading privilege tables.
It is safe to answer Y to all those questions.
After completing the mysql _ secure _ installation script, MariaDB will start running using its default configuration.
The standard ERPNext installation uses MariaDB's root user for all database operations.
While that approach may be convenient on single server setups, it is not considered a good security practice.
Therefore, in the next section you will learn how to avoid this issue by creating a new user with special privileges.
Creating a MariaDB Super Admin User
ERPNext expects to use MariaDB's root user for managing database connections, but this is not always ideal.
To overcome this limitation and let a non-root user manage MariaDB you will now manually create a database named after the user.
Then you will be able to assign special privileges to the new user to drive ERPNext database operations.
Open up the MariaDB prompt:
Now create a new database named after the user you want to assign for MariaDB connections.
This tutorial will use < ^ > sammy < ^ > but you can choose a different name:
Confirm that the database was created using this SQL statement:
Now create the MariaDB user < ^ > sammy < ^ > with privileges similar to root and then give the user a strong password of your choice.
Keep the password in a secure place; you will need it later:
Now confirm both the user creation and the new user's privileges:
Now flush privileges to apply all changes:
Once you finish, exit the session:
Now that you have created a database user you only need to fine-tune MariaDB to ensure proper ERPNext 12 operation.
Fortunately, the ERPNext team provides an excellent configuration template that you will use as a starting point for your implementation.
In the next section, you will learn how to properly configure the MariaDB database using that template.
Step 4 & mdash; Configuring MariaDB for ERPNext
With MariaDB installed and secured it's time to fine-tune it for ERPNext connections.
First, stop mariadb.service:
Now use nano or your favorite text editor to create a MariaDB configuration file called mariadb.cnf:
Now add ERPNext's official configuration template:
For more detailed information about these configurations, review this template file on ERPNext's Github repo.
This is a useful starting point for exploring these options.
The configuration file, / etc / mysql / mariadb.conf.d / mariadb.cnf, complements and also overrides a few values included in the default MariaDB configuration located at / etc / mysql / my.cnf.
This file gives you a curated template that greatly enhances database performance for ERPNext.
Keep in mind, however, that while this template is a great starting point nothing prevents you from improving MariaDB's performance even further by adjusting these parameters to fit your needs.
Testing the MariaDB Connection
Since ERPNext relies on the database connection for almost all of its internal operations, it is a good idea to test the connection before continuing.
Start mariadb.service:
To test the connection you can use the following command.
Remember to replace < ^ > sammy < ^ > and < ^ > mariadb _ password < ^ > with your credentials:
You will see an output showing MariaDB's basic help content and several parameters.
This means your connection was successful:
If you need to make any adjustments to MariaDB's settings or fix any errors, remember to reload the service using the following command:
Once you are done, enable MariaDB:
Now that you have tested the database connection, you can continue with the installation of your ERPNext application.
Step 5 & mdash; Setting Up ERPNext 12
Now that your database backend is ready you can continue setting up your ERPNext web application.
In this section, you will learn how to install and configure all components required by ERPNext 12 and then install the application itself.
Start by preparing the server with all the system packages required by ERPNext 12. Install system-wide dependencies using the following command:
The DEBIAN _ FRONTEND = noninteractive variable has been passed to the installation command in order to avoid Postfix prompts.
For detailed information regarding Postfix configuration please read our guide on How To Install and Configure Postfix on Ubuntu 20.04
Next, update pip3, which is Python's standard package manager, and then install the latest versions of three additional Python modules:
setuptools facilitates the installation and upgrading of Python packages, cryptography adds encryption capabilities to your stack, and psutil aids with system monitoring.
Now that you have installed all necessary global dependencies, you will now install all the services and libraries required by ERPNext 12.
Setting Up Node.js and Yarn
ERPNext 12 can work with version 8 + of the Node.js server environment.
In fact, at the time of this writing, the official ERPNext easy _ install script uses Node 8. But from a security perspective, it's advisable to install a newer version because Node 8 reached its End Of Life (EOL) in 2020 and thus will not receive any more security patches.
At the time of this writing, Ubuntu 20.04 contains version 10.19 of Node.js.
Although this version is still maintained, for similar reasons (EOL in less than a year) it's highly advisable to avoid using it. For this guide, Node.js version 12 LTS will be installed along with the corresponding npm and yarn package managers.
Please note that the Frappe framework uses yarn to install dependencies.
If you decide to use an alternative installation method then make sure that you end up with version 1.12 + of yarn running in your system.
Add the NodeSource repository to your system:
Now you can inspect the contents of the downloaded script:
Once you are satisfied with the script's contents you can run the script:
This script will automatically update the apt list.
Now you can install nodejs on your server:
Next, install yarn globally using the npm package manager:
Now that you have installed Node you can continue to configure wkhtmltopdf for your platform.
ERPNext uses the wkhtmltopdf open source tool to convert HTML content into PDF using the Qt WebKit rendering engine.
This feature is mostly used for printing invoices, quotations, and other reports.
In the case of ERPNext 12, a specific version of wkhtmltopdf is required, 0.12.5 with patched Qt.
To install wkhtmltopdf, start by switching to a suitable directory to download the package, in this case / tmp:
Download the appropriate wkhtmltopdf version and package for Ubuntu 20.04 from the project's page:
Now install the package using the dpkg tool:
Next, copy all relevant executables to your / usr / bin / directory:
Once the files are in place, change their permissions to make them executable:
Now that wkhtmltopdf is properly installed we will add Redis to our database stack.
Installing Redis
ERPNext 12 uses Redis to enhance MariaDB's performance.
Specifically, Redis assists with caching.
First, install Redis from the official Ubuntu 20.04 repository:
Then enable Redis on startup:
Now that you have added Redis to your stack let's take a moment to summarize what you have accomplished so far.
Up to this point, you have installed all the major components required by ERPNext 12, which include:
A MariaDB database backend
The Node.js JavaScript server environment
The Yarn package manager
A Redis database cache
The wkhtmltopdf PDF documents generator
Whether you are installing the ERP system for development or for production, you are now ready for the next step, which is installing the Frappe full-stack framework and the actual ERPNext 12 web application.
Step 6 & mdash; Installing Frappe Bench CLI
Now that you have installed all of ERPNext's stack requirements you can unleash the flexibility of Frappe's bench command-line utility.
The bench CLI was designed with the purpose of assisting users in the process of installing, setting up, and managing applications like ERPNext that are based on the Frappe Framework.
In the coming sections, you will install the bench CLI and then use it to complete the process of setting up ERPNext 12.
Make sure that the Frappe user (in this case < ^ > sammy < ^ >) has the proper rights on its home directory:
Now clone the frappe / bench repository to your home directory.
Remember to replace < ^ > sammy < ^ > with your system username:
Install the bench CLI:
This guide is assuming that you are installing ERPNext 12 for testing / production scenarios and thus that you are using the master branch.
But if your intention is to develop applications or custom ERPNext modules, then the develop branch might be a better option.
In either case, you are now prepared to install the Frappe Framework.
This will be your final step before installing ERPNext itself.
Setting Up Frappe Framework Environment
In this section, you will create a Frappe environment using the bench CLI.
During Frappe's installation you may exceed Ubuntu's file watch limit, which by default is set to 8192.
To avoid this issue set a higher limit using the following command:
The tee command will append the contents of your echo command to the called file while also printing the output to your console.
Next, initialize Frappe Framework 12. Replace Sammy with your system username:
During execution you may see one error about your path, along with several warnings.
Let the process continue until the end.
Once it's finished, you will see an output similar to the following one, indicating that your environment was successfully created:
< $> note Note: The bench init process could halt if a spawn ENOMEM error is encountered.
This error is caused when your system runs out of memory.
You must fix the issue before continuing, either by installing more physical memory or allocating SWAP space.
Let's take a closer look at the command used to create the environment:
/ home / < ^ > sammy < ^ > / < ^ > frappe-bench < ^ > is the path where the Frappe Framework, the websites, and associated applications will be installed.
A new directory, called < ^ > frappe-bench < ^ > in this example, will be created to accommodate all necessary files.
--frappe-path points to Frappe repository, which in this case is the official Github repository.
--frappe-branch is the Frappe version to be installed.
Because you want to install ERPNext 12, the chosen version is Frappe 12.
--python is the Python version that will be used.
ERPNext 12 requires Python 3.6 +.
Prior versions, however, still use Python 2.7.
For more information regarding bench CLI commands please refer to the Bench Commands Cheatsheet.
The flexibility offered by the Frappe framework goes way beyond using isolated environments.
You can also create different websites and install applications into them.
Step 7 & mdash; Installing the ERPNext 12 Web Application
In this section, you will set up a Frappe-based site and then install the ERPNext 12 application on it.
Change to the directory where Frappe was initialized.
Before continuing, you will need to install specific versions of Python libraries numpy and pandas into the Frappe virtual environment.
Install these packages using the following command:
At this point the installation might halt for about 10 to 20 minutes while displaying this message:
This has to do with a bug related to pandas and Ubuntu 20.04, which, at the time of writing, is still rather new.
Nevertheless the packages will build, and once they complete you will see an output like this:
Download ERPNext 12 from its repository using the bench CLI:
Next, create the new site, replacing < ^ > your _ domain < ^ > with the domain that you have associated with this server's IP:
Let's take a moment to review the options used in the command above:
bench new-site creates a new site based on the Frappe Framework.
< ^ > your _ domain < ^ > is the name for the new site.
Make sure that your domain's DNS has an A record pointing at your server's IP.
Keep this password in a safe place & mdash; you will need it shortly.
< ^ > mariadb _ password < ^ > is the password that you created at the beginning of the guide for the MariaDB user < ^ > sammy < ^ >.
Following this, install the ERPNext application onto the site:
Once the installation completes, you will have a working ERPNext 12 application.
Now let's test it using a bench command:
The above will initiate a real-time monitoring console showing you various messages regarding the webserver and other services.
Open a web browser and navigate to localhost: 8000 (for local installations) or < ^ > your _ domain < ^ >: 8000 (if you are using a remote server).
You will see the ERPNext login screen (we will proceed with login and setup in a later step, once we have made our site production-ready).
After visiting your test deployment, return to your terminal and press CTRL + C.
This will stop ERPNext and exit the monitoring console.
If your main goal is creating modules or modifying ERPNext 12, then you could stop at this point.
No more components are needed for development purposes.
However, if what you need is a production-ready system that does not require a manual initialization, then you will need to install and configure a few additional components.
This is your next step.
Step 8 & mdash; Setting Up ERPNext 12 For Production
Although your ERPNext 12 application is ready, the system as a whole is not yet prepared for production.
To ensure ERPNext's reliability and security you will need to enable a few additional services:
Fail2ban provides an extra layer of protection against brute force attempts from malicious users and bots.
Nginx functions mainly as a web proxy, redirecting all traffic from port 8000 to port 80 (HTTP) or port 443 (HTTPS)
Supervisor ensures that ERPNext's key processes are constantly up and running, restarting them as necessary.
Up to this point, you have installed and configured ERPNext 12 manually, which has allowed you to customize the process to match any particular use case.
Nevertheless, for the rest of the production setup, you can leverage the convenience of the bench CLI and let it automate the installation and configuration of these remaining services.
Ensure you are in the Frappe working directory:
Now use the following command to finish setting up ERPNext 12 for production:
The above will install and configure Nginx, Supervisor, and Fail2Ban and set < ^ > sammy < ^ > as the production environment owner.
The configuration files created by the bench command are:
Two Nginx configuration files located at / etc / nginx / nginx.conf and / etc / nginx / conf.d / < ^ > frappe-bench < ^ > .conf
One Fail2Ban proxy jail located at / etc / fail2ban / jail.d / nginx-proxy.conf and one filter located at / etc / fail2ban / filter.d / nginx-proxy.conf
These default configurations will suffice for this tutorial, but you should feel free to explore and adjust these files to match your requirements.
You can stop all services by running:
And then, once you are ready, you can restart your services:
Now you are ready to test your installation.
Testing Your ERPNext 12 Installation
First of all, verify that key production services are running.
Use the following systemctl command and then pipe it to grep:
After confirming that everything is working as expected, you can test ERPNext 12 live on your server.
Open your favorite browser and navigate < ^ > your _ domain < ^ >, or whereever you are hosting your ERPNext 12 application.
After a few seconds, you should see the ERPNext 12 login screen.
Use Administrator for the username (email) and the < ^ > erpnext _ admin _ password < ^ > you created previously for the password.
ERPNext Login Screen
In the next screen you will see a dropdown menu where you can select the UI language for the application:
Language Selection
Following language selection, ERPNext will prompt you about your country, timezone, and currency:
Select Your Region
Once you complete your region information, you will be able to create your first ERPNext user.
The information you provide will be used as the user's login credentials.
First ERPNext User
In the next screen, you will be asked about what ERPNext calls Domains.
If you are not sure what your domain is, then select Distribution and click the Next button.
Select your Domains
Next, you will need to provide a company name and abbreviation.
Company Name
On the last screen, ERPNext will ask you what your company does, its bank name, the type of charts of accounts, and the fiscal year period.
You will be able to enter additional banks later.
For now, fill in all the fields as you like and then click the Complete Setup button.
Financial Information
Next, you will see a progress bar.
Setting Up ERPNext
Once the setup process completes, the ERPNext 12 main Dashboard will appear.
ERPNext 12 Dashboard
You now have fully installed and configured an ERPNext 12 application.
Now that you have properly installed your ERPNext 12 application, you might want to start implementing the system for your business needs.
A good starting point is clicking the Getting Started button on the ERPNext Dashboard.
ERPNext will then help you configure the platform for all your business and e-commerce needs.
Getting Started
You may also wish to enhance ERPNext's speed.
If that is the case, then you can read about ERPNext performance tuning, which will guide you through best practices and how to debug performance-related issues.
How To Set Up a Remote Desktop with X2Go on Ubuntu 20.04
6064
The author selected Software in the Public Interest (SPI) to receive a donation as part of the Write for DOnations program.
Usually, Linux-based servers don't come with a graphical user interface (GUI) pre-installed.
Whenever you want to run GUI applications on your instance, the typical solution is to employ Virtual Network Computing (VNC).
Unfortunately, VNC solutions can be sluggish and insecure; many also require a lot of manual configuration.
By contrast, X2Go provides a working "cloud desktop," complete with all the advantages of an always-online, remotely-accessible, and easily-scalable computing system with a fast network.
It is also more responsive and more secure than many VNC solutions.
In this tutorial, you'll use X2Go to create an Ubuntu 20.04 XFCE desktop environment that you can access remotely.
This cloud desktop will include the same utilities that you would obtain had you installed Ubuntu 20.04 and the XFCE environment on your personal computer (almost identical to a Xubuntu setup).
The setup described in this tutorial is useful when:
You need access to a Linux-based operating system, complete with a desktop environment, but can't install it on your personal computer.
You use multiple devices in multiple locations and want a consistent work environment with the same tools, look, files, and performance.
Your Internet service provider gives you very little bandwidth, but you need access to tens or hundreds of gigabytes of data.
Long-running jobs make your local computer unavailable for hours or days.
Imagine that you have to compile a large project, which will take 8 hours on your laptop.
You won't be able to watch movies or do anything else very resource-intensive while your project compiles.
But if you run that job on your server, now your computer is free to perform other tasks.
You're working with a team, and it benefits them to have a shared computer that they can access to collaborate on a project.
An Ubuntu 20.04 x64 instance with 2GB of RAM or more.
2GB is minimal, but a server with 4GB or more is ideal if you have memory-hungry applications that you plan to run.
You can use a DigitalOcean Droplet if you like.
A user with sudo privileges and an SSH key.
Follow this guide to get started: Initial Server Setup with Ubuntu 20.04.
Make sure you complete Step 4 and configure your firewall to restrict all connections except for OpenSSH.
Step 1 & mdash; Installing the Desktop Environment on Your Server
With your server up and your firewall configured, you are now ready to install the graphical environment for the X2Go server.
First, update the package manager's information about the latest software available:
In this tutorial, you are installing XFCE as the desktop environment.
XFCE doesn't use graphical effects like compositing, making it more compatible with X2Go and optimizing screen updates.
For reference, the LXDE desktop environment and the MATE desktop environment (with compositing disabled) also work fine, but you'll have to change the command in this tutorial where you install the desktop environment.
For example, instead of sudo apt-get install xubuntu-desktop, you would type sudo apt-get install < ^ > lubuntu < ^ > -desktop to install LXDE.
There are two ways to install XFCE; the Minimal Desktop Environment or the Full Desktop Environment.
The best choice for you will depend on your needs, which we will cover next.
Choose one of the two.
The Full Desktop Environment
Recommended for most use cases.
If you don't want to handpick every component you need and would rather have a default set of packages, like a word processor, web browser, email client, and other accessories pre-installed, you can choose xubuntu-desktop.
Install and configure the Full Desktop Environment.
The Full Desktop Environment is similar to what you would get if you installed Xubuntu from a bootable DVD / USB memory stick to your local PC:
When prompted to choose a display manager, pick lightdm.
Choosing lightdm as display manager
The Minimal Desktop Environment
Alternately, if you want to install a small, core set of packages and then build on top of them by manually adding whatever you need, you can use the xubuntu-core meta-package.
A meta-package doesn't contain a single package; instead, a meta-package includes an entire package collection.
Installing a meta-package saves the user from manually installing numerous components.
Install xfce4 and all of the additional dependencies needed to support it:
You have installed a graphical environment.
Now you will establish a way to view it remotely.
Step 2 & mdash; Installing X2Go on the Server
X2Go comes with two main components: the server, which starts and manages the graphical session on the remote machine, and the client, which you install on your local computer to view and control the remote desktop or application.
In previous versions of Ubuntu (before 18.04), x2goserver wasn't included in the default repositories, so you'd have to follow steps like these to get the software package.
We're leaving the link here, just for reference, in case the package gets dropped in future versions of Ubuntu.
Fortunately, Ubuntu 20.04, codenamed Focal Fossa, includes the package you need in its default repositories, so the installation is faster.
To install X2Go on your server, type the following command:
At this point, your server requires no further setup.
However, keep in mind that if you followed the recommendation of setting up SSH keys in the Initial Server Setup with Ubuntu 20.04, then you will need to have your SSH private key available on every local machine that you intend to use.
If you didn't set up an SSH private key, make sure you choose a strong password.
< $> note Note: Remember that if you run out of RAM, the Linux kernel might abruptly terminate some applications, resulting in lost work.
If you are using a DigitalOcean Droplet and you notice that your programs require more RAM, you can temporarily power off your Droplet and upgrade (resize) to one with more memory.
You have configured your server.
Type exit or close your terminal window.
The rest of the steps will focus on configuring the client on your local machine.
Step 3 & mdash; Installing the X2Go Client Locally
X2Go is ready to use out of the box.
If you're using Windows or Mac OS X on your local machine, you can download the X2Go client software here.
If you're using Debian or Ubuntu you can install the X2Go client with this command on your local machine:
After downloading the software, you are ready to install it. Open the installer and select your preferred language.
Now agree to the license and let the wizard guide you through the remaining steps.
Typically, there shouldn't be any reason to change the pre-filled, default values in these steps.
X2Go works well out of the box, but it is also highly customizable.
If you'd like additional information, visit X2Go's official documentation.
Now that you have installed the desktop client, you can configure its settings and connect to the X2Go server to use your remote XFCE desktop.
Step 4 & mdash; Connecting To the Remote Desktop
When you first open the X2Go client, a window will appear.
If it doesn't, click Session in the top-left menu and then select New session....
X2Go Client Screenshot - Creating a New Session
In the Session name field, enter something to help differentiate between servers.
Using a session name is particularly useful if you plan on connecting to multiple machines.
Enter your server's IP address or a fully qualified domain name (FQDN) in the Host field under Server.
Enter the username you used for your SSH connection in the Login field.
Since you installed XFCE in Step Two, choose XFCE as your Session type.
Finally, because you connect to the server with SSH keys, click the folder icon next to Use RSA / DSA key for ssh connection and browse to your private key.
If you didn't opt to use the more secure SSH keys, leave this empty; the X2Go client will ask for a password each time you log in.
The rest of the default settings will suffice for now, but as you get more familiar with the software, you can fine-tune the client based on your individual preferences.
After pressing the OK button, you can start your graphical session by clicking the white box that includes your session's name on the box's top-right side.
X2Go Main Window - Session List
If you are running OS X on your local machine, OS X might prompt you to install XQuartz, which is required to run X11.
If so, follow the instructions to install it now.
In a few seconds, your remote desktop will appear, and you can start interacting with it.
There are a few useful keyboard shortcuts you can use for a better experience on Windows and Linux-based operating systems.
< $> note Note: These first two options can exhibit buggy behavior on modern Windows editions.
You can still test them at this point, in case later versions of X2Go fix the issues.
If they fail, just avoid using the same keyboard shortcut in the future.
CTRL + ALT + F will toggle full-screen mode on and off.
Working in full-screen mode can feel more like a local desktop experience.
The full-screen mode also helps the remote machine grab keyboard shortcuts instead of your local machine.
CTRL + ALT + M will minimize the remote view, even if you are in full-screen mode.
CTRL + ALT + T will disconnect from the session but leave the GUI running on the server.
It's just a quick way of disconnecting without logging off or closing applications on the server.
The same will happen if you click the window's close button.
Lastly, there are two ways you can end the remote session and close all of the graphical programs running in it. You can log off remotely from XFCE's start menu, or you can click the button marked with a circle and a small line (like a power / standby icon) in the bottom-right corner of the main portion of the X2Go screen.
The first method is cleaner but may leave programs like session-managing software running.
The second method will close everything but may do so forcefully if a process can't exit cleanly.
In either case, be sure to save your work before proceeding.
X2Go Main Window - Terminate Session Button
You have now successfully accessed and configured your remote desktop.
In this tutorial, you used X2Go to create a robust and remote GUI-environment for the Ubuntu operating system.
Now that you are up and running, here are a few ideas about using this desktop:
You could centralize your development work by creating a git repository.
You could install an IDE / code editor like NetBeans or Eclipse.
You could also use Visual Studio Code for remote development via the Remote-SSH Plugin.
You could configure a web server for testing web applications.
You could also enhance your remote desktop with a good backup scheme to preserve your work environment and essential data in case something ever goes wrong.
With DigitalOcean, you can also snapshot your Droplets when you're happy with a particular setup.
This way, you can test risky changes and always come back to a known, working state.
If you'd like to learn more, visit X2Go's official documentation website.
How To Build a Discord Bot with Node.js
6155
Discord is a chat application that allows millions of users across the globe to message and voice chat online in communities called guilds or servers.
Discord also provides an extensive API that developers can use to build powerful Discord bots.
Bots can perform various actions such as sending messages to servers, DM-ing users, moderating servers, and playing audio in voice chats.
This allows developers to craft powerful bots that include advanced, complex features like moderation tools or even games.
For example, the utility bot Dyno serves millions of guilds and contains useful features such as spam protection, a music player, and other utility functions.
Learning how to create Discord bots allows you to implement many possibilities, which thousands of people could interact with every day.
In this tutorial, you will build a Discord bot from scratch, using Node.js and the Discord.js library, which allows users to directly interact with the Discord API.
You'll set up a profile for a Discord bot, get authentication tokens for the bot, and program the bot with the ability to process commands, with arguments, from users.
Any text editor of your choice, such as Visual Studio Code, Atom, Sublime, or Nano.
A free Discord account with a verified email account and a free Discord server you will use to test your Discord bot.
Step 1 - Setting Up a Discord Bot
In this step, you'll use the Discord developers GUI to set up a Discord bot and get the bot's token, which you will pass into your program.
In order to register a bot on the Discord platform, use the Discord application dashboard.
Here developers can create Discord applications including Discord bots.
Image of the Discord application dashboard after first visiting https: / / discord.com / developers / applications
To get started, click New Application.
Discord will ask you to enter a name for your new application.
Then click Create to create the application.
Image of the prompt to create an application, with "Test Node.js Bot" entered as the name of the application
< $> note Note: The name for your application is independent from the name of the bot, and the bot doesn't have to have the same name as the application.
Now open up your application dashboard.
To add a bot to the application, navigate to the Bot tab on the navigation bar to the left.
Image of the bot tab of the application dashboard
Click the Add Bot button to add a bot to the application.
Click the Yes, do it!
button when it prompts you for confirmation.
You will then be on a dashboard containing details of your bot's name, authentication token, and profile picture.
Dashboard containing details of your bot
You can modify your bot's name or profile picture here on the dashboard.
< $> warning Warning: Never share or upload your bot token as it allows anyone to log in to your bot. < $>
Now you need to create an invite that allows you to add the bot Discord guilds where you can test the bot. First, navigate to the OAuth2 tab of the application dashboard.
To create an invite, scroll down and select bot under scopes.
You must also set permissions to control what actions your bot can perform in guilds.
For the purposes of this tutorial, select Administrator, which will give your bot permission to perform nearly all actions in guilds.
Copy the link with the Copy button.
OAuth2 tab, with scope set to "bot" and permissions set to "administator"
Next, add the bot to a server.
Follow the invite link you just created.
You can add the bot to any server you own, or have administrator permissions in, from the drop-down menu.
Page from following the invite link, allowing users to add the bot to servers
Now click Continue.
Ensure you have the tickbox next to Administrator ticked - this will grant the bot administrator permissions.
Then click Authorize.
Discord will ask you to solve a CAPTCHA before the bot joins the server.
You'll now have the Discord bot on the members list in the server you added the bot to under offline.
Members list of a Discord server with the newly created bot under the "offline" section of the members list
You've successfully created a Discord bot and added it to a server.
Next, you will write a program to log in to the bot.
Step 2 - Creating Your Project
In this step, you'll set up the basic coding environment where you will build your bot and log in to the bot programmatically.
First, you need to set up a project folder and necessary project files for the bot.
Create your project folder:
Move into the project folder you just created:
Next, use your text editor to create a file named config.json to store your bot's authentication token:
Then add the following code to the config file, replacing the highlighted text with your bot's authentication token:
Next you'll create a package.json file, which will store details of your project and information about the dependencies you'll use for the project.
You'll create a package.json file by running the following npm command:
npm will ask you for various details about your project.
If you would like guidance on completing these prompts, you can read about them in How To Use Node.js Modules with npm and package.json.
You'll now install the discord.js package that you will use to interact with the Discord API.
You can install discord.js through npm with the following command:
Now you've set up the configuration file and installed the necessary dependency, you're ready to begin building your bot. In a real-world application, a large bot would be split across many files, but for the purposes of this tutorial, the code for your bot will be in one file.
First, create a file named index.js in the < ^ > discord-bot < ^ > folder for the code:
Begin coding the bot by requiring the discord.js dependency and the config file with the bot's token:
Following this, add the next two lines of code:
The first line of code creates a new Discord.Client and assigns it to the constant client.
This client is partly how you will interact with the Discord API and how Discord will notify you of events such as new messages.
The client, in effect, represents the Discord bot.
The second line of code uses the login method on the client to log in to the Discord bot you created, using the token in the config.json file as a password.
The token lets the Discord API know which bot the program is for and that you're authenticated to use the bot.
Now, execute the index.js file using Node:
Your bot's status will change to online in the Discord server you added it to.
Image of the bot online
You've successfully set up a coding environment and created the basic code for logging in to a Discord bot. In the next step you'll handle user commands and get your bot to perform actions, such as sending messages.
Step 3 - Handling Your First User Command
In this step, you will create a bot that can handle user commands.
You will implement your first command ping, which will respond with "pong" and the time taken to respond to the command.
First, you need to detect and receive any messages users send so you can process any commands.
Using the on method on the Discord client, Discord will send you a notification about new events.
The on method takes two arguments: the name of an event to wait for and a function to run every time that event occurs.
With this method you can wait for the event message - this will occur every time a message is sent to a guild where the bot has permission to view messages.
Therefore let's create a function, which runs every time a message is sent, to process commands.
First open your file:
This function, which runs on the message event, takes message as a parameter. message will have the value of a Discord.js message instance, which contains information about the sent message and methods to help the bot respond.
Now add the following line of code to your command-handling function:
This line checks if the author of the message is a bot, and if so, stops processing the command.
This is important as generally you don't want to process, or respond to, bots' messages.
Bots usually don't need to, or want to, be using our bot, so ignoring their messages saves processing power and helps prevent accidental replies.
Now you'll write a command handler.
To accomplish this, it's good to understand the usual format of a Discord command.
Typically, the structure of a Discord command contains three parts in the following order: a prefix, a command name, and (sometimes) command arguments.
An image of a typical Discord command reading "!
add 1 2 "
Prefix: the prefix can be anything, but is typically a piece of punctuation or abstract phrase that wouldn't normally be at the start of a message.
This means that when you include the prefix at the start of the message, the bot will know that the intention for this command is for a bot to process it.
Command name: The name of the command the user wants to use.
This means the bot can support multiple commands with different functionality and allow users to choose between them by supplying a different command name.
Arguments: Sometimes if the command requires or uses extra information from the user, the user can supply arguments after the command name, with each argument separated by a space.
< $> note Note: There is no enforced command structure and bots can process commands how they like, but the structure presented here is an efficient structure that the vast majority of bots use.
To begin creating a command parser that handles this format, add the following lines of code to the message handling function:
You add the first line of code to assign the value "!"
to the constant prefix, which you will use as the bot's prefix.
The second line of code you add checks if the content of the message the bot is processing begins with the prefix you set, and if it doesn't, stops the message from continuing to process.
Now you must convert the rest of the message into a command name and any arguments that may exist in the message.
Add the following highlighted lines:
You use the first line here to remove the prefix from the message content and assign the result to the constant commandBody.
This is necessary as you don't want to include the prefix in the parsed command name.
The second line takes the message with the removed prefix and uses the split method on it, with a space as the separator.
This splits it into an array of sub-strings, making a split wherever there is a space.
This results in an array containing the command name then, if included in the message, any arguments.
You assign this array to the constant args.
The third line removes the first element from the args array (which will be the command name provided), converts it to lowercase, and then assigns it to the constant command.
This allows you to isolate the command name and leave only arguments in the array.
You also use the method toLowerCase as commands are typically case insensitive in Discord bots.
You've completed building a command parser, implementing a required prefix, and getting the command name and any arguments from messages.
You will now implement and create the code for the specific commands.
Add the following code to start implementing the ping command:
This if statement checks if the command name you parsed (assigned to the constant command) matches "ping".
If it does, that indicates the user wants to use the "ping" command.
You will nest the code for the specific command inside the if statement block.
You will repeat this pattern for other commands you want to implement.
Now, you can implement the code for the "ping" command:
This calculates how long the message took to process and the "ping" of the bot.
The second line responds to user's command using the reply method on the message constant.
The reply method pings (which notifies the user and highlights the message for the specified user) the user who invoked the command, followed by the content provided as the first argument to the method.
You provide a template literal containing a message and the calculated ping as the response that the reply method will use.
This concludes implementing the "ping" command.
Run your bot using the following command (in the same folder as index.js):
You can now use the command "!
ping "in any channel the bot can view and message in, resulting in a response.
Image of bot replying in Discord to "!
ping "with" @ T0M, Pong!
This message had a latency of 1128ms. "
You have successfully created a bot that can handle user commands and you have implemented your first command.
In the next step, you will continue developing your bot by implementing a sum command.
Step 4 - Implementing the Sum Command
Now you will extend your program by implementing the "!
sum "command.
The command will take any number of arguments and add them together, before returning the sum of all the arguments to the user.
If your Discord bot is still running, you can stop its process with CTRL + C.
Open your index.js file again:
To begin implementing the "!
sum "command you will use an else-if block.
After checking for the ping command name, it will check if the command name is equal to "sum".
We use an else-if block since only one command will process at a time, so if the program matches the command name "ping", it doesn't have to check for the "sum" command.
Add the following highlighted lines to your file:
You can begin implementing the code for the "sum" command.
The code for the "sum" command will go inside the else-if block you just created.
Now, add the following code:
You use the map method on the arguments list to create a new list by using the parseFloat function on each item in the args array.
This creates a new array (assigned to the constant numArgs) in which all of the items are numbers instead of strings.
This means later you can successfully find the sum of the numbers by adding them together.
The second line uses the reduce method on the constant numArgs providing a function that totals all the elements in the list.
You assign the sum of all the elements in numArgs to the constant sum.
You then use the reply method on the message object to reply to the user's command with a template literal, which contains the sum of all the arguments the user sends to the bot.
This concludes implementing the "sum" command.
Now run the bot using the following command (in the same folder as index.js):
You can now use the "!
sum "command in any channel the bot can view and message in.
Image of bot replying "The sum of all the arguments you provided is 6!"
to "!
sum 1 2 3 ", then replying" The sum of all the arguments you provided is 13! to "!
sum 1.5 1.5 10 "
The following is a completed version of the index.js bot script:
In this step, you have further developed your Discord bot by implementing the sum command.
You have successfully implemented a Discord bot that can handle multiple, different user commands and command arguments.
If you want to expand on your bot, you could possibly implement more commands or try out more parts of the Discord API to craft a powerful Discord bot. You can review the Discord.js documentation or the Discord API documentation to expand your knowledge of the Discord API.
While creating Discord bots, you must always keep in mind the Discord API terms of service, which outlines how developers must use the Discord API.
You could also read this set of guidelines on how to best implement a Discord bot and provides tips on how to design Discord bots.
If you would like learn more about Node.js check out our How To Code in Node.js series.
How To Install an ERPNext Stack on Ubuntu 18.04
6035
In this tutorial you will install and configure an ERPNext stack on one server running Ubuntu 18.04.
One Ubuntu 18.04 server with at least 4 GB of RAM and a non-root sudo user.
You can set up your server and user by following the Ubuntu 18.04 initial server setup guide.
22 / tcp for SSH (if you have not already enabled OpenSSH)
8000 / tcp for development testing before deploying your site
After enabling the firewall, confirm the status of your open ports:
For more information regarding the firewall setup please read our guide How To Set Up a Firewall with UFW on Ubuntu 18.04.
The localectl utility is used by Ubuntu 18.04 and other Linux distributions to control and change system-wide locale and keyboard layout settings before the user logs in, which is exactly what ERPNext 12 requires.
Give your server a few minutes to reboot and then ssh back inside.
Step 3 & mdash; Installing MariaDB 10.4
ERPNext 12 requires MariaDB 10.2 +, but the version included in Ubuntu 18.04's official repository is 10.1, which means that you will need to install a higher version.
For the purposes of this guide, you will use the latest stable release of MariaDB, which, at the time of this writing, is version 10.4.
To install MariaDB 10.4 on Ubuntu 18.04 you will need to add the appropriate signature key and repository.
You can find this information on the MariaDB Foundation's repository wizard.
Visit this URL in your web browser.
Now, under 1. Choose a Distro, click Ubuntu.
A second column titled 2. Choose a Release will appear.
Beneath this title click 18.04 LTS "bionic".
A third column titled 3.Choose a Version will then appear.
Beneath this click 10.4 stable.
A third column titled 4.Choose a Mirror will then appear.
Choose a mirror based on your location, and then MariaDB will populate the appropriate commands for your custom installation.
MariaDB repo wizard
Run the three populated commands, which will properly add the MariaDB repository and key.
Your own commands will look something like this:
Once you have finished adding the repository, install MariaDB:
ERPNext 12 is a Python application and thus it requires the python3-mysqldb library for database management.
Concerning libmysqlclient-dev, mariadb-client, and libmariadbclient18: those packages let users communicate with the MariaDB service. ntpdate and libdate-manip-perl are used by ERPNext for server time synchronization.
Next, add a basic layer of security to the MariaDB server by running the mysql _ secure _ installation script:
Next, you will have to decide on using Unix authentication or not.
Answer Y to accept this authentication method.
When asked about changing the MariaDB root password, answer N. Using the default password along with Unix authentication is the recommended setup for Ubuntu-based systems because the root account is closely related to automated system maintenance tasks.
To overcome this limitation and let a non-root user manage MariaDB you will have to manually create a database named after the user.
This tutorial will use < ^ > sammy < ^ > but you are free to choose your own name:
Now use nano or your favorite text editor to create a MariaDB configuration file called settings.cnf:
Now add ERPNext's configuration template:
Next, create another file called erpnext.cnf:
The first file, / etc / mysql / conf.d / settings.cnf, complements and also overrides a few values included in the default MariaDB configuration located at / etc / mysql / my.cnf.
Keep in mind though that while this template is a great starting point, nothing prevents you from improving MariaDB's performance even further by adjusting these parameters to fit your needs.
The second file, / etc / mysql / mariadb.conf.d / erpnext.cnf, also overrides some values by introducing specific information regarding your database connection.
Remember to replace < ^ > sammy < ^ > and < ^ > mariadb _ password < ^ > with your own credentials:
For detailed information regarding Postfix configuration please read our guide on How To Install and Configure Postfix on Ubuntu 18.04
Next, update pip3 and then install the latest versions of three additional Python modules required by ERPNext:
In fact, at the time of this writing, the official ERPNext easy _ install script uses Node 8. But from a security perspective it's advisable to install a newer version because Node 8 reached its End Of Life (EOL) in 2020 and thus will not receive any more security patches.
For the purpose of this guide, Node.js version 12 LTS will be installed along with the corresponding npm and yarn package managers.
Once you are satisfied you can run the script:
Next, install yarn globally using the included npm package:
Download the appropriate wkhtmltopdf version and package for Ubuntu 18.04 from the project's page:
First, install Redis from the official Ubuntu 18.04 repository:
Up to this point you have installed all the major components needed by ERPNext 12, which include:
Now download ERPNext 12 from its repository using the bench CLI:
& lt; ^ & gt; erpnext _ admin _ password & lt; ^ & gt; is the desired password for ERPNext's Administrator user.
Although ERPNext 12 application is ready, the system as a whole it's not completely prepared for production yet.
Nginx will be mainly used as a web proxy, redirecting all traffic from port 8000 to port 80 (HTTP) or port 443 (HTTPS)
Supervisor this service ensures that ERPNext key processes are constantly up and running, restarting them as necessary.
These default configurations will suffice for this tutorial, but you should feel free to explore and adjust these files to match your own requirements.
First of all, verify that key production services are running by using the following systemctl command and then piping it to grep:
Open your favorite browser and navigate to the domain where you are hosting your ERPNext 12 application.
Use Administrator for the username and the < ^ > erpnext _ admin _ password < ^ > you created previously for the password.
Once you complete your region information, you will be able to create the first ERPNext user.
If that is the case, then you can read ERPNext Performance Tuning, which will guide you through best practices and how to debug performance-related issues.
How To Install TensorFlow on Ubuntu 20.04
6034
An open-source machine learning software library, TensorFlow is used to train neural networks.
Expressed in the form of stateful dataflow graphs, each node in the graph represents the operations performed by neural networks on multi-dimensional arrays.
These multi-dimensional arrays are commonly known as "tensors," hence the name TensorFlow.
In this tutorial, you "ll install TensorFlow in a Python virtual environment with virtualenv.
This approach isolates the TensorFlow installation and gets things up and running quickly.
Once you complete the installation, you "ll validate your installation by importing Tensorflow to ensure you have no errors.
One Ubuntu 20.04 server with at least 4GB of RAM set up by following the Ubuntu 20.04 initial server setup guide, including a sudo non-root user and a firewall.
Python 3.8 or higher and virtualenv installed.
Follow How To Install Python 3 on Ubuntu 20.04 to configure Python and virtualenv.
Step 1 - Creating a Programming Environment
In this step, we "ll create a virtual environment in order to install TensorFlow into it without compromising our other programming projects.
If you already have a clean programming environment set up, feel free to skip this step.
First, create a project directory.
We "ll call it tf-demo for demonstration purposes, but choose a directory name that is meaningful to you:
Navigate to your newly created tf-demo directory:
Then create a new virtual environment called tensorflow-dev, for instance.
Run the following command to create the environment:
This creates a new tensorflow-dev directory which will contain all of the packages that you install while this environment is activated.
It also includes pip and a standalone version of Python.
Now activate your virtual environment:
Once activated, your terminal prompt will reflect that you are in the virtual environment:
At this point you can install TensorFlow in your virtual environment.
Step 2 - Installing TensorFlow
When installing TensorFlow, we want to make sure we are installing and upgrading to the newest version available in PyPi.
Therefore, we "ll be using the following command syntax with pip:
Once you press ENTER, TensorFlow will install, and you should receive output that indicates that the install along with any dependent packages was successful.
< $> note You can deactivate your virtual environment at any time by using the following command:
To reactivate the environment later, navigate to your project directory and run source < ^ > tensorflow-dev < ^ > / bin / activate.
Now that you have installed TensorFlow, let "s make sure the TensorFlow installation works.
Step 3 - Validating Installation
To validate the installation of TensorFlow, we are going to ensure that we can import the TensorFlow package.
The following prompt will appear on your terminal:
This is the prompt for the Python interpreter, and it indicates that it "s ready for you to start entering some Python statements.
First, type this line to import the TensorFlow package and make it available as the local variable tf.
Press ENTER after typing in the line of code:
As long as you have received no errors, you have installed TensorFlow successfully.
If you have received an error, you should ensure that your server is powerful enough to handle TensorFlow.
You may need to resize your server, making sure it has at least 4GB of memory.
In this tutorial, you have installed TensorFlow in a Python virtual environment and validated that TensorFlow works by importing it.
TensorFlow "s programmer" s guide provides a useful resource and reference for TensorFlow development.
You can also explore Kaggle, a competitive environment for practical application of machine learning concepts that pit you against other machine learning, data science, and statistics enthusiasts.
6031
Python threads are a form of parallelism that allow your program to run multiple procedures at once.
Parallelism in Python can also be achieved using multiple processes, but threads are particularly well suited to speeding up applications that involve significant amounts of I / O (input / output).
Example I / O-bound operations include making web requests and reading data from files.
In contrast to I / O-bound operations, CPU-bound operations (like performing math with the Python standard library) will not benefit much from Python threads.
Python 3 includes the ThreadPoolExecutor utility for executing code in a thread.
In this tutorial, we will use ThreadPoolExecutor to make network requests expediently.
We'll define a function well suited for invocation within threads, use ThreadPoolExecutor to execute that function, and process results from those executions.
For this tutorial, we "ll make network requests to check for the existence of Wikipedia pages.
< $> note Note: The fact that I / O-bound operations benefit more from threads than CPU-bound operations is caused by an idiosyncrasy in Python called the, global interpreter lock.
If you'd like, you can learn more about Python's global interpreter lock in the official Python documentation.
To get the most out of this tutorial, it is recommended to have some familiarity with programming in Python and a local Python programming environment with requests installed.
How To Install Python 3 and Set Up a Local Programming Environment on Ubuntu 18.04
To install the requests package into your local Python programming environment, you can run this command:
Step 1 - Defining a Function to Execute in Threads
Let's start by defining a function that we'd like to execute with the help of threads.
Using nano or your preferred text editor / development environment, you can open this file:
For this tutorial, we'll write a function that determines whether or not a Wikipedia page exists:
The get _ wiki _ page _ existence function accepts two arguments: a URL to a Wikipedia page (wiki _ page _ url), and a timeout number of seconds to wait for a response from that URL.
get _ wiki _ page _ existence uses the requests package to make a web request to that URL.
Depending on the status code of the HTTP response, a string is returned that describes whether or not the page exists.
Different status codes represent different outcomes of a HTTP request.
This procedure assumes that a 200 "success" status code means the Wikipedia page exists, and a 404 "not found" status code means the Wikipedia page does not exist.
As described in the Prerequisites section, you'll need the requests package installed to run this function.
Let's try running the function by adding the url and function call following the get _ wiki _ page _ existence function:
Once you've added the code, save and close the file.
If we run this code:
We'll see output like the following:
Calling the get _ wiki _ page _ existence function with a valid Wikipedia page returns a string that confirms the page does, in fact, exist.
< $> warning Warning: In general, it is not safe to share Python objects or state between threads without taking special care to avoid concurrency bugs.
When defining a function to execute in a thread, it is best to define a function that performs a single job and does not share or publish state to other threads. get _ wiki _ page _ existence is an example of such a function.
Step 2 - Using ThreadPoolExecutor to Execute a Function in Threads
Now that we have a function well suited to invocation with threads, we can use ThreadPoolExecutor to perform multiple invocations of that function expediently.
Let's add the following highlighted code to your program in wiki _ page _ function.py:
Let's take a look at how this code works:
concurrent.futures is imported to give us access to ThreadPoolExecutor.
A with statement is used to create a ThreadPoolExecutor instance executor that will promptly clean up threads upon completion.
Four jobs are submitted to the executor: one for each of the URLs in the wiki _ page _ urls list.
Each call to submit returns a Future instance that is stored in the futures list.
The as _ completed function waits for each Future get _ wiki _ page _ existence call to complete so we can print its result.
If we run this program again, with the following command:
This output makes sense: 3 of the URLs are valid Wikipedia pages, and one of them this _ page _ does _ not _ exist is not.
Note that your output may be ordered differently than this output.
The concurrent.futures.as _ completed function in this example returns results as soon as they are available, regardless of what order the jobs were submitted in.
Step 3 - Processing Exceptions From Functions Run in Threads
In the previous step, get _ wiki _ page _ existence successfully returned a value for all of our invocations.
In this step, we'll see that ThreadPoolExecutor can also raise exceptions generated in threaded function invocations.
Let's consider the following example code block:
This code block is nearly identical to the one we used in Step 2, but it has two key differences:
We now pass timeout = 0.00001 to get _ wiki _ page _ existence.
Since the requests package won't be able to complete its web request to Wikipedia in 0.00001 seconds, it will raise a ConnectTimeout exception.
We catch ConnectTimeout exceptions raised by future.result () and print out a string each time we do so.
If we run the program again, we'll see the following output:
You've now seen that if a function call submitted to a ThreadPoolExecutor raises an exception, then that exception can get raised normally by calling Future.result.
Calling Future.result on all your submitted invocations ensures that your program won't miss any exceptions raised from your threaded function.
Step 4 - Comparing Execution Time With and Without Threads
Now let's verify that using ThreadPoolExecutor actually makes your program faster.
First, let's time get _ wiki _ page _ existence if we run it without threads:
In the code example we call our get _ wiki _ page _ existence function with fifty different Wikipedia page URLs one by one.
We use the time.time () function to print out the number of seconds it takes to run our program.
If we run this code again as before, we'll see output like the following:
Entries 2-47 in this output have been omitted for brevity.
The number of seconds printed after Without threads time will be different when you run it on your machine - that's OK, you are just getting a baseline number to compare with a solution that uses ThreadPoolExecutor.
In this case, it was ~ 5.803 seconds.
Let's run the same fifty Wikipedia URLs through get _ wiki _ page _ existence, but this time using ThreadPoolExecutor:
The code is the same code we created in Step 2, only with the addition of some print statements that show us the number of seconds it takes to execute our code.
If we run the program again, we'll see the following:
Again, the number of seconds printed after Threaded time will be different on your computer (as will the order of your output).
You can now compare the execution time for fetching the fifty Wikipedia page URLs with and without threads.
On the machine used in this tutorial, without threads took ~ 5.803 seconds, and with threads took ~ 1.220 seconds.
Our program ran significantly faster with threads.
In this tutorial, you have learned how to use the ThreadPoolExecutor utility in Python 3 to efficiently run code that is I / O bound.
You created a function well suited to invocation within threads, learned how to retrieve both output and exceptions from threaded executions of that function, and observed the performance boost gained by using threads.
From here you can learn more about other concurrency functions offered by the concurrent.futures module.
Understanding Relational Databases
6075
Database management systems (DBMS) are computer programs that allow users to interact with a database.
A DBMS allows users to control access to a database, write data, run queries, and perform any other tasks related to database management.
In order to perform any of these tasks, though, the DBMS must have some kind of underlying model that defines how the data are organized.
The relational model is one approach for organizing data that has found wide use in database software since it was first devised in the late 1960s, so much so that, as of this writing, four of the top five most popular DBMSs are relational.
This conceptual article outlines the history of the relational model, how relational databases organize data, and how they're used today.
History of the Relational Model
Databases are logically modelled clusters of information, or data. Any collection of data is a database, regardless of how or where it is stored.
Even a file cabinet containing payroll information is a database, as is a stack of hospital patient forms, or a company's collection of customer information spread across multiple locations.
Before storing and managing data with computers was common practice, physical databases like these were the only ones available to government and business organizations that needed to store information.
Around the middle of the 20th century, developments in computer science led to machines with more processing power, as well as greater local and external storage capacity.
These advancements led computer scientists to start recognizing the potential these machines had for storing and managing ever larger amounts of data.
However, there weren't any theories for how computers could organize data in meaningful, logical ways.
It's one thing to store unsorted data on a machine, but it's much more complicated to design systems that allow you to add, retrieve, sort, and otherwise manage that data in consistent, practical ways.
The need for a logical framework for storing and organizing data led to a number of proposals for how to harness computers for data management.
One early database model was the hierarchical model, in which data are organized in a tree-like structure, similar to modern-day filesystems.
The following example shows how the layout of part of a hierarchical database used to categorize animals might look:
Example Hierarchical Database: Animal categorization
The hierarchical model was widely implemented in early database management systems, but it also proved to be somewhat inflexible.
In this model, even though individual records can have multiple "children," each record can only have one "parent" in the hierarchy.
Because of this, these earlier hierarchical databases were limited to representing only "one-to-one" and "one-to-many" relationships.
This lack of "many-to-many" relationships could lead to problems when you're working with data points that you'd like to associate with more than one parent.
In the late 1960s, Edgar F. Codd, a computer scientist working at IBM, devised the relational model of database management.
Codd's relational model allowed individual records to be associated with more than one table, thereby enabling "many-to-many" relationships between data points in addition to "one-to-many" relationships.
This provided more flexibility than other existing models when it came to designing database structures, and meant that relational database management systems (RDBMSs) could meet a much wider range of business needs.
Codd proposed a language for managing relational data, known as Alpha, which influenced the development of later database languages.
Two of Codd's colleagues at IBM, Donald Chamberlin and Raymond Boyce, created one such language inspired by Alpha.
They called their language SEQUEL, short for Structured English Query Language, but because of an existing trademark they shortened the name of their language to SQL (referred to more formally as Structured Query Language).
Due to hardware constraints, early relational databases were still prohibitively slow, and it took some time before the technology became widespread.
But by the mid-1980s, Codd's relational model had been implemented in a number of commercial database management products from both IBM and its competitors.
These vendors also followed IBM's lead by developing and implementing their own dialects of SQL.
By 1987, both the American National Standards Institute and the International Organization for Standardization had ratified and published standards for SQL, solidifying its status as the accepted language for managing RDBMSs.
The relational model's wide use across multiple industries led to it becoming recognized as the standard model for data management.
Even with the rise of various NoSQL databases in more recent years, relational databases remain the dominant tools for storing and organizing data.
How Relational Databases Organize Data
Now that you have a general understanding of the relational model's history, let's take a closer look at how the model organizes data.
The most fundamental elements in the relational model are relations, which users and modern RDBMSs recognize as tables.
A relation is a set of tuples, or rows in a table, with each tuple sharing a set of attributes, or columns:
Diagram example of how relations, tuples, and attributes relate to one another
A column is the smallest organizational structure of a relational database, and represents the various facets that define the records in the table.
Hence their more formal name, attributes.
You can think of each tuple as a unique instance of whatever type of people, objects, events, or associations the table holds.
These instances might be things like employees at a company, sales from an online business, or lab test results.
For example, in a table that holds employee records of teachers at a school, the tuples might have attributes like name, subjects, start _ date, and so on.
When creating columns, you specify a data type that dictates what kind of entries are allowed in that column.
RDBMSs often implement their own unique data types, which may not be directly interchangeable with similar data types in other systems.
Some common data types include dates, strings, integers, and Booleans.
In the relational model, each table contains at least one column that can be used to uniquely identify each row, called a primary key.
This is important, because it means that users don't need to know where their data is physically stored on a machine; instead, their DBMS can keep track of each record and return them on an ad hoc basis.
In turn, this means that records have no defined logical order, and users have the ability to return their data in whatever order or through whatever filters they wish.
If you have two tables that you'd like to associate with one another, one way you can do so is with a foreign key.
A foreign key is essentially a copy of one table's (the "parent" table) primary key inserted into a column in another table (the "child ").
The following example highlights the relationship between two tables, one used to record information about employees at a company and another used to track the company's sales.
In this example, the primary key of the EMPLOYEES table is used as the foreign key of the SALES table:
Diagram example of how the EMPLOYEE table's primary key acts as the SALES table's foreign key
If you try to add a record to the child table and the value entered into the foreign key column doesn't exist in the parent table's primary key, the insertion statement will be invalid.
This helps to maintain relationship-level integrity, as the rows in both tables will always be related correctly.
The relational model's structural elements help to keep data stored in an organized way, but storing data is only useful if you can retrieve it. To retrieve information from an RDBMS, you can issue a query, or a structured request for a set of information.
As mentioned previously, most relational databases use SQL to manage and query data. SQL allows you to filter and manipulate query results with a variety of clauses, predicates, and expressions, giving you fine control over what data will appear in the result set.
Advantages and Limitations of Relational Databases
With the underlying organizational structure of relational databases in mind, let's consider some of their advantages and disadvantages.
Today, both SQL and the databases that implement it deviate from Codd's relational model in several ways.
For instance, Codd's model dictates that each row in a table should be unique while, for reasons of practicality, most modern relational databases do allow for duplicate rows.
There are some that don't consider SQL databases to be true relational databases if they fail to adhere to each of Codd's specifications for the relational model.
In practical terms, though, any DBMS that uses SQL and at least somewhat adheres to the relational model is likely to be referred to as a relational database management system.
Although relational databases quickly grew in popularity, a few of the relational model's shortcomings started to become apparent as data became more valuable and businesses began storing more of it. For one thing, it can be difficult to scale a relational database horizontally.
This is often contrasted with vertical scaling which involves upgrading the hardware of an existing server, usually by adding more RAM or CPU.
The reason it's difficult to scale a relational database horizontally has to do with the fact that the relational model is designed to ensure consistency, meaning clients querying the same database will always retrieve the same data. If you were to scale a relational database horizontally across multiple machines, it becomes difficult to ensure consistency since clients may write data to one node but not the others.
There would likely be a delay between the initial write and the time when the other nodes are updated to reflect the changes, resulting in inconsistencies between them.
Another limitation presented by RDBMSs is that the relational model was designed to manage structured data, or data that aligns with a predefined data type or is at least organized in some predetermined way, making it easily sortable and searchable.
With the spread of personal computing and the rise of the internet in the early 1990s, however, unstructured data - such as email messages, photos, videos, etc. - became more common.
None of this is to say that relational databases aren't useful.
Quite the contrary, the relational model is still the dominant framework for data management after over 40 years.
Their prevalence and longevity mean that relational databases are a mature technology, which is itself one of their major advantages.
There are many applications designed to work with the relational model, as well as many career database administrators who are experts when it comes to relational databases.
There's also a wide array of resources available in print and online for those looking to get started with relational databases.
Another advantage of relational databases is that almost every RDBMS supports transactions.
A transaction consists of one or more individual SQL statements performed in sequence as a single unit of work.
Transactions present an all-or-nothing approach, meaning that every SQL statement in the transaction must be valid; otherwise, the entire transaction will fail.
This is very helpful for ensuring data integrity when making changes to multiple rows or tables.
Lastly, relational databases are extremely flexible.
They've been used to build a wide variety of different applications, and continue working efficiently even with very large amounts of data. SQL is also extremely powerful, allowing you to add and change data on the fly, as well as alter the structure of database schemas and tables without impacting existing data.
Thanks to their flexibility and design for data integrity, relational databases are still the primary way data are managed and stored more than fifty years after they were first conceived of.
Even with the rise of various NoSQL databases in recent years, understanding the relational model and how to work with RDBMSs are key for anyone who wants to build applications that harness the power of data.
To learn more about a few popular open-source RDBMSs, we encourage you to check out our comparison of various open-source relational SQL databases.
If you're interested in learning more about databases generally, we encourage you to check out our complete library of database-related content.
How To Develop a Drupal 9 Website on Your Local Machine Using Docker and DDEV
6214
DDEV is an open-source tool that uses Docker to build local development environments for many different PHP frameworks.
Using the power of containerization, DDEV can greatly simplify how you work on multiple projects that use multiple tech stacks and multiple cloud servers.
DDEV includes templates for WordPress, Laravel, Magento, TYPO3, Drupal, and more.
Drupal 9 was released on June 3, 2020 for the Drupal CMS.
Known for its ease of use and a massive library of modules and themes, Drupal is a popular PHP framework for building and maintaining various websites and applications of all sizes.
In this tutorial, you will begin developing a Drupal 9 website on your local machine using DDEV.
This will allow you to build your website first, and then later, when you are ready, deploy your project to a production server.
One local machine running Linux or macOS
For macOS: the Homebrew package manager, which you will use to install DDEV.
To install Homebrew on your local machine, follow Step 3 - Installing and Setting up Homebrew in this Ruby tutorial.
Docker and Docker Compose installed on your local machine.
For Linux: You can install Docker and Docker Compose following these two tutorials: How to Install and Use Docker and How to Install Docker Compose.
Choose your Linux distribution from the list and follow the included instructions.
For macOS: Docker Compose was formerly available as part of Docker Toolbox, but Docker Toolbox is now a legacy solution.
Today, Docker officially recommends that you install Docker Desktop, which includes Docker Compose, Docker Engine, and more.
Follow Docker's official guide to install Docker Desktop on macOS.
For more information you can read the official Docker Desktop getting started guide.
If you previously used Docker Toolbox to install various Docker tools, you can read this official article about the differences between Docker Toolbox and Docker Desktop and how they can coexist.
< $> note Note: It is possible to develop Drupal 9 using DDEV on a remote server, but you will need a solution to access localhost in a web browser.
For personal use, you could also install a GUI on your remote server and access your development site through a web browser inside that interface.
To do this, you could follow our guide on how to install and configure VNC on Ubuntu 20.04.
For an even quicker GUI solution you can follow our guide on how to set up a remote desktop with X2Go on Ubuntu 20.04.
Step 1 & mdash; Installing DDEV
In this step you will install DDEV on your local machine.
Option 1 includes instructions for macOS while Option 2 provides instructions for Linux.
This tutorial was tested on DDEV version 1.15.0.
Option 1 & mdash; Installing DDEV on macOS
DDEV advises that macOS users install their tool using the Homebrew package manager.
Use the following brew command to install the newest stable release:
If you prefer the absolute newest version, you can use brew to install ddev-edge:
If you already have a version of DDEV installed, or if you ever wish to update your version, shut down DDEV and use brew to update your installation:
Once you have installed or updated DDEV, run ddev version to verify your software:
DDEV includes a powerful CLI, or command line interface.
Run ddev to learn about some common commands:
For more information about using the DDEV CLI, visit the official DDEV documentation.
With DDEV installed on your local machine, you are now ready to install Drupal 9 and begin developing a website.
Option 2 & mdash; Installing DDEV on Linux
On a Linux operating system, you can install DDEV using Homebrew for Linux or using the official installation script.
On Ubuntu, begin by updating your list of packages in the apt package manager (you can use apt in Debian, otherwise use the equivalent package manager associated with your Linux distribution):
Now install some prerequisite packages from Ubuntu's official repository:
These packages will allow you to download the DDEV installation script from their official GitHub repository.
Now download the script:
Before running the script, open it in nano or your preferred text editor and inspect its contents:
Once you have reviewed the script's contents and you are satisfied, save and close the file.
Now you are ready to run the installation script.
Use the chmod command to make the script executable:
The installation process might prompt you to confirm some settings or to enter your sudo password.
Once the installation completes, you will have DDEV available on your Linux operating system.
Run ddev version to verify your software:
DDEV is a powerful CLI, or command line interface.
Run ddev without anything else to learn about some common commands:
With DDEV installed on your local machine, you are now ready to deploy Drupal 9 and begin developing a website.
Step 2 & mdash; Deploying a New Drupal 9 Site Using DDEV
With DDEV running, you will now use it to create a Drupal-specific filesystem, install Drupal 9, and then initiate a standard website project.
First, you will create a project root directory and then move inside it. You will run all remaining commands from this location.
This tutorial will use < ^ > d9test < ^ >, but you are free to name your directory something else.
Note, however, that DDEV doesn't handle hyphenated names well.
It is considered a best practice to avoid directory names like < ^ > my-project < ^ > or < ^ > drupal-site-1 < ^ >.
Create your project root directory and navigate inside:
DDEV excels at creating directory trees that match specific CMS platforms.
Use the ddev config command to create a directory structure specific to Drupal 9:
Because you passed --project-type = < ^ > drupal9 < ^ > to your ddev config command, DDEV created several subdirectories and files that represent the default organization for a Drupal website.
Your project directory tree will now look like this:
.ddev / will be the main folder for the ddev configuration. web / will be the docroot for your new project; it will contain several specific settings. files.
You now have the initial scaffolding for your new Drupal project.
Your next step is to initialize your platform, which will build the necessary containers and networking configurations.
DDEV binds to ports 80 and 443, so if you are running a web server like Apache on your machine, or anything else that uses those ports, stop those services before continuing.
Use the ddev start command to initialize your platform:
This will build all the Docker-based containers for your project, which include a web container, a database container, and phpmyadmin.
When the initialization completes you will see an output like this (your port number might differ):
< $> note Note: Remember that DDEV is starting Docker containers behind the scenes here.
If you want to view those containers or verify that they are running, you can always use the docker ps command:
Alongside any other containers that you are currently running, you will find four new containers, each running a different image: php-myadmin, ddev-webserver, ddev-router, and ddev-dbserver-mariadb.
ddev start has successfully built your containers and given you an output with two URLs.
While this output says that your project "can be reached at http: / / < ^ > d9test < ^ > .ddev.site and http: / / 127.0.0.1: < ^ > 32773 < ^ >," visiting these URLs right now will throw an error.
Starting with Drupal 8, the Drupal core and the contrib modules function like dependencies.
Therefore, you'll first need to finish installing Drupal using Composer, the package manager for PHP projects, before anything loads in your web browser.
One of the most useful and elegant features of DDEV is that you can pass Composer commands through the DDEV CLI and into your containerized environment.
This means that you can separate your machine's specific configuration from your development environment.
You no longer have to manage the various file path, dependency, and version issues that generally accompany local PHP development.
Moreover, you can quickly context-switch between multiple projects using different frameworks and tech stacks with minimal effort.
Use the ddev composer command to download drupal / recommended-project.
This will download Drupal core, its libraries, and other related resources and then create a default project:
Now download one final component called Drush, or Drupal Shell.
This tutorial will only use one drush command, and this tutorial provides an alternative, but drush is a powerful CLI for Drupal development that can improve your efficiency.
Use ddev composer to install drush:
You have now built a default Drupal 9 project and installed drush.
Now you will view your project in a browser and configure your website's settings.
Step 3 & mdash; Configuring Your Drupal 9 Project
Now that you have installed Drupal 9 you can visit your new project in your browser.
To do this, you can rerun ddev start and copy one of the two URLs that it outputs, or you can use the following command, which will automatically launch your site in a new browser window:
You will encounter the standard Drupal installation wizard.
Drupal 9 installer from browser
Here you have two options.
You can use this UI and follow the wizard through installation, or you can return to your terminal and pass a drush command through ddev.
The latter option will automate the installation process and set admin as both your username and password.
Option 1 & mdash; Using the Wizard
Return to the wizard in your browser.
Under Choose language select a language from the drop-down menu and click Save and continue.
Now select an installation profile.
You can choose between Standard, Minimal, and Demo.
Make your choice and then click Save and continue.
Drupal will automatically verify your requirements, set up a database, and install your site.
Your last step is to customize a few configurations.
Add a site name and a site email address that ends in your domain.
Then choose a username and password.
Choose a strong password and keep your credentials somewhere safe.
Lastly, add a private email address that you regularly check, fill in the regional settings, and press Save and continue.
Drupal 9 welcome message with a warning about permissions
Your new site will load with a welcome message.
Option 2 & mdash; Using the Command Line
From your project's root directory, run this ddev exec command to install a default Drupal site using drush:
This will create your site just like the wizard will but with some boilerplate configurations.
Your username and password will be admin.
Now launch the site to view it in your browser:
You are now ready to begin building your website, but it is considered best practice to check that your permissions are correct for the / sites / web / default directory.
While you are working locally, this is not a significant concern, but if you transfer these permissions to a production server, they will pose a security risk.
Step 4 & mdash; Checking Your Permissions
During the wizard installation, or when your welcome page first loads, you might see a warning about the permissions settings on your / sites / web / default directory and one file inside that directory: settings.php.
After the installation script runs, Drupal will try to set the web / sites / default directory permissions to read and execute for all groups: this is a 555 permissions setting.
It will also attempt to set permissions for default / settings.php to read-only, or 444. If you encounter this warning, run these two chmod commands from your project's root directory.
Failure to do so poses a security risk:
To verify that you have the correct permissions, run this ls command with the a, l, h, and d switches:
Check that your permissions match the following output:
You are now ready to develop a Drupal 9 website on your local machine.
Step 5 & mdash; Creating Your First Post in Drupal
To test some of Drupal's functionality, you will now create a post using the web UI.
From your site's initial page, click the Content button on the upper menu's left-hand edge.
Now click the blue add content button.
A new page will appear.
Click Article, and another page will appear.
Drupal 9 Create Article Prompt
Add whatever title and content you like.
You can add an image, too, like one of DigitalOcean's wallpapers.
When ready, click the blue save button.
Your first post will appear on your website.
Drupal 9 Created Post
You are now developing a Drupal 9 website on your local machine without ever interacting with a server, thanks to Docker and DDEV.
In the following step, you will manage the DDEV container to accomodate your workflow.
Step 6 & mdash; Managing the DDEV Container
When you have finished developing your project, or when you want to take a break, you can stop your DDEV container without worrying about data loss.
DDEV can manage rapid context-switching among many projects; this is one of its most useful features.
Your code and data are always preserved in your project directory, even after you stop or delete the DDEV container.
To free up resources, you can stop DDEV at any time.
From your project's root directory, run the following command:
DDEV is available globally, so you can run ddev commands from anywhere, as long as you specify the DDEV project:
You can also view all your projects at once using ddev list:
DDEV includes many other useful commands.
You can restart DDEV and continue developing locally at any time.
In this tutorial, you used Docker and the power of containerization to develop a Drupal site locally, with the help of DDEV.
DDEV also integrates well with numerous IDEs, and it provides built-in PHP debugging for Atom, PHPStorm, and Visual Studio Code (vscode).
From here, you can also learn more about creating development environments for Drupal with DDEV or developing other PHP frameworks like Wordpress.
How To Use subprocess to Run External Programs in Python 3
6167
Python 3 includes the subprocess module for running external programs and reading their outputs in your Python code.
You might find subprocess useful if you want to use another program on your computer from within your Python code.
For example, you might want to invoke git from within your Python code to retrieve files in your project that are tracked in git version control.
Since any program you can access on your computer can be controlled by subprocess, the examples shown here will be applicable to any external program you might want to invoke from your Python code.
subprocess includes several classes and functions, but in this tutorial we'll cover one of subprocess's most useful functions: subprocess.run.
We'll review its different uses and main keyword arguments.
Running an External Program
You can use the subprocess.run function to run an external program from your Python code.
First, though, you need to import the subprocess and sys modules into your program:
If you run this, you will receive output like the following:
Let's review this example:
sys.executable is the absolute path to the Python executable that your program was originally invoked with.
For example, sys.executable might be a path like / usr / local / bin / python.
subprocess.run is given a list of strings consisting of the components of the command we are trying to run.
Since the first string we pass is sys.executable, we are instructing subprocess.run to execute a new Python program.
The -c component is a python command line option that allows you to pass a string with an entire Python program to execute.
In our case, we pass a program that prints the string ocean.
You can think of each entry in the list that we pass to subprocess.run as being separated by a space.
For example, [sys.executable, "-c", "print (' ocean ')"] translates roughly to / usr / local / bin / python -c "print (' ocean ')".
Note that subprocess automatically quotes the components of the command before trying to run them on the underlying operating system so that, for example, you can pass a filename that has spaces in it.
< $> warning Warning: Never pass untrusted input to subprocess.run.
Since subprocess.run has the ability to perform arbitrary commands on your computer, malicious actors can use it to manipulate your computer in unexpected ways.
Capturing Output From an External Program
Now that we can invoke an external program using subprocess.run, let's see how we can capture output from that program.
For example, this process could be useful if we wanted to use git ls-files to output all your files currently stored under version control.
< $> note Note: The examples shown in this section require Python 3.7 or higher.
In particular, the capture _ output and text keyword arguments were added in Python 3.7 when it was released in June 2018.
Let's add to our previous example:
This example is largely the same as the one introduced in the first section: we are still running a subprocess to print ocean.
Importantly, however, we pass the capture _ output = True and text = True keyword arguments to subprocess.run.
subprocess.run returns a subprocess.CompletedProcess object that is bound to result.
The subprocess.CompletedProcess object includes details about the external program's exit code and its output. capture _ output = True ensures that result.stdout and result.stderr are filled in with the corresponding output from the external program.
By default, result.stdout and result.stderr are bound as bytes, but the text = True keyword argument instructs Python to instead decode the bytes into strings.
In the output section, stdout is ocean (plus the trailing newline that print adds implicitly), and we have no stderr.
Let's try an example that produces a non-empty value for stderr:
This code runs a Python subprocess that immediately raises a ValueError.
When we inspect the final result, we see nothing in stdout and a Traceback of our ValueError in stderr.
This is because by default Python writes the Traceback of the unhandled exception to stderr.
Raising an Exception on a Bad Exit Code
Sometimes it's useful to raise an exception if a program we run exits with a bad exit code.
Programs that exit with a zero code are considered successful, but programs that exit with a non-zero code are considered to have encountered an error.
As an example, this pattern could be useful if we wanted to raise an exception in the event that we run git ls-files in a directory that wasn't actually a git repository.
We can use the check = True keyword argument to subprocess.run to have an exception raised if the external program returns a non-zero exit code:
This output shows that we ran a subprocess that raised an error, which is printed in stderr in our terminal.
Then subprocess.run dutifully raised a subprocess.CalledProcessError on our behalf in our main Python program.
Alternatively, the subprocess module also includes the subprocess.CompletedProcess.check _ returncode method, which we can invoke for similar effect:
If we run this code, we'll receive:
Since we didn't pass check = True to subprocess.run, we successfully bound a subprocess.CompletedProcess instance to result even though our program exited with a non-zero code.
Calling result.check _ returncode (), however, raises a subprocess.CalledProcessError because it detects the completed process exited with a bad code.
Using timeout to Exit Programs Early
subprocess.run includes the timeout argument to allow you to stop an external program if it is taking too long to execute:
The subprocess we tried to run used the time.sleep function to sleep for 2 seconds.
However, we passed the timeout = 1 keyword argument to subprocess.run to time out our subprocess after 1 second.
This explains why our call to subprocess.run ultimately raised a subprocess.TimeoutExpired exception.
Note that the timeout keyword argument to subprocess.run is approximate.
Python will make a best effort to kill the subprocess after the timeout number of seconds, but it won't necessarily be exact.
Passing Input to Programs
Sometimes programs expect input to be passed to them via stdin.
The input keyword argument to subprocess.run allows you to pass data to the stdin of the subprocess.
We'll receive output like the following after running this code:
In this case, we passed the bytes underwater to input.
Our target subprocess used sys.stdin to read the passed in stdin (underwater) and printed it out in our output.
The input keyword argument can be useful if you want to chain multiple subprocess.run calls together passing the output of one program as the input to another.
The subprocess module is a powerful part of the Python standard library that lets you run external programs and inspect their outputs easily.
In this tutorial, you have learned to use subprocess.run to control external programs, pass input to them, parse their output, and check their return codes.
The subprocess module exposes additional classes and utilities that we did not cover in this tutorial.
Now that you have a baseline, you can use the subprocess module's documentation to learn more about other available classes and utilities.
How To Use the Python Map Function
6188
We can use the Python built-in function map () to apply a function to each item in an iterable (like a list or dictionary) and return a new iterator for retrieving the results. map () returns a map object (an iterator), which we can use in other parts of our program.
We can also pass the map object to the list () function, or another sequence type, to create an iterable.
The syntax for the map () function is as follows:
Instead of using a for loop, the map () function provides a way of applying a function to every item in an iterable.
Therefore it can often be more performant since it is only applying the function one item at a time rather than making copies of the items into another iterable.
This is particularly useful when working on programs processing large data sets. map () can also take multiple iterables as arguments to the function by sending one item from each iterable to the function at a time.
In this tutorial, we'll review three different ways of working with map (): with a lambda function, with a user-defined function, and finally with a built-in function using multiple iterable arguments.
Using a Lambda Function
The first argument to map () is a function, which we use to apply to each item.
Python calls the function once for every item in the iterable we pass into map () and it returns the manipulated item within a map object.
For the first function argument, we can either pass a user-defined function or we can make use of lambda functions, particularly when the expression is less complex.
The syntax of map () with a lambda function is as follows:
With a list like the following, we can implement a lambda function with an expression that we want to apply to each item in our list:
To apply an expression against each of our numbers, we can use map () and lambda:
Here we declare an item in our list as x. Then we add our expression.
We pass in our list of numbers as the iterable for map ().
In order to receive the results of this immediately we print a list of the map object:
We have used list () so that the map object is returned to us as a list, rather than a less human-readable object like: < map object at 0x7fc250003a58 >.
The map object is an iterator over our results, so we could loop over it with for or we can use list () to turn it into a list.
We "re doing this here because it" s a good way to review the results.
Ultimately map () is most useful when working with large datasets, so we would likely work with the map object further, and generally would not be using a constructor like list () on them.
For smaller datasets, list comprehensions may be more suitable, but for the purposes of this tutorial we "re using a small dataset to demonstrate map ().
Implementing a User-defined Function
Similarly to a lambda we can use a function we have defined to apply to an iterable.
While lambda functions are more useful to implement when you're working with a one-line expression, user-defined functions are more appropriate when the expression grows in complexity.
Furthermore, when we need to pass another piece of data to the function that you're applying to your iterable, user-defined functions can be a better choice for readability.
For example, in the following iterable, each item is a dictionary that contains different details about each of our aquarium creatures:
We've decided that all the aquarium creatures are in fact going to move into the same tank.
We need to update our records to reflect that all of our creatures are moving into tank 42. To have map () access each dictionary and each key: value pair in the dictionaries, we construct a nested function:
We define an assign _ to _ tank () function that takes aquarium _ creatures and new _ tank _ number as parameters.
In assign _ to _ tank () we pass apply () as the function to map () on the final line.
The assign _ to _ tank function will return the iterator resulting from map ().
apply () takes x as an argument, which represents an item in our list - a single dictionary.
Next we define that x is the "tank number" key from aquarium _ creatures and that it should store the passed in new _ tank _ number.
We return each item after applying the new tank number.
We call assign _ to _ tank () with our list of dictionaries and the new tank number we want to replace for each creature:
Once the function completes we have our map object stored in the assigned _ tanks variable, which we turn into a list and print:
We'll receive the following output from this program:
We've mapped the new tank number to our list of dictionaries.
Using a function that we define, we can incorporate map () to apply the function efficiently on each item of the list.
Using a Built-in Function with Multiple Iterables
In the same way as lambda functions or our own defined functions, we can use Python built-in functions with map ().
To apply a function with multiple iterables, we pass in another iterable name following the first one.
Here we have our lists of integers that we would like to use with pow ():
Next we pass in pow () as our function into map () and provide the two lists as our iterables:
map () will apply the pow () function to the same item in each list to provide the power.
Therefore our results will show 2 * * 1, 4 * * 2, 6 * * 3, and so on:
If we were to provide map () with an iterable that was longer than the other, map () would stop calculating once it reaches the end of the shortest iterable.
In the following program we "re extending base _ numbers with three additional numbers:
As a result, nothing will change within the calculation of this program and so it will still yield the same result:
We've used the map () function with a Python built-in function and have seen that it can handle multiple iterables.
We've also reviewed that map () will continue to process multiple iterables until it has reached the end of the iterable with the fewest items.
In this tutorial, we've learned the different ways of using the map () function in Python.
Now you can use map () with your own function, a lambda function, and with any other built-in functions.
You can also implement map () with functions that require multiple iterables.
In this tutorial, we printed the results from map () immediately to a list format for demonstration purposes.
In our programs we would typically use the returned map object to further manipulate the data.
How To Build a REST API with Prisma and PostgreSQL
6223
Prisma is an open source database toolkit.
It consists of three main tools:
Prisma Client: An auto-generated and type-safe query builder for Node.js and TypeScript.
Prisma Migrate: A declarative data modeling and migration system.
Prisma Studio: A GUI to view and edit data in your database.
These tools aim to increase an application developer's productivity in their database workflows.
One of the top benefits of Prisma is the level of abstraction it provides: Instead of figuring out complex SQL queries or schema migrations, application developers can reason about their data in a more intuitive way when using Prisma to work with their database.
In this tutorial, you will build a REST API for a small blogging application in TypeScript using Prisma and a PostgreSQL database.
You will set up your PostgreSQL database locally with Docker and implement the REST API routes using Express.
At the end of the tutorial, you will have a web server running locally on your machine that can respond to various HTTP requests and read and write data in the database.
This tutorial assumes the following:
Node.js v10 or higher installed on your machine.
You can use one of the How To Install Node.js and Create a Local Development Environment guides for your OS to set this up.
Docker installed on your machine (to run the PostgreSQL database).
You can install on macOS and Windows via the Docker website, or follow How To Install and Use Docker for Linux distributions.
Basic familiarity with TypeScript and REST APIs is helpful but not required for this tutorial.
Step 1 - Creating Your TypeScript Project
In this step, you will set up a plain TypeScript project using npm.
This project will be the foundation for the REST API you're going to build throughout the course of this tutorial.
First, create a new directory for your project:
Next, navigate into the directory and initialize an empty npm project.
Note that the -y option here means that you're skipping the interactive prompts of the command.
To run through the prompts, remove -y from the command:
For more details on these prompts, you can follow Step 1 in How To Use Node.js Modules with npm and package.json.
You'll receive output similar to the following with the default responses in place:
This command creates a minimal package.json file that you use as the configuration file for your npm project.
You're now ready to configure TypeScript in your project.
Execute the following command for a plain TypeScript setup:
This installs three packages as development dependencies in your project:
typescript: The TypeScript toolchain.
ts-node: A package to run TypeScript applications without prior compilation to JavaScript.
@ types / node: The TypeScript type definitions for Node.js.
The last thing to do is to add a tsconfig.json file to ensure TypeScript is properly configured for the application you're going to build.
First, run the following command to create the file:
Add the following JSON code into the file:
This is a standard and minimal configuration for a TypeScript project.
If you want to learn about the individual properties of the configuration file, you can look them up in the TypeScript documentation.
You've set up your plain TypeScript project using npm.
Next you'll set up your PostgreSQL database with Docker and connect Prisma to it.
Step 2 - Setting Up Prisma with PostgreSQL
In this step, you will install the Prisma CLI, create your initial Prisma schema file, and set up PostgreSQL with Docker and connect Prisma to it. The Prisma schema is the main configuration file for your Prisma setup and contains your database schema.
Start by installing the Prisma CLI with the following command:
As a best practice, it is recommended to install the Prisma CLI locally in your project (as opposed to a global installation).
This helps avoid version conflicts in case you have more than one Prisma project on your machine.
Next, you'll set up your PostgreSQL database using Docker.
Create a new Docker Compose file with the following command:
Now add the following code to the newly created file:
This Docker Compose file configures a PostgreSQL database that can be accessed via port 5432 of the Docker container.
Also note that the database credentials are currently set as sammy (user) and your _ password (password).
Feel free to adjust these credentials to your preferred user and password.
With this setup in place, go ahead and launch the PostgreSQL database server with the following command:
The output of this command will be similar to this:
You can verify that the database server is running with the following command:
This will output something similar to this:
With the database server running, you can now create your Prisma setup.
Run the following command from the Prisma CLI:
Note that as a best practice, you should prefix all invocations of the Prisma CLI with npx.
This ensures your local installation is being used.
After you ran the command, the Prisma CLI created a new folder called prisma in your project.
It contains the following two files:
schema.prisma: The main configuration file for your Prisma project (will include your data model).
.env: A dotenv file to define your database connection URL.
To make sure Prisma knows about the location of your database, open the .env file and adjust the DATABASE _ URL environment variable.
First open the .env file:
Now you can set the environment variable as follows:
Make sure to change the database credentials to the ones you specified in the Docker Compose file.
To learn more about the format of the connection URL, visit the Prisma docs.
In this step, you set up your PostgreSQL database with Docker, installed the Prisma CLI, and connected Prisma to the database via an environment variable.
In the next section, you'll define your data model and create your database tables.
Step 3 - Defining Your Data Model and Creating Database Tables
In this step, you will define your data model in the Prisma schema file.
This data model will then be mapped to the database with Prisma Migrate, which will generate and send the SQL statements for creating the tables that correspond to your data model.
Since you're building a blogging application, the main entities of the application will be users and posts.
Prisma uses its own data modeling language to define the shape of your application data.
First, open your schema.prisma file with the following command:
Now, add the following model definitions to it. You can place the models at the bottom of the file, right after the generator client block:
You are defining two models, called User and Post.
Each of these has a number of fields that represent the properties of the model.
The models will be mapped to database tables; the fields represent the individual columns.
Also note that there's a one-to-many relation between the two models, specified by the posts and author relation fields on User and Post.
This means that one user can be associated with many posts.
With these models in place, you can now create the corresponding tables in the database using Prisma Migrate.
In your terminal run the following command:
This command creates a new migration on your filesystem.
Here's a quick overview of the three options that are provided to the command:
--experimental: Required because Prisma Migrate is currently in an experimental state.
--create-db: Enables Prisma Migrate to create the database named my-blog that's specified in the connection URL.
--name "init ": Specifies the name of the migration (will be used to name the migration folder that's created on your filesystem).
Feel free to explore the migration files that have been created in the prisma / migrations directory.
To run the migration against your database and create the tables for your Prisma models, run the following command in your terminal:
Prisma Migrate now generates the SQL statements that are required for the migration and sends them to the database.
The following are the SQL statements that created the tables:
In this step, you defined your data model in your Prisma schema and created the respective databases tables with Prisma Migrate.
In the next step, you'll install Prisma Client in your project so that you can query the database.
Step 4 - Exploring Prisma Client Queries in a Plain Script
Prisma Client is an auto-generated and type-safe query builder that you can use to programmatically read and write data in a database from a Node.js or TypeScript application.
You will use it for database access within your REST API routes, replacing traditional ORMs, plain SQL queries, custom data access layers, or any other method of talking to a database.
In this step, you will install Prisma Client and get familiar with the queries you can send with it. Before implementing the routes for your REST API in the next steps, you will first explore some of the Prisma Client queries in a plain, executable script.
First, go ahead and install Prisma Client in your project by opening up your terminal and installing the Prisma Client npm package:
Next, create a new directory called src that will contain your source files:
Now create a TypeScript file inside of the new directory:
All of the Prisma Client queries return promises that you can await in your code.
This requires you to send the queries inside of an async function.
Add the following boilerplate with an async function that's executed in your script:
Here's a quick breakdown of the boilerplate:
You import the PrismaClient constructor from the previously installed @ prisma / client npm package.
You instantiate PrismaClient by calling the constructor and obtain an instance called prisma.
You define an async function called main where you'll add your Prisma Client queries next.
You call the main function, while catching any potential exceptions and ensuring Prisma Client closes any open database connections by calling prisma.disconnect ().
With the main function in place, you can start adding Prisma Client queries to the script.
Adjust index.ts to look as follows:
In this code, you're using two Prisma Client queries:
create: Creates a new User record.
Notice that you're actually using a nested write, meaning you're creating both a User and Post record in the same query.
findMany: Reads all existing User records from the database.
You're providing the include option that additionally loads the related Post records for each User record.
Now run the script with the following command:
You will receive the following output in your terminal:
Note: If you are using a database GUI you can validate that the data was created by looking at the User and Post tables.
Alternatively, you can explore the data in Prisma Studio by running npx prisma studio --experimental.
You've now used Prisma Client to read and write data in your database.
In the remaining steps, you'll apply that new knowledge to implement the routes for a sample REST API.
Step 5 - Implementing Your First REST API Route
In this step, you will install Express in your application.
Express is a popular web framework for Node.js that you will use to implement your REST API routes in this project.
The first route you will implement will allow you to fetch all users from the API using a GET request.
The user data will be retrieved from the database using Prisma Client.
Go ahead and install Express with the following command:
Since you're using TypeScript, you'll also want to install the respective types as development dependencies.
With the dependencies in place, you can set up your Express application.
Start by opening your main source file again:
Now delete all the code in index.ts and replace it with the following to start your REST API:
Here's a quick breakdown of the code:
You import PrismaClient and express from the respective npm packages.
You create your Express app by calling express ().
You add the express.json () middleware to ensure JSON data can be processed properly by Express.
You start the server on port 3000.
Now you can implement your first route.
Between the calls to app.use and app.listen, add the following code:
Once added, save and exit your file.
Then start your local web server using the following command:
To access the / users route you can point your browser to http: / / localhost: 3000 / users or any other HTTP client.
In this tutorial, you will test all REST API routes using curl, a terminal-based HTTP client.
Note: If you prefer to use a GUI-based HTTP client, you can use alternatives like Postwoman or the Advanced REST Client.
To test your route, open up a new terminal window or tab (so that your local web server can keep running) and execute the following command:
You will receive the User data that you created in the previous step:
Note that the posts array is not included this time.
This is because you're not passing the include option to the findMany call in the implementation of the / users route.
You've implemented your first REST API route at / users.
In the next step you will implement the remaining REST API routes to add more functionality to your API.
Step 6 - Implementing the Remaining REST API Routes
In this step, you will implement the remaining REST API routes for your blogging application.
At the end, your web server will serve various GET, POST, PUT, and DELETE requests.
Here is an overview of the different routes you will implement:
HTTP Method
Route
GET
/ feed
Fetches all published posts.
/ post /: id
Fetches a specific post by its ID.
POST
/ user
Creates a new user.
/ post
Creates a new post (as a draft).
PUT
/ post / publish /: id
Sets the published field of a post to true.
DELETE
post /: id
Deletes a post by its ID.
Go ahead and implement the remaining GET routes first.
Open up the index.ts with the following command:
Next, add the following code following the implementation of the / users route:
This code implements the API routes for two GET requests:
/ feed: Returns a list of published posts.
/ post /: id: Returns a specific post by its ID.
Prisma Client is used in both implementations.
In the / feed route implementation, the query you send with Prisma Client filters for all Post records where the published column contains the value true.
Additionally, the Prisma Client query uses include to also fetch the related author information for each returned post.
In the / post /: id route implementation, you are passing the ID that is retrieved from the URL's path in order to read a specific Post record from the database.
You can stop the server hitting CTRL + C on your keyboard.
Then, restart the server using:
To test the / feed route, you can use the following curl command:
Since no posts have been published yet, the response is an empty array:
To test the / post /: id route, you can use the following curl command:
This will return the post you initially created:
Next, implement the two POST routes.
Add the following code to index.ts following the implementations of the three GET routes:
This code implements the API routes for two POST requests:
/ user: Creates a new user in the database.
/ post: Creates a new post in the database.
Like before, Prisma Client is used in both implementations.
In the / user route implementation, you're passing in the values from the body of the HTTP request to the Prisma Client create query.
The / post route is a bit more involved: Here you can't directly pass in the values from the body of the HTTP request; instead you first need to manually extract them to pass them to the Prisma Client query.
The reason for this is that the structure of the JSON in the request body does not match the structure that's expected by Prisma Client, so you need to manually create the expected structure.
You can test the new routes by stopping the server with CTRL + C.
To create a new user via the / user route, you can send the following POST request with curl:
This will create a new user in the database, printing the following output:
To create a new post via the / post route, you can send the following POST request with curl:
This will create a new post in the database and connect it to the user with the email bob @ prisma.io.
Finally, you can implement the PUT and DELETE routes.
Next, following the implementation of the two POST routes, add the highlighted code:
This code implements the API routes for one PUT and one DELETE request:
/ post / publish /: id (PUT): Publishes a post by its ID.
/ post /: id (DELETE): Deletes a post by its ID.
Again, Prisma Client is used in both implementations.
In the / post / publish /: id route implementation, the ID of the post to be published is retrieved from the URL and passed to the update query of Prisma Client.
The implementation of the / post /: id route to delete a post in the database also retrieves the post ID from the URL and passes it to the delete query of Prisma Client.
Again, stop the server with CTRL + C on your keyboard.
You can test the PUT route with the following curl command:
This is going to publish the post with an ID value of 2. If you resend the / feed request, this post will now be included in the response.
Finally, you can test the DELETE route with the following curl command:
This is going to delete the post with an ID value of 1. To validate that the post with this ID has been deleted, you can resend a GET request to the / post / 1 route.
In this step, you implemented the remaining REST API routes for your blogging application.
The API now responds to various GET, POST, PUT, and DELETE requests and implements functionality to read and write data in the database.
In this article, you created a REST API server with a number of different routes to create, read, update, and delete user and post data for a sample blogging application.
Inside of the API routes, you are using the Prisma Client to send the respective queries to your database.
As next steps, you can implement additional API routes or extend your database schema using Prisma Migrate.
Be sure to visit the Prisma documentation to learn about different aspects of Prisma and explore some ready-to-run example projects in the prisma-examples repository - using tools such as GraphQL or grPC APIs.
How To Use .map () to Iterate Through Array Items in JavaScript
One of the most popular methods of iterating through datasets in JavaScript is the .map () method. .map () creates an array from calling a specific function on each item in the parent array. .map () is a non-mutating method that creates a new array instead of changing the original.
In this tutorial, we'll look at four noteworthy uses of .map () in JavaScript: calling a function of array elements, converting strings to arrays, rendering lists in libraries, and reformatting array objects.
3609
From the classic forloop to the forEach () method, various techniques and methods are used to iterate through datasets in JavaScript.
One of the most popular methods is the .map () method. .map () creates an array from calling a specific function on each item in the parent array. .map () is a non-mutating method that creates a new array as opposed to mutating methods, which only make changes to the calling array.
This method can have many uses when working with arrays.
In this tutorial, you'll look at four noteworthy uses of .map () in JavaScript: calling a function of array elements, converting strings to arrays, rendering lists in JavaScript libraries, and reformatting array objects.
This tutorial does not require any coding, but if you are interested in following along with the examples, you can either use the Node.js REPL or browser developer tools.
To install Node.js locally, you can follow the steps at How to Install Node.js and Create a Local Development Environment.
Chrome DevTools are available by downloading and installing the latest version of Google Chrome.
Step 1 - Calling a Function on Each Item in an Array
.map () accepts a callback function as one of its arguments, and an important parameter of that function is the current value of the item being processed by the function.
This is a required parameter.
With this parameter, you can modify each item in an array and create a new function.
Here's an example:
This output is logged to the console:
This can be simplified further to make it cleaner with:
The same output is logged to the console:
Having code like sweetArray.map (makeSweeter) makes your code a bit more readable.
Step 2 - Converting a String to an Array
.map () is known to belong to the array prototype.
In this step you will use it to convert a string to an array.
You are not developing the method to work for strings here.
Rather, you will use the special .call () method.
Everything in JavaScript is an object, and methods are functions attached to these objects. .call () allows you to use the context of one object on another.
Therefore, you would be copying the context of .map () in an array over to a string.
.call () can be passed arguments of the context to be used and parameters for the arguments of the original function.
Here, you used the context of .map () on a string and passed an argument of the function that .map () expects.
This functions like the .split () method of a string, only that each individual string characters can be modified before being returned in an array.
Step 3 - Rendering Lists in JavaScript Libraries
JavaScript libraries like React use .map () to render items in a list.
This requires JSX syntax, however, as the .map () method is wrapped in JSX syntax.
Here's an example of a React component:
This is a stateless component in React, which renders a div with a list.
The individual list items are rendered using .map () to iterate over the names array initially created.
This component is rendered using ReactDOM on the DOM element with Id of root.
Step 4 - Reformatting Array Objects
.map () can be used to iterate through objects in an array and, in a similar fashion to traditional arrays, modify the content of each individual object and return a new array.
This modification is done based on what is returned in the callback function.
Here, you modified each object in the array using the bracket and dot notation.
This use case can be employed to process or condense received data before being saved or parsed on a frontend application.
In this tutorial, we looked at four uses of the .map () method in JavaScript.
In combination with other methods, the functionality of .map () can be extended.
For more information, see our How To Use Array Methods in JavaScript: Iteration Methods article.
How To Use EJS to Template Your Node Application
When creating quick on-the-fly Node applications, an easy and fast way to template our application is sometimes necessary.
So far we "ve been building out full...
6453
Jade comes as the view engine for Express by default but Jade syntax can be overly complex for many use cases.
EJS is one alternative does that job well and is very easy to set up.
Let's take a look at how we can create a simple application and use EJS to include repeatable parts of our site (partials) and pass data to our views.
Setting up the Demo App
We will be making two pages for our application with one page with full width and the other with a sidebar.
Get the code: You can find a git repo of the complete demo code on GitHub here
File Structure
Here are the files we'll need for our application.
We'll do our templating inside of the views folder and the rest is pretty standard Node practices.
package.json will hold our Node application information and the dependencies we need (express and EJS). server.js will hold our Express server setup, configuration.
We'll define our routes to our pages here.
Node Setup
Let's go into our package.json file and set up our project there.
All we will need is Express and EJS.
Now we have to install the dependencies we just defined.
Go ahead and run:
With all of our dependencies installed, let's configure our application to use EJS and set up our routes for the two pages we need: the index page (full width) and the about page (sidebar).
We will do all of this inside our server.js file.
Here we define our application and set it to show on port 8080.
We also have to set EJS as the view engine for our Express application using app.set (' view engine ',' ejs ');.
Notice how we send a view to the user by using res.render ().
It is important to note that res.render () will look in a views folder for the view.
So we only have to define pages / index since the full path is views / pages / index.
Start Up our Server
Go ahead and start the server using:
Now we can see our application in the browser at http: / / localhost: 8080 and http: / / localhost: 8080 / about.
Our application is set up and we have to define our view files and see how EJS works there.
Create the EJS Partials
Like a lot of the applications we build, there will be a lot of code that is reused.
We'll call those partials and define three files we'll use across all of our site: head.ejs, header.ejs, and footer.ejs.
Let's make those files now.
Add the EJS Partials to Views
We have our partials defined now.
All we have to do is include them in our views.
Let's go into index.ejs and about.ejs and use the include syntax to add the partials.
Syntax for including an EJS Partial
Use <% < ^ > - < ^ > include (< ^ > 'RELATIVE / PATH / TO / FILE' < ^ >)% > to embed an EJS partial in another file.
The hyphen < ^ > <% - < ^ > instead of just <% to tell EJS to render raw HTML.
The path to the partial is relative to the current file.
Now we can see our defined view in the browser at http: / / localhost: 8080. node-ejs-templating-index
For the about page, we also add a bootstrap sidebar to demonstrate how partials can be structured to reuse across different templates and pages.
If we visit http: / / localhost: 8080 / about, we can see our about page with a sidebar!
node-ejs-templating-about
Now we can start using EJS for passing data from our Node application to our views.
Pass Data to Views and Partials
Let's define some basic variables and a list to pass to our home page.
Go back into your server.js file and add the following inside your app.get (' / ') route.
We have created a list called mascots and a simple string called tagline.
Let's go into our index.ejs file and use them.
Render a Single Variable in EJS
To echo a single variable, we just use <% = tagline% >.
Let's add this to our index.ejs file:
Loop Over Data in EJS
To loop over our data, we will use .forEach.
Let's add this to our view file:
Now we can see in our browser the new information we have added!
node-ejs-templating-rendered
Pass Data to a Partial in EJS
The EJS partial has access to all the same data as the parent view.
But be careful: If you are referencing a variable in a partial, < ^ > it needs to be defined in every view that uses the partial < ^ > or it will throw an error.
You can also define and pass variables to an EJS partial in the include syntax like this:
But you need to again be careful about assuming a variable has been defined.
If you want to reference a variable in a partial that may not always be defined, and give it a default value, you can do so like this:
In the line above, the EJS code is rendering the value of variant if it's defined, and default if not.
EJS lets us spin up quick applications when we don't need anything too complex.
By using partials and having the ability to easily pass variables to our views, we can build some great applications quickly.
For more reference on EJS see the official docs here.
How To Set Up a Node Project With Typescript
Writing server-side JavaScript can be challenging as a codebase grows.
TypeScript is a typed (optional) super-set of JavaScript that can help with building and managing large-scale JavaScript projects.
It can be thought of as JavaScript with additional features like strong static typing, compilation and object oriented programming.
This tutorial will explore how to use the Express framework with TypeScript.
3569
Node is a run-time environment that makes it possible to write server-side JavaScript.
It has gained widespread adoption since its release in 2011.
Writing server-side JavaScript can be challenging as a codebase grows due to the nature of the JavaScript language; dynamic and weak typed.
Developers coming to JavaScript from other languages often complain about its lack of strong static typing, but this is where TypeScript comes into the picture, to bridge this gap.
Note: TypeScript is technically a super-set of JavaScript, which means that all JavaScript code is valid TypeScript code.
Here are some benefits of using TypeScript:
Optional static typing.
Type inference.
Ability to use Interfaces.
In this tutorial you will set up a Node project with TypeScript.
You will build an Express application using TypeScript and transpile it down to neat and reliable JavaScript code.
Before you begin this guide, you will need Node.js installed on your machine.
You can accomplish this by following the How to Install Node.js and Create a Local Development Environment guide for your operating system.
Step 1 - Initializing an npm Project
To get started, create a new folder named < ^ > node _ project < ^ > and move into that directory.
Next, initialize it as an npm project:
After running npm init, you will need to supply npm with information about your project.
If you'd rather let npm assume sensible defaults, then you can add the y flag to skip the prompts for additional information:
Now that your project space is set up, you are ready to move on to install the necessary dependencies.
Step 2 - Installing Dependencies
With a bare npm project initialized, the next step is to install the dependencies that are required to run TypeScript.
Run the following commands from your project directory to install the dependencies:
The -D flag is the shortcut for: --save-dev.
You can learn more about this flag in the npmjs documentation.
Now, it is time to install the Express framework:
The second command installs the Express types for TypeScript support.
Types in TypeScript are files, normally with an extension of .d.ts.
The files are used to provide type information about an API, in this case the Express framework.
This package is required because TypeScript and Express are independent packages.
Without the @ types / express package, there is no way for TypeScript to know about the types of Express classes.
Step 3 - Configuring TypeScript
In this section, you will setup TypeScript and configure linting for TypeScript.
TypeScript uses a file called tsconfig.json to configure the compiler options for a project.
Create a tsconfig.json file in the root of the project directory and paste in the following snippet:
Let's go over some of the keys in the JSON snippet above:
module: Specifies the module code generation method.
Node uses commonjs.
target: Specifies the output language level.
moduleResolution: This helps the compiler figure out what an import refers to.
The value node mimics the Node module resolution mechanism.
outDir: This is the location to output .js files after transpilation.
In this tutorial you will save it as dist.
An alternative to manually creating and populating the tsconfig.json file is by running the following command:
This command will generate a nicely commented tsconfig.json file.
To learn more about the key value options available, the official TypeScript documentation offers explanations of every option.
Now you can configure TypeScript linting for the project.
In a terminal running in the root of your project's directory, which this tutorial established as < ^ > node _ project < ^ >, run the following command to generate a tslint.json file:
Open the newly generated tslint.json file and add the no-console rule accordingly:
By default, the TypeScript linter prevents the use of debugging using console statements, hence the need to explicitly tell the linter to revoke the default no-console rule.
Step 4 - Updating the package.json File
At this point in the tutorial, you can either run functions in the terminal individually, or create an npm script to run them.
In this step you will make a start script that will compile and transpile the TypeScript code, and then runs the resulting .js application.
Open the package.json file and update it accordingly:
In the snippet above, you updated the main path and added the start command to the scripts section.
When looking at the start command, you'll see that first the tsc command is run, and then the node command.
This will compile and then run the generated output with node.
The tsc command tells TypeScript to compile the application and place the generated .js output in the specified outDir directory as it is set in the tsconfig.json file.
Step 5 - Creating and Running a Basic Express Server
Now that TypeScript and its linter are configured, it is time to build a Node Express Server.
First, create a src folder in the root of your project directory:
Then create a file named app.ts within it:
At this point, the folder structure should look like this:
Open up the app.ts file with a text editor of your choice and paste in the following code snippet:
The code above creates Node Server that listens on the port 3000 for requests.
Run the app using the following command:
If it runs successfully, a message will be logged to the terminal:
Now, you can visit http: / / localhost: 3000 in your browser and you should see the message:
Browser window with the message: The sedulous hyena ate the antelope!
Open the dist / app.js file and you will find the transpiled version of the TypeScript code:
At this point you have successfully set up your Node project to use TypeScript.
In this tutorial, you learned about why TypeScript is useful for writing reliable JavaScript code.
You also learned about some of benefits to working with TypeScript.
Finally, you set up a Node project using the Express framework, but compiled and ran the project using TypeScript.
