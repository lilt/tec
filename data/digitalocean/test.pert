Verwenden von Routing mit React Navigation in React Native
React Navigation ist eine beliebte Bibliothek für das Routing und die Navigation in einer React Native-Anwendung.
In diesem Tutorial erstellen Sie eine Social-Media-Anwendung, um zu erkunden, wie Sie mit react-navigation mobile App-Bildschirme navigieren können.
4936
Einführung
Diese Bibliothek hilft bei der Navigation zwischen mehreren Bildschirmen und dem Austausch von Daten zwischen ihnen.
Am Ende dieses Tutorials haben Sie ein rudimentäres soziales Netzwerk.
Es zeigt die Anzahl der Freundschaften eines Benutzers an und bietet eine Möglichkeit, sich mit zusätzlichen Freunden zu verbinden.
Sie verwenden diese Beispielanwendung zur Erkundung, wie Sie mobile App-Bildschirme mit react-navigation navigieren können.
Voraussetzungen
Um dieses Tutorial zu absolvieren, benötigen Sie:
Eine lokale Entwicklungsumgebung für Node.js.
Folgen Sie Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung.
Vertrautheit mit der Einrichtung Ihrer Umgebung zur Erstellung eines neuen React Native-Projekts und der Verwendung der iOS- oder Android-Simulatoren können von Vorteil sein.
< $> note
Hinweis: Wenn Sie in der Vergangenheit mit react-navigation gearbeitet haben, können Sie ggf. auf einige Unterschiede stoßen.
Sie können die Dokumentation für Leitfäden zur Migration von 3.x und Migration von 4.x konsultieren.
< $>
Dieses Tutorial wurde mit Node v14.7.0, npm v6.14.7, react v16.13.1, react-native v0.63.2, @ react-navigation / native v5.7.3 und @ react-navigation / stack v5.9.0 verifiziert.
Schritt 1 - Erstellen einer neuen React Native-Anwendung
Erstellen Sie zunächst eine neue React Native-Anwendung, indem Sie den folgenden Befehl in Ihrem Terminal eingeben:
Navigieren Sie dann zum neuen Verzeichnis:
Und starten Sie die Anwendung für iOS:
Alternativ für Android:
Hinweis: Wenn Probleme auftreten, müssen Sie möglicherweise Probleme bei der Fehlerbehebung für die React Native-CLI konsultieren.
Dadurch wird ein grundlegendes Projekt für Sie erstellt.
Es sieht noch nicht wirklich wie ein soziales Netzwerk aus.
Wir werden das beheben.
Öffnen Sie App.js:
Ersetzen Sie den Inhalt von App.js durch den folgenden Code, um eine Willkommensnachricht anzuzeigen:
Speichern Sie die Datei.
Wenn Sie nun die Anwendung ausführen, wird die Nachricht Willkommen bei MySocialNetwork!
in Ihrem Simulator angezeigt.
Im nächsten Schritt fügen Sie weitere Bildschirme zu Ihrer Anwendung hinzu.
Schritt 2 - Erstellen eines HomeScreen und eines FriendsScreen
Derzeit haben Sie einen einzigen Bildschirm, der eine Willkommensnachricht anzeigt.
In diesem Schritt erstellen Sie die folgenden beiden Bildschirme für Ihre Anwendung: HomeScreen und FriendsScreen.
HomeScreen
Ihre Anwendung wird einen HomeScreen benötigen.
Der HomeScreen zeigt die Anzahl der Freunde an, die bereits in Ihrem Netzwerk sind.
Nehmen Sie den Code von App.js und fügen Sie ihn zu einer neuen Datei namens HomeScreen.js hinzu.
Öffnen Sie HomeScreen.js:
Ändern Sie HomeScreen.js, um HomeScreen anstelle von App zu verwenden.
Dieser Code erstellt eine Platzhalternachricht für Du hast (undefiniert) Freunde..
Sie werden später einen Wert angeben.
FriendsScreen
Ihre Anwendung wird auch einen FriendsScreen benötigen.
Auf dem FriendsScreen können Sie neue Freunde hinzufügen.
Nehmen Sie den Code von App.js und fügen Sie ihn zu einer neuen Datei namens FriendsScreen.js hinzu.
Öffnen Sie FriendsScreen.js:
Ändern Sie FriendsScreen.js, um FriendsScreen anstelle von App zu verwenden.
Dieser Code erstellt eine Nachricht für Hier Freunde hinzufügen!

Zu diesem Zeitpunkt haben Sie einen HomeScreen und einen FriendsScreen.
Es gibt jedoch keine Möglichkeit, zwischen ihnen zu navigieren.
Sie werden diese Funktionalität im nächsten Schritt erstellen.
Schritt 3 - Verwenden von StackNavigator mit React Navigation
Für die Navigation zwischen Bildschirmen verwenden Sie einen StackNavigator.
Ein StackNavigator funktioniert genau wie ein Call Stack.
Jeder Bildschirm, zu dem Sie navigieren, wird nach oben auf den Stack gelegt.
Jedes Mal, wenn Sie auf die Back-Taste drücken, verschwinden die Bildschirme vom oberen Teil des Stacks.
Installieren Sie zunächst @ react-navigation / native:
Installieren Sie dann @ react-navigation / stack und seine Peer-Abhängigkeiten:
Hinweis: Wenn Sie für iOS entwickeln, müssen Sie möglicherweise zum Verzeichnis ios navigieren und pod install ausführen.
Als Nächstes kehren Sie zu App.js zurück:
Fügen Sie NavigationContainer und createStackNavigator zu App.js hinzu.
Ersetzen Sie dann den Inhalt von render:
Fügen Sie in < Stack.Navigator > verschachtelt den HomeScreen hinzu:
Dieser Code erstellt einen sehr kleinen Stack für Ihren Navigator mit nur einem Bildschirm: HomeScreen.
Fügen Sie in < Stack.Navigator > den FriendsScreen hinzu:
Dieser Code fügt den FriendsScreen zum Navigator hinzu.
Hinweis: Dies unterscheidet sich von der Art und Weise, wie createStackNavigator in vorherigen Versionen von React Navigation verwendet wurde.
Nun ist sich der Navigator der beiden Bildschirme bewusst.
Hinzufügen von Schaltflächen zu HomeScreen und FriendsScreen
Fügen Sie abschließend Schaltflächen hinzu, um zwischen Ihren beiden Bildschirmen wechseln zu können.
Fügen Sie in HomeScreen.js den folgenden Code hinzu:
Fügen Sie in FriendsScreen.js den folgenden Code hinzu:
Sprechen wir über this.props.navigation.
Solange Ihr Bildschirm im StackNavigator enthalten ist, erbt er automatisch viele nützliche Props aus dem Objekt navigation.
In diesem Fall haben Sie navigate verwendet, um zu einer anderen Seite zu wechseln.
HomeScreen und FriendsScreen
Wenn Sie nun Ihren Simulator öffnen, können Sie zwischen HomeScreen und FriendsScreen navigieren.
Schritt 4 - Verwenden von Context zur Weitergabe von Daten an andere Bildschirme
In diesem Schritt erstellen Sie ein Array mit möglichen Freunden - Alice, Bob und Sammy - und ein leeres Array mit aktueller Freunden.
Sie werden auch eine Funktion erstellen, mit der der Benutzer mögliche Freunde zu seinen aktuellen Freunden hinzufügen kann.
Öffnen Sie App.js:
Fügen Sie possibleFriends und currentFriends zum Zustand der Komponente hinzu:
Fügen Sie als Nächstes eine Funktion hinzu, mit der sich ein möglicher Freund in die Liste aktueller Freunde verschieben lässt:
Jetzt haben Sie die Funktionalität zum Hinzufügen von Freunden fertiggestellt.
Hinzufügen von FriendsContext zu App
Jetzt können Sie Freunde in App.js hinzufügen, aber Sie wollen auch, dass sie zu FriendsScreen.js hinzugefügt und in HomeScreen.js angezeigt werden.
Da dieses Projekt mit React erstellt wurde, können Sie diese Funktionalität mit Kontext in Ihre Bildschirme injizieren.
Hinweis: In vorherigen Versionen von React Navigation war es möglich, screenProps zu verwenden, um Daten zwischen Bildschirmen auszutauschen.
In der aktuellen Version von React Navigation wird empfohlen, React Context für den Austausch von Daten zwischen Bildschirmen zu nutzen.
Um eine zirkuläre Referenz zu vermeiden, sollten Sie eine neue Datei namens FriendsContext erstellen:
Exportieren Sie FriendsContext:
Fügen Sie den FriendsContext hinzu:
Dieser Code erstellt FriendsContext als neues Context-Objekt und umschließt den NavigationContainer in einer Context.Provider-Komponente, damit alle untergeordneten Elemente im Komponentenbaum Kontextänderungen abonnieren können.
Da Sie Ansicht oder Text nicht mehr verwenden, ist es möglich, diese Importe aus react-native zu entfernen.
Sie müssen einen Wert angeben, um Daten Verbrauchern zugänglich zu machen:
Dadurch werden der HomeScreen und FriendsScreen alle Kontextänderungen bei currentFriends und possibleFriends referenzieren können.
Jetzt können Sie daran arbeiten, den Kontext in Ihren Bildschirmen zu referenzieren.
Hinzufügen von FriendsContext zu HomeScreen
In diesem Schritt richten Sie die Anwendung so ein, dass die aktuelle Freundeszahl angezeigt wird.
Fügen Sie den FriendsContext hinzu:
Dieser Code erstellt einen Class.contextType.
Sie können nun auf Kontext in Ihren Bildschirmen zugreifen.
Lassen Sie uns beispielsweise Ihren HomeScreen anzeigen, wie viele currentFriends Sie haben:
Wenn Sie Ihre Anwendung im Simulator erneut öffnen und den HomeScreen anzeigen, sehen Sie die Nachricht: Du hast 0 Freunde!.
Hinzufügen von FriendsContext zu FriendsScreen
In diesem Schritt richten Sie die Anwendung so ein, dass die möglichen Freunde angezeigt und Schaltflächen zum Hinzufügen zu den aktuellen Freunden bereitgestellt werden.
Öffnen Sie als Nächstes FriendsScreen.js:
Erstellen Sie nun eine Schaltfläche zum Hinzufügen von Freunden in FriendsScreen.js:
Wenn Sie Ihre Anwendung im Simulator erneut öffnen und den FriendsScreen anzeigen, sehen Sie eine Liste von Freunden, die hinzugefügt werden können.
HomeScreen mit 0 currentFriends und FriendScreen mit 3 possibleFriends
Wenn Sie den FriendsScreen besuchen und auf die Schaltfläche zum Hinzufügen von Freunden klicken, sehen Sie, dass die Liste von possibleFriends kleiner wird.
Wenn Sie den HomeScreen besuchen, sehen Sie, dass die Anzahl der Freunde zunimmt.
Sie können nun zwischen Bildschirmen navigieren und Daten zwischen ihnen austauschen.
Zusammenfassung
In diesem Tutorial haben Sie eine React Native-Anwendung mit mehreren Bildschirmen erstellt.
Mithilfe von React Navigation haben Sie eine Möglichkeit zur Navigation zwischen Bildschirmen eingerichtet.
Mithilfe von React Context haben Sie eine Möglichkeit entwickelt, um Daten zwischen Bildschirmen auszutauschen.
Der komplette Quellcode für dieses Tutorial ist auf GitHub verfügbar.
Wenn Sie mehr über React Navigation erfahren möchten, sehen Sie sich die entsprechende Dokumentation an.
React Navigation ist nicht die einzige Routing- und Navigationslösung.
Es gibt auch React Native Navigation, React Native Router Flux und React Router Native.
Wenn Sie mehr über React erfahren möchten, sehen Sie sich unsere Reihe Codieren in React.js an oder besuchen Sie unsere React-Themenseite für Übungen und Programmierprojekte.
Der Autor hat den COVID-19 Relief Fund dazu ausgewählt, eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
Eine wenig bekannte Funktion von Gmail und Google Apps E-Mail ist der portable SMTP-Server von Google.
Sie benötigen lediglich entweder (i) ein kostenloses Gmail-Konto oder (ii) ein kostenpflichtiges G Suite-Konto.
Vorteile
SMTP-Passwort: < ^ > Ihr Gmail- oder G Suite-E-Mail-Passwort < ^ >
Sie können die Liste überprüfen, indem Sie auf dem Einstellungsbildschirm auf die Registerkarte Konten und Import gehen.
Diese Begrenzung beschränkt die Anzahl der täglich gesendeten Nachrichten auf 99 E-Mails; die Beschränkung wird automatisch 24 Stunden nach Erreichen der Beschränkung aufgehoben.
In diesem Artikel werden Sie jedes Prinzip einzeln kennenlernen, um zu verstehen, wie SOLID Ihnen dabei helfen kann, ein besserer Entwickler zu werden.
Anmerkung: Obwohl diese Prinzipien auf verschiedene Programmiersprachen angewendet werden können, wird der in diesem Artikel enthaltene Beispielcode PHP verwendet.
L - Liskovsches Substitutionsprinzip
D - Dependency-Inversion-Prinzip (Abhängigkeit-Umkehr-Prinzip)
Single-Responsibility-Prinzip
Der Flächeninhalt eines Quadrats wird durch die Länge zum Quadrat berechnet.
Hier ist ein Beispiel mit einer Sammlung von drei Formen:
Ein Kreis mit einem Radius von 2
Jetzt wird die Logik, die Sie zur Ausgabe der Daten an den Benutzer benötigen, von der Klasse SumCalculatorOutputter bearbeitet.
Objekte oder Entitäten sollten offen für Erweiterungen, aber geschlossen für Änderungen sein.
Hier ist die in Square definierte Methode area:
Woher wissen Sie, dass das an den AreaCalculator übergebene Objekt tatsächlich eine Form ist oder ob die Form eine Methode namens area aufweist?
Ändern Sie Ihre Formklassen, um das ShapeInterface mit implement zu implementieren.
Wenn Sie die Methode HTML auf dem Objekt $output2 aufrufen, erhalten Sie einen Fehler E _ NOTICE, der Sie über eine Array-zu-String-Konvertierung informiert.
Weiterhin aufbauend auf dem vorherigen Beispiel ShapeInterface, müssen Sie die neuen dreidimensionalen Formen Cuboid und Spheroid unterstützen, und diese Formen müssen auch das Volumen berechnen.
Dies ist ein wesentlich besserer Ansatz, aber ein Fallstrick, auf den Sie achten müssen, wenn Sie diese Schnittstellen mit Typ-Hinweisen versehen.
Anstatt ein ShapeInterface oder ein ThreeDimensionalShapeInterface zu verwenden, können Sie eine andere Schnittstelle erstellen, vielleicht ManageShapeInterface, und diese sowohl für die flachen als auch für die dreidimensionalen Formen implementieren.
Dieses Prinzip ermöglicht die Entkopplung.
In diesem Artikel Ihnen die fünf Prinzipien von SOLID Code vorgestellt.
Lernen Sie weiter, indem Sie über andere Praktiken für die agile und adaptive Softwareentwicklung lesen.
In diesem Tutorial erstellen Sie mit der Fetch-API sowohl GET- als auch POST-Anfragen.
Um dieses Tutorial zu absolvieren, benötigen Sie Folgendes:
Zur Installation von Node unter macOS folgen Sie den in diesem Tutorial Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung unter macOS beschriebenen Schritten.
Lesen Sie den Abschnitt Promises dieses Artikels über die Ereignisschleife, Callbacks, Promises und async / await in JavaScript.
Wenn dies geschieht, wird das Promise reject zurückgegeben.
Erstellen Sie eine konstante Variable namens url, die die API enthält, die zehn zufällige Benutzer zurückgibt:
Der Parameter resp nimmt den Wert des von fetch (url) zurückgegebenen Objekts an.
Verwenden Sie die Methode json (), um resp in JSON-Daten zu konvertieren:
Die API bietet einen Namen für den Autor und ein Bild, das zu diesem Namen gehört.
Mit der Eigenschaft innerHTML und der String-Interpolation können Sie dies tun:
Da beide Funktionen then () abgeschlossen sind, können Sie nun die Funktion catch () hinzufügen.
Diese Funktion protokolliert den potenziellen Fehler auf der Konsole:
Schritt 3 - Handhaben von POST-Anfragen
Dies wird ein Objekt namens data mit dem Schlüssel name und dem Wert Sammy (oder Ihrem Namen) sein:
Die Schnittstelle Headers ist eine Eigenschaft der Fetch-API, mit der Sie verschiedene Aktionen für HTTP-Anfragen und Antwort-Header durchführen können.
Mit diesem Code kann die POST-Anfrage unter Verwendung der Fetch-API erstellt werden.
Das Konstrukt new Request nimmt zwei Argumente an: die API-URL (url) und ein Objekt.
Jetzt kann request als einziges Argument für fetch () verwendet werden, da es auch die API-URL enthält:
Gestalten von Bildlaufleisten in Chrome, Edge und Safari
Hier ist ein Beispiel, das die Pseudoelemente:: -webkit-scrollbar,:: -webkit-scrollbar-track und:: webkit-scrollbar-thumb verwendet:
Dieser Code funktioniert in den neuesten Versionen von Chrome, Edge und Safari.
Erstellen von zukunftssicheren Bildlaufleisten-Stilen
Sie können Ihr CSS in einer Weise schreiben, um sowohl die Spezifikationen -webkit-scrollbar als auch CSS Scrollbars zu unterstützen.
In diesem Artikel haben Sie einen Überblick über die Verwendung von CSS zur Gestaltung von Bildlaufleisten erhalten und wie Sie sicherstellen, dass diese Stile in den meisten modernen Browsern erkannt werden.
Diese Ansätze stoßen jedoch an ihre Grenzen, wenn es darum geht, Erfahrungen wie Trägheits-Scrolling (z. B. die abklingende Bewegung beim Scrollen über Trackpads) zu reproduzieren.
Mit CSS und CSS3 können Sie viele Dinge tun, doch das Einstellen einer Opazität für einen CSS-Hintergrund gehört nicht dazu.
Beide folgenden Methoden haben eine hervorragende Browserunterstützung bis hinunter zu Internet Explorer 8.
Sie müssen nur das Bild in einen Container position: relative: platzieren.
Und hier, wie Ihr CSS aussehen wird:
Hier ist eine Live-Demo:
Auch hier müssen wir den z-index des Inhalts (in diesem Fall das < h1 >) über das Hintergrund-Pseudoelement verschieben, und wir müssen die position: absolute; und den z-index: 1 auf dem Pseudoelement: before explizit definieren.
In diesem Artikel wird erläutert, wie Sie root-Berechtigungen korrekt und sicher erhalten. Ein besonderer Schwerpunkt liegt dabei auf der Bearbeitung der Datei / etc / sudoers.
Anmelden als root user
Wenn Sie keine SSH-Schlüssel für den root user eingerichtet haben, geben Sie das Root-Passwort ein, wenn Sie dazu aufgefordert werden.
Verwenden Sie su, um ein root-Benutzer zu werden
Geben Sie Folgendes ein, um root-Berechtigungen zu erhalten:
Wenn Sie die Aufgaben abgeschlossen haben, für die root-Berechtigungen erforderlich sind, kehren Sie zu Ihrer normalen Shell zurück, indem Sie Folgendes eingeben:
Verwenden Sie sudo, um Befehle als root-Benutzer auszuführen
Im Gegensatz zu su fordert der Befehl sudo das Passwort des aktuellen Benutzers an, nicht das root-Passwort.
In unseren Schnellstart-Tutorials zum Erstellen eines neuen Sudo-fähigen Benutzers für Ubuntu und CentOS erfahren Sie, wie Sie einen Sudo-fähigen Benutzer einrichten.
Was ist Visudo?
Der Befehl sudo wird über eine Datei unter / etc / sudoers konfiguriert.
< $> Warnung
Da eine falsche Syntax in der Datei / etc / sudoers zu einem Systembruch führen kann, bei dem es nicht möglich ist, erhöhte Berechtigungen zu erhalten, ist es wichtig, den Befehl visudo zum Bearbeiten der Datei zu verwenden.
Der Befehl visudo öffnet wie gewohnt einen Texteditor, überprüft jedoch beim Speichern die Syntax der Datei.
Dies verhindert, dass Konfigurationsfehler sudo-Vorgänge blockieren. Dies ist möglicherweise Ihre einzige Möglichkeit, root-Berechtigungen zu erhalten.
Traditionell öffnet visudo die Datei / etc / sudoers mit dem vi-Texteditor.
Unter CentOS können Sie diesen Wert ändern, indem Sie Ihrem ~ / .bashrc die folgende Zeile hinzufügen:
In Ihrem ausgewählten Texteditor wird die Datei / etc / sudoers angezeigt.
Werfen wir einen Blick darauf, was diese Zeilen bewirken.
In der ersten Zeile "Defaults env _ reset" wird die Terminalumgebung zurückgesetzt, um alle Benutzervariablen zu entfernen.
Standardmäßig ist dies das root-Konto.
Die dritte Zeile, die mit "Defaults Secure _ path =..." beginnt, gibt den Pfad an (die Stellen im Dateisystem, nach denen das Betriebssystem nach Anwendungen sucht), die für sudo-Operationen verwendet werden.
root ALL = (ALL: < ^ > ALL < ^ >) ALL
Gruppenberechtigungszeilen
Wie bei der Datei / etc / sudoers selbst sollten Sie Dateien im Verzeichnis / etc / sudoers.d immer mit visudo bearbeiten.
Die häufigste Operation, die Benutzer beim Verwalten von sudo-Berechtigungen ausführen möchten, besteht darin, einem neuen Benutzer allgemeinen sudo-Zugriff zu gewähren.
Oder unter Verwendung von gpasswd:
Wir können dann Mitgliedern von GROUPTWO erlauben, die apt-Datenbank zu aktualisieren, indem wir eine Regel wie die folgende erstellen:
Auf diese Weise kann jeder, der Mitglied von GROUPONE ist, Befehle als www-Datenbenutzer oder Apache-Benutzer ausführen.
Es hat einen Begleitbefehl namens PASSWD, der das Standardverhalten ist.
Sie werden aufgefordert, Ihr Passwort einzugeben, das für spätere sudo-Verwendungen zwischengespeichert wird, bis der sudo-Zeitrahmen abläuft.
Dies gibt Ihnen eine gute Vorstellung davon, was Sie mit sudo als Benutzer tun dürfen oder nicht.
Für ein bisschen Spaß können Sie Ihrer / etc / sudoers-Datei mit visudo die folgende Zeile hinzufügen:
Sie sollten nun ein grundlegendes Verständnis für das Lesen und Ändern der sudoers-Datei haben und einen Überblick über die verschiedenen Methoden haben, mit denen Sie root-Berechtigungen erhalten können.
Nginx Server- und Standortblockauswahlalgorithmen verstehen
In diesem Leitfaden werden wir erläutern, wie Nginx den Server und den Speicherblock auswählt, der die Abfrage eines bestimmten Clients verarbeitet.
1381
In diesem Leitfaden werden wir einige der Details hinter den Kulissen erörtern, die bestimmen, wie Nginx Client-Abfragen verarbeitet.
Das Verständnis dieser Ideen kann das Rätselraten beim Entwerfen von Server- und Standortblöcken erleichtern und die Bearbeitung von Abfragen weniger unvorhersehbar erscheinen lassen.
Nginx unterteilt die Konfigurationen, die unterschiedliche Inhalte bereitstellen sollen, logisch in Blöcke, die in einer hierarchischen Struktur leben.
Die Hauptblöcke, die wir diskutieren werden, sind der Server-Block und der Standort-Block.
Es ist ein extrem flexibles Modell.
Dies wird mit der listen-Direktive jedes Servers verglichen, um eine Liste der Serverblöcke zu erstellen, die möglicherweise die Abfrage auflösen können.
Die listen-Direktive definiert typisch, auf welche IP-Adresse und Port der Serverblock reagieren wird.
Die listen-Direktive kann auf Folgendes eingestellt werden:
Ein einzelner Port, der jede Schnittstelle an diesem Port überwacht.
Die letzte Option hat im Allgemeinen nur Auswirkungen beim Übergeben von Abfragen zwischen verschiedenen Servern.
Wenn Sie versuchen, zu bestimmen, an welchen Serverblock eine Abfrage gesendet wird, wird Nginx zunächst versuchen, anhand der Spezifität der listen-Direktive mit den folgenden Regeln zu entscheiden:
Ein Block ohne listen-Direktive verwendet den Wert 0.0.0.0: 80.
Ein Block, der auf Port 8888 ohne IP-Adresse festgelegt ist, wird zu 0.0.0.0: 8888
Analysieren der Direktive "server _ name", um eine Übereinstimmung auszuwählen
Wenn mit einem führenden Platzhalter keine Übereinstimmung gefunden wird, sucht Nginx nach einem Serverblock mit einem server _ name, der mit einem nachfolgenden Platzhalter übereinstimmt (angezeigt durch einen Servernamen, der in der Konfiguration mit einem * endet).
Wenn eine gefunden wird, wird dieser Block verwendet, um die Abfrage zu bedienen.
Wenn mehrere Übereinstimmungen gefunden werden, wird die längste Übereinstimmung verwendet, um die Abfrage zu bedienen.
Der erste server _ name mit einem regulären Ausdruck, der der "Host" -Überschrift entspricht, wird zur Bearbeitung der Abfrage verwendet.
Der erste übereinstimmende reguläre Ausdruck wird ausgewählt, um auf die Abfrage zu antworten.
Standortblöcke befinden sich in Serverblöcken (oder anderen Standortblöcken) und werden verwendet, um zu entscheiden, wie der Abfrage-URI (der Teil der Abfrage, der nach dem Domainnamen oder der IP-Adresse / dem IP-Port kommt) verarbeitet werden soll.
Das oben angezeigte < ^ > location _ match < ^ > definiert, gegen was Nginx den Abfrage-URI prüfen soll.
~: Wenn ein Tilde-Modifikator vorhanden ist, wird dieser Speicherort als Übereinstimmung zwischen regulären Ausdrücken und Groß- und Kleinschreibung interpretiert.
~ * *: Wenn ein Tilde- und ein Sternchen-Modifikator verwendet werden, wird der Positionsblock als Übereinstimmung zwischen regulären Ausdrücken ohne Berücksichtigung der Groß- und Kleinschreibung interpretiert.
Schließlich würde dieser Block verhindern, dass eine Übereinstimmung mit regulären Ausdrücken auftritt, wenn festgestellt wird, dass dies die beste Übereinstimmung mit nicht regulären Ausdrücken ist.
Wenn der am längsten übereinstimmende Präfixstandort den Modifikator ^ ~ hat, beendet Nginx die Suche sofort und wählt diesen Standort aus, um die Abfrage zu bearbeiten.
Wenn keine Standorte für reguläre Ausdrücke gefunden werden, die mit dem Abfrage-URI übereinstimmen, wird der zuvor gespeicherte Präfixstandort ausgewählt, um die Abfrage zu bedienen.
Es ist wichtig, zu verstehen, dass Nginx standardmäßig Übereinstimmungen mit regulären Ausdrücken anstelle von Präfixübereinstimmungen bereitstellt.
Obwohl dies eine allgemeine Regel ist, mit der Sie Ihre Standortblöcke auf vorhersehbare Weise entwerfen können, ist es wichtig zu wissen, dass es manchmal Zeiten gibt, in denen eine neue Standortsuche durch bestimmte Anweisungen innerhalb des ausgewählten Standortes ausgelöst wird.
In diesem Beispiel wird der erste Standort mit einem Abfrage-URI von / exact abgeglichen. Um die Abfrage zu verarbeiten, initiiert die vom Block geerbte Indexanweisung eine interne Umleitung zum zweiten Block:
Sie können beispielsweise einen ungültigen index für diesen Block festlegen und den autoindex aktivieren:
Dies ist eine Möglichkeit, um zu verhindern, dass ein index den Kontext wechselt, ist jedoch für die meisten Konfigurationen wahrscheinlich nicht hilfreich.
Dies löst eine weitere Standortsuche aus, die vom zweiten Standortblock abgefangen wird.
Eine weitere Direktive, die dazu führen kann, dass ein Standortblock übergeben wird, ist die Direktive rewrite.
Wenn jedoch eine Abfrage für / rewriteme / fallback / hello gestellt wird, stimmt der erste Block erneut überein.
Sie ist bekannt für ihre Skalierbarkeit, Leistung, Zuverlässigkeit und Benutzerfreundlichkeit.
7257
Ein Verständnis der Unterschiede zwischen JSON- und BSON-Daten in MongoDB.
Sobald der Download abgeschlossen ist, sollten Sie eine Datei mit dem Namen primer-dataset.json (12 MB Größe) im aktuellen Verzeichnis haben.
Da wir keine Datenbank mit dem Namen newdb haben, hat MongoDB sie automatisch erstellt.
Das Ergebnis zeigt 25359, was die Anzahl importierter Dokumente ist.
Für eine noch bessere Überprüfung können Sie das erste Dokument aus der Sammlung Restaurants wie folgt auswählen:
Eine solche detaillierte Überprüfung kann Probleme mit den Dokumenten wie ihrem Inhalt, der Codierung usw. enthüllen. Das Format json verwendet UTF-8-Codierung und Ihre Exporte und Importe sollten in dieser Codierung vorhanden sein.
Andernfalls verarbeitet MongoDB dies automatisch für Sie.
Ein einfaches mongoexport-Beispiel wäre die Sammlung Restaurants aus der newdb-Datenbank, die wir zuvor importiert haben.
Im obigen Befehl verwenden wir --db, um die Datenbank -c für die Sammlung und ---out für die Datei zu spezifizieren, in der die Daten gespeichert werden.
In einigen Fällen müssen Sie möglicherweise nur einen Teil Ihrer Sammlung exportieren.
Um die MongoDB-Eingabeaufforderung zu beenden, geben Sie exit ein:
Ähnlich müssen Sie alle anderen Sonderzeichen in der Abfrage umgehen.
Einrichten eines OpenVPN-Servers auf Ubuntu 18.04
2644
Eine frühere Version dieses Tutorials wurde von Justin Ellingwood verfasst
Möchten Sie sicher von Ihrem Smartphone oder Laptop auf das Internet zugreifen, wenn Sie mit einem nicht vertrauenswürdigen Netzwerk, wie dem WLAN eines Hotels oder Cafés, verbunden sind?
Ein Virtual Private Network (VPN) ermöglicht die private und sichere Nutzung nicht vertrauenswürdiger Netzwerke, als ob Sie sich in einem privaten Netzwerk befinden würden.
Der Verkehr wird vom VPN-Server ausgegeben und wird bis zum Ziel übermittelt.
In Kombination mit HTTPS-Verbindungen können Sie mit diesem Setup Ihre drahtlosen Logins und Transaktionen sichern.
Sie können geografische Beschränkungen und Zensuren umgehen und Ihren Ort und jeglichen unverschlüsselten HTTP-Verkehr vom nicht vertrauenswürdigen Netzwerk abschirmen.
OpenVPN ist eine Open-Source Secure Socket Layer (SSL) VPN-Lösung mit umfassender Funktionalität, die eine Vielzahl von Konfigurationen unterstützt.
In diesem Tutorial richten Sie einen OpenVPN-Server auf einem Ubuntu 18.04-Server ein und konfigurieren dann den Zugriff auf ihn von Windows, MacOS, iOS und / oder Android.
Dieses Tutorial wird die Installations- und Konfigurationsschritte für jedes dieser Setups so einfach wie möglich halten.
< $> Anmerkung Anmerkung: Wenn Sie planen, einen OpenVPN-Server auf einem DigitalOcean-Droplet einzurichten, sollten Sie sich bewusst sein, dass wir, wie viele Hosting-Anbieter, Zusatzgebühren für das Überschreiten des Bandbreitenlimits verlangen können.
Bedenken Sie aus diesem Grund, wie viel Datenverkehr Ihr Server verarbeiten kann.
Siehe diese Seite für weitere Informationen.
Um dieses Tutorial zu absolvieren, benötigen Sie Zugriff auf einen Ubuntu 18.04-Server, der Ihren OpenVPN-Dienst hosten kann.
Sie müssen einen Benutzer ohne Root-Berechtigung mit sudo-Rechten konfigurieren, bevor Sie diesen Leitfaden starten können.
Sie können unseren Leitfaden zur Ersteinrichtung des Servers mit Ubuntu 18.04 befolgen, um einen Benutzer mit entsprechenden Berechtigungen einzurichten.
Das verknüpfte Tutorial richtet auch eine Firewall ein, und in diesem Leitfaden wird davon ausgegangen, dass sie vorhanden ist.
Zusätzlich benötigen Sie einen separaten Computer, der als Ihre Zertifizierungsstelle (Certificate Authority, CA) dient.
Während es technisch möglich ist, Ihren OpenVPN-Server oder Ihren lokalen Computer als CA zu verwenden, wird dies nicht empfohlen, da Ihr VPN dadurch gewissen Sicherheitsschwachstellen ausgesetzt ist.
Gemäß der offiziellen OpenVPN-Dokumentation sollten Sie Ihre CA auf einem eigenständigen Computer ablegen, der für das Importieren und Signieren von Zertifikatsanforderungen bestimmt ist.
Aus diesem Grund wird in diesem Leitfaden davon ausgegangen, dass sich Ihre CA auf einem separaten Ubuntu 18.04-Server befindet, der auch einen Benutzer ohne Root-Berechtigung mit Sudo-Rechten und eine einfache Firewall aufweist.
Bitte beachten Sie, dass wenn Sie beim Konfigurieren dieser Server die Passwort-Authentifizierung deaktivieren, Sie später beim Übertragen von Dateien zwischen ihnen gemäß Leitfaden Schwierigkeiten haben können.
Um dieses Problem zu lösen, könnten Sie auf jedem Server die Passwort-Authentifizierung neu aktivieren.
Alternativ könnten Sie für jeden Server ein SSH-Schlüsselpaar erstellen, dann den öffentlichen SSH-Schlüssel des OpenVPN-Servers zur Datei authorized _ keys des CA-Computers hinzufügen und umgekehrt.
Anweisungen zur Ausführung dieser Lösungen finden Sie unter Einrichten von SSH-Schlüsseln auf Ubuntu 18.04.
Wenn diese Voraussetzungen erfüllt sind, können Sie mit Schritt 1 dieses Tutorials fortfahren.
Schritt 1 - Installieren von OpenVPN und EasyRSA
Aktualisieren Sie zunächst den Paketindex Ihres VPN-Servers und installieren Sie OpenVPN.
OpenVPN steht in Ubuntu Standard-Repositorien zur Verfügung, damit Sie apt für die Installation verwenden können:
OpenVPN ist ein TLS / SSL VPN.
Das bedeutet, dass es Zertifikate verwendet, um den Verkehr zwischen dem Server und Clients zu verschlüsseln.
Um vertrauenswürdige Zertifikate auszugeben, richten Sie Ihre eigene einfache Zertifizierungsstelle (CA) ein.
Dazu laden wir die neueste Version von EasyRSA herunter, mit der wir unsere Public-Key-Infrastruktur (PKI) der CA vom offiziellen GitHub-Repository des Projekts aufbauen können.
Wie in den Voraussetzungen erwähnt, erstellen wir die CA auf einem eigenständigen Server.
Der Grund für diesen Ansatz ist folgender: Wenn ein Angreifer Ihren Server infiltrieren könnte, wäre er in der Lage, auf Ihren Private-Key der CA zuzugreifen und neue Zertifikate zu signieren, wodurch er Zugriff auf das VPN erhalten würde.
Dementsprechend kann die Verwaltung der CA über einen eigenständigen Computer dazu beitragen, den Zugriff auf Ihr VPN durch unbefugte Benutzer zu verhindern.
Beachten Sie auch, dass als weitere Vorsichtsmaßnahme empfohlen wird, den CA-Server ausgeschaltet zu lassen, wenn er nicht zum Signieren von Schlüsseln verwendet wird.
Um mit dem Aufbau der CA- und PKI-Infrastruktur zu beginnen, verwenden Sie wget, um die neueste Version von EasyRSA sowohl auf Ihren CA-Computer als auch auf Ihren OpenVPN-Server herunterzuladen.
Um die neueste Version zu erhalten, gehen Sie auf die Seite Releases im offiziellen EasyRSA GitHub-Projekt, kopieren Sie den Download-Link für die Datei, die in .tgz endet, und fügen Sie sie dann in den folgenden Befehl ein:
Extrahieren Sie dann den Tarball:
Sie haben alle erforderlichen Software auf Ihrem Server und dem CA-Computer erfolgreich installiert.
Fahren Sie mit der Konfiguration der von EasyRSA verwendeten Variablen fort und richten Sie ein CA-Verzeichnis ein, aus dem Sie die für Ihren Server und die Clients benötigten Schlüssel und Zertifikate generieren, die für den Zugriff auf das VPN erforderlich sind.
Schritt 2 - Konfiguration der EasyRSA-Variablen und Aufbau der CA
In EasyRSA ist eine Konfigurationsdatei vorinstalliert, die Sie zum Definieren einer Anzahl von Variablen für Ihre CA bearbeiten können.
Navigieren Sie auf Ihrem CA-Computer zum EasyRSA-Verzeichnis:
In diesem Verzeichnis befindet sich eine Datei namens vars.example.
Erstellen Sie eine Kopie dieser Datei und benennen Sie die Kopie vars ohne Datei-Erweiterung:
Öffnen Sie diese neue Datei mit Ihrem bevorzugten Textbearbeitungsprogramm:
Finden Sie die Einstellungen, mit denen Feldstandardeinstellungen für neue Zertifikate festgelegt werden.
Dies sollte ungefähr so aussehen:
Kommentieren Sie diese Zeilen aus und aktualisieren Sie die markierten Werte auf von Ihnen bevorzugte Werte; lassen Sie sie jedoch nicht leer:
Wenn Sie dies abgeschlossen haben, speichern und schließen Sie die Datei.
Im EasyRSA-Verzeichnis befindet sich ein Skript namens easyrsa, das zum Aufruf einer Vielzahl von Aufgaben verwendet wird, die mit dem Aufbau und der Verwaltung der CA im Zusammenhang stehen.
Führen Sie dieses Skript mit der Option init-pki aus, um die Public-Key-Infrastruktur auf dem CA-Server einzuleiten:
Anschließend rufen Sie das easyrsa-Skript erneut auf und führen die Option build-ca aus.
Dadurch wird die CA erstellt und es werden zwei wichtige Dateien angelegt - ca.crt und caskey -, die die öffentlichen und privaten Seiten eines SSL-Zertifikats darstellen.
ca.crt ist die öffentliche Zertifikatdatei der CA, die der Server und der Client im Zusammenhang mit OpenVPN dazu verwenden, einander darüber zu informieren, dass sie demselben vertrauenswürdigen Web angehören und keinen Man-in-the-Middle-Angriff ausführten.
Aus diesem Grund benötigen Ihr Server und alle Ihre Clients eine Kopie der Datei ca.crt.
ca.key ist der private Schlüssel, mit dem der CA-Computer Schlüssel und Zertifikate für Server und Clients signiert.
Wenn ein Angreifer Zugriff auf Ihre CA und die Datei ca.key erhält, kann er Zertifikatanforderungen signieren und Zugriff auf Ihr VPN erhalten, wodurch Ihre Sicherheit gefährdet wird.
Deshalb sollte sich Ihre Datei ca.key nur auf Ihrem CA-Computer befinden und Ihr CA-Computer im Idealfall als zusätzliche Sicherheitsmaßnahme offline bleiben, wenn keine Zertifikatanforderungen signiert werden.
Wenn Sie nicht bei jeder Interaktion mit Ihrer CA zur Eingabe eines Passwortes aufgefordert werden möchten, können Sie den Befehl build-ca mit der Option nopass ausführen, wie folgt:
In der Ausgabe werden Sie aufgefordert, den gemeinsamen Namen für Ihre CA zu bestätigen:
Der gemeinsame Name ist der Name, der verwendet wird, um im Kontext der Zertifizierungsstelle auf diesen Computer zu verweisen.
Sie können als geläufigen Namen der CA eine beliebige Zeichenfolge eingeben, aber drücken Sie der Einfachheit halber die ENTER, um den Standardnamen zu akzeptieren.
Damit ist Ihre CA bereit, Zertifikatanforderungen zu signieren.
Schritt 3 - Erstellen des Server-Zertifikats, Schlüssels und der Verschlüsselungsdateien
Da Ihre CA jetzt einsatzbereit ist, können Sie von Ihrem Server einen privaten Schlüssel und eine Zertifikatsanforderung erstellen und die Anforderung dann zu Ihrer CA zwecks Signieren übertragen, wodurch das angeforderte Zertifikat erstellt wird.
Sie können auch zusätzliche Dateien erstellen, die während des Verschlüsselungsprozesses verwendet werden.
Beginnen Sie, indem Sie auf Ihrem OpenVPN-Server in das EasyRSA-Verzeichnis navigieren:
Führen Sie dort das easyrsa-Skript mit der Option init-pki aus.
Obwohl Sie diesen Befehl bereits auf dem CA-Computer ausgeführt haben, muss er hier ausgeführt werden, weil Ihr Server und die CA über separate PKI-Verzeichnisse verfügen:
Dann rufen Sie das easyrsa-Skript erneut auf, diesmal mit der Option gen-req, gefolgt von einem gemeinsamen Namen für den Computer.
Auch hier können Sie eine beliebige Zeichenfolge eingeben, aber es kann hilfreich sein, eine aussagekräftige Bezeichnung zu wählen.
In diesem Tutorial wählen wir als geläufigen Namen des OpenVPN-Servers einfach "Server".
Achten Sie darauf, auch die Option nopass einzubeziehen.
Wenn das nicht geschieht, wird die Anforderungsdatei passwortgeschützt, was später zu Berechtigungsproblemen führen könnte:
< $> Anmerkung Anmerkung: Wenn Sie hier einen anderen Namen als "Server" wählen, müssen Sie einige der nachstehenden Anweisungen anpassen.
Wenn Sie beispielsweise die erzeugten Dateien zum Verzeichnis / etc / openvpn kopieren, müssen Sie die richtigen Namen einfügen.
Sie müssen auch später die Datei / etc / openvpn / server.conf ändern, um auf die richtigen Dateien .crt- und .key zu verweisen.
Dadurch wird ein privater Schlüssel für den Server und eine Zertifikatsanforderungsdatei namens server.req erstellt.
Kopieren Sie den Serverschlüssel in das Verzeichnis / etc / openvpn /:
Mit einem sicheren Verfahren (wie SCP in unserem nachstehenden Beispiel) können Sie die Datei server.req zu Ihrem CA-Computer übertragen:
Navigieren Sie als Nächstes auf Ihrem CA-Computer in das EasyRSA-Verzeichnis:
Mit dem easyrsa-Skript wird die Datei server.req importiert, wobei der geläufige Name auf den Dateipfad folgt:
Dann signieren Sie die Anforderung, indem Sie das easyrsa-Skript mit der Option sign-req ausführen, gefolgt vom Anforderungstyp und dem gemeinsamen Namen.
Der Anforderungstyp kann entweder client oder server sein; achten Sie daher darauf, für die Zertifikatsanforderung des OpenVPN-Servers den Anforderungstyp server zu verwenden:
In der Ausgabe müssen Sie verifizieren, dass die Anforderung von einer vertrauenswürdigen Quelle stammt.
Geben Sie Ja ein, und drücken Sie dann zur Bestätigung die EINGABETASTE:
Wenn Sie Ihren CA-Schlüssel verschlüsselt haben, werden Sie zu diesem Zeitpunkt zur Eingabe Ihres Passworts aufgefordert.
Übertragen Sie dann das signierte Zertifikat im sicheren Verfahren zurück auf Ihren VPN-Server:
Bevor Sie sich bei Ihrem CA-Computer abmelden, übertragen Sie auch die Datei ca.crt zu Ihrem Server:
Melden Sie sich wieder auf Ihrem OpenVPN-Server an und kopieren Sie die Dateien server.crt und ca.crt in das Verzeichnis / etc / openvpn /:
Navigieren Sie dann in Ihr EasyRSA-Verzeichnis:
Erstellen Sie dort einen starken Diffie-Hellman-Schlüssel, der während des Schlüsselaustauschs verwendet wird, indem Sie Folgendes eingeben:
Dies kann einige Minuten dauern.
Generieren Sie nach Abschluss dieses Verfahrens eine HMAC-Signatur, um die TLS-Integritätsverifizierungsfähigkeiten des Servers zu verbessern:
Kopieren Sie nach Beendigung des Befehls die beiden neuen Dateien in das Verzeichnis / etc / openvpn /:
Damit wurden alle von Ihrem Server benötigten Zertifikats- und Schlüsseldateien generiert.
Jetzt können Sie die entsprechenden Zertifikate und Schlüssel erstellen, die Ihr Client-Computer für den Zugriff auf Ihren OpenVPN-Server verwendet.
Schritt 4 - Generieren eines Client-Zertifikats und eines Schlüsselpaars
Sie können zwar einen privaten Schlüssel und eine Zertifikatsanforderung auf Ihrem Client-Computer erstellen und dann zwecks Signierung zur CA senden, aber dieser Leitfaden beschreibt einen Prozess zum Generieren der Zertifikatsanforderung auf dem Server.
Der Vorteil besteht darin, dass wir ein Skript erstellen können, das automatisch Client-Konfigurationsdateien generiert, die alle benötigten Schlüssel und Zertifikate enthalten.
Damit können Sie vermeiden, dass Schlüssel, Zertifikate und Konfigurationsdateien an Clients übertragen werden müssen, und der Prozess des Verbindungsaufbaus zum VPN wird gestrafft.
In diesem Leitfaden generieren wir einen einzelnen Client-Schlüssel und ein Zertifikat-Paar.
Wenn Sie mehr als einen Client haben, können Sie diesen Prozess für jeden Client wiederholen.
Bitte beachten Sie allerdings, dass Sie für jeden Client einen eindeutigen Namenswert an das Skript übergeben müssen.
In diesem Tutorial wird das erste Zertifikat / Schlüsselpaar als client1 bezeichnet.
Beginnen Sie, indem Sie eine Verzeichnisstruktur in Ihrem Stammverzeichnis erstellen, um das Client-Zertifikat und die Schlüsseldateien zu speichern:
Da Sie das Zertifikat / die Schlüsselpaare Ihrer Clients und die Konfigurationsdateien in diesem Verzeichnis speichern, sollten Sie als Sicherheitsmaßnahme die Berechtigungen jetzt sperren:
Als Nächstes navigieren Sie zurück zum EasyRSA-Verzeichnis und führen das easyrsa-Skript mit den Optionen gen-req und nopass zusammen mit dem gemeinsamen Namen für den Client aus:
Drücken Sie die EINGABETASTE, um den gemeinsamen Namen zu bestätigen.
Kopieren Sie dann die Datei client1.key zum zuvor erstellten Verzeichnis / client-configs / keys /:
Als Nächstes übertragen Sie die Datei client1.req mit einem sicheren Verfahren an Ihren CA-Computer:
Melden Sie sich bei Ihrem CA-Computer an, navigieren Sie zum EasyRSA-Verzeichnis und importieren die Zertifikatsanforderung:
Signieren Sie dann die Anforderung, wie Sie dies im vorherigen Schritt für den Server getan haben.
Dieses Mal muss jedoch der Anforderungstyp client angegeben werden:
Geben Sie bei der Eingabeaufforderung Ja ein, um zu bestätigen, dass Sie beabsichtigen, die Zertifikatsanforderung zu signieren, und dass sie aus einer vertrauenswürdigen Quelle stammt:
Wenn Sie Ihren CA-Schlüssel verschlüsselt haben, werden Sie erneut zur Eingabe Ihres Passworts aufgefordert.
Damit wird ein Client-Zertifikat namens client1.crt erstellt.
Übertragen Sie diese Datei wieder zum Server:
Stellen Sie mit SSH wieder eine Verbindung mit dem OpenVPN-Server her und kopieren Sie das Client-Zertifikat zum Verzeichnis / client-configs / keys /:
Kopieren Sie dann auch die Dateien ca.crt und ta.key in das Verzeichnis / client-configs / keys /:
Damit wurden alle Zertifikate des Servers und Clients sowie alle Schlüssel generiert und in den entsprechenden Verzeichnissen auf Ihrem Server gespeichert.
Es gibt noch einige Aktionen, die mit diesen Dateien ausgeführt werden müssen, aber diese werden in einem späteren Schritt beschrieben.
Jetzt können Sie mit der Konfiguration von OpenVPN auf Ihrem Server fortfahren.
Schritt 5 - Konfiguration des OpenVPN-Dienstes
Da jetzt sowohl die Client- als auch Server-Zertifikate und die Schlüssel generiert wurden, können Sie mit der Konfiguration des OpenVPN-Dienstes beginnen, um diese Anmeldedaten zu verwenden.
Beginnen Sie, indem Sie eine OpenVPN-Konfigurationsbeispieldatei in das Konfigurationsverzeichnis kopieren und dann dort extrahieren, um sie als Grundlage für Ihr Setup zu verwenden:
Öffnen Sie die Konfigurationsdatei des Servers in Ihrem bevorzugten Textbearbeitungsprogramm:
Lokalisieren Sie den HMAC-Abschnitt, indem Sie nach der Anweisung tls-auth suchen.
Diese Zeile sollte bereits auskommentiert sein, aber wenn das nicht der Fall ist, entfernen Sie das "; ", um sie auszukommentieren:
Als Nächstes lokalisieren Sie den Abschnitt über kryptographische Verschlüsselungen, indem Sie nach den auskommentierten Zeilen cipher suchen.
Die Verschlüsselung AES-256-CBC bietet eine gute Verschlüsselungsstufe und wird ausreichend unterstützt.
Diese Zeile sollte ebenfalls bereits auskommentiert sein, aber wenn das nicht der Fall ist, entfernen Sie ganz einfach das vorangestellte "; ":
Darunter fügen Sie die Anweisung auth hinzu, um den Digestalgorithmus der HMAC-Nachricht zu wählen.
Dafür ist SHA256 eine gute Wahl:
Suchen Sie als Nächstes nach der Zeile mit der Anweisung dh, die die Diffie-Hellman-Parameter definiert.
Aufgrund einiger jüngster Änderungen an EasyRSA kann sich der Dateiname für den Diffie-Hellman-Schlüssel von demjenigen in der Konfigurationsbeispieldatei unterscheiden.
Wenn erforderlich, ändern Sie den hier aufgeführten Datennamen, indem Sie 2048 entfernen, so dass er dem im vorherigen Schritt generierten Schlüssel entspricht:
Schließlich lokalisieren Sie die Einstellungen für user und group und entfernen jeweils das "; "am Anfang, um diese Zeilen auszukommentieren:
Die Änderungen, die Sie bisher an der Beispieldatei server.conf vorgenommen haben, sind für das Funktionieren von OpenVPN notwendig.
Die nachstehend aufgeführten Änderungen sind zwar optional, werden aber auch für viele häufige Anwendungsfälle benötigt.
(Optional) DNS-Änderungen mithilfe von Push übertragen, um den gesamten Verkehrs durch das VPN umzuleiten
Mit den obigen Einstellungen wird die VPN-Verbindung zwischen den beiden Computern erstellt, die Verbindungen werden allerdings nicht zum Einsatz des Tunnels gezwungen.
Wenn das VPN für das Routen des gesamten Verkehrs verwendet werden soll, sollten Sie wahrscheinlich die DNS-Einstellungen mithilfe von Push an die Client-Computer übertragen.
In der Datei server.conf gibt es einige Anweisungen, die Sie ändern müssen, um diese Funktionalität zu aktivieren.
Lokalisieren Sie den Abschnitt redirect-gateway und entfernen das Semikolon "; ", vom Anfang der Zeile redirect-gateway, um den Kommentar zu entfernen:
Direkt darunter befindet sich der Abschnitt dhcp-option.
Entfernen Sie erneut das "; "zu Beginn der beiden Zeilen, um die Kommentare zu entfernen:
Dadurch werden die Clients bei der Neukonfigurierung ihrer DNS-Einstellungen unterstützt, damit der VPN-Tunnel als Standard-Gateway verwendet werden kann.
(Optional) Anpassung des Ports und Protokolls
Standardmäßig verwendet der OpenVPN-Server Port 1194 und das UDP-Protokoll, um Client-Verbindungen zu akzeptieren.
Wenn Sie aufgrund von restriktiven Netzwerkumgebungen, in denen sich Ihre Clients eventuell befinden, einen anderen Port verwenden müssen, können Sie die Option port ändern.
Wenn Sie auf Ihrem OpenVPN-Server keine Web-Inhalte hosten, ist Port 443 eine gängige Wahl, da er üblicherweise von den Firewall-Regeln zugelassen wird.
Häufig ist das Protokoll auch auf diesen Port beschränkt.
Wenn das der Fall ist, ändern Sie proto von UDP zu TCP:
Wenn Sie das Protokoll tatsächlich auf TCP ändern, müssen Sie den Wert der Anweisung explicit-exit-notify von 1 auf 0 ändern, da diese Anweisung nur von UDP verwendet wird.
Wenn dies während der Verwendung von TCP nicht befolgt wird, treten beim Starten des OpenVPN-Dienstes Fehler auf:
Wenn Sie keinen anderen Port und kein anderes Protokoll verwenden müssen, ist es am besten, diese beiden Einstellungen in ihren Standardeinstellungen zu belassen.
(Optional) Auf nicht standardmäßige Anmeldedaten verweisen
Wenn Sie zuvor während des Befehls. / build-key-server einen anderen Namen ausgewählt haben, ändern Sie die Zeilen cert und key so ab, dass Sie auf die entsprechenden Dateien .crt und .key verweisen.
Wenn Sie den Standardnamen "server" verwendet haben, ist dieser bereits korrekt festgelegt:
Nachdem Sie die OpenVPN-Konfiguration Ihres Servers überprüft und alle für Ihren spezifischen Anwendungsfall erforderlichen Änderungen vorgenommen haben, können Sie damit beginnen, Änderungen am Netzwerk Ihres Servers vorzunehmen.
Schritt 6 - Anpassung der Netzwerkkonfiguration des Servers
Es gibt einige Aspekte der Netzwerkkonfiguration des Servers, die angepasst werden müssen, damit OpenVPN den Verkehr korrekt durch das VPN routen kann.
Als Erstes muss die IP-Weiterleitung angepasst werden, eine Methode zur Bestimmung, wohin der IP-Verkehr geleitet werden sollte.
Diese Änderung ist für die von Ihrem Server bereitgestellte VPN-Funktionalität von entscheidender Bedeutung.
Passen Sie die Standardeinstellung Ihres Servers für die IP-Weiterleitung an, indem Sie die Datei / etc / sysctl.conf ändern:
Suchen Sie dort nach der kommentierten Zeile, die net.ip4.ip _ forward festlegt.
Entfernen Sie das Zeichen "#" vom Beginn der Zeile, um diese Einstellung auszukommentieren:
Wenn Sie fertig sind, speichern und schließen Sie die Datei.
Um die Datei zu lesen und die Werte für die aktuelle Sitzung anzupassen, geben Sie Folgendes ein:
Wenn Sie den Leitfaden zur Ersteinrichtung des Servers mit Ubuntu 18.04 befolgt haben, der in den Voraussetzungen aufgeführt ist, sollte eine UFW-Firewall vorhanden sein.
Unabhängig davon, ob Sie die Firewall verwenden, um unerwünschten Verkehr zu blockieren (was Sie fast immer tun sollten), benötigen Sie für diesen Leitfaden eine Firewall, um einen Teil des eingehenden Verkehrs zu manipulieren.
Einige der Firewall-Regeln müssen geändert werden, damit Masquerading aktiviert werden kann, ein iptables-Konzept, das eine dynamische Netzwerkadressenübersetzung (NAT) bereitstellt, um Client-Verbindungen korrekt zu routen.
Bevor die Firewall-Konfigurationsdatei geöffnet wird, um Masquerading-Regeln hinzuzufügen, müssen Sie zunächst die öffentliche Netzwerkschnittstelle Ihres Computers finden.
Dazu geben Sie Folgendes ein:
Ihre öffentliche Schnittstelle ist die in der Ausgabe dieses Befehls enthaltene Zeichenfolge, die dem Wort "dev" folgt.
Dieses Ergebnis zeigt beispielsweise die im Folgenden hervorgehobene Schnittstelle wlp11s0 an:
Wenn Sie die mit Ihrer Standardroute verknüpfte Schnittstelle gefunden haben, öffnen Sie die Datei / etc / ufw / before.rules, um die entsprechende Konfiguration hinzuzufügen:
UFW-Regeln werden typischerweise mit dem Befehl ufw hinzugefügt.
Regeln, die jedoch in der Datei before.rules aufgeführt sind, werden gelesen und implementiert, bevor die herkömmlichen UFW-Regeln geladen werden.
Fügen Sie im oberen Teil der Datei die nachstehend hervorgehobenen Zeilen hinzu.
Damit wird die Standardrichtlinie für die POSTROUTING-Kette in der Tabelle nat festgelegt und für jeden vom VPN eingehenden Verkehr ein Masquerading ausgeführt.
Denken Sie daran, < ^ > wlp11s0 < ^ > in der nachstehenden Zeile -A POSTROUTING durch die im obigen Befehl gefundene Schnittstelle zu ersetzen:
Als Nächstes müssen Sie UFW mitteilen, weitergeleitete Pakete standardmäßig zuzulassen.
Dazu öffnen Sie die Datei / etc / default / ufw:
Lokalisieren Sie dort die Anweisung DEFAULT _ FORWARD _ POLICY und ändern Sie den Wert von DROP auf ACCEPT:
Passen Sie dann die Firewall an, um den Verkehr zu OpenVPN zuzulassen.
Wenn Sie den Port und das Protokoll in der Datei / etc / openvpn / server.conf nicht geändert haben, müssen Sie den UDP-Verkehr auf Port 1194 öffnen.
Wenn Sie den Port und / oder das Protokoll modifiziert haben, ersetzen Sie die hier ausgewählten Werte.
Sollten Sie vergessen haben, den SSH-Port während der Ausführung des Tutorials zu den Voraussetzungen hinzuzufügen, fügen Sie ihn hier hinzu:
Deaktivieren und reaktivieren Sie nach Hinzufügen dieser Regeln die UFW, um sie neu zu starten. Laden Sie die Änderungen von allen von Ihnen modifizierten Dateien:
Ihr Server ist jetzt so konfiguriert, dass der OpenVPN-Verkehr korrekt abgewickelt werden kann.
Schritt 7 - Starten und Aktivieren des OpenVPN-Dienstes
Sie sind nun bereit, den OpenVPN-Dienst auf Ihrem Server zu starten.
Dazu wird das systemd-Dienstprogramm systemctl verwendet.
Starten Sie den OpenVPN-Server, indem Sie Ihren Konfigurationsdateinamen als Instanzvariable nach dem systemd-Unit-Dateinamen angeben.
Die Konfigurationsdatei für Ihren Server lautet / etc / openvpn / < ^ > server < ^ > .conf; fügen Sie also beim Aufruf am Ende Ihrer Unit-Datei < ^ > @ server < ^ > hinzu:
Überprüfen Sie, ob der Dienst erfolgreich gestartet wurde, indem Sie Folgendes eingeben:
Bei einem erfolgreichen Start sieht Ihre Ausgabe in etwa wie folgt aus:
Sie können auch prüfen, ob die OpenVPN tun0-Schnittstelle verfügbar ist, indem Sie Folgendes eingeben:
Damit wird eine konfigurierte Schnittstelle ausgegeben:
Aktivieren Sie sie nach Starten des Dienstes, damit sie beim Hochfahren automatisch startet:
Ihr OpenVPN-Dienst wird jetzt ausgeführt.
Bevor Sie allerdings mit seiner Verwendung beginnen können, müssen Sie zunächst eine Konfigurationsdatei für den Client-Computer erstellen.
Dieses Tutorial hat bereits die Erstellung von Zertifikaten / Schlüsselpaaren für Clients behandelt. Im nächsten Schritt zeigen wir, wie man eine Infrastruktur erstellt, die problemlos Client-Konfigurationsdateien generiert.
Schritt 8 - Erstellen der Client-Konfigurationsinfrastruktur
Das Erstellen von Konfigurationsdateien für OpenVPN-Clients kann etwas komplex sein, da jeder Client seine eigene Konfiguration haben muss, die sich mit den in der Konfigurationsdatei des Servers aufgeführten Einstellungen decken muss.
Anstatt eine einzige Konfigurationsdatei zu erstellen, die nur auf einem Client verwendet werden kann, wird in diesem Schritt ein Prozess zur Erstellung einer Client-Konfigurationsinfrastruktur beschrieben, mit der Sie dynamisch Konfigurationsdateien generieren können.
Sie erstellen zunächst eine "Basis" -Konfigurationsdatei und dann ein Skript, mit dem Sie je nach Bedarf eindeutige Client-Konfigurationsdateien, Zertifikate und Schlüssel generieren können.
Beginnen Sie, indem Sie ein neues Verzeichnis anlegen, in dem Sie Client-Konfigurationsdateien in dem zuvor erstellten Verzeichnis client-configs speichern:
Kopieren Sie dann ein Client-Konfigurationsbeispiel als Basiskonfiguration in das Verzeichnis client-configs:
Öffnen Sie diese neue Datei mit Ihrem Textbearbeitungsprogramm:
Lokalisieren Sie die Anweisung remote.
Damit wird der Client auf Ihre OpenVPN-Serveradresse verwiesen - die öffentliche IP-Adresse Ihres OpenVPN-Servers.
Wenn Sie beschlossen haben, den Port zu ändern, auf dem der OpenVPN-Server lauscht, müssen Sie außerdem 1194 auf den von Ihnen ausgewählten Port ändern:
Vergewissern Sie sich, dass das Protokoll mit dem Wert übereinstimmt, den Sie in der Server-Konfiguration verwenden:
Als Nächstes kommentieren Sie die Anweisungen user und group aus, indem Sie das "; "am Anfang jeder Zeile entfernen:
Lokalisieren Sie die Anweisungen, die ca, cert und key festlegen.
Kommentieren Sie diese Anweisungen aus, da Sie die Zertifikate und Schlüssel in der Datei selbst hinzufügen werden:
Kommentieren Sie auch die Anweisung tls-auth aus, da Sie der Client-Konfigurationsdatei ta.key direkt hinzufügen werden:
Verwenden Sie dieselben Einstellungen für cipher und auth, die Sie in der Datei / etc / openvpn / server.conf festgelegt haben:
Fügen Sie dann die Anweisung key-direction an einer beliebigen Stelle in der Datei hinzu.
Sie müssen diesen Wert auf "1" festlegen, damit das VPN auf dem Client-Computer korrekt funktioniert:
Fügen Sie schließlich einige auskommentierte Zeilen hinzu.
Sie können diese Anweisungen zwar in jede Client-Konfigurationsdatei einbinden, müssen sie allerdings nur für Linux-Clients aktivieren, in deren Lieferumfang die Datei / etc / openvpn / update-resolv-conf enthalten ist.
Dieses Skript verwendet das Dienstprogramm resolvconf, um DNS-Informationen für Linux-Clients zu aktualisieren.
Wenn Ihr Client Linux ausführt und eine Datei / etc / openvpn / update-resolv-conf enthält, kommentieren Sie diese Zeilen aus der Konfigurationsdatei des Clients aus, nachdem sie generiert wurde.
Erstellen Sie dann ein einfaches Skript, das Ihre Basiskonfiguration mit dem entsprechenden Zertifikat, Schlüssel und den Verschlüsselungsdateien kompiliert. Platzieren Sie dann die generierte Konfiguration in das Verzeichnis ~ / client-configs / files.
Öffnen Sie eine neue Datei namens make _ config.sh im Verzeichnis ~ / client-configs:
Fügen Sie der Datei den folgenden Inhalt hinzu:
Bevor Sie fortfahren, markieren Sie diese Datei als ausführbar, indem Sie Folgendes eingeben:
Dieses Skript erstellt eine Kopie der von Ihnen angelegten Datei base.conf, sammelt alle für Ihren Client erstellten Zertifikate und Schlüsseldateien, extrahiert ihre Inhalte, fügt sie der Kopie der Basiskonfigurationsdatei an und exportiert alle Inhalte in eine neue Client-Konfigurationsdatei.
Das bedeutet, dass alle erforderlichen Informationen an einem Ort gespeichert werden und die Konfiguration des Clients, die Zertifikate und Schlüsseldateien nicht getrennt verwaltet werden müssen.
Der Vorteil besteht darin, dass Sie, falls Sie je einmal einen Client hinzufügen müssen, ganz einfach dieses Skript ausführen können, um rasch die Konfigurationsdatei zu erstellen. Dabei wird sichergestellt, dass alle wichtigen Informationen an einem einzigen, einfach zugänglichen Ort gespeichert werden.
Bitte beachten Sie, dass bei jedem Hinzufügen eines neuen Clients neue Schlüssel und Zertifikate für ihn generiert werden müssen, bevor Sie dieses Skript ausführen und die entsprechende Konfigurationsdatei generieren können.
Im nächsten Schritt können Sie die Verwendung dieses Skripts üben.
Schritt 9 - Generieren von Client-Konfigurationen
Durch Befolgung der Anweisungen im Leitfaden haben Sie in Schritt 4 ein Client-Zertifikat und einen Schlüssel namens client1.crt bzw. client1.key erstellt. Sie können eine Konfigurationsdatei für diese Anmeldedaten generieren, indem Sie zum Verzeichnis ~ / client-configs wechseln und das am Ende des vorherigen Schrittes erstellte Skript ausführen:
Dadurch wird eine Datei namens client1.ovpn in Ihrem Verzeichnis ~ / client-configs / files erstellt:
Sie müssen diese Datei auf das Gerät übertragen, das Sie als Client verwenden wollen.
Das könnte beispielsweise Ihr lokaler Computer oder ein mobiles Gerät sein.
Die genauen Anwendungen, die zur Durchführung dieser Übertragung eingesetzt werden, hängen zwar vom Betriebssystem Ihres Geräts und Ihren persönlichen Präferenzen ab, aber die Verwendung von SFTP (SSH File Transfer Protocol) oder SCP (Secure Copy) im Backend ist ein zuverlässiges und sicheres Verfahren.
Damit werden die VPN-Authentifizierungsdateien Ihres Clients über eine verschlüsselte Verbindung übertragen.
Hier ist ein SFTP-Befehl, in dem beispielsweise < ^ > client1.ovpn < ^ > verwendet wird, was vom lokalen Computer (macOS oder Linux) ausgeführt werden kann.
Damit wird die Datei .ovpn in Ihrem Stammverzeichnis abgelegt:
Hier sind einige Tools und Tutorials für die sichere Übertragung von Dateien vom Server auf einen lokalen Computer:
WinSCP
Nutzung von SFTP zur sicheren Übertragung von Dateien mit einem Remote-Server
Nutzung von Filezilla zur Übertragung und Verwaltung von Dateien auf Ihrem VPS
Schritt 10 - Installieren der Client-Konfiguration
Dieser Abschnitt behandelt die Installation eines Client-VPN-Profils auf Windows, macOS, Linux, iOS und Android.
Keine dieser Client-Anleitungen bauen aufeinander auf. Daher können Sie je nach verwendetem Gerät direkt zur entsprechenden Anleitung navigieren.
Die OpenVPN-Verbindung hat den gleichen Namen wie die von Ihnen benannte Datei .ovpn.
In diesem Tutorial bedeutet dies, dass die Verbindung den Namen client1.ovpn erhält, was sich mit der von Ihnen erzeugten ersten Client-Datei deckt.
Windows
Installation
Laden Sie die OpenVPN-Client-Anwendung für Windows von der OpenVPN-Downloadseite herunter.
Wählen Sie das entsprechende Installationsprogramm für Ihre Windows-Version aus.
< $> Anmerkung label Anmerkung OpenVPN benötigt für die Installation Administratorrechte.
Nach der Installation von OpenVPN kopieren Sie die Datei .ovpn zu:
Beim Starten von OpenVPN sieht es automatisch das Profil und stellt es zur Verfügung.
Sie müssen OpenVPN bei jedem Einsatz, auch durch Administratorkonten, als Administrator ausführen.
Wenn Sie nicht bei jeder Verwendung des VPN mit der rechten Maustaste klicken und Als Administrator ausführen auswählen möchten, müssen Sie von einem Administrator-Konto aus eine Voreinstellung vornehmen.
Das bedeutet auch, dass Standardbenutzer zur Verwendung von OpenVPN das Passwort des Administrators eingeben müssen.
Andererseits können Standardbenutzer nur dann eine korrekte Verbindung mit dem Server herstellen, wenn die OpenVPN-Anwendung auf dem Client Administrationsrechte hat. Somit sind die höheren Berechtigungen notwendig.
Um die OpenVPN-Anwendung so einzustellen, dass sie immer als Administrator ausgeführt wird, klicken Sie auf das entsprechende Verknüpfungssymbol und gehen zu Eigenschaften.
Klicken Sie im unteren Teil der Registerkarte Kompatibilität auf die Schaltfläche Einstellungen für alle Benutzer ändern.
Aktivieren Sie im neuen Fenster Programm als Administrator ausführen.
Verbindungsherstellung
Jedes Mal, wenn Sie die OpenVPN-GUI starten, fragt Windows Sie, ob das Programm Änderungen an Ihrem Computer vornehmen darf.
Klicken Sie auf Ja.
Durch das Starten der OpenVPN-Client-Anwendung wird nur das Applet in die Taskleiste eingefügt, damit Sie je nach Bedarf die Verbindung mit dem VPN herstellen und trennen können; es stellt die VPN-Verbindung nicht tatsächlich her.
Sobald OpenVPN gestartet wird, stellen Sie eine Verbindung her, indem Sie zum Taskleisten-Applet navigieren und mit der rechten Maustaste auf das OpenVPN-Applet-Symbol klicken.
Dadurch wird das Kontextmenü geöffnet.
Wählen Sie client1 oben im Menü (hierbei handelt es sich um Ihr Client1.ovpn-Profil) und anschließend Verbinden.
Ein Statusfenster wird geöffnet und zeigt die Protokollausgabe an, während die Verbindung hergestellt wird. Sobald der Client verbunden ist, erscheint eine Meldung.
Trennen Sie die Verbindung mit dem VPN in der gleichen Weise: Gehen Sie zum Taskleisten-Applet, klicken Sie mit der rechten Maustaste auf das OpenVPN-Applet-Symbol, wählen Sie das Client-Profil und klicken Sie auf Verbindung trennen.
macOS
Tunnelblick ist ein kostenloser Open-Source-OpenVPN-Client für macOS.
Sie können das neueste Datenträgerabbild von der Tunnelblick-Downloadseite herunterladen.
Doppelklicken Sie auf die heruntergeladene Datei .dmg und befolgen Sie die Aufforderungen zur Installation.
Am Ende des Installationsprozesses fragt Tunnelblick Sie, ob Sie Konfigurationsdateien haben.
Antworten Sie Ich habe Konfigurationsdateien und lassen Sie Tunnelblick den Vorgang beenden.
Öffnen Sie ein Finder-Fenster und doppelklicken Sie auf client1.ovpn.
Tunnelblick installiert das Client-Profil.
Administratorrechte sind erforderlich.
Starten Sie Tunnelblick, indem Sie auf das Tunnelblick-Symbol im Ordner Anwendungen doppelklicken.
Nach dem Starten von Tunnelblick befindet sich oben rechts am Bildschirm in der Menüleiste ein Tunnelblick-Symbol zur Steuerung von Verbindungen.
Klicken Sie auf das Symbol und dann auf den Menüpunkt Connect client1, um die VPN-Verbindung herzustellen.
Linux
Wenn Sie Linux verwenden, gibt es eine Vielzahl von Tools, die Sie je nach Distribution verwenden können.
Ihre Desktop-Umgebung oder Ihr Fenstermanager enthält möglicherweise auch Verbindungs-Utilitys.
Die universellste Art der Verbindungsherstellung besteht jedoch darin, einfach die OpenVPN-Software zu verwenden.
Unter Ubuntu oder Debian können Sie sie wie auf dem Server installieren, indem Sie Folgendes eingeben:
Unter CentOS können Sie die EPEL-Repositorys aktivieren und dann installieren, indem Sie Folgendes eingeben:
Konfiguration
Überprüfen Sie, ob Ihre Distribution ein Skript / etc / openvpn / update-resolv-conf enthält:
Bearbeiten Sie als Nächstes die von Ihnen übertragene OpenVPN-Client-Konfigurationsdatei:
Wenn Sie eine Datei update-resolv-conf finden konnten, kommentieren Sie die drei Zeilen aus, die Sie hinzugefügt haben, um die DNS-Einstellungen anzupassen:
Wenn Sie CentOS verwenden, ändern Sie die Anweisung group von nogroup auf nobody, um sie an die verfügbaren Gruppen der Distribution anzugleichen:
Speichern und schließen Sie die Datei.
Jetzt können Sie eine Verbindung zum VPN herstellen, indem Sie mit dem Befehl openvpn auf die Client-Konfigurationsdatei zeigen:
Dies sollte Sie mit Ihrem VPN verbinden.
iOS
Suchen Sie im iTunes App Store nach OpenVPN Connect, der offiziellen iOS OpenVPN-Client-Anwendung, und installieren Sie sie.
Um Ihre iOS-Client-Konfiguration auf das Gerät zu übertragen, schließen Sie es direkt an einen Computer an.
Der Vorgang zum Abschließen der Übertragung mit iTunes wird hier beschrieben.
Öffnen Sie iTunes auf dem Computer und klicken Sie auf iPhone > Apps.
Scrollen Sie nach unten zum Abschnitt File Sharing und klicken Sie auf die OpenVPN-App. Das leere Fenster auf der rechten Seite, OpenVPN-Dokumente, dient zum Freigeben von Dateien.
Ziehen Sie die Datei .ovpn in das OpenVPN-Dokumentfenster.
iTunes zeigt das VPN-Profil, das auf dem iPhone zum Laden bereit ist
Starten Sie jetzt die OpenVPN-App auf dem iPhone.
Sie erhalten eine Benachrichtigung, dass ein neues Profil zum Import bereit ist.
Tippen Sie auf das grüne Pluszeichen, um es zu importieren.
Die OpenVPN iOS-App zeigt ein neues Profil an, das zum Import bereit ist
OpenVPN kann jetzt mit dem neuen Profil verwendet werden.
Starten Sie die Verbindung, indem Sie die Taste Connect in die Position On schieben.
Trennen Sie die Verbindung, indem Sie dieselbe Taste in die Position Off schieben.
< $> note label Anmerkung Der VPN-Switch unter Einstellungen kann nicht zur Verbindung mit dem VPN verwendet werden.
Wenn Sie es versuchen, erhalten Sie eine Mitteilung, die Verbindung nur mit der OpenVPN-App herzustellen. < $>
Die mit dem VPN verbundene OpenVPN iOS-App
Android
Öffnen Sie den Google Play Store.
Suchen und installieren Sie Android OpenVPN Connect, die offizielle Android OpenVPN-Client-Anwendung.
Sie können das Profil .ovpn übertragen, indem Sie das Android-Gerät per USB an Ihren Computer anschließen und die Datei kopieren.
Wenn Sie einen SD-Kartenleser haben, können Sie auch die SD-Karte des Geräts entfernen, das Profil auf sie kopieren und dann die Karte wieder in das Android-Gerät einlegen.
Starten Sie die OpenVPN-App und tippen Sie auf das Menü, um das Profil zu importieren.
Die Menüauswahl für den Import des Profils in der OpenVPN-App für Android
Navigieren Sie dann zum Speicherort des Profils (im Screenshot wird / sdcard / Download / verwendet), und wählen Sie die Datei aus.
Die App merkt sich, dass das Profil importiert wurde.
Die OpenVPN-App für Android, die das zu importierende VPN-Profil auswählt
Um eine Verbindung herzustellen, tippen Sie einfach auf die Schaltfläche Verbinden.
Sie werden gefragt, ob Sie der OpenVPN-Anwendung vertrauen.
Wählen Sie OK, um die Verbindung herzustellen.
Um die Verbindung zum VPN zu trennen, kehren Sie zur OpenVPN-App zurück und wählen Sie Trennen.
Die OpenVPN-App für Android ist zur Herstellung einer Verbindung mit dem VPN bereit
Schritt 11 - Testen Ihrer VPN-Verbindung (optional)
< $> Anmerkung Anmerkung: Diese Methode zum Testen Ihrer VPN-Verbindung funktioniert nur, wenn Sie sich in Schritt 5 dafür entschieden haben, den gesamten Verkehr durch das VPN zu leiten. < $>
Sobald alles installiert ist, wird durch eine einfache Überprüfung bestätigt, dass alles ordnungsgemäß funktioniert.
Öffnen Sie einen Browser, ohne dass eine VPN-Verbindung aktiviert ist, und rufen Sie DNSLeakTest auf.
Die Site gibt die von Ihrem Internetdienstanbieter zugewiesene IP-Adresse zurück und zeigt, wie Sie dem Rest der Welt erscheinen.
Um Ihre DNS-Einstellungen über dieselbe Website zu überprüfen, klicken Sie auf Erweiterter Test. Dann erfahren Sie, welche DNS-Server Sie verwenden.
Verbinden Sie nun den OpenVPN-Client mit dem VPN Ihres Droplets und aktualisieren Sie den Browser.
Jetzt sollte eine völlig andere IP-Adresse (die Ihres VPN-Servers) angezeigt werden. So sehen Sie für die Welt aus.
Erneut werden mit Erweiterter Test von DNSLeakTest Ihre DNS-Einstellungen überprüft, und es wird bestätigt, dass Sie jetzt die von Ihrem VPN per Push übertragene DSN-Resolver verwenden.
Schritt 12 - Sperren von Client-Zertifikaten
Gelegentlich müssen Sie möglicherweise ein Client-Zertifikat sperren, um den weiteren Zugriff auf den OpenVPN-Server zu verhindern.
Navigieren Sie dazu zum EasyRSA-Verzeichnis auf Ihrem CA-Computer:
Führen Sie als Nächstes das Skript easyrsa mit der Option revoke aus, gefolgt vom Namen des zu sperrenden Clients:
Sie werden dazu aufgefordert, das Sperren durch Eingabe von Ja zu bestätigen:
Nach Bestätigung der Aktion sperrt die Zertifizierungsstelle das Client-Zertifikat vollständig.
Ihr OpenVPN-Server kann derzeit jedoch nicht prüfen, ob Zertifikate von Clients gesperrt wurden und ob ein Client weiterhin Zugriff auf das VPN hat.
Um dies zu korrigieren, erstellen Sie eine Zertifikatsperrliste (Certificate Revocation List, CRL) auf Ihrem CA-Computer:
Dadurch wird eine Datei namens crl.pem. crl.pem generiert.
Übertragen Sie diese Datei sicher auf Ihren OpenVPN-Server:
Kopieren Sie diese Datei auf Ihrem OpenVPN-Server in Ihr Verzeichnis / etc / openvpn /:
Öffnen Sie als Nächstes die OpenVPN-Serverkonfigurationsdatei:
Fügen Sie am Ende der Datei die Option crl-verify hinzu, mit der der OpenVPN-Server angewiesen wird, die Zertifikatssperrliste zu überprüfen, die wir bei jedem Verbindungsversuch erstellt haben:
Starten Sie zum Abschluss OpenVPN neu, um die Zertifikatsperre zu implementieren:
Der Client sollte mit den alten Anmeldedaten keine Verbindung mehr zum Server herstellen können.
Um weitere Clients zu sperren, gehen Sie wie folgt vor:
Sperren Sie das Zertifikat mit dem Befehl. / easyrsa revoke < ^ > client _ name < ^ >
Generieren Sie eine neue CRL
Übertragen Sie die neue Datei crl.pem auf Ihren OpenVPN-Server und kopieren Sie sie in das Verzeichnis / etc / openvpn, um die alte Liste zu überschreiben.
Starten Sie den OpenVPN-Dienst neu.
Mit diesem Vorgang können Sie alle zuvor für Ihren Server ausgestellten Zertifikate sperren.
Sie sind jetzt sicher im Internet unterwegs und schützen Ihre Identität, Ihren Standort und Ihren Datenverkehr vor Schnüfflern und Zensoren.
Um weitere Clients zu konfigurieren, müssen Sie nur für jedes zusätzliche Gerät die Schritte 4 und 9-11 ausführen.
Um den Zugriff auf Clients zu sperren, führen Sie einfach Schritt 12 aus.
So konfigurieren Sie ein Galera-Cluster mit MySQL auf Ubuntu 18.04-Servern
3324
Der Autor wählte den Free and Open Source Fund, um eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
Clustering fügt Ihrer Datenbank eine hohe Verfügbarkeit hinzu, indem Änderungen auf verschiedene Server verteilt werden.
Falls eine der Instanzen ausfällt, sind andere schnell verfügbar, um den Dienst fortzusetzen.
Cluster gibt es in zwei allgemeinen Konfigurationen: aktiv-passiv und aktiv-aktiv.
In Aktiv-Passiv-Clustern werden alle Schreibvorgänge auf einem einzigen aktiven Server ausgeführt und dann auf einen oder mehrere passive Server kopiert, die nur im Falle eines Ausfalls des aktiven Servers übernehmen.
Einige Aktiv-Passiv-Cluster ermöglichen auch SELECT-Operationen auf passiven Knoten.
In einem Aktiv-Aktiv-Cluster wird jeder Knoten gelesen und geschrieben, und eine Änderung an einem Knoten wird auf alle repliziert.
MySQL ist ein relationales Open-Source-Datenbankmanagementsystem, das eine gängige Wahl für SQL-Datenbanken ist.
Galera ist eine Datenbank-Clustering-Lösung, die es Ihnen ermöglicht, Multi-Master-Cluster durch synchrone Replikation einzurichten.
Galera kümmert sich automatisch darum, die Daten auf verschiedenen Knoten synchron zu halten, während Sie Lese- und Schreibabfragen an jeden beliebigen Knoten im Cluster senden können.
Auf der offiziellen Dokumentationsseite können Sie mehr über Galera erfahren.
In dieser Anleitung werden Sie einen Aktiv-Aktiv MySQL Galera-Cluster konfigurieren.
Zu Demonstrationszwecken werden Sie drei Ubuntu 18.04 Droplets konfigurieren und testen, die als Knoten im Cluster fungieren werden.
Diese Anzahl von Knoten ist der kleinste konfigurierbare Cluster.
Um mitzumachen, benötigen Sie zusätzlich zu dem Folgenden ein DigitalOcean-Konto:
Drei Ubuntu 18.04 Droplets mit aktiviertem privaten Netzwerk, jedes mit einem Nicht-Root-Benutzer mit sudo-Berechtigungen.
Um ein privates Netzwerk auf den drei Droplets einzurichten, folgen Sie unserer Schnellstartanleitung für private Netzwerke.
Hilfe beim Einrichten eines Benutzers ohne Root-Berechtigung und mit sudo-Berechtigungen erhalten Sie in unserem Tutorial Ersteinrichtung des Servers mit Ubuntu 18.04.
Die Schritte in diesem Tutorial wurden zwar für DigitalOcean-Droplets geschrieben und mit ihnen getestet, aber viele von ihnen sind auch auf Nicht-DigitalOcean-Server mit aktivierter privater Vernetzung anwendbar.
Schritt 1 - Hinzufügen der MySQL-Repositories zu allen Servern
In diesem Schritt fügen Sie die relevanten MySQL- und Galera-Paket-Repositories zu jedem Ihrer drei Server hinzu, damit Sie die richtige Version von MySQL und Galera, die in diesem Tutorial verwendet wird, installieren können.
< $> note Anmerkung: Codership, das Unternehmen hinter Galera Cluster, pflegt das Galera Repository, aber seien Sie sich bewusst, dass nicht alle externen Repositories zuverlässig sind.
Stellen Sie sicher, dass Sie nur von vertrauenswürdigen Quellen installieren.
In diesem Tutorial werden Sie MySQL Version 5.7 verwenden.
Als erstes fügen Sie das externe Ubuntu-Repository, das vom Galera-Projekt verwaltet wird, zu allen drei Servern hinzu.
Sobald die Repositories auf allen drei Servern aktualisiert sind, können Sie MySQL zusammen mit Galera installieren.
Zuerst fügen Sie auf allen drei Servern den Galera-Repository-Schlüssel mit dem Befehl apt-key hinzu, den der APT-Paketmanager zur Überprüfung der Authentizität des Pakets verwendet:
Nach einigen Sekunden erhalten Sie die folgende Ausgabe:
Sobald Sie den vertrauenswürdigen Schlüssel in der Datenbank jedes Servers haben, können Sie die Repositories hinzufügen.
Dazu erstellen Sie auf jedem Server eine neue Datei namens galera.list im Verzeichnis / etc / apt / sources.list.d /:
Fügen Sie im Texteditor die folgenden Zeilen hinzu, die dem APT-Paketmanager die entsprechenden Repositories zur Verfügung stellen:
Speichern und schließen Sie die Dateien auf jedem Server (drücken Sie STRG + X, Y, dann EINGABE).
Die Codership-Repositories sind nun für alle drei Server verfügbar.
Es ist jedoch wichtig, dass Sie apt. anweisen, die Codership-Repositories anderen vorzuziehen, um sicherzustellen, dass die gepatchten Versionen der Software installiert werden, die zur Erstellung eines Galera-Clusters benötigt werden.
Dazu erstellen Sie eine weitere neue Datei namens galera.pref im Verzeichnis / etc / apt / preferences.d / jedes Servers:
Fügen Sie in dem Texteditor die folgende Zeilen hinzu:
Speichern und schließen Sie diese Datei und führen Sie dann den folgenden Befehl auf jedem Server aus, um Paketmanifeste aus den neuen Repositories aufzunehmen:
Nachdem Sie nun erfolgreich das Paket-Repository auf allen drei Servern hinzugefügt haben, können Sie MySQL im nächsten Abschnitt installieren.
Schritt 2 - Installation von MySQL auf allen Servern
In diesem Schritt werden Sie das MySQL-Paket auf Ihren drei Servern installieren.
Führen Sie den folgenden Befehl auf allen drei Servern aus, um eine für die Arbeit mit Galera gepatchte Version von MySQL sowie das Galera-Paket zu installieren.
Sie werden zur Bestätigung aufgefordert, ob Sie mit der Installation fortfahren möchten.
Geben Sie Y ein, um mit der Installation fortzufahren.
Während der Installation werden Sie auch aufgefordert, ein Passwort für den MySQL-Administrationsbenutzer festzulegen.
Legen Sie ein sicheres Passwort fest und drücken Sie EINGABE, um fortzufahren.
Sobald MySQL installiert ist, werden Sie das Standard-AppArmor-Profil deaktivieren, um sicherzustellen, dass Galera gemäß der offiziellen Galera-Dokumentation einwandfrei funktioniert.
AppArmor ist ein Kernel-Modul für Linux, das Zugriffskontrollfunktionen für Dienste über Sicherheitsprofile bietet.
Deaktivieren Sie AppArmor, indem Sie auf jedem Server Folgendes ausführen:
Dieser Befehl fügt einen symbolischen Link des MySQL-Profils in das Verzeichnis disable ein, wodurch das Profil beim Booten deaktiviert wird.
Führen Sie dann den folgenden Befehl aus, um die MySQL-Definition zu entfernen, die bereits in den Kernel geladen wurde.
Wenn Sie MySQL installiert und das AppArmor-Profil auf Ihrem ersten Server deaktiviert haben, wiederholen Sie diese Schritte für Ihre beiden anderen Server.
Nachdem Sie nun MySQL erfolgreich auf jedem der drei Server installiert haben, können Sie mit dem Konfigurationsschritt im nächsten Abschnitt fortfahren.
Schritt 3 - Konfigurieren des ersten Knotens
In diesem Schritt werden Sie Ihren ersten Knoten konfigurieren.
Jeder Knoten im Cluster muss eine nahezu identische Konfiguration haben.
Aus diesem Grund werden Sie die gesamte Konfiguration auf Ihrem ersten Rechner vornehmen und sie dann auf die anderen Knoten kopieren.
Standardmäßig ist MySQL so konfiguriert, dass das Verzeichnis / etc / mysql / conf.d überprüft wird, um zusätzliche Konfigurationseinstellungen aus Dateien mit der Endung .cnf zu erhalten.
Erstellen Sie auf Ihrem ersten Server in diesem Verzeichnis eine Datei mit allen Ihren cluster-spezifischen Anweisungen:
Fügen Sie die folgende Konfiguration in die Datei ein.
Die Konfiguration gibt verschiedene Cluster-Optionen, Details über den aktuellen Server und die anderen Server im Cluster sowie replikationsbezogene Einstellungen an.
Beachten Sie, dass die IP-Adressen in der Konfiguration die privaten Adressen Ihrer jeweiligen Server sind; ersetzen Sie die markierten Zeilen durch die entsprechenden IP-Adressen.
Im ersten Abschnitt werden die MySQL-Einstellungen geändert oder neu festgelegt, damit der Cluster korrekt arbeiten kann.
Zum Beispiel arbeitet Galera nicht mit MyISAM oder ähnlichen nicht-transaktionalen Speicher-Engines, und mysqld darf nicht an die IP-Adresse für localhost gebunden sein.
Sie können sich über die Einstellungen auf der Galera Cluster System-Konfigurationsseite näher informieren.
Der Abschnitt "Galera Provider Configuration" konfiguriert die MySQL-Komponenten, die eine WriteSet-Replikations-API bereitstellen.
Das bedeutet in Ihrem Fall Galera, da Galera ein wsrep (WriteSet Replication) Anbieter ist.
Sie geben die allgemeinen Parameter zur Konfiguration der anfänglichen Replikationsumgebung an.
Dies erfordert keine Anpassung, aber Sie können mehr über die Galera-Konfigurationsoptionen in der Dokumentation erfahren.
Der Abschnitt "Galera Cluster Configuration" definiert den Cluster, identifiziert die Cluster-Mitglieder anhand der IP-Adresse oder des auflösbaren Domänennamens und erstellt einen Namen für den Cluster, um sicherzustellen, dass die Mitglieder der richtigen Gruppe beitreten.
Sie können den wsrep _ cluster _ name in etwas aussagekräftigeres als test _ cluster ändern oder ihn so belassen, wie er ist, aber Sie müssen wsrep _ cluster _ address mit den privaten IP-Adressen Ihrer drei Server aktualisieren.
Der Abschnitt Galera Synchronization Configuration definiert, wie der Cluster zwischen den Mitgliedern kommunizieren und Daten synchronisieren wird.
Dies wird nur für den Zustandstransfer verwendet, der stattfindet, wenn ein Knoten online geht.
Verwenden Sie für Ihre Ersteinrichtung rsync, da es allgemein verfügbar ist und das tut, was Sie momentan benötigen.
Der Abschnitt Galera Node Configuration verdeutlicht die IP-Adresse und den Namen des aktuellen Servers.
Dies ist hilfreich, wenn versucht wird, Probleme in den Protokollen zu diagnostizieren und jeden Server auf mehrere Arten zu referenzieren.
Die wsrep _ node _ address muss mit der Adresse des Rechners, auf dem Sie sich befinden, übereinstimmen, aber Sie können einen beliebigen Namen wählen, um den Knoten in den Protokolldateien zu identifizieren.
Wenn Sie mit Ihrer Cluster-Konfigurationsdatei zufrieden sind, kopieren Sie den Inhalt in die Zwischenablage und speichern und schließen Sie die Datei.
Nachdem Sie Ihren ersten Knoten erfolgreich konfiguriert haben, können Sie im nächsten Abschnitt mit der Konfiguration der verbleibenden Knoten fortfahren.
Schritt 4 - Konfigurieren der verbleibenden Knoten
In diesem Schritt konfigurieren Sie die verbleibenden zwei Knoten.
Auf Ihrem zweiten Knoten öffnen Sie die Konfigurationsdatei:
Fügen Sie die Konfiguration ein, die Sie vom ersten Knoten kopiert haben, und aktualisieren Sie dann die Galera Node Configuration, um die IP-Adresse oder den auflösbaren Domänennamen für den spezifischen Knoten, den Sie einrichten, zu verwenden.
Aktualisieren Sie anschließend seinen Namen, den Sie beliebig festlegen können, um den Knoten in Ihren Logdateien zu identifizieren:
Speichern und schließen Sie die Datei.
Wenn Sie diese Schritte abgeschlossen haben, wiederholen Sie sie auf dem dritten Knoten.
Sie sind fast bereit, den Cluster zu starten, aber vergewissern Sie sich vorher, dass die entsprechenden Ports in Ihrer Firewall geöffnet sind.
Schritt 5 - Öffnen der Firewall auf jedem Server
In diesem Schritt werden Sie Ihre Firewall so konfigurieren, dass die für die Kommunikation zwischen den Knoten erforderlichen Ports offen sind.
Überprüfen Sie auf jedem Server den Status der Firewall, indem Sie ausführen:
In diesem Fall wird nur SSH durchgelassen:
Da in diesem Fall nur SSH-Datenverkehr erlaubt ist, müssen Sie Regeln für MySQL- und Galera-Datenverkehr hinzufügen.
Wenn Sie versuchen würden, den Cluster zu starten, würde er an diesen Firewall-Regeln scheitern.
Galera kann vier Ports verwenden:
3306 Für MySQL-Client-Verbindungen und State Snapshot Transfer, die die Methode mysqldump verwenden.
4567 Für Galera Cluster-Replikations-Datenverkehr.
Die Multicast-Replikation verwendet auf diesem Port sowohl sowohl den UDP- als auch den TCP-Transport.
4568 Für inkrementelle State Transfers.
4444 Für alle anderen State Snapshot-Transfers.
In diesem Beispiel öffnen Sie während der Einrichtung alle vier Ports.
Sobald Sie bestätigt haben, dass die Replikation funktioniert, sollten Sie alle Ports schließen, die Sie nicht tatsächlich verwenden, und den Datenverkehr auf die Server im Cluster beschränken.
Öffnen Sie die Ports mit den folgenden Befehlen:
< $> note Anmerkung: Abhängig davon, was sonst noch auf Ihren Servern ausgeführt wird, sollten Sie den Zugriff ggf. sofort einschränken.
Die Anleitung UFW-Grundlagen: Allgemeine Firewall-Regeln und -Befehle kann Ihnen dabei helfen.
Nachdem Sie Ihre Firewall auf dem ersten Knoten konfiguriert haben, erstellen Sie die gleichen Firewall-Einstellungen auf dem zweiten und dritten Knoten.
Nachdem Sie die Firewalls nun erfolgreich konfiguriert haben, können Sie den Cluster im nächsten Schritt starten.
Schritt 6 - Starten des Clusters
In diesem Schritt starten Sie Ihren MySQL Galera-Cluster.
Zuvor aktivieren Sie jedoch den MySQL-Dienst systemd, sodass MySQL automatisch gestartet wird, wenn der Server neu gestartet wird.
Start von MySQL beim Start auf allen drei Servern aktivieren
Verwenden Sie den folgenden Befehl auf allen drei Servern, um den Dienst MySQL systemd zu aktivieren:
Sie sehen die folgende Ausgabe, die zeigt, dass der Dienst erfolgreich mit der Liste der Startdienste verknüpft wurde:
Nachdem Sie nun mysql aktiviert haben, sodass er bei einem Systemstart auf allen Servern gestartet wird, sind Sie bereit, mit dem Hochfahren des Clusters fortzufahren.
Den ersten Knoten hochfahren
Um den ersten Knoten zu aktivieren, müssen Sie ein spezielles Startskript verwenden.
Entsprechend der Konfiguration Ihres Clusters wird jeder Knoten, der online geht, versuchen, sich mit mindestens einem anderen Knoten zu verbinden, der in seiner Datei galera.cnf angegeben ist, um seinen Anfangsstatus zu erhalten.
Ohne die Verwendung des Skripts mysqld _ bootstrap, das es systemd erlaubt, den Parameter --wsrep-new-cluster zu übergeben, würde ein normales systemctl start mysql fehlschlagen, weil keine Knoten laufen, mit denen der erste Knoten eine Verbindung herstellen könnte.
Führen Sie folgendes auf Ihrem ersten Server aus:
Dieser Befehl zeigt bei erfolgreicher Ausführung keine Ausgabe an.
Wenn dieses Skript erfolgreich ist, wird der Knoten als Teil des Clusters registriert, und Sie können ihn mit dem folgenden Befehl anzeigen:
Nachdem Sie Ihr Passwort eingegeben haben, sehen Sie die folgende Ausgabe, die anzeigt, dass es einen Knoten im Cluster gibt:
Auf den verbleibenden Knoten können Sie mysql normal starten.
Sie werden nach einem beliebigen Mitglied der Cluster-Liste suchen, das online ist, und wenn sie ein Mitglied finden, werden sie dem Cluster beitreten.
Den zweiten Knoten hochfahren
Jetzt können Sie den zweiten Knoten aktivieren.
Starten Sie mysql:
Bei erfolgreicher Ausführung wird keine Ausgabe angezeigt.
Die Größe Ihres Clusters nimmt mit jedem Knoten zu, der online geht:
Sie sehen die folgende Ausgabe, die anzeigt, dass der zweite Knoten dem Cluster beigetreten ist und dass es insgesamt zwei Knoten gibt.
Den dritten Knoten hochfahren
Jetzt ist es an der Zeit, den dritten Knoten zu aktivieren.
Führen Sie den folgenden Befehl aus, um die Clustergröße zu ermitteln:
Sie sehen die folgende Ausgabe, die anzeigt, dass der dritte Knoten dem Cluster beigetreten ist und dass die Gesamtzahl der Knoten im Cluster drei beträgt.
Zu diesem Zeitpunkt ist der gesamte Cluster online und kommuniziert erfolgreich.
Als Nächstes können Sie die funktionierende Einrichtung sicherstellen, indem Sie im folgenden Abschnitt die Replikation testen.
Schritt 7 - Testen der Replikation
Sie haben die Schritte bis zu diesem Punkt durchgeführt, so dass Ihr Cluster die Replikation von jedem beliebigen Knoten zu jedem anderen Knoten durchführen kann, die so genannte Aktiv-Aktiv-Replikation.
In diesem Schritt werden Sie einen Test durchführen und feststellen, ob die Replikation wie erwartet funktioniert.
In den ersten Knoten schreiben
Sie beginnen mit Datenbankänderungen in Ihrem ersten Knoten.
Die folgenden Befehle erzeugen eine Datenbank namens playground und eine Tabelle innerhalb dieser Datenbank namens equipment.
Im vorhergehenden Befehl wird mit der Anweisung CREATE DATABASE eine Datenbank mit dem Namen playground erstellt.
Die Anweisung CREATE erzeugt eine Tabelle mit dem Namen equipment innerhalb der Datenbank playground mit einer automatisch inkrementierenden Identifizierungsspalte namens id und anderen Spalten.
Die Spalte type, die Spalte quant und die Spalte color werden definiert, um den Typ, die Anzahl bzw. die Farbe der Ausstattung zu speichern.
Die Anweisung INSERT fügt einen Eintrag von Typ slide, Anzahl 2 und Farbe blue ein.
Sie haben nun einen Wert in Ihrer Tabelle.
Auf dem zweiten Knoten lesen und schreiben
Schauen Sie sich als nächstes den zweiten Knoten an, um zu überprüfen, ob die Replikation erfolgreich ist:
Die Daten, die Sie in den ersten Knoten eingegeben haben, werden hier auf dem zweiten Knoten sichtbar sein, um zu zeigen, dass die Replikation stattfindet:
Schreiben Sie von demselben Knoten aus Daten in den Cluster.
Auf dem dritten Knoten lesen und schreiben
Von dem dritten Knoten aus können Sie alle diese Daten durch erneute Abfrage der Tabelle lesen:
Sie sehen die folgende Ausgabe, die die beiden Zeilen zeigt:
Sie können von diesem Knoten aus wieder einen weiteren Wert hinzufügen:
Auf dem ersten Knoten lesen
Zurück auf dem ersten Knoten können Sie überprüfen, ob Ihre Daten überall verfügbar sind:
Sie sehen die folgende Ausgabe, die anzeigt, dass die Zeilen auf dem ersten Knoten verfügbar sind.
Sie haben nun erfolgreich überprüft, dass Sie in alle Knoten schreiben können und dass die Replikation ordnungsgemäß durchgeführt wird.
Jetzt haben Sie einen funktionierenden Galera-Testcluster mit drei Knoten konfiguriert.
Wenn Sie planen, einen Galera-Cluster in einer Produktionssituation zu verwenden, wird empfohlen, mit nicht weniger als fünf Knoten zu beginnen.
Vor dem produktiven Einsatz sollten Sie sich einige der anderen State Snapshot Transfer (sst) Agenten wie xtrabackup ansehen, die es Ihnen ermöglichen, neue Knoten schnell und ohne große Unterbrechungen Ihrer aktiven Knoten einzurichten.
Dies wirkt sich nicht auf die eigentliche Replikation aus, ist aber ein Problem bei der Initialisierung von Knoten.
Vielleicht sind Sie auch an anderen Clustering-Lösungen für MySQL interessiert. In diesem Fall können Sie sich unser Tutorial "Erstellen eines MySQL Cluster mit mehreren Knoten unter Ubuntu 18.04" ansehen.
Wenn Sie eine verwaltete Datenbanklösung ausprobieren möchten, lesen Sie unsere DigitalOcean Dokumentation für verwaltete Datenbanken.
So containerisieren Sie eine Laravel-Anwendung zur Entwicklung mit Docker Compose unter Ubuntu 18.04
3403
Die Containerisierung einer Anwendung bezeichnet den Anpassungsprozess einer Anwendung und ihrer Komponenten, um sie in einfachen Umgebungen ausführen zu können, die als Container bekannt sind.
Derartige Umgebungen sind isoliert und können gelöscht werden. Sie lassen sich zur Entwicklung, zum Test und zur Bereitstellung von Anwendungen zu Produktionszwecken nutzen.
In diesem Leitfaden verwenden wir Docker Compose, um eine Laravel-Anwendung für die Entwicklung zu containerisieren.
Wenn Sie fertig sind, haben Sie eine Testversion der Laravel-Anwendung, die auf drei separaten Service-Containern ausgeführt wird:
einen app-Dienst, der PHP7.4-FPM ausführt;
einen db-Dienst, der MySQL 5.7 ausführt;
einen nginx Dienst, der den app Dienst verwendet, um den PHP-Code zu parsen, bevor die Laravel-Anwendung dem Endbenutzer bereitgestellt wird.
Um einen gestrafften Entwicklungsprozess zu ermöglichen und das Debugging der Anwendung zu ermöglichen, halten wir Anwendungsdateien durch den Einsatz von gemeinsam genutzten Volumina synchron.
Auch zeigen wir, wie Sie docker-compose exec Befehle verwenden, um Composer und Artisan auf dem app Container auszuführen.
Zugriff auf einen lokalen Ubuntu-18.04-Rechner oder einen Entwicklungsserver als Benutzer ohne Rootberechtigung und mit sudo-Privilegien.
Wenn Sie einen Remote-Server verwenden, ist es ratsam, eine aktive Firewall installiert zu haben.
Um diese einzurichten, beziehen Sie sich auf unseren Leitfaden zur Ersteinrichtung des Servers unter Ubuntu 18.04.
Docker muss auf Ihrem Server installiert sein. Befolgen Sie dazu die Schritte 1 und 2 in So installieren und verwenden Sie Docker unter Ubuntu 18.04.
Docker Compose muss auf Ihrem Server installiert sein. Befolgen Sie dazu Schritt 1 in So installieren Sie Docker Compose unter Ubuntu 18.04.
Schritt 1 - Erhalt der Demo-Anwendung
Zu Beginn rufen wir die Demoversion der Laravel-Anwendung aus dem Github Repository ab.
Wir sind an dem tutorial-01 Zweig interessiert, der die Laravel-Standardanwendung enthält, die wir im ersten Leitfaden dieser Serie erstellt haben.
Um den mit diesem Tutorial kompatiblen Anwendungscode zu bekommen, laden Sie wie folgt tutorial-1.0.1 in Ihr Stammverzeichnis herunter:
Den unzip-Befehl brauchen wir, um den Anwendungscode zu dekomprimieren.
Wenn Sie dieses Paket noch nicht installiert haben, sollten Sie es jetzt wie folgt tun:
Dekomprimieren Sie nun den Inhalt der Anwendung und benennen Sie das entpackte Verzeichnis für leichteren Zugriff um:
Navigieren Sie zum Verzeichnis travellist-demo:
Im nächsten Schritt erstellen wir eine .env-Konfigurationsdatei, um die Anwendung einzurichten.
Schritt 2 - Einrichten der .env-Datei der Anwendung
Die Laravel-Konfigurationsdateien befinden sich im Verzeichnis config, das Sie im Stammverzeichnis der Anwendung finden.
Zusätzlich wird eine .env Datei verwendet, um eine umgebungsabhängige Konfiguration einzurichten, wie z. B. Anmeldedaten und Informationen, die sich zwischen Bereitstellungen ändern können.
Diese Datei ist nicht Teil der Revisionskontrolle.
< $> warning Warnung: Die Umgebungs-Konfigurationsdatei enthält sensible Informationen über Ihren Server, einschließlich Anmeldedaten zur Datenbank und Sicherheitsschlüssel.
Aus diesem Grund sollten Sie diese Datei nie öffentlich teilen.
Die Werte in der .env-Datei haben Vorrang vor den Werten, die in regelmäßigen Konfigurationsdateien festgelegt sind, die sich im Verzeichnis config befinden.
Jetzt erstellen wir eine neue .env-Datei, um die Konfigurationsoptionen für die Entwicklungsumgebung anzupassen, die wir einrichten.
Jetzt erstellen wir eine neue .env-Datei, um die Konfigurationsoptionen für die Entwicklungsumgebung anzupassen, die wir einrichten.
Laravel wird mit einer .env Musterdatei geliefert, die wir zur Erstellung unserer eigenen kopieren können:
Öffnen Sie diese Datei mit nano oder dem Texteditor Ihrer Wahl:
Die aktuelle .env-Datei aus der travellist Demo-Anwendung enthält Einstellungen zum Einsatz einer lokalen MySQL-Datenbank, wobei 127.0.0.1 der Datenbank-Host ist.
Wir müssen die Variable DB _ HOST aktualisieren, damit sie auf den Datenbankdienst verweist, den wir in unserer Docker-Umgebung erstellen.
In diesem Leitfaden nennen wir unseren Datenbankdienst db.
Ersetzen Sie also den aufgelisteten Wert von DB _ HOST durch den Datenbankdienstnamen:
Sie können den Datenbanknamen, den Benutzernamen und das Passwort auch nach Bedarf ändern.
Diese Variablen werden in einem späteren Schritt genutzt, wo wir die Datei docker-compose.yml einrichten, um unsere Dienste zu konfigurieren.
Speichern Sie die Datei, wenn die Bearbeitung abgeschlossen ist.
Wenn Sie nano verwendet haben, können Sie zur Bestätigung Ctrl + x (Strg + x), dann Y und Enter (Eingabetaste) drücken.
Schritt 3 - Einrichten des Dockerfiles der Anwendung
Obwohl unsere MySQL- und Nginx Dienste auf Standardbildern basieren, die wir aus dem Docker Hub erhalten, müssen wir trotzdem ein benutzerdefiniertes Bild für den Anwendungs-Container erstellen.
Dafür erstellen wir ein neues Dockerfile.
Unser travellist-Image basiert auf dem offiziellen PHP-Image php: 7.4-fpm von Docker Hub.
Über diese PHP-FPM-Umgebung hinaus installieren wir ein paar PHP-Extramodule und das Composer Abhängigkeitsmanagement-Tool.
Außerdem erstellen wir einen neuen Systembenutzer; dies ist notwendig, um artisan- und composer- Befehle während der Entwicklung der Anwendung auszuführen.
Die uid-Einstellung stellt sicher, dass der Benutzer im Container dieselbe UID wie der Systembenutzer auf Ihrem Host-Computer hat, auf dem Docker läuft.
Auf diese Weise werden alle von diesen Befehlen erstellten Dateien mit den richtigen Berechtigungen im Host repliziert.
Es bedeutet auch, dass Sie auf dem Host-Rechner den Code-Editor Ihrer Wahl verwenden können, um die Anwendung für die Container zu entwickeln.
Erstellen Sie ein neues Dockerfile mit:
Kopieren Sie den folgenden Inhalt in Ihr Dockerfile:
Vergessen Sie nicht, die Datei zu speichern, wenn Sie fertig sind.
Unsere Dockerfile beginnt, indem Sie das Basisbild definieren, das wir verwenden: php: 7.4-fpm.
Nach Installation von Systempaketen und PHP-Erweiterungen installieren wir Composer, indem wir den ausführbaren composer aus dem letzten offiziellen Image in unser eigenes Anwendungsimage kopieren.
Dann wird mit den Argumenten user und uid ein neuer Systembenutzer erstellt und eingerichtet. Diese Argumente wurden zu Beginn des Dockerfiles deklariert.
Diese Werte werden von Docker Compose zum Zeitpunkt des Build injiziert.
Schließlich legen wir das Standardarbeitsverzeichnis als / var / www fest und wechseln auf den neu erstellten Benutzer.
Somit gewährleisten Sie, dass Sie sich als regelmäßiger Benutzer verbinden und im richtigen Verzeichnis sind, wenn Sie composer- und artisan-Befehle im Anwendungs-Container ausführen.
Schritt 4 - Einrichten der Nginx-Konfiguration und der Datenbank-Dump-Dateien
Wenn Sie Entwicklungsumgebungen mit Docker Compose erstellen, müssen Sie die Konfigurations- oder Initialisierungsdateien häufig mit Dienst-Containern teilen, um diese Dienste einzurichten oder im Bootstrap-Verfahren zu laden.
Diese Vorgehensweise ermöglicht die Änderung der Konfigurationsdateien, um Ihre Umgebung während der Anwendungsentwicklung genau einzustellen.
Jetzt erstellen wir einen Ordner mit Dateien, die zur Konfiguration und Initialisierung unserer Dienst-Container verwendet werden.
Um Nginx einzurichten, teilen wir eine travellist.conf Datei, die festlegt, wie die Anwendung bereitgestellt wird.
Erstellen Sie den Ordner docker-compose / nginx wie folgt:
Öffnen Sie eine neue Datei namens travellist.conf in diesem Verzeichnis:
Kopieren Sie die folgende Nginx-Konfiguration in diese Datei:
Mit dieser Datei wird Nginx konfiguriert, um auf Port 80 zu lauschen und index.php als standardmäßige Index-Seite zu verwenden.
Damit wird der Dokumentenstamm auf / var / www / public festgelegt und dann Nginx so konfiguriert, dass er den app-Dienst auf Port 9000 verwendet, um * .php-Dateien zu verarbeiten.
Speichern und schließen Sie die Datei, wenn die Bearbeitung abgeschlossen ist.
Um die MySQL-Datenbank einzurichten, teilen wir einen Datenbank-Dump, der bei Initialisierung des Containers importiert wird.
Dies ist eine Eigenschaft, die vom MySQL 5.7-Image bereitgestellt wird, das wir in dem Container verwenden.
Erstellen Sie einen neuen Ordner für Ihre MySQL-Initialisierung im Ordner docker-compose:
Öffnen Sie eine neue .sql-Datei:
Der folgende MySQL-Dump basiert auf der Datenbank, die wir in unserem Leitfaden zu Laravel mit LEMP eingerichtet haben.
Damit wird eine neue Tabelle namens places erstellt.
Dann füllt er die Tabelle mit einem Satz von Musterstellen.
Fügen Sie den folgenden Code zur Datei hinzu:
Die Tabelle places enthält drei Felder: id, name und visited.
Das Feld visited ist ein Flag, das als Identifizierung der Stellen dient, die noch ausstehen.
Stellen Sie diese Musterstellen nach Belieben um oder setzen Sie neue hinzu.
Speichern und schließen Sie die Datei, wenn Sie fertig sind.
Damit ist die Einrichtung des Dockerfiles der Anwendung und der Konfigurationsdateien des Dienstes abgeschlossen.
Als Nächstes richten wir Docker Compose ein, damit es bei der Erstellung unserer Dienste diese Dateien verwendet.
Schritt 5 - Erstellen einer Multi-Container-Umgebung mit Docker Compose
Docker Compose ermöglicht es Ihnen, Multi-Container-Umgebungen für auf Docker laufende Anwendungen zu erstellen.
Er verwendet Dienst-Definitionen zum Aufbau voll anpassbarer Umgebungen mit mehreren Containern, die Netzwerke und Datenvolumes teilen können.
Damit wird eine nahtlose Integration zwischen Anwendungskomponenten möglich.
Um unsere Dienst-Definitionen einzurichten, erstellen wir eine neue Datei namens docker-compose.yml.
Normalerweise befindet sich diese Datei im Stamm des Anwendungsordners und definiert die containerisierte Umgebung, einschließlich der Standardimages, die Sie zum Aufbau Ihrer Container verwenden, und der Art und Weise, wie Ihre Dienste interagieren.
Wir definieren drei verschiedene Dienste in unserer docker-compose.yml Datei: app, db und nginx.
Der app-Dienst stellt ein Image mit der Bezeichnung travellist auf Basis des zuvor erstellten Dockerfiles zusammen.
Der durch diesen Dienst definierte Container führt einen php-fpm-Server aus, um PHP-Code zu parsen und die Ergebnisse an den nginx-Dienst zurückzusenden, der in einem separaten Container läuft.
Der mysql-Dienst definiert einen Container, der einen MySQL 5.7-Server ausführt.
Unsere Dienste teilen ein Brückennetzwerk namens travellist.
Die Anwendungsdateien werden in sowohl den app- als auch den nginx-Diensten über Bind-Bereitstellungen synchronisiert.
Bind-Bereitstellungen sind in Entwicklungsumgebungen nützlich, weil sie eine performante zweispurige Synchronisierung zwischen Host-Rechner und Containern ermöglichen.
Erstellen Sie eine neue docker-compose.yml-Datei im Stammverzeichnis der Anwendung:
Eine typische docker-compose.yml-Datei beginnt mit einer Versionsdefinition gefolgt von einem services-Knoten, in dem alle Dienste definiert sind.
Die geteilten Netzwerke werden normalerweise unten in der Datei definiert.
Kopieren Sie zu Beginn diesen Standardcode in Ihre docker-compose.yml-Datei:
Jetzt bearbeiten wir den services-Knoten, um die app-, db- und nginx-Dienste aufzunehmen.
Der app-Dienst
Der app-Dienst richtet einen Container namens travellist-app ein.
Basierend auf einem Dockerfile mit dem gleichen Pfad wie die docker-compose.yml-Datei baut er ein neues Docker-Image auf.
Das neue Image wird lokal unter dem Namen travellist gespeichert.
Obwohl sich der als Anwendung dienende Dokumentenstamm im nginx-Container befindet, sollten auch die Anwendungsdateien irgendwo im app-Container vorhanden sein, damit wir Befehlszeilenvorgänge mit dem Laravel Artisan-Tool ausführen können.
Kopieren Sie die folgende Dienst-Definition in Ihrem services-Knoten aus der docker-compose.yml-Datei:
Diese Einstellungen bewirken Folgendes:
build: Diese Konfiguration weist Docker Compose an, ein lokales Image für den app-Dienst zu erstellen, wobei es den angegebenen Pfad (context) und das Dockerfile für Anweisungen verwendet.
Die Argumente user und uid werden in das Dockerfile injiziert, um die Befehle zur Benutzereinrichtung zur Buildzeit anzupassen.
image: Der Name, der für das zusammengestellte Image verwendet wird.
container _ name: Richtet den Container-Namen für diesen Dienst ein.
restart: Es wird immer ein Neustart durchgeführt, es sei denn, der Dienst wird angehalten.
working _ dir: Richtet das Standardverzeichnis für diesen Dienst als / var / www ein.
volumes: Erstellt ein gemeinsam genutztes Volume, das den Inhalt des aktuellen Verzeichnisses auf / var / www im Container synchronisiert.
Beachten Sie, dass es sich nicht um Ihren Dokumentenstamm handelt, da dieser sich im nginx-Container befinden wird.
networks: Richtet diesen Dienst auf Nutzung eines Netzwerks namens travellist ein.
Der db-Dienst
Der db-Dienst verwendet ein vorab zusammengestelltes MySQL 5.7-Image von Docker Hub.
Da Docker Compose die .env-Variablendateien automatisch lädt, die sich im gleichen Verzeichnis wie die docker-compose.yml-Datei befinden, erhalten wir unsere Datenbankeinstellungen aus der Laravel .env-Datei, die wir im vorherigen Schritt erstellt haben.
Setzen Sie die folgende Dienst-Definition in Ihrem services-Knoten direkt hinter den app-Dienst:
image: Definiert das Docker-Image, das für diesen Container verwendet werden sollte.
In diesem Fall verwenden wir ein MySQL 5.7-Image von Docker Hub.
container _ name: Richtet den Container-Namen für diesen Dienst ein: travellist-db.
restart: Dieser Dienst wird immer neu gestartet, es sei denn, er wurde ausdrücklich angehalten.
environment: Definiert Umgebungsvariablen im neuen Container.
Wir verwenden Werte aus der Laravel .env-Datei, um unseren MySQL-Dienst einzurichten. Damit werden automatisch, basierend auf den bereitgestellten Umgebungsvariablen, eine neue Datenbank und ein Benutzer erstellt.
volumes: Erstellt ein Volume, um einen .sql Datenbank-Dump zu teilen, der zur Initialisierung der Anwendungsdatenbank verwendet wird.
Das MySQL-Image importiert automatisch .sql-Dateien, die im Verzeichnis / docker-entrypoint-initdb.d im Container abgelegt werden.
Der nginx-Dienst
Der nginx-Dienst verwendet ein vorab zusammengestelltes Nginx-Image auf Alpine, einer einfachen Linux-Distribution.
Damit wird ein Container namens travellist-nginx erstellt und die ports-Definition verwendet, um eine Umleitung von Port 8000 im Host-System auf Port 80 im Container zu schaffen.
Setzen Sie die folgende Dienst-Definition in Ihrem services-Knoten direkt hinter den db-Dienst:
In diesem Fall verwenden wir das Alpine Nginx 1.17-Image.
container _ name: Richtet den Container-Namen für diesen Dienst ein: travellist-nginx.
ports: Richtet eine Port-Umleitung ein, die den externen Zugriff über Port 8000 auf den Web-Server ermöglicht, der auf Port 80 im Container läuft.
volumes: Erstellt zwei gemeinsam genutzte Volumes.
Das erste wird den Inhalt aus dem aktuellen Verzeichnis in / var / www im Container synchronisieren.
Auf diese Weise werden bei lokalen Änderungen an den Anwendungsdateien diese schnell in der Anwendung widergespiegelt, die von Nginx im Container bereitgestellt wird.
Das zweite Volume stellt sicher, dass unsere Nginx-Konfigurationsdatei, die sich auf docker-compose / nginx / travellist.conf befindet, in den Nginx-Konfigurationsordner des Containers kopiert wird.
Fertiggestellte docker-compose.yml-Datei
So sieht unsere fertiggestellte docker-compose.yml-Datei aus:
Denken Sie daran, die Datei zu speichern, wenn Sie Ihre Bearbeitung abgeschlossen haben.
Schritt 6 - Ausführung der Anwendung mit Docker Compose
Jetzt verwenden wir docker-compose-Befehle, um das Anwendungsimage zu erstellen und die Dienste auszuführen, die wir in unserem Setup festgelegt haben.
Erstellen Sie das app-Image mit dem folgenden Befehl:
Dies kann einige Minuten dauern.
Sie werden eine Ausgabe sehen, die dieser ähnelt:
Wenn der Build abgeschlossen ist, können Sie die Umgebung im Hintergrundmodus ausführen:
Damit werden Ihre Container im Hintergrund ausgeführt.
Um Informationen über den Zustand Ihrer aktiven Dienste anzuzeigen, führen Sie Folgendes aus:
Die Ausgabe sieht dann so aus:
Jetzt ist Ihre Umgebung einsatzbereit, aber wir müssen noch ein paar Befehle ausführen, um das Setup der Anwendung abzuschließen.
Sie können den Befehl docker-compose exec verwenden, um Befehle in den Dienst-Containern auszuführen, wie z. B. ls -l, der detaillierte Informationen über Dateien im Anwendungsverzeichnis anzeigt:
Jetzt führen wir composer install aus, um die Anwendungsabhängigkeiten zu installieren:
Als Letztes vor dem Anwendungstest verbleibt die Generierung eines eindeutigen Anwendungsschlüssels mit dem artisan Laravel-Tool auf Befehlszeilenebene.
Dieser Schlüssel wird verwendet, um die Benutzersitzungen und andere sensible Daten zu verschlüsseln:
Gehen Sie jetzt in Ihren Browser und greifen Sie über Port 8000 auf den Domänenamen oder die IP-Adresse Ihres Servers zu:
Sie sehen in etwa folgende Seite:
Testversion der Laravel-Anwendung
Sie können den Befehl logs verwenden, um die von den Diensten generierten Protokolle zu überprüfen:
Wenn Sie Ihre Docker-Compose-Umgebung unter Beibehaltung des Zustandes aller Dienste anhalten möchten, führen Sie Folgendes aus:
So können Sie dann Ihre Dienste wiederaufnehmen:
Um Ihre Docker-Compose-Umgebung herunterzufahren und alle Container, Netzwerke und Volumes zu entfernen, führen Sie Folgendes aus:
Eine Übersicht aller Docker-Compose-Befehle finden Sie in der Docker Compose Befehlszeilenreferenz.
In diesem Leitfaden haben wir mit Docker Compose eine Docker-Umgebung mit drei Containern eingerichtet, um eine Infrastruktur in einer YAML-Datei festzulegen.
Ab jetzt können Sie in Ihrer Laravel-Anwendung arbeiten, ohne einen lokalen Webserver für die Entwicklung und Tests installieren und einrichten zu müssen.
Außerdem arbeiten Sie mit einer löschbaren Testumgebung, die leicht repliziert und verteilt werden kann, was bei der Anwendungsentwicklung und auch beim Wechsel in eine Produktivumgebung hilfreich sein kann.
So installieren Sie Apache Kafka unter Debian 10
3330
Apache Kafka ist ein beliebter Distributed Message Broker, der für die Verarbeitung großer Mengen von Echtzeitdaten ausgelegt ist. Ein Kafka-Cluster ist hochgradig skalierbar und fehlertolerant, hat bietet einen wesentlich höheren Durchsatz im Vergleich zu anderen Message Brokern wie ActiveMQ und RabbitMQ.
Obwohl er im Allgemeinen als Publish / Subsribe-Messaging-System verwendet wird, verwenden Ihn viele Organisationen auch für die Protokollaggregation, da er einen persistenten Speicher für veröffentlichte Nachrichten bietet.
Ein Publish / Subscribe-Messaging-System ermöglicht einem oder mehreren Erzeugern die Veröffentlichung von Nachrichten, ohne die Anzahl der Konsumenten oder die Art und Weise der Nachrichtenverarbeitung zu berücksichtigen.
Abonnierte Clients werden automatisch über Aktualisierungen und die Erstellung neuer Nachrichten informiert.
Dieses System ist effizienter und skalierbar als Systeme, bei denen Clients in regelmäßigen Abständen abfragen, ob neue Nachrichten verfügbar sind.
In diesem Tutorial werden Sie Apache Kafka 2.1.1 sicher auf einem Debian 10-Server installieren und konfigurieren und anschließend Ihre Einrichtung testen, indem Sie eine Nachricht Hello World erzeugen und konsumieren.
Sie werden dann wahlweise KafkaT installieren, um Kafka zu überwachen und einen Kafka-Mehrknoten-Cluster einzurichten.
Um dem Tutorial zu folgen, benötigen Sie Folgendes:
Einen Debian-10-Server mit mindestens 4 GB RAM und einem Benutzer ohne Root-Berechtigung und mit sudo-Privilegien.
Wenn Sie keinen Nicht-Root-Benutzer eingerichtet haben, folgen Sie den Schritten in unserem Leitfaden zur Ersteinrichtung von Debian 10.
Auf Ihrem Server installiertes OpenJDK 11.
Um diese Version zu installieren, folgen Sie den Anweisungen in Installieren von Java mit Apt unter Debian 10 zur Installation bestimmter Versionen von OpenJDK.
Kafka ist in Java geschrieben und benötigt daher eine JVM.
< $> note Hinweis: Installationen ohne 4GB RAM können zum Ausfall des Kafka-Dienstes führen, wobei die Java Virtual Machine (JVM) während des Starts eine Out Of Memory-Ausnahme auslöst.
Schritt 1 - Erstellen eines Benutzers für Kafka
Da Kafka Anfragen über ein Netzwerk verarbeiten kann, ist die Einrichtung eines dedizierten Benutzers für das Netzwerk eine bewährte Methode. Dies minimiert den Schaden auf Ihrem Debian-Rechner, sollte der Kafka-Server kompromittiert werden.
In diesem Schritt erstellen Sie den dedizierten Benutzer kafka.
Melden Sie sich als Benutzer ohne Root- und mit sudo-Berechtigung an und erstellen Sie mit dem Befehl useradd einen Benutzer namens kafka:
Das Flag -m stellt sicher, dass ein Home-Verzeichnis für den Benutzer erstellt wird.
Dieses Home-Verzeichnis, / home / kafka, wird als Ihr neues Arbeitsbereichsverzeichnis für die spätere Ausführung von Befehlen dienen.
Legen Sie das Passwort mit passwd fest:
Geben Sie das für diesen Benutzer gewünschte Passwort ein.
Fügen Sie als Nächstes den Benutzer kafka mit dem Befehl adduser zur Gruppe sudo hinzu, damit er über die erforderlichen Berechtigungen zur Installation der Abhängigkeiten von Kafka verfügt:
Ihr Benutzer kafka ist nun bereit.
Melden Sie sich mit su bei diesem Konto an:
Nachdem Sie nun den Kafka-spezifischen Benutzer erstellt haben, können Sie mit dem Herunterladen und Extrahieren der Kafka-Binärdateien fortfahren.
Schritt 2 - Herunterladen und Extrahieren der Kafka-Binärdateien
In diesem Schritt laden Sie die Kafka-Binärdateien herunter und extrahieren sie in spezielle Ordner im Home-Verzeichnis ihres Benutzers kafka.
Erstellen Sie zu Beginn ein Verzeichnis in / home / kafka namens Downloads, um Ihre Downloads zu speichern:
Installieren Sie als Nächstes curl unter Verwendung von apt-get, damit Sie Remote-Dateien herunterladen können:
Wenn Sie dazu aufgefordert werden, geben Sie Y ein, um den Download von curl zu bestätigen.
Sobald curl installiert ist, verwenden Sie es zum Herunterladen der Kafka-Binärdateien:
Erstellen Sie ein Verzeichnis namens kafka und wechseln Sie in dieses Verzeichnis.
Dies ist das Basisverzeichnis der Kafka-Installation:
Extrahieren Sie das von Ihnen heruntergeladene Archiv mit dem Befehl tar:
Sie haben das Flag --strip-1 angegeben, um sicherzustellen, dass der Inhalt des Archivs in ~ / kafka / selbst und nicht in ein anderes Verzeichnis innerhalb des Archivs extrahiert wird, wie z. B. ~ / kafka _ < ^ > 2.12-2.1.1 < ^ >.
Nachdem Sie die Binärdateien erfolgreich heruntergeladen und extrahiert haben, können Sie mit der Konfiguration von Kafka fortfahren, um das Löschen von Themen zu erlauben.
Schritt 3 - Konfigurieren des Kafka-Servers
Das Standardverhalten von Kafka erlaubt es uns nicht, ein Thema, eine Kategorie, eine Gruppe oder den Feed-Namen zu löschen, an die Nachrichten veröffentlicht werden können.
Um dies zu ändern, bearbeiten Sie die Konfigurationsdatei.
Die Konfigurationsoptionen von Kafka werden in server.properties angegeben.
Öffnen Sie diese Datei mit nano oder Ihrem bevorzugten Editor:
Fügen wir eine Einstellung hinzu, die es uns erlaubt, Kafka-Themen zu löschen.
Fügen Sie die folgende hervorgehobene Zeile am Ende der Datei hinzu:
Speichern Sie die Datei und beenden Sie nano.
Nachdem Sie Kafka konfiguriert haben, können Sie systemd Unit-Dateien erstellen, um Kafka beim Start auszuführen und zu aktivieren.
Schritt 4 - Erstellen von Systemd Unit-Dateien und Starten des Kafka-Servers
In diesem Abschnitt erstellen Sie systemd Unit-Dateien für den Kafka-Dienst.
Dies wird Ihnen bei der Durchführung allgemeiner Dienstaktionen wie dem Starten, Stoppen und Neustarten von Kafka in einer Weise helfen, die mit anderen Linux-Diensten konsistent ist.
ZooKeeper ist ein Dienst, den Kafka zur Verwaltung seines Clusterstatus und seiner Konfigurationen verwendet.
Er wird häufig in verteilten Systemen als integraler Bestandteil verwendet.
In diesem Tutorial werden Sie Zookeeper verwenden, um diese Aspekte von Kafka zu verwalten.
Wenn Sie mehr darüber erfahren möchten, besuchen Sie die offiziellen ZooKeeper Dokumentationen.
Erstellen Sie zuerst die Unit-Datei für zookeeper:
Geben Sie die folgende Unit-Definition in die Datei ein:
Der Abschnitt [Unit] legt fest, dass ZooKeeper eine Vernetzung erfordert und das Dateisystem bereit sein muss, bevor es gestartet werden kann.
Der Abschnitt [Service] legt fest, dass systemd die Shell-Dateien zookeeper-server-start.sh und zookeeper-server-start.sh zum Starten und Stoppen des Dienstes verwenden soll.
Er legt auch fest, dass ZooKeeper automatisch neu gestartet werden soll, wenn er abnormal endet.
Erstellen Sie als Nächstes die Dienstdatei systemd für kafka:
Der Abschnitt [Unit] legt fest, dass diese Unit-Datei von zookeeper.service abhängt.
Dadurch wird sichergestellt, dass zookeeper automatisch gestartet wird, wenn der Dienst kafka startet.
Der Abschnitt [Service] legt fest, dass systemd die Shell-Dateien kafka-server-start.sh und kafka-server-stop.sh zum Starten und Stoppen des Dienstes verwenden soll.
Er legt auch fest, dass ZooKeeper automatisch neu gestartet werden soll, wenn er abnormal endet.
Nachdem die Units definiert wurden, starten Sie Kafka mit dem folgenden Befehl:
Um sicherzustellen, dass der Server erfolgreich gestartet wurde, überprüfen Sie die Journalprotokolle für die Unit kafka.
Sie sehen eine Ausgabe, die der folgenden ähnelt:
Sie haben nun einen Kafka-Server, der Port 9092, den Standardport für Kafka, überwacht.
Sie haben den Dienst kafka gestartet, aber wenn Sie Ihren Server neu starten würden, würde der Dienst noch nicht automatisch gestartet.
Um kafka beim Serverstart zu aktivieren, führen Sie Folgendes aus:
Nachdem Sie die Dienste nun gestartet und aktiviert haben, ist es an der Zeit, die Installation zu überprüfen.
Schritt 5 - Testen der Installation
Veröffentlichen und konsumieren wir eine Nachricht Hello World, um sicherzustellen, dass sich der Kafka-Server korrekt verhält.
Die Veröffentlichung von Nachrichten in Kafka erfordert:
Einen Erzeuger, der die Veröffentlichung von Datensätze und Daten zu Themen ermöglicht.
Einen Konsumenten, der Nachrichten und Daten zu Themen liest.
Erstellen Sie zuerst ein Thema namens TutorialTopic, indem Sie Folgendes eingeben:
Sie können einen Erzeuger von der Befehlszeile aus mit dem Skript kafka-console-producer.sh erstellen.
Es erwartet den Hostnamen des Kafka-Servers, den Port und einen Themennamen als Argumente.
Veröffentlichen Sie den String Hello, World zum Thema TutorialTopic, indem Sie eingeben:
Das Flag --broker-list bestimmt die Liste der Nachrichten-Broker, an die die Nachricht gesendet werden soll, in diesem Fall localhost: 9092. --topic bezeichnet das Thema als TutorialTopic.
Als Nächstes können Sie mit dem Skript kafka-console-consumer.sh einen Kafka-Consumer erstellen.
Es erwartet den Hostnamen des ZooKeeper-Servers, den Port und einen Themennamen als Argumente.
Der folgende Befehl konsumiert Nachrichten von TutorialTopic.
Beachten Sie die Verwendung des Flags --from-beginning, das den Verbrauch von Nachrichten erlaubt, die vor dem Start des Konsumenten veröffentlicht wurden:
--bootstrap-server bietet eine Liste von Eintritten in den Kafka-Cluster.
In diesem Fall verwenden Sie localhost: 9092.
Sie sehen Hello, World auf Ihrem Terminal:
Das Skript wird kontinuierlich weiter ausgeführt und darauf warten, dass weitere Nachrichten zu dem Thema veröffentlicht werden.
Zögern Sie nicht, ein neues Terminal zu öffnen und Entwickler zu starten, um weitere Nachrichten zu veröffentlichen.
Sie sollten sie alle in der Ausgabe des Consumers sehen können.
Wenn Sie mehr über die Verwendung von Kafka erfahren möchten, lesen Sie die offizielle Kafka-Dokumentation.
Wenn Sie dem Testen fertig sind, drücken Sie STRG + C, um das Consumer-Skript zu stoppen.
Nachdem Sie die Installation getestet haben, können Sie mit der Installation von KafkaT fortfahren, um Ihren Kafka-Cluster besser zu verwalten.
Schritt 6 - Installieren von KafkaT (optional)
KafkaT ist ein Tool von Airbnb, das die Anzeige von Details über Ihren Kafka-Cluster und die Durchführung bestimmter administrativer Aufgaben von der Kommandozeile aus erleichtert.
Da es sich um ein Ruby Gem handelt, benötigen Sie Ruby, um es zu benutzen. Sie benötigen weiterhin das Paket build-essential um die anderen Gems zu erstellen, von denen es abhängt.
Installieren Sie es mit apt:
Sie können KafkaT nun mit dem Befehl gem installieren:
Die Option CFLAGS = -Wno-error = format-overflow deaktiviert die Formatüberlauf-Warnungen und ist für das ZooKeeper Gem erforderlich, das eine Abhängigkeit von KafkaT ist.
KafkaT verwendet .kafkatcfg als Konfigurationsdatei, um die Installations- und Protokollverzeichnisse Ihres Kafka-Servers zu bestimmen.
Es sollte auch einen Eintrag zum Verweis von KafkaT auf Ihre ZooKeeper-Instanz haben.
Erstellen Sie eine neue Datei namens .kafkatcfg:
Fügen Sie die folgenden Zeilen hinzu, um die erforderlichen Informationen über Ihren Kafka-Server und die Zookeeper-Instanz zu spezifizieren:
Sie können KafkaT nun verwenden.
Als erstes wird hier beschrieben, wie Sie es verwenden würden, um Details über alle Kafka-Partitionen anzuzeigen:
Sie sehen den folgenden Output:
Diese Ausgabe zeigt TutorialTopic sowie _ _ consumer _ offsets, ein internes Thema, das von Kafka zur Speicherung von clientbezogenen Informationen verwendet wird.
Sie können die mit _ _ consumer _ offsets beginnenden Zeilen sicher ignorieren.
Weitere Informationen über KafkaT finden Sie in seinem GitHub Repository.
Nachdem Sie KafkaT installiert haben, können Sie Kafka optional auf einem Cluster von Debian 10-Servern einrichten, um einen Mehrfachkonten-Cluster zu erstellen.
Schritt 7 - Einrichten eines Mehrfachknoten-Clusters (optional)
Wenn Sie einen Multi-Broker-Cluster unter Verwendung von weiteren Debian 10-Servern erstellen möchten, wiederholen Sie Schritt 1, Schritt 4 und Schritt 5 auf jedem der neuen Rechner.
Nehmen Sie zusätzlich die folgenden Änderungen in der Datei ~ / kafka / config / server.properties für jeden Rechner vor:
Ändern Sie den Wert der Eigenschaft broker.id so, dass er im gesamten Cluster eindeutig ist.
Diese Eigenschaft identifiziert jeden Server im Cluster eindeutig und kann einen beliebigen String als Wert haben.
Beispielsweise wären "server1", "server2" etc. als Identifikatoren nützlich.
Ändern Sie den Wert der Eigenschaft zookeeper.connect so, dass alle Knoten auf die gleiche ZooKeeper-Instanz verweisen.
Diese Eigenschaft gibt die Adresse der zookeeper-Instanz an und folgt dem Format < HOSTNAME / IP _ ADDRESS >: < PORT >.
Für dieses Tutorial würden Sie < ^ > your _ first _ server _ IP < ^ >: 2181 verwenden, wobei Sie < ^ > your _ first _ server _ IP < ^ > durch die IP-Adresse des von Ihnen bereits eingerichteten Debian 10-Servers ersetzen.
Wenn Sie mehrere ZooKeeper-Instanzen für Ihren Cluster haben möchten, sollte der Wert der Eigenschaft zookeeper.connect auf jedem Knoten eine identische, durch Komma getrennten Zeichenkette sein, die die IP-Adressen und die Portnummern aller ZooKeeper-Instanzen auflistet.
< $> note Hinweis: Wenn Sie eine Firewall auf dem Debian 10-Server mit installiertem Zookeeper aktiviert haben, stellen Sie sicher, dass Port 2181 geöffnet ist, um eingehende Anfragen der anderen Knoten im Cluster zu ermöglichen.
Schritt 8 - Einschränkung des Kafka-Benutzers
Nachdem jetzt alle Installationen abgeschlossen sind, können Sie die Admin-Berechtigungen des Benutzers kafka entfernen.
Bevor Sie dies tun, melden Sie sich wieder als beliebiger Benutzer ohne Root- und mit sudo-Berechtigung an.
Wenn Sie noch die gleiche Shell-Sitzung ausführen, mit der Sie das Tutorial gestartet haben, geben Sie einfach exit ein.
Entfernen Sie den Benutzer kafka aus der Gruppe sudo:
Um die Sicherheit Ihres Kafka-Servers weiter zu verbessern, sperren Sie das Passwort des Benutzer kafka mit dem Befehl passwd.
Dadurch wird sichergestellt, dass sich niemand mit diesem Konto direkt am Server anmelden kann:
Zu diesem Zeitpunkt kann sich nur der Benutzer root oder sudo als kafka anmelden, indem folgender Befehl eingegeben wird:
Wenn Sie es in Zukunft entsperren möchten, verwenden Sie passwd mit der Option -u:
Sie haben nun erfolgreich die Admin-Berechtigungen des Benutzers kafka eingeschränkt.
Apache Kafka wird nun sicher auf Ihrem Debian-Server ausgeführt.
Sie können es in Ihren Projekten verwenden, indem Sie Kafka-Ersteller und -Konsumenten mit Kafka-Clients erstellen, die für die meisten Programmiersprachen verfügbar sind.
Weitere Informationen über Kafka können Sie auch der Apache Kafka-Dokumentation entnehmen.
So paketieren und veröffentlichen Sie eine Snap-Anwendung unter Ubuntu 18.04
3857
Der Autor wählte die Electronic Frontier Foundation, um eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
Eine der größten Herausforderungen in der Anwendungsentwicklung ist der letzte Schritt der Verteilung des fertigen Produkts an Ihre Benutzer oder Kunden.
Viele bestehende Methoden zur Anwendungsbereitstellung sind nicht benutzerfreundlich und sicher oder bieten keine Methoden zur automatischen Aktualisierung einer Anwendung nach ihrer Installation.
Snap ist ein modernes Anwendungsverpackungsformat mit leistungsstarken Sandboxing- und Sicherheitsfunktionen, u. a. Dateisystem-Isolation, automatische Updates und integriertes Abhängigkeitsmanagement.
Snap-Anwendungen, die als Snaps bezeichnet werden, können ähnlich wie apt oder yum mit einem Befehlszeilenprogramm heruntergeladen und installiert werden.
Ubuntu wird mit vorinstalliertem Snap bereitgestellt, was bedeutet, dass es ein breites Publikum für Snap-Anwendungen gibt.
In diesem Tutorial werden Sie eine Snap-Anwendung erstellen und im Snap Store veröffentlichen.
Um diesem Tutorial zu folgen, benötigen Sie:
Einen Ubuntu-18.04-Server, der gemäß Ersteinrichtung eines Servers mit Ubuntu 18.04 eingerichtet wurde, einschließlich eines Benutzers ohne Root- und mit sudo-Berechtigungen.
Eine Anwendung, die Sie paketieren und als Snap freigeben möchten.
Dabei kann es sich um eine komplexe Anwendung handeln, die Sie erstellt haben, um ein gemeinsames Open-Source-Projekt oder um ein einfaches "Hello, World!" -
Programm.
Wenn Sie noch keine Anwendung haben, wird in Schritt 1 dieses Tutorials behandelt, wie Sie ein Hello World-Programm in Go erstellen können.
Ein Konto auf dem Snapcraft Developer Dashboard.
Sobald Sie diese zur Verfügung haben, melden Sie sich zunächst als Nicht-Root-Benutzer auf Ihrem Server an.
Schritt 1 - Vorbereiten Ihrer Anwendung für die Paketierung
Zunächst bereiten Sie Ihre Anwendung für die Paketierung als Snap-Anwendung vor, indem Sie sicherstellen, dass alles Erforderliche in einem einzigen Verzeichnis vorhanden ist.
Beginnen Sie mit der Erstellung eines neuen Verzeichnisses für Ihr Snap und wechseln Sie in dieses:
Wenn Sie bereits eine Anwendung haben, legen Sie als Nächstes eine vollständige Kopie des Quellcodes Ihrer Anwendung in dem Verzeichnis ab, das Sie gerade erstellt haben.
Der Prozess wird je nach der genauen Anwendung, die Sie paketieren, erheblich variieren. Für den Fall, dass der Quellcode in einem Git-Repository gespeichert ist, können Sie mit dem Befehl git init ein Repository im Verzeichnis anlegen und den gesamten relevanten Code abrufen.
Wenn Sie noch keine Anwendung haben, die Sie paketieren möchten, können Sie ein Hello World-Programm erstellen, das Sie stattdessen verwenden.
Wenn Sie mehr Kontext zum Schreiben dieses Programms mit Go wünschen, lesen Sie das Tutorial Schreiben eines ersten Prgramms in Go.
Sie können dies tun, indem Sie zuerst eine neue Go-Datei erstellen und mit Ihrem bevorzugten Texteditor öffnen:
Fügen Sie als Nächstes den folgenden Code in die Datei ein:
Speichern und schließen Sie die Datei dann.
Wenn Sie Go nicht installiert haben, können Sie es mit dem folgenden Befehl installieren:
Nachdem Go installiert ist, können Sie Ihr neues Programm ausführen, um zu überprüfen, ob es funktioniert:
Sie sehen die folgende Ausgabe:
Sie haben Ihre Anwendung für die Paketierung als Snap vorbereitet.
Als Nächstes installieren Sie die Software, die für den Beginn des Paketierungsvorgangs erforderlich ist.
Schritt 2 - Installieren von Snapcraft
In diesem Schritt laden Sie Snapcraft, das offizielle Snap-Anwendungs-Paketierungs-Tool, herunter und installieren es.
Snapcraft ist über den Snap Store verfügbar, der standardmäßig in Ubuntu integriert ist.
Das bedeutet, dass Sie Snapcraft von der Befehlszeile aus imt dem Befehl snap installieren können.
Der Befehl snap entspricht dem Befehl apt, jedoch können Sie ihn zur Installation von Software aus dem Snap Store anstelle von Paketen aus den Apt Repositories verwenden.
Um Snapcraft zu installieren, führen Sie den folgenden Befehl aus:
Verwenden Sie das Befehlsargument --classic, sodass Snapcraft ohne die strengen Sandboxing-Funktionen installiert wird, die Snaps normalerweise verwenden.
Snapcraft benötigt dieses Argument, da es einen privilegierten Zugriff auf Ihr System erfordert, um Anwendungen zuverlässig zu paketieren.
Sobald Sie Snapcraft installiert haben, sehen Sie Folgendes:
Abschließend können Sie die Snapcraft-Installation überprüfen, indem Sie Folgendes ausführen:
Dadurch wird in etwa Folgendes angezeigt:
Nachdem Sie Snapcraft installiert haben, können Sie nun die Konfiguration und Metadaten für Ihre Snap-Anwendung definieren.
Schritt 3 - Definieren der Konfiguration und Metadaten für Ihr Snap
In diesem Schritt werden Sie zunächst die Konfiguration, die Struktur und die Metadaten für Ihre Snap-Anwendung definieren.
Stellen Sie zuerst sicher, dass Sie immer noch in Ihrem Snap-Anwendungsverzeichnis arbeiten:
Erstellen und bearbeiten Sie als Nächstes die Datei snapcraft.yaml mit Ihrem bevorzugten Texteditor:
Sie werden die Datei snapcraft.yaml verwenden, um die gesamte Konfiguration für Ihre Snap-Anwendung zu speichern, einschließlich des Namens, der Beschreibung und der Version, sowie Einstellungen im Zusammenhang mit der Abhängigkeitsverwaltung und Sandboxing.
Beginnen Sie mit der Definition des Namens, der Zusammenfassung, der Beschreibung und der Versionsnummer für Ihre Anwendung:
Der Name Ihres Snap muss eindeutig sein, wenn Sie es im Snap Store veröffentlichen möchten -Suchen Sie nach anderen Anwendungen mit demselben Namen, um sicherzustellen, dass er nicht bereits vergeben ist.
Als Nächstes können Sie den / die Befehl (e) definieren, den / die Sie mit Ihrer Anwendung verknüpfen möchten.
Dadurch kann Ihr Snap direkt von der Bash-Befehlszeile aus als normaler Befehl verwendet werden.
Fügen Sie Folgendes zu Ihrer Datei snapcraft.yaml hinzu:
your-snap-command ist der Name des Befehls, den Sie definieren möchten.
Beispielsweise können Sie den Befehl helloworld verwenden, um Ihr Hello World-Programm auszuführen.
Verwenden Sie command: < ^ > your-snap < ^ >, um Snapcraft mitzuteilen, was zu tun ist, wenn der Anwendungsbefehl ausgeführt ist.
Im Fall des Hello World-Programms würden Sie den Wert helloworld verwenden, um auf die Datei helloworld.go zu verweisen, die Snapcraft die erfolgreiche Ausführung Ihres Programms ermöglicht.
Daraus ergibt sich die folgende Beispielkonfiguration:
Wenn der Befehlsname genau mit dem Namen des Snaps übereinstimmt, können Sie ihn direkt von der Befehlszeile aus ausführen.
Wenn der Befehl nicht mit dem Snap-Namen übereinstimmt, wird dem Befehl automatisch der Name des Snaps vorangestellt.
Beispielsweise heloworld.command1.
Abschließend können Sie die Teile definieren, aus denen sich Ihre Snap-Anwendung zusammensetzt.
Snap-Anwendungen bestehen aus mehreren Teilen, die alle die Komponenten Ihrer Anwendung darstellen.
In vielen Fällen gibt es nur einen Teil, nämlich die Anwendung selbst.
Jeder Teil hat ein zugehöriges Plugin.
Beispielsweise wird für in Ruby geschriebene Komponenten Ihrer Anwendung das Plugin ruby verwendet, und für in Go geschriebene Komponenten wird das Plugin go verwendet.
Sie können den Snapcraft-Befehl list-plugins verwenden, um das / die richtigen Plugin (s) für Ihre Anwendung zu identifizieren:
Dadurch wird in etwa folgende Liste ausgegeben:
Die am häufigsten verwendeten Plugins sind solche für gängige Programmiersprachen wie Go, Rust, Ruby oder Python.
Sobald Sie die richtigen Plugins für Ihre Anwendung identifiziert haben, können Sie damit beginnen die Konfiguration parts zu Ihrer Datei snapcraft.yaml hinzuzufügen:
Verwenden Sie Konfigurationsparameter source, um den relativen Pfad zu dem Quellcode Ihrer Anwendung anzugeben.
Normalerweise ist dies das gleiche Verzeichnis wie die Datei snapcraft.yaml selbst, sodass der Quellwert ein einzelner Punkt (.) ist.
< $> note Anmerkung: Wenn Ihre Anwendungskomponente Abhängigkeiten hat, die für das Erstellen oder Ausführen der Anwendung erforderlich sind, können Sie diese mit den Attributen build-packages und stage-packages angeben.
Die angegebenen Namen der Abhängigkeiten werden dann automatisch von dem Standard-Paketmanager für Ihr System geholt.
Beispiel:
Einige Snapcraft-Plugins haben ihre eigenen spezifischen Optionen, die für Ihre Anwendung erforderlich sein können. Daher ist es sinnvoll, die entsprechenden Handbuchseiten für Ihr Plugin hinzuzuziehen.
Im Fall von Go würden Sie auch den go-importpath angeben.
Für die Hello World-Konfiguration ergibt sich daraus die folgende Beispielkofiguation:
Sie können Ihre Datei snapcraft.yaml offen lassen, um im nächsten Schritt weitere Konfigurationen hinzuzufügen.
Sie haben die Basiskonfiguration für Ihre Snap-Anwendung definiert.
Als Nächstes konfigurieren Sie die Sicherheits- und Sandboxing-Aspekte Ihrer Anwendung.
Schritt 4 - Sichern Ihrer Snap-Anwendung
Snap-Anwendungen sind so konzipiert, dass Sie in einer Sandbox-Umgebung ausgeführt werden können. In diesem Schritt konfigurieren Sie also die Sandbox für Ihr Snap.
Als Erstes müssen Sie das Sandboxing für Ihre Anwendung aktivieren, was innerhalb von Snapcraft als confinement bekannt ist.
Dadurch wird das Sandboxing für Ihre Anwendung aktivieren und verhindert, dass sie auf das Internet, andere laufende Snaps oder das Host-System selbst zugreifen kann.
In den meisten Fällen müssen Anwendungen jedoch außerhalb ihrer Sandbox kommunizieren können, beispielsweise wenn sie auf das Internet zugreifen oder das Dateisystem lesen / schreiben müssen.
Diese Berechtigungen, die innerhalb von Snapcraft als Schnittstellen bekannt sind, können Ihrer Snap-Anwendung mit Plugs gewährt werden.
Mithilfe von Plugs können Sie eine sehr feine Kontrolle über das Sandboxing für Ihre Anwendung haben, um ihr nur den benötigten Zugriff zu geben, und nicht mehr (Prinzip der geringsten Privilegien).
Die genau benötigten Schnittstellen hängen von Ihrer Anwendung ab.
Einige der am häufigsten verwendeten Schnittstellen sind:
audio-playback - Erlaubt die Audio-Ausgabe / Wiedergabe von Sounds.
audio-record - Erlaubt die Audio-Eingabe / Aufnahme.
camera - Erlaubt den Zugriff auf verbundene Webcams.
home - Erlaubt Zugriff auf nicht versteckte Dateien in Ihrem Home-Verzeichnis.
network - Erlaubt den Zugriff auf das Netzwerk / Internet.
network-bind - Erlaubt die Bindung an Ports, um als Netzwerkdienst zu arbeiten.
system-files - Erlaubt den Zugriff auf das gesamte Dateisystem des Host-Rechners.
Die vollständige Liste der verfügbaren Schnittstellen ist in der Snapcraft-Dokumentation unter Unterstützte Schnittstellen zu finden.
Sobald Sie alle erforderlichen Schnittstellen für Ihre Anwendung identifiziert haben, können Sie damit beginnen, diese den Plugs innerhalb Ihrer Datei snapcraft.yaml zuzuweisen.
Die folgende Beispielkonfiguration ermöglicht der Anwendung, den Zugriff auf das Netzwerk und den Home-Bereich der Benutzer:
Speichern und schließen Sie Ihre Datei.
Der Name des Plug sollte ein beschreibender Name sein, um Benutzern die Identifizierung des Zwecks des Plug zu erleichtern.
Sie haben das Sandboxing für Ihr Snap aktiviert und einige Plugs konfiguriert, um begrenzten Zugriff auf System-Ressourcen zu gewähren.
Als Nächstes werden Sie die Erstellung Ihrer Snap-Anwendung abschließen.
Schritt 5 - Erstellen und Testen Ihrer Snap-Anwendung
Nachdem Sie nun die gesamte erforderliche Konfiguration für Ihr Snap geschrieben haben, können Sie mit der Erstellung und dem lokalen Testen des Snap-Paketes fortfahren.
Wenn Sie diesem Tutorial mit einem Hello World-Programm als Ihre Anwendung gefolgt sind, sieht Ihre vollständige Datei snapcraft.yaml nun in etwa wie folgt aus:
Um Ihre Snap-Anwendung zu erstellen, führen Sie den Befehl snapcraft aus dem Verzeichnis für Ihr Snap aus:
Snapcraft startet dann automatisch einen virtuellen Rechner (VM) und beginnt mit der Erstellung Ihres Snap.
Sobald die Erstellung abgeschlossen ist, wird Snapcraft beendet und Sie sehen in etwa Folgendes:
Jetzt können Sie Ihr Snap lokal installieren, um zu überprüfen, ob es funktioniert:
Das Befehlsargument --dangerous ist erforderlich, da Sie ein lokales Snap installieren, das nicht signiert wurde.
Sobald der Installationsvorgang abgeschlossen ist, können Sie Ihr Snap mit dem zugehörigen Befehl ausführen.
Im Fall des Beispielprogamms Hello World würde die folgende Ausgabe erfolgen:
Sie können ebenfalls die Sandboxing-Richtlinie für Ihr Snap anzeigen, um sicherzustellen, dass die zugewiesenen Berechtigungen ordnungsgemäß erteilt wurden:
Dadurch wird eine Liste von Plugs und Schnittstellen ausgegeben, die in etwas wie folgt aussieht:
In diesem Schritt haben Sie Ihr Snap erstellt und lokal installiert, um zu testen, ob es funktioniert.
Als Nächstes veröffentlichen Sie Ihr Snap im Snap Store.
Schritt 6 - Veröffentlichen Ihres Snap
Nachdem Sie Ihre Snap-Anwendung erstellt und getestet haben, ist es nun an der Zeit, sie im Snap Store zu veröffentlichen.
Melden Sie sich mit der Snapcraft-Befehlszeilenanwendung an Ihrem Snap Developer-Konto an:
Folgen Sie den Eingabeaufforderungen, um Ihre E-Mail-Adresse und Ihr Passwort einzugeben.
Als Nächstes müssen Sie den Namen der Anwendung im Snap Store registrieren:
Sobald Sie den Snap-Namen registriert haben, können Sie das erstellte Snap-Paket in den Shop verschieben:
Sie sehen eine Ausgabe, die der folgenden ähnelt:
Bei jeder Verschiebung in den Snap Store wird die Versionsnummer erhöht, beginnend mit eins.
Dies ist nützlich, um die verschiedenen Builds Ihres Snap zu identifizieren.
Abschließend können Sie Ihr Snap für die Öffentlichkeit freigeben:
Wenn dies Ihre erste Verschiebung in den Snap Store ist, wird die Versionsnummer 1 sein. Sie können auch zwischen der Freigabe in den Kanälen stable, candidate, beta und edge wählen, wenn Sie mehrere Versionen Ihrer Anwendung in verschiedenen Entwicklungsstufen haben.
Mit dem folgenden Befehl wird beispielsweise die Version 1 des Hello World Snap für den Kanal stable freigegeben:
Sie können nun im Snap Store nach Ihrer Anwendung suchen und auf einem Ihrer Geräte installieren.
Snapcraft Store mit in den Suchergebnissen angezeigter HelloWorld-Anwendung
In diesem letzten Schritt haben Sie Ihr erstelltes Snap-Paket in den Snap Store hochgeladen und für die Öffentlichkeit freigegeben.
In diesem Artikel haben Sie eine Snap-Anwendung konfiguriert und erstellt und über den Snap Store für die Öffentlichkeit freigegeben.
Sie verfügen nun über die grundlegenden Kenntnisse, die für die Wartung Ihrer Anwendung und die Erstellung neuer Anwendungen erforderlich sind.
Wenn Sie Snaps weiter erkunden möchten, können Sie den gesamten Snap Store durchsuchen.
Vielleicht möchten Sie auch die Snapcraft YAML-Referenz durchsehen, um mehr darüber zu erfahren und zusätzliche Attribute für Snap-Konfiguration zu identifizieren.
Wenn Sie sich näher über die Snap-Entwicklung informieren möchten, empfehlen wir Ihnen, mehr über Snap Hooks und deren Implementierung zu lesen. Sie ermöglichen es Snaps, dynamisch auf Systemänderungen wie Upgrades oder Anpassungen der Sicherheitsrichtlinien zu reagieren.
So richten Sie eine Passwort-Authentifizierung mit Apache unter Ubuntu 18.04 ein Schnellstart
3693
Dieses Tutorial führt Sie durch den Passwortschutz von Assets auf einem Apache-Webserver, der unter Ubuntu 18.04. ausgeführt wird.
Durch die Ausführung dieser Schritte wird Ihrem Server zusätzliche Sicherheit bereitgestellt, damit unbefugte Benutzer auf bestimmte Teile Ihrer Seite nicht zugreifen können.
Eine ausführlichere Version dieses Tutorials mit detaillierteren Erklärungen zu den einzelnen Schritten finden Sie unter So richten Sie die Passwort-Authentifizierung mit Apache unter Ubuntu 18.04 ein.
Um dieses Tutorial zu absolvieren, benötigen Sie auf einem Ubuntu 18.04-Server Zugriff auf Folgendes:
Einen Sudo-Benutzer auf Ihrem Server
Einen Apache2 Web-Server
Eine Site, die mit SSL gesichert ist
Schritt 1 - Installation des Apache-Dienstprogramm-Pakets
Wir installieren ein Dienstprogramm namens htpasswd, das Teil des apache2-utils-Pakets ist, um Benutzernamen und Passwörter mit Zugriff auf den Inhalt zu verwalten.
Schritt 2 - Erstellen der Passwort-Datei
Wir erstellen den ersten Benutzer wie folgt (ersetzen Sie '< ^ > first _ username < ^ > durch einen Benutzernamen Ihrer Wahl):
Sie werden aufgefordert, ein Passwort für den Benutzer anzugeben und zu bestätigen.
Geben Sie nicht das Argument -c für zusätzliche Benutzer an, die Sie hinzufügen möchten, um die Datei nicht zu überschreiben:
Schritt 3 - Einrichten der Apache Passwort-Authentifizierung
In diesem Schritt müssen wir Apache konfigurieren, um diese Datei zu überprüfen, bevor wir unseren geschützten Inhalt bereitstellen.
Dazu verwenden wir die virtuelle Host-Datei der Site, aber es gibt eine weitere Option, die im längeren Tutorial ausführlich beschrieben wird, wenn Sie keinen Zugriff haben oder lieber .htaccess-Dateien verwenden.
Öffnen Sie die virtuelle Host-Datei, zu der Sie mit einem Textbearbeitungsprogramm wie nano, eine Einschränkung hinzufügen möchten:
Die Authentifizierung wird pro Verzeichnis durchgeführt.
In unserem Beispiel beschränken wir den gesamten Dokumentenstamm, aber können Sie diese Liste ändern, um nur auf ein bestimmtes Verzeichnis im Web-Raum abzuzielen.
In diesem Schritt fügen Sie die folgenden hervorgehobenen Zeilen zu Ihrer Datei hinzu:
Überprüfen Sie die Konfiguration mit dem folgenden Befehl:
Sie können den Server neu starten, um Ihre Passwort-Richtlinien umzusetzen und dann den Status Ihres Servers zu überprüfen.
Schritt 4 - Bestätigung der Passwort-Authentifizierung
Um zu bestätigen, dass Ihr Inhalt geschützt ist, versuchen Sie, auf Ihren begrenzten Inhalt in einem Web-Browser zuzugreifen.
Sie sollten eien Benutzernamen- und Passwort-Eingabeaufforderung sehen:
Eingabeaufforderung für ein Apache2-Passwort
Relevante Tutorials
Hier sehen Sie Links zu detaillierteren Leitfäden, die in Verbindung mit diesem Tutorial stehen:
So richten Sie eine Passwort-Authentifizierung mit Apache unter Ubuntu 18.04 ein
Sich mit wichtigen Apache-Dateien und -Verzeichnissen in unserem Apache-Installationshandbuch vertraut machen.
So richten Sie Apache Virtual Hosts unter Ubuntu 18.04 ein
So verwenden Sie die .htaccess-Datei.
So richten Sie SSH-Schlüssel unter CentOS 8 ein
3697
SSH oder Secure Shell ist ein verschlüsseltes Protokoll zur Verwaltung und Kommunikation mit Servern.
Wenn Sie mit einem CentOS-Server arbeiten, verbringen Sie wahrscheinlich die meiste Zeit in einer Terminalsitzung, die über SSH mit Ihrem Server verbunden ist.
In diesem Leitfaden konzentrieren wir uns auf die Einrichtung von SSH-Schlüsseln für einen CentOS 8 Server.
SSH-Schlüssel bieten eine einfache und sichere Methode zur Anmeldung bei Ihrem Server und werden für alle Benutzer empfohlen.
Schritt 1 - Erstellen des RSA-Schlüsselpaars
Der erste Schritt besteht darin, ein Schlüsselpaar auf dem Client-Rechner (üblicherweise Ihr lokaler Computer) zu erstellen:
Standardmäßig erzeugt ssh-keygen ein 2048-Bit-RSA-Schlüsselpaar, das für die meisten Anwendungsfälle sicher genug ist (Sie können optional das Flag -b 4096 übergeben, um einen größeren 4096-Bit-Schlüssel zu erzeugen).
Nach Eingabe des Befehls sollten Sie die folgende Eingabeaufforderung sehen:
Drücken Sie die Eingabetaste, um das Schlüsselpaar im Unterverzeichnis .ssh / in Ihrem Stammverzeichnis zu speichern, oder geben Sie einen alternativen Pfad an.
Wenn Sie zuvor ein SSH-Schlüsselpaar erstellt haben, wird möglicherweise die folgende Eingabeaufforderung angezeigt:
Wenn Sie den Schlüssel auf der Festplatte überschreiben, können Sie sich nicht mehr mit dem vorherigen Schlüssel authentifizieren.
Seien Sie sehr vorsichtig bei der Auswahl von "Ja", da dies ein destruktiver Prozess ist, der nicht rückgängig gemacht werden kann.
Sie sollten dann die folgende Eingabeaufforderung sehen:
Hier können Sie optional eine sichere Passphrase eingeben, was dringend empfohlen wird.
Eine Passphrase fügt Ihrem Schlüssel eine zusätzliche Sicherheitsebene hinzu, um zu verhindern, dass sich unberechtigte Benutzer anmelden.
Sie sollten dann die folgende Ausgabe sehen:
Sie haben jetzt einen öffentlichen und privaten Schlüssel, die Sie zur Authentifizierung verwenden können.
Der nächste Schritt besteht darin, den öffentlichen Schlüssel auf Ihren Server zu übertragen, damit Sie sich mit einer auf SSH-Schlüssel basierenden Authentifizierung anmelden können.
Schritt 2 - Kopieren des öffentlichen Schlüssels auf Ihren CentOS-Server
Der schnellste Weg zum Kopieren Ihres öffentlichen Schlüssels auf den CentOS-Host ist die Verwendung eines Dienstprogramms namens ssh-copy-id.
Diese Methode wird dringend empfohlen, falls sie verfügbar ist.
Wenn Sie ssh-copy-id auf Ihrem Client-Rechner nicht verfügbar haben, können Sie eine der beiden nachfolgenden alternativen Methoden verwenden (Kopieren über passwortbasierte SSH oder manuelles Kopieren des Schlüssels).
Kopieren Ihres öffentlichen Schlüssels mit ssh-copy-id
Das Tool ssh-copy-id ist in vielen Betriebssystemen standardmäßig enthalten, sodass es möglicherweise auf Ihrem lokalen System zur Verfügung steht.
Damit diese Methode funktioniert, müssen Sie bereits über einen passwortbasierten SSH-Zugriff auf Ihren Server verfügen.
Um dieses Dienstprogramm zu verwenden, müssen Sie nur den Remote-Host, zu dem Sie eine Verbindung herstellen möchten, und das Benutzerkonto angeben, zu dem Sie Passwort-SSH-Zugang haben.
Dies ist das Konto, auf das Ihr öffentlicher SSH-Schlüssel kopiert wird:
Möglicherweise wird die folgende Meldung angezeigt:
Das bedeutet, dass Ihr lokaler Computer den Remote-Host nicht erkennt.
Dies geschieht, wenn Sie zum ersten Mal eine Verbindung zu einem neuen Host herstellen.
Geben Sie yes ein und drücken Sie ENTER, um fortzufahren.
Als nächstes durchsucht das Utility Ihr lokales Konto nach dem Schlüssel id _ rsa.pub, den wir zuvor erstellt haben.
Wenn der Schlüssel gefunden wurde, werden Sie zur Eingabe des Passworts für das Konto des Remotebenutzers aufgefordert:
Geben Sie das Passwort ein (Ihre Eingabe wird aus Sicherheitsgründen nicht angezeigt) und drücken Sie die ENTER.
Das Utility stellt mit dem von Ihnen angegebenen Passwort eine Verbindung zum Konto auf dem Remote-Host her.
Es kopiert dann den Inhalt Ihres Schlüssels ~ / .ssh / id _ rsa.pub in die Datei ~ / .ssh / authorized _ keys.
Sie sollten die folgende Ausgabe sehen:
Zu diesem Zeitpunkt wurde Ihr Schlüssel id _ rsa.pub in das Remote-Konto hochgeladen.
Sie können mit Schritt 3 fortfahren.
Kopieren des öffentlichen Schlüssels mit SSH
Wenn Sie ssh-copy-id nicht zur Verfügung, aber einen passwortbasierten SSH-Zugang zu einem Konto auf Ihrem Server haben, können Sie Ihre Schlüssel mit einer konventionelleren SSH-Methode hochladen.
Wir können dazu den Befehl cat verwenden, um den Inhalt des öffentlichen SSH-Schlüssels auf dem lokalen Computer zu lesen und über eine SSH-Verbindung zum Remote-Server weiterzuleiten.
Auf der anderen Seite können wir sicherstellen, dass das Verzeichnis ~ / .ssh existiert und die richtigen Berechtigungen für das von uns verwendete Konto besitzt.
Wir können dann den Inhalt, den wir übergeben haben, in eine Datei namens authorized _ keys in diesem Verzeichnis ausgeben.
Wir verwenden das Umleitungssymbol > >, um den Inhalt anzuhängen, anstatt ihn zu überschreiben. Dadurch können wir Schlüssel hinzufügen, ohne zuvor hinzugefügte Schlüssel zu zerstören.
Der vollständige Befehl sieht wie folgt aus:
Anschließend sollten Sie aufgefordert werden, das Passwort für das Remote-Benutzerkonto einzugeben:
Nach Eingabe Ihres Passworts wird der Inhalt Ihres Schlüssels id _ rsa.pub an das Ende der Datei authorized _ keys des Kontos des Remote-Benutzers kopiert.
Wenn dies erfolgreich war, fahren Sie mit Schritt 3 fort.
Manuelles Kopieren des öffentlichen Schlüssels
Wenn Sie keinen passwortbasierten SSH-Zugriff auf Ihren Server haben, müssen Sie den obigen Vorgang manuell ausführen.
Wir werden den Inhalt Ihrer Datei id _ rsa.pub manuell an die Datei ~ / .ssh / authorized _ keys auf Ihrem Remote-Computer anhängen.
Um den Inhalt Ihres Schlüssels id _ rsa.pub anzuzeigen, geben Sie Folgendes in Ihren lokalen Computer ein:
Der Inhalt des Schlüssels wird angezeigt, der ungefähr so aussehen sollte:
Melden Sie sich mit der Ihnen zur Verfügung stehenden Methode an Ihrem Remote-Host an.
Sobald Sie Zugriff auf Ihr Konto auf dem Remote-Server haben, sollten Sie sicherstellen, dass das Verzeichnis ~ / .ssh vorhanden ist.
Dieser Befehl erstellt bei Bedarf das Verzeichnis oder unternimmt nichts, wenn es bereits vorhanden ist:
Jetzt können Sie die Datei authorized _ keys in diesem Verzeichnis erstellen oder ändern.
Sie können den Inhalt Ihrer Datei id _ rsa.pub an das Ende der Datei authorized _ keys anfügen und diese bei Bedarf mit folgendem Befehl erstellen:
Ersetzen Sie im obigen Befehl < ^ > public _ key _ string < ^ > ​ ​ ​ ​ durch die Ausgabe des Befehls cat ~ / .ssh / id _ rsa.pub, den Sie auf Ihrem lokalen System ausgeführt haben.
Sie sollte mit ssh-rsa AAAA... beginnen.
Schließlich stellen wir sicher, dass für das Verzeichnis ~ / .ssh und die Datei authorized _ keys die folgenden Berechtigungen festgelegt sind:
Dadurch werden rekursiv alle "Gruppen-" und "anderen" Berechtigungen auf das Verzeichnis ~ / .ssh / entfernt.
Wenn Sie das Konto root zum Einrichten von Schlüsseln für ein Benutzerkonto verwenden, ist es auch wichtig, dass das Verzeichnis ~ / .ssh dem Benutzer und nicht root gehört:
In diesem Tutorial heißt unser Benutzer < ^ > sammy < ^ >; aber Sie sollten den entsprechenden Benutzernamen in den obigen Befehl einsetzen.
Wir können jetzt eine schlüsselbasierte Authentifizierung mit unserem CentOS-Server versuchen.
Schritt 3 - Anmelden an Ihrem CentOS-Server mit SSH-Schlüsseln
Wenn Sie eines der oben genannten Verfahren erfolgreich abgeschlossen haben, sollten Sie sich jetzt ohne das Passwort des Remote-Kontos am Remote-Host anmelden können.
Der anfängliche Prozess ist derselbe wie bei der passwortbasierten Authentifizierung:
Wenn Sie zum ersten Mal eine Verbindung zu diesem Host herstellen (wenn Sie die letzte Methode oben verwendet haben), wird möglicherweise Folgendes angezeigt:
Geben Sie yes ein und drücken Sie dann ENTER, um fortzufahren.
Wenn Sie bei der Erstellung Ihres Schlüsselpaars in Schritt 1 keine Passphrase angegeben haben, werden Sie sofort angemeldet.
Wenn Sie eine Passphrase angegeben haben, werden Sie aufgefordert, diese jetzt einzugeben.
Nach der Authentifizierung sollte sich eine neue Shell-Sitzung mit dem konfigurierten Konto auf dem CentOS-Server für Sie öffnen.
Wenn die schlüsselbasierte Authentifizierung erfolgreich war, fahren Sie fort, um zu erfahren, wie Sie Ihr System durch Deaktivieren der passwortbasierten Authentifizierung Ihres SSH-Servers weiter sichern können.
Schritt 4 - Deaktivieren der Passwortauthentifizierung auf Ihrem Server
Wenn Sie sich mit SSH ohne Passwort bei Ihrem Konto anmelden konnten, haben Sie die auf SSH-Schlüssel basierte Authentifizierung für Ihr Konto erfolgreich konfiguriert.
Ihr passwortbasierter Authentifizierungsmechanismus ist jedoch weiterhin aktiv. Dies bedeutet, dass Ihr Server weiterhin Brute-Force-Angriffen ausgesetzt ist.
Stellen Sie vor dem Ausführen der in diesem Abschnitt beschriebenen Schritte sicher, dass entweder die auf SSH-Schlüssel basierte Authentifizierung für das Root-Konto auf diesem Server konfiguriert ist oder dass vorzugsweise die auf SSH-Schlüssel basierte Authentifizierung für ein Nicht-Root-Konto mit sudo-Privilegien konfiguriert ist.
Durch diesen Schritt werden passwortbasierte Anmeldungen gesperrt. Daher ist es von entscheidender Bedeutung, dass Sie weiterhin über Administratorrechte verfügen.
Sobald Sie bestätigt haben, dass Ihr Remote-Konto über Administratorrechte verfügt, melden Sie sich mit SSH-Schlüsseln bei Ihrem Remote-Server an, entweder als Root-Benutzer oder mit einem Konto mit sudo-Privilegien.
Öffnen Sie dann die Konfigurationsdatei des SSH-Daemons:
Suchen Sie in der Datei nach einer Anweisung namens PasswordAuthentication.
Dies kann mit einem # Hash auskommentiert werden.
Drücken Sie i, um vi in den Einfügemodus zu versetzen, entkommentieren Sie dann die Zeile und setzen Sie den Wert auf no. Dadurch wird die Möglichkeit deaktiviert, sich über SSH mit Kontopasswörtern anzumelden:
Wenn Sie mit den Änderungen fertig sind, drücken Sie ESC und dann: wq, um die Änderungen in die Datei zu schreiben und diese zu verlassen.
Um diese Änderungen tatsächlich zu implementieren, müssen wir den sshd-Dienst neu starten:
Öffnen Sie vorsichtshalber ein neues Terminalfenster und testen Sie, ob der SSH-Dienst ordnungsgemäß funktioniert, bevor Sie Ihre aktuelle Sitzung schließen:
Nachdem Sie die ordnungsgemäße Ausführung Ihres SSH-Dienstes überprüft haben, können Sie alle aktuellen Serversitzungen sicher schließen.
Der SSH-Daemon auf Ihrem CentOS-Server reagiert jetzt nur noch auf SSH-Schlüssel.
Die passwortbasierte Authentifizierung wurde erfolgreich deaktiviert.
Auf Ihrem Server sollte jetzt die auf SSH-Schlüsseln basierte Authentifizierung konfiguriert sein, damit Sie sich anmelden können, ohne ein Kontopasswort anzugeben.
Wenn Sie mehr über das Arbeiten mit SSH erfahren möchten, sehen Sie sich unseren Leitfaden über SSH-Grundlagen an.
So richten Sie die Code-Server-Cloud-IDE-Plattform unter Ubuntu 18.04 ein
3341
Code-Server ist Microsoft Visual Studio Code, der auf einem Remote-Server läuft und auf den Sie direkt von Ihrem Browser aus zugreifen können.
Dadurch können Sie verschiedene Geräte verwenden, die verschiedene Betriebssysteme ausführen und immer eine einheitliche Entwicklungsumgebung haben.
In diesem Tutorial richten Sie die Code-Server Cloud IDE-Plattform auf Ihrem Ubuntu 18.04 Computer ein und zeigen sie auf Ihrer Domäne, geschützt durch Let 's Encrypt.
Eine ausführlichere Version dieses Tutorials finden Sie unter So richten Sie die Code-Server-Cloud-IDE-Plattform unter Ubuntu 18.04 ein.
Einen Server mit Ubuntu 18.04 mit mindestens 2 GB RAM, Root-Zugriff und einem Sudo ohne Rootberechtigung.
Sie können dies einrichten, indem Sie sich an die Anweisungen unter Ersteinstellung des Servers für Ubuntu 18.04. halten.
Nginx, das auf Ihrem Server installiert ist.
Führen Sie dazu die Schritte 1 bis 4 aus So installieren Sie Nginx auf Debian 10 aus.
Einen vollständig registrierten Domänennamen für den Host Code-Server, der auf Ihren Server verweist.
Dieses Tutorial verwendet durchgehend code-server. < ^ > your-domain < ^ >.
Sie können einen Domänennamen unter Namecheap günstig erwerben oder einen kostenlosen von Freenom herunterladen oder einfach die Domänenregistrierungsstelle Ihrer Wahl verwenden.
Die beiden folgenden DNS-Einträge wurden für Ihren Server eingerichtet.
Sie finden in dieser Einführung in DigitalOcean DNS Details dazu, wie Sie sie hinzufügen können.
Einen Eintrag mit < ^ > your-domain < ^ >, der auf die öffentliche IP-Adresse Ihres Servers verweist.
Einen Eintrag mit < ^ > your-domain < ^ >, der auf die öffentliche IP-Adresse Ihres Servers verweist.
Schritt 1 - Installation des Code-Servers
Erstellen Sie das Verzeichnis, um alle Daten für den Code-Server zu speichern:
Navigieren Sie dorthin:
Besuchen Sie die Seite Github-Veröffentlichungen des Code-Servers und wählen Sie den neuesten Linux-Build aus.
Laden Sie ihn wie folgt herunter:
Entpacken Sie das Archiv:
Navigieren Sie zum Verzeichnis mit der ausführbaren Datei des Code-Servers:
Um die ausführbare Datei des Code-Servers auf Ihrem System zuzugreifen, kopieren Sie diesen wie folgt:
Erstellen Sie einen Ordner für den Code-Server, um die Benutzerdaten zu speichern:
Erstellen Sie einen systemd-Dienst (code-server.service) im Verzeichnis / lib / systemd / system:
Fügen Sie die folgenden Zeilen hinzu:
--host 127.0.0.1 verbindet ihn mit localhost.
--user-data-dir / var / lib / code-server legt sein Verzeichnis für die Benutzerdaten fest.
--auth Passwort gibt an, dass es die Besucher mit einem Passwort authentifizieren sollte.
Denken Sie daran, < ^ > your _ password < ^ > durch Ihr gewünschtes Passwort zu ersetzen.
Starten Sie den Code-Server-Dienst:
Überprüfen Sie, ob es richtig gestartet wurde:
Sie sehen eine Ausgabe, die der nachfolgenden ähnelt:
Aktivieren Sie den Code-Server Dienst, um automatisch nach dem Neustart eines Servers zu starten:
Schritt 2 - Code-Server verfügbar machen
Jetzt konfigurieren Sie Nginx als Reverseproxy für den Code-Server.
Erstellen Sie code-server.conf, um die Konfiguration zu speichern, damit der Code-Server in Ihrer Domäne verfügbar gemacht wird:
Fügen Sie die folgenden Zeilen hinzu, um Ihren Server-Block mit den erforderlichen Anweisungen einzurichten:
Ersetzen Sie < ^ > code-server.your _ domain < ^ > durch Ihre gewünschte Domäne. speichern und schließen Sie dann die Datei.
Um diese Site-Konfiguration aktiv zu machen, erstellen Sie einen Symlink davon:
Testen Sie die Gültigkeit der Konfiguration:
Starten Sie Nginx neu, damit die Konfiguration wirksam wird.
Schritt 3 - Sicherung Ihrer Domäne
Jetzt sichern Sie Ihre Domäne mit einem Let 's Encrypt TLS-Zertifikat.
Fügen Sie das Certbot-Paket zu Ihrem Server hinzu:
Installieren Sie Certbot und sein Nginx Plugin:
Konfiguration von ufw, um den verschlüsselten Verkehr zu akzeptieren:
Die Ausgabe sieht wie folgt aus:
Laden Sie ihn neu, damit die Konfiguration wirksam wird:
Die Ausgabe zeigt Folgendes:
Navigieren Sie zu Ihrer code-server-Domäne.
Eingabeaufforderung des Code-Server-Login
Geben Sie Ihr Code-Server-Passwort ein.
Daraufhin ist die Schnittstelle auf Ihrer Domäne verfügbar.
Code-Server GUI
Installieren Sie zur Sicherung ein Encrypt TLS-Zertifikat mit Certbot.
Fordern Sie ein Zertifikat für Ihre Domäne wie folgt an:
Geben Sie eine E-Mail-Adresse für dringende Informationen an, akzeptieren Sie die Geschäftsbedingungen von EFF und entscheiden Sie, ob Sie den gesamten HTTP-Verkehr auf HTTPS umleiten möchten.
Die Ausgabe sieht ungefähr wie folgt aus:
Certbot hat TLS-Zertifikate erfolgreich erstellt und auf die Nginx-Konfiguration für Ihre Domäne angewendet.
Jetzt haben Sie den Code-Server, eine vielseitige Cloud, auf Ihrem Ubuntu 18.04 Server installiert, auf Ihrer Domäne verf ╣ gbar gemacht und mit Let 's Encrypt gesichert.
Weitere Informationen finden Sie in der Visual Studio Code Dokumentation, wo Sie zusätzliche Funktionen und detaillierte Anweisungen zu anderen Komponenten des Code-Server nachlesen können.
So automatisieren Sie Aufgaben mit Cron unter CentOS 8
3753
Eine frühere Version dieses Tutorials wurde von Shaun Lewis geschrieben.
Cron ist ein zeitbasierter Daemon für Feinterminierung, der in Unix-ähnlichen Betriebssystemen gefunden wird, einschließlich Linux-Distributionen.
Cron wird im Hintergrund ausgeführt. Aufgaben, die mit cron geplant werden, heißen "cron-jobs" und werden automatisch ausgeführt. Daher ist cron nützlich für die Automatisierung von wartungsbezogenen Aufgaben.
Dieser Leitfaden bietet einen Überblick über die Planung von Aufgaben mit crons spezieller Syntax.
Überdies erklärt er einige Kurzbefehle, mit deren Hilfe man das Planen von Aufträgen leichter schreiben und verstehen kann.
Um diesen Leitfaden zu absolvieren, benötigen Sie Zugriff auf einen Computer, der CentOS 8 ausführt. Das könnte Ihr lokaler Rechner, ein virtueller Rechner oder ein virtueller privater Server sein.
Egal welchen Computer Sie für diesen Leitfaden benutzen - er sollte einen Benutzer ohne Rootberechtigung mit konfigurierten Administratorberechtigungen haben.
Folgen Sie zur Einrichtung unserem Leitfaden zur Ersteinrichtung des Servers für CentOS 8.
Installation von Cron
Bei fast jeder Linux-Distribution ist Cron auf irgendeine Weise standardmäßig installiert.
Wenn Sie jedoch einen CentOS-Rechner verwenden, auf dem cron nicht installiert ist, können Sie es mit dnf installieren.
Um cron auf einem CentOS-Rechner zu installieren, aktualisieren Sie zuerst den lokalen Paketindex des Computers.
Installieren Sie dann den Cron-Daemon mit dem folgenden Befehl:
Dieser Befehl fordert Sie auf, zu bestätigen, dass Sie das Paket crontabs und seine Abhängigkeiten installieren möchten.
Bestätigen Sie, indem Sie y und dann die EINGABETASTE drücken.
Dadurch wird cron auf Ihrem System installiert, aber den Daemon müssen Sie manuell starten.
Außerdem müssen Sie sicherstellen, dass eine Ausführung bei jedem Server-Boot festgelegt ist.
Sie können beide Aktionen mit dem Befehl systemctl ausführen.
Um den cron-Daemon zu starten, führen Sie folgenden Befehl aus:
Um einzustellen, dass cron bei jedem Start des Servers ausgeführt wird, geben Sie Folgendes ein:
Danach wird cron auf Ihrem System installiert und Sie können mit der Planung von Aufträgen beginnen.
So funktioniert Cron
Cron-Aufträge werden aufgezeichnet und in einer speziellen Datei namens crontab verwaltet.
Jedes Benutzerprofil im System kann über ein eigenes crontab verfügen, bei dem Aufträge geplant werden können. Das ist unter / var / spool / cron / gespeichert.
Um einen Auftrag zu planen, müssen Sie nur Ihr crontab für die Bearbeitung öffnen und eine Aufgabe hinzufügen, die in Form eines Cron-Ausdrucks geschrieben wird.
Die Syntax für cron-Ausdrücke kann in zwei Elemente unterteilt werden: die Planung und der Befehl zur Ausführung.
Der Befehl kann praktisch jeder Befehl sein, den Sie normalerweise auf der Befehlszeile ausführen würden.
Die Planungs-Komponente der Syntax besteht aus 5 verschiedenen Feldern, die in folgender Reihenfolge geschrieben werden:
Feld
Zulässige Werte
Minute
0-59
Stunde
0-23
Tag des Monats
1-31
Monat
1-12 oder JAN-DEC
Tag der Woche
0-6 oder SUN-SAT
Zusammen werden Aufgaben, die in einem crontab geplant werden, wie folgt strukturiert:
Hier ist ein funktionelles Beispiel eines Cron-Ausdrucks.
Dieser Ausdruck führt jeden Dienstag um 17: 30 Uhr den Befehl curl http: / / www.google.com aus:
Außerdem gibt es einige spezielle Zeichen, die Sie in die Planungs-Komponente eines cron-Ausdrucks aufnehmen können, um die Planung zu erleichtern:
*: Ein Sternchen ist in einem cron-Ausdruck ein Platzhalter, der "alle" repräsentiert.
Folglich wird eine Aufgabe, die mit * * * * *... ​ ​ ​ geplant wurde, jede Minute jeder Stunde an jedem Tag in jedem Monat ausgeführt.
,: Kommas brechen die Planungswerte auf, um eine Liste zu bilden.
Wenn Sie zu Beginn und in der Mitte jeder Stunde eine Aufgabe ausführen möchten, können Sie, anstatt zwei separate Aufgaben auszuschreiben (z. B. 0 * * * *... ​ ​ ​ ​ ​ und ​ ​ ​ 30 * * * *... ​ ​ ​), die gleiche Funktionalität mit einer erreichen (0,30 * * * *...).
-: Ein Bindestrich stellt im Planungs-Feld einen Wertbereich dar.
Anstatt 30 separate geplante Aufgaben für einen Befehl zu haben, den Sie in den ersten 30 Minuten jeder Stunde ausführen möchten (wie in 0 * * * *... ​ ​ ​, 1 * * * *..., 2 * * * *..., usw.), können Sie es folgendermaßen festlegen: 0-29 * * * *... ​ ​ ​.
/: Sie können einen Schrägstrich mit einem Sternchen verwenden, um einen Schrittwert auszudrücken.
Anstatt zum Beispiel acht separate cron-Aufgaben auszuschreiben, um einen Befehl alle drei Stunden auszuführen (0 0 * * *... ​ ​ ​ ​ ​, ​ ​ ​ ​ ​ ​ 0 3 * * *... ​ ​ ​ ​ ​ ​, ​ ​ ​ ​ ​ ​ 0 6 * * *... ​ ​ ​ ​ ​ ​, usw.), können Sie es folgendermaßen festlegen: ​ ​ ​ ​ ​ ​ 0 * / 3 * * *... ​ ​ ​ ​ ​ ​.
< $> note Anmerkung: Sie können Schrittwerte nicht willkürlich ausdrücken. Sie können nur ganze Zahlen verwenden, die sich gleichmäßig in den vom betreffenden Feld erlaubten Bereich teilen lassen.
Im Feld "Stunden" könnten Sie nach einem Schrägstrich beispielsweise nur 1, 2, 3, 4, 6, 8 oder 12 eingeben.
Hier sind weitere Beispiele für die Verwendung von Crons Planungs-Komponente:
* * * * * - Den Befehl jede Minute ausführen.
12 * * * * - Den Befehl 12 Minuten nach jeder vollen Stunde ausführen.
0,15,30,45 * * * * - Den Befehl alle 15 Minuten ausführen.
* / 15 * * * * - Den Befehl alle 15 Minuten ausführen.
0 4 * * * - Den Befehl jeden Tag um 4: 00 Uhr ausführen.
0 4 * * 2-4 - Den Befehl jeden Dienstag, Mittwoch und Donnerstag um 4: 00 Uhr ausführen.
20,40 * / 8 * 7-12 * - Den Befehl 20 und 40 Minuten nach jeder achten Stunde jedes Tages der letzten 6 Monate des Jahres ausführen.
Wenn etwas für Sie unklar ist oder wenn Sie beim Schreiben Ihrer eigenen cron-Aufgaben Hilfe benötigen, bietet Cronitor einen praktischen Planungs-Ausdruck-Editor für cron namens "Crontab Guru", mithilfe dessen Sie überprüfen können, ob Ihre Pläne gültig sind.
Crontabs verwalten
Sobald Sie einen Zeitplan festgelegt haben und wissen, welchen Auftrag Sie ausführen möchten, müssen Sie ihn irgendwo ablegen, wo Ihr Daemon ihn lesen kann.
Wie zuvor erwähnt ist ein crontab eine spezielle Datei, die den Zeitplan für die Aufträge enthält, die Cron ausführen wird.
Diese sollen jedoch nicht direkt bearbeitet werden.
Stattdessen wird empfohlen, den Befehl crontab zu verwenden.
Dadurch können Sie das crontab Ihres Benutzerprofils bearbeiten, ohne Ihre Berechtigungen mit sudo zu ändern.
Der Befehl crontab lässt Sie auch wissen, ob die Syntaxfehler im crontab haben. Eine direkte Bearbeitung wird dies nicht tun.
Sie können Ihren crontab mit dem folgenden Befehl bearbeiten:
Dadurch wird Ihr crontab in dem Standard-Texteditor Ihres Benutzerprofils geöffnet.
< $> note Anmerkung: Auf neuen CentOS-8-Servern öffnet der Befehl crontab -e den crontab Ihres Benutzers standardmäßig mit vi. vi ist ein extrem leistungsfähiger und flexibler Texteditor. Er kann jedoch für Benutzer, die keine Erfahrung damit haben, schwer verständlich wirken.
Wenn Sie einen einfacheren Texteditor als Ihren Standard-Editor für crontab verwenden möchten, können Sie nano als solchen installieren und konfigurieren.
Installieren Sie dazu nano mit dnf:
Wenn Sie aufgefordert werden, drücken Sie y und dann ENTER, um zu bestätigen, dass Sie nano installieren möchten.
Um nano als Ihren visuellen Standard-Editor Ihres Benutzerprofils einzustellen, öffnen Sie die Datei .bash _ profile zur Bearbeitung.
Nachdem Sie nano jetzt installiert haben, können Sie dies damit tun:
Fügen Sie am unteren Ende der Datei die folgende Zeile hinzu:
Dadurch wird die Umgebungsvariable VISUAL auf nano festgelegt.
VISUAL ist eine Unix-Umgebungsvariable, die viele Programme - einschließlich crontab - zur Bearbeitung einer Datei aufrufen.
Speichern und schließen Sie die Datei nach dem Hinzufügen dieser Zeile, indem Sie STRG + X, Y, dann ENTER drücken.
Laden Sie dann .bash _ profile neu, damit die Shell die neue Änderung aufnimmt:
Im Editor können Sie jeden Auftrag für Ihren Zeitplan auf einer neuen Zeile eingeben.
Andernfalls können Sie den crontab vorerst speichern und schließen.
Wenn Sie Ihren crontab mit vi, dem Standard-Texteditor von CentOS 8, geöffnet haben, können Sie das tun, indem Sie ESC drücken, um sicherzustellen, dass Sie sich im Befehlsmodus von vi befinden. Dann geben Sie: x ein und drücken ENTER.
Bitte beachten Sie, dass auf Linux-Systemen ein weiteres crontab unter dem Verzeichnis / etc / gespeichert ist.
Dies ist ein systemweites crontab, das ein zusätzliches Feld für das Benutzerprofil hat, unter dem jeder cron-Auftrag ausgeführt werden soll.
Dieses Tutorial konzentriert sich auf nutzerspezifische crontabs. Wenn Sie den systemweiten crontab bearbeiten möchten, können Sie das mit dem folgenden Befehl tun:
Wenn Sie den Inhalt Ihres crontab anzeigen, aber ihn nicht bearbeiten möchten, können Sie folgenden Befehl verwenden:
Sie können Ihren crontab mit dem folgenden Befehl löschen:
< $> warning Warnung: Der folgende Befehl wird Sie nicht auffordern, zu bestätigen, dass Sie Ihren crontab löschen möchten.
Führen Sie ihn nur aus, wenn Sie sicher sind, dass Sie ihn löschen möchten. < $>
Dieser Befehl löscht den crontab des Benutzers sofort.
Sie können jedoch das Kennzeichen -i inkludieren, damit der Befehl Sie auffordert, zu bestätigen, dass Sie den crontab des Benutzers tatsächlich löschen möchten:
Wenn Sie aufgefordert wird, müssen Sie y eingeben, um den crontab zu löschen oder n, um die Löschung abzubrechen.
Cron-Auftrags-Ausgabe verwalten
Da cron-Aufträge im Hintergrund ausgeführt werden, ist nicht immer erkennbar, dass sie erfolgreich ausgeführt wurden.
Sie wissen bereits, wie Sie den Befehl crontab verwenden und einen cron-Auftrag planen können. Jetzt können Sie verschiedene Möglichkeiten der Umleitung der Ausgabe von cron-Aufträgen ausprobieren, damit Sie deren erfolgreiche Ausführung besser nachverfolgen können.
Wenn ein mail transfer agent ​ ​ - wie z. B. Sendmail - auf Ihrem Server installiert und richtig konfiguriert ist, können Sie die Ausgabe von cron-Aufgaben an die E-Mail-Adresse senden, die mit Ihrem Linux-Benutzerprofil verknüpft ist.
Sie können auch eine E-Mail-Adresse manuell angeben, indem Sie oben im crontab die Einstellung MAILTO bereitstellen.
Sie können einem crontab beispielsweise die folgenden Zeilen hinzufügen.
Dazu gehören ein MAILTO-Statement gefolgt von einer Beispiels-E-Mail-Adresse, eine SHELL-Anweisung, die die Shell angibt, die ausgeführt werden soll (in diesem Fall bash), eine HOME-Anweisung, die auf den Pfad verweist, auf dem nach der cron-Binärdatei gesucht werden soll sowie eine einzelne cron-Aufgabe:
Dieser spezielle Auftrag gibt "Diesen Befehl jede Minute ausführen" zurück und diese Ausgabe wird jede Minute an die E-Mail-Adresse gesendet, die nach der MAILTO-Anweisung angegeben wurde.
Sie können die Ausgabe einer cron-Aufgabe auch in eine Protokolldatei oder einen leeren Ort umleiten, um eine E-Mail mit der Ausgabe zu verhindern.
Um die Ausgabe eines geplanten Befehls einer Protokolldatei anzufügen, fügen Sie > > am Ende des Befehls hinzu, gefolgt von dem Namen und dem Ort Ihrer gewünschten Protokolldatei. Das sieht folgendermaßen aus:
Nehmen wir an, Sie möchten cron verwenden, um ein Skript auszuführen, aber Sie möchten es im Hintergrund ausführen.
Dazu können Sie die Ausgabe des Skripts auf einen leeren Ort wie / dev / null umleiten, der alle Daten, die ihm geschrieben werden, sofort löscht. Der folgende cron-Auftrag führt z. B. ein PHP-Skript aus und führt es im Hintergrund aus:
Dieser cron-Auftrag leitet auch standard error - durch 2 dargestellt - zu Standard-Ausgabe um (> & 1).
Da die Standard-Ausgabe bereits auf / dev / null umgeleitet wird, ermöglicht dies dem Skript im Grunde eine stille Ausführung.
Selbst wenn der crontab ein MAILTO-Statement enthält, wird die Ausgabe des Befehls nicht an die angegebene E-Mail-Adresse gesendet.
Zugriff einschränken
Mit den Dateien cron.allow und cron.deny können Sie verwalten, welche Benutzer den Befehl crontab verwenden dürfen. Beide sind im Verzeichnis / etc / gespeichert.
Wenn die Datei cron.deny vorhanden ist, wird jeder Benutzer, der in der Datei aufgeführt ist, von der Bearbeitung ihres crontabs gesperrt sein.
Wenn cron.allow vorhanden ist, können nur Benutzer, die darin aufgelistet sind, ihre crontabs bearbeiten.
Wenn beide Dateien vorhanden sind und der gleiche Benutzer in beiden Listen aufgeführt ist, überschreibt die Datei cron.allow cron.deny und der Benutzer kann seinen crontab bearbeiten.
Um beispielsweise allen Benutzern den Zugriff zu verwehren und dann dem Benutzer ishmael Zugriff zu geben, können Sie die folgende Befehlssequenz verwenden:
Wir sperren zunächst alle Benutzer, indem wir ALL an die Datei cron.deny anfügen.
Indem wir dann den Benutzernamen an die Datei cron.allow anfügen, geben wir dem Benutzerprofil ishmael Zugriff zum Ausführen von Cron-Aufträgen.
Beachten Sie, dass ein Benutzer mit sudo-Berechtigungen das crontab eines anderen Benutzers mit dem folgenden Befehl bearbeiten können:
Wenn jedoch cron.deny vorhanden ist und < ^ > user < ^ > darin aufgelistet ist und sie nicht in cron.allow aufgelistet sind, erhalten Sie nach Ausführung des vorherigen Befehls den folgenden Fehler:
Standardmäßig nehmen die meisten cron-Daemons an, dass alle Benutzer Zugriff auf cron haben, es sei denn, entweder cron.allow oder cron.deny ist vorhanden.
Spezielle Syntax
Es gibt auch verschiedene Kurzhand-Befehle, die Sie in Ihrer crontab-Datei verwenden können, um die Auftragsplanung zu optimieren.
Diese sind im Grunde Shortcuts für den angegebenen entsprechenden numerischen Zeitplan:
Shortcut
Kurzschrift für
@ hourly
0 * * * *
@ daily
0 0 * * *
@ weekly
0 0 * * 0
@ monthly
0 0 1 * *
@ yearly
0 0 1 1 *
< $> note Anmerkung: Nicht alle cron-Daemons können diese Syntax analysieren (insbesondere ältere Versionen). Vergewissern Sie sich daher, dass es funktioniert, bevor Sie sich darauf verlassen. < $>
Zusätzlich führt die Kurzschrift @ reboot jeden beliebigen Befehl aus, der ihr folgt, wenn der Server gestartet wird:
Verwenden Sie diese Shortcuts wann immer es möglich ist, um die Auslegung des Aufgabenplans in Ihrem crontab zu erleichtern.
Cron ist ein flexibles und leistungsfähiges Dienstprogramm, das die Belastung vieler Aufgaben in Zusammenhang mit der Systemverwaltung verringern kann.
In Kombination mit Shell-Skripts können Sie Aufgaben automatisieren, die normalerweise mühsam und kompliziert sind.
Empfohlene Schritte zum Härten von Apache unter FreeBSD 12.0
3757
Die standardmäßige Installation eines Apache-HTTP-Servers ist zwar bereits sicher, seine Konfiguration kann jedoch mit einigen Änderungen erheblich verbessert werden.
Sie können bereits vorhandene Sicherheitsmechanismen ergänzen, indem Sie z. B. Schutzmechanismen für Cookies und Header einrichten, damit Verbindungen auf der Client-Ebene des Benutzers nicht manipuliert werden können.
Dadurch können Sie die Möglichkeiten mehrerer Angriffsmethoden wie Cross-Site Scripting (auch als XSS bezeichnet) drastisch reduzieren.
Sie können auch andere Arten von Angriffen verhindern, wie z. B. Cross-Site Request Forgery, Session Hijacking und Denial-of-Service-Angriffe.
In diesem Tutorial werden Sie einige empfohlene Schritte implementieren, um die Informationen zu reduzieren, die auf Ihrem Server verfügbar sind.
Sie werden die Verzeichnislisten verifizieren und die Indizierung deaktivieren, um den Zugriff auf Ressourcen zu überprüfen.
Außerdem ändern Sie den Standardwert der Direktive timeout, um das Risiko von Angriffen wie Denial of Service zu verringern.
Des Weiteren deaktivieren Sie die TRACE-Methode, damit Sitzungen nicht umgekehrt und gehackt werden können.
Abschließend sichern Sie Header und Cookies.
Die meisten Konfigurationseinstellungen werden auf die Hauptkonfigurationsdatei Apache HTTP angewendet, die sich in / usr / local / etc / apache24 / httpd.conf befindet.
Bevor Sie diese Anleitung beginnen, benötigen Sie Folgendes:
Einen FreeBSD-12-Server, der gemäß dem Tutorial So starten Sie mit FreeBSD eingerichtet wird.
Eine Firewall, die gemäß der Konfiguration eines Firewall-Abschnitts im Artikel Empfohlenen Schritte für neue FreeBSD-12.0-Server eingerichtet ist.
Einen vollständigen FAMP Stack, der gemäß dem Tutorial So installieren Sie Apache, MySQL und PHP (FAMP) Stack unter FreeBSD 12.0 installiert wurde.
Ein Let 's-Encrypt-Zertifikat, das gemäß dem Tutorial So sichern Sie Apache mit Let' s Encrypt unter FreeBSD installiert wurde.
Mit diesen Voraussetzungen haben Sie ein FreeBSD-System mit einem Stack, das Webinhalt mit allem in PHP Geschriebenen, wie wesentliche CMS-Software, bereitstellen kann.
Außerdem haben Sie sichere Verbindungen über Let 's Encrypt verschlüsselt.
Reduzieren der Serverinformation
Das Betriebssystem-Banner ist eine Methode, die von Computern, Servern und Geräten aller Art zur Bereitstellung in Netzwerken verwendet wird.
Bösartige Akteure können diese Informationen verwenden, um relevante Systeme anzugreifen.
In diesem Abschnitt reduzieren Sie die Menge an Informationen, die von diesem Banner veröffentlicht werden.
Anweisungssätze ​ ​ ​ kontrollieren, wie diese Informationen angezeigt werden.
Zu diesem Zweck ist die Anweisung ServerTokens wichtig: Sie zeigt standardmäßig dem Client, zu dem die Verbindung hergestellt wird, alle Details über das Betriebssystem sowie die kompilierte Module an.
Sie verwenden ein Netzwerk-Scanning-Tool um zu prüfen, welche Informationen angezeigt werden, bevor Sie alle Änderungen vornehmen.
Um nmap zu installieren, führen Sie den folgenden Befehl aus:
Um die IP-Adresse Ihres Servers zu erhalten, können Sie den folgenden Befehl ausführen:
Sie können die Antwort des Webservers mit dem folgenden Befehl überprüfen:
Rufen Sie zum Ausführen eines Scans nmap auf (folglich das Flag -s), um die Version (das Flag -V) auf Port 80 (das Flag -p) auf der gegebenen IP oder Domäne anzuzeigen.
Sie erhalten Informationen über Ihren Webserver, ähnlich wie die Folgenden:
Diese Ausgabe zeigt, dass Informationen wie das Betriebssystem, die Apache-HTTP-Version und OpenSSL sichtbar sind.
Damit können Angreifer Informationen über den Server erhalten (zum Beispiel ein Sicherheitsrisiko in der Software, die auf dem Server ausgeführt wird) und die richtigen Tools für den Angriff auswählen.
Platzieren Sie die Direktive ServerTokens in die Hauptkonfigurationsdatei, da sie nicht standardmäßig konfiguriert ist.
Weil diese Konfiguration fehlt, zeigt Apache HTTP die vollständigen Informationen über den Server als Dokumentationszustand an.
Um die Informationen zu begrenzen, die über Ihren Server und Ihre Konfiguration preisgegeben werden, platzieren Sie die Direktive ServerTokens in die Hauptkonfigurationsdatei.
Platzieren Sie diese Direktive nach der ServerName-Eingabe in die Konfigurationsdatei.
Führen Sie den folgenden Befehl aus, um die Direktive zu suchen:
Danach können Sie die Zeilennummer mit vi suchen:
Führen Sie den folgenden Befehl aus:
Fügen Sie die folgende hervorgehobene Zeile hinzu:
Speichern und schließen Sie die Datei mit: wq und der EINGABETASTE.
Durch die Einstellung der Direktive ServerTokens auf Prod wird lediglich angezeigt, dass dies ein Apache-Webserver ist.
Damit dies in Kraft tritt, starten Sie den Apache-HTTP-Server neu:
Um die Änderungen zu testen, führen Sie den folgenden Befehl aus:
Sie sehen eine ähnliche Ausgabe wie die folgende, mit weniger Informationen über Ihren Apache-Webserver:
Sie haben gesehen, welche Informationen der Server vor der Änderung preisgegeben hat, und Sie haben dies nun auf ein Minimum reduziert.
Dadurch geben Sie einem externen Akteur weniger Hinweise über Ihren Server.
Im nächsten Schritt verwalten Sie die Verzeichnislisten für Ihren Webserver.
Verwalten von Verzeichnislisten
In diesem Schritt stellen Sie sicher, dass die Verzeichnisliste korrekt konfiguriert ist, damit die gewünschten Informationen über das System öffentlich verfügbar und die übrigen geschützt sind.
< $> note Anmerkung: Wenn ein Argument deklariert ist, ist es aktiv, aber das + kann visuell untermauern, dass es tatsächlich aktiviert ist.
Wenn ein Minuszeichen - eingegeben wird, wird das Argument verweigert, z. B. Options -Indexes.
Argumente mit + und / oder - können nicht gemischt werden - dies wird als schlechte Syntax in Apache HTTP erachtet und könnte beim Starten verweigert werden.
Durch Hinzufügen der Anweisung Options -Indexes wird der Inhalt im Datenpfad / usr / local / www / apache24 / data so eingestellt, dass (read listed) nicht automatisch indiziert wird, wenn keine .html-Datei vorhanden ist, sowie nicht gezeigt wird, wenn eine URL diesem Verzeichnis zugeordnet ist.
Dies gilt auch bei der Verwendung von virtuellen Host-Konfigurationen wie z. B. jener, die für das prerequisite tutorial für das Let 's-Encrypt-Zertifikat verwendet wird.
Sie richten die Direktive Options mit dem Argument -Indexes und der Direktive + FollowSymLinks ein, wodurch symbolische Links besucht werden können.
Sie verwenden das Symbol +, um die Konventionen von Apache HTTP zu befolgen.
Führen Sie den folgenden Befehl aus, um die Zeile zu suchen, die in der Konfigurationsdatei bearbeitet werden soll:
Führen Sie diesen Befehl aus, um direkt auf die Zeile zur Bearbeitung zuzugreifen:
Bearbeiten Sie nun die Zeile gemäß der Konfiguration:
Starten Sie Apache HTTP zur Implementierung dieser Änderungen neu:
Auf Ihrer Domäne im Browser sehen Sie eine "Zugriff verboten" -Nachricht, auch als Fehler 403 bekannt.
Grund dafür sind Ihre Änderungen.
Durch die Eingabe -Indexes in die Direktive Options wurde die Funktion Auto-Index von Apache HTTP deaktiviert und daher gibt es im Datenpfad keine index.html-Datei.
Das können Sie lösen, indem Sie eine index.html-Datei im VirtualHost ablegen, den Sie im prerequisite tutorial ​ ​ ​ für die Let 's-Encrypt-Zertifikate aktiviert haben.
Verwenden Sie den Standardblock innerhalb von Apache HTTP und legen Sie ihn im gleichen Ordner wie die DocumentRoot ab, die Sie im virtuellen Host deklariert haben.
Verwenden Sie hierzu den folgenden Befehl:
Jetzt sehen Sie die Nachricht It works!
beim Besuch Ihrer Domäne.
In diesem Abschnitt haben Sie der Direktive Indexes Beschränkungen hinzugefügt, um nicht automatisch unbeabsichtigten Inhalt einzutragen und anzuzeigen.
Wenn nun keine index.html-Datei im Datenpfad ist, erstellt Apache HTTP nicht automatisch ein Verzeichnis des Inhalts.
Im nächsten Schritt gehen Sie über das Verbergen von Informationen hinaus und passen verschiedene Anweisungen an.
Den Wert der Timeout-Direktive verringern
Die Direktive Timeout legt das Limit für die Zeit fest, die Apache HTTP auf eine neue Eingabe / Ausgabe wartet, bevor die Verbindungsanforderung fehlschlägt.
Dieser Fehler kann aufgrund von abweichenden Umständen auftreten, wie z. B. Pakete, die nicht beim Server ankommen, oder Daten, die vom Client nicht als empfangen bestätigt werden.
Standardmäßig ist das Zeitlimit auf 60 Sekunden eingestellt.
In Umgebungen, in denen das Internet langsam ist, ist dieser Standardwert eher sinnvoll. Eine Minute ist jedoch ziemlich lange, insbesondere dann, wenn der Server Benutzer mit schnellerem Internet abdeckt.
Außerdem kann die Zeit, in der der Server die Verbindung nicht abschließt, für Denial-of-Service-Angriffe (DoS) ausgenützt werden.
Wenn es zu einer Vielzahl dieser schädlichen Verbindungen kommt, gerät der Server ins Stolpern und könnte gesättigt werden und nicht mehr reagieren.
Um den Wert zu ändern, finden Sie die Timeout-Einträge in der Datei httpd-default.conf:
Sie sehen eine ähnliche Ausgabe wie die folgende:
In der Ausgabezeile legt 10 den Wert der Timeout-Direktive fest.
Um direkt auf diese Zeile zuzugreifen, führen Sie den folgenden Befehl aus:
Stellen Sie es zum Beispiel auf 30 Sekunden ein, wie nachfolgend gezeigt:
Der Wert der Direktive Timeout muss groß genug sein, damit eine legitime und erfolgreiche Verbindung hergestellt werden kann, und klein genug, um unerwünschte Verbindungs-Versuche zu verhindern.
< $> note Anmerkung: Denial-of-Service-Angriffe können die Ressourcen des Servers ziemlich effektiv erschöpfen.
Eine komplementäre und sehr fähige Gegenmaßnahme ist die Verwendung eines Threaded MPM, um für die Verbindungen und Prozesse von Apache HTTP die beste Leistung zu erzielen.
Im Tutorial So konfigurieren Sie Apache HTTP mit MPM Event und PHP-FPM unter FreeBSD 12.0 finden Sie Schritte zur Aktivierung dieser Fähigkeit.
Damit diese Änderung in Kraft tritt, starten Sie den Apache-HTTP-Server neu:
Sie haben den Standardwert der Timeout-Direktive geändert, um das Risiko von DoS-Angriffen teilweise zu reduzieren.
Deaktivieren der TRACE-Methode
Das Hypertext-Übertragungsprotokoll wurde nach einem Client-Server-Modell entwickelt und als solches hat das Protokoll Anforderungsmethoden, um Informationen vom Server abzurufen oder im Server abzulegen.
Der Server muss diese Methoden-Sätze und die Interaktion zwischen ihnen verstehen.
In diesem Schritt konfigurieren Sie die mindestens erforderlichen Methoden.
Die Methode TRACE, die als harmlos angesehen wurde, wurde für die Durchführung von Cross-Site-Tracing-Angriffen genutzt.
Diese Art von Angriffen ermöglichen bösartigen Akteuren, Benutzersitzungen mithilfe dieser Methode zu stehlen.
Die Methode wurde zum Zwecke des Debugging konzipiert, wobei der Server die gleiche Anfrage zurückgibt, die ursprünglich vom Client gesendet wurde.
Da das Cookie aus der Sitzung des Browsers an den Server gesendet wird, wird es erneut zurückgesendet.
Das könnte jedoch eventuell von einem bösartigen Akteur abgefangen werden, der dann die Verbindung eines Browsers zu einer Website, über die er die Kontrolle hat, umleiten kann anstatt zum ursprünglichen Server.
Aufgrund des potenziellen Missbrauchs der TRACE-Methode wird empfohlen, sie nur für das Debugging und nicht in der Produktion zu verwenden.
In diesem Abschnitt deaktivieren Sie diese Methode.
Bearbeiten Sie die Datei httpd.conf mit dem folgenden Befehl und drücken Sie dann G, um das Ende der Datei zu erreichen:
Fügen Sie am Ende der Datei folgenden Einstiegspfad hinzu:
Eine gute Praxis ist die Angabe der Methoden, die Sie auf Ihrem Apache-HTTP-Webserver verwenden.
Dadurch werden potenzielle Einstiegspunkte für bösartige Akteure begrenzt.
LimitExcept kann für diesen Zweck nützlich sein, da es keine anderen Methoden zulässt als die in ihm deklarierten. So kann beispielsweise eine Konfiguration wie diese erstellt werden:
Wie in der Direktive LimitExcept deklariert, sind nur die Methoden GET, POST und HEAD in der Konfiguration erlaubt.
Die GET-Methode ist Teil des HTTP-Protokolls und dient zum Abrufen von Daten.
Die POST-Methode ist auch Teil des HTTP-Protokolls und dient dazu, Daten an den Server zu senden.
Die HEAD-Methode ist ähnlich wie GET, hat jedoch keinen Antworttext.
Verwenden Sie den folgenden Befehl und legen Sie den LimitExcept-Block in der Datei ab:
Um diese Konfiguration einzustellen, legen Sie den folgenden Block im Direktiven-Eintrag DocumentRoot ab, aus dem der Inhalt gelesen wird, und zwar im Eintrag des Verzeichnisses:
Starten Sie Apache HTTP neu, damit die Änderungen übernommen werden:
Die neuere Direktive AllowedMethods ​ ​ ​ hat eine ähnliche Funktionalität, ihr Status ist jedoch noch experimentell.
Sie kennen nun HTTP-Methoden, ihre Verwendung und den Schutz, den sie vor bösartigen Aktivitäten mithilfe der TRACE-Methode bieten. Des Weiteren wissen Sie, wie man deklariert, welche Methoden verwendet werden sollen.
Als Nächstes arbeiten Sie mit weiteren Schutzmaßnahmen für HTTP-Header und Cookies.
Header und Cookies sichern
In diesem Schritt richten Sie spezifische Direktiven ein, um die Sitzungen zu schützen, die die Client-Rechner beim Besuch Ihres Apache-HTTP-Webservers öffnen.
So lädt Ihr Server keinen unerwünschten Inhalt, die Verschlüsselung wird nicht heruntergeladen und Sie vermeiden Content Sniffing.
Header sind Bestandteile der Anforderungsmethoden.
Es gibt Header zum Anpassen der Authentifizierung, für die Kommunikation zwischen Server und Client, Zwischenspeicherung, Inhaltsaushandlung, usw.
Cookies sind Informationen, die vom Server an den Browser gesendet werden.
Diese Informationen ermöglichen es dem Server, den Client-Browser von unterschiedlichen Computern zu unterscheiden.
Außerdem ermöglichen sie Servern, Benutzersitzungen zu erkennen.
Sie können beispielsweise den Einkaufswagen eines angemeldeten Benutzers, Zahlungsinformationen, den Verlauf, etc. verfolgen.
Cookies werden im Webbrowser des Clients verwendet und gespeichert, da HTTP ein zustandsloses Protokoll ist. Das bedeutet, sobald die Verbindung geschlossen wird, erinnert sich der Server nicht mehr an die Anfragen der einzelnen Clients.
Es ist wichtig, sowohl Header als auch Cookies zu schützen, da sie eine Kommunikation zwischen Webbrowser-Client und Webserver bereitstellen.
Das Modul Header ist standardmäßig aktiviert.
Überprüfen Sie mit dem folgenden Befehl, ob es geladen wurde:
Wenn Sie keine Ausgabe sehen, überprüfen Sie, ob das Modul in der Apache-Datei httpd.conf aktiviert ist:
Als Ausgabe sehen Sie eine unkommentierte Zeile, die auf das spezifische Modul für Header verweist:
Entfernen Sie den Hashtag am Anfang der Zeile mod _ headers.so, falls vorhanden, um die Direktive zu aktivieren.
Durch die Verwendung der folgenden Apache-HTTP-Direktiven schützen Sie Header und Cookies vor bösartigen Aktivitäten, um das Risiko für Clients und Server zu verringern.
Jetzt richten Sie den Schutz des Headers ein.
Legen Sie all diese Header-Werte in einen Block ab.
Sie können diese Werte nach eigenem Ermessen anwenden, es werden jedoch alle empfohlen.
Legen Sie den folgenden Block am Ende der Datei ab:
Header Strict-Transport-Security "max-age ​ ​, includeSubDomains ": HTTP Strict Transport Security (HTSTS) ist ein Mechanismus für Webserver und Clients (hauptsächlich Browser) zur Einrichtung von Kommunikationen mit ausschließlich HTTPS.
Durch die Implementierung vermeiden Sie Man-in-the-middle-Angriffe, bei denen ein Dritter in der Kommunikation potenziell auf Informationen zugreifen und diese manipulieren kann.
Header always edit Set-Cookie (.
*) "$1; HttpOnly; Secure ": Die Flags HttpOnly and Secure auf den Headern verringern das Risiko von Cross-Site-Scripting-Angriffen (XSS).
Cookies können von Angreifern missbraucht werden, um sich als legitime Besucher bzw. als eine andere Person auszugeben (Identitätsdiebstahl), oder sie können manipuliert werden.
Header set Referrer-Policy "strict-origin ​ ​ ": Der Header Referrer-Policy legt fest, welche Informationen im Header-Feld als Verweiser-Information enthalten sind.
Header set Content-Security-Policy "default-src 'self '; upgrade-insecure-requests; ": Der Content-Security-Policy header (CSP) verhindert, dass Inhalt geladen wird, der in den Parametern nicht angegeben ist. Das ist hilfreich, um Cross-Site-Scripting-Angriffe (XSS) zu verhindern.
Es gibt viele Parameter für die Konfiguration der Richtlinie dieses Headers.
Entscheidend ist die Konfiguration zum Laden von Inhalt derselben Site und das Aktualisieren jedes Inhalts mit HTTP-Ursprung.
Header set X-XSS-Protection Protection "1; mode = block ": Dies unterstützt ältere Browser, die mit Content-Security-Policy nicht fertig werden.
Der Header 'X-XSS-Protection' bietet Schutz vor Cross-Site-Scripting-Angriffen.
Sie müssen diesen Header nicht einrichten, außer Sie müssen alte Browser-Versionen unterstützen, was selten ist.
Header set X-Frame-Options: "deny ": Das verhindert clickjacking-Angriffe.
Der Header 'X-Frame-Options' teilt einen Browser mit, wenn eine Seite in ein < frame >, < iframe >, < embed > oder < object > gerendert werden kann.
So kann Inhalt von Sites nicht in andere eingebettet werden, wodurch Clickjacking-Angriffe verhindert werden.
Hier verweigern Sie jegliches Frame-Rendering, damit die Webseite nicht irgendwo anders eingebettet werden kann, nicht einmal innerhalb derselben Website.
Sie können dies Ihren Bedürfnissen anpassen, wenn Sie z. B. das Rendering bestimmter Seiten autorisieren müssen, da es sich um Anzeigen oder Kooperationen mit spezifischen Websites handelt.
Header set X-Content-Type-Options "nosniff ": Der Header 'X-Content-Type-Options' steuert MIME types, damit sie nicht geändert und gefolgt werden können.
MIME types sind Dateiformat-Standards: Sie funktionieren mit Text, Audio, Video, Image, usw.
Dieser Header hält bösartige Akteure von Content Sniffing der Dateien ab, sowie vor dem Versuch, Dateitypen zu ändern.
Starten Sie Apache neu, damit die Änderungen wirksam werden:
Um die Sicherheitsebenen Ihrer Konfigurationseinstellungen zu überprüfen, besuchen Sie die Website Security Headers.
Nach Ausführung der Schritte in diesem Tutorial wird Ihre Domäne die Note A erhalten.
< $> note Anmerkung: Wenn Sie Ihre Header auf https: / / securityheaders.com / überprüfen und ein F erhalten, könnte das daran liegen, dass in der DocumentRoot Ihrer Website kein index.html ist, wie am Ende von Schritt 2 beschrieben. Wenn Sie Ihre Header überprüfen und eine andere Note als A oder F erhalten, prüfen Sie jede Header set-Zeile auf Tippfehler, die zu der Herabstufung geführt haben könnten.
In diesem Schritt haben Sie mit bis zu sieben Einstellungen gearbeitet, um die Sicherheit Ihrer Header und Cookies zu verbessern.
Dadurch wird das Risiko von Cross-Site-Scripting, Clickjacking und anderen Arten von Angriffen verhindert.
In diesem Tutorial haben Sie sich mit verschiedenen Sicherheitsaspekten befasst, wie der Preisgabe von Informationen, dem Schutz von Sitzungen sowie mit alternativen Konfigurationseinstellungen für wichtige Funktionen.
Weitere Ressourcen zum Härten von Apache finden Sie hier:
Sicherheitstipps von Apache HTTP
Mozillas Sicherheitsrichtlinien
Empfehlungen für die Überwachung für Apache HTTP vom Center for Internet Security
Weitere Tools zum Schutz von Apache HTTP:
mod _ evasive ist ein nützliches Tool zur Eindämmung von DoS-Angriffen.
Weitere Informationen finden Sie im Tutorial So schützen Sie gegen DoS und DDoS mit mod _ evasive für Apache.
fail2ban ist eine Eindringschutz-Software zum Blockieren von wiederholten Anmeldeversuchen von nicht autorisierten Benutzern.
Weitere Informationen finden Sie im Tutorial So schützen Sie einen Apache-Server mit Fail2Ban.
ModSecurity ist eine Web Application Firewall (oder WAF) und bietet eine Vielzahl von Möglichkeiten auf der Grundlage von vordefinierten Regeln, die von SpyderLabs und Community-Mitgliedern geschrieben wurden.
Weitere Informationen dazu finden Sie im Tutorial So richten Sie ModSecurity mit Apache ein.
Grundlagen von Map und Set Objects in JavaScript
3752
Der Autor hat den Open Internet / Free Speech Fund dazu ausgewählt, eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
In JavaScript verbringen Entwickler oft viel Zeit bei der Entscheidung, welche Datenstruktur sie verwenden sollen.
Dies liegt daran, dass die Auswahl der richtigen Datenstruktur die spätere Datenbearbeitung erleichtert, Zeit spart, und Code leichter verständlich macht.
Die beiden vorherrschenden Datenstrukturen zur Speicherung von Datensammlungen sind Objects und Arrays (ein Objekttyp).
Entwickler verwenden Objects zum Speichern von Schlüssel / Wertpaaren und Arrays zum Speichern indizierter Listen.
Um Entwicklern jedoch mehr Flexibilität zu geben, hat die Spezifikation ECMAScript 2015 zwei neue Arten von iterierbaren Objekten eingeführt: Maps, die geordneten Sammlungen von Schlüssel / Wertpaaren, und Sets, die Sammlungen von eindeutigen Werten.
In diesem Artikel lernen Sie die Objekte Map und Set kennen sowie ihre Ähnlichkeiten oder Unterschiede zu Objects und Arrays, ihre verfügbaren Eigenschaften und Methoden und Beispiele praktischer Anwendungen.
Maps
Eine Map ist eine Sammlung von Schlüssel / Wertpaaren, die jeden Datentyp als Schlüssel verwenden und die Reihenfolge ihrer Einträge beibehalten kann.
Maps haben Elemente von Objects (ein eindeutiges Schlüssel / Wertpaar) und Arrays (eine geordnete Sammlung), sie sind jedoch konzeptionell den Objects ähnlicher.
Das liegt daran, dass die Einträge selbst Schlüssel / Wertpaare wie Objects sind, obwohl die Größe und Reihenfolge der Einträge wie bei einem Array beibehalten wird.
Maps können mit der Syntax new Map () initialisiert werden:
Dadurch erhalten Sie eine leere Map:
Einer Map Werte hinzufügen
Mit der Methode set () können Sie einer Map Werte hinzufügen.
Das erste Argument ist der Schlüssel und das zweite Argument ist der Wert.
Das Folgende fügt der map drei Schlüssel / Wertpaare hinzu:
Hier sehen wir, dass Maps Elemente von Objects und Arrays haben.
Wie bei einem Array haben wir eine nullindizierte Sammlung und wir können auch sehen, wie viele Elemente standardmäßig in der Map sind.
Maps verwenden die Syntax = > um Schlüssel / Wertpaare als key = > value zu kennzeichnen:
Dieses Beispiel sieht ähnlich wie ein reguläres Object mit Schlüsseln basierend auf Zeichenfolgen aus, aber mit Maps können wir jeden Datentyp als Schlüssel verwenden.
Zusätzlich zur manuellen Einstellung von Werten auf einer Map, können wir eine Map auch bereits mit Werten initialisieren.
Das tun wir mit einem Array von Arrays, die zwei Elemente enthalten, die jeweils Schlüssel / Wertpaare sind. Das sieht folgendermaßen aus:
Mit der folgenden Syntax können wir die gleiche Map neu erstellen:
< $> note Anmerkung: In diesem Beispiel werden nachgestellte Kommas - auch nachstehende Kommas genannt - verwendet.
Dies ist eine übliche Formatierung bei JavaScript, bei der das letzte Element in einer Reihe ein Komma am Ende hat, wenn eine Sammlung von Daten deklariert wird.
Diese Formatierung kann für sauberere diffs und leichtere Code-Bearbeitung gewählt werden, ihre Anwendung ist jedoch optional.
Weitere Informationen zu nachgestellten Kommas finden Sie im Artikel Nachgestelltes Komma in den MDN Web-Dokumenten.
Zufälligerweise ist diese Syntax die gleiche wie das Ergebnis des Aufrufs Object.entries () auf einem Object.
Das ist eine gebrauchsfertige Möglichkeit, ein Object in eine Map umzuwandeln, wie im folgenden Codeblock gezeigt:
Alternativ können Sie mit einer einzelnen Codezeile eine Map wieder in ein Objekt oder ein Array umwandeln.
Folgendes wandelt eine Map in ein Objekt um:
Dadurch ergibt sich der folgende Werte von obj:
Jetzt wandeln wir eine Map in ein Array um:
Dadurch ergibt sich das folgende Array für arr:
Map-Schlüssel
Maps akzeptieren jeden Datentyp als Schlüssel und lassen das Duplizieren von Schlüsselwerten nicht zu.
Zur Veranschaulichung können wir eine Map erstellen und Werte ohne Zeichenfolge als Schlüssel verwenden, sowie zwei Werte dem gleichen Schlüssel zuweisen.
Zuerst initialisieren wir eine Map mit Schlüssel ohne Zeichenfolgen:
Dieses Beispiel überschreibt den ersten Schlüssel von 1 mit dem nachfolgenden Schlüssel und behandelt die Zeichenfolge '1' und die Zahl 1 als eindeutige Schlüssel:
Obwohl viele glauben, dass ein reguläres JavaScript-Object bereits mit Zahlen, Booleans und anderen primitiven Datentypen wie Schlüssel umgehen kann, ist das nicht der Fall, da Objects alle Schlüssel in Zeichenfolgen umändern.
Initialisieren Sie zum Beispiel ein Objekt mit einem numerischen Schlüssel und vergleichen Sie den Wert mit einem numerischen Schlüssel 1 und einem Zahlenfolge-Schlüssel "1 ":
Das ist der Grund, weshalb die Zeichenfolge object Object gedruckt wird, wenn Sie versuchen, ein Object als Schlüssel zu verwenden.
Erstellen Sie zum Beispiel ein Object und verwenden Sie es dann als Schlüssel eines anderen Objects:
Dadurch erhalten Sie Folgendes:
Dies ist nicht der Fall mit Map.
Versuchen Sie, ein Object zu erstellen und es als Schlüssel einer Map festzulegen:
Der key des Map-Elements ist nun das Object, das wir erstellt haben.
Folgendes ist bei der Verwendung von Object oder Array als Schlüssel wichtig zu beachten: Die Map verwendet zum Vergleich der Gleichheit die Referenz zum Object, nicht den Literalwert.
In JavaScript gibt {} = = = {} false zurück, da die beiden Objects nicht die gleichen zwei Objects sind, obwohl sie den gleichen (leeren) Wert haben.
Das bedeutet, dass das Hinzufügen von zwei eindeutigen Objects mit dem gleichen Wert eine Map mit zwei Einträgen erstellt:
Wenn Sie dieselbe Object-Referenz zweimal verwenden, wird jedoch eine Map mit einem Eintrag erstellt.
Das führt zu Folgendem:
Das zweite set () aktualisiert den exakt gleichen Schlüssel wie das erste, sodass wir letztendlich eine Map mit nur einem Wert haben.
Elemente von einer Map abrufen und löschen
Einer der Nachteile von Objects ist, dass es schwierig sein kann, diese aufzuzählen oder mit allen Schlüsseln oder Werten zu arbeiten.
Die Map-Struktur hat dagegen eine Menge von integrierten Eigenschaften, wodurch man direkter mit ihren Elementen arbeitet kann.
Wir können eine neue Map initialisieren, um die folgenden Methoden und Eigenschaften zu demonstrieren: delete (), has (), get () und size.
Verwenden Sie die Methode has (), um das Vorhandensein eines Elements in einer Map zu überprüfen. has () gibt ein Boolean zurück.
Verwenden Sie die Methode get (), um mit einem Schlüssel einen Wert abzurufen.
Ein besonderer Vorteil von Maps gegenüber Objects ist, dass Sie jederzeit die Größe einer Map finden können, so wie Sie es mit einem Array können.
Die Anzahl der Elemente in einer Map können Sie mit der Eigenschaft size abrufen.
Dies beinhaltet weniger Schritte als die Umwandlung eines Objects in ein Array, um die length zu finden.
Verwenden Sie die Methode delete (), um ein Element mithilfe eines Schlüssels von einer Map zu entfernen.
Die Methode gibt ein Boolean true zurück, wenn ein Element vorhanden war und gelöscht wurde, und false, wenn es mit keinem Element übereinstimmt.
Dadurch ergibt sich folgende Map:
Zu guter Letzt können alle Werte in einer Map mit map.clear () gelöscht werden.
Dadurch erhalten Sie Folgendes:
Schlüssel, Werte und Einträge für Maps
Objects können Schlüssel, Werte und Einträge abrufen, indem Sie die Eigenschaften des Object-Konstruktors verwenden.
Maps hingegen haben Prototyp-Methoden, die es uns ermöglichen, die Schlüssel, Werte und Einträge der Map-Instanz direkt abzurufen.
Die Methoden keys () ​ ​ ​, values () und entries () geben alle einen MapIterator zurück, der ähnlich ist wie ein Array insofern als Sie for... of verwenden können, um die Werte zu durchlaufen.
Hier ist ein weiteres Beispiel einer Map, die wir zur Demonstration dieser Methoden verwenden können:
Die Methode keys () gibt die Schlüssel zurück:
Die Methode values () gibt die Werte zurück:
Die Methode entries () gibt ein Array von Schlüssel / Wert-Paaren zurück:
Iteration mit Map
Map verfügt - ähnlich wie bei einem Array - über eine integrierte forEach-Methode für die integrierte Iteration.
Worüber sie iterieren, unterscheidet sich jedoch ein wenig.
Das Callback von forEach einer Map iteriert über value, key und die map selbst, während die Array-Version über item, index und das array selbst iteriert.
Das ist ein großer Vorteil für Maps gegenüber Objects, da Objects mit keys (), values () oder entries () umgewandelt werden müssen und es keinen einfachen Weg gibt, die Eigenschaften eines Objects ohne Umwandlung abzurufen.
Um dies zu demonstrieren, iterieren wir über unsere Map und protokollieren die Schlüssel / Wertpaare in der Konsole:
Dies ergibt:
Da eine for... of-Schleife über Iterablen wie Map und Array iteriert, können wir durch Destrukturierung des Array der Map-Elemente genau das gleiche Ergebnis erhalten.
Eigenschaften und Methoden von Map
Die folgende Tabelle zeigt eine Liste der Eigenschaften und Methoden von Map für einen schnellen Überblick:
Eigenschaften / Methoden
Beschreibung
Rückgabe
set (key, value)
Hängt einer Map ein Schlüssel / Wert-Paar an
Map Object
delete (key)
Entfernt ein Schlüssel / Wertpaar von einer Map mithilfe eines Schlüssels
Boolean
get (key)
Gibt einen Wert mithilfe eines Schlüssels zurück
Wert
has (key)
Überprüft das Vorhandensein eines Elements in einer Map mithilfe eines Schlüssels
clear ()
Entfernt alle Elemente von einer Map
N / A
keys ()
Gibt alle Schlüssel in einer Map zurück
MapIterator object
values ()
Gibt alle Werte in einer Map zurück
entries ()
Gibt alle Schlüssel und Werte einer Map als [key, value] zurück
forEach ()
Iteriert über die Map in der Reihenfolge der Einfügungen
size
Gibt die Anzahl der Elemente in einer Map zurück
Zahl
Wann man Map verwendet
Zusammengefasst kann man sagen: Maps sind insofern Objekten ähnlich, als sie Schlüssel / Wert-Paare enthalten, aber Maps haben mehrere Vorteile gegenüber Objekten:
Size - Maps haben die Eigenschaft size, während Objects keine integrierte Möglichkeit zum Abruf ihrer size haben.
Iteration - Maps sind direkt iterierbar, Objekte nicht.
Flexibilität - Maps können einen beliebigen Datentyp (primitiv oder Object) als Schlüssel für einen Wert haben, während Objects nur Zeichenfolgen haben können.
Geordnet - Maps behalten die Reihenfolge ihrer Einfügungen bei, während Objects keine garantierte Reihenfolge haben.
Aufgrund dieser Faktoren sind Maps eine leistungsstarke Datenstruktur, die in Betracht gezogen werden sollte.
Objects haben jedoch auch einige wichtige Vorteile:
JSON - Objects funktionieren einwandfrei mit JSON.parse () und JSON.stringify (), zwei wesentliche Funktionen zum Arbeiten mit JSON, einem gängigen Datenformat, mit dem viele REST-APIs zu tun haben.
Mit einem einzelnen Element arbeiten - Wenn Sie mit einem bekannten Wert in einem Object arbeiten, können Sie direkt mit dem Schlüssel darauf zugreifen, ohne eine Methode wie Maps get () verwenden zu müssen.
Das Hilf Ihnen bei der Entscheidung, ob Map oder Object die richtige Datenstruktur für Ihren Anwendungsfall ist.
Set
Ein Set ist eine Sammlung von eindeutigen Werten.
Im Gegensatz zu einer Map ist ein Set einem Array konzeptionell ähnlicher als einem Object, da es eine Liste von Werten und nicht Schlüssel / Wertpaaren ist.
Set ist jedoch kein Ersatz für Arrays, sondern eine Ergänzung für zusätzliche Unterstützung für die Arbeit mit duplizierten Daten.
Sie können Sets mit der Syntax new Set () initialisieren.
Dadurch erhalten Sie ein leeres Set:
Elemente können einem Set mit der Methode add () hinzugefügt werden.
(Das darf nicht mit der Methode set () für Map verwechselt werden, obwohl sie ähnlich ist.)
Nachdem Sets nur eindeutige Werte enthalten können, wird jeder Versuch, einen bereits vorhandenen Wert hinzuzufügen, ignoriert.
< $> note Anmerkung: Derselbe Gleichheits-Vergleich, der für Map-Schlüssel gilt, gilt auch für Set-Elemente.
Zwei Objekte, die den gleichen Wert haben, aber nicht die gleiche Referenz teilen, werden nicht als gleich angesehen.
Sie können Sets auch mit einem Array von Werten initialisieren.
Wenn im Array doppelte Werte vorhanden sind, werden sie vom Set entfernt.
Umgekehrt kann ein Set mit einer Code-Zeile in ein Array umgewandelt werden:
Set hat viele gleiche Methoden und Eigenschaften wie Map, einschließlich delete (), ​ has (), clear () und size.
Beachten Sie, dass Set keine Möglichkeit hat, auf einen Wert über einen Schlüssel oder einen Index zuzugreifen, wie Map.get (key) oder arr [index].
Schlüssel, Werte und Einträge für Sets
Map und Set haben beide die Methoden keys (), values () und entries (), die einen Iterator zurückgeben.
Bei Map hat jede dieser Methoden ihren eigenen Zweck. Sets haben jedoch keine Schlüssel und daher sind Schlüssel ein Alias für Werte.
Das bedeutet, dass keys () und values () beide den gleichen Iterator zurückgeben und entries () den Wert zweimal zurückgeben.
Es ist am sinnvollsten, nur values () mit Set zu verwenden, da die beiden anderen Methoden für die Einheitlichkeit und übergreifende Kompatibilität mit Map vorhanden sind.
Iteration mit Set
Wie Map hat auch Set eine integrierte forEach () -Methode.
Da Sets keine Schlüssel haben, geben der erste und der zweite Parameter des forEach () -Callbacks den gleichen Wert zurück, sodass es dafür keinen Anwendungsfall außerhalb der Kompatibilität mit Map gibt.
Die Parameter von forEach () sind (value, key, set).
Sowohl forEach () als auch for... of können für Set verwendet werden.
Sehen wir uns zuerst die Iteration forEach () an:
Dann können wir die Version for... of schreiben:
Beide Strategien ergeben Folgendes:
Eigenschaften und Methoden von Set
Die folgende Tabelle zeigt eine Liste der Eigenschaften und Methoden von Set für einen schnellen Überblick:
add (value)
Hängt einem Set ein neues Element an
Set Object
delete (value)
Entfernt das angegebene Element von einem Set
has ()
Überprüft, ob ein Element in einem Set vorhanden ist
Entfernt alle Elemente von einem Set
Gibt alle Werte eines Sets zurück (genau wie values ())
SetIterator object
Gibt alle Werte eines Sets zurück (genau wie values ())
Gibt alle Werte eines Sets als [value, value] zurück
Iteriert über das Set in der Reihenfolge der Einfügungen
Gibt die Anzahl der Elemente eines Sets zurück
Wann man Set verwendet
Set ist eine nützliche Ergänzung zu Ihrem JavaScript-Toolkit, insbesondere für die Arbeit mit doppelten Werten bei Daten.
In einer einzelnen Zeile können wir ein neues Array erstellen, ohne doppelte Werte eines Arrays, das doppelte Werte hat.
Set kann verwendet werden, um Union, Intersection und Difference von zwei Datensätzen zu suchen. Arrays haben Sets gegenüber jedoch den erheblichen Vorteil der zusätzlichen Datenmanipulation aufgrund der Methoden sort (), map (), filter () und reduce () sowie der direkten Kompatibilität mit JSON-Methoden.
In diesem Artikel haben Sie gelernt, dass eine Map eine Sammlung von geordneten Schlüssel / Wertpaaren und ein Set eine Sammlung von eindeutigen Werten ist.
Beide Datenstrukturen fügen JavaScript zusätzliche Fähigkeiten hinzu und vereinfachen jeweils gängige Aufgaben wie das Auffinden der Länge eines Schlüssel / Wertpaares und das Entfernen doppelter Elemente aus einem Datensatz.
Andererseits werden Objects und Arrays traditionell für die Datenspeicherung und -Bearbeitung in JavaScript verwendet und haben direkte Kompatibilität mit JSON, wodurch sie weiterhin die wichtigsten Datenstrukturen bleiben, insbesondere für die Arbeit mit REST-APIs.
Maps und Sets sind in erster Linie nützlich als unterstützende Datenstrukturen für Objekte und Arrays.
Wenn Sie mehr über JavaScript erfahren möchten, besuchen Sie unsere Homepage für unsere Reihe So codieren Sie in JavaScript oder durchsuchen Sie unsere Reihe So codieren Sie in Node.js für Artikel über Back-End-Entwicklung.
Empfohlene Schritte zur Sicherung eines DigitalOcean Kubernetes Cluster
3870
Der Autor wählte die Kooperation Open Sourcing Mental Illness, um eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
Kubernetes, die Open-Source-Container-Orchestrierungsplattform, wird immer mehr zur bevorzugten Lösung für die Automatisierung, Skalierung und Verwaltung von Hochverfügbarkeits-Clustern.
Aufgrund der zunehmenden Popularität wird die Sicherheit von Kubernetes immer wichtiger.
In Anbetracht der beweglichen Teile, die in Kubernetes involviert sind, und der Vielfalt der Einsatzszenarien kann die Sicherung von Kubernetes manchmal komplex sein.
Ziel dieses Artikels ist es daher, eine solide Sicherheitsbasis für einen DigitalOcean Kubernetes (DOKS) Cluster bereitzustellen.
Beachten Sie, dass dieses Tutorial grundlegende Sicherheitsmaßnahmen für Kubernetes behandelt und eher als Startpunkt und nicht als erschöpfender Leitfaden gedacht ist.
Zusätzliche Schritte finden Sie in der offiziellen Kubernetes-Dokumentation.
In diesem Tutorial nehmen Sie grundlegende Schritte zur Sicherung Ihres DigitalOcean Kubernetes Clusters vor.
Sie konfigurieren eine sichere lokale Authentifizierung mit TLS / SSL-Zertifikaten, vergeben Berechtigungen an lokale Benutzer mit rollenbasierten Zugriffskontrollen (RBAC) sowie an Kubernetes-Anwendungen und -Bereitstellungen mit Service Accounts, und richten Ressourcenbegrenzungen mit den Zugriffs-Controllern ResourceQuota und LimitRange ein.
Um dieses Tutorial auszuführen, benötigen Sie Folgendes:
Einen von DigitalOcean Kubernetes (DOKS) verwalteten Cluster mit 3 Standardknoten, die jeweils mit mindestens 2 GB RAM und 1 vCPU konfiguriert sind.
Eine detaillierte Anleitung für die Erstellung eines DOKS-Clusters finden Sie in unserem Leitfaden Kubernetes Quickstart.
Dieses Tutorial verwendet die DOKS-Version 1.16.2-do.
Einen für die Verwaltung des DOKS-Clusters konfigurierten lokalen Client, mit einer vom DigitalOcean Control Panel heruntergeladenen Cluster-Konfigurationsdatei, gespeichert unter ~ / .kube / config.
Eine detaillierte Anleitung für die Konfiguration eines auf Remote-Ebene verwalteten DOKS finden Sie im Leitfaden Verbinden mit einem DigitalOcean Kubernetes Cluster.
Im Einzelnen benötigen Sie Folgendes:
Die auf Ihrem lokalen Rechner installierte Befehlszeilenschnittstelle kubectl.
Mehr über die Installation und Konfiguration von kubectl können Sie der offiziellen Dokumentation entnehmen.
Dieses Tutorial verwendet die kubectl-Version 1.17.0-00.
Das offizielle DigitalOcean-Befehlszeilen-Tool doctl.
Anweisungen für die Installation finden Sie auf der Seite doctl GitHub Page.
Dieses Tutorial verwendet die Version doctl 1.36.0.
Schritt 1 - Aktivieren der Remotebenutzer-Authentifizierung
Nachdem Erfüllung der Voraussetzungen erhalten Sie einen Kubernetes-Superuser, der sich über ein vordefiniertes DigitalOcean-Träger-Token authentifiziert.
Die gemeinschaftliche Nutzung dieses Berechtigungsnachweises ist jedoch keine gute Sicherheitspraxis, da dieses Konto umfangreiche und möglicherweise destruktive Änderungen an Ihrem Cluster verursachen kann.
Um diese Möglichkeit einzugrenzen, können Sie zusätzliche Benutzer einrichten, die von ihren jeweiligen lokalen Clients authentifiziert werden.
In diesem Abschnitt authentifizieren Sie neue Benutzer beim DOKS-Remote-Cluster von lokalen Clients aus mit sicheren SSL / TLS-Zertifikaten.
Dies ist ein dreistufiger Prozess: Zuerst generieren Sie für jeden Benutzer eine Zertifikatsignierungsanforderung, die Certificate Signing Requests (CSR) ​ ​. Danach genehmigen Sie diese Zertifikate direkt im Cluster über kubectl.
Abschließend erstellen Sie für jeden Benutzer eine kubeconfig-Datei mit den entsprechenden Zertifikaten.
Weitere Informationen über zusätzliche von Kubernetes unterstützte Authentifizierungsverfahren finden Sie in der Kubernetes-Authentifizierungsdokumentation.
Erstellen von Zertifikatsignierungsanforderungen für neue Benutzer
Prüfen Sie vor dem Start die DOKS-Clusterverbindung von dem lokalen Rechner aus, der bei den Voraussetzungen konfiguriert wurde:
Je nach Ihrer Konfiguration sehen Sie ein ähnliches Output:
Das bedeutet, dass Sie mit dem DOKS-Cluster verbunden sind.
Erstellen Sie als Nächstes einen lokalen Ordner für die Client-Zertifikate.
In diesem Tutorial nutzen Sie zur Speicherung aller Zertifikate ~ / certs.
Sie autorisieren in diesem Tutorial einen neuen Benutzer namens sammy, um auf den Cluster zuzugreifen.
Sie können dies auch auf einen Benutzer Ihrer Wahl ändern.
Generieren Sie mit der SSL- und TLS-Bibliothek OpenSSL einen neuen privaten Schlüssel für Ihren Benutzer mit dem folgenden Befehl:
Das -out-Flag erzeugt die Output-Datei ~ / < ^ > certs < ^ > / < ^ > sammy < ^ > .key, und 4096 setzt den Schlüssel als 4096-Bit.
Weitere Informationen über OpenSSL finden Sie in unserem Leitfaden OpenSSL Essentials.
Erstellen Sie nun eine Konfigurationsdatei zur Zertifikatsignierungsanforderung.
Öffnen Sie die folgende Datei mit einem Texteditor (für dieses Tutorial verwenden wir nano):
Fügen Sie folgenden Inhalt in die Datei < ^ > sammy.csr.cnf < ^ > ein, um im Subjekt den gewünschten Benutzernamen als "common name" (CN) und die Gruppe als "organization" (O) festzulegen:
Die Konfigurationsdatei für die Zertifikatsignierungsanforderung enthält alle erforderlichen Informationen, die Benutzeridentität und entsprechende Verwendungsparameter für den Benutzer.
Das letzte Argument extendedKeyUsage = serverAuth, clientAuth ermöglicht Benutzern die Authentifizierung ihrer lokalen Clients mit dem DOKS-Cluster mittels des Zertifikats, sobald es unterzeichnet wurde.
Erstellen Sie als Nächstes die Zertifikatsignierungsanforderung (CSR) für sammy:
Mit -config können Sie die Konfigurationsdatei für die CSR festlegen, und -new signalisiert die Erstellung einer neuen CSR für den durch -key festgelegten Schlüssel.
Sie können Ihre Zertifikatsignierungsanforderung überprüfen, indem Sie folgenden Befehl ausführen:
Hier geben Sie die CSR mit -in an und verwenden -text, um die Zertifikatsanforderung in Text auszudrucken.
Das Output zeigt die Zertifikatsanforderung, deren Anfang folgendermaßen aussieht:
Wiederholen Sie den gleichen Vorgang, um CSRs für alle weiteren Benutzer zu erstellen.
Wenn Sie alle Zertifikatsignierungsanforderungen in dem Ordner ~ / certs des Administrators gespeichert haben, fahren Sie mit dem nächsten Schritt fort, um sie zu genehmigen.
Verwalten von Zertifikatsignierungsanforderungen mit der Kubernetes-API
Sie können die TLS-Zertifikate, die für die Kubernetes API ausgegeben wurden, mit dem Befehlszeilen-Tool kubectl entweder genehmigen oder ablehnen.
So können Sie sicherstellen, dass der angeforderte Zugang für den jeweiligen Benutzer geeignet ist.
In diesem Abschnitt senden Sie die Zertifikatsanforderung für sammy und genehmigen sie.
Zum Senden einer CSR an den DOKS-Cluster verwenden Sie den folgenden Befehl:
Dieser Befehl mit einem Bash-Heredoc nutzt cat, um die Zertifikatsanforderung an kubectl apply weiterzuleiten.
Sehen Sie sich die Zertifikatsanforderung genauer an:
name: < ^ > sammy-authentication < ^ > erstellt einen Metadaten-Identifizierer, hier < ^ > sammy-authentication < ^ > genannt.
request: $(cat ~ / < ^ > certs < ^ > / < ^ > sammy < ^ > .csr | Basis64 | tr -d '\ n ') sendet die Zertifikatsignierungsanforderung sammy.csr als Base64 kodiert an den Cluster.
server auth und client auth geben die beabsichtigte Verwendung des Zertifikats an.
In diesem Fall ist der Zweck die Benutzerauthentifizierung.
Das Output sieht ungefähr wie folgt aus:
Sie können den Status der Zertifikatsignierungsanforderung mit diesem Befehl prüfen:
Je nach Ihrer Cluster-Konfiguration sehen Sie ein ähnliches Output:
Als Nächstes genehmigen Sie die CSR durch Verwendung des Befehls:
Sie erhalten eine Nachricht, die der Operation bestätigt:
< $> note Anmerkung: Als Administrator können Sie eine CSR auch mit dem Befehl kubectl certificate deny < ^ > sammy-authentication < ^ > ablehnen.
Weitere Informationen über die Verwaltung von TLS-Zertifikaten finden Sie in der offiziellen Dokumentation von Kubernetes.
Nachdem die CSR nun genehmigt wurde, können Sie sie zum lokalen Rechner herunterladen, indem Sie Folgendes ausführen:
Dieser Befehl dekodiert das Base64-Zertifikat für eine richtige Anwendung durch kubectl und speichert es dann als ~ / < ^ > certs < ^ > / < ^ > sammy < ^ > .crt.
Mit dem signierten sammy-Zertifikat können Sie nun die kubeconfig-Datei des Benutzers erstellen.
Erstellen der Remotebenutzer-Kubeconfig
Als Nächstes erstellen Sie eine spezifische kubeconfig-Datei für den sammy-Benutzer.
Dadurch erhalten Sie mehr Kontrolle über den Zugriff des Benutzers auf Ihren Cluster.
Der erste Schritt beim Erstellen einer neuen kubeconfig ist die Erstellung einer Kopie der aktuellen kubeconfig-Datei.
In diesem Tutorial benennen Sie die neue kubeconfig-Datei < ^ > config-sammy < ^ >:
Als Nächstes editieren Sie die neue Datei:
Behalten Sie die ersten acht Zeilen dieser Datei, da sie die erforderlichen Informationen für die SSL / TLS-Verbindung mit dem Cluster enthalten.
Ersetzen Sie dann ausgehend vom Benutzerparameter den Text durch die folgenden hervorgehobenen Zeilen, damit die Datei wie folgt aussieht:
< $> note Anmerkung: Sowohl für client-certificate als auch für client-key verwenden Sie den absoluten Pfad zu deren entsprechenden Zertifikatsstandorten.
Andernfalls erzeugt kubectl einen Fehler.
Sie können die neue Benutzerverbindung mit kubectl cluster-info testen:
Sie erhalten einen ähnlichen Fehler:
Dieser Fehler wird erwartet, weil der Benutzer sammy noch keine Berechtigung hat, eine Ressource auf dem Cluster zu listen.
Die Erteilung von Berechtigungen an Benutzer wird im nächsten Schritt gezeigt.
Das jetzige Output bestätigt, dass die SSL / TLS-Verbindung erfolgreich war und die Authentifizierungsangaben von sammy von der Kubernetes-API akzeptiert wurden.
Schritt 2 - Autorisieren von Benutzern über die Rollenbasierte Zugriffskontrolle (RBAC)
Nach der Authentifizierung eines Benutzers bestimmt die API seine Berechtigungen mit dem in Kubernetes integrierten Modell der Role Based Access Control (RBAC), der rollenbasierten Zugriffskontrolle.
RBAC ist eine wirksame Methode zur Beschränkung von Benutzerrechten auf der Grundlage der jeweils zugewiesenen Rolle. Aus sicherheitstechnischer Sicht erlaubt RBAC das Festlegen genau definierter Berechtigungen und limitiert damit die Möglichkeiten von Benutzern, auf vertrauliche Daten zuzugreifen oder Befehle auf Superuser-Ebene auszuführen.
Weitere detaillierte Informationen über Benutzerrollen finden Sie in Kubernetes-RBAC-Dokumentation.
In diesem Schritt verwenden Sie kubectl, um die vordefinierte Rolle edit dem Benutzer sammy im default-Namensraum zuzuweisen.
In einer produktiven Umgebung können Sie auch benutzerdefinierte Rollen und / oder benutzerdefinierte Rollenbindungen verwenden.
Berechtigungen erteilen
In Kubernetes bedeutet die Erteilung von Berechtigungen, einem Benutzer die gewünschte Rolle zuzuweisen.
Erteilen Sie edit-Berechtigungen an den Benutzer sammy im default-Namensraum mit diesem Befehl:
Sie erhalten ein Output, das dem Folgenden ähnelt:
Im Einzelnen setzt sich der Befehl wie folgt zusammen:
create rolebinding < ^ > sammy-edit-role < ^ > erstellt eine neue Rollenbindung, in diesem Fall namens < ^ > sammy-edit-role < ^ >.
--clusterrole = edit wendet die vordefinierte Rolle edit auf einen globalen Gültigkeitsbereich (Cluster-Rolle) an.
--user = < ^ > sammy < ^ > gibt an, an welchen Benutzer die Rolle gebunden wird.
--namespace = default erteilt die Rollenberechtigung des Benutzers im angegebenen Namensraum, in diesem Fall default.
Als Nächstes verifizieren Sie die Benutzerberechtigung, indem Sie Pods in den default-Namensraum auflisten.
Sie können erkennen, ob die RBAC-Autorisation wie erwartet funktioniert, wenn keine Fehler angezeigt werden.
Sie erhalten folgendes Output:
Sie haben sammy nun Berechtigungen zugewiesen. Im nächsten Abschnitt sehen Sie, wie Sie diese Berechtigungen wieder entziehen.
Berechtigungen entziehen
In Kubernetes entziehen Sie Berechtigungen, indem Sie die Rollenbindung des Benutzers entfernen.
In diesem Tutorial löschen Sie die edit-Rolle des Benutzers sammy, indem Sie folgenden Befehl ausführen:
Überprüfen Sie, ob die Berechtigungen wie erwartet entzogen wurden, indem Sie Pods im default-Namensraum auflisten:
Sie erhalten die folgende Fehlermeldung:
Diese zeigt, dass die Autorisierung entzogen wurde.
Aus sicherheitstechnischer Sicht gibt das Kubernetes-Autorisierungsmodell Cluster-Administratoren die Flexibilität, die Benutzerrechte bei Bedarf zu ändern.
Außerdem ist die rollenbasierte Zugriffskontrolle nicht auf einen physischen Benutzer beschränkt; Sie können auch Cluster-Diensten Berechtigungen erteilen und entziehen. Mehr dazu erfahren Sie im nächsten Abschnitt.
Weitere Informationen über die RBAC-Autorisation und wie Sie benutzerdefinierte Rollen erstellen, finden Sie in der offiziellen Dokumentation.
Schritt 3 -Verwalten von Anwendungsberechtigungen mit Service Accounts
Wie im vorherigen Abschnitt erwähnt, erstrecken sich die RBAC-Autorisierungsmechanismen nicht nur auf menschliche Benutzer.
Nicht-menschliche Cluster-Benutzer wie Anwendungen, Dienste und Prozesse, die in Pods ausgeführt werden, authentifizieren sich mit dem API-Server über Dienstkonten, die in Kubernetes Service Accounts (SAs) genannt werden.
Wenn ein Pod in einem Namensraum erstellt wird, können Sie hierfür entweder das default-Service Account verwenden oder ein Service Account Ihrer Wahl definieren.
Die Möglichkeit, Anwendungen und Prozessen individuelle SAs zuzuordnen, gibt Administratoren die Freiheit, Berechtigungen nach Bedarf zu erteilen oder zu entziehen.
Außerdem wird die Zuordnung bestimmter SAs zu produktionskritischen Anwendungen als beste Sicherheitspraxis angesehen.
Da Service Accounts für die Authentifizierung und damit für die RBAC-Autorisierungsprüfungen verwendet werden, können Cluster-Administratoren Sicherheitsbedrohungen eindämmen, indem sie die Zugriffsrechte für Service Accounts ändern und den fehlerhaften Prozess isolieren.
Zur Veranschaulichung von Service Accounts verwendet dieses Tutorial einen Nginx-Webserver als Beispielanwendung.
Bevor Sie Ihrer Anwendung ein bestimmtes SA zuweisen, müssen Sie dieses SA erstellen.
Erstellen Sie ein neues Service Account namens < ^ > nginx-sa < ^ > im default-Namensraum:
Sie erhalten Folgendes:
Überprüfen Sie, ob das Service Account erstellt wurde, indem Sie Folgendes ausführen:
Dadurch erhalten Sie eine Liste Ihrer Service Accounts:
Nun erteilen Sie dem Service Account < ^ > nginx-sa < ^ > eine Rolle.
In diesem Beispiel erteilen Sie < ^ > nginx-sa < ^ > dieselben Berechtigungen wie dem Benutzer sammy:
Nach der Ausführung erhalten Sie Folgendes:
Dieser Befehl verwendet das gleiche Format wie für den Benutzer sammy. Ausgenommen hierbei ist das Flag serviceaccount = default: nginx-sa, wo Sie das Service Account < ^ > nginx-sa < ^ > dem default-Namensraum zuweisen.
Überprüfen Sie mit diesem Befehl, ob die Rollenbindung erfolgreich war:
Dadurch erhalten Sie folgendes Output:
Nachdem Sie bestätigt haben, dass die Rollenbindung für das Service Account erfolgreich konfiguriert wurde, können Sie das Service Account einer Anwendung zuweisen.
Wenn Sie einer Anwendung ein bestimmtes Service Account zuweisen, können Sie dessen Zugriffsrechte in Echtzeit verwalten und somit die Sicherheit des Clusters erhöhen.
In diesem Tutorial dient ein nginx-Pod als Beispielanwendung.
Erstellen Sie den neuen Pod und bestimmen Sie das Service Account < ^ > nginx-sa < ^ > mit dem folgenden Befehl:
Der erste Teil des Befehls erstellt einen neuen Pod, der einen nginx-Webserver auf Port: 80 ausführt. Der zweite Teil --serviceaccount = "< ^ > nginx-sa < ^ >" zeigt an, dass dieser Pod das < ^ > nginx-sa < ^ > -Service Account und nicht das default-Service Account verwenden soll.
Sie erhalten ein Output, das dem Folgenden ähnelt:
Überprüfen Sie mit kubectl describe, ob die neue Anwendung das Service Account verwendet:
Dadurch erhalten Sie eine längere Beschreibung der Bereitstellungsparameter.
Unter dem Abschnitt Pod Template sehen Sie ein Output, das diesem ähnelt:
In diesem Abschnitt haben Sie das Service Account < ^ > nginx-sa < ^ > im default-Namensraum erstellt und dem nginx-Webserver zugewiesen.
Sie können nun die nginx-Berechtigungen in Echtzeit steuern, indem Sie seine Rolle nach Bedarf ändern.
Sie können Anwendungen auch gruppieren, indem Sie jeder dasselbe Service Account zuweisen und dann Mehrfachänderungen (bulk changes) an Berechtigungen vornehmen.
Sie können zudem kritische Anwendungen isolieren, indem Sie diesen ein individuelles SA zuweisen.
Die Idee hinter der Zuweisung von Rollen an Ihre Anwendungen / Bereitstellungen besteht auch darin, eine Feinabstimmung der Berechtigungen vornehmen zu können.
In realen Produktionsumgebungen haben Sie möglicherweise mehrere Bereitstellungen, die unterschiedliche Berechtigungen erfordern, die von reinen Leseberechtigungen bis hin zu vollen Administratorrechten reichen.
Mit RBAC können Sie den Zugriff auf den Cluster flexibel nach Bedarf einschränken.
Als Nächstes richten Sie Zugangs-Controller ein, um Ressourcen zu kontrollieren und gegen übermäßigen Ressourcenverbrauch zu schützen.
Schritt 4 - Einrichten von Zugriffs-Controllern
Die Zugriffs-Controller von Kubernetes sind optionale Plugins, die in das kube-apiserver-Binärprogramm kompiliert werden, um die Sicherheitsoptionen zu erweitern.
Zugriffs-Controller fangen Anfragen ab, nachdem diese die Authentifizierungs- und Autorisierungsphase durchlaufen haben.
Nach dem Abfangen der Anfrage führen die Zugriffs-Controller den jeweils festgelegten Code aus, bevor die Anfrage ausgeführt wird.
Während das Ergebnis einer Authentifizierungs- oder Autorisierungsprüfung ein boolescher Wahr-oder-Falsch-Wert ist, der die Anfrage entweder erlaubt oder ablehnt, können Zugriffs-Controller wesentlich vielfältiger agieren.
Zugriffs-Controller können Anfragen in gleicher Weise wie eine Authentifizierung validieren. Sie können jedoch auch zusätzlich Anfragen mutieren und ändern sowie Objekte modifizieren, bevor diese zugelassen werden.
In diesem Schritt verwenden Sie die Zugriffs-Controller ResourceQuota und LimitRange. Diese schützen Ihren Cluster, indem Sie Anfragen, die zu einem übermäßigen Ressourcenverbrauch oder einem Denial-of-Service-Angriff führen können, mutieren.
Der Zugriffs-Controller ResourceQuota ermöglicht Administratoren, die Rechnerressourcen, die Speicherressourcen und die Quantität jedes Objekts in einem Namensraum zu beschränken. Der Zugriffs-Controller LimitRange beschränkt die Anzahl der von Containern verwendeten Ressourcen.
Die gleichzeitige Verwendung dieser beiden Zugriffs-Controller schützt Ihren Cluster vor Angriffen, durch die Ihre Ressourcen nicht mehr verfügbar wären.
Um zu zeigen, wie ResourceQuota funktioniert, implementieren Sie nun einige Beschränkungen im default-Namensraum.
Beginnen Sie mit dem Erstellen einer neuen ResourceQuota-Objektdatei:
Fügen Sie die folgende Objektdefinition hinzu, um Beschränkungen für den Ressourcenverbrauch im default-Namensraum festzulegen.
Sie können die Werte entsprechend der physischen Ressourcen Ihrer Knoten nach Bedarf anpassen:
Diese Definition verwendet das Schlüsselwort hard, um harte Beschränkungen wie z. B. die maximale Anzahl von pods, configmaps, PersistentVolumeClaims, ReplicationControllers, secrets, services und loadbalancers festzulegen.
Sie setzt auch Beschränkungen für Rechnerressourcen wie:
requests.cpu, das den maximalen CPU-Wert von Anfragen in milliCPU oder einem Tausendstel eines CPU-Kerns festlegt.
requests.memory, das den maximalen Speicherwert von Anfragen in Bytes festlegt.
limits.cpu, das den maximalen CPU-Wert von Limits in milliCPUs festlegt.
requests.memory, das den maximalen Speicherwert von Limits in Bytes festlegt.
Erstellen Sie nun mit dem folgenden Befehl das Objekt im Namensraum:
Beachten Sie, dass Sie das Flag -f verwenden, um Kubernetes den Speicherort der ResourceQuota-Datei anzuzeigen, sowie das Flag --namespace, um anzugeben, welcher Namensraum aktualisiert wird.
Nach dem Erstellen des Objekts ist Ihr ResourceQuota aktiv.
Sie können die default-Namensraumquoten mit describe quota prüfen:
Das Output sieht dem Folgenden ähnlich; mit den harten Limits, die Sie in der Datei < ^ > resource-quota-default < ^ > .yaml festgelegt haben:
ResourceQuotas werden in absoluten Einheiten ausgedrückt. Daher erhöht das Hinzufügen zusätzlicher Knoten nicht automatisch die hier definierten Werte.
Wenn weitere Knoten hinzugefügt werden, müssen Sie die Werte hier manuell bearbeiten, um die Ressourcen zu proportionieren.
ResourceQuotas können so oft wie nötig modifiziert werden. Sie können jedoch nicht entfernt werden, es sei denn, der gesamte Namensraum wird entfernt.
Wenn Sie einen bestimmten ResourceQuota-Controller ändern möchten, aktualisieren Sie die entsprechende .yaml-Datei und wenden Sie die Änderungen mit dem folgenden Befehl an:
Weitere Informationen über den Zugriffs-Controller ResourceQuota finden Sie in der offiziellen Dokumentation.
Nachdem Ihr ResourceQuota nun eingerichtet ist, fahren Sie mit der Konfiguration des Zugriffs-Controllers LimitRange fort.
Ähnlich wie der Controller ResourceQuota Limits für Namensräume umsetzt, setzt der Controller LimitRange die durch Validierung und Mutation von Containern deklarierten Beschränkungen durch.
Wie zuvor beginnen Sie mit dem Erstellen der Objektdatei:
Nun können Sie mit dem LimitRange-Objekt den Ressourcenverbrauch nach Bedarf beschränken.
Fügen Sie folgenden Inhalt als Beispiel eines typischen Anwendungsfalls hinzu:
Die in < ^ > limit-ranges-default < ^ > .yaml verwendeten Beispielwerte beschränken den Containerspeicher auf ein Maximum von 1Gi und die CPU-Nutzung auf ein Maximum von 400 m, was einer Kubernetes-Metrik von 400 milliCPU entspricht und bedeutet, dass der Container auf nahezu die Hälfte seines Kerns beschränkt ist.
Als Nächstes stellen Sie das Objekt mit dem folgenden Befehl für den API-Server bereit:
Nun können Sie die neuen Limits mit dem folgenden Befehl überprüfen:
Das Output sieht ungefähr wie folgt aus:
Um den LimitRanger arbeiten zu sehen, stellen Sie einen Standard-nginx-Container mit dem folgenden Befehl bereit:
Überprüfen Sie, wie der Zugriffs-Controller den Container mutiert hat, indem Sie folgenden Befehl ausführen:
Dadurch erhalten Sie viele Zeilen an Output.
Im Abschnitt der Container-Spezifizierung finden Sie die im LimitRange angegebenen Ressourcenlimits:
Dies wäre genauso, wenn Sie die Angaben für resources und requests manuell in der Container-Spezifikation festlegen würden.
In diesem Schritt haben Sie die Zugriffs-Controller ResourceQuota and LimitRange verwendet, um die Ressourcen Ihres Clusters vor schädlichen Angriffen zu schützen.
Weitere Informationen über den Zugriffs-Controller LimitRange finden Sie in der offiziellen Dokumentation.
In diesem Tutorial haben Sie eine grundlegende Kubernetes-Sicherheitsvorlage konfiguriert.
Dadurch wurden die Benutzerauthentifizierung und -autorisierung, die Anwendungsberechtigungen und der Schutz der Cluster-Ressourcen bestimmt.
Wenn Sie alle in diesem Tutorial behandelten Vorschläge kombinieren, haben Sie eine solide Grundlage für den Einsatz eines Kubernetes-Clusters in einer produktiven Umgebung.
Nun können Sie beginnen, individuelle Aspekte Ihres Clusters je nach Ihrer Vorstellung zu härten.
Wenn Sie mehr über Kubernetes erfahren möchten, besuchen Sie unsere Kubernetes-Ressourcenseite oder folgen Sie dem selbstgeführten Kurs Kubernetes für umfassende Entwickler.
Anzeigen von Daten aus der DigitalOcean-API mit Django
3190
Der Autor hat die Mozilla Foundation dazu ausgewählt, im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
Da der Bedarf nach einer Full-Stack-Entwicklung weiter wächst, sorgen Web-Frameworks für Entwicklungsabläufe, die einfacher und effizienter sind; Django ist eines dieser Frameworks.
Django kommt bei bekannten Websites wie Mozilla, Pinterest und Instagram zum Einsatz.
Im Gegensatz zu Flask, das ein neutrales Mikro-Framework ist, enthält das Django PyPI-Paket alles, was Sie zur Full-Stack-Entwicklung benötigen; eine Datenbank oder Kontrolltafel für die Entwicklung muss nicht eingerichtet werden.
Ein verbreitetes Anwendungsbeispiel für Django besteht in der Anzeige von Informationen aus APIs (wie Instagram-Posts oder GitHub-Repositorys) in Ihren eigenen Websites und Web-Apps.
Zwar ist dies auch mit anderen Frameworks möglich, doch bedeutet das Motto "Batterien inbegriffen" von Django, dass der Aufwand geringer ist und weniger Pakete benötigt werden, um dasselbe Ergebnis zu erzielen.
In diesem Tutorial erstellen Sie ein Django-Projekt, das die Droplet-Informationen Ihres DigitalOcean-Kontos mit der DigitalOcean v2 API anzeigt.
Sie werden insbesondere eine Webseite erstellen, die eine Tabelle mit Droplets anzeigt, in der ihre jeweiligen IP-Adressen, IDs, Hosting-Regionen und Ressourcen aufgeführt werden.
Ihre Website wird BulmaCSS verwenden, um die Seite so zu gestalten, dass Sie sich auf die Entwicklung konzentrieren können und die Seite am Ende trotzdem gut aussieht.
Nach Abschluss dieses Tutorials werden Sie über ein Django-Projekt verfügen, das eine Webseite erstellen kann, die wie folgt aussieht:
Vorlage mit Tabelle von Droplet-Daten
Ein DigitalOcean-Konto mit mindestens einem Droplet und einem persönlichen Zugriffstoken.
Stellen Sie sicher, dass Sie das Token an einem sicheren Ort aufbewahren; Sie werden es später in dem Tutorial noch benötigen.
Vertrautheit mit dem Richten von Anfragen an APIs.
Ein umfassendes Tutorial zur Arbeit mit APIs finden Sie unter Verwenden von Web-APIs in Python3.
Eine lokale virtuelle Umgebung für Python zur Aufrechterhaltung von Abhängigkeiten.
In diesem Tutorial verwenden wir den Namen < ^ > do _ django _ api < ^ > für unser Projektverzeichnis und < ^ > env < ^ > für unsere virtuelle Umgebung.
Vertrautheit mit der Vorlagenlogik von Django beim Rendern von Seiten mit API-Daten.
Vertrautheit mit der Anzeigelogik von Django bei der Handhabung von Daten, die von der API empfangen werden, und Übergabe an eine Vorlage für das Rendering.
Schritt 1 - Einrichten eines einfachen Django-Projekts
Installieren Sie Django aus der virtuellen Umgebung < ^ > env < ^ > heraus:
Jetzt können Sie ein Django-Projekt starten und einige anfängliche Einrichtungsbefehle ausführen.
Verwenden Sie django-admin startproject < ^ > < name > < ^ >, um im Projektordner, der den Namen Ihres Django-Projekts trägt, ein Unterverzeichnis zu erstellen; wechseln Sie anschließend in dieses Verzeichnis.
Nach der Erstellung finden Sie in diesem Unterverzeichnis manage.py; das ist die übliche Methode, um mit Django zu interagieren und Ihr Projekt auszuführen.
Verwenden Sie migrate, um die Entwicklungsdatenbank von Django zu aktualisieren:
Sie werden eine Ausgabe sehen, die folgendermaßen aussieht, während die Datenbank aktualisiert wird:
Verwenden Sie als Nächstes den Befehl runserver, um das Projekt auszuführen, damit Sie es testen können:
Die Ausgabe sieht wie folgt aus, während der Server gestartet wird:
Sie verfügen nun über ein einfaches Django-Projekt und einen Entwicklungsserver, der ausgeführt wird.
Um Ihren ausgeführten Entwicklungsserver anzuzeigen, rufen Sie in einem Browser 127.0.0.1: 8000 auf.
Damit wird die Startseite von Django angezeigt:
Allgemeine Django-Startseite
Als Nächstes erstellen Sie eine Django-App und konfigurieren Ihr Projekt, um eine Ansicht von dieser App auszuführen, damit Sie etwas Aufregenderes als die Standardseite sehen.
Schritt 2 - Einrichten einer einfachen Django-App
In diesem Schritt schaffen Sie das Fundament für die App, die Ihre Droplet-Ergebnisse enthalten wird.
Sie werden später wieder zu dieser App zurückkehren, sobald Sie den API-Aufruf eingerichtet haben, um die App mit Daten zu füllen.
Stellen Sie sicher, dass Sie sich im Verzeichnis do _ django _ project befinden, und erstellen Sie mit dem folgenden Befehl eine Django-App:
Jetzt müssen Sie die neue App INSTALLED _ APPS in der Datei settings.py hinzufügen, damit Django sie erkennen kann. settings.py ist eine Django-Konfigurationsdatei, die sich im Django-Projekt in einem anderen Unterverzeichnis befindet und den gleichen Namen trägt wie der Projektordner (do _ django _ project).
Django hat beide Ordner für Sie erstellt.
Wechseln Sie zum Verzeichnis do _ django _ project:
Bearbeiten Sie settings.py in einem Editor Ihrer Wahl:
Fügen Sie Ihre neue App dem Abschnitt INSTALLED _ APPS der Datei hinzu:
Anzeigefunktion GetDroplets
Als Nächstes erstellen Sie eine Funktion namens GetDroplets in der Datei views.py der App display _ droplets.
Diese Funktion rendert die Vorlage, die Sie zur Anzeige von Droplet-Daten verwenden werden, als context von der API. context ist ein Wörterbuch, das es erlaubt, Daten aus Python-Code zu übernehmen und an eine HTML-Vorlage zu senden, damit sie sich in einer Webseite anzeigen lassen.
Wechseln Sie zum Verzeichnis display _ droplets:
Öffnen Sie views.py, um die Datei zu bearbeiten:
Später werden Sie diese Funktion ausfüllen und die Datei droplets.html erstellen. Zuerst konfigurieren wir aber urls.py, um diese Funktion anzurufen, wenn Sie das Stammverzeichnis des Entwicklungsservers (127.0.0.1: 8000) aufrufen.
Wechseln Sie wieder zum Verzeichnis do _ django _ project:
Öffnen Sie urls.py, um die Datei zu bearbeiten:
Fügen Sie eine import-Anweisung für GetDroplets und dann einen zusätzlichen zu urlpatterns hinzu, der auf die neue Ansicht verweist.
Wenn Sie Ihre eigenen Pfade einrichten möchten, ist der erste Parameter die URL (wie z. B. example.com / * * admin * *), der zweite Parameter die Funktion, die zum Erstellen der Webseite aufgerufen wird, und der dritte nur ein Name für den Pfad.
Droplets-Vorlage
Als Nächstes arbeiten Sie mit Vorlagen.
Vorlagen sind HTML-Dateien, die Django zur Erstellung von Webseiten verwendet.
In diesem Fall verwenden Sie eine Vorlage, um eine HTML-Seite zu erstellen, die die API-Daten anzeigt.
Wechseln Sie wieder zum Verzeichnis display _ droplets:
Erstellen Sie in diesem Verzeichnis einen Ordner template und wechseln Sie in dieses Verzeichnis:
Erstellen Sie droplets.html und öffnen Sie die Datei, um sie zu bearbeiten:
Um kein CSS für dieses Projekt schreiben zu müssen, verwenden wir Bulma CSS, da es sich dabei um ein kostenloses und schlankes CSS-Framework für die Erstellung von übersichtlichen Webseiten handelt. Sie müssen der HTML lediglich einige Klassenattribute hinzufügen.
Jetzt erstellen wir eine Vorlage mit einer einfachen Navigationsleiste.
Fügen Sie der Datei droplets.html den folgenden Code hinzu:
Dieser Code sorgt dafür, dass Bulma in den HTML-Baustein importiert und eine nav-Leiste zur Anzeige von "Droplets" erstellt wird.
Aktualisieren Sie Ihren Browser-Tab, um die Änderungen, die Sie an der Vorlage vorgenommen haben, anzuzeigen.
Vorlage mit einfacher Kopfzeile
Bisher haben Sie noch keine APIs verwendet; Sie haben vielmehr ein Fundament für das Projekt geschaffen.
Als Nächstes nutzen Sie diese Seite sinnvoll, indem Sie einen API-Aufruf einrichten und die Droplet-Daten bereitstellen.
Schritt 3 - Einrichten des API-Aufrufs
In diesem Schritt richten Sie einen API-Aufruf ein und senden die Droplet-Daten als Kontext an die Vorlage, um in einer Tabelle angezeigt zu werden.
Abrufen von Droplet-Daten
Navigieren Sie zurück zum App-Verzeichnis display _ droplets:
Installieren Sie die Bibliothek requests, damit Sie mit der API kommunizieren können:
Mithilfe der Bibliothek requests kann Ihr Code Daten von APIs anfordern und Kopfzeilen hinzufügen (zusätzliche Daten, die mit unserer Anfrage gesendet werden).
Als Nächstes erstellen Sie eine Datei services.py, über die Sie den API-Aufruf vornehmen.
Diese Funktion verwendet requests, um mit https: / / api.digitalocean.com / v2 / droplets zu kommunizieren und jedes Droplet in der JSON-Datei anzufügen, das an eine Liste zurückgegeben wird.
Öffnen Sie services.py, um die Datei zu bearbeiten:
Innerhalb der Funktion get _ droplets passieren zwei Dinge: Es wird eine Anfrage vorgenommen und Daten werden analysiert. url enthält die URL, die Droplet-Daten von der DigitalOcean-API anfordert. r speichert die angeforderten Daten.
requests übernimmt in diesem Fall zwei Parameter: url und headers.
Wenn Sie Daten von einer anderen API nutzen möchten, ersetzen Sie den Wert url durch die entsprechende URL. headers sendet DigitalOcean Ihr Zugriffstoken, damit das Unternehmen weiß, dass Sie die Anfrage vornehmen dürfen und für welches Konto die Anfrage vorgenommen wird.
droplets enthält die Informationen aus der Variable r, wurde nun jedoch von JSON, dem Format, in dem die API Informationen sendet, in ein Wörterbuch konvertiert, das bequem in einer for-Schleife zu verwenden ist.
Die nächsten drei Zeilen erstellen ein Array namens droplet _ list [].
Dann iteriert eine for-Schleife über die Informationen in droplets und fügt jedes Element der Liste hinzu.
Alle von der API übernommenen und in droplets gespeicherten Informationen finden Sie in der Entwicklerdokumentation von DigitalOcean.
< $> note Anmerkung: Vergessen Sie nicht, access _ token durch Ihr Zugriffstoken zu ersetzen.
Bewahren Sie das Token außerdem sicher auf und veröffentlichen Sie es niemals online.
Schutz Ihres Zugriffstokens
Sie sollten Ihr Zugriffstoken stets verstecken. Wenn aber auch andere Personen das Projekt ausführen dürfen, sollten Sie ihnen eine einfache Möglichkeit bieten, um eigene Zugriffstoken hinzuzufügen, ohne dass Python-Code bearbeitet werden muss.
DotENV ist die Lösung, wenn Variablen in einer .env-Datei aufbewahrt werden, die bequem bearbeitet werden kann.
Wechseln Sie wieder zum Verzeichnis do _ django _ project:
Um mit Umgebungsvariablen zu arbeiten, installieren Sie python-dotenv:
Nach der Installation müssen Sie Django für den Umgang mit Umgebungsvariablen konfigurieren, damit Sie in Code auf sie verweisen können.
Dazu müssen Sie einige Codezeilen zu manage.py und wsgi.py hinzufügen.
Öffnen Sie manage.py, um die Datei zu bearbeiten:
Fügen Sie folgenden Code hinzu:
Das Hinzufügen von dem Code in manage.py bedeutet, dass es bei der Ausgabe von Befehlen an Django in der Entwicklung Umgebungsvariablen von Ihrer .env-Datei verarbeiten wird.
Wenn Sie in Ihren Produktionsprojekten Umgebungsvariablen bearbeiten müssen, können Sie dies mit der Datei wsgi.py tun.
Wechseln Sie zum Verzeichnis do _ django _ project:
Öffnen Sie anschließend wsgi.py, um die Datei zu bearbeiten:
Fügen Sie wsgi.py folgenden Code hinzu:
Dieser Codeausschnitt weist einen zusätzlichen os.path.dirname () auf, weil wsgi.py zwei Verzeichnisse zurückgehen muss, um die Datei .env zu finden.
Dieser Ausschnitt ist nicht identisch mit dem, der für manage.py verwendet wird.
Nun können Sie anstelle Ihres Zugriffstokens eine Umgebungsvariable in services.py verwenden.
Ersetzen Sie nun Ihr Zugriffstoken durch eine Umgebungsvariable:
Der nächste Schritt besteht aus der Erstellung einer .env-Datei.
Erstellen Sie eine .env-Datei und öffnen Sie die Datei, um sie zu bearbeiten:
Fügen Sie in .env Ihr Token als Variable DO _ ACCESS _ TOKEN hinzu:
< $> note Anmerkung: Fügen Sie .env zu Ihrer .gitignore-Datei hinzu, damit sie nie in Ihren Commits enthalten ist.
Die API-Verbindung ist nun eingerichtet und konfiguriert; zudem haben Sie Ihr Zugriffstoken geschützt.
Es ist an der Zeit, die Informationen, die Sie für den Benutzer abgerufen haben, bereitzustellen.
Schritt 4 - Umgang mit Droplet-Daten in Ansichten und Vorlagen
Nachdem Sie nun API-Aufrufe vornehmen können, müssen Sie die Droplet-Daten zum Rendern an die Vorlage senden.
Kehren wir zum Stub der Funktion GetDroplets zurück, die Sie zuvor in views.py erstellt haben.
In der Funktion senden Sie droplet _ list als Kontext an die Vorlage droplets.html.
Fügen Sie views.py folgenden Code hinzu:
Die an die Vorlage droplets.html gesendeten Informationen werden mit dem Wörterbuch context verwaltet.
Aus diesem Grund dient droplets als Schlüssel, während das von get _ droplets () zurückgegebene Array als Wert fungiert.
Bereitstellen der Daten in der Vorlage
In der Vorlage droplets.html erstellen Sie eine Tabelle und befüllen sie mit den Droplet-Daten.
Wechseln Sie zum Verzeichnis templates:
Öffnen Sie droplets.html, um die Datei zu bearbeiten:
Fügen Sie nach dem Element nav in droplets.html folgenden Code hinzu:
{% for droplet in droplets%}... {% endfor%} ist eine Schleife, die durch das Array von Droplets, die aus views.py abgerufen wurden, iteriert.
Jedes Droplet wird in eine Tabellenzeile eingefügt.
Die verschiedenen {{droplet. < attribute >} -Zeilen rufen dieses Attribut für jedes Droplet in der Schleife ab und fügen es in eine Tabellenzelle ein.
Aktualisieren Sie Ihren Browser; es wird eine Liste von Droplets angezeigt.
Nun können Sie die DigitalOcean-API in Ihren Django-Projekten verwenden.
Sie haben die von der API abgerufenen Daten übernommen und in die zuvor erstellte Vorlage eingespeist, um die Informationen auf lesbare und flexible Weise anzuzeigen.
In diesem Artikel haben Sie ein Django-Projekt erstellt, das Droplet-Daten aus der DigitalOcean-API im Bulma CSS-Stil anzeigen kann.
In diesem Tutorial haben Sie drei wichtige Fähigkeiten erlernt:
Verwalten von API-Anfragen in Python mit den Modulen requests und json.
Anzeigen von API-Daten in einem Django-Projekt unter Verwendung der Logiken view und template.
Sicheres Verwalten Ihrer API-Token in Django mit dotenv.
Nachdem Sie eine Einführung in die Handhabung von APIs in Django erhalten haben, können Sie nun ein eigenes Projekt erstellen: entweder mit einer anderen Funktion aus der DigitalOcean-API oder einer ganz anderen API.
Zudem können Sie andere Django-Tutorials bzw. ein ähnliches Tutorial mit dem React-Framework konsultieren.
Verwalten und Verwenden von MySQL-Datenbank-Triggern unter Ubuntu 18.04
3270
Der Autor hat die Apache Software Foundation dazu ausgewählt, im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
In MySQL ist ein trigger ein benutzerdefinierter SQL-Befehl, der während eines INSERT-, DELETE- oder UPDATE-Vorgangs automatisch aufgerufen wird.
Der Triggercode ist mit einer Tabelle verbunden und wird nach dem Verwerfen einer Tabelle zerstört.
Sie können eine Triggeraktionszeit festlegen und bestimmen, ob die Triggeraktion vor oder nach dem definierten Datenbankereignis ausgelöst werden soll.
Trigger bieten verschiedene Vorteile.
Sie können sie beispielsweise verwenden, um bei einer INSERT-Anweisung den Wert einer abgeleiteten Spalte zu generieren.
Ein weiterer Anwendungsfall ist die Durchsetzung der referenziellen Integrität, wo Sie einen Trigger verwenden können, um einen Eintrag in mehreren verknüpften Tabellen zu speichern.
Weitere Vorteile sind die Protokollierung von Benutzeraktionen in Prüftabellen sowie das Live-Kopieren von Daten in verschiedenen Datenbankschemata für Redundanzzwecke, um einzelne Ausfallpunkte (Single Points of Failure) zu verhindern.
Sie können Trigger außerdem verwenden, um Validierungsregeln auf der Datenbankebene zu pflegen.
So können Sie die Datenquelle in mehreren Anwendungen freigeben, ohne die Geschäftslogik zu stören.
Dadurch werden Roundtrips zum Datenbankserver stark reduziert, was die Antwortzeit Ihrer Anwendungen verbessert.
Da der Datenbankserver Trigger ausführt, können Sie auch optimierte Serverressourcen wie RAM und CPU nutzen.
In diesem Tutorial erstellen, verwenden und löschen Sie verschiedene Arten von Triggern in Ihrer MySQL-Datenbank.
Bevor Sie beginnen, stellen Sie sicher, dass Sie über Folgendes verfügen:
Eine MySQL-Datenbank, die ausgeführt wird durch folgenden Schritt: Installieren von MySQL unter Ubuntu 18.04
Erstellen Sie für Ihre MySQL-Datenbank Kontoanmeldeinformationen für einen root user.
Schritt 1 - Erstellen einer Beispieldatenbank
In diesem Schritt erstellen Sie eine exemplarische Kundendatenbank mit mehreren Tabellen, um zu sehen, wie MySQL-Trigger funktionieren.
Wenn Sie mehr über MySQL-Abfragen wissen möchten, lesen Sie unsere Einführung zu Abfragen in MySQL.
Melden Sie sich zunächst als Root-Benutzer bei Ihrem MySQL-Server an:
Geben Sie auf Aufforderung Ihr MySQL-Root-Passwort ein und drücken Sie zum Fortfahren die Eingabetaste.
Wenn Sie die Eingabeaufforderung mysql > sehen, führen Sie folgenden Befehl aus, um eine < ^ > test _ db < ^ > -Datenbank zu erstellen:
Wechseln Sie als Nächstes zu test _ db mit:
Sie erstellen zunächst eine Tabelle namens customers.
Diese Tabelle wird die Datensätze von Kunden enthalten, einschließlich customer _ id, customer _ name und level.
Es wird zwei Kundenlevel geben: BASIC und VIP.
Fügen Sie nun der Tabelle customers ein paar Einträge hinzu.
Führen Sie dazu nacheinander folgende Befehle aus:
Sie erhalten nach dem Ausführen der einzelnen INSERT-Befehle folgende Ausgabe:
Um zu überprüfen, ob die Beispieleinträge erfolgreich eingefügt wurden, führen Sie den Befehl SELECT aus:
Außerdem erstellen Sie eine weitere Tabelle, um entsprechende Informationen über das Konto customers zu speichern.
Die Tabelle wird über die Felder customer _ id und status _ notes verfügen.
Als Nächstes erstellen Sie eine Tabelle namens sales (Verkäufe).
In dieser Tabelle werden über die Spalte customer _ id Verkaufsdaten für die verschiedenen Kunden gespeichert:
In den nächsten Schritten fügen Sie den sales-Daten beim Testen der Trigger Beispieldaten hinzu.
Als Nächstes erstellen Sie eine Tabelle namens audit _ log, um an der Tabelle sales vorgenommene Aktualisierungen zu protokollieren, wenn Sie den Trigger AFTER UPDATE in Schritt 5 implementieren:
Nachdem Sie über die Datenbank < ^ > test _ db < ^ > und die vier Tabellen verfügen, fahren Sie nun mit den verschiedenen MySQL-Triggern in Ihrer Datenbank fort.
Schritt 2 - Erstellen eines Before Insert-Triggers
In diesem Schritt prüfen Sie die Syntax eines MySQL-Triggers, bevor Sie diese Logik anwenden, um einen BEFORE INSERT-Trigger zu erstellen, der das Feld sales _ amount beim Einfügen von Daten in die Tabelle sales validiert.
Die allgemeine Syntax zum Erstellen eines MySQL-Triggers wird im folgenden Beispiel gezeigt:
Die Struktur des Triggers umfasst:
DELIMITER / /: Das Standardtrennzeichen von MySQL ist; - es ist notwendig, das Zeichen zu ändern, wenn MySQL folgende Zeilen als einen Befehl behandeln soll, bis er auf Ihr benutzerdefiniertes Trennzeichen trifft.
In diesem Beispiel wird das Trennzeichen in / / geändert; das Trennzeichen wird am Ende neu definiert.
[TRIGGER _ NAME]: Ein Trigger muss einen Namen haben; hier schließen Sie den Wert ein.
[TRIGGER TIME]: Ein Trigger kann zu verschiedenen Zeitpunkten aufgerufen werden.
Mit MySQL können Sie definieren, ob der Trigger vor oder nach der Datenbankoperation gestartet werden soll.
[TRIGGER EVENT]: Trigger werden nur von INSERT-, UPDATE- und DELETE-Operationen aufgerufen.
Sie können hier einen beliebigen Wert verwenden, je nach dem, was Sie erreichen möchten.
[TABLE]: Jeder Trigger, den Sie in der MySQL-Datenbank erstellen, muss mit einer Tabelle verknüpft werden.
FOR EACH ROW: Diese Anweisung informiert MySQL, dass der Triggercode für jede Zeile ausgeführt werden soll, auf die sich der Trigger auswirkt.
[TRIGGER BODY]: Der Code, der beim Aufruf des Triggers ausgeführt wird, wird als Trigger Body bezeichnet.
Dabei kann es sich um eine einzelne SQL-Anweisung oder mehrere Befehle handeln.
Hinweis: Wenn Sie mehrere SQL-Anweisungen auf den Trigger Body ausführen, müssen Sie diese zwischen einem BEGIN...
END-Block umschließen.
< $> note Anmerkung: Beim Erstellen des Trigger Body können Sie die Schlüsselwörter OLD und NEW verwenden, um die bei einem INSERT-, UPDATE- und DELETE-Vorgang eingegebenen alten und neuen Spaltenwerte aufzurufen.
In einem DELETE-Trigger kann nur das Schlüsselwort OLD verwendet werden (was Sie in Schritt 4 nutzen werden).
Nun erstellen Sie Ihren ersten BEFORE INSERT-Trigger.
Dieser Trigger wird mit der Tabelle sales verbunden und aufgerufen, bevor ein Eintrag eingefügt wird, um die Verkaufssumme (sales _ amount) zu validieren.
Aufgabe des Triggers ist es zu prüfen, ob die in die Umsatztabelle eingefügte Verkaufssumme (sales _ amount) höher als 10000 ist und einen Fehler ergeben soll, wenn zu true ausgewertet wird.
Stellen Sie sicher, dass Sie beim MySQL-Server angemeldet sind.
Geben Sie dann nacheinander folgende MySQL-Befehle ein:
Sie verwenden die IF...
THEN...
END IF-Anweisung um zu auszuwerten, ob die bei der INSERT-Anweisung bereitgestellte Summe innerhalb Ihres Bereichs liegt.
Der Trigger kann den neu bereitgestellten Wert von sales _ amount durch Verwendung des Schlüsselworts NEW extrahieren.
Zum Ausgeben einer generischen Fehlermeldung verwenden Sie folgende Zeilen, um den Benutzer über den Fehler zu informieren:
Als Nächstes fügen Sie einen Eintrag mit der Verkaufssumme (sales _ amount) 11000 zur Tabelle sales hinzu, um zu überprüfen, ob der Trigger die Operation stoppen wird:
Dieser Fehler zeigt, dass der Triggercode wie erwartet funktioniert.
Probieren Sie es nun mit einem neuen Eintrag, der den Wert 7500 hat, um zu überprüfen, ob der Befehl erfolgreich ausgeführt wird:
Da der Wert im empfohlenen Bereich liegt, sehen Sie die folgende Ausgabe:
Um zu überprüfen, ob die Daten eingefügt wurden, führen Sie folgenden Befehl aus:
Die Ausgabe zeigt, dass sich die Daten in der Tabelle befinden:
In diesem Schritt haben Sie Trigger getestet, um Daten zu validieren, bevor sie in eine Datenbank eingefügt werden.
Als Nächstes verwenden Sie den AFTER INSERT-Trigger, um verwandte Daten in verschiedene Tabellen zu speichern.
Schritt 3 - Erstellen eines After Insert-Triggers
AFTER INSERT-Trigger werden ausgeführt, wenn Einträge erfolgreich in eine Tabelle eingefügt werden.
Diese Funktion kann dazu dienen, andere geschäftliche Logiken automatisch auszuführen.
In einer Bankanwendung kann beispielsweise ein AFTER INSERT-Trigger ein Kreditkonto schließen, sobald ein Kunde seinen Kredit abbezahlt hat.
Der Trigger kann alle in eine Transaktionstabelle eingefügten Zahlungen überwachen und den Kredit automatisch schließen, sobald das Kreditsaldo null beträgt.
In diesem Schritt arbeiten Sie mit der Tabelle customer _ status und verwenden einen AFTER INSERT-Trigger, um verwandte Kundeneinträge einzugeben.
Um den AFTER INSERT-Trigger zu erstellen, geben Sie folgende Befehle ein:
Hier weisen Sie MySQL an, einen weiteren Eintrag in der Tabelle customer _ status zu speichern, sobald ein neuer Kundendateneintrag in die Tabelle customers eingefügt wird.
Fügen Sie nun einen neuen Eintrag in die Tabelle customers ein, um zu bestätigen, dass Ihr Triggercode aufgerufen wird:
Nachdem der Eintrag erfolgreich eingefügt wurde, überprüfen Sie, ob in die Tabelle customer _ status ein neuer Statuseintrag eingefügt wurde:
Die Ausgabe zeigt, dass der Trigger erfolgreich ausgeführt wurde.
Der AFTER INSERT-Trigger hilft dabei, den Lebenszyklus eines Kunden zu überwachen.
In einer Produktionsumgebung können Konten von Kunden verschiedene Stufen durchlaufen, wie Eröffnen, Suspendieren und Schließen.
In den folgenden Schritten arbeiten Sie mit UPDATE-Triggern.
Schritt 4 - Erstellen eines Before Update-Triggers
Ein BEFORE UPDATE-Trigger ähnelt dem BEFORE INSERT-Trigger - er wird jedoch zu einem anderen Zeitpunkt aufgerufen.
Sie können den BEFORE UPDATE-Trigger verwenden, um eine Geschäftslogik zu überprüfen, bevor ein Eintrag aktualisiert wird.
Verwenden Sie zum Testen die Tabelle customers, in der Sie bereits Daten eingefügt haben.
Sie verfügen in der Datenbank über zwei verschiedene Level für Ihre Kunden.
In diesem Beispiel kann ein Kundenkonto nach dem Hochstufen auf das Level VIP nicht mehr auf das Level BASIC herabgestuft werden.
Um eine solche Regel durchzusetzen, erstellen Sie einen BEFORE UPDATE-Trigger, der vor der UPDATE-Anweisung ausgeführt wird (wie nachfolgend gezeigt).
Wenn ein Datenbankbenutzer einen Kunden auf das Level BASIC herunterzustufen will (vom Level VIP aus), wird eine benutzerdefinierte Ausnahme ausgelöst.
Geben Sie nacheinander folgende SQL-Befehle ein, um den BEFORE UPDATE-Trigger zu erstellen:
Verwenden Sie das Schlüsselwort OLD, um das Level zu erfassen, das der Benutzer beim Ausführen des Befehls UPDATE bereitstellt.
Auch hier verwenden Sie die IF...
END IF-Anweisung, um dem Benutzer eine generische Fehlermeldung anzuzeigen.
Führen Sie als Nächstes den folgenden SQL-Befehl aus, der versucht, ein mit der customer _ id 3 verbundenes Kundenkonto herabzustufen:
Sie sehen die folgende Ausgabe, die den SET MESSAGE _ TEXT bereitstellt:
Wenn Sie den gleichen Befehl für einen BASIC-Kunden ausführen und versuchen, das Konto auf das Level VIP hochzustufen, wird der Befehl erfolgreich ausgeführt:
Sie haben den BEFORE UPDATE-Trigger verwendet, um eine Geschäftsregel durchzusetzen.
Jetzt fahren Sie mit der Verwendung eines AFTER UPDATE-Triggers zur Überwachungsprotokollierung fort.
Schritt 3 - Erstellen eines After Update-Triggers
Ein AFTER UPDATE-Trigger wird aufgerufen, sobald ein Datenbankeintrag erfolgreich aktualisiert wurde.
Dieses Verhalten sorgt dafür, dass sich der Trigger für die Überwachungsprotokollierung eignet.
In einer Umgebung mit mehreren Benutzern möchte der Administrator aus Auditgründen möglicherweise einen Verlauf der Benutzer anzeigen, die Einträge in einer bestimmten Tabelle aktualisiert haben.
Sie erstellen einen Trigger, der die Aktualisierungsaktivität der Tabelle sales protokolliert.
Die Tabelle audit _ log enthält Informationen über die MySQL-Benutzer, die die Tabelle sales aktualisieren, das Datum der Aktualisierung und die neuen und alten Werte von sales _ amount.
Um den Trigger zu erstellen, führen Sie folgende SQL-Befehle aus:
Sie fügen in die Tabelle audit _ log einen neuen Eintrag ein.
Sie verwenden das Schlüsselwort NEW, um den Wert der sales _ id und die neue Verkaufssumme (sales _ amount) abzurufen.
Außerdem verwenden Sie das Schlüsselwort OLD, um die vorherige Verkaufssumme (sales _ amount) abzurufen, da Sie für Auditzwecke beide Beträge protokollieren möchten.
Der Befehl SELECT USER () ruft den aktuellen Benutzer ab, der die Operation ausführt; die Anweisung NOW () ruft den Wert des aktuellen Datums und der Uhrzeit vom MySQL-Server ab.
Wenn nun ein Benutzer versucht, den Wert eines beliebigen Eintrags in der Tabelle sales zu aktualisieren, wird der Trigger log _ sales _ updates einen neuen Eintrag in die Tabelle audit _ log einfügen.
Nun erstellen wir einen neuen Verkaufseintrag mit einer zufälligen Verkaufs-ID (sales _ id) von 5 und versuchen, ihn zu aktualisieren. Fügen Sie zunächst den Verkaufseintrag ein mit:
Aktualisieren Sie als Nächstes den Eintrag:
Führen Sie nun den folgenden Befehl aus, um zu überprüfen, ob der AFTER UPDATE-Trigger einen neuen Eintrag in der Tabelle audit _ log registrieren konnte:
Der Trigger hat die Aktualisierung protokolliert.
Die Ausgabe zeigt die vorherige Verkaufssumme (sales _ amount) und die neue Summe (new amount), die für den Benutzer registriert sind, der die Einträge aktualisiert hat:
Außerdem verfügen Sie über Datum und Uhrzeit der vorgenommenen Aktualisierung, was für Auditzwecke nützlich ist.
Als Nächstes verwenden Sie den DELETE-Trigger, um auf der Ebene der Datenbank die referenzielle Integrität durchzusetzen.
Schritt 6 - Erstellen eines Before Delete-Triggers
BEFORE DELETE-Trigger werden aufgerufen, bevor in einer Tabelle eine DELETE-Anweisung ausgeführt wird.
Diese Arten von Triggern dienen normalerweise dazu, die referenzielle Integrität verschiedener verbundener Tabellen sicherzustellen.
Beispielsweise bezieht sich jeder Eintrag in der Tabelle sales auf eine customer _ id aus der Tabelle customers.
Wenn ein Datenbankbenutzer einen Eintrag aus der Tabelle customers gelöscht hat, der in der Tabelle sales einen verknüpften Eintrag enthält, hätten Sie keine Möglichkeit zu wissen, welcher Kunde mit diesem Eintrag verknüpft war.
Um das zu verhindern, können Sie zum Durchsetzen Ihrer Logik einen BEFORE DELETE-Trigger erstellen.
Führen Sie nacheinander folgende SQL-Befehle aus:
Versuchen Sie nun, einen Kunden zu löschen, der über einen verknüpften Verkaufseintrag verfügt:
Dadurch erhalten Sie die folgende Ausgabe:
Der BEFORE DELETE-Trigger kann ein versehentliches Löschen verknüpfter Daten in einer Datenbank verhindern.
In bestimmten Situationen möchten Sie jedoch alle Einträge, die mit einem bestimmten Eintrag verbunden sind, aus den verschiedenen verknüpften Tabellen löschen.
In diesem Fall verwenden Sie den AFTER DELETE-Trigger, den Sie im nächsten Schritt testen.
Schritt 7 - Erstellen eines After Delete-Triggers
AFTER DELETE-Trigger werden aktiviert, sobald ein Eintrag erfolgreich gelöscht wurde.
Ein Beispiel dafür, wie Sie einen AFTER DELETE-Trigger verwenden können, ist eine Situation, in der die Rabattstufe, die ein bestimmter Kunde erhält, durch die Anzahl der in einem definierten Zeitraum vorgenommenen Käufe bestimmt wird.
Wenn einige der Einträge des Kunden aus der Tabelle sales gelöscht werden, müsste die Rabattstufe der Kunden herabgestuft werden.
Ein weiteres Einsatzbeispiel für den AFTER DELETE-Trigger ist das Löschen verknüpfter Daten aus einer anderen Tabelle, sobald ein Eintrag aus einer Basistabelle gelöscht wird.
Beispielsweise können Sie einen Trigger einrichten, der den Kundendatensatz löscht, wenn die Verkaufseinträge mit der verknüpften customer _ id aus der Tabelle sales gelöscht werden.
Führen Sie den folgenden Befehl aus, um Ihren Trigger zu erstellen:
Führen Sie als Nächstes Folgendes aus, um alle mit der customer _ id 2 verknüpften Verkaufseinträge zu löschen:
Überprüfen Sie nun, ob es Einträge für den Kunden aus der Tabelle sales gibt:
Sie erhalten eine Empty Set-Ausgabe, da der Kundendatensatz, der mit der customer _ id 2 verknüpft ist, durch den Trigger gelöscht wurde:
Sie haben nun alle verschiedenen Formen von Triggern verwendet, um bestimmte Funktionen auszuführen.
Als Nächstes erfahren Sie, wie Sie einen Trigger aus der Datenbank entfernen können, wenn Sie ihn nicht mehr brauchen.
Schritt 8 - Löschen von Triggern
Ähnlich wie andere Datenbankobjekte auch können Sie Trigger mit dem Befehl DROP löschen.
Die Syntax zum Löschen eines Triggers sieht folgendermaßen aus:
Um beispielsweise den letzten AFTER DELETE-Trigger zu löschen, den Sie erstellt haben, führen Sie den folgenden Befehl aus:
Sie müssen Trigger löschen, wenn Sie deren Struktur neu erstellen möchten.
In diesem Fall können Sie den Trigger entfernen und mit den verschiedenen Triggerbefehlen einen neuen Trigger definieren.
In diesem Tutorial haben Sie die verschiedenen Arten von Triggern einer MySQL-Datenbank erstellt, verwendet und gelöscht.
Unter Verwendung einer kundenbezogenen Datenbank haben Sie Trigger für verschiedene Einsatzfälle wie Datenvalidierung, Anwendung von Geschäftslogik, Überwachungsprotokollierung und Durchsetzung der referenziellen Integrität implementiert.
Weitere Informationen zur Verwendung Ihrer MySQL-Datenbank finden Sie in folgenden Abschnitten:
Optimieren von MySQL mit Abfrage-Cache unter Ubuntu 18.04
Implementieren von Paginierung in MySQL mit PHP unter Ubuntu 18.04
Beheben von Problemen in MySQL
Erstellen eines Node.js-Moduls
3214
In Node.js ist ein Modul eine Sammlung von JavaScript-Funktionen und -Objekten, die von externen Anwendungen verwendet werden können.
Die Beschreibung eines Code-Stücks als Modul bezieht sich weniger darauf, was der Code ist, als vielmehr darauf, was er tut - jede Node.js-Datei oder Sammlung von Dateien kann als Modul betrachtet werden, wenn ihre Funktionen und Daten für externe Programme gemacht werden.
Da Module Funktionseinheiten bereitstellen, die in vielen größeren Programmen wiederverwendet werden können, ermöglichen Sie es Ihnen die Erstellung lose gekoppelter Anwendungen, die mit der Komplexität skalieren, und öffnen Ihnen die Tür, um Ihren Code mit anderen Entwicklern zu teilen.
Die Fähigkeit, Module zum Exportieren nützlicher Funktionen und Daten zu schreiben, ermöglicht es Ihnen, zur breiteren Node.js-Gemeinschaft beizutragen - tatsächlich wurden alle Pakete, die Sie auf npm verwenden, gebündelt und als Module freigegeben.
Dies macht das Erstellen von Modulen zu einer unverzichtbaren Fertigkeit für einen Node.js-Entwickler.
In diesem Tutorial erstellen Sie ein Node.js-Modul, das vorschlägt, welche Farbe Web-Entwickler in ihren Designs verwenden sollten.
Sie werden das Modul entwickeln, indem Sie die Farben als Array speichern und eine Funktion zum zufälligen Abrufen einer Farbe bereitstellen.
Anschließend werden Sie verschiedene Möglichkeiten zum Importieren eines Moduls in eine Node.js-Anwendung ausführen.
Sie müssen Node.js und npm in Ihrer Entwicklungsumgebung installiert haben.
Dieses Tutorial verwendet Version 10.17.0.
Um dies unter MacOS oder Ubuntu 18.04 zu installieren, folgen Sie den Schritten in Installation von Node.js und Erstellen einer lokalen Entwicklungsumgebung auf MacOS oder dem Abschnitt Installation unter Verwendung eines PPA von Installation von Node.js auf Ubuntu 18.04.
Durch die Installation von Node.js haben Sie auch npm installiert; dieses Tutorial verwendet Version 6.11.3.
Sie sollten auch mit der Datei package.json vertraut sein, und Erfahrung mit npm-Befehlen wäre ebenfalls nützlich.
Um diese Erfahrung zu sammeln, folgen Sie der Anleitung Verwenden von Node.js-Modulen mit nmp und package.json, insbesondere dem Schritt 1 - Erstellen einer package.json-Datei.
Hilfreich ist es auch, sich mit der REPL (Read-Evaluate-Print-Loop) von Node.js vertraut zu machen.
Sie werden diese zum Testen Ihres Moduls verwenden.
Wenn Sie weitere Informationen dazu benötigen, lesen Sie unseren Leitfaden Verwenden der Node.js REPL.
Schritt 1 - Erstellen eines Moduls
Dieser Schritt führt Sie durch die Erstellung Ihres ersten Node.js-Moduls.
Ihr Modul wird eine Sammlung von Farben in einem Array und eine Funktion zum zufälligen Abrufen einer Farbe enthalten.
Sie werden die in Node.js integrierte Eigenschaft exports verwenden, um die Funktion und das Array für externe Programme verfügbar zu machen.
Zuerst entscheiden Sie, welche Daten über Farben Sie in Ihrem Modul speichern werden.
Jede Farbe wird ein Objekt sein, das eine Eigenschaft name enthält, die von Menschen leicht identifiziert werden kann, sowie eine Eigenschaft code, die eine Zeichenfolge ist, die einen HTML-Farbcode enthält.
HTML-Farbcodes sind sechsstellige hexadezimale Zahlen, mit denen Sie die Farbe von Elementen auf einer Webseite ändern können.
Der Artikel HTML-Farbcodes und -namen bietet Ihnen weitere Informationen über HTM-Farbcodes.
Dann entscheiden Sie, welche Farben Sie in Ihrem Modul unterstützen möchten.
Ihr Modul wird ein Array namens allColors enthalten, das sechs Farben enthalten wird.
Ihr Modul wird auch eine Funktion namens getRandomColor () enthalten, die zufällig eine Farbe aus Ihrem Array auswählt und zurückgibt.
Erstellen Sie in Ihrem Terminal einen neuen Ordner namens colors und wechseln Sie in diesen:
Initialisieren Sie npm, damit andere Programme dieses Modul später im Tutorial importieren können:
Sie haben das Flag -y verwendet, um die üblichen Eingabeaufforderungen zur Anpassung der package.json zu überspringen.
Wenn dies ein Modul wäre, das Sie in npm veröffentlichen möchten, würden Sie alle diese Eingabeaufforderungen mit relevanten Daten beantworten, wie in Verwenden von Node.js-Modulen mit npm und package.json erklärt.
In diesem Fall wird Ihre Ausgabe sein:
Öffnen Sie nun einen Befehlszeilen-Texteditor wie nano und erstellen eine neue Datei, die als Einstiegspunkt für Ihr Modul dient:
Ihr Modul wird einige Dinge tun.
Zuerst definieren Sie eine Klasse Color.
Ihre Klasse Color wird mit ihrem Namen und HTML-Code instanziert.
Fügen Sie die folgenden Zeilen hinzu, um die Klasse zu erstellen:
Nachdem Sie nun Ihre Datenstruktur für Color haben, fügen Sie einige Instanzen in Ihr Modul ein.
Schreiben Sie das folgende hervorgehobene Array in Ihre Datei:
Geben Sie anschließend eine Funktion ein, die zufällig ein Element aus dem Array allColors, das Sie soeben erstellt haben, auswählt:
Das Schlüsselwort exports verweist auf ein globales Objekt, das in jedem Modul Node.js verfügbar ist.
Alle im Objekt exports eines Moduls gespeicherten Funktionen und Objekte werden beim Import von anderen Node.js-Modulen verfügbar gemacht. Die Funktion getRandomColor () wurde zum Beispiel direkt auf dem Objekt exports erstellt.
Dann haben Sie dem Objekt export eine Eigenschaft allColors hinzugefügt, die auf das zuvor im Skript erstellte lokale konstante Array allColors verweist.
Wenn andere Module dieses Modul importieren, werden sowohl allColors als auch getRandomColor () verfügbar sein.
Bisher haben Sie ein Modul erstellt, das ein Array von Farben und eine Funktion enthält, die eine Farbe nach dem Zufallsprinzip zurückgibt.
Sie haben das Array und die Funktion auch exportiert, damit externe Programme sie verwenden können.
Im nächsten Schritt werden Sie Ihr Modul in anderen Anwendungen verwenden, um die Auswirkungen von export zu demonstrieren.
Schritt 2 - Testen Ihres Moduls mit der REPL
Bevor Sie eine vollständige Anwendung erstellen, nehmen Sie sich einen Moment Zeit, um die Funktionsfähigkeit Ihres Moduls zu bestätigen.
In diesem Schritt werden Sie das REPL zum Laden des Moduls colors verwenden.
Während Sie sich in REPL befinden, werden Sie die Funktion getRandomColor () aufrufen, um zu sehen, ob sie sich wie von Ihnen erwartet verhält.
Starten Sie die Node.js REPL in dem gleichen Ordner wie die Datei index.js:
Wenn die REPL gestartet ist, sehen Sie die Eingabeaufforderung >.
Das bedeutet, dass Sie JavaScript-Code eingeben können, der sofort ausgewertet wird.
Wenn Sie mehr darüber lesen möchten, folgen Sie unserem Leitfaden über das Verwenden der REPL.
Geben Sie zunächst Folgendes ein:
In diesem Befehl lädt require () das Modul colors an seinem Eintrittspunkt.
Wenn Sie die Eingabetaste drücken, erhalten Sie:
Die REPL zeigt uns den Wert von colors, was alle Funktionen und Objekte sind, die aus der Datei index.js importiert wurden.
Bei der Verwendung des Schlüsselworts require gibt Node.js den Inhalt innerhalb des Objekts exports eines Moduls zurück.
Erinnern Sie sich, dass Sie getRandomColor () und allColors zu exports in dem Modul colors hinzugefügt haben.
Aus diesem Grund sehen Sie sie beide, wenn sie in die REPL importiert werden.
Testen Sie an der Eingabeaufforderung die Funktion getRandomColor ():
Ihnen wird eine zufällige Farbe ausgegeben:
Da der Index zufällig ist, kann Ihre Ausgabe variieren.
Nachdem Sie nun bestätigt haben, dass das Modul colors funktioniert, beenden Sie die Node.js REPL:
Dadurch kehren Sie zu Ihrer Terminal-Befehlszeile zurück.
Sie haben gerade mit der REPL bestätigt, dass Ihr Modul wie erwartet funktioniert.
Als Nächstes werden Sie dieselben Konzepte anwenden und Ihr Modul in eine Anwendung laden, wie Sie es in einem echten Projekt tun würden.
Schritt 3 - Speichern Ihres lokalen Moduls als Abhängigkeit
Beim Testen Ihres Moduls in der REPL haben Sie sie mit einem relativen Pfad importiert.
Das bedeutet, dass Sie den Speicherort der Datei index.js in Bezug auf das Arbeitsverzeichnis verwendet haben, um ihren Inhalt zu erhalten.
Das funktioniert zwar, jedoch ist es in der Regel eine bessere Programmiererfahrung, Module anhand ihrer Namen zu importieren, damit der Import bei der Änderung des Kontexts nicht unterbrochen wird.
In diesem Schritt werden Sie das Modul colors mit der Funktion des lokalen Moduls install von npm installieren.
Richten Sie ein neues Node.js-Modul außerhalb des Ordners colors ein.
Gehen Sie zunächst in das vorherige Verzeichnis und erstellen Sie einen neuen Ordner:
Gehen Sie nun in Ihr neues Projekt:
Initialisieren Sie Ihren Ordner, wie bei dem Modul colors, mit npm:
Die folgende package.json wird generiert:
Installieren Sie nun Ihr Modul colors und verwenden Sie das Flag --save, damit es in Ihrer Datei package.json aufgezeichnet wird:
Sie haben soeben Ihr Modul colors im neuen Projekt installiert.
Öffnen Sie die Datei package.json, um die neue lokale Abhängigkeit zu sehen:
Sie werden feststellen, dass die folgenden hervorgehobenen Zeilen hinzugefügt wurden:
Beenden Sie die Datei.
Das Modul colors wurde in Ihr Verzeichnis node _ modules kopiert.
Verifizieren Sie dies mit dem folgenden Befehl:
Verwenden Sie Ihr installiertes lokales Modul in diesem neuen Programm.
Öffnen Sie erneut Ihren Texteditor und erstellen Sie eine weitere JavaScript-Datei:
Ihr Programm importiert zunächst das Modul colors.
Dann wählt es unter Verwendung der von dem Modul bereitgestellten Funktion getRandomColor () eine Farbe nach dem Zufallsprinzip aus.
Schließlich wird es eine Meldung an die Konsole ausgeben, die dem Benutzer mitteilt, welche Farbe zu verwenden ist.
Geben Sie den folgenden Code in index.js ein:
Speichern und schließen Sie diese Datei.
Ihre Anwendung teilt dem Benutzer nun eine zufällige Farboption für eine Webseiten-Komponente mit.
Führen Sie dieses Skript aus mit:
Ihre Ausgabe wird der folgenden ähneln:
Sie haben nun das Modul colors erfolgreich installiert und können es wie jedes andere in Ihrem Projekt verwendete npm-Paket verwalten.
Wenn Sie Ihrem lokalen Modul colors jedoch mehr Farben und Funktionen hinzugefügt haben, müssten Sie npm update in Ihren Anwendungen ausführen, um die neuen Optionen verwenden zu können.
Im nächsten Schritt werden Sie das lokale Modul colors auf eine andere Art und Weise verwenden und automatisch Aktualisierungen erhalten, wenn sich der Modulcode ändert.
Schritt 4 - Verknüpfen eines lokalen Moduls
Wenn sich Ihr lokales Modul in intensiver Entwicklung befindet, kann die kontinuierliche Aktualisierung von Paketen mühsam sein.
Eine Alternative wäre es, die Module zu verknüpfen.
Die Verknüpfung eines Moduls stellt sicher, dass alle Aktualisierungen des Moduls sofort in den Anwendungen, die es verwenden, berücksichtigt werden.
In diesem Schritt werden Sie das Modul colors mit Ihrer Anwendung verknüpfen.
Sie werden das Modul colors auch modifizieren und bestätigen, dass seine letzten Änderungen in der Anwendung funktionieren, ohne dass eine Neuinstallation oder eine Aktualisierung erforderlich ist.
Deinstallieren Sie zunächst Ihr lokales Modul:
npm verknüpft Module durch die Verwendung von symbolischen Verknüpfungen (oder Symlinks), die Verweise sind, die auf Dateien oder Verzeichnisse auf Ihrem Computer verweisen.
Die Verknüpfung eines Moduls erfolgt in zwei Schritten:
Erstellen einer globalen Verknüpfung mit dem Modul. npm erstellt einen Symlink zwischen Ihrem globalen Verzeichnis node _ modules und dem Verzeichnis Ihres Moduls.
Das globale Verzeichnis node _ modules ist der Ort, an dem alle Ihre systemweiten npm-Pakete installiert werden (jedes Paket, das Sie mit dem Flag -g installieren).
Erstellen einer lokalen Verknüpfung. npm erstellt einen Symlink zwischen Ihrem lokalen Projekt, das das Modul verwendet, und der globalen Verknüpfung des Moduls.
Erstellen Sie zunächst die globale Verknüpfung, indem Sie zu dem Ordner colors zurückkehren und den Befehl link verwenden:
Nach dem Abschluss wird Ihre Shell ausgeben:
Sie haben soeben in Ihrem Ordner node _ modules einen Symlink auf Ihr Verzeichnis colors erstellt.
Kehren Sie zu dem Ordner really-large-application zurück und verknüpfen Sie das Paket:
Sie werden eine Ausgabe sehen, die etwa folgendermaßen aussieht:
< $> note Anmerkung: Wenn Sie etwas weniger tippen möchten, können Sie anstelle von link auch ln verwenden.
Zum Beispiel hätte npm ln colors auf genau dieselbe Weise funktioniert.
Wie die Ausgabe zeigt, haben Sie soeben einen Symlink vom lokalen Verzeichnis node _ modules Ihrer really-large-application zu dem Symlink colors in ihrem globalen node _ modules erstellt, der auf das tatsächliche Verzeichnis mit dem Modul colors verweist.
Der Verknüpfungsvorgang ist abgeschlossen.
Führen Sie Ihre Datei aus, um sicherzustellen, dass sie immer noch funktioniert:
Ihre Programmfunktionalität ist intakt.
Testen Sie als Nächstes, dass Aktualisierungen sofort angewendet werden.
Öffnen Sie in Ihrem Texteditor erneut die Datei index.js im Modul colors:
Fügen Sie nun eine Funktion hinzu, die den besten vorhandenen Blauton auswählt.
Sie benötigt keine Argumente und und gibt immer das dritte Element des Arrays allColors zurück.
Fügen Sie diese Zeilen dem Ende der Datei hinzu:
Speichern und schließen Sie die Datei und öffnen Sie dann erneut die Datei index.js im Ordner really-large-application:
Rufen Sie die neu erstellte Funktion getBlue () auf und drucken Sie einen Satz mit den Eigenschaften der Farbe aus.
Fügen Sie diese Anweisungen am Ende der Datei hinzu:
Der Code verwendet nun die neu erstellte Funktion getBlue ().
Führen Sie die Datei wie zuvor aus:
Sie erhalten eine Ausgabe wie:
Ihr Skript konnte die neueste Funktion in Ihrem Modul colors verwenden, ohne npm update ausführen zu müssen
Dadurch wird es einfacher, Änderungen an dieser Anwendung in der Entwicklung vorzunehmen.
Wenn Sie größere und komplexere Anwendungen schreiben, denken Sie darüber nach, wie verwandter Code in Module gruppiert werden kann und wie diese Module eingerichtet werden sollen.
Wenn Ihr Modul nur von einem Programm verwendet werden soll, kann es innerhalb desselben Projekts bleiben und durch einen relativen Pfad referenziert werden.
Wenn Ihr Modul später separat gemeinsam genutzt werden soll oder an ganz anderer Stelle existiert als das Projekt, an dem Sie gerade arbeiten, könnte eine Installation oder Verknüpfung sinnvoller sein.
Modulen in aktiver Entwicklung profitieren auch von der automatischen Aktualisierung der Verknüpfung.
Wenn sich das Modul nicht in der aktiven Entwicklung befindet, kann die Verwendung von npm install die einfachere Option sein.
In diesem Tutorial haben Sie gelernt, dass ein Node.js-Modul eine JavaScript-Datei mit Funktionen und Objekten ist, die von anderen Programmen verwendet werden können.
Anschließend haben Sie ein Modul erstellt und Ihre Funktionen und Objekte an das lokale Objekt exports angehängt, um sie externen Programmen zur Verfügung zu stellen.
Schließlich haben Sie dieses Modul in ein Programm importiert, um zu demonstrieren, wie Module zu größeren Anwendungen zusammengefügt werden.
Nachdem Sie nun wissen, wie Sie Module erstellen, denken Sie über die Art des Programms nach, das Sie schreiben möchten und zerlegen Sie es in verschiedene Komponenten, wobei Sie jeden eindeutigen Satz von Aktivitäten und Daten in ihren eigenen Modulen aufbewahren.
Je mehr Übung Sie beim Schreiben von Modulen erhalten, desto besser wird Ihre Fähigkeit, qualitativ hochwertige Node.js-Programme zu schreiben.
Ein Beispiel für eine Node.js-Anwendung, die Module verwendet, finden Sie in unserem Tutorial Einrichten einer Node.js-Anwendung für die Produktion unter Ubuntu 18.04.
Installieren des OpenLiteSpeed Web-Servers unter Ubuntu 18.04
3208
OpenLiteSpeed ist ein optimierter Open-Source-Webserver, der zur Verwaltung und Bereitstellung von Websites verwendet werden kann.
OpenLiteSpeed verfügt über einige nützliche Funktionen, die ihn zu einer soliden Wahl für viele Installationen machen: Apache-kompatible Neuschreibregeln, eine integrierte webbasierte Verwaltungsoberfläche und eine angepasste, für den Server optimierte PHP-Verarbeitung.
In diesem Leitfaden zeigen wir Ihnen, wie Sie OpenLiteSpeed auf einem Ubuntu 18.04 Server installieren und konfigurieren.
Um dieses Tutorial zu absolvieren, benötigen Sie einen Ubuntu 18.04-Server mit einem sudo-fähigen Nicht-Root-Benutzer und aktivierter ufw Firewall.
Anweisungen zur Erfüllung dieser Anforderungen finden Sie in unserem Tutorial Ersteinrichtung des Servers mit Ubuntu 18.04.
Schritt 1 - Installieren von OpenLiteSpeed
OpenLiteSpeed bietet ein Software-Repository, das wir zum Herunterladen und Installieren des Servers mit dem Standardbefehl apt von Ubuntu verwenden können.
Um dieses Repository für Ihr Ubuntu-System zu aktivieren, laden Sie zunächst den Software-Signaturschlüssel des Entwicklers herunter:
Dieser Schlüssel wird zur kryptographischen Verifizierung verwendet, dass die Software, die wir herunterladen möchten, nicht manipuliert wurde.
Als Nächstes fügen wir die Informationen des Repository in unser System ein:
Nachdem das Repository hinzugefügt wurde, wird unsere Paket-Cache mit dem Befehl add-apt-repository aktualisiert und die neue Software kann installiert werden.
Installieren Sie den OpenLiteSpeed-Server und seinen PHP-Prozessor mit apt install:
Erstellen Sie abschließend einen Softlink zu dem PHP-Prozessor, den wir gerade installiert haben.
Dadurch wird der OpenLiteSpeed-Server zur Verwendung der richtigen Version angewiesen:
Nachdem nun der OpenLiteSpeed-Server installiert ist, sichern wir ihn durch die Aktualisierung des Standardkontos admin.
Schritt 2 - Festlegen des administrativen Passworts
Bevor wir den Server testen, sollten wir ein neues administratives Passwoert für OpenLiteSpeed festlegen.
Standardmäßig ist das Passwort auf 123456 festgelegt, sodass wir dies sofort ändern sollten.
Dies können wir tun, indem wir ein von OpenLiteSpeed bereitgestelltes Skript ausführen:
Sie werden aufgefordert, einen Benutzernamen für den administrativen Benutzer anzugeben.
Wenn Sie die Eingabetaste drücken, ohne einen neuen Benutzernamen zu wählen, wird die Standardeinstellung von admin verwendet.
Dann werden Sie aufgefordert, ein neues Passwort für das Konto zu erstellen und zu bestätigen.
Tun Sie dies und drücken Sie dann ein letztes Mal die Eingabetaste.
Das Skript wird eine erfolgreiche Aktualisierung bestätigen:
Nachdem wir nun das Konto admin gesichert haben, testen wir den Server und stellen sicher, dass er einwandfrei läuft.
Schritt 3 - Starten und Verbinden mit dem Server
OpenLiteSpeed sollte nach der Installation automatisch gestartet werden.
Wir können dies mit dem Befehl lswsctrl überprüfen:
Wenn Sie keine ähnliche Meldung sehen, können Sie den Server mit lswsctrl starten:
Der Server sollte nun laufen.
Bevor wir ihn in unserem Browser besuchen können, müssen wir einige Ports in unserer Firewall öffnen.
Wir tun dies mit dem Befehl ufw:
Der erste Port, 8088, ist der Standardport für die Beispielseite von OpenLiteSpeed.
Es sollte nun für die Öffentlichkeit zugänglich sein.
Navigieren Sie in Ihrem Webbrowser zu dem Domänennamen oder der IP-Adresse Ihres Servers gefolgt von: 8088, um den Port anzugeben:
Ihr Browser sollte die Standardseite von OpenLiteSpeed laden, die wie folgt aussieht:
Screenshot der Standard-Demoseite von OpenLiteSpeed
Die Links am Ende der Seite sollen verschiedene Funktionen des Servers demonstrieren.
Wenn Sie diese nacheinander anklicken, werden Sie feststellen, dass diese Funktionen bereits installiert und richtig konfiguriert sind.
So ist zum Beispiel ein CGI-Beispielskript verfügbar, eine angepasste PHP-Instanz wird ausgeführt und benutzerdefinierte Fehlerseiten und Authentifizierungsgates sind konfiguriert.
Klicken Sie herum, um sich ein wenig umzusehen.
Wenn Sie mit der Standardseite zufrieden sind, können Sie zu der administrativen Oberfläche übergehen.
Navigieren Sie in Ihrem Webbrowser unter Verwendung von HTTPS zu dem Domänennamen oder der IP-Adresse Ihres Servers gefolgt von: 7080, um den Port anzugeben:
Sie werden wahrscheinlich eine Seite mit einer Warnung sehen, dass das SSL-Zertifikat von dem Server nicht validiert werden kann.
Da es sich um ein selbstsigniertes Zertifikat handelt, ist dies zu erwarten.
Klicken Sie durch die verfügbaren Optionen, um zu der Website zu gelangen.
In Chrome müssen Sie auf "Erweitert" und dann auf "Weiter zu..." klicken.
Sie werden aufgefordert, den administrativen Benutzernamen und das Passwort einzugeben, die Sie im vorherigen Schritt mit dem Skript admpass.sh ausgewählt haben:
Screenshot der OpenLiteSpeed Admin-Anmeldeseite
Sobald Sie sich korrekt authentifiziert haben, wird Ihnen die Administrationsoberfläche von OpenLiteSpeed angezeigt:
Screenshot des OpenLiteSpeed Admin-Dashboards
Hier findet der Großteil Ihrer Konfiguration für den Webserver statt.
Als Nächstes erkunden wir diese Oberfläche, indem wir eine allgemeine Konfigurationsaufgabe durchgehen: die Aktualisierung des von der Standardseite verwendeten Ports.
Schritt 4 - Ändern des Ports für die Standardseite
Um die Konfiguration von Optionen über die Weboberfläche zu demonstrieren, werden wir den Port, den die Standardseite verwendet, von 8088 auf den herkömmlichen HTTP-Port 80 ändern.
Um dies zu erreichen, klicken Sie zunächst in der Liste der Optionen auf der linken Seite der Oberfläche auf Listeners.
Eine Liste aller verfügbaren Listener wird geladen.
Klicken Sie in der Liste der Listener auf die Schaltfläche "Anzeigen / Bearbeiten" für den Listener Default:
Screenshot der Zusammenfassung der Listeners von OpenLiteSpeed
Dadurch wird eine Seite mit weiteren Einzelheiten über den Listener Default geladen.
Klicken Sie der oberen rechten Ecke der Tabelle "Address Settings" auf die Schaltfläche "Bearbeiten", um ihre Werte zu ändern:
Screenshot der Listener Detailseite von OpenLiteSpeed
Ändern Sie auf dem nächsten Bildschirm Port 8088 in Port 80 und klicken Sie anschließend auf das Diskettensymbol Save:
Screenshot der Aktualisierungsoberfläche für Listener von OpenLiteSpeed
Nach der Änderung müssen Sie den Server neu starten.
Klicken Sie auf das Pfeilsymbol "Neu laden", um OpenLiteSpeed neu zu starten:
Schaltfläche "Unterbrechungsfreier Neustart"
Zusätzlich müssen Sie nun Port 80 in Ihrer Firewall öffnen:
Die Standard-Webseite sollte nun in Ihrem Browser auf Port 80 anstatt Port 8088 zugänglich sein.
Wenn Sie den Domänennamen oder die IP-Adresse Ihres Servers aufrufen, ohne eine Port-Nummer anzugeben, wird nun die Website angezeigt.
OpenLiteSpeed ist ein mit allen Funktionen ausgestatteter Webserver, der in erster Linie über die administrative Weboberfläche verwaltet wird.
Ein vollständiger Überblick darüber, wie Sie Ihre Website über diese Oberfläche konfigurieren können, liegt außerhalb des Rahmens dieses Leitfadens.
Um Ihnen den Einstieg zu erleichtern, werden wir jedoch im Folgenden auf einige wichtige Punkte eingehen:
Alles im Zusammenhang mit OpenLiteSpeed finden Sie unter dem Verzeichnis / usr / local / lsws.
Der Dokumentenstamm (von dem aus Ihre Dateien bereitgestellt werden) für den standardmäßigen virtuellen Host befindet sich unter / usr / local / lsws / DEFAULT / html.
Die Konfiguration und die Protokolle für diesen virtuellen Host finden Sie unter dem Verzeichnis / usr / local / lsws / DEFAULT.
Sie können mit der Admin-Oberfläche neue virtuelle Hosts für verschiedene Websites erstellen.
Alle Verzeichnisse, auf die Sie bei der Einrichtung Ihrer Konfiguration verweisen, müssen jedoch zuvor auf Ihrem Server angelegt werden.
OpenLiteSpeed kann die Verzeichnisse nicht erstellen.
Sie können Virtual Host-Vorlagen für virtuelle Hosts einrichten, die das gleiche allgemeine Format besitzen.
Häufig ist es am einfachsten, die Verzeichnisstruktur und Konfiguration des standardmäßigen virtuellen Hosts zu kopieren, um sie als Ausgangspunkt für neue Konfigurationen zu verwenden.
Die Admin-Oberfläche verfügt über ein integriertes Tooltip-Hilfesystem für fast alle Felder.
Im linken Menü gibt es auch eine Help-Menüoption, die auf die Serverdokumentation verweist.
Konsultieren Sie diese Informationen während der Konfiguration, wenn Sie weitere Hilfe benötigen.
Weitere Information zur Sicherung Ihrer OpenLiteSpeed-Installation mit HTTPS finden Sie im Abschnitt SSL-Einrichtung der offiziellen Dokumentation.
Jetzt sollten OpenLiteSpeed und PHP auf einem Ubuntu 18.04 Server installiert sein und ausgeführt werden.
OpenLiteSpeed bietet hervorragende Leistung, eine webbasierte Konfigurationsoberfläche und vorab konfigurierte Optionen für die Skriptverarbeitung.
Installieren von MySQL unter CentOS 8
3941
MySQL ist ein Open-Source-Datenbankmanagementsystem, das üblicherweise als Teil des beliebten LEMP-Stacks (Linux, Nginx, MySQL / MariaDB, PHP / Python / Perl) installiert wird.
Es implementiert das relationale Modell und die Structured Query Language (SQL) zur Verwaltung und Abfrage von Daten.
Dieses Tutorial erklärt, wie MySQL Version 8 auf einem CentOS 8-Server installiert wird.
Um diesem Tutorial zu folgen. benötigen Sie einen Server, auf dem CentOS 8 ausgeführt wird. Dieser Server sollte einen Nicht-Root-Benutzer mit Administratorberechtigungen und eine mit firewalld konfigurierte Firewall haben.
Folgen Sie zur Einrichtung unserem Leitfaden zur Ersteinrichtung des Servers für CentOS 8.
Schritt 1 - Installieren von MySQL
Unter CentOS 8 ist MySQL Version 8 aus den Standard-Repositorys verfügbar.
Führen Sie den folgenden Befehl aus, um das Paket mysql-server und eine Reihe seiner Abhängigkeiten zu installieren:
Wenn Sie aufgefordert werden, drücken Sie y und dann die Eingabetaste, um zu bestätigen, dass Sie fortfahren möchten:
Damit ist MySQL auf Ihrem Server installiert, aber noch nicht betriebsbereit.
Das gerade von Ihnen installierte Paket konfiguriert MySQL so, dass es als Dienst systemd mit dem Namen mysqld.service ausgeführt wird.
Damit Sie MySQL verwenden können, müssen Sie es mit dem Befehl systemctl starten:
Führen Sie den nachfolgenden Befehl aus, um die korrekte Ausführung des Dienstes zu überprüfen.
Beachten Sie, dass Sie für viele systemctl-Befehle - einschließlich start und, wie hier gezeigt, status - nach dem Dienstnamen .service nicht einbinden müssen:
Wurde MySQL erfolgreich gestartet, zeigt die Ausgabe, dass der Dienst MySQL aktiv ist:
Damit MySQL bei jedem Hochfahren des Servers startet, stellen Sie MySQL anschließend mit folgendem Befehl ein:
< $> note Anmerkung: Wenn Sie dieses Verhalten zu einem beliebigen Zeitpunkt ändern und den Start von MySQL beim Hochfahren deaktivieren möchten, können Sie dies tun, indem Sie folgendes ausführen:
MySQL ist nun installiert, wird ausgeführt und ist auf Ihrem Server aktiviert.
Als Nächstes gehen wir durch, wie Sie die Sicherheit Ihrer Datenbank mit einem Shell-Skript, das mit Ihrer MySQL-Instanz vorinstalliert wurde, erhöhen können.
Schritt 2 - Sichern von MySQL
MySQL enthält ein Sicherheitsskript, mit dem Sie einige Optionen der Standardkonfiguration ändern können, um die Sicherheit von MySQL zu verbessern.
Zur Verwendung des Sicherheitsskripts führen Sie den folgenden Befehl aus:
Dies führt Sie durch eine Reihe von Aufforderungen, in denen Sie gefragt werden, ob Sie bestimmte Änderungen an den Sicherheitsoptionen Ihrer MySQL-Installation vornehmen möchten.
In der ersten Eingabeaufforderung werden Sie gefragt, ob Sie das Plugin Validate Password einrichten möchten, mit dem Sie die Stärke Ihres MySQL-Passworts testen können.
Wenn Sie sich für die Einrichtung des Validate Password-Plugins entscheiden, werden Sie von dem Skript gefragt, ob Sie eine Passwort-Validierungsstufe auswählen möchten.
Die stärkste Stufe - die Sie durch Eingabe von 2 auswählen - erfordert, dass Ihr Passwort mindestens acht Zeichen lang ist und eine Mischung aus Groß- und Kleinbuchstaben, Zahlen und Sonderzeichen enthält:
Unabhängig davon, ob Sie sich für die Einrichtung des Validate Password-Plugins entscheiden, werden Sie als nächstes dazu aufgefordert, ein Passwort für den MySQL-Benutzer root festzulegen.
Gehen Sie ein sicheres Passwort Ihrer Wahl ein und bestätigen es dann:
Wenn Sie das Validate Password-Plugin verwendet haben, erhalten Sie eine Rückmeldung über die Stärke Ihres neuen Passworts.
Dann fragt das Skript, ob Sie mit dem von Ihnen gerade eingegebenen Passwort fortfahren oder ein neues Passwort eingeben möchten.
Wenn Sie mit der Stärke des von Ihnen gerade eingegebenen Passworts zufrieden sind, geben Sie Y ein, um das Skript fortzusetzen:
Danach können Sie Y und dann die Eingabetaste drücken, um die Standardeinstellungen für alle nachfolgenden Fragen zu akzeptieren.
Damit werden einige anonyme Benutzer und die Testdatenbank entfernt, ferngesteuerte Root-Logins deaktiviert und diese neuen Regeln geladen, damit MySQL die Änderungen, die Sie gerade vorgenommen haben, unverzüglich anwendet.
Damit haben Sie MySQL auf Ihrem CentOS 8-Server installiert und gesichert.
In einem letzten Schritt testen wir, ob die Datenbank zugänglich ist und wie erwartet funktioniert.
Schritt 3 - Testen von MySQL
Sie können Ihre Installation überprüfen und Informationen darüber erhalten, indem Sie sich mit dem Tool mysqladmin, einem Client, mit dem Sie administrative Befehle ausführen können, verbinden.
Verwenden Sie den folgenden Befehl, um sich als root (-u root) mit MySQL zu verbinden, zur Eingabe eines Passworts (-p) aufgefordert zu werden und die Version der Installation ausgegeben zu bekommen:
Sie werden eine ähnliche Ausgabe wie diese sehen:
Diese zeigt an, dass Ihre Installation erfolgreich war.
Wenn Sie sich mit MySQL verbinden und mit dem Hinzufügen von Daten beginnen möchten, führen Sie Folgendes aus:
Wie der vorherige Befehl mysqladmin enthält dieser Befehl die Option -u, mit der Sie den Benutzer angeben können, als der Sie sich verbinden möchten (in diesem Fall root), sowie die Option -p, die den Befehl anweist, Sie nach dem im vorherigen Schritt festgelegten Benutzerpasswort zu fragen.
Nachdem Sie das Passwort Ihres MySQL-Benutzers root eingegeben haben, sehen Sie die MySQL-Eingabeaufforderung:
Von dort aus können Sie mit der Verwendung Ihrer MySQL-Installation beginnen, um Datenbanken zu erstellen und zu laden, sowie Abfragen auszuführen.
Wenn Sie diesem Tutorial gefolgt sind, haben Sie MySQL auf einem CentOS 8-Server installiert und gesichert.
Von hier aus können Sie Nginx und PHP installieren, um einen voll funktionsfähigen LEMP-Stack auf Ihrem Server zu haben.
Um mehr über die Verwendung von MySQL zu erfahren, empfehlen wir Ihnen, die offizielle Dokumentation zu lesen.
Konfigurieren von Packet Filter (PF) unter FreeBSD 12.1
4005
Die Firewall ist wahrscheinlich eine der wichtigsten Verteidigungslinien gegen Cyberangriffe.
Die Fähigkeit zur grundlegenden Konfiguration einer Firewall ist enorm wichtig, da sie es Administratoren ermöglicht, ihre Netzwerke richtig zu kontrollieren.
Packet Filter (PF) ist eine bekannte Firewall Anwendung, die vom sicherheitsorientierten OpenBSD-Projekt verwaltet wird.
Genauer gesagt handelt es sich um um ein Paketfilter-Tool (daher der Name), das für seine einfache Syntax, hohe Benutzerfreundlichkeit und umfangreichen Funktionen geschätzt wird.
PF ist standardmäßig eine Stateful-Firewall, die Informationen über Verbindungen in einer Tabelle für Zustandserfassung speichert, auf die für Analysezwecke zugegriffen werden kann.
PF ist Teil des FreeBSD-Basissystems und wird von einer starken Entwickler-Community unterstützt.
Obwohl es hinsichtlich der Kernel-Architektur Unterschiede zwischen den FreeBSD- und OpenBSD-Versionen von PF gibt, ist ihre Syntax im Allgemeinen ähnlich.
Je nach Komplexität können gängige Regelsätze für beide Distributionen mit relativ geringem Aufwand modifiziert werden.
In diesem Tutorial erstellen Sie mit PF von Grund auf eine Firewall auf einem FreeBSD 12.1-Server.
Sie werden einen grundlegenden Regelsatz einrichten, der als Vorlage für zukünftige Projekte dienen kann.
Außerdem werden Sie einige der fortgeschrittenen Funktionen von PF wie Packet-Hygiene, Abwehr von Brute-Force-Angriffen, Überwachung und Protokollierung sowie Tools anderer Anbieter kennenlernen.
Bevor Sie mit diesem Tutorial beginnen, benötigen Sie Folgendes:
Einen 1G FreeBSD 12.1-Server (entweder ZFS oder UFS).
Sie können unser Tutorial Erste Schritte mit FreeBSD nutzen, um Ihren Server mit Ihrer bevorzugten Konfiguration einzurichten.
Standardmäßig ist bei FreeBSD keine Firewall aktiviert - Anpassung ist ein besonderes Kennzeichen des Ethos von FreeBSD.
Wenn Sie Ihren Server zum ersten Mal starten, benötigen Sie also vorübergehend Schutz, während PF konfiguriert wird.
Wenn Sie DigitalOcean verwenden, können Sie Ihre Cloud-Firewall sofort nach dem Starten des Servers aktivieren.
Konsultieren Sie den Firewall Schnellstart von DigitalOcean, um Anweisungen zur Konfiguration einer Cloud-Firewall zu erhalten.
Wenn Sie einen anderen Cloud-Anbieter nutzen, bestimmen Sie den schnellsten Weg, um vor dem Loslegen sofortigen Schutz zu erreichen.
Egal welche Methode Sie wählen, darf Ihre temporäre Firewall nur eingehenden SSH-Verkehr zulassen, dafür aber alle Arten von ausgehendem Datenverkehr zulassen.
Schritt 1 - Erstellen Ihres vorläufigen Regelsatzes
Sie legen mit diesem Tutorial los, indem Sie einen vorläufigen Regelsatz erarbeiten, der grundlegenden Schutz und Zugriff auf kritische Dienste im Internet bietet.
An diesem Punkt verfügen Sie über einen ausgeführten FreeBSD 12.1-Server mit einer aktiven Cloud-Firewall.
Es gibt zwei Ansätze zum Erstellen einer Firewall: default deny und default permit.
Beim default deny-Ansatz wird der ganze Verkehr blockiert und nur zugelassen, was in einer Regel angegeben ist.
Der default permit-Ansatz bewirkt das genaue Gegenteil: Er überträgt den gesamten Datenverkehr und blockiert nur das, was in einer Regel angegeben ist.
Sie werden den default deny-Ansatz verwenden.
PF-Regelsätze werden in eine Konfigurationsdatei namens / etc / pf.conf geschrieben, was auch ihr Standardspeicherort ist.
Es ist in Ordnung, diese Datei an einem anderen Ort zu speichern, solange dieser in der Konfigurationsdatei / etc / rc.conf angegeben wird.
In diesem Tutorial verwenden Sie den Standardspeicherort.
Melden Sie sich mit Ihrem non-root user bei Ihrem Server an:
Als Nächstes erstellen Sie Ihre / etc / pf.conf-Datei:
< $> note Anmerkung: Wenn Sie an einem beliebigen Punkt im Tutorial den vollständigen Basisregelsatz anzeigen möchten, können Sie die Beispiele in Schritt 4 oder Schritt 8 konsultieren. < $>
PF filtert Pakete anhand von drei zentralen Aktionen: block, pass und match.
In Kombination mit anderen Optionen bilden diese Regeln.
Eine Maßnahme wird ergriffen, wenn ein Paket die in einer Regel angegebenen Kriterien erfüllt.
Wie Sie möglicherweise erwarten, sorgen die Regeln pass und block dafür, dass Datenverkehr durchgelassen bzw. blockiert wird.
Eine match-Regel führt eine Aktion für ein Paket aus, wenn sie ein übereinstimmendes Kriterium findet, lässt das Paket aber weder zu noch blockiert sie es. Zum Beispiel können Sie die Netzwerkadressenübersetzung (NAT) für ein übereinstimmendes Paket ausführen, ohne es zu übertragen oder zu blockieren; es wird dort bleiben, bis Sie es in einer anderen Regel dazu anweisen, etwas zu tun (z. B. Weiterleitung an ein anderes Gerät oder Gateway).
Fügen Sie anschließend die erste Regel in Ihre / etc / pf.conf-Datei ein:
Diese Regel blockiert alle Arten von Datenverkehr in jede Richtung.
Da keine Richtung angegeben ist, gilt sie standardmäßig für sowohl eingehenden als auch ausgehenden Datenverkehr. Diese Regel ist anwendbar auf eine lokale Workstation, die gegenüber dem Rest der Welt isoliert werden soll, ist jedoch weitgehend unpraktisch und wird bei Remoteservern nicht funktionieren, da sie keinen SSH-Verkehr zulässt.
Wenn PF aktiviert gewesen wäre, hätten Sie sich sogar aus dem Server ausgesperrt.
Bearbeiten Sie Ihre Datei / etc / pf.conf mit der folgenden hervorgehobenen Zeile,, um SSH-Verkehr zu erlauben:
< $> note Anmerkung: Alternativ können Sie den Namen des Protokolls verwenden:
Aus Konsistenzgründen werden wir Portnummern verwenden, es sei denn, es gibt einen triftigen Grund dafür, es nicht zu tun.
Es gibt in der Datei / etc / services eine detaillierte Liste der Protokolle und ihrer jeweiligen Portnummern, die Sie sich ansehen sollten.
PF verarbeitet Regeln sequentiell von oben nach unten; d. h. Ihr aktueller Regelsatz blockiert zunächst den gesamten Datenverkehr, lässt ihn dann aber passieren, wenn die Kriterien in der nächsten Zeile erfüllt werden, was in diesem Fall SSH-Verkehr ist.
Sie können nun SSH-Daten an Ihren Server übertragen, blockieren aber weiterhin alle Formen von ausgehendem Datenverkehr.
Das ist ein Problem, da Sie keine kritischen Dienste aus dem Internet aufrufen können, um Pakete zu installieren, die Zeiteinstellungen zu aktualisieren und so weiter.
Fügen Sie darum am Ende Ihrer Datei / etc / pf.conf die folgende hervorgehobene Regel hinzu:
Ihr Regelsatz lässt nun ausgehenden SSH-, DNS-, HTTP-, NTP- und HTTPS-Verkehr zu und blockiert den gesamten eingehenden Datenverkehr (mit Ausnahme von SSH).
Sie platzieren die Portnummern und Protokolle in geschweiften Klammern, wodurch in PF-Syntax eine Liste gebildet wird, sodass Sie bei Bedarf mehr Portnummern hinzufügen können.
Sie fügen außerdem eine Pass-Out-Regel für das UDP-Protokoll an den Ports 53 und 123 hinzu, da DNS und NTP oft zwischen den TCP- und UDP-Protokollen wechseln.
Sie sind mit dem vorläufigen Regelsatz fast fertig und müssen nur noch ein paar Regeln hinzufügen, um grundlegende Funktionalität zu erreichen.
Vervollständigen Sie den vorläufigen Regelsatz mit den hervorgehobenen Regeln:
Sie erstellen für das Loopbackgerät eine set skip-Regel, da es Datenverkehr nicht filtern muss und Ihren Server wahrscheinlich spürbar verlangsamen würde.
Sie fügen eine pass out inet-Regel für das ICMP-Protokoll hinzu, was es Ihnen ermöglicht, das Dienstprogramm ping (8) zur Problembehandlung zu verwenden.
Die Option inet repräsentiert die IPv4-Adressfamilie.
ICMP ist ein Mehrzweckprotokoll, das von Netzwerkgeräten für verschiedene Arten von Kommunikation verwendet wird.
Das ping-Dienstprogramm zum Beispiel verwendet eine Art von Nachricht, die als Echoanforderung bekannt ist und welche Sie Ihrer icmp _ type-Liste hinzugefügt haben.
Vorsorglich lassen Sie nur Nachrichtentypen zu, die Sie brauchen, um zu verhindern, dass unerwünschte Geräte Ihren Server kontaktieren.
Wenn sich Ihre Anforderungen erhöhen, können Sie mehr Nachrichtentypen in Ihre Liste aufnehmen.
Sie haben nun einen funktionierenden Regelsatz, der grundlegende Funktionalität für die meisten Rechner bereitstellt.
Im nächsten Abschnitt überprüfen wir, ob alles richtig funktioniert, indem wir PF aktivieren und den vorläufigen Regelsatz testen.
Schritt 2 - Testen Ihres vorläufigen Regelsatzes
In diesem Schritt testen Sie Ihren vorläufigen Regelsatz und stellen von Ihrer Cloud-Firewall auf die PF-Firewall um, damit PF vollständig übernimmt.
Sie aktivieren Ihren Regelsatz mit dem Dienstprogramm pfctl, das das integrierte Befehlszeilentool von PF und die primäre Schnittstellenmethode zu PF ist.
PF-Regelsätze sind nichts weiter als Textdateien, was bedeutet, dass mit dem Laden neuer Regelsätze keine empfindlichen Verfahren verbunden sind.
Sie können einen neuen Regelsatz laden und der alte verschwindet.
Einen bestehenden Regelsatz müssen Sie nur selten (wenn überhaupt) leeren.
FreeBSD verwendet ein Netz von Shellskripten, das als rc-System bekannt ist, um zu verwalten, wie Dienste zur Startzeit gestartet werden; wir geben diese Dienste in verschiedenen rc-Konfigurationsdateien an.
Für globale Dienste wie PF verwenden Sie die Datei / etc / rc.conf.
Da rc-Dateien für das Wohlbefinden eines FreeBSD-Systems kritisch sind, sollten sie nicht direkt bearbeitet werden.
Stattdessen bietet FreeBSD ein Befehlszeilenprogramm namens sysrc, das Ihnen dabei hilft, diese Dateien sicher zu bearbeiten.
Aktivieren wir PF nun unter Verwendung des Befehlszeilenprogramms sysrc:
Überprüfen Sie diese Änderungen durch Drucken des Inhalts Ihrer Datei / etc / rc.conf:
Außerdem aktivieren Sie den Dienst pflog, der wiederum den Daemon pflog für die Anmeldung bei PF aktiviert.
Mit der Anmeldung arbeiten Sie in einem späteren Schritt.
Sie geben zwei globale Dienste in Ihrer Datei / etc / rc.conf an; sie werden aber erst dann initialisiert, wenn Sie den Server neu starten oder die Dienste manuell starten.
Starten Sie den Server neu, damit Sie auch Ihren SSH-Zugriff testen können.
Starten Sie PF, indem Sie den Server neu starten:
Die Verbindung wird verworfen.
Das Aktualisieren dauert ein paar Minuten.
Stellen Sie nun wieder eine SSH-Verbindung zum Server her:
Sie haben zwar Ihre PF-Dienste initialisiert, doch haben Sie Ihren Regelsatz / etc / pf.conf nicht geladen, was bedeutet, dass Ihre Firewall noch nicht aktiv ist.
Laden Sie den Regelsatz mit pfctl:
Wenn keine Fehler oder Nachrichten vorhanden sind, bedeutet dies, dass Ihr Regelsatz keine Fehler aufweist und die Firewall aktiv ist.
Nachdem PF nun ausgeführt wird, können Sie Ihren Server von Ihrer Cloud-Firewall trennen.
Dies können Sie im Bedienfeld in Ihrem DigitalOcean-Konto tun, indem Sie Ihr Droplet aus dem Portal Ihrer Cloud-Firewall entfernen.
Wenn Sie einen anderen Cloudanbieter verwenden, stellen Sie sicher, dass alles, was Sie als temporären Schutz nutzen, deaktiviert wird.
Eine Ausführung von zwei verschiedenen Firewalls auf einem Server wird mit hoher Wahrscheinlichkeit Probleme verursachen.
Starten Sie Ihren Server sicherheitshalber neu:
Stellen Sie nach ein paar Minuten erneut eine SSH-Verbindung mit Ihrem Server her:
PF ist nun Ihre aktive Firewall.
Sie können sich vergewissern, dass sie ausgeführt wird, indem Sie mit dem Dienstprogramm pfctl ein paar Daten aufrufen.
Sehen wir uns nun mit pfctl -si einige Statistiken und Zähler an:
Sie übergeben die Flags -si, die für show info stehen.
Dies ist eine der vielen Filterparameterkombinationen, die Sie mit pfctl verwenden können, um Daten über Ihre Firewallaktivitäten zu analysieren.
Sie sehen die folgenden Tabellendaten (die Werte variieren von Gerät zu Gerät):
Da Sie Ihren Regelsatz gerade aktiviert haben, gibt es noch nicht sehr viele Informationen.
Die Ausgabe zeigt jedoch, dass PF bereits 23 abgeglichene Regeln erfasst hat, was bedeutet, dass die Kriterien Ihres Regelsatzes 23 mal abgeglichen wurden.
Die Ausgabe bestätigt auch, dass Ihre Firewall funktioniert.
Außerdem lässt Ihr Regelsatz zu, dass ausgehender Datenverkehr auf einige kritische Dienste aus dem Internet zugreift, einschließlich des ping-Dienstprogramms.
Überprüfen wir nun mit ping über google.com die Internetverbindung und den DNS-Dienst.
Da Sie das Zähl-Flag -c 3 ausgeführt haben, werden Ihnen drei erfolgreiche Verbindungsantworten angezeigt:
Stellen Sie sicher, dass Sie mit dem folgenden Befehl auf das Repository pkgs zugreifen können:
Wenn Pakete aktualisiert werden müssen, fahren Sie fort und aktualisieren Sie sie.
Wenn beide Dienste funktionieren, bedeutet das, dass Ihre Firewall funktioniert und Sie jetzt fortfahren können.
Ihr vorläufiger Regelsatz bietet zwar Schutz und Funktionalität, doch handelt es sich dabei immer noch um einen elementaren Regelsatz, der weiter verbessert werden kann.
In den verbleibenden Abschnitten werden Sie Ihren Basisregelsatz vervollständigen und einige der fortgeschrittenen Funktionen von PF verwenden.
Schritt 3 - Vervollständigen Ihres Basisregelsatzes
In diesem Schritt bauen Sie auf dem vorläufigen Regelsatz auf, um Ihren Basisregelsatz zu vervollständigen.
Sie werden einige Ihrer Regeln neu organisieren und mit fortgeschritteneren Konzepten arbeiten.
Integration von Makros und Tabellen
In Ihrem vorläufigen Regelsatz haben Sie alle Ihre Parameter als vordefinierten Code in jede Regel aufgenommen (d. h. die Portnummern, aus denen die Listen bestehen).
Dies kann in Zukunft je nach Art Ihrer Netzwerke unüberschaubar werden.
Für organisatorische Zwecke umfasst PF Makros, Listen und Tabellen.
Sie haben bereits Listen direkt in Ihre Regeln aufgenommen, Sie können sie aber auch von Ihren Regeln trennen und unter Verwendung von Makros einer Variable zuweisen.
Öffnen Sie Ihre Datei, um einige Ihrer Parameter in Makros zu übertragen:
Fügen Sie nun ganz oben im Regelsatz den folgenden Inhalt ein:
Ändern Sie Ihre vorherigen SSH- und ICMP-Regeln mit Ihren neuen Variablen:
Ihre vorherigen SSH- und ICMP-Regeln verwenden jetzt Makros.
Die Variablennamen werden mit der Dollarzeichensyntax von PF bezeichnet.
Sie weisen Ihrer vtnet0-Schnittstelle aus Formalitätsgründen einer Variable mit dem gleichen Namen zu, was Ihnen die Option bietet, sie bei Bedarf in Zukunft umzubenennen.
Andere gängige Variablennamen für öffentliche Schnittstellen sind unter anderem $pub _ if oder $ext _ if.
Als Nächstes implementieren Sie eine Tabelle, die einem Makro ähnelt, jedoch der Aufnahme von Gruppen von IP-Adressen dient.
Erstellen wir nun eine Tabelle für nicht routingfähige IP-Adressen, die oft bei Denial of Service-Angriffen (DOS) eine Rolle spielen.
Sie können die in RFC6890 angegebenen IP-Adressen verwenden, in der spezielle IP-Adresseinträge definiert sind.
Ihr Server sollte über die öffentliche Schnittstelle keine Pakete an diese Adressen senden oder von diesen Adressen empfangen.
Erstellen Sie diese Tabelle, indem Sie den folgenden Inhalt direkt unter dem Makro icmp _ types hinzufügen:
Fügen Sie nun Ihre Regeln für die Tabelle < rfc6890 > unter der Regel set skip on lo0 hinzu:
Hier geben Sie die return-Option ein, die Ihre block out-Regel ergänzt.
Damit werden die Pakete verworfen; zudem wird eine RST-Nachricht an den Host gesendet, der versucht hat, diese Verbindungen herzustellen. Das hilft bei der Analyse der Hostaktivität.
Dann fügen Sie das Schlüsselwort egress hinzu, um automatisch die Standardrouten für die jeweiligen Schnittstellen zu finden.
Dies ist in der Regel eine sauberere Methode für die Suche nach Standardrouten, insbesondere bei komplexen Netzwerken.
Das Schlüsselwort quick führt die Regeln sofort aus, ohne den Rest des Regelsatzes zu berücksichtigen.
Wenn beispielsweise ein Paket mit unlogischen IP-Adressen versucht, sich mit dem Server zu verbinden, sollte die Verbindung sofort verworfen werden und es gibt keinen Grund dafür, das Paket im Laufe des verbleibenden Regelsatzes ausführen.
Schützen Ihrer SSH-Ports
Da Ihr SSH-Port für die Öffentlichkeit geöffnet ist, kann er missbraucht werden.
Ein offensichtliches Warnzeichen, das auf einen Angreifer hinweist, ist eine große Zahl von Anmeldeversuchen.
Wenn beispielsweise die gleiche IP-Adresse zehnmal in einer Sekunde versucht, sich bei Ihrem Server anzumelden, können Sie davon ausgehen, dass dies nicht durch menschliche Hände geschieht, sondern durch Computersoftware, die versucht hat, Ihr Anmeldepasswort zu knacken.
Diese Arten von systematischen Exploits werden oft als Brute Force-Angriffe bezeichnet und sind in der Regel erfolgreich, wenn der Server schwache Passwörter aufweist.
< $> warning Warnung: Wir empfehlen für alle Server dringend die Verwendung einer Authentifizierung mit öffentlichen Schlüsseln.
Konsultieren Sie das Tutorial von DigitalOcean zu schlüsselbasierter Authentifizierung.
PF verfügt über integrierte Funktionen für die Handhabung von Brute Force- und ähnlichen Angriffen.
Mit PF können Sie die Anzahl von gleichzeitigen Verbindungsversuchen, die von einem einzelnen Host zulässig sind, einschränken.
Wenn ein Host diese Grenzwerte überschreitet, wird die Verbindung verworfen und der Host vom Server verbannt.
Dazu verwenden Sie den Überladungsmechanismus von PF, der eine Tabelle mit gesperrten IP-Adressen pflegt.
Ändern Sie Ihre vorherige SSH-Regel, um die Anzahl der gleichzeitigen Verbindungen von einem einzelnen Host wie folgt zu begrenzen:
Sie fügen die Option keep state hinzu, mit der Sie die Zustandskriterien für die Überladungstabelle definieren können.
Sie übergeben den Parameter max-src-conn, um die Anzahl der gleichzeitigen Verbindungen anzugeben, die von einem einzelnen Host pro Sekunde erlaubt sind, sowie den Parameter max-src-conn-rate, um die Anzahl der neuen Verbindungen anzugeben, die von einem einzelnen Host pro Sekunde erlaubt sind.
Sie geben 15 Verbindungen für max-src-conn und 3 Verbindungen für max-src-conn-rate an.
Wenn diese Grenzwerte von einem Host überschritten werden, fügt der Mechanismus für die Überladung die Quell-IP-Adresse der Tabelle < bruteforce > hinzu, um diesen Host vom Server zu verbannen.
Schließlich sorgt die Option flush global dafür, dass die Verbindung sofort verworfen wird.
Sie haben in Ihrer SSH-Regel eine Überladungstabelle definiert, die Tabelle jedoch noch nicht in Ihrem Regelsatz deklariert.
Fügen Sie die Tabelle < bruteforce > unter dem Makro icmp _ types hinzu:
Das Schlüsselwort persist ermöglicht es, dass im Regelsatz eine leere Tabelle existiert.
Ohne dieses Schlüsselwort wird sich PF beschweren, dass es in der Tabelle keine IP-Adressen gibt.
Diese Maßnahmen stellen sicher, dass Ihr SSH-Port durch einen leistungsstarken Sicherheitsmechanismus geschützt ist.
PF ermöglicht Ihnen eine Konfiguration von schnellen Lösungen, um Sie vor katastrophalen Formen von Exploits zu schützen.
In den nächsten Abschnitten ergreifen Sie Schritte zur Bereinigung von Paketen bei ihrer Ankunft am Server.
Bereinigen Ihres Datenverkehrs
< $> note Anmerkung: In den folgenden Abschnitten werden die Grundlagen der TCP / IP-Protokoll-Suite beschrieben.
Wenn Sie das Einrichten von Webanwendungen oder Netzwerken planen, ist es in Ihrem Interesse, diese Konzepte zu beherrschen.
Sehen Sie sich das Tutorial von DigitalOcean Einführung in Netzwerkterminologie, -schnittstellen und -protokolle an.
Aufgrund der Komplexität der TCP / IP-Protokoll-Suite und der Hartnäckigkeit von bösartigen Akteuren kommen Pakete oft mit Diskrepanzen und Unklarheiten (wie überlappende IP-Fragmente, falsche IP-Adressen usw.) an.
Es ist dringend geboten, Ihren Datenverkehr zu bereinigen, bevor er in das System gelangt.
Der technische Begriff für dieses Verfahren ist Normalisierung.
Wenn Daten über das Internet übertragen werden, werden sie in der Regel an ihrer Quelle in kleinere Fragmente zerlegt, um die Übertragungsparameter des Zielhosts zu erfüllen, wo sie wieder zu kompletten Paketen zusammengesetzt werden.
Leider können Angreifer diesen Prozess auf verschiedene Arten, die über den Umfang dieses Tutorials hinausgehen, kapern.
Dank PF können Sie Fragmentierung jedoch mit einer Regel verwalten.
PF enthält ein Schlüsselwort scrub, das Pakete normalisiert.
Fügen Sie das Schlüsselwort scrub direkt vor der Regel block all hinzu:
Diese Regel wendet Bereinigung auf allen eingehenden Datenverkehr an.
Sie fügen die Option fragment reassemble hinzu, die verhindert, dass Fragmente in das System gelangen.
Stattdessen werden sie im Arbeitsspeicher zwischengespeichert, bis sie wieder zu kompletten Paketen zusammengesetzt werden; das bedeutet, dass Ihre Filterregeln nur einheitliche Paketen handhaben müssen.
Außerdem fügen Sie die Option max-mss 1440 hinzu, die die maximale Segmentgröße von wieder zusammengesetzten TCP-Paketen repräsentiert (auch als Nutzlast bezeichnet).
Sie geben einen Wert von 1.440 Byte an, was ein geeignetes Gleichgewicht zwischen Größe und Leistung darstellt, sodass viel Platz für die Header bleibt.
Ein weiterer wichtiger Aspekt der Fragmentierung ist ein Begriff, der als maximale Übertragungseinheit (MTU) bekannt ist.
Die TCP / IP-Protokolle erlauben es Geräten, Paketgrößen zur Herstellung von Verbindungen auszuhandeln.
Der Zielhost verwendet ICMP-Nachrichten, um die Quell-IP-Adresse seiner MTU zu informieren (ein Prozess, der als MTU-Pfadsuche bezeichnet wird).
Der spezifische ICMP-Nachrichtentyp lautet Ziel nicht erreichbar.
Sie aktivieren die MTU-Pfadsuche, indem Sie den Nachrichtentyp unreach in Ihre Liste icmp _ types aufnehmen.
Sie verwenden die Standard-MTU Ihres Servers von 1.500 Byte, die mit dem Befehl ifconfig festgelegt werden kann:
Sie sehen die folgende Ausgabe, die Ihre aktuelle MTU enthält:
Aktualisieren Sie die Liste icmp _ types, um den Nachrichtentyp Ziel nicht erreichbar aufzunehmen:
Nachdem Sie nun über Richtlinien zum Umgang mit Fragmentierung verfügen, sind die Pakete, die in Ihr System gelangen, einheitlich und konsistent.
Dies ist wünschenswert, da es extrem viele verschiedene Geräte gibt, die Daten über das Internet austauschen.
Sie werden nun daran arbeiten, ein anderes Sicherheitsproblem zu verhindern, das als IP-Spoofing bekannt ist.
Angriffe ändern oftmals ihre Quell-IP-Adressen, um so zu tun, als befänden sie sich innerhalb des Unternehmens in einem vertrauenswürdigen Knoten.
PF umfasst eine Antispoofing-Direktive für den Umgang mit gespooften Quell-IP-Adressen.
Wenn es auf eine bestimmte Schnittstelle angewendet wird, sperrt Antispoofing allen Datenverkehr aus dem Netzwerk an dieser Schnittstelle (es sei denn, er stammt von dieser Schnittstelle).
Wenn Sie zum Beispiel Antispoofing auf Schnittstellen anwenden, die sich bei 5.5.5.1 / 24 befinden, kann der gesamte Datenverkehr vom Netzwerk 5.5.5.0 / 24 nicht mit dem System kommunizieren, es sei denn, er stammt von diesen Schnittstellen.
Fügen Sie den folgenden hervorgehobenen Inhalt hinzu, um Antispoofing auf Ihre vtnet0-Schnittstelle anzuwenden:
Diese Antispoofing-Regel besagt, dass der gesamte Datenverkehr von vtnet0-Netzwerken nur über die vtnet0-Schnittstelle übertragen werden darf; andernfalls wird der Datenverkehr mit dem Schlüsselwort quick sofort verworfen.
Bösartige Akteure können sich so nicht im Netzwerk von vtnet0 verstecken und mit anderen Knoten kommunizieren.
Um Ihre Antispoofing-Regel zu testen, drucken Sie Ihren Regelsatz auf dem Bildschirm in ausführlicher Form aus. Regeln in PF sind typischerweise in einer verkürzten Form verfasst, können aber auch in einer ausführlichen Form geschrieben sein. Er ist im Allgemeinen unpraktisch, Regeln ausführlich zu schreiben, kann aber für Testzwecke hilfreich sein.
Drucken Sie den Inhalt von / etc / pf.conf mit pfctl und dem folgenden Befehl:
Der Befehl pfctl benötigt die Flags -nvf, die den Regelsatz drucken und testen, ohne dass irgendetwas tatsächlich geladen wird (auch als Probelauf bekannt).
Sie sehen nun den gesamten Inhalt von / etc / pf.conf in seiner ausführlichen Form.
Sie sehen im Antispoofing-Abschnitt etwas, das der folgenden Ausgabe ähnelt:
Ihre Antispoofing-Regel hat erkannt, dass sie Teil des Netzwerks < ^ > your _ server _ ip < ^ / 20 ist.
Außerdem hat sie ermittelt, dass der Server (im Beispiel dieses Tutorials) Teil eines < ^ > network _ address < ^ > / 16-Netzwerks ist und über eine zusätzliche IPv6-Adresse verfügt.
Antispoofing hindert alle diese Netzwerke daran, mit dem System zu kommunizieren, es sei denn, ihr Datenverkehr wird über die vtnet0-Schnittstelle übertragen.
Ihre Antispoofing-Regel ist die letzte Ergänzung zu Ihrem Basisregelsatz.
Im nächsten Schritt werden Sie diese Änderungen initiieren und einige Tests durchführen.
Schritt 4 - Testen Ihres Basisregelsatzes
In diesem Schritt überprüfen und testen Sie Ihren Basisregelsatz, um sicherzustellen, dass alles richtig funktioniert.
Am besten vermeiden Sie es, zu viele Regeln auf einmal zu implementieren, ohne sie getestet zu haben.
Eine bewährte Methode besteht darin, mit den Grundlagen zu beginnen, inkrementell zu erweitern und Ergebnisse bei Konfigurationsänderungen zu sichern.
Hier ist Ihr vollständiger Basisregelsatz:
Stellen Sie sicher, dass Ihre Datei / etc / pf.conf mit diesem vollständigen Basisregelsatz übereinstimmt, bevor Sie fortfahren.
Ihr vollständiger Basisregelsatz bietet Ihnen:
Eine Sammlung von Makros, die wichtige Dienste und Geräte definieren können.
Richtlinien zur Netzwerkhygiene für Paketfragmentierung und illogische IP-Adressen.
Eine default deny-Filterstruktur, die alles blockiert und nur zulässt, was Sie angeben.
Eingehenden SSH-Zugriff mit Begrenzungen der Anzahl der gleichzeitigen Verbindungen, die von einem Host hergestellt werden können.
Richtlinien für ausgehenden Datenverkehr, die Ihnen Zugriff auf einige kritische Dienste aus dem Internet erlauben.
ICMP-Richtlinien, die Zugriff auf das ping-Dienstprogramm und die MTU-Pfadsuche bieten.
Führen Sie den folgenden pfctl-Befehl aus, um einen Probelauf vorzunehmen:
Sie übergeben die -nf-Flags, die pfctl anweisen, den Regelsatz auszuführen, ohne in zu laden. Das führt zur Ausgabe von Fehlern, wenn etwas falsch ist.
Laden Sie nun, da keine Fehler gefunden wurden, den Regelsatz:
Wenn keine Fehler vorliegen, bedeutet dies, dass Ihr Basisregelsatz aktiv ist und ordnungsgemäß funktioniert.
Wie zuvor in diesem Tutorial führen Sie einige Tests für Ihren Regelsatz aus.
Prüfen Sie zunächst die Internetverbindung und den DNS-Dienst:
Überprüfen Sie dann, ob Sie das Repository pkgs erreichen können:
Aktualisieren Sie wiederum Pakete, wenn dies benötigt wird.
Starten Sie schließlich Ihren Server neu:
Geben Sie Ihrem Server einige Minuten zum Neustart.
Sie haben Ihren Basisregelsatz fertig gestellt und implementiert, was ein signifikanter Fortschritt ist.
Sie können nun einige der fortgeschrittenen Funktionen von PF erkunden.
Im nächsten Schritt verhindern Sie auch Brute Force-Angriffe verhindern.
Schritt 5 - Verwalten Ihrer Überladungstabelle
Mit der Zeit füllt sich die Überladungstabelle < bruteforce > mit bösartigen IP-Adressen; darum müssen Sie sie regelmäßig löschen.
Es ist unwahrscheinlich, dass ein Angreifer die gleiche IP-Adresse weiter verwenden wird. Daher ist es wenig sinnvoll, sie für lange Zeiträume in der Überladungstabelle zu speichern.
Sie verwenden pfctl mit dem folgenden Befehl, um IP-Adressen manuell zu löschen, die in der Überladungstabelle für 48 Stunden oder länger gespeichert waren:
Sie werden eine Ausgabe sehen, die der folgenden ähnelt:
Sie übergeben das Flag -t bruteforce, das für table bruteforce steht, und das Flag -T, mit dem sich eine Handvoll von integrierten Befehlen ausführen lässt.
In diesem Fall führen Sie den Befehl expire aus, um alle Einträge aus -t bruteforce zu löschen, wobei der Zeitwert in Sekunden dargestellt wird.
Da Sie mit einem neuen Server arbeiten, gibt es in der Überladungstabelle wahrscheinlich noch keine IP-Adressen.
Diese Regel eignet sich für schnelle Korrekturen, eine robustere Lösung bestünde jedoch darin, den Prozess mit cron, dem Auftragsplaner von FreeBSD, zu automatisieren.
Lassen Sie uns stattdessen ein Shell-Skript erstellen, das diese Befehlssequenz ausführt.
Erstellen Sie eine Shell-Skript-Datei im Verzeichnis / usr / local / bin:
Fügen Sie Ihrem Shell-Skript den folgenden Inhalt hinzu:
Machen Sie die Datei mit dem folgenden Befehl ausführbar:
Als Nächstes erstellen Sie einen Cron Job.
Das sind Jobs, die anhand einer Zeit, die Sie angeben, wiederholt ausgeführt werden.
Sie werden häufig für Backups oder andere Prozesse verwendet, die jeden Tag zur gleichen Zeit ausgeführt werden sollen.
Sie erstellen Cron-Jobs mit crontab-Dateien.
Konsultieren Sie bitte die Hauptseiten, um mehr über cron (8) und crontab (5) zu erfahren.
Erstellen Sie mit dem folgenden Befehl eine crontab-Datei für einen Benutzer mit Rootberechtigung:
Fügen Sie der Datei crontab nun den folgenden Inhalt hinzu:
< $> note Anmerkung: Richten Sie jeden Wert an seinem entsprechenden Tabelleneintrag aus, um die Lesbarkeit zu erhöhen, falls Dinge beim Hinzufügen des Inhalts nicht richtig ausgerichtet sind.
Dieser Cron-Job führt jeden Tag um Mitternacht das Skript clear _ overload.sh aus, um IP-Adressen, die 48 Stunden alt sind, aus der Überladungstabelle < bruteforce > zu entfernen.
Als Nächstes fügen Sie Ihrem Regelsatz Anker hinzu.
Schritt 6 - Hinzufügen von Ankern zu Ihren Regelsätzen
In diesem Schritt fügen Sie Anker ein, die zur Einbeziehung von Regeln in den Hauptregelsatz dienen, entweder manuell oder aus einer externen Textdatei.
Anker können Regelausschnitte, Tabellen und auch andere Anker enthalten, die als verschachtelte Anker bekannt sind.
Wir zeigen Ihnen nun, wie Anker funktionieren, indem wir eine Tabelle zu einer externen Datei hinzufügen und sie in Ihren Basisregelsatz einbinden.
Ihre Tabelle wird eine Gruppe von internen Hosts enthalten, bei denen Sie verhindern möchten, dass sie sich mit der Außenwelt verbinden können.
Erstellen Sie eine Datei namens / etc / blocked-hosts-anchor:
Fügen Sie der Datei folgende Inhalte hinzu:
Diese Regeln deklarieren und definieren die Tabelle < blocked-hosts > und hindern dann alle IP-Adressen in der Tabelle < blocked-hosts > daran, auf Dienste aus der Außenwelt zuzugreifen.
Sie verwenden das Schlüsselwort egress als bevorzugte Methode zum Auffinden der Standardroute oder als Weg hinaus in das Internet.
Sie müssen den Anker noch in Ihrer Datei / etc / pf.conf deklarieren:
Fügen Sie nun nach der Regel block all die folgenden Ankerregeln hinzu:
Diese Regeln deklarieren die Tabelle blocked _ hosts und laden die Ankerregeln aus der Datei / etc / blocked-hosts-anchor in Ihren Hauptregelsatz.
Initiieren Sie nun diese Änderungen durch Neuladen Ihres Regelsatzes mit pfctl:
Wenn keine Fehler vorhanden sind, bedeutet dies, dass es keine Fehler im Regelsatz gibt und Ihre Änderungen aktiv sind.
Verwenden Sie pfctl, um zu überprüfen, ob Ihr Anker ausgeführt wird:
Das Flag -s Anchors steht für "show anchors" (Anker anzeigen).
Das Dienstprogramm pfctl kann außerdem die spezifischen Regeln Ihres Ankers mit den Flags -a und -s parsen:
Eine weitere Eigenschaft von Ankern besteht darin, dass sie es erlauben, Regeln bei Bedarf hinzuzufügen, ohne dass der Regelsatz neu geladen werden muss.
Dies kann nützlich sein für Tests, schnelle Problembehebungen, Notfälle und so weiter.
Wenn sich ein interner Host beispielsweise merkwürdig verhält und Sie ihn daran hindern möchten, ausgehende Verbindungen herzustellen, kann ein Anker eingerichtet werden, der es Ihnen ermöglicht, schnell über die Befehlszeile einzugreifen.
Öffnen Sie die Datei / etc / pf.conf und fügen Sie einen weiteren Anker hinzu:
Sie nennen den Anker rogue _ hosts und platzieren ihn in der Regel block all:
Um diese Änderungen zu initiieren, laden Sie den Regelsatz mit pfctl neu:
Verwenden Sie erneut pfctl, um zu überprüfen, ob der Anker ausgeführt wird:
Dadurch erhalten Sie folgende Ausgabe:
Jetzt wo der Anker ausgeführt wird, können Sie jederzeit Regeln hinzufügen.
Testen Sie dies durch Hinzufügen der folgenden Regel:
Dadurch werden der Befehl echo und der Inhalt seiner Zeichenfolge aufgerufen, was dann mit dem Symbol | in das Dienstprogramm pfctl übertragen und dort zu einer Ankerregel verarbeitet wird.
Öffnen Sie mit dem Befehl sh-c eine weitere Shell-Sitzung.
Das liegt daran, dass Sie eine Pipe zwischen zwei Prozessen einrichten, aber sudo-Berechtigungen benötigen, damit in der gesamten Befehlssequenz Persistenz herrscht.
Es gibt verschiedene Möglichkeiten, um dies zu lösen; hier öffnen Sie einen zusätzlichen Shell-Prozess mit sudo-Berechtigungen unter Verwendung von sudo sh -c.
Verwenden Sie nun erneut pfctl, um sich zu vergewissern, dass diese Regeln aktiv sind:
Die Verwendung von Ankern ist vollständig situationsabhängig und oftmals subjektiv.
Wie bei jeder anderen Funktion ist der Einsatz von Ankern mit Vor-und Nachteilen verbunden.
Manche Anwendungen wie z. B. blacklistd verbinden sich absichtlich mit Ankern.
Als Nächstes konzentrieren Sie sich auf die Protokollierung mit PF, was ein kritischer Aspekt der Netzwerksicherheit ist.
Ihre Firewall nützt wenig, wenn Sie nicht sehen können, was sie tut.
Schritt 7 - Protokollieren der Aktivität Ihrer Firewall
In diesem Schritt arbeiten Sie mit der Protokollierung von PF, die von einer Pseudoschnittstelle namens pflog verwaltet wird.
Das Protokollieren wird zur Startzeit aktiviert, indem Sie pflog _ enabled = YES zur Datei / etc / rc.conf hinzufügen; das haben Sie in Schritt 2 getan. Dadurch wird der Daemon pflogd aktiviert, der eine Schnittstelle namens pflog0 hervorbringt und Protokolle im Binärformat in eine Datei namens / var / log / pflog schreibt.
Protokolle können von der Schnittstelle in Echtzeit analysiert oder mit dem Dienstprogramm tcpdump (8) aus der Datei / var / log / pflog gelesen werden.
Greifen Sie zunächst auf einige Protokolle aus der Datei / var / log / pflog zu:
Sie übergeben die Flags -ner, die die Ausgabe zur besseren Lesbarkeit formatieren, und geben auch eine Datei an, die ausgelesen werden soll (in Ihrem Fall / var / log / pflog).
In dieser frühen Phase sind in der Datei / var / log / pflog möglicherweise noch keine Daten vorhanden.
Innerhalb kurzer Zeit beginnt die Protokolldatei zu wachsen.
Sie können die Protokolle auch in Echtzeit über die pflog0-Schnittstelle anzeigen, indem Sie den folgenden Befehl verwenden:
Sie übergeben die Flags -nei, die ebenfalls die Ausgabe zur besseren Lesbarkeit formatieren; dieses Mal geben Sie jedoch eine Schnittstelle an (in Ihrem Fall pflog0).
Sie sehen nun Verbindungen in Echtzeit.
Wenn dies möglich ist, senden Sie von einem Remotecomputer einen Ping an Ihren Server um zu sehen, welche Verbindungen es gibt.
Der Server bleibt in diesem Zustand, bis Sie ihn beenden.
Um diesen Zustand zu beenden und zur Befehlszeile zurückzukehren, drücken Sie Strg + Z.
Über tcpdump (8) gibt es eine Fülle von Informationen im Internet, einschließlich der offiziellen Website.
Zugreifen auf Protokolldateien mit pftop
Das Dienstprogramm pftop ist ein Tool zur schnellen Betrachtung der Firewall-Aktivitäten in Echtzeit.
Sein Name ist vom bekannten Unix-Dienstprogramm top beeinflusst.
Um das Programm verwenden zu können, müssen Sie das Paket pftop installieren:
Führen Sie nun die Binärdatei pftop aus:
Dadurch wird die folgende Ausgabe generiert (Ihre IP-Adressen werden sich unterscheiden):
Erstellen zusätzlicher Protokollschnittstellen
Wie bei anderen Schnittstellen auch können mehrere Protokollschnittstellen erstellt und mit einer / etc / hostname-Datei benannt werden.
Sie finden dies ggf. nützlich für organisatorische Zwecke, z. B. wenn Sie bestimmte Arten von Aktivitäten getrennt protokollieren möchten.
Erstellen Sie eine zusätzliche Protokollierungsschnittstelle namens pflog1:
Fügen Sie der Datei / etc / hostname.pflog1 folgende Inhalte hinzu:
Aktivieren Sie nun das Gerät zur Startzeit in Ihrer Datei / etc / rc.conf:
Jetzt können Sie Ihre Firewall überwachen und protokollieren.
Dadurch sehen Sie, wer Verbindungen mit Ihrem Server herstellt und welche Arten von Verbindungen hergestellt werden.
In diesem Tutorial haben Sie einige erweiterte Konzepte in Ihren PF-Regelsatz eingefügt.
Es ist nur notwendig, erweiterte Funktionen zu implementieren, wenn Sie sie tatsächlich benötigen.
In jedem Fall setzen Sie alles im nächsten Schritt wieder auf den Basisregelsatz zurück.
Schritt 8 - Zurücksetzen auf Ihren Basisregelsatz
In diesem abschließenden Abschnitt kehren Sie wieder zu Ihrem Basisregelsatz zurück.
Dies ist ein schneller Schritt, der Sie wieder in einen minimalen Funktionsstand zurückversetzt.
Öffnen Sie den Basisregelsatz mit dem folgenden Befehl:
Löschen Sie den aktuellen Regelsatz in Ihrer Datei und ersetzen Sie ihn durch den folgenden Basisregelsatz:
Laden Sie den Regelsatz neu:
Wenn es keine Fehler durch den Befehl gibt, heißt das, dass es auch keine Fehler in Ihrem Regelsatz gibt und Ihre Firewall ordnungsgemäß funktioniert.
Außerdem müssen Sie die von Ihnen erstellte pflog1-Schnittstelle deaktivieren.
Da Sie ggf. nicht wissen, ob Sie sie noch benötigen, können Sie pflog1 mit dem Dienstprogramm sysrc deaktivieren:
Entfernen Sie nun die Datei / etc / hostname.pflog1 aus dem Verzeichnis / etc:
Starten Sie vor der Abmeldung den Server neu, um sicherzustellen, dass alle Ihre Änderungen aktiv und persistent sind:
Warten Sie ein paar Minuten, bevor Sie sich wieder bei Ihrem Server anmelden.
Wenn Sie PF optional mit einem Webserver implementieren möchten, stellt Folgendes einen Regelsatz für dieses Szenario dar.
Dieser Regelsatz ist ein ausreichender Ausgangspunkt für die meisten Webanwendungen.
Damit wird eine Überladungstabelle namens < webcrawlers > erstellt, die eine liberalere Überladungsrichtlinie aufweist als Ihr SSH-Port - auf Grundlage der Werte von max-src-conn 45 und max-src-conn-rate.
Das liegt daran, dass nicht alle Überladungen von bösartigen Akteuren stammen.
Sie können auch von nicht bösartigen Netbots stammen, sodass Sie übermäßige Sicherheitsmaßnahmen an den Ports 80 und 443 vermeiden. Wenn Sie sich entscheiden, den Regelsatz für Webserver zu implementieren, müssen Sie die Tabelle < webcrawlers > der Datei / etc / pf.conf hinzufügen und die IP-Adressen regelmäßig aus der Tabelle löschen.
Konsultieren Sie diesbzüglich Schritt 5.
In diesem Tutorial haben Sie PF unter FreeBSD 12.1 konfiguriert.
Sie verfügen nun über einen Basisregelsatz, der als Ausgangspunkt für alle Ihre FreeBSD-Projekte dienen kann.
Weitere Informationen zu PF finden Sie in den man-Seiten zu pf.conf (5).
Besuchen Sie unsere FreeBSD-Themenseite für weitere Tutorials sowie Fragen und Antworten.
Erstellen eines neuen Sudo-fähigen Benutzers unter Ubuntu 18.04 Schnellstart
3991
Der Befehl sudo bietet ein Verfahren zur Gewährung von Administratorberechtigungen - die normalerweise nur dem root user zur Verfügung stehen - für normale Benutzer.
Dieser Leitfaden zeigt Ihnen, wie Sie unter Ubuntu 18.04 einen neuen Benutzer mit sudo-Zugriff erstellen können, ohne die Datei / etc / sudoers Ihres Servers ändern zu müssen.
Wenn Sie sudo für einen bestehenden Benutzer konfigurieren möchten, springen Sie zu Schritt 3.
Schritt 1 - Anmelden bei Ihrem Server
Stellen Sie als Root-Benutzer eine SSH-Verbindung zu Ihrem Server her:
Schritt 2 - Hinzufügen eines neuen Benutzers zum System
Verwenden Sie den Befehl adduser, um Ihrem System einen neuen Benutzer hinzuzufügen:
Stellen Sie sicher, dass Sie < ^ > sammy < ^ > durch den Benutzernamen ersetzen, den Sie erstellen möchten.
Sie werden dazu aufgefordert, ein Passwort für den Benutzer einzurichten und zu verifizieren:
Als Nächstes werden Sie dazu aufgefordert, einige Informationen über den neuen Benutzer einzugeben.
Es ist in Ordnung, die Standardeinstellungen zu akzeptieren und alle diese Informationen leer zu lassen:
Schritt 3 - Hinzufügen des Benutzers zur Gruppe sudo
Verwenden Sie den Befehl usermod, um den Benutzer der Guppe sudo hinzuzufügen:
Stellen Sie auch hier sicher, dass Sie < ^ > sammy < ^ > durch den von Ihnen gerade hinzugefügten Benutzernamen ersetzen.
Standardmäßig haben unter Ubuntu alle Mitglieder der Gruppe sudo volle sudo-Berechtigungen.
Schritt 4 - Testen des sudo-Zugriffs
Um zu testen, ob die neuen sudo-Berechtigungen funktionieren, verwenden Sie zunächst den Befehl su, um zum neuen Benutzerkonto zu wechseln:
Überprüfen Sie als neuer Benutzer, ob Sie sudo verwenden können, indem Sie dem Befehl, den Sie mit Superuser-Berechtigungen ausführen möchten, sudo voranstellen:
Sie können beispielsweise den Inhalt des Verzeichnisses / root auflisten, das normalerweise nur für den Benutzer mit Rootberechtigung zugänglich ist:
Wenn Sie sudo in einer Sitzung zum ersten Mal verwenden, werden Sie zur Eingabe des Passworts für dieses Benutzerkonto aufgefordert.
Geben Sie das Passwort ein, um fortzufahren:
< $> note Anmerkung: Hier werden Sie nicht nach dem Root-Passwort gefragt!
Geben Sie das Passwort des sudo-fähigen Benutzers ein, nicht ein Root-Passwort.
Wenn Ihr Benutzer der richtigen Gruppe angehört und Sie das Passwort korrekt eingegeben haben, wird der Befehl, den Sie mit sudo ausgegeben haben, mit Root-Berechtigungen ausgeführt.
In diesem Schnellstart-Tutorial haben wir ein neues Benutzerkonto erstellt und der Gruppe sudo hinzugefügt, um sudo-Zugriff zu aktivieren.
Detaillierte Informationen zur Einrichtung eines Ubuntu 18.04-Servers finden Sie in unserem Tutorial zur Ersteinrichtung eines Servers unter Ubuntu 18.04.
Installieren und Konfigurieren von SimpleSAMLphp zur SAML-Authentifizierung unter Ubuntu 18.04
4008
SimpleSAMLphp ist eine Open-Source-basierte PHP-Authentifizierungsanwendung, die Unterstützung für SAML 2.0 als Dienstanbieter (SP) oder Identitätsanbieter (IdP) bietet.
SAML (Security Assertion Markup Language) ist ein sicheres, XML-basiertes Kommunikationsverfahren zum Austausch von Authentifizierungs- und Autorisierungsdaten zwischen Organisationen und Anwendungen.
Häufig wird es zur Implementierung von Web SSO (Single Sign On) verwendet.
Dadurch wird vermieden, dass Sie für unterschiedliche Organisationen verschiedene Anmeldeinformationen für Authentifizierung verwalten müssen.
Einfach ausgedrückt: Sie können für Zugriff auf verschiedene Anwendungen eine Identität verwenden, z. B. einen Benutzernamen und ein Passwort.
Eine Instanz von SimpleSAMLphp verbindet sich mit einer Authentifizierungsquelle, nämlich einem Identitätsanbieter wie LDAP oder einer Benutzerdatenbank.
Sie authentifiziert Benutzer bei dieser Authentifizierungsquelle, bevor sie Zugriff auf Ressourcen gewährt, die bei verknüpften Dienstanbietern verfügbar sind.
In diesem Tutorial installieren Sie SimpleSAMLphp und konfigurieren es für die Verwendung einer MySQL-Datenbank als Authentifizierungsquelle.
Sie werden in der MySQL-Datenbank Benutzer und verschlüsselte Passwörter speichern und testen, ob Sie diese Benutzer zur Anmeldung verwenden können.
Einen Ubuntu 18.04-Server, der gemäß der Anleitung zum Setup des Ubuntu 18.04-Servers eingerichtet wurde, einschließlich eines Sudo-Benutzers ohne Rootberechtigung und einer Firewall.
Apache, MySQL und PHP, auf dem Server unter Befolgung von Installieren von Linux, Apache, MySQL und PHP (LAMP-Stack) unter Ubuntu 18.04 installiert.
Ein Domänenname, der so konfiguriert ist, dass er auf Ihren Server verweist.
Sie erfahren, wie Sie Domänen auf DigitalOcean-Droplets verweisen können, indem Sie das Tutorial Verweisen auf DigitalOcean-Nameserver von gängigen Domain-Registrierungsstellen absolvieren.
Ein virtueller Host, für die Domäne mit der Direktive ServerName konfiguriert.
Folgen Sie Einrichten von virtuellen Apache-Hosts unter Ubuntu 18.04, um eine Direktive für Ihren Domänenamen einzurichten.
Ein Let 's Encrypt-Zertifikat, das für die Domäne eingerichtet wurde, die Sie unter Befolgung des Leitfadens Sichern von Apache mit Let' s Encrypt unter Ubuntu 18.04 konfiguriert haben.
Schritt 1 - Herunterladen und Installieren von SimpleSAMLphp
Die Installation von SimpleSAMLphp umfasst einige Schritte.
Wir müssen die Software selbst sowie einige zusätzliche Komponenten und Voraussetzungen herunterladen.
Außerdem müssen wir einige Änderungen an unserer virtuellen Host-Konfiguration vornehmen.
Melden Sie sich bei Ihrem Server an, wenn Sie es noch nicht getan haben.
Laden Sie SimpleSAMLphp von der Website des Projekts herunter.
SimpleSAMLphp verbindet immer die letzte stabile Version seiner Software mit derselben URL.
Das bedeutet, dass wir die neueste Version erhalten, indem wir Folgendes eingeben:
Dadurch wird eine komprimierte Datei namens download? latest heruntergeladen, die SimpleSAMLphp enthält.
Extrahieren Sie den Inhalt mit dem Befehl tar:
Die Dateien werden in einem neuen Verzeichnis namens simplesamlphp-1. < ^ > x.y < ^ > extrahiert, wobei < ^ > x.y < ^ > die aktuelle Versionsnummer ist.
Verwenden Sie den Befehl ls, um die Datei zu identifizieren:
Der Dateiname wird angezeigt:
Kopieren Sie nun mit dem Befehl cp den Inhalt des Verzeichnisses / var / simplesamlphp.
Stellen Sie sicher, dass Sie die Versionsnummer durch die Version ersetzen, die Sie verwenden:
Der Wechsel -a sorgt dafür, dass zusammen mit den Dateien und Ordnern auch die Dateiberechtigungen kopiert werden.
Der Punkt am Ende der Quelldatei stellt sicher, dass alles im Quellverzeichnis in das Zielverzeichnis kopiert wird (einschließlich versteckter Dateien).
< $> note Anmerkung: Wenn Sie die Dateien an einem anderen Ort installieren wollen, müssen Sie mehrere Dateien aktualisieren.
Konsultieren Sie die offizielle Installationsdokumentation von SimpleSAMLphp, um Details zu erhalten.
Es gibt einige zusätzliche Softwarepakete, die SimpleSAMLphp benötigt, einschließlich PHP-Erweiterungen für die Arbeit mit XML, Mehrbyte-Zeichenfolgen, curl und LDAP.
Außerdem wird memcached benötigt.
Installieren Sie diese Pakete mit Ihrem Paketmanager.
Aktualisieren Sie zunächst Ihre Paketliste:
Installieren Sie anschließend die Pakete:
Starten Sie Apache nach Abschluss der Installation neu, um die neuen PHP-Erweiterungen zu aktivieren:
Nachdem SimpleSAMLphp nun installiert ist, konfigurieren wir Apache, um die Dateien bereitzustellen.
Schritt 2 - Konfigurieren von Apache zum Bereitstellen von SimpleSAMLphp
Sie haben bereits eine Domäne konfiguriert und auf diesen Server verwiesen sowie einen virtuellen Host so eingerichtet, dass er HTTPS nutzt, indem Sie Apache mit Let 's Encrypt schützen.
Verwenden wir das, um SimpleSAMLphp bereitzustellen.
Das einzige SimpleSAMLphp-Verzeichnis, das für das Web sichtbar sein muss, ist / var / simplesamlphp / www.
Um es für das Web verfügbar zu machen, bearbeiten Sie die Virtual Host SSL Apache-Konfigurationsdatei für Ihre Domäne.
Wenn Ihre virtuelle Host-Konfigurationsdatei < ^ > your _ domain < ^ > .conf heißt, hat Let 's Encrypt eine neue Konfigurationsdatei namens < ^ > your _ domain < ^ > -le-ssl.conf erstellt, die HTTPS-Anfragen für Ihre Domäne behandelt.
Öffnen Sie mit dem folgenden Befehl die SSL-Konfigurationsdatei, um sie zu bearbeiten.
Stellen Sie sicher, dass Sie < ^ > your _ domain < ^ > durch den tatsächlichen Namen der Datei ersetzen:
Die Datei sollte ungefähr wie folgt aussehen, auch wenn die tatsächliche Datei möglicherweise mehr deskriptive Kommentare aufweist:
Die Direktive ServerName legt die Basisdomäne fest, die mit dieser Definition des virtuellen Hosts übereinstimmen sollte.
Das sollte der Domänenamen sein, für den Sie im Abschnitt Voraussetzungen ein SSL-Zertifikat eingerichtet haben.
Fügen Sie nun eine Alias-Direktive hinzu, die SimpleSAMLphp die Kontrolle über alle URLs verschafft, die mit https: / / < ^ > your _ domain < ^ > / simplesaml / * übereinstimmen.
Fügen Sie dazu der Konfigurationsdatei die folgende Zeile hinzu:
Das bedeutet, dass alle URLs, die mit < ^ > domain _ name < ^ > / simplesaml / * übereinstimmen, zum Verzeichnis var / simplesamlphp / www umgeleitet werden; dadurch erhält SimpleSAMLphp die Kontrolle.
Als Nächstes gewähren wir Zugriff auf das Verzeichnis / var / simplesamlphp / www, indem wir eine Zugriffskontrolle vom Typ Require all granted dafür angeben. Dadurch wird der SimpleSAMLphp-Dienst über das Web zugänglich gemacht.
Fügen Sie dazu der Konfigurationsdatei Folgendes hinzu:
Starten Sie Apache neu, damit die Änderungen wirksam werden:
Nachdem Apache nun so konfiguriert ist, dass die Anwendungsdateien bereitgestellt werden, konfigurieren wir SimpleSAMLphp.
Schritt 3 - Konfigurieren von SimpleSAMLphp
Als Nächstes müssen wir verschiedene Änderungen an der grundlegenden SimpleSAMLphp-Konfigurationsdatei vornehmen, die wir unter / var / simplesamlphp / config / config.php finden.
Öffnen Sie die Datei in Ihrem Editor:
Legen Sie das Administratorpasswort fest, indem Sie nach der Zeile 'auth.adminpassword' suchen und den Standardwert < ^ > 123 < ^ > durch ein sichereres Passwort ersetzen.
Mit diesem Passwort können Sie auf einige der Seiten in Ihrer SimpleSAMLphp-Installations-Weboberfläche zugreifen:
Legen Sie als Nächstes ein geheimes Salt fest, das aus einer zufällig erzeugten Zeichenfolge bestehen sollte.
Einige Teile von SimpleSAMLphp verwenden dieses Salt, um kryptographisch sichere Hashes zu erstellen.
Sie erhalten Fehler, wenn das Salt vom Standardwert nicht geändert wird.
Sie können die OpenSSL-Funktion rand verwenden, um eine zufällige Zeichenfolge zu generieren, die als Ihre geheime Salt-Zeichenfolge dient.
Öffnen Sie ein neues Terminal, verbinden Sie sich erneut mit Ihrem Server und führen Sie den folgenden Befehl aus, um diese Zeichenfolge zu generieren:
Die Option -base64 32 sorgt für eine Base64-codierte Zeichenfolge, die 32 Zeichen lang ist.
Suchen Sie dann in der Konfigurationsdatei den Eintrag 'secretsalt' und ersetzen Sie < ^ > defaultsecretsalt < ^ > durch die von Ihnen erstellte Zeichenfolge:
Legen Sie anschließend die technischen Kontaktdaten fest.
Diese Informationen finden Sie in den erzeugten Metadaten; SimpleSAMLphp sendet automatisch generierte Fehlerberichte an die von Ihnen angegebene E-Mail-Adresse.
Suchen Sie nach folgendem Abschnitt:
Ersetzen Sie < ^ > Administrator < ^ > und < ^ > na @ example.org < ^ > durch entsprechende Werte.
Legen Sie dann die Zeitzone fest, die Sie verwenden möchten.
Suchen Sie nach diesem Abschnitt:
Ersetzen Sie < ^ > null < ^ > durch eine bevorzugte Zeitzone aus dieser Liste von Zeitzonen für PHP.
Stellen Sie sicher, dass der Wert in Anführungszeichen eingeschlossen ist:
Sie sollten nun in Ihrem Browser auf die Site zugreifen können, indem Sie https: / / < ^ > your _ domain < ^ > / simplesaml aufrufen.
Sie sehen den folgenden Bildschirm in Ihrem Browser:
simplesaml-Weboberfläche
Um sicherzustellen, dass Ihre PHP-Installation alle Anforderungen für eine reibungslose Ausführung von SimpleSAMLphp erfüllt, wählen Sie die Registerkarte Konfiguration und klicken Sie auf den Link Als Administrator anmelden.
Verwenden Sie dann das Administratorpasswort, das Sie in der Konfigurationsdatei in Schritt 3 festgelegt haben:
Sobald Sie angemeldet sind, sehen Sie eine Liste der erforderlichen und optionalen PHP-Erweiterungen, die von SimpleSAMLphp verwendet werden.
Überprüfen Sie, ob Sie jede Erweiterung installiert haben, ausgenommen predis / predis:
Alle installierten Erweiterungen
Wenn erforderliche Komponenten fehlen, lesen Sie dieses Tutorial noch einmal und installieren Sie die fehlenden Komponenten, bevor Sie fortfahren.
Außerdem sehen Sie einen Link, der besagt Integritätsprüfung für Ihre SimpleSAMLphp-Einrichtung.
Klicken Sie auf diesen Link, um eine Liste von auf Ihre Einrichtung angewandten Prüfungen zu erhalten und zu sehen, ob sie erfolgreich waren.
Fahren wir nun mit der Konfiguration einer Authentifizierungsquelle für SimpleSAMLphp fort.
Schritt 4 - Konfigurieren der Authentifizierungsquelle
Nachdem SimpleSAMLphp nun installiert und eingerichtet ist, konfigurieren wir eine Authentifizierungsquelle, damit wir Benutzer authentifizieren können.
Wir verwenden eine MySQL-Datenbank, um eine Liste von Benutzernamen und Passwörtern zu speichern, gegen die eine Authentifizierung erfolgt.
Melden Sie sich zunächst beim Root-Konto von MySQL an:
Sie werden zur Eingabe des Passworts für das Root-Konto von MySQL aufgefordert.
Geben Sie es ein, um fortzufahren.
Erstellen Sie als Nächstes eine Datenbank, die als Authentifizierungsquelle dient.
Wir nennen sie auth.
Sie können Ihre Datenbank auf Wunsch auch anders nennen:
Jetzt erstellen wir einen separaten MySQL-Benutzer, der ausschließlich für unsere auth-Datenbank verwendet wird.
Aus einer Verwaltungs- und Sicherheitsperspektive ist es eine gute Praxis, unifunktionale Datenbanken und Konten zu erstellen.
Wir nennen unseren Benutzer authuser.
Führen Sie den folgenden Befehl aus, um den Benutzer zu erstellen, setzen Sie ein Passwort und gewähren Sie Zugriff auf unsere auth-Datenbank.
Denken Sie daran, ein starkes Passwort für Ihren neuen Datenbankbenutzer zu wählen:
Erstellen Sie nun eine users-Tabelle für Benutzer, die aus zwei Feldern besteht: username und passwort.
Für zusätzliche Sicherheit werden wir die Funktion MySQL AES _ ENCRYPT () nutzen, um die Passwordzeichnefolge zu verschlüsseln, damit wir die Passwörter nicht im Klartext speichern.
Diese Funktion verschlüsselt eine Zeichenfolge und gibt eine binäre Zeichenfolge zurück.
Fügen Sie dann drei Benutzer in die neu erstellte Tabelle ein.
Hier verwenden wir die Funktion AES _ ENCRYPT (), um die Werte für das Passwort-Feld zu verschlüsseln.
Sie müssen eine Zeichenfolge angeben, die als Verschlüsselungsschlüssel verwendet wird.
Stellen Sie sicher, dass Sie diese durch Ihre eigene Zeichenfolge ersetzen, was eine beliebige Zeichenfolge sein kann, solange sie komplex ist.
Verwenden Sie für jeden Benutzer den gleichen Schlüssel und vergessen Sie ihn nicht, damit Sie ihn in Zukunft erneut verwenden können, um zusätzliche Benutzer zu erstellen.
Außerdem verwenden Sie diesen geheimen Schlüssel in der SimpleSAMLphp-Konfiguration, damit Sie die Passwörter entschlüsseln und mit den von Benutzern eingegebenen Passwörtern vergleichen können.
Wir müssen die Berechtigungen leeren, damit die aktuelle Instanz von MySQL die neuesten Änderungen erkennt:
Beenden Sie die MySQL-Eingabeaufforderung durch die Eingabe von:
Damit die Funktionalität des Identitätsanbieters in SimpleSAMLphp aktiviert wird, müssen wir die Datei / var / simplesamlphp / config / config.php bearbeiten.
Es gibt verschiedene Optionen, da sich dieser Leitfaden aber auf die Unterstützung für SAML 2.0 konzentriert, wollen wir die Option eable.saml20-idp aktivieren.
Öffnen Sie dazu / var / simplesamlphp / config / config.php und aktivieren die Unterstützung für SAML 2.0:
Suchen Sie nach diesem Abschnitt der Datei und ersetzen Sie false durch true:
Speichern Sie anschließend die Datei und beenden Sie den Editor.
Nachdem die Funktionalität des Identitätsanbieters nun aktiviert ist, müssen wir das zu verwendende Authentifizierungsmodul angeben.
Da wir über eine Benutzertabelle in einer MySQL-Datenbank verfügen, werden wir das SQL-Authentifizierungsmodul verwenden.
Öffnen Sie die Konfigurationsdatei authsources:
Suchen Sie nach folgendem Block, der auskommentiert ist:
Dieser Code definiert eine Datenbankverbindung und eine Abfrage, mit denen SimpleSAMLphp einen Benutzer in einer Datenbanktabelle namens users finden kann.
Wir müssen die Kommentierung aufheben und die Abfrage so ändern, dass sie mit der Funktion AES _ DECRYPT () nach einem Benutzer in unserer Tabelle sucht.
Wir müssen die Funktion AES _ DECRYPT () mit demselben Schlüssel bereitstellen, den wir in der Abfrage zum Verschlüsseln der Passwörter verwendet haben.
Ändern Sie den Abschnitt der Datei, um die Datenbankverbindung und die Abfrage anzugeben:
Stellen Sie sicher, dass Sie Ihren festgelegten geheimen Schlüssel anstelle von < ^ > your _ secret _ key < ^ > angeben.
Testen wir nun unseren Identitätsanbieter.
Schritt 5 - Testen des Identitätsanbieters mit der SAML 2.0 SP-Demo
Sie können die gerade eingerichtete MySQL-Authentifizierunsquelle testen, indem Sie zur Registerkarte Authentifizierung navigieren und auf den Link Konfigurierte Authentifizierungsquellen testen klicken.
Ihnen wird eine Liste der bereits konfigurierten Authentifizierungsquellen angezeigt.
Die Liste der konfigurierten Authentifizierungsquellen
Klicken Sie auf example-sql; das ist der Anbieter, den Sie im vorherigen Schritt konfiguriert haben.
Eine Eingabeaufforderung zur Eingabe eines Benutzernamens und eines Passworts wird angezeigt.
Geben Sie eine der drei in die MySQL-Benutzertabelle eingefügten Testbenutzer- und Passwortkombinationen ein.
Versuchen Sie es mit user1 und dem Passwort user1pass.
Bei einem erfolgreichen Versuch wird Ihnen die Seite SAML 2.0 SP-Demobeispiel angezeigt:
Die erfolgreiche Demo-Seite
Wenn Sie sich nicht anmelden können und wissen, dass das Passwort korrekt ist, vergewissern Sie sich, dass Sie für die Funktion AES _ ENCRYPT () (als Sie den Benutzer erstellt haben) und die Funktion AES _ DECRYPT () (als Sie nach dem Benutzer gesucht haben) den gleichen Schlüssel verwendet haben.
Jetzt können Sie SimpleSAMLphp mit Ihren eigenen Anwendungen integrieren, indem Sie der SimpleSAMLphp-API-Dokumentation folgen.
Jetzt haben Sie die SimpleSAMLphp-Anwendung in Ihrem Ubuntu 18.04 VPS richtig installiert und konfiguriert.
SimpleSAMLphp ermöglicht durch Design auch eine umfassende Anpassung der Benutzeroberfläche.
Weitere Informationen dazu finden Sie in der Design-Dokumentation.
Installieren von MariaDB unter Ubuntu 18.04
3996
MariaDB ist ein Open-Source-basiertes Datenbank-Managementsystem, das häufig als Alternative für den MySQL-Teil des beliebten LAMP-Stacks (Linux, Apache, MySQL, PHP / Python / Perl) verwendet wird.
Es dient als integrierbarer Ersatz für MySQL.
Die kurze Version dieses Installationsleitfadens umfasst die folgenden drei Schritte:
Aktualisieren Ihres Paketindex mit apt
Installieren Sie mit apt das Paket mariadb-server.
Das Paket bezieht auch verwandte Tools ein, um mit MariaDB zu interagieren.
Führen Sie das enthaltene Sicherheitsskript mysql _ secure _ installation aus, um den Zugriff auf den Server zu einschränken.
In diesem Tutorial erfahren Sie, wie Sie MariaDB auf einem Ubuntu 18.04-Server installieren und überprüfen können, ob das System ausgeführt wird und über eine sichere Erstkonfiguration verfügt.
Um dieser Anleitung zu folgen, benötigen Sie:
Einen Ubuntu 18.04-Server, der gemäß diesem Leitfaden zur Ersteinrichtung des Servers eingerichtet wurde, einschließlich eines Benutzers ohne Rootberechtigung, aber mit sudo-Berechtigungen, und einer Firewall.
Schritt 1 - Installieren von MariaDB
Bei Ubuntu 18.04 ist MariaDB 10.1 standardmäßig in den APT-Paket-Repositorys enthalten.
Um MariaDB zu installieren, aktualisieren Sie mit apt den Paketindex auf Ihrem Server:
Installieren Sie anschließend das Paket:
Diese Befehle installieren MariaDB, fordern Sie aber nicht dazu auf, ein Passwort festzulegen oder andere Konfigurationsänderungen vorzunehmen.
Da die standardmäßige Konfiguration Ihre Installation von MariaDB unsicher ist, verwenden wir ein Skript, das das Paket mariadb-server zur Einschränkung des Zugangs zum Server und zur Entfernung unbenutzter Konten bereitstellt.
Schritt 2 - Konfigurieren von MariaDB
Bei neuen MariaDB-Installationen besteht der nächste Schritt aus der Ausführung des enthaltenen Sicherheitsskripts.
Dieses Skript ändert einige der weniger sicheren Standardoptionen.
Wir verwenden es, um ferngesteuerte Root-Anmeldungen zu blockieren und nicht verwendete Datenbankbenutzer zu entfernen.
Führen Sie das Sicherheitsskript aus:
Dieses Skript führt Sie durch eine Reihe von Aufforderungen, in denen Sie verschiedene Änderungen an den Sicherheitseinstellungen Ihrer MariaDB-Einrichtung vornehmen können.
Bei der ersten Eingabeaufforderung werden Sie gebeten, das aktuelle Passwort der Datenbank-Root einzugeben.
Da wir noch kein Passwort eingerichtet haben, drücken Sie die Eingabetaste, um "none" (keines) anzugeben.
Bei der nächsten Eingabeaufforderung werden Sie gefragt, ob Sie ein Datenbank-Root Passwort einrichten möchten.
Geben Sie N ein und drücken Sie dann ENTER.
Bei Ubuntu ist das Root-Konto für MariaDB eng mit der automatisierten Systemwartung verbunden. Daher sollten wir die für dieses Konto konfigurierten Authentifizierungsmethoden nicht ändern.
Sonst wäre es möglich, dass ein Paket-Update das Datenbanksystem durch Entfernen von Zugriff auf das Administratorkonto unterbricht.
Danach behandeln wir die Frage, wie wir optional ein zusätzliches Administratorkonto für Passwortzugriff einrichten können, wenn die Socket-Authentifizierung für Ihren Anwendungsfall nicht geeignet ist.
Dannach können Sie Y und dann ENTER drücken, um die Standardeinstellungen für alle nachfolgenden Fragen zu akzeptieren.
Damit werden einige anonyme Benutzer und die Testdatenbank entfernt, ferngesteuerte Root-Logins deaktiviert und diese neuen Regeln geladen, damit MySQL die Änderungen, die Sie gerade vorgenommen haben, unverzüglich anwendet.
Schritt 3 - (Optional) Anpassen der Benutzerauthentifizierung und -berechtigungen
Bei Ubuntu mit MariaDB 10.1 ist der Root-MariaDB-Benutzer standardmäßig so eingerichtet, dass die Authentifizierung mit dem unix _ socket-Plugin und nicht mit einem Passwort vorgenommen wird.
Dadurch entsteht in vielen Fällen mehr Sicherheit und Benutzerfreundlichkeit, können Aufgaben aber auch komplizierter werden, wenn Sie einem externen Programm (z. B. phpMyAdmin) administrativen Zugriff erteilen müssen.
Da der Server das Root-Konto für Aufgaben wie Protokollrotation sowie Starten und Anhalten des Servers nutzt, ist es am besten, die Authentifizierungsdaten des Root-Kontos nicht zu ändern.
Das Ändern von Anmeldeinformationen in der Konfigurationsdatei / etc / mysql / debian.cnf kann zunächst funktionieren; Paket-Updates können diese Änderungen jedoch ggf. überschreiben.
Statt das Root-Konto zu ändern, empfehlen Paketverwalter die Erstellung eines separaten Administratorkontos für passwortbasierten Zugriff.
Dazu erstellen wir ein neues Konto namens admin mit den gleichen Fähigkeiten wie beim Root-Konto, aber konfiguriert mit Passwortauthentifizierung.
Öffnen Sie dazu die MariaDB-Eingabeaufforderung in Ihrem Terminal:
Jetzt erstellen wir einen neuen Benutzer mit Root-Berechtigungen und passwortbasiertem Zugriff.
Ändern Sie den Benutzernamen und das Passwort entsprechend Ihren Einstellungen:
Bestätigen Sie die Berechtigungen, um sicherzustellen, dass sie in der aktuellen Sitzung gespeichert und verfügbar sind:
Beenden Sie anschließend die MariaDB Shell:
Lassen Sie uns die MariaDB-Installation abschließend testen:
Schritt 4 - Testen von MariaDB
Bei Installation aus den Standard-Repositorys sollte die Ausführung von MariaDB automatisch gestartet werden.
Überprüfen Sie zum Testen den Status von MariaDB.
Sie erhalten eine Ausgabe, die der folgenden ähnelt:
Wenn MariaDB nicht ausgeführt wird, können Sie das System mit dem Befehl sudo systemctl start mariadb starten.
Für einen zusätzlichen Test können Sie mit dem Tool mysqladmin eine Verbindung zur Datenbank herstellen. Das ist ein Client, der Sie administrative Befehle ausführen lässt.
Der Befehl beinhaltet zum Beispiel die Anweisung, als Root eine Verbindung mit MariaDB herzustellen und mit dem Unix-Socket die Version zurückzugeben:
Sie sollten eine Ausgabe erhalten, die der folgenden ähnelt:
Wenn Sie einen separaten administrativen Benutzer mit Passwortauthentifizierung konfiguriert haben, können Sie die gleiche Aufgabe erledigen, indem Sie Folgendes eingeben:
Das bedeutet, dass MariaDB erfolgreich ausgeführt wird und sich Ihr Benutzer erfolgreich authentifizieren kann.
In diesem Leitfaden haben Sie MariaDB installiert, um als SQL-Server zu fungieren.
Im Rahmen der Installation haben Sie außerdem den Server gesichert.
Optional haben Sie auch einen separaten administrativen Benutzer mit Passwortauthentifizierung erstellt.
Nachdem Sie nun über einen laufenden und sicheren MariaDB-Server verfügen, finden Sie hier einige Beispiele für nächste Schritte, die Ihnen das Arbeiten mit dem Server erlauben:
Import und Export von Datenbanken
Sie können MariaDB auch in einen größeren Anwendungsstapel einfügen:
Installieren von Linux, Nginx, MariaDB, PHP (LEMP-Stack) unter Ubuntu 18.04
Installieren von Software in Kubernetes-Clustern mit dem Helm 3-Paketmanager
4012
Helm ist ein Paketmanager für Kubernetes, der Entwicklern und Bedienern die Konfiguration und Bereitstellung von Anwendungen in Kubernetes-Clustern erleichtert.
In diesem Tutorial richten Sie Helm 3 ein und verwenden die Lösung zum Installieren, Neukonfigurieren, Zurücksetzen und Löschen einer Instanz der Kubernetes Dashboard-Anwendung.
Das Dashboard ist eine offizielle webbasierte Kubernetes-Oberfläche.
Eine konzeptuelle Übersicht über Helm und das Verpackungs-Ökosystem finden Sie in unserem Artikel Eine Einführung zu Helm.
Für dieses Tutorial benötigen Sie Folgendes:
Einen Kubernetes-Cluster mit aktivierter rollenbasierter Zugriffskontrolle (RBAC).
Helm 3.1 unterstützt Cluster von Versionen 1.14 bis 1.17.
Weitere Informationen finden Sie auf der Helm Release-Seite.
Das auf Ihrem lokalen Computer installierte Befehlszeilentool kubectl, das zur Verbindung mit Ihrem Cluster konfiguriert ist.
Weitere Informationen zur Installation von kubectl finden Sie in der offiziellen Dokumentation.
Sie können Ihre Verbindung mit dem folgenden Befehl testen:
Wenn Sie keine Fehler sehen, sind Sie mit dem Cluster verbunden.
Wenn Sie mit kubectl mehrere Cluster aufrufen, stellen Sie sicher, dass Sie den richtigen Cluster-Kontext ausgewählt haben:
In diesem Beispiel zeigt das Sternchen (< ^ > * < ^ >) an, dass wir mit dem Cluster do-fra1-helm3-example verbunden sind.
Um den Cluster zu wechseln, führen Sie Folgendes aus:
Wenn Sie mit dem richtigen Cluster verbunden sind, fahren Sie mit Schritt 1 zum Installieren von Helm fort.
Schritt 1 - Installieren von Helm
Installieren Sie zunächst das helm-Befehlszeilendienstprogramm auf Ihrem lokalen Computer.
Helm bietet ein Skript, das den Installationsprozess unter MacOS, Windows oder Linux verwaltet.
Wechseln Sie in ein beschreibbares Verzeichnis und laden Sie das Skript aus dem GitHub-Repository von Helm herunter:
Machen Sie das Skript mit chmod ausführbar:
Sie können Ihren bevorzugten Texteditor verwenden, um das Skript zu öffnen und zu überprüfen, ob es sicher ist.
Wenn Sie zufrieden damit sind, führen Sie es aus:
Möglicherweise werden Sie zur Eingabe Ihres Passworts aufgefordert.
Geben Sie es ein und drücken Sie die Eingabetaste, um fortzufahren.
Die Ausgabe sieht in etwa folgendermaßen aus:
Nachdem Helm nun installiert ist, können Sie Helm zur Installation Ihres ersten Chart verwenden.
Schritt 2 - Installieren eines Helm Charts
Helm-Softwarepakete werden als Charts bezeichnet.
Es gibt ein kuratiertes Chart-Repository namens stable, das größtenteils aus gängigen Charts besteht, die Sie in ihrem GitHub-Repository anzeigen können.
Helm ist dafür nicht vorkonfiguriert, sodass Sie die Datei manuell hinzufügen müssen. Dann installieren Sie zum Beispiel das Kubernetes Dashboard.
Fügen Sie das Repository stable hinzu, indem Sie Folgendes ausführen:
Verwenden Sie dann helm, um das Paket kubernetes-dashboard aus dem Repository stable zu installieren:
Mit dem Parameter --set können Sie Chart-Variablen anpassen, die das Chart verfügbar machen, sodass Sie dessen Konfiguration anzupassen können.
Hier setzen Sie die Variable rbac.clusterAdminRole auf true, um dem Kubernetes Dashboard Zugriff auf den gesamten Cluster zu gewähren.
Die Ausgabe sieht ungefähr so aus:
Beachten Sie die Zeile NAME, die in der obigen Beispielausgabe hervorgehoben ist.
In diesem Fall haben Sie den Namen dashboard-demo angegeben.
Das ist der Name der Version.
Eine Helm-Version ist eine einzelne Bereitstellung eines Charts mit einer bestimmten Konfiguration.
Sie können verschiedene Versionen des gleichen Charts bereitstellen, jedes mit einer eigenen Konfiguration.
Sie können alle Versionen im Cluster auflisten:
Sie können nun kubectl verwenden, um sich zu vergewissern, dass ein neuer Dienst im Cluster bereitgestellt wurde:
Beachten Sie, dass der der Version entsprechende Dienstname standardmäßig eine Kombination aus der Helm-Version und dem Chart-Namen ist.
Nachdem Sie nun die Anwendung bereitgestellt haben, verwenden Sie Helm, um dessen Konfiguration zu ändern und die Bereitstellung zu aktualisieren.
Schritt 3 - Aktualisieren einer Version
Der Befehl helm upgrade kann dazu genutzt werden, eine Version mit einem neuen oder aktualisierten Chart zu aktualisieren oder deren Konfigurationsoptionen (Variablen) zu aktualisieren.
Sie werden eine einfache Änderung an der Version dashboard-demo vornehmen, um den Aktualisierungs- und Rollback-Prozess zu testen: Sie ändern den Namen des Dashboard-Dienstes in nur kubernetes-dashboard (anstelle von dashboard-demo-kubernetes-dashboard).
Das Chart kubernetes-dashboard bietet eine Konfigurationsoption namens fullnameOverride, um den Dienstnamen zu kontrollieren.
Um die Version zu umbenennen, führen Sie helm upgrade mit diesem Optionssatz aus:
Durch Übergeben des Arguments --reuse-values stellen Sie sicher, dass die zuvor eingerichteten Chart-Variablen bei der Aktualisierung nicht zurückgesetzt werden.
Sie sehen eine Ausgabe, die dem anfänglichen Schritt helm install ähnelt.
Überprüfen Sie, ob Ihre Kubernetes-Dienste die aktualisierten Werte widerspiegeln:
Die Ausgabe sieht ungefähr wie folgt aus:
Beachten Sie, dass der Dienstname auf den neuen Wert aktualisiert wurde.
< $> note Anmerkung: Zu diesem Zeitpunkt möchten Sie vielleicht das Kubernetes Dashboard tatsächlich in Ihren Browser laden und überprüfen. Dazu führen Sie zunächst den folgenden Befehl aus:
Dadurch wird ein Proxy erstellt, mit dem Sie von Ihrem lokalen Computer auf Ressourcen im Remotecluster zugreifen können.
Basierend auf den vorherigen Anweisungen heißt Ihr Dashboard-Dienst kubernetes-dashboard und wird im standardmäßigen Namespace ausgeführt.
Sie können nun mit der folgenden URL auf das Dashboard zugreifen:
Anweisungen zur tatsächlichen Verwendung des Dashboards sind nicht Bestandteil dieses Tutorials. Weitere Informationen finden Sie jedoch in den offiziellen Kubernetes Dashboard-Dokumenten.
Als Nächstes sehen Sie sich die Möglichkeit von Helm an, Versionen zurückzusetzen und zu löschen.
Schritt 4 - Zurücksetzen und Löschen einer Version
Wenn Sie die Version < ^ > dashboard-demo < ^ > im vorherigen Schritt aktualisiert haben, haben Sie eine zweite Revision der Version erstellt.
Helm behält alle Details der vorherigen Versionen bei, falls Sie eine frühere Konfiguration oder ein früheres Chart wiederherstellen müssen.
Verwenden Sie helm list, um die Version erneut zu prüfen:
Die Spalte REVISION teilt Ihnen mit, dass dies nun die zweite Revision ist.
Verwenden Sie helm rollback, um die erste Revision wiederherzustellen:
Sie sollten die folgende Ausgabe sehen, was bedeutet, dass das Zurücksetzen erfolgreich war:
Wenn Sie kubectl get services an dieser Stelle erneut ausführen, werden Sie feststellen, dass der Dienstname wieder seinen vorherigen Wert aufweist.
Helm hat die Anwendung mit der Konfiguration von Revision 1 neu bereitgestellt.
Helm-Versionen können mit dem Befehl helm delete gelöscht werden:
Sie können versuchen, Helm-Versionen aufzulisten:
Sie sehen, dass es keine gibt:
Jetzt ist die Version wirklich gelöscht und Sie können den Versionsnamen wiederverwenden.
In diesem Tutorial haben Sie das helm-Befehlszeilentool installiert und sich mit dem Installieren, Aktualisieren, Zurücksetzen und Löschen von Helm-Charts und -Versionen durch Verwalten des Chart kubernetes-dashboard vertraut gemacht.
Weitere Informationen zu Helm und Helm-Charts finden Sie in der offiziellen Helm-Dokumentation.
Ausführen mehrerer PHP-Versionen auf einem Server unter Verwendung von Apache und PHP-FPM unter Ubuntu 18.04
4003
Der Apache-Webserver verwendet virtuelle Hosts zur Verwaltung mehrerer Domänen auf einer einzigen Instanz.
In ähnlicher Weise verwendet PHP-FPM einen Daemon, um mehrere PHP-Versionen auf einer einzigen Instanz zu verwalten.
Zusammen können Sie Apache und PHP-FPM verwenden, um mehrere PHP-Webanwendungen zu hosten, von denen jede eine andere Version von PHP verwendet, alle auf dem gleichen Server und alle zur gleichen Zeit.
Dies ist nützlich, da verschiedene Anwendungen möglicherweise verschiedene Versionen von PHP erfordern, aber einige Server-Stacks, wie ein herkömmlich konfigurierter LAMP-Stack, nur einen verwalten können.
Die Kombination von Apache mit PHP-FPM ist auch eine kosteneffizientere Lösung als das Hosting jeder Anwendung auf ihrer eigenen Instanz.
PHP-FPM bietet auch Konfigurationsoptionen für stderr- und stdout-Protokollierung, Notfall-Neustarts und adaptives Prozess-Spawning, was bei stark ausgelasteten Sites nützlich ist.
Tatsächlich ist der Einsatz von Apache mit PHP-FPM einer der besten Stacks für das Hosting von PHP-Anwendungen, insbesondere, wenn es um die Leistung geht.
In diesem Tutorial richten Sie zwei PHP-Sites auf einer einzigen Instanz ein.
Jede Site verwendet ihre eigene Domäne und jede Domäne stellt ihre eigene PHP-Version bereit.
Die erste, < ^ > site1.your _ domain < ^ >, stellt PHP 7.0 bereit.
Die zweite, < ^ > site2.your _ domain < ^ >, stellt PHP 7.2 bereit.
Einen Ubuntu 18.04-Server mit mindestens 1 GB RAM, der nach der Ersteinrichtung des Servers unter Ubuntu 18.04 eingerichtet wurde, einschließlich eines sudo-Benutzers ohne Rootberechtigung und einer Firewall.
Einen Apache-Webserver, der nach Installieren des Apache-Webservers unter Ubuntu 18.04 eingerichtet und konfiguriert wurde.
Einen Domänennamen, der so konfiguriert ist, dass er auf Ihren Ubuntu 18.04-Server verweist.
Sie können erfahren, wie Sie Domänen auf DigitalOcean-Droplets verweisen, indem Sie Verweisen auf DigitalOcean-Nameserver von gängigen Domain-Registrierungsstellen durcharbeiten.
Für die Zwecke dieses Tutorials werden wir zwei Unterdomänen verwenden, die jeweils mit einem A-Datensatz in unseren DNS-Einstellungen angegeben sind: < ^ > site1.your _ domain < ^ > und < ^ > site2.your _ domain < ^ >.
Schritt 1 - Installieren von PHP Versionen 7.0 und 7.2 mit PHP-FPM
Nachdem die Voraussetzungen erfüllt sind, installieren Sie nun PHP-Versionen 7.0 und 7.2 sowie PHP-FPM als auch mehrere zusätzliche Erweiterungen.
Dazu müssen Sie jedoch zunächst das Ondrej PHP-Repository zu Ihrem System hinzufügen.
Führen Sie den Befehl apt-get aus, um software-properties-common zu installieren:
Das Paket software-properties-common bietet das Befehlszeilenprogramm apt-add-repository, das Sie zum Hinzufügen des PPA-Repository (Persönliches Paketarchiv) ondrej / php verwenden.
Fügen Sie nun das Repository ondrej / php zu Ihrem System hinzu.
Das PPA ondrej / php hat aktuellere Versionen von PHP als die offiziellen Repositories von Ubuntu und ermöglicht Ihnen auch, mehrere PHP-Versionen im gleichen System zu installieren:
Aktualisieren Sie das Repository:
Installieren Sie als Nächstes php7.0, php7.0-fpm, php7.0-mysql, libapache2-mod-php7.0, und libapache2-mod-fcgid mit den folgenden Befehlen:
php7.0 ist ein Metapaket zur Ausführung von PHP-Anwendungen.
php7.0-fpm bietet den Fast Process Manager-Interpreter, der als Daemon ausgeführt wird und Fast / CGI-Anfragen empfängt.
php7.0-mysql verbindet PHP mit der MySQL-Datenbank.
libapahce2-mod-php7.0 stellt das PHP-Modul für den Apache-Webserver bereit.
libapache2-mod-fcgid enthält eine mod _ fcgid, die eine Reihe von CGI-Programminstanzen startet, um gleichzeitige Anfragen zu bearbeiten.
Wiederholen Sie den Vorgang für PHP-Version 7.2.
Installieren Sie php7.2, php7.2-fpm, php7.2-mysql und libapache2-mod-php7.2:
Starten Sie nach Installation beider PHP-Versionen den Dienst php7.0-fpm:
Überprüfen Sie als Nächstes den Status des php7.0-fpm-Dienstes:
Wiederholen Sie diesen Vorgang nun zum Starten des php7.2-fpm-Dienstes:
Und überprüfen Sie den Status des php7.2-fpm-Dienstes:
Abschließend müssen Sie mehrere Module aktivieren, damit Ihr Apache2-Dienst mit mehreren PHP-Versionen arbeiten kann:
actions wird für die Ausführung von CGI-Skripten verwendet, die auf Medientyp oder Anfragemethode basieren.
fcgid ist eine hochleistungsfähige Alternative zu mod _ cgi, die eine ausreichende Anzahl von Instanzen des CGI-Programms startet, um gleichzeitige Anfragen zu bearbeiten.
alias stellt die Abbildung verschiedener Teile des Host-Dateisystems im Dokumentenbaum bereit und dient der URL-Umleitung.
proxy _ fcgi ermöglicht Apache die Weiterleitung von Anfragen an PHP-FPM.
Starten Sie nun den Apache-Dienst neu, um Ihre Änderungen zu übernehmen:
Zu diesem Zeitpunkt haben Sie zwei PHP-Versionen auf Ihrem Server installiert.
Als Nächstes erstellen Sie eine Verzeichnisstruktur für jede Website, die Sie bereitstellen möchten.
Schritt 2 - Erstellen von Verzeichnisstrukturen für beide Websites
In diesem Abschnitt erstellen Sie für beide Websites ein Dokumentenverzeichnis und eine Indexseite.
Erstellen Sie zunächst Dokument-Stammverzeichnisse sowohl für < ^ > site1.your _ domain < ^ > als auch für < ^ > site2.your _ domain < ^ >:
Standardmäßig wird der Apache-Webserver als Benutzer www-data und als Gruppe www-data ausgeführt.
Um sicherzustellen, dass Sie über die korrekte Eigentümerschaften und Berechtigungen für die Stammverzeichnisse Ihrer Website verfügen, führen Sie die folgenden Befehle aus:
Erstellen Sie als Nächstes in jedem Stammverzeichnis Ihrer Website eine Datei info.php.
Dadurch werden die PHP-Version jeder Website angezeigt.
Beginnen Sie mit < ^ > site1 < ^ >:
Fügen Sie die folgende Zeile hinzu:
Kopieren Sie nun die von Ihnen erstellte Datei info.php nach < ^ > site2 < ^ >:
Ihr Webserver sollte nun über die Stammverzeichnisse der Dokumente verfügen, die jede Site benötigt, um Besuchern Daten zur Verfügung zu stellen.
Als Nächstes konfigurieren Sie Ihren Apache-Webserver, um mit zwei verschiedenen PHP-Versionen zu arbeiten.
Schritt 3 - Konfigurieren von Apache für beide Websites
In diesem Abschnitt erstellen Sie zwei Konfigurationsdateien für virtuelle Hosts.
Dadurch können Ihre beiden Websites gleichzeitig mit zwei verschiedenen PHP-Versionen arbeiten.
Damit Apache diesen Inhalt bereitstellen kann, ist es erforderlich, eine virtuelle Host-Datei mit den richtigen Anweisungen zu erstellen.
Anstatt die Standardkonfigurationsdatei unter / etc / apache2 / sites-available / 000-default.conf zu ändern, erstellen Sie zwei neue in dem Verzeichnis / etc / apache2 / sites-available.
Erstellen Sie zunächst eine neue Konfigurationsdatei für einen virtuellen Host für die Website < ^ > site1.your _ domain < ^ >.
Hier weisen Sie Apache an, Inhalte mit php7.0 zu rendern:
Fügen Sie folgenden Inhalt hinzu:
Stellen Sie sicher, dass der Website-Verzeichnispfad, der Servername und die PHP-Version mit Ihrer Einrichtung übereinstimmen:
In dieser Datei haben Sie die DocumentRoot auf Ihr neues Verzeichnis und ServerAdmin auf eine E-Mail aktualisiert, auf die der Administrator der Website your _ domain zugreifen kann.
Weiterhin haben Sue ServerName aktualisiert, der die Basisdomäne für diese Konfiguration des virtuellen Hosts festlegt, und Sie haben eine Anweisung SetHandler hinzugefügt, um PHP als fastCGI-Prozessserver auszuführen.
Erstellen Sie als Nächstes eine neue Konfigurationsdatei des virtuellen Hosts für die Website < ^ > site2.your _ domain < ^ >.
Sie geben diese Subdomäne für die Bereitstellung von php7.2 an:
Stellen Sie auch hier sicher, dass der Website-Verzeichnispfad, der Servername und die PHP-Version mit Ihrer Einrichtung übereinstimmen:
Überprüfen Sie anschließend die Apache-Konfigurationsdatei auf etwaige Syntaxfehler:
Aktivieren Sie dann beide Konfigurationsdateien des virtuellen Hosts:
Deaktivieren Sie nun die Standardseite, da Sie sie nicht benötigen:
Starten Sie schließlich den Apache-Dienst neu, um Ihre Änderungen zu übernehmen:
Nachdem Sie Apache nun konfiguriert haben, um jede Site zu bedienen, werden Sie sie testen, um sicherzustellen, dass die richtigen PHP-Versionen ausgeführt werden.
Schritt 4 - Testen beider Websites
Zu diesem Zeitpunkt haben Sie zwei Websites konfiguriert, um zwei verschiedene PHP-Versionen auszuführen.
Testen Sie nun die Ergebnisse.
Öffnen Sie Ihren Webbrowser und besuchen Sie beide Sites http: / / < ^ > site1.your _ domain < ^ > und http: / / < ^ > site2.your _ domain < ^ >.
Sie sehen zwei Seiten, die wie folgt aussehen:
PHP 7.0 Infoseite PHP 7.2 Infoseite
Beachten Sie die Titel.
Die erste Seite zeigt an, dass < ^ > site1.your _ domain < ^ > PHP-Version 7.0 bereitstellt.
Die zweite zeigt an, dass < ^ > site2.your _ domain < ^ > PHP-Version 7.2 bereitstellt.
Nachdem Sie nun Ihre Sites getestet haben, entfernen Sie die Dateien info.php.
Da sie sensible Informationen über Ihren Server enthalten und für unberechtigte Benutzer zugänglich sind, stellen sie eine Sicherheitsbedrohung dar.
Führen Sie zur Entfernung beider Dateien die folgenden Befehle aus:
Sie haben nun einen einzigen Ubuntu 18.04-Server, der zwei Websites mit zwei verschiedenen PHP-Versionen verwaltet.
PHP-FPM ist jedoch nicht auf diese eine Anwendung beschränkt.
Sie haben nun virtuelle Hosts und PHP-FPM kombiniert, um mehrere Websites und mehrere PHP-Versionen auf einem einzigen Server bereitzustellen.
Die einzige praktische Beschränkung der Anzahl der PHP-Sites und PHP-Versionen, die Ihr Apache-Dienst verarbeiten kann, ist die Verarbeitungsleistung Ihrer Instanz.
Von hier aus können Sie die fortgeschritteneren Funktionen von PHP-FPM erkunden, wie den adaptiven Spawning-Prozess oder wie sdtout und stderr protokolliert werden können.
Alternativ können Sie jetzt auch Ihre Websites sichern.
Zu diesem Zweck können Sie unserem Tutorial zum Sichern Ihrer Sites mit kostenlosen TLS / SSL-Zertifikaten von Let "s Encrypt folgen.
Einrichten einer Firewall mit firewalld unter CentOS
4018
firewalld ist eine für viele Linux-Distributionen verfügbare Firewall-Verwaltungssoftware, die als Frontend für die kernelinternen nftables- oder iptables-Paketfiltersysteme von Linux dient.
In diesem Leitfaden zeigen wir Ihnen, wie Sie eine Firewall für Ihren CentOS 8-Server einrichten, und behandeln die Grundlagen der Verwaltung der Firewall mit dem Verwaltungstool firewall-cmd.
Um dieses Tutorial zu absolvieren, benötigen Sie einen Server, auf dem CentOS 8 ausgeführt wird. Wir gehen davon aus, dass Sie als Benutzer ohne Rootberechtigung, aber mit sudo-Berechtigungen bei diesem Server angemeldet sind.
Folgen Sie zur Einrichtung unserem Leitfaden zur Ersteinrichtung des Servers für CentOS 8.
Grundkonzepte in firewalld
Bevor wir damit beginnen, über die tatsächliche Verwendung des Dienstprogramms firewall-cmd zur Verwaltung Ihrer Firewall-Konfiguration zu sprechen, sollten wir uns mit einigen Konzepten vertraut machen, die das Tool vorstellt.
Zonen
Der Daemon firewalld verwaltet Gruppen mithilfe von Entitäten, die Zonen genannt werden.
Zonen sind Regelwerke die vorgeben, welcher Datenverkehr abhängig von der Vertrauensstufe, die Sie im Netzwerk haben, zugelassen werden soll.
Netzwerkschnittstellen werden einer Zone zugeordnet, um das Verhalten zu bestimmen, das die Firewall zulassen soll.
Für Computer, die häufig zwischen Netzwerken hin- und herwechseln (wie Laptops), bietet diese Art der Flexibilität eine gute Methode, Ihre Regeln je nach Umgebung zu ändern.
Möglicherweise haben Sie strenge Regeln, die den meisten Datenverkehr verbieten, wenn Sie in einem öffentlichen WLAN-Netzwerk arbeiten, während Sie entspanntere Einschränkungen zulassen, wenn Sie mit Ihrem Heimnetzwerk verbunden sind.
Für einen Server sind diese Zonen oft nicht so wichtig, da sich die Netzwerkumgebung selten, wenn überhaupt, ändert.
Unabhängig davon, wie dynamisch Ihre Netzwerkumgebung auch sein mag, ist es dennoch nützlich, sich mit der allgemeinen Idee hinter jeder der vordefinierten Zonen für firewalld vertraut zu machen.
Die vordefinierten Zonen innerhalb von firewalld sind in der Reihenfolge von am wenigsten vertrauenswürdig bis am meisten vertrauenswürdig:
drop: die niedrigste Vertrauensstufe.
Alle eingehenden Verbindungen werden ohne Antwort verworfen und nur ausgehende Verbindungen sind möglich.
block: Ähnlich wie oben, aber statt Verbindungen einfach zu vewerfen, werden eingehende Anfragen mit einer Nachricht icmp-host-prohibited oder icmp6-adm-prohibited zurückgewiesen.
public: Repräsentiert öffentliche, nicht vertrauenswürdige Netzwerke.
Sie vertrauen anderen Computern nicht, können aber von Fall zu Fall ausgewählte eingehende Verbindungen zulassen.
external: Externe Netzwerke für den Fall, dass Sie die Firewall als Gateway verwenden.
Es ist für NAT-Maskierung konfiguriert, sodass Ihr internes Netzwerk privat, aber erreichbar bleibt.
internal: Die andere Seite der externen Zone, die für den internen Teil eines Gateway verwendet wird.
Die Computer sind recht vertrauenswürdig und einige zusätzliche Dienste sind verfügbar.
dmz: Wird für Computer verwendet, die sich in einer DMZ befinden (isolierte Computer, die keinen Zugriff auf den Rest Ihres Netzwerks haben).
Es sind nur bestimmte eingehende Verbindungen erlaubt.
work: Wird für Arbeitscomputer verwendet.
Vertrauen Sie den meisten Computern im Netzwerk.
Einige weitere Dienste können zugelassen sein.
home: Eine private Umgebung.
Im Allgemeinen setzt dies voraus, dass Sie den meisten anderen Computern vertrauen und dass einige weitere Dienste akzeptiert werden.
trusted: Vertrauen Sie allen Rechnern im Netzwerk.
Die offenste der verfügbaren Optionen. Sie sollte sparsam eingesetzt werden.
Wir können zur Verwendung der Firewall Regeln erstellen und die Eigenschaften unserer Zonen ändern und dann unsere Netzwerkschnittstellen den am besten geeigneten Zonen zuweisen.
Regeldauerhaftigkeit
In firewalld können Regeln auf den aktuellen Laufzeit-Regelsatz angewendet oder dauerhaft festgelegt werden.
Wenn eine Regel hinzugefügt oder geändert wird, wird standardmäßig nur die aktuell laufende Firewall geändert.
Nach dem nächsten Neustart - oder Neuladen des Dienstes firewalld - bleiben nur die dauerhaften Regeln erhalten.
Die meisten firewall-cmd-Operationen können mit einem Flag --permanent gekennzeichnet werden, um anzuzeigen, dass die Änderungen auf die dauerhafte Konfiguration angewendet werden sollen.
Zusätzlich kann die aktuell laufende Firewall mit dem Befehl firewall-cmd --runtime-to-permanent in der dauerhaften Konfiguration gespeichert werden.
Diese Trennung von Laufzeit und permanenter Konfiguration bedeutet, dass Sie die Regeln in Ihrer aktiven Firewall gefahrlos testen und bei Problemen erneut laden können, um neu zu beginnen.
Installieren und Aktivieren von firewalld
firewalld ist standardmäßig auf einigen Linux-Distributionen installiert, darunter viele Images von CentOS 8. Es kann jedoch erforderlich sein, dass Sie firewalld selbst installieren müssen:
Nach der Installation von firewalld können Sie den Dienst aktivieren und Ihren Server neu starten.
Denken Sie daran, dass die Aktivierung von firewalld dazu führt, dass der Dienst beim Systemstart gestartet wird.
Es empfiehlt sich, Ihre Firewall-Regeln zu erstellen und bei dieser Gelegenheit zu testen, bevor Sie dieses Verhalten konfigurieren, um mögliche Probleme zu vermeiden.
Wenn der Server neu startet, sollte Ihre Firewall hochgefahren werden, Ihre Netzwerkschnittstellen sollten in die von Ihnen konfigurierten Zonen gelegt werden (oder auf die konfigurierte Standardzone zurückgreifen), und alle mit der oder den Zonen verknüpften Regeln werden auf die verknüpften Schnittstellen angewendet.
Wir können die Ausführung und Erreichbarkeit des Dienstes prüfen, indem wir eingeben:
Dies zeigt an, dass unsere Firewall mit der Standardkonfiguration einsatzbereit und erreichbar ist.
Vertrautmachen mit den aktuellen Firewall-Regeln
Bevor wir mit den Modifikationen beginnen, sollten wir uns mit der Standardumgebung und den Regeln von firewalld vertraut machen.
Erkunden der Standardeinstellungen
Wir können sehen, welche Zone derzeit als Standard ausgewählt ist, indem wir eingeben:
Da wir firewalld keine Befehle zum Abweichen von der Standardzone gegeben haben und keine unserer Schnittstellen so konfiguriert ist, dass sie sich an eine andere Zone bindet, wird diese Zone auch die einzige active Zone sein (die Zone, die den Verkehr für unsere Schnittstellen steuert).
Wir können dies überprüfen, indem wir eingeben:
Hier können wir sehen, dass unser Beispielserver zwei Netzwerkschnittstellen hat, die von der Firewall kontrolliert werden (eth0 und eth1).
Beide werden derzeit gemäß den für die Zone public definierten Regeln verwaltet.
Woher wissen wir aber, welche Regeln mit der Zone public verbunden sind?
Wir können die Konfiguration der Standardzone ausgeben, indem wir eingeben:
Aus der Ausgabe können wir erkennen, dass diese Zone sowohl die Standardzone als auch die aktive Zone ist und dass die Schnittstellen eth0 und eth1 mit dieser Zone verbunden sind (all dies wussten wir bereits aus unseren früheren Anfragen).
Wir können jedoch auch erkennen, dass diese Zone den Verkehr für einen DHCP-Client (für die Zuweisung von IP-Adressen), für SSH (zur Fernverwaltung) und für Cockpit (eine webbasierte Konsole) zulässt.
Erkunden alternativer Zonen
Jetzt haben wir eine gute Vorstellung von der Konfiguration für die Standardzone und die aktive Zone.
Wir können auch Informationen über andere Zonen erhalten.
Um eine Liste der verfügbaren Zonen zu erhalten, geben Sie folgendes ein:
Wir können die mit einer Zone verbundene spezifische Konfiguration sehen, indem wir den Parameter --zone = in unseren Befehl --list-all aufnehmen:
Sie können alle Zonendefinitionen ausgeben, indem Sie die Option --list-all-zones verwenden.
Wahrscheinlich werden Sie die Ausgabe zur einfacheren Anzeige in einen Pager leiten wollen:
Als Nächstes werden wir uns mit der Zuweisung von Zonen zu Netzwerkschnittstellen befassen.
Auswählen von Zonen für Ihre Schnittstellen
Wenn Sie Ihre Netzwerkschnittstellen nicht anders konfiguriert haben, wird jede Schnittstelle beim Start der Firewall in die Standardzone gesetzt.
Ändern der Zone einer Schnittstelle
Sie können eine Schnittstelle während einer Sitzung zwischen Zonen verschieben, indem Sie den Parameter --zone = in Kombination mit dem Parameter --change-interface = verwenden.
Wie bei allen Befehlen, die die Firewall verändern, müssen Sie sudo verwenden.
Wir können zum Beispiel unsere eth0-Schnittstelle in die Zone home verschieben, indem wir folgendes eingeben:
< $> note Anmerkung: Wann immer Sie eine Schnittstelle in eine neue Zone verschieben, sollten Sie sich bewusst sein, dass Sie wahrscheinlich ändern, welche Dienste einsatzbereit sein werden.
Hier geht es zum Beispiel um die Verschiebung in die Zone home, in der SSH verfügbar ist.
Das bedeutet, dass unsere Verbindung nicht unterbrochen werden sollte.
In einigen anderen Zonen ist SSH nicht standardmäßig aktiviert, und der Wechsel zu einer dieser Zonen könnte dazu führen, dass Ihre Verbindung abstürzt und Sie sich nicht mehr bei Ihrem Server anmelden können.
Wir können überprüfen, ob dies erfolgreich war, indem wir die aktiven Zonen erneut anfordern:
Anpassen der Standardzone
Wenn alle Ihre Schnittstellen von einer einzigen Zone gut verarbeitet werden können, ist es wahrscheinlich am einfachsten, einfach die beste Zone als Standard zu bestimmen und diese dann für Ihre Konfiguration zu verwenden.
Sie können die Standardzone mit dem Parameter --set-default-zone = ändern.
Dadurch wird jede Schnittstelle, die die Standardzone verwendet, sofort geändert:
Festlegen von Regeln für Ihre Anwendungen
Lassen Sie uns die grundlegende Methode zur Definition von Firewall-Ausnahmen für die Dienste, die Sie zur Verfügung stellen möchten, durchgehen.
Hinzufügen eines Dienstes zu Ihren Zonen
Die einfachste Methode besteht darin, die von Ihnen benötigten Dienste oder Ports zu den von Ihnen benutzten Zonen hinzuzufügen.
Sie können eine Liste der verfügbaren Dienstdefinitionen mit der Option --get-services erhalten:
< $> note Anmerkung: Sie können weitere Einzelheiten über jeden dieser Dienste erhalten, indem Sie sich die zugehörige .xml-Datei im Verzeichnis / usr / lib / firewalld / services ansehen.
Zum Beispiel ist der SSH-Dienst wie folgt definiert:
Sie können einen Dienst für eine Zone mit dem Parameter --add-service = aktivieren.
Die Operation zielt auf die Standardzone oder die durch den Parameter --zone = angegebene Zone ab.
Standardmäßig wird dadurch nur die aktuelle Firewallsitzung angepasst.
Sie können die dauerhafte Firewall-Konfiguration anpassen, indem Sie das Flag --permanent einfügen.
Wenn wir beispielsweise einen Webserver betreiben, der konventionellen HTTP-Verkehr bedient, können wir diesen Verkehr vorübergehend für Schnittstellen in unserer Zone public zulassen, indem wir eingeben:
Wenn Sie die Standardzone ändern möchten, können Sie das Flag --zone = weglassen.
Wir können überprüfen, ob die Operation erfolgreich war, indem wir die Operationen --list-all oder --list-services verwenden:
Nachdem Sie die Funktionsfähigkeit geprüft haben, werden Sie wahrscheinlich die Regeln der dauerhaften Firewall so ändern wollen, dass Ihr Dienst auch nach einem Neustart noch verfügbar ist.
Wir können unsere vorherige Änderung dauerhaft machen, indem wir sie neu eingeben und das Flag --permanent hinzufügen:
Alternativ dazu können Sie das Flag --runtime-to-permanent verwenden, um die aktuell laufende Firewall-Konfiguration in der dauerhaften Konfiguration zu speichern:
Seien Sie damit vorsichtig, da alle an der laufenden Firewall vorgenommenen Änderungen dauerhaft übernommen werden.
Welche Methode Sie auch immer gewählt haben, Sie können überprüfen, ob sie erfolgreich war, indem Sie das Flag --permanent zur Operation --list-services hinzufügen.
Sie müssen sudo für alle --permanent Operationen verwenden:
Ihre Zone public erlaubt nun HTTP-Webverkehr auf Port 80. Wenn Ihr Webserver für die Verwendung von SSL / TLS konfiguriert ist, werden Sie auch den Dienst https hinzufügen wollen.
Diesen können wir der aktuellen Sitzung und dem dauerhaften Regelsatz hinzufügen, indem wir eingeben:
Was, wenn kein passender Dienst verfügbar ist?
Die Dienste, die mit der Installation von firewalld mitgeliefert werden, stellen viele der häufigsten Anwendungen dar, auf die Sie den Zugriff erlauben möchten.
Es wird jedoch wahrscheinlich Szenarien geben, in denen diese Dienste nicht Ihren Anforderungen entsprechen.
In dieser Situation haben Sie zwei Möglichkeiten.
Öffnen eines Port für Ihre Zonen
Der einfachste Weg, Unterstützung für Ihre spezifische Anwendung hinzuzufügen, besteht darin, die von ihr verwendeten Ports in der oder den entsprechenden Zonen zu öffnen.
Dies geschieht durch Angabe des Ports oder Portbereichs und des zugehörigen Protokolls (TCP oder UDP) für die Ports.
Wenn unsere Anwendung beispielsweise auf Port 5000 läuft und TCP verwendet, könnten wir diese mit dem Parameter --add-port = vorübergehend zur Zone public hinzufügen.
Protokolle können entweder als tcp oder udp zugewiesen werden:
Wir können überprüfen, ob dies erfolgreich war, indem wir die Operation --list-ports verwenden:
Es ist auch möglich, einen sequentiellen Bereich von Ports anzugeben, indem der Anfangs- und Endport in dem Bereich durch einen Bindestrich getrennt wird.
Wenn unsere Anwendung zum Beispiel die UDP-Ports 4990 bis 4999 verwendet, könnten wir diese für public freigeben, indem wir eingeben:
Nach dem Testen würden wir diese wahrscheinlich der dauerhaften Firewall hinzufügen wollen.
Verwenden Sie dazu sudo firewall-cmd --runtime-to-permanent oder führen Sie die Befehle mit dem Flag --permanent erneut aus:
Definieren eines Dienstes
Das Öffnen von Ports für Ihre Zonen ist eine unkomplizierte Lösung, aber es kann schwierig sein, den Überblick darüber zu behalten, wofür die einzelnen Ports vorgesehen sind.
Wenn Sie jemals einen Dienst auf Ihrem Server außer Betrieb nehmen, fällt es Ihnen vielleicht schwer, sich daran zu erinnern, welche geöffneten Ports noch benötigt werden.
Um diese Situation zu vermeiden, kann ein neuer Dienst definiert werden.
Dienste sind Sammlungen von Ports mit einem zugehörigen Namen und einer Beschreibung.
Die Verwendung von Diensten ist einfacher zu verwalten als Ports, erfordert aber ein wenig Vorarbeit.
Der einfachste Weg, damit zu beginnen, besteht darin, ein vorhandenes Skript (zu finden in / usr / lib / firewalld / services) in das Verzeichnis / etc / firewalld / services zu kopieren, in dem die Firewall nach Nicht-Standard-Definitionen sucht.
Wir könnten zum Beispiel die SSH-Dienstdefinition kopieren, um sie so für unsere Dienstdefinition example zu verwenden.
Der Dateiname abzüglich des .xml-Suffix diktiert den Namen des Dienstes innerhalb der Liste der Firewall-Dienste:
Jetzt können Sie die in der kopierten Datei gefundene Definition anpassen.
Öffnen Sie sie zunächst in Ihrem bevorzugten Texteditor.
Wir verwenden hier vi:
Zu Beginn enthält die Datei die SSH-Definition, die Sie kopiert haben:
Der Großteil dieser Definition sind eigentlich Metadaten.
Sie werden den Kurznamen für den Dienst innerhalb der Tags < short > ändern wollen.
Dies ist ein von Menschen lesbarer Name für Ihren Dienst.
Sie sollten auch eine Beschreibung hinzufügen, damit Sie über zusätzliche Informationen verfügen, falls Sie den Dienst jemals überprüfen müssen.
Die einzige Konfiguration, die Sie vornehmen müssen und die sich tatsächlich auf die Funktionalität des Dienstes auswirkt, wird wahrscheinlich die Port-Definition sein, in der Sie die Port-Nummer und das Protokoll identifizieren, die Sie öffnen möchten.
Es können mehrere < port / > -Tags angegeben werden.
Stellen Sie sich für unseren Dienst example vor, dass wir Port 7777 für TCP und 8888 für UDP öffnen müssen.
Wir können die bestehende Definition mit etwas wie dem Folgenden modifizieren:
Laden Sie Ihre Firewall neu, um Zugriff auf Ihren neuen Dienst zu erhalten:
Sie können sehen, dass er sich jetzt in der Liste der verfügbaren Dienste befindet:
Sie können diesen Dienst nun in Ihren Zonen wie gewohnt nutzen.
Erstellen Ihrer eigenen Zonen
Während die vordefinierten Zonen für die meisten Benutzer wahrscheinlich mehr als ausreichend sein werden, kann es hilfreich sein, eigene Zonen zu definieren, die ihre Funktion besser beschreiben.
Sie könnten zum Beispiel eine Zone für Ihren Webserver erstellen, die publicweb genannt wird.
Möglicherweise möchten Sie jedoch eine andere Zone für den DNS-Dienst konfigurieren haben, den Sie in Ihrem privaten Netzwerk anbieten.
Vielleicht möchten Sie dafür eine Zone namens "privateDNS" einrichten.
Wenn Sie eine Zone hinzufügen, müssen Sie sie zur dauerhaften Firewall-Konfiguration hinzufügen.
Sie können dann neu laden, um die Konfiguration in Ihre laufende Sitzung zu übernehmen.
Wir könnten zum Beispiel die beiden oben besprochenen Zonen erstellen, indem wir Folgendes eingeben:
Sie können überprüfen, ob diese in Ihrer dauerhaften Konfiguration vorhanden sind, indem Sie sie eingeben:
Wie bereits erwähnt, werden diese in der Laufzeit-Firewall noch nicht verfügbar sein:
Laden Sie die Firewall neu, um diese neuen Zonen in die aktive Laufzeitkonfiguration einzubinden:
Jetzt können Sie damit beginnen, Ihren Zonen die entsprechenden Dienste und Ports zuzuweisen.
Es ist normalerweise eine gute Idee, die Laufzeit-Firewall anzupassen und diese Änderungen nach dem Testen in der dauerhaften Konfiguration zu speichern.
Für die Zone publicweb könnten Sie beispielsweise die SSH-, HTTP- und HTTPS-Dienste hinzufügen:
Ebenso können wir den DNS-Dienst zu unserer Zone privateDNS hinzufügen:
Wir könnten dann unsere Schnittstellen auf diese neuen Zonen umstellen, um sie zu testen:
An dieser Stelle haben Sie die Möglichkeit, Ihre Konfiguration zu testen.
Wenn diese Werte für Sie funktionieren, werden Sie diese Regeln der dauerhaften Konfiguration hinzufügen wollen.
Das könnten Sie tun, indem Sie alle Befehle mit angehängtem Flag --permanent erneut ausführen, aber in diesem Fall werden wir das Flag --runtime-to-permanent verwenden, um unsere gesamte Laufzeitkonfiguration dauerhaft zu speichern:
Nachdem Sie diese Regeln dauerhaft angewendet haben, laden Sie die Firewall neu, um zu testen, ob die Änderungen bestehen bleiben:
Bestätigen Sie, dass die richtigen Zonen zugewiesen wurden:
Und prüfen Sie, ob die entsprechenden Dienste für beide Zonen verfügbar sind:
Sie haben Ihre eigenen Zonen erfolgreich eingerichtet!
Wenn Sie eine dieser Zonen zum Standard für andere Schnittstellen machen wollen, denken Sie daran, dieses Verhalten mit dem Parameter --set-default-zone = zu konfigurieren:
Sie sollten nun ein recht umfassendes Verständnis davon haben, wie Sie den Dienst firewalld auf Ihrem CentOS-System für den täglichen Gebrauch verwalten.
Der Dienst firewalld ermöglicht es Ihnen, wartbare Regeln und Regelsätze zu konfigurieren, die Ihre Netzwerkumgebung berücksichtigen.
Er ermöglicht Ihnen den nahtlosen Übergang zwischen verschiedenen Firewall-Richtlinien durch die Verwendung von Zonen und gibt Administratoren die Möglichkeit, die Portverwaltung in freundlichere Dienstdefinitionen zu abstrahieren.
Wenn Sie sich mit diesem System vertraut machen, können Sie die Flexibilität und Leistungsfähigkeit, die dieses Tool bietet, nutzen.
Weitere Informationen zu firewalld finden Sie in der offiziellen Dokumentation zu firewalld.
Verwenden der PDO-PHP-Erweiterung zur Durchführung von MySQL-Transaktionen in PHP unter Ubuntu 18.04
3992
Eine MySQL-Transaktion ist eine Gruppe von logisch zusammenhängenden SQL-Befehlen, die in der Datenbank als eine einzige Einheit ausgeführt werden.
Transaktionen werden verwendet, um die ACID (Atomicity, Consistency, Isolation und Durability) -Konformität in einer Anwendung durchzusetzen.
Dabei handelt es sich um eine Reihe von Standards, die die Zuverlässigkeit der Verarbeitungsvorgänge in einer Datenbank regeln.
Atomarität (Atomicity) gewährleistet den Erfolg zusammengehöriger Transaktionen oder einen vollständigen Ausfall, wenn ein Fehler auftritt.
Konsistenz (Consistency) garantiert die Gültigkeit der an die Datenbank übermittelten Daten gemäß der definierten Geschäftslogik.
Isolierung (Isolation) ist die korrekte Ausführung von gleichzeitigen Transaktionen, wobei sichergestellt wird, dass die Auswirkungen verschiedener Clients, die sich mit einer Datenbank verbinden, sich nicht gegenseitig beeinflussen.
Dauerhaftigkeit (Durability) stellt sicher, dass logisch zusammenhängende Transaktionen dauerhaft in der Datenbank verbleiben.
Über eine Transaktion ausgegebene SQL-Anweisungen sollten entweder erfolgreich sein oder ganz fehlschlagen.
Wenn eine der Abfragen fehlschlägt, macht MySQL die Änderungen rückgängig und sie werden niemals in die Datenbank übertragen.
Ein gutes Beispiel zum Verständnis der Funktionsweise von MySQL-Transaktionen ist eine E-Commerce-Website.
Wenn ein Kunde eine Bestellung aufgibt, fügt die Anwendung je nach Geschäftslogik Datensätze in mehrere Tabellen ein, wie beispielsweise: orders und orders _ products.
Mehrtabellen-Datensätze, die sich auf eine einzelne Bestellung beziehen, müssen als eine einzige logische Einheit atomar an die Datenbank gesendet werden.
Ein weiterer Anwendungsfall ist in einer Bankanwendung.
Wenn ein Kunde Geld überweist, werden einige Transaktionen an die Datenbank gesendet.
Das Konto des Senders wird belastet und dem Konto der Partei des Empfängers gutgeschrieben.
Die beiden Transaktionen müssen gleichzeitig bestätigt werden.
Wenn eine von ihnen fehlschlägt, kehrt die Datenbank in ihren ursprünglichen Zustand zurück, und es sollten keine Änderungen auf der Festplatte gespeichert werden.
In diesem Tutorial werden Sie die PDO-PHP-Erweiterung verwenden, die eine Schnittstelle für die Arbeit mit Datenbanken in PHP bietet, um MySQL-Transaktionen auf einem Ubuntu 18.04-Server durchzuführen.
Bevor Sie beginnen, benötigen Sie Folgendes:
Auf Ihrem System installiertes Apache, MySQL und PHP.
Sie können dem Leitfaden Installieren des Linux, Apache, MySQL, PHP (LAMP) -Stacks unter Ubuntu 18.04. folgen.
Sie können Schritt 4 (Einrichten virtueller Hosts) überspringen und direkt mit den Apache-Standardeinstellungen arbeiten.
Schritt 1 - Erstellen einer Beispieldatenbank und von Tabellen
Als Erstes erstellen Sie eine Beispieldatenbank und fügen einige Tabellen hinzu, bevor Sie mit MySQL-Transaktionen zu arbeiten beginnen.
Wenn Sie dazu aufgefordert werden, geben Sie Ihr MySQL-Root-Passwort ein und drücken Sie ENTER, um fortzufahren.
Erstellen Sie dann eine Datenbank. Für die Zwecke dieses Tutorials werden wir die Datenbank sample _ store nennen:
Erstellen Sie einen Benutzer namens sample _ user für Ihre Datenbank.
Denken Sie daran, PASSWORD durch einen starken Wert zu ersetzen:
Erteilen Sie Ihrem Benutzer uneingeschränkte Berechtigungen für die Datenbank sample _ store:
Laden Sie abschließend die MySQL-Berechtigungen neu:
Sobald Sie Ihren Benutzer angelegt haben, sehen Sie die folgende Ausgabe:
Mit der vorhandenen Datenbank und dem Benutzer können Sie jetzt mehrere Tabellen erstellen, um zu demonstrieren, wie MySQL-Transaktionen funktionieren.
Melden Sie sich vom MySQL-Server ab:
Sobald das System Sie abmeldet, sehen Sie die folgende Ausgabe:
Melden Sie sich dann mit den Anmeldeinformationen des Benutzers sample _ user an, den Sie gerade angelegt haben:
Geben Sie das Passwort für den sample _ user ein und drücken Sie ENTER, um fortzufahren.
Wechseln Sie zu sample _ store, um sie zur aktuell ausgewählten Datenbank zu machen:
Sobald die Datenbank ausgewählt ist, sehen Sie die folgende Ausgabe:
Erstellen Sie als Nächstes eine Tabelle products:
Mit diesem Befehl wird eine Tabelle products mit einem Feld namens product _ id erstellt.
Verwenden Sie einen Datentyp BIGINT, der einen großen Wert von bis zu 2 ^ 63-1 aufnehmen kann.
Sie verwenden dasselbe Feld als PRIMARY KEY zur eindeutigen Identifizierung von Produkten.
Das Schlüsselwort AUTO _ INCREMENT weist MySQL an, den nächsten numerischen Wert zu erzeugen, wenn neue Produkte eingefügt werden.
Das Feld product _ name ist vom Typ VARCHAR und kann bis zu 50 Buchstaben oder Zahlen enthalten.
Für den Produktpreis (price) verwenden Sie einen Datentyp DOUBLE, um Fließkommaformate in Preisen mit Dezimalzahlen zu berücksichtigen.
Zuletzt verwenden Sie die InnoDB als ENGINE, weil sie im Gegensatz zu anderen Speicher-Engines wie MyISAM MySQL-Transaktionen komfortabel unterstützt.
Sobald Sie Ihre Tabelle products erstellt haben, erhalten Sie die folgende Ausgabe:
Fügen Sie als Nächstes einige Artikel zur Tabelle products hinzu, indem Sie die folgenden Befehle ausführen:
Nach jeder Operation INSERT sehen Sie eine Ausgabe ähnlich der folgenden:
Überprüfen Sie dann, ob die Daten der Tabelle products hinzugefügt wurden:
Sie sehen eine Liste mit den vier von Ihnen eingefügten Produkten:
Als Nächstes erstellen Sie eine Tabelle customers, die grundlegende Informationen über Kunden enthält:
Wie in der Tabelle products verwenden Sie für die customer _ id den Datentyp BIGINT, wodurch sichergestellt wird, dass die Tabelle viele Kunden mit bis zu 2 ^ 63-1 Datensätzen unterstützt.
Das Schlüsselwort AUTO _ INCREMENT erhöht den Wert der Spalten, sobald Sie einen neuen Kunden einfügen.
Da die Spalte customer _ name alphanumerische Werte akzeptiert, verwenden Sie den Datentyp VARCHAR mit einer Beschränkung von 50 Zeichen.
Auch hier verwenden Sie die InnoDB Storage ENGINE zur Unterstützung von Transaktionen.
Nachdem Sie den vorherigen Befehl zum Anlegen der Tabelle customers ausgeführt haben, sehen Sie die folgende Ausgabe:
Sie fügen der Tabelle drei Musterkunden hinzu.
Führen Sie die folgenden Befehle aus:
Sobald die Kunden hinzugefügt wurden, sehen Sie eine Ausgabe ähnlich der folgenden:
Überprüfen Sie dann die Daten in der Tabelle customers:
Sie sehen eine Liste mit den drei Kunden:
Anschließend erstellen Sie eine Tabelle orders, um die von verschiedenen Kunden aufgegebenen Bestellungen aufzuzeichnen.
Führen Sie den folgenden Befehl aus, um die Tabelle orders zu erstellen:
Verwenden Sie die Spalte order _ id als PRIMARY KEY.
Der Datentyp BIGINT ermöglicht Ihnen die Aufnahme von bis zu 2 ^ 63-1 Bestellungen und wird nach jeder Bestellungseinfügung automatisch erhöht.
Das Feld order _ date enthält das tatsächliche Datum und die tatsächliche Uhrzeit der Bestellung, daher verwenden Sie den Datentyp DATETIME.
Die customer _ id bezieht sich auf die zuvor von Ihnen erstellte Tabelle customers.
Da die Bestellung eines einzelnen Kunden mehrere Artikel enthalten kann, müssen Sie eine Tabelle orders _ products erstellen, um diese Informationen zu speichern.
Führen Sie den folgenden Befehl aus, um die Tabelle orders _ products zu erstellen:
Verwenden Sie die ref _ id als PRIMARY KEY, der nach jeder Einfügung eines Datensatzes automatisch erhöht wird.
Die order _ id und product _ id beziehen sich auf die Tabelle orders und products.
Die Spalte price ist vom Datentyp DOUBLE, um gleitende Werte aufzunehmen.
Die Speicher-Engine InnoDB muss mit den zuvor erstellten Tabellen übereinstimmen, da die Bestellung eines einzelnen Kunden unter Verwendung von Transaktionen gleichzeitig auf mehrere Tabellen auswirkt.
Ihre Ausgabe bestätigt die Erstellung der Tabelle:
Sie werden vorerst keine Daten zu den Tabellen orders und orders _ products hinzufügen. Dies geschieht jedoch später mit einem PHP-Skript, das MySQL-Transaktionen implementiert.
Ihr Datenbankschema ist nun vollständig und Sie haben es mit einigen Datensätzen gefüllt.
Nun werden Sie eine PHP-Klasse für die Handhabung von Datenbankverbindungen und MySQL-Transaktionen erstellen.
Schritt 2 - Entwerfen einer PHP-Klasse zur Abwicklung von MySQL-Transaktionen
In diesem Schritt erstellen Sie eine PHP-Klasse, die PDO (PHP Data Objects) zur Abwicklung von MySQL-Transaktionen verwendet.
Die Klasse wird eine Verbindung zu Ihrer MySQL-Datenbank herstellen und Daten atomar in die Datenbank einfügen.
Speichern Sie die Klassendatei im Stammverzeichnis Ihres Apache-Webservers.
Erstellen Sie dazu mit Ihrem Texteditor eine Datei DBTransaction.php:
Fügen Sie dann den folgenden Code in die Datei ein:
Ersetzen Sie PASSWORD mit dem in Schritt 1 erstellten Wert:
Zu Beginn der Klasse DBTransaction verwendet das PDO die Konstanten (DB _ HOST, DB _ NAME, DB _ USER und DB _ PASSWORD) zur Initialisierung und Verbindung mit der Datenbank, die Sie in Schritt 1 erstellt haben.
< $> note Anmerkung: Da wir hier MySQL-Transaktionen in kleinem Maßstab demonstrieren, haben wir die Datenbankvariable in der Klasse DBTransaction deklariert.
In einem großen Produktionsprojekt würden Sie normalerweise eine separate Konfigurationsdatei erstellen und die Datenbankkonstanten aus dieser Datei mit Hilfe einer PHP-Anweisung require _ once laden.
Als Nächstes legen Sie zwei Attribute für die Klasse PDO fest:
ATTR _ ERRMODE, PDO:: ERRMODE _ EXCEPTION: Dieses Attribut weist PDO an, eine Ausnahme auszulösen, wenn ein Fehler auftritt.
Solche Fehler können zum Debugging protokolliert werden.
ATTR _ EMULATE _ PREPARES, false: Diese Option deaktiviert die Emulation vorbereiteter Anweisungen und erlaubt der MySQL-Datenbank-Engine, die Anweisungen selbst vorzubereiten.
Fügen Sie nun den folgenden Code in Ihre Datei ein, um die Methoden für Ihre Klasse zu erstellen:
Speichern und schließen Sie die Datei, indem Sie STRG + X, Y, dann ENTER drücken.
Um mit MySQL-Transaktionen zu arbeiten, erstellen Sie in der Klasse DBTransaction drei Hauptmethoden: startTransaction, insertTransaction und submitTransaction.
startTransaction: Diese Methode weist PDO an, eine Transaktion zu starten, und schaltet Auto-Commit aus, bis ein Commit-Befehl ausgegeben wird.
insertTransaction: Diese Methode benötigt zwei Argumente.
Die Variable $sql enthält die auszuführende SQL-Anweisung, während die Variable $data ein Array mit Daten ist, die an die SQL-Anweisung gebunden werden sollen, da Sie vorbereitete Anweisungen verwenden.
Die Daten werden als Array an die Methode insertTransaction übergeben.
submitTransaction: Diese Methode bindet die Änderungen dauerhaft an die Datenbank, indem Sie einen Befehl commit () ausgibt.
Wenn jedoch ein Fehler auftritt und die Transaktionen ein Problem haben, ruft die Methode die Methode rollBack () auf, um die Datenbank in ihren ursprünglichen Zustand zurückzuversetzen, falls eine PDO-Ausnahme ausgelöst wird.
Ihre Klasse DBTransaction initialisiert eine Transaktion, bereitet die verschiedenen auszuführenden SQL-Befehle vor, und bindet schließlich die Änderungen atomar in die Datenbank ein, wenn es keine Probleme gibt. Andernfalls wird die Transaktion rückgängig gemacht.
Darüber hinaus ermöglicht Ihnen die Klasse, den Datensatz order _ id abzurufen, den Sie gerade durch Zugriff auf die öffentliche Eigenschaft last _ insert _ id erstellt haben.
Die Klasse DBTransaction ist nun bereit, von jedem PHP-Code, den Sie als Nächstes erstellen, aufgerufen und verwendet zu werden.
Schritt 3 - Erstellen eines PHP-Skripts zur Verwendung der Klasse DBTransaction
Sie erstellen ein PHP-Skript, das die Klasse DBTransaction implementiert und eine Gruppe von SQL-Befehlen in die MySQL-Datenbank sendet.
Sie imitieren den Workflow einer Kundenbestellung in einem Online-Einkaufswagen.
Diese SQL-Abfragen wirken sich auf die Tabellen orders und orders _ products aus.
Ihre Klasse DBTransaction sollte nur Änderungen in der Datenbank zulassen, wenn alle Abfragen fehlerfrei ausgeführt werden.
Andernfalls erhalten Sie einen Fehler zurück, und alle Änderungsversuche werden rückgängig gemacht.
Sie erstellen eine einzelne Bestellung für den mit customer _ id 1 identifizierten Kunden JOHN DOE. Die Bestellung des Kunden enthält drei verschiedene Artikel mit unterschiedlichen Mengen aus der Tabelle products.
Ihr PHP-Skript nimmt die Bestelldaten des Kunden und übergibt sie an die Klasse DBTransaction.
Erstellen Sie die Datei orders.php:
Fügen Sie dann den folgenden Code in die Datei ein:
Sie haben ein PHP-Skript erstellt, das eine Instanz der Klasse DBTransaction initialisiert, die Sie in Schritt 2 erstellt haben.
In diesem Skript fügen Sie die Datei DBTransaction.php ein und initialisieren die Klasse DBTransaction.
Als Nächstes bereiten Sie ein mehrdimensionales Array aller Produkte vor, die der Kunde im Geschäft bestellt.
Sie rufen auch die Methode startTransaction () auf, um eine Transaktion zu starten.
Anschließend fügen Sie den folgenden Code hinzu, um Ihr Skript orders.php zu vervollständigen:
Bereiten Sie den Befehl vor, der über die Methode insertTransaction in die Tabelle orders eingefügt werden soll.
Danach rufen Sie den Wert der öffentlichen Eigenschaft last _ insert _ id aus der Klasse DBTransaction ab und verwenden ihn als $order _ id.
Sobald Sie über ein $order _ id verfügen, verwenden Sie die eindeutige ID, um die von dem Kunden bestellten Artikel in die Tabelle orders _ products einzufügen.
Anschließend rufen Sie die Methode submitTransaction auf, um die gesamten Bestelldetails des Kunden in die Datenbank zu übertragen, wenn es keine Probleme gibt.
Andernfalls macht die Methode submitTransaction die versuchten Änderungen rückgängig.
Jetzt führen Sie das Skript orders.php in Ihrem Browser aus.
Führen Sie das Folgende aus und ersetzen Sie your-server-IP mit der öffentlichen IP-Adresse Ihres Servers:
http: / / < ^ > your-server-IP < ^ > / orders.php
Sie erhalten eine Bestätigung, dass die Datensätze erfolgreich übermittelt wurden.
PHP-Ausgabe aus der MySQL-Klasse Transactions
Ihr PHP-Skript funktioniert wie erwartet und die Bestellung zusammen mit den zugehörigen Bestellprodukten wurde atomar in die Datenbank übertragen.
Sie haben die Datei orders.php in einem Browserfenster ausgeführt.
Das Skript hat die Klasse DBTransaction aufgerufen, die ihrerseits die Details der orders in die Datenbank übermittelte.
Im nächsten Schritt überprüfen Sie, ob die Datensätze in den Bezugstabellen der Datenbank gespeichert wurden.
Schritt 4 - Bestätigen der Einträge in Ihrer Datenbank
In diesem Schritt überprüfen Sie, ob die vom Browserfenster aus initiierte Transaktion für die Bestellung des Kunden wie erwartet in die Datenbanktabellen verbucht wurde.
Dazu melden Sie sich erneut an Ihrer MySQL-Datenbank an:
Geben Sie das Passwort für den sample _ user ein und drücken Sie ENTER, um fortzufahren.
Wechseln Sie zur Datenbank sample _ store:
Stellen Sie sicher, dass die Datenbank geändert wird, bevor Sie fortfahren, indem Sie die folgende Ausgabe bestätigen:
Geben Sie dann den folgenden Befehl ein, um Datensätze aus der Tabelle orders abzurufen:
Dadurch wird die folgende Ausgabe angezeigt, die die Bestellung des Kunden beschreibt:
Rufen Sie dann die Datensätze aus der Tabelle orders _ products ab:
Sie sehen eine Ausgabe ähnlich der folgenden mit einer Liste von Produkten aus der Bestellung des Kunden:
Die Ausgabe bestätigt, dass die Transaktion in der Datenbank gespeichert wurde und Ihre Helferklasse DBTransaction wie erwartet funktioniert.
In diesem Leitfaden haben Sie die PHP PDO verwendet, um mit MySQL-Transaktionen zu arbeiten.
Obwohl dies kein erschöpfender Artikel über die Gestaltung einer e-Commerce-Software ist, hat er ein Beispiel für die Verwendung von MySQL-Transaktionen in Ihren Anwendungen bereitgestellt.
Weitere Informationen über das Modell MySQL ACID finden Sie in dem Leitfaden InnoDB und das ACID-Modell auf der offiziellen MySQL-Website.
Besuchen Sie unsere MySQL-Inhaltsseite für weitere verwandte Tutorials, Artikel und Fragen und Antworten.
Hinzufügen und Löschen von Benutzern unter CentOS 8
4020
Wenn Sie zum ersten Mal einen neuen Linux-Server einsetzen, ist das Hinzufügen und Entfernen von Benutzern oft eines der ersten Dinge, die Sie tun müssen.
In diesem Leitfaden erfahren Sie, wie Sie auf einem CentOS 8-Server Benutzerkonten erstellen, sudo-Privilegien zuweisen und Benutzer löschen.
In diesem Tutorial wird davon ausgegangen, dass Sie mit einem non-root sudo-fähigen Benutzer an einem CentOS 8-Server angemeldet sind.
Wenn Sie stattdessen als root angemeldet sind, können Sie den sudo-Teil aller folgenden Befehle weglassen, aber sie funktionieren so oder so.
Hinzufügen von Benutzern
Während des gesamten Tutorials werden wir mit dem Benutzer sammy arbeiten.
Bitte ersetzen Sie ihn mit dem Benutzernamen Ihrer Wahl.
Sie können einen neuen Benutzer hinzufügen, indem Sie Folgendes eingeben:
Als Nächstes müssen Sie Ihrem Benutzer ein Passwort geben, damit er sich anmelden kann.
Benutzen Sie hierfür den Befehl passwd:
Sie werden aufgefordert, das Passwort zweimal einzugeben, um es zu bestätigen. Nun ist Ihr neuer Benutzer eingerichtet und einsatzbereit!
< $> note Anmerkung: Wenn Ihr SSH-Server passwortbasierte Authentifizierung nicht zulässt, können Sie noch keine Verbindung mit Ihrem neuen Benutzernamen herstellen.
Einzelheiten zur Einrichtung der schlüsselbasierten SSH-Authentifizierung für den neuen Benutzer finden Sie in Schritt 5 von Ersteinrichtung des Servers unter CentOS 8. < $>
Erteilen von Sudo-Berechtigungen an einen Benutzer
Wenn Ihr neuer Benutzer in der Lage sein soll, Befehle mit (administrativen) root-Berechtigungen auszuführen, müssen Sie ihm Zugriff auf sudo gewähren.
Wir können dies tun, indem wir den Benutzer zu der Gruppe wheel hinzufügen (die standardmäßig allen ihren Mitgliedern Zugriff auf sudo gewährt).
Verwenden Sie den Befehl usermod, um den Benutzer der Guppe wheel hinzuzufügen:
Jetzt kann Ihr neuer Benutzer Befehle mit administrativen Berechtigungen ausführen.
Stellen Sie dazu sudo dem Befehl voran, den Sie als Administrator ausführen möchten:
Sie werden aufgefordert, das Passwort Ihres Benutzerkontos einzugeben (nicht das root-Passwort).
Sobald das korrekte Passwort eingegeben wurde, wird der von Ihnen eingegebene Befehl mit root-Berechtigungen ausgeführt.
Verwalten von Benutzern mit Sudo-Berechtigungen
Während Sie mit usermod Benutzer zu einer Gruppe hinzufügen und aus einer Gruppe entfernen können, hat der Befehl keine Möglichkeit, anzuzeigen, welche Benutzer Mitglieder einer Gruppe sind.
Um zu sehen, welche Benutzer Teil der Guppe wheel sind (und somit sudo-Berechtigungen haben), können Sie den Befehl lid verwenden. lid wird normalerweise verwendet, um anzuzeigen, zu welchen Gruppen ein Benutzer gehört. Aber mit dem Flag -g können Sie es umkehren und anzeigen, welche Benutzer zu einer Gruppe gehören:
Die Ausgabe wird Ihnen die mit der Gruppe verbundenen Benutzernamen und UIDs zeigen.
Dies ist eine gute Möglichkeit, um zu bestätigen, dass Ihre vorherigen Befehle erfolgreich waren und dass der Benutzer über die erforderlichen Berechtigungen verfügt.
Löschen von Benutzern
Wenn Sie ein Benutzerkonto haben, das Sie nicht mehr benötigen, ist es am besten, es zu löschen.
Um den Benutzer zu löschen, ohne eine seiner Dateien zu löschen, verwenden Sie den Befehl userdel:
Wenn Sie das Home-Verzeichnis des Benutzers zusammen mit seinem Konto löschen wollen, fügen Sie das Flag -r zu userdel hinzu:
Mit beiden Befehlen wird der Benutzer automatisch aus allen Gruppen entfernt, denen er hinzugefügt wurde, einschließlich der Gruppe wheel, falls zutreffend.
Wenn Sie später einen weiteren Benutzer mit dem gleichen Namen hinzufügen, müssen Sie ihn wieder der Gruppe wheel hinzufügen, um sudo-Zugriff zu erhalten.
Sie sollten nun einen guten Überblick über das Hinzufügen und Löschen von Benutzern von Ihrem CentOS 8-Server haben.
Eine effektive Benutzerverwaltung ermöglicht es Ihnen, Benutzer zu trennen und ihnen nur den Zugriff zu gewähren, den sie für ihre Arbeit benötigen.
Sie können nun damit fortfahren, Ihren CentOS 8-Server für die Software zu konfigurieren, die Sie benötigen, wie z. B. einen LAMP- oder LEMP-Web-Stack.
Installieren eines Linux-, Apache-, MariaDB-, PHP- (LAMP-) Stacks unter CentOS 8
4022
Ein "LAMP" -Stack ist eine aus Open-Source-Software bestehende Gruppe, die normalerweise zusammenhängend installiert wird, damit ein Server dynamische Websites und in PHP geschriebene Web-Apps hosten kann.
Dieser Begriff ist ein Akronym, das für das Linux-Betriebssystem mit dem Webserver Apache steht.
Die Backend-Daten werden in einer MariaDB-Datenbank gespeichert und die dynamische Verarbeitung wird mit PHP ausgeführt.
Die Datenbankschicht in einem LAMP-Stack ist in der Regel ein MySQL-Datenbankserver, aber vor der Veröffentlichung von CentOS 8 war MySQL nicht über die standardmäßigen CentOS-Repositorys verfügbar.
Aus diesem Grund wurde MariaDB, eine Abspaltung von MySQL, zu einer weithin akzeptierten Alternative zu MySQL als Standard-Datenbanksystem für LAMP-Stacks auf CentOS-Rechnern.
MariaDB dient als vollwertiger Ersatz für den originalen MySQL-Server, was in der Praxis bedeutet, dass Sie zu MariaDB wechseln können, ohne Änderungen an der Konfiguration oder den Codes Ihrer Anwendung vorzunehmen.
In diesem Leitfaden installieren Sie einen LAMP-Stack auf einem CentOS 8-Server unter Verwendung von MariaDB als Datenbankverwaltungssystem.
Um diesem Leitfaden zu folgen, benötigen Sie Zugriff auf einen CentOS 8 Server als non-root user mit Sudo-Berechtigungen, und eine auf Ihrem Server installierte aktive Firewall.
Um dies einzurichten, können Sie unseren Leitfaden zur Ersteinrichtung des Servers für CentOS 8 folgen.
Schritt 1 - Installieren des Apache-Webservers
Um Websites für unsere Websitebesucher anzuzeigen, setzen wir Apache ein, einen populären Open-Source-Webserver, der für die Bedienung von PHP-Seiten konfiguriert werden kann.
Sie verwenden den dnf Paketmamager, den neuen Standard-Paketmanager auf CentOS 8, um diese Software zu installieren.
Installieren Sie das Paket httpd mit:
Geben Sie bei der entsprechenden Aufforderung y ein, um zu bestätigen, dass Sie Apache installieren möchten.
Wenn die Installation abgeschlossen ist, führen Sie den folgenden Befehl aus, um den Server zu aktivieren und zu starten:
Wenn Sie die Firewall firewalld gemäß unserem Leitfaden zur Ersteinrichtung des Servers aktiviert haben, müssen Sie Verbindungen zu Apache zulassen.
Der folgende Befehl ermöglicht dauerhaft HTTP-Verbindungen, die auf Port 80 standardmäßig ausgeführt werden:
Um zu verifizieren, dass die Änderung angewendet wurde, können Sie Folgendes ausführen:
Laden Sie die Firewall-Konfiguration neu, damit die Änderungen wirksam werden:
Nachdem die neue Firewall hinzugefügt wurde, können Sie testen, ob der Server läuft, indem Sie auf die öffentliche IP-Adresse des Servers oder den Domänennamen von Ihrem Webbrowser aus zugreifen.
< $> note Anmerkung: Falls Sie DigitalOcean als DNS-Hosting-Anbieter verwenden, konsultieren Sie unsere Produktdokumente für detaillierte Anweisungen, wie Sie einen neuen Domänennamen einrichten und ihn auf Ihren Server verweisen.
Wenn kein Domänenname auf den Server verweist und Sie die öffentliche IP-Adresse Ihres Servers nicht kennen, können Sie diese mit dem folgenden Befehl finden:
Hierdurch erhalten Sie einige IP-Adressen.
Sie können diese abwechselnd in Ihrem Webbrowser ausprobieren.
Als Alternative können Sie prüfen, welche IP-Adresse von anderen Stellen im Internet aus erreichbar ist:
Geben Sie die Adresse ein, die Sie in Ihrem Webbrowser erhalten, und Sie werden zur Standard-Startseite von Apache weitergeleitet:
Standard Apache-Seite CentOS 8
Wenn Sie diese Seite sehen, ist Ihr Webserver korrekt installiert.
Schritt 2 - Installieren von MariaDB
Nachdem Sie nun einen funktionierenden Webserver eingerichtet haben, müssen Sie ein Datenbanksystem installieren, um Daten für Ihre Website speichern und verwalten zu können.
Sie installieren nun MariaDB, eine weiterentwickelte Kopie des originalen MySQL Servers von Oracle.
Um diese Software zu installieren, führen Sie Folgendes aus:
Wenn die Installation abgeschlossen ist, aktivieren und starten Sie den MariaDB-Server mit:
Um die Sicherheit Ihres Datenbankservers zu verbessern, wird empfohlen, ein Sicherheitsskript auszuführen, das in MariaDB vorinstalliert ist.
Dieses Skript entfernt einige unsichere Standardeinstellungen und sperrt den Zugriff auf Ihr Datenbanksystem.
Starten Sie das interaktive Skript, indem Sie Folgendes ausführen:
Dieses Skript führt Sie durch eine Reihe von Aufforderungen, mit denen Sie verschiedene Änderungen an Ihrem MariaDB-Setup vornehmen können.
Verwechseln Sie dies nicht mit dem System-root user.
Der Benutzer der Datenbank-Root ist ein administrativer Benutzer mit vollen Berechtigungen über das Datenbank-System.
Weil Sie MariaDB gerade erst installiert und noch keine Konfigurationsänderungen vorgenommen haben, wird dieses Passwort leer sein, also drücken Sie bei der Eingabeaufforderung einfach ENTER.
Und weil MariaDB eine spezielle Authentifizierungsmethode für den root user verwendet, die typischerweise sicherer ist als ein Passwort, müssen Sie das jetzt nicht einrichten.
Damit werden anonyme Benutzer und die Testdatenbank entfernt, ferngesteuerte root Logins deaktiviert und die neuen Regeln geladen, sodass der Server die Änderungen, die Sie vorgenommen haben, unverzüglich anwendet.
Wenn Sie fertig sind, melden Sie sich bei MariaDB an, indem Sie Folgendes eingeben:
Damit wird eine Verbindung zum MariaDB-Server als administrativer Datenbankbenutzer root hergestellt, was durch die Verwendung von sudo bei Ausführung dieses Befehls abgeleitet wird.
Die Ausgabe sollte wie folgt aussehen:
Beachten Sie, dass Sie kein Passwort bereitstellen mussten, um sich als root Benutzer zu verbinden.
Das funktioniert, weil die standardmäßige Authentifizierungsmethode für den administrativen MariaDB-Benutzer unix _ socket ist und nicht password.
Auch wenn dies zunächst wie ein Sicherheitsproblem aussieht, macht es den Datenbankserver sicherer, da sich nur die Systembenutzer mit sudo-Privilegien über die Konsole oder über eine Anwendung, die mit den gleichen Privilegien läuft, als root MariaDB-Benutzer anmelden dürfen.
Praktisch bedeutet dies, dass Sie den administrativer Datenbank root Benutzer nicht verwenden können, um sich von Ihrer PHP-Anwendung aus zu verbinden.
Um die Sicherheit zu erhöhen, richten Sie am besten für jede Datenbank zugeordnete Benutzerkonten mit weniger expansiven Berechtigungen ein, insbesondere dann, wenn Sie mehrere Datenbanken auf Ihrem Server hosten möchten.
Um eine solche Einrichtung zu zeigen, erstellen Sie eine Datenbank namens example\ _ database und einen Benutzer namens example\ _ user. Sie können diese Namen jedoch durch andere Werte ersetzen.
Um eine neue Datenbank zu erstellen, führen Sie den folgenden Befehl von Ihrer MariaDB-Konsole aus:
Jetzt können Sie einen neuen Benutzer erstellen und ihm volle Berechtigungen zu der benutzerdefinierten Datenbank gewähren, die Sie gerade erstellt haben:
Der folgende Befehl definiert das Passwort dieses Benutzers als < ^ > password < ^ >, aber Sie sollten diesen Wert durch ein sicheres Passwort Ihrer Wahl ersetzen:
Damit werden dem example\ _ user volle Berechtigungen über die example\ _ database gegeben, während dieser Benutzer gleichzeitig daran gehindert wird, andere Datenbanken auf Ihrem Server zu erstellen oder zu ändern.
Sie können testen, ob der neue Benutzer die richtigen Berechtigungen hat, indem Sie sich erneut in die MariaDB-Konsole einloggen, diesmal mit den benutzerdefinierten Anmeldedaten:
Beachten Sie das Flag -p in diesem Befehl, das Sie nach dem Passwort fragt, welches Sie bei der Erstellung des Benutzers example\ _ user gewählt haben.
Nachdem Sie sich in die MariaDB-Konsole einloggt haben, bestätigen Sie, dass Sie Zugriff auf die Datenbank example\ _ database haben:
Damit erhalten Sie die folgende Ausgabe:
Um die MariaDB Shell zu beenden, geben Sie Folgendes ein:
Zu diesem Zeitpunkt ist Ihr Datenbanksystem eingerichtet und Sie können zur Installation von PHP, der letzten Komponente des LAMP-Stacks, übergehen.
Schritt 3 - Installieren von PHP
Sie haben Apache zur Bereitstellung Ihrer Inhalte und MariaDB zur Speicherung und Verwaltung Ihrer Daten installiert. PHP ist die Komponente unserer Einrichtung, die Code verarbeitet, um dynamische Inhalte für den Endbenutzer anzuzeigen.
Zusätzlich zum php-Paket benötigen Sie php-mysqlnd, ein PHP-Modul, das PHP die Kommunikation mit MySQL-basierten Datenbanken ermöglicht.
Core-PHP-Pakete werden automatisch als Abhängigkeiten installiert.
Um die Pakete php und php-mysqlnd mit dem dnf-Paketmanager zu installieren, führen Sie Folgendes aus:
Nach Abschluss der Installation müssen Sie den Apache-Webserver neu starten, um das PHP-Modul zu aktivieren:
Ihr Webserver ist nun voll eingerichtet.
Im nächsten Schritt erstellen Sie ein PHP-Testscript, um sicherzustellen, dass alles wie erwartet funktioniert.
Schritt 4 - Testen von PHP mit Apache
Die standardmäßige Apache-Installation unter CentOS 8 erzeugt ein Dokument-Stammverzeichnis, das sich unter / var / www / html befindet.
Damit PHP auf Ihrem Webserver korrekt funktioniert, müssen Sie keine Änderungen an den Standardeinstellungen von Apache vornehmen.
Die einzige Anpassung, die wir vornehmen werden, ist die Änderung der standardmäßigen Berechtigungseinstellungen in Ihrem Dokument-Stammverzeichnis von Apache.
Auf diese Weise können Sie Dateien in diesem Verzeichnis mit Ihrem regulären Systembenutzer erstellen und ändern, ohne dass Sie jedem Befehl sudo voranstellen müssen.
Der folgende Befehl ändert die Eigentümerschaft des standardmäßigen Stammverzeichnisses des Apache-Dokuments auf einen Benutzer und eine Gruppe namens < ^ > sammy < ^ >. Achten Sie also darauf, den hervorgehobenen Benutzernamen und die Gruppe in diesem Befehl zu ersetzen, um den Benutzernamen und die Gruppe Ihres Systems widerzuspiegeln.
Sie erstellen nun eine PHP-Testseite, um sicherzustellen, dass der Webserver wie erwartet funktioniert.
Der mit CentOS 8 bereitgestellte Standard-Texteditor ist vi. vi ist ein extrem leistungsfähiger Texteditor. Für Benutzer mit wenig Erfahrung kann er etwas stumpfsinnig sein. Eventuell möchten Sie einen benutzerfreundlicheren Editor wie nano installieren, um die Bearbeitung von Dateien auf Ihrem CentOS 8-Server zu erleichtern:
Geben Sie bei der Aufforderung zur Bestätigung der Installation y ein.
Erstellen Sie nun eine neue PHP-Datei namens info.php im Verzeichnis / var / www / html:
Der folgende PHP-Code zeigt Informationen über die aktuelle PHP-Umgebung, die auf dem Server läuft:
Wenn Sie nano verwenden, können Sie STRG + X drücken, dann Y eingeben und zur Bestätigung ENTER drücken.
Jetzt können Sie testen, ob Ihr Webserver den Inhalt korrekt anzeigt, der durch ein PHP-Skript erstellt wird.
Gehen Sie zu Ihrem Browser und greifen Sie auf Ihren Server-Hostnamen oder die IP-Adresse zu, gefolgt von / info.php:
Sie sehen eine Seite, die so ähnlich wie die Folgende aussieht:
CentOS 8 Standard-PHP-Info Apache
Nachdem Sie die relevanten Informationen über Ihren PHP-Server über diese Seite überprüft haben, ist es am besten, die von Ihnen erstellte Datei zu entfernen, da sie sensible Informationen über Ihre PHP-Umgebung und Ihren CentOS-Server enthält.
Sie können rm verwenden, um diese Datei zu entfernen:
Sie können diese Datei jederzeit regenerieren, falls Sie sie später benötigen.
Als Nächstes testen Sie die Datenbankverbindung von der PHP-Seite.
Schritt 5 - Testen der Datenbankverbindung von PHP (optional)
Wenn Sie testen möchten, ob PHP eine Verbindung mit MariaDB herstellen kann und Datenbankabfragen ausführt, können Sie eine Testtabelle mit Pseudodaten erstellen und die Inhalte mit einem PHP-Skript abfragen.
Verbinden Sie sich zuerst mit der MariaDB-Konsole mit dem Datenbankbenutzer, den Sie in Schritt 2 dieses Leitfadens erstellt haben:
Erstellen Sie eine Tabelle namens todo _ list.
Führen Sie die folgende Anweisung aus MariaDB aus:
Geben Sie nun einige Zeilen an Inhalt in die Testtabelle ein.
Sie können den nächsten Befehl ein paar Mal wiederholen, indem Sie verschiedene Werte verwenden:
Um zu bestätigen, dass die Daten erfolgreich in Ihrer Tabelle gespeichert wurden, führen Sie Folgendes aus:
Nachdem Sie bestätigt haben, dass Sie gültige Daten in Ihrer Testtabelle haben, können Sie die MariaDB-Konsole verlassen:
Sie können nun das PHP-Skript erstellen, das sich mit MariaDB verbindet und Ihre Inhalte abfragen.
Erstellen Sie mit Ihrem bevorzugten Editor eine neue PHP-Datei in Ihrem benutzerdefinierten Stammverzeichnis.
Verwenden Sie hierzu nano:
Fügen Sie Ihrem PHP-Skript den folgenden Inhalt hinzu:
Sie können diese Seite jetzt in Ihrem Webbrowser aufrufen, indem Sie den Hostnamen oder die öffentliche IP-Adresse Ihres Servers aufrufen, gefolgt von / todo _ list.php:
Sie sollten nun eine Seite ähnlich wie diese sehen, die den Inhalt anzeigt, den Sie in Ihre Testtabelle eingefügt haben:
Beispiel PHP To-Do-List
Das bedeutet, dass Ihre PHP-Umgebung zur Verfügung steht, um mit Ihrem MariaDB-Server zu interagieren.
In diesem Leitfaden haben Sie eine flexible Grundlage für die Bereitstellung von PHP-Websites und -Anwendungen für Ihre Besucher unter Verwendung von Apache als Webserver erstellt.
Sie haben Apache zur Bearbeitung von PHP-Anfragen eingerichtet und haben auch eine MariaDB-Datenbank zur Speicherung der Daten Ihrer Website eingerichtet.
Installieren von MariaDB unter CentOS 8
4019
In diesem Tutorial erklären wir Ihnen, wie Sie die neueste Version von MariaDB auf einem CentOS 8-Server installieren.
Wenn Sie sich über MySQL im Vergleich zu MariaDB wundern: MariaDB ist das bevorzugte Paket und sollte nahtlos anstelle von MySQL funktionieren.
Wenn Sie MySQL spezifisch benötigen, lesen Sie den Leitfaden Installieren von MySQL unter CentOS 8.
Um diesem Tutorial folgen zu können, benötigen Sie einen CentOS 8-Server mit einem Nicht-root sudo-fähigen Benutzer.
Weitere Informationen über die Einrichtung eines Benutzers mit diesen Berechtigungen finden Sie im Leitfaden Ersteinrichtung des Servers unter CentOS 8.
Verwenden Sie zunächst dnf zur Installation des Pakets MariaDB:
Sie werden aufgefordert, die Aktion zu bestätigen.
Drücken Sie y und dann ENTER, um fortzufahren.
Sobald die Installation abgeschlossen ist, starten Sie den Dienst mit systemctl:
Überprüfen Sie dann den Status des Dienstes:
Wenn MariaDB erfolgreich gestartet wurde, sollte die Ausgabe active (running) zeigen und die letzte Zeile sollte in etwa so aussehen:
Nehmen wir uns als Nächstes einen Moment Zeit, um sicherzustellen, dass MariaDB beim Booten startet, indem wir den Befehl systemctl enable verwenden:
MariaDB wird nun ausgeführt und ist konfiguriert, um beim Hochfahren zu starten.
Als Nächstes wenden wir unsere Aufmerksamkeit der Sicherung unserer Installation zu.
Schritt 2 - Sichern des MariaDB-Servers
MariaDB enthält ein Sicherheitsskript, um einige der weniger sicheren Standardoptionen für Dinge wie Remote-root-Logins und Beispielbenutzer zu ändern.
Verwenden Sie diesen Befehl zur Ausführung des Sicherheitsskripts:
Das Skript bietet eine detaillierte Erklärung für jeden Schritt.
Der erste Schritt fragt nach dem root-Passwort, das noch nicht festgelegt wurde, sodass wir wie empfohlen ENTER drücken.
Im Folgenden werden wir aufgefordert, dieses root-Passwort festzulegen.
Denken Sie daran, dass dies für den root-Datenbankbenutzer gilt, nicht den root user Ihres CentOS-Servers selbst.
Geben Sie Y und dann ENTER ein, um ein Passwort für den root-Datenbankbenutzer einzugeben und folgen Sie dann der Eingabeaufforderung.
Nach der Aktualisierung des Passworts akzeptieren wir alle folgenden Sicherheitsvorschläge, indem wir y und dann ENTER drücken.
Dadurch werden anonyme Benutzer entfernt, die Fernanmeldung als root user verhindert, die Testdatenbank entfernt und die Berechtigungstabelle neu geladen.
Nachdem wir nun die Installation gesichert haben, überprüfen wir ihre Funktionalität, indem wir uns mit der Datenbank verbinden.
Schritt 3 - Testen der Installation
Wir können unsere Installation überprüfen und Informationen darüber erhalten, indem wir uns mit dem Tool mysqladmin, einem Client, mit dem Sie administrative Befehle ausführen können, verbinden.
Verwenden Sie den folgenden Befehl, um sich als root (-u root) mit MariaDB zu verbinden, zur Eingabe eines Passworts (-p) aufgefordert zu werden und die Version der Installation ausgegeben zu bekommen.
Sie sollten eine ähnliche Ausgabe wie diese sehen:
Dadurch wird angezeigt, dass die Installation erfolgreich war.
Vielleicht möchten Sie Datenbanken importieren und exportieren
Sie könnten MariaDB in einen größeren Softwarestack wie beispielsweise den LAMP-Stack einbinden: Installieren von Linux, Apache, MariaDB, PHP (LAMP-Stack) unter CentOS7
Möglicherweise müssen Sie Ihre firewalld-Firewall aktualisieren, um externen Datenbankverkehr zuzulassen
Installieren des Ampache Music Streaming-Servers unter Ubuntu 18.04
4023
Der Autor hat den Open Internet / Free Speech Fund dazu ausgewählt, eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
Ampache ist ein Open-Source Musik-Streaming-Server, der es Ihnen ermöglicht, Ihre digitale Musiksammlung auf Ihrem eigenen Server zu hosten und zu verwalten.
Ampache kann Ihre Musik an Ihren Computer, Smartphone, Tablet oder Smart TV streamen.
Das bedeutet, dass Sie keine Mehrfachkopien Ihrer Musik auf dem Gerät, das Sie zum Anhören verwenden möchten, verwalten müssen. Mit Ampache können Sie Ihre Sammlung auf Ihrem Server über die Webschnittstelle von Ampache verwalten und an jedem beliebigen Ort anhören.
In diesem Tutorial werden Sie den Apache-Webserver und PHP installieren und konfigurieren, die Ihre Ampache-Instanz bedienen werden.
Anschließend erstellen Sie eine MySQL-Datenbank, die Ampache zur Speicherung aller Betriebsinformationen verwendet.
Abschließend werden Sie Ihre Musiksammlung hochladen, damit Sie mit dem Streaming Ihrer Musik beginnen können.
Einen Ubuntu-18.04-Server, der gemäß Ersteinrichtung eines Servers mit Ubuntu 18.04 eingerichtet wurde, einschließlich eines non-root sudo-Benutzers.
Auf Ihrem Server installiertes Apache, MySQL und PHP, wie in Installieren des Linux-, Apache-, MySQL-, PHP (LAMP) -Stacks unter Ubuntu 18.04 gezeigt.
Sie können lernen, wie Sie Domänen auf DigitalOcean Droplets verweisen, indem Sie dem Tutorial Einrichten eines Hostnamens mit DigitalOcean folgen.
Schritt 1 - Installieren von Ampache
In diesem Schritt laden Sie das Ampache-ZIP-Archiv auf Ihren Server herunter, entpacken es in das Home-Verzeichnis der Installation und nehmen einige notwendige Änderungen am Dateisystem vor.
Melden Sie sich zunächst als non-root user an Ihrem Server an.
Erstellen Sie dann das Home-Verzeichnis von Ampache mit dem folgenden Befehl:
Installieren Sie als Nächstes das Dienstprogramm zip, das Sie zum Entpacken des Ampache-Archivs benötigen:
Laden Sie anschließend das ZIP-Archiv der neuesten Version herunter.
Den Link zu der neuesten Version finden Sie au der GitHub-Seite von Ampache.
In diesen Beispielen wird Version < ^ > 4.1.1 < ^ > verwendet:
Entpacken Sie dann das ZIP-Archiv in das Verzeichnis / var / www / ampache / mit der Option -d:
Legen Sie danach die Benutzer- und Gruppenidentität der Dateien von Ampache fest, sodass Apache die Ampache Instanzdateien lesen, schreiben und ausführen kann:
Die Option --recursive veranlasst chown, die Eigentümerschaft und die Gruppenidentität aller Dateien und des Unterverzeichnisses unter / var / www / ampache / auf die Benutzer und Gruppen www-data von Apache zu ändern.
Benennen Sie anschließend die .htaccess-Dateien, die im ZIP-Archiv enthalten sind, um. .htaccess-Dateien enthalten Sicherheits- und andere Betriebsinformationen für Apache, aber sie funktionieren nur mit der Dateierweiterung .htaccess.
Benennen Sie die bereitgestellte .htaccess.dist mit den folgenden Befehlen in .htaccess um:
Jetzt erstellen Sie das Verzeichnis, das Ihre Musikdateien enthalten wird.
Aus Sicherheitsgründen wird dieses Verzeichnis am besten außerhalb des Installationsverzeichnisses von Ampache angelegt.
Auf diese Weise kann es nicht von böswilligen Web-Abfragen gelesen oder geändert werden, da es sich außerhalb des DocumentRoot von Ampache befindet.
Erstellen Sie das Verzeichnis, / data / Musik, mit dem folgenden Befehl:
Ändern Sie dann seinen Eigentümer und die Gruppenidentität, damit Apache es lesen und darin schreiben kann:
Um die Einrichtung abzuschließen, installieren Sie FFmpeg, ein Dienstprogramm, das Audio und Video von einem Format in ein anderes konvertiert.
Sie könnten es zum Beispiel verwenden, um eine MP3-Musikdatei in eine OPUS-Musikdatei zu konvertieren.
Ampache verwendet FFmpeg, um Audio ohne Vorbereitung aus dem Format, in dem es hochgeladen wurde, in ein Format zu konvertieren, das das Wiedergabegerät abspielen kann.
Dies ist ein Vorgang, der als Transkodierung bekannt ist.
Dies ist nützlich, da nicht alle Geräte alle Musikformate wiedergeben können.
Ampache kann erkennen, welche Formate auf dem Wiedergabegerät unterstützt werden, und Ihre Musik automatisch in dem unterstützten Format bereitstellen.
Installieren Sie FFmpeg mit dem folgenden Befehl:
Sie haben nun Ihre Ampache-Instanz entpackt und für das Web-Installationsprogramm vorbereitet sowie das Dienstprogramm FFmpeg installiert.
Als Nächstes konfigurieren Sie Apache und PHP, um Ihre Ampache-Instanz zu bedienen.
Schritt 2 - Konfigurieren von Apache und PHP
In diesem Abschnitt konfigurieren Sie Apache, indem Sie eine neue VirtualHost-Datei erstellen, die die Konfiguration bereitstellt, die Apache zur Bedienung Ihrer Ampache-Serverdomäne benötigt.
Sie werden auch einige zusätzliche PHP-Module installieren, die die Fähigkeiten von PHP erweitern, um die Anforderungen von Ampache abzudecken.
Installieren Sie zunächst einige zusätzliche PHP-Module, die nicht in der Standardinstallation von PHP enthalten waren:
Diese Module bieten die folgenden zusätzlichen Funktionen:
php-mysql - Ermöglicht PHP die Kommunikation mit einer MySQL-Datenbank.
php-curl -- Ermöglicht PHP die Verwendung des Dienstprogramms curl zum Herunterladen von Dateien, wie beispielsweise Album-Cover-Kunst, von entfernten Servern.
php-json - Ermöglicht PHP, JSON-formatierte Textdateien zu lesen und zu bearbeiten.
php-gd - Ermöglicht PHP die Bearbeitung und Erstellung von Bilddateien.
php7.2-xml - Ermöglicht PHP das Lesen und Bearbeiten von XML-formatierten Textdateien.
Aktivieren Sie anschließend einige Apache-Module mit dem Dienstprogramm a2enmod:
Diese Apache-Module ermöglichen Apache Folgendes:
rewrite - Modifizieren oder Neuschreiben von URLs nach den von Ampache gelieferten Regeln.
expires - Einstellen der Cache-Verfallszeiten für Objekte wie Bilder, damit diese von den Browsern effizienter gespeichert werden.
Nun werden Sie die VirtualHost-Datei erstellen, die Apache mitteilt, wie und wo die Ampache-Instanz geladen werden soll.
< $> note Anmerkung: Wenn Sie eine Testdatei VirutalHost erstellt haben, die Ihren Ampache-Domänennamen verwendet, als Sie dem Leitfaden "Installieren eines Linux-, Apache-, MySQL-, PHP (LAMP) -Stack unter Ubuntu 18.04" gefolgt sind, müssen Sie dises deaktivieren. Verwenden Sie zum Deaktiveren das Dienstprogramm a2dissite:
Erstellen und öffnen Sie nun die VirtualHost-Datei mit Ihrem Texteditor unter / etc / apache2 / sites-available / ampache.conf:
Fügen Sie die folgende VirtualHost-Vorlage in Ihre Datei ein:
Sie müssen < ^ > your _ domain < ^ > in den Domänennamen ändern, den Sie auf Ihren Server umgeleitet haben.
Wenn Sie die Bearbeitung dieser Datei abgeschlossen haben, speichern Sie sie und verlassen Sie den Editor.
Die Anweisungen in dieser VirtualHost-Datei lauten:
ServerName - Der Domänenname, den Apache zur Bedienung Ihrer Ampache-Instanz verwendet.
DocumentRoot - Der Speicherort des Dateisystems auf Ihrem Server, an dem sich die Ampache-Instanz befindet.
Dies ist der gleiche Ort, in den Sie in Schritt 1 das ZIP-Archiv entpackt haben.
Directory - Dieser Abschnitt übergibt eine Konfiguration an Apache, die für die im Pfad enthaltenen Dateien und Verzeichnisse gilt.
RewriteEngine - Aktiviert das Apache-Modul zum Neuschreiben von Dateien.
CustomLog - Erstellt eine Protokolldatei, die Apache zur Aufzeichnung aller Zugriffsprotokolle für Ihren Ampache-Server verwendet.
ErrorLog - Erstellt eine Protokolldatei, die Apache zur Aufzeichnung aller von Ihrem Ampache-Server erzeugten Fehlerprotokolle verwendet.
Prüfen Sie anschließend mit dem Dienstprogramm apachectl, ob die von Ihnen erstellte VirtualHost-Datei fehlerfrei ist:
Wenn Ihre Konfiguration keine Fehler enthält, sehen Sie nach dem Befehl die folgende Ausgabe:
Wenn Ihre Konfiguration Fehler enthält, werden in der Ausgabe der Dateiname und die Zeilennummer, in der der Fehler aufgetreten ist, ausgegeben.
< $> note Anmerkung: Wenn Sie den Fehler sehen:
Dann müssen Sie die Hauptkonfigurationsdatei von Apache unter / etc / apache2 / apache2.conf bearbeiten und die folgende Zeile hinzufügen:
Aktivieren Sie anschließend die neue VirtualHost-Konfiguration mit dem Dienstprogramm a2ensite:
Die endgültige Konfiguration ist optional, wird jedoch empfohlen.
Die Standardeinstellung für Datei-Uploads in PHP ist, dass Dateien, die größer als 2 MB sind, nicht hochgeladen werden können.
Musikdateien neigen dazu, größer zu sein. Wenn Sie diesen Wert erhöhen, können Sie die Ampache-Schnittstelle verwenden, um größere Dateien in Ihrer Musiksammlung hochzuladen.
Öffnen Sie / etc / 7.2 / 7.2 / apache2 / php.ini mit dem folgenden Befehl:
Und ändern Sie die folgenden Zeilen:
Zu:
Sie können nun Musikdateien bis zu 100 MB hochladen.
Verwenden Sie einen größeren Wert, wenn Sie beabsichtigen, Dateien mit darüber hinausgehender Größe hochzuladen.
Laden Sie abschließend Ihre aktualisierte Apache-Konfiguration neu:
Sie haben Apache jetzt so konfiguriert, dass Ampache über HTTP bedient wird.
Als Nächstes erhalten Sie ein TLS-Zertifikat und konfigurieren Apache zur Verwendung desselben, sodass Sie über HTTPS sicher auf Ampache zugreifen können.
Schritt 3 - Aktivieren von HTTPS
In diesem Schritt erhalten Sie mit dem Dienstprogramm Certbot ein kostenloses Let "s Encrypt TLS-Zertifikat, das HTTPS ermöglicht.
Certbot erstellt das Zertifikat, generiert automatisch die erforderliche Apache-Konfiguration und verwaltet die automatische Erneuerung des Zertifikats.
Dies ist wichtig, denn Sie senden bei jeder Anmeldung bei Ampache Ihren Benutzernamen und Ihr Passwort über das Internet.
Wenn Sie kein HTTPS verwenden, wird Ihr Passwort im Klartext verschickt, der auf seinem Weg durch das Internet gelesen werden kann.
Die LTS-Veröffentlichungen von Ubuntu neigen dazu, nicht über die aktuellsten Pakete zu verfügen, und dies trifft auch auf das Certbot-Programm zu
Die Certbot-Entwickler unterhalten ein dediziertes Ubuntu-Repository, das als PPA bekannt ist, sodass Ubuntu-Benutzer eine aktuelle Kopie des Certbot verwenden können.
Installieren Sie das Certbot-Repository mit dem folgenden Befehl:
Installieren Sie nun das Dienstprogramm certbot:
Verwenden Sie anschließend certbot, um das TLS-Zertifikat zu erhalten:
Die Option --apache verwendet das Apache-Plugin, das es Certbot ermöglicht, Apache automatisch zu lesen und zu konfigurieren. -d < ^ > your _ domain < ^ > gibt den Domänennamen an, für den Cerbot das Zertifikat erstellen soll.
Wenn Sie den Befehl certbot ausführen, werden Ihnen eine Reihe von Fragen gestellt.
Sie werden zur Eingabe einer E-Mail-Adresse und der Zustimmung zu den Nutzungsbedingungen aufgefordert.
Wenn certbot erfolgreich bestätigt, dass Sie Ihre Domäne kontrollieren, werden Sie aufgefordert, Ihre HTTPS-Einstellungen zu konfigurieren:
No redirect: Apache bedient Ampache über HTTP und HTTPS.
Redirect: Apache leitet alle HTTP-Verbindungen automatisch zu HTTPS um.
Das bedeutet, dass Ihr Ampache-Server nur über HTTPS verfügbar ist.
Diese Option ist die sicherere und hat keinen Einfluss darauf, wie sich Ihre Ampache-Instanz verhält.
Dies ist die empfohlene Wahl.
Testen Sie abschließend, ob die automatische Erneuerung des Zertifikats erfolgreich abläuft, indem Sie den folgenden Befehl ausführen:
Die Option --dry-run bedeutet, dass certbot einen Erneuerungsversuch testet, ohne dauerhafte Änderungen an Ihrem Server vorzunehmen.
Wenn der Test erfolgreich war, enthält die Ausgabe die folgende Zeile:
Apache und PHP können nun Ihre Ampache-Instanz bedienen.
Im nächsten Schritt erstellen und konfigurieren Sie die Datenbank von Ampache.
Schritt 4 - Erstellen einer MySQL-Datenbank
Ampache verwendet eine MySQL-Datenbank, um Informationen wie Wiedergabelisten, Benutzereinstellungen usw. zu speichern.
In diesem Schritt erstellen Sie eine Datenbank und einen MySQL-Benutzer, mit denen Ampache auf diese Datenbank zugreift.
Sie müssen drei Informationen auswählen, um die folgenden Anweisungen zur Erstellung der Datenbank von Ampache auszuführen:
< ^ > ampache _ database < ^ >: Der Name der Ampache-Datenbank.
< ^ > database _ user < ^ >: Der MySQL-Benutzer, den Ampache für den Zugriff auf die Datenbank verwenden wird.
Dieser ist kein Systembenutzer er kann nur auf die Datenbank zugreifen.
< ^ > database _ password < ^ >: Das Passwort des Datenbankbenutzers.
Achten Sie darauf, ein sicheres Passwort zu wählen.
Notieren Sie sich diese Angaben, da Sie sie später benötigen werden.
Öffnen Sie zunächst die interaktive MySQL-Shell mit dem Befehl mysql:
--user = root öffnet die MySQL-Shell als root user von MySQL und --password fordert zur Eingabe des Passworts des root users auf.
Der folgende Befehl erstellt eine leere Datenbank:
Erstellen Sie anschließend den MySQL-Benutzer:
Geben Sie dem neuen Benutzer nun vollen Zugriff auf die Datenbank:
Überprüfen Sie abschließend, ob die neue Datenbank vorhanden ist, indem Sie den folgenden Befehl ausführen:
Verlassen Sie die MySQL-Shell durch Eingabe von exit;.
Testen Sie abschließend die Datenbank, den Benutzernamen und das Passwort, indem Sie versuchen, sich mit dem < ^ > database _ user < ^ > an der MySQL-Shell anzumelden.
Geben Sie den folgenden Befehl ein, um sich als neuer Benutzer an der MySQL-Shell anzumelden:
Sie haben nun die Datenbank erstellt, die Ampache verwenden wird.
Sie haben Ihre Serverkonfiguration abgeschlossen und sind bereit, die Installation mit dem Webinstaller abzuschließen.
Schritt 5 - Verwenden des Webinstallers
In diesem Schritt verwenden Sie den Webinstaller von Ampache, um die Installation abzuschließen, indem Ampache die Informationen geben, die es zum Ausführen benötigt, wie z. B. einen Weboberflächen-Adminbenutzer, die Datenbankdetails und andere Einstellungen.
Beginnen Sie die Web-Installation, indem Sie https: / / < ^ > your _ domain < ^ > in Ihren Browser eingeben.
Wählen Sie die Installationssprache
Wählen Sie die Sprache der Benutzeroberfläche von Ampache und klicken Sie auf die Schaltfläche Start Configuration, um fortzufahren.
Anforderungen
Auf dieser Seite überprüft Ampache, ob der Server die Anforderungen erfüllt.
Jede Zeile auf dieser Seite stellt einen Test dar, den das Installationsprogramm durchführt, um sicherzustellen, dass z. B. alle erforderlichen PHP-Module vorhanden sind und funktionieren.
Sie werden sehen, dass jeder Test mit einem grünen Häkchen versehen ist, um anzuzeigen, dass Ihr Server für Ampache bereit ist.
Klicken Sie auf die Schaltfläche Continue, um zur nächsten Seite zu gelangen.
Einfügen der Ampache-Datenbank (Insert Ampache Database)
Diese Seite erstellt die Datenbank von Ampache, falls sie nicht existiert, und formatiert sie. Füllen Sie die Felder wie folgt aus:
Desired Database Name: < ^ > ampache _ database < ^ >
MySQL Hostname: localhost
MySQL Port (optional):
MySQL Administrative Username: < ^ > database _ user < ^ >
MySQL Administrative Password: < ^ > database _ password < ^ >
Create Database:
Create Tables (ampache.sql):
Create Database User:
Bild des ausgefüllten Formulars
Klicken Sie auf die Schaltfläche Insert Database, um fortzufahren.
Erstellen der Konfigurationsdatei (Generate Configuration File)
Diese Seite erstellt die Konfigurationsdatei, die Ampache zur Ausführung verwenden wird.
Füllen Sie die Felder wie folgt aus:
Web Path:
Database Name: < ^ > ampache _ database < ^ >
MySQL Username: < ^ > database _ user < ^ >
MySQL Password: < ^ > database _ password < ^ >
Installation Type Belassen Sie diese bei der Standardeinstellung.
Allow Transcoding Wählen Sie aus der Dropdown-iste ffmpeg aus.
Players Belassen sie diese bei den Standardeinstellungen.
Klicken Sie auf die Schaltfläche Create Config, um fortzufahren.
Erstellen des Admin-Kontos (Create Admin Account)
Diese Seite erstellt den ersten Benutzer der Weboberfläche.
Dieser Benutzer wird mit vollen administrativen Berechtigungen erstellt und ist derjenige, mit dem Sie sich zum ersten Mal anmelden und Ampache konfigurieren werden.
Wählen Sie einen Benutzernamen und ein sicheres Passwort und geben Sie es in die Felder Password und Confirm Password ein.
Klicken Sie auf die Schaltfläche Create Account, um fortzufahren.
Ampache Update
Auf dieser Seite werden alle administrativen Änderungen an der Datenbank von Ampache durchgeführt, die vorgenommen werden müssen.
Diese Änderungen werden bei Versions-Upgrades vorgenommen, aber da es sich um eine Neuinstallation handelt, wird das Installationsprogramm keine Änderungen vornehmen.
Klicken Sie auf die Schaltfläche Update Now!,
um fortzufahren.
Diese Seite zeigt und erklärt alle Aktualisierungen, die das Installationsprogramm im vorherigen Schritt vorgenommen hat.
Sie sollten keine Aktualisierungen aufgelistet sehen.
Klicken Sie auf den Link Return to main page (Zurück zur Hauptseite), um mit der Anmeldeseite fortzufahren.
Geben Sie Ihren Benutzernamen und Ihr Passwort ein, das Sie für die Anmeldung bei Ihrem Ampache-Server festgelegt haben.
Ampache ist noch nicht ganz fertig eingerichtet und nicht ganz einsatzbereit.
Sie schließen nun die Einrichtung ab, indem Sie Ihre Musik hinzufügen, damit Sie Ihren neuen Ampache-Server nutzen können.
Schritt 6 - Hinzufügen Ihrer Musik zu Ampache
Ein Musikserver nützt nichts, wenn keine Musik zur Wiedergabe vorhanden ist.
In diesem Schritt werden Sie einen Musikkatalog konfigurieren und Musik hochladen.
"Catalog" ist der Name, den Ampache einer Musiksammlung gibt.
Ampache kann Musik aus vielen Quellen sowohl auf dem Server als auch außerhalb des Servers lesen, aber in diesem Tutorial werden Sie Ihre Musik auf den Server hochladen und in einem lokalen Katalog, wie Ampache ihn bezeichnet, speichern.
Klicken Sie zunächst auf den Link add a Catalog in der folgenden Zeile auf der ersten Seite, die Sie sehen, wenn Sie sich bei Ampache anmelden.
No Catalog configured yet.
To start streaming your media, you now need to add a Catalog.
Dadurch gelangen Sie zu der Seite Add Catalog.
Catalog Name: Geben Sie dem Katalog einen kurzen, einprägsamen Namen.
Catalog Type: local
Filename Pattern:
Folder Pattern: Belassen Sie dies mit den Standardeinstellungen.
Gather Art:
Build Playlists from Playlist Files. (m3u, m3u8, asx, pls, xspf):
Path: / data / Music
Bild des ausgefüllten Formulars Katalog hinzufügen
Klicken Sie auf die Schaltfläche Add Catalog, um diese Seite abzuschließen.
Auf der folgenden Seite klicken Sie auf die Schaltfläche Continue.
Sie gelangen auf die Seite Show Catalogs, auf der die Details zu dem von Ihnen erstellten Katalog angezeigt werden.
Sie können die Katalogeinstellungen jederzeit später ändern.
Aktivieren Sie nun die Web-Upload-Funktion von Ampache, indem Sie auf das dritte Navigationssymbol klicken, um die erweiterten Einstellungen zu öffnen:
Bild des dritten Einstellungssymbols
Scrollen Sie nach unten zu dem Abschnitt Server Config und klicken Sie auf den Link System, um die Seite mit den Systemeinstellungen zu öffnen.
Suchen Sie die Zeile Allow user uploads und wählen Sie Enable aus dem Dropdown-Menü in der Spalte Value.
Sie können auch wählen, welche Benutzerebene Musikdateien hochladen darf.
Die Standardebene ist Catalog Manager, die es dem Catalog Manager und allen Benutzern mit höheren Berechtigungen ermöglicht, Musik hochzuladen.
In diesem Fall ist das der Administrator.
Sie müssen auch den Katalog festlegen, zu dem die Musik hinzugefügt werden soll.
Legen Sie dies mit der Zeile Destination catalog fest.
Wählen Sie in der Dropdown-Liste Value den von Ihnen erstellten Katalog aus.
Bild mit Darstellung der Zeilen "Update erlauben" und "Zielkatalog"
Klicken Sie am unteren Ende der Seite auf die Schaltfläche Update Preferences, um die Konfiguration abzuschließen.
Sie können nun Musik hochladen.
Klicken Sie zunächst auf das erste Einstellungensymbol:
Symbol zeigt das erste Einstellungensymbol
Klicken Sie dann im Abschnitt Music auf den Link Upload.
Bild zeigt Link zum Upload
Klicken Sie auf der Seite Upload die Schaltfläche Browse und suchen Sie Ihre Musikdateien auf Ihrem Computer und laden Sie sie hoch.
Wenn Sie die Angaben Artist und Album leer lassen, liest Ampache die ID3-Tags der Musikdateien, um den Künstler und das Album automatisch zu ermitteln.
Nachdem Sie einige Musikdateien hochgeladen haben, können Sie diese nun finden, indem Sie auf die Links Songs, Albums oder Artists im Abschnitt Music in der Navigationsleiste auf der linken Seite klicken.
Ihr Ampache-Musikserver ist nun bereit, mit dem Streaming Ihrer Musik zu beginnen.
In diesem Artikel haben Sie einen Ampache-Musik-Streaming-Server installiert und konfiguriert und einen Teil Ihrer Musik hochgeladen.
Jetzt können Sie Ihre Musik überall auf jedem Ihrer Geräte wiedergeben.
Die Dokumentation von Ampache hilft Ihnen bei der Verwendung und Erweiterung Ihres Streaming-Servers.
Diese Android-Apps und diese iOS-App werden Ihre Musik auf Ihr Handy streamen.
Ampache organisiert Ihre Musik auf dem Server anhand der ID3-Tags in den Musikdateien.
Das Programm MusicMrainz hilft Ihnen bei der Verwaltung der ID3-Tags Ihrer Musikdateien.
Erstellen von benutzerdefinierten Komponenten in React
4101
Der Autor hat Creative Commons dazu ausgewählt, im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
In diesem Tutorial lernen Sie, wie Sie in React benutzerdefinierte Komponenten erstellen können.
Komponenten sind voneinander unabhängige Funktionselemente, die Sie in Ihrer Anwendung wiederverwenden können, und stellen die Grundbausteine aller React-Anwendungen dar.
Häufig können sie aus einfachen JavaScript-Funktionen und -Klassen bestehen, aber Sie verwenden sie so, als wären sie angepasste HTML-Elemente.
Schaltflächen, Menüs und alle anderen Frontend-Seiteninhalte können als Komponenten erstellt werden.
Komponenten können auch Statusinformationen und Display-Markdown enthalten.
Nach dem Erlernen der Erstellung von Komponenten in React können Sie komplexe Anwendungen in kleine Teile aufteilen, die sich leichter einrichten und pflegen lassen.
In diesem Tutorial erstellen Sie eine Liste von Emojis, deren Namen angezeigt werden, wenn Sie darauf klicken.
Die Emojis werden unter Verwendung einer benutzerdefinierten Komponente erstellt und aus einer anderen benutzerdefinierten Komponente heraus aufgerufen.
Am Ende dieses Tutorials haben Sie mithilfe von JavaScript-Klassen und JavaScript-Funktionen benutzerdefinierte Komponenten erstellt und wissen, wie Sie bestehenden Code in wiederverwendbare Stücke aufteilen und die Komponenten in einer lesbaren Dateistruktur speichern können.
Sie benötigen eine Entwicklungsumgebung, die Node.js ausführt; dieses Tutorial wurde mit der Node.js-Version 10.20.1 und der npm-Version 6.14.4 getestet.
Um dies unter MacOS oder Ubuntu 18.04 zu installieren, folgen Sie den Schritten in Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung unter MacOS oder dem Abschnitt Installieren unter Verwendung eines PPA in Installieren von Node.js unter Ubuntu 18.04.
Sie müssen Anwendungen mit Create React App erstellen können.
Anweisungen zur Installation einer Anwendung mit Create React App finden Sie unter Einrichten eines React-Projekts mit der Create React App ein.
Sie werden JSX-Syntax verwenden, über die Sie in unserem Tutorial Erstellen von Elementen mit JSX mehr erfahren können.
Sie benötigen auch Grundkenntnisse über JavaScript, die Sie in Codieren in JavaScript finden, sowie Grundkenntnisse über HTML und CSS.
Eine gute Ressource für HTML und CSS ist das Mozilla Developer Network.
Schritt 1 - Einrichten des React-Projekts
In diesem Schritt erstellen Sie unter Verwendung von Create React App eine Grundlage für Ihr Projekt.
Sie werden das Standardprojekt ändern, um Ihr Basisprojekt zu erstellen, indem Sie ein Mapping über eine Liste von Emojis vornehmen und etwas Styling hinzufügen.
Erstellen Sie zuerst ein neues Projekt.
Öffnen Sie ein Terminal und führen Sie dann den folgenden Befehl aus:
Wechseln Sie danach in das Projektverzeichnis:
Öffnen Sie den App.js-Code in einem Texteditor:
Erstellen Sie als Nächstes den von Create React App erstellten Vorlagencode und ersetzen Sie dann den Inhalt durch neuen React-Code, der eine Liste von Emojis anzeigt:
Dieser Code verwendet die Syntax von JSX, um mithilfe von map () über das emojis array zu mappen und sie als < li > -Listenelemente aufzulisten.
Außerdem fügt er onClick-Ereignisse an, um Emoji-Daten im Browser anzuzeigen.
Um den Code näher kennenzulernen, konsultieren Sie Erstellen von React-Elementen mit JSX, wo Sie eine detaillierte Erklärung zu JSX finden.
Sie können die Datei logo.svg nun löschen, da sie Teil der Vorlage war und Sie nicht mehr auf sie verweisen:
Aktualisieren Sie nun das Styling.
Öffnen Sie src / App.css:
Ersetzen Sie den Inhalt mit dem folgenden CSS, um die Elemente zu zentrieren und die Schrift anzupassen:
Dabei wird flex zum Zentrieren des Hauptelements < h1 > und der Listenelemente verwendet.
Außerdem werden standardmäßige Schaltflächenstile und < li > -Stile entfernt, damit die Emojis in einer Reihe angezeigt werden.
Weitere Informationen finden Sie unter Erstellen von React-Elementen mit JSX.
Öffnen Sie ein anderes Terminalfenster im Stammverzeichnis Ihres Projekts.
Starten Sie das Projekt mit dem folgenden Befehl:
Nach der Ausführung des Befehls sehen Sie, dass das Projekt in Ihrem Webbrowser unter http: / / localhost: 3000 ausgeführt wird.
Lassen Sie dies die ganze Zeit, die Sie an Ihrem Projekt arbeiten, laufen.
Bei jeder Speicherung des Projekts wird der Browser automatisch aktualisiert und zeigt den aktuellsten Code an.
Sie werden Ihre Projektseite mit Hello, World und den drei Emojis sehen, die Sie in Ihrer App.js-Datei aufgeführt haben:
Browser mit Emoji
Nachdem Sie Ihren Code eingerichtet haben, können Sie nun mit der Zusammenstellung von Komponenten in React beginnen.
Schritt 2 - Erstellen einer unabhängigen Komponente mit React-Klassen
Nachdem Ihr Projekt nun ausgeführt wird, können Sie damit beginnen, Ihre benutzerdefinierte Komponente zu erstellen.
In diesem Schritt erstellen Sie eine unabhängige React-Komponente, indem Sie die grundlegende React Component-Klasse erweitern.
Sie werden eine neue Klasse erstellen, Methoden hinzufügen und die Renderingfunktion zum Anzeigen von Daten verwenden.
React-Komponenten sind in sich geschlossene Elementen, die Sie in allen Bereichen einer Seite wiederverwenden können.
Indem Sie kleine, fokussierte Codestücke erstellen, können Sie Stücke verschieben und wiederverwenden, wenn Ihre Anwendung wächst.
Das Entscheidende ist, dass sie in sich geschlossen und fokussiert sind, sodass Sie Code in logische Teile trennen können.
Sie haben in Wahrheit bereits mit logisch getrennten Komponenten gearbeitet: Die App.js-Datei ist eine funktionelle Komponente, über die Sie in Schritt 3 mehr erfahren werden.
Es gibt zwei Arten von benutzerdefinierten Komponenten: klassenbasiert und funktional.
Die erste Komponente, die Sie erstellen werden, ist eine klassenbasierte Komponente.
Sie werden eine neue Komponente namens Instructions erstellen, die die Anweisungen für den Emoji-Viewer erklärt.
< $> note Anmerkung: Klassenbasierte Komponenten waren einmal die beliebteste Möglichkeit zur Erstellung von React-Komponenten.
Seit der Einführung von React Hooks wechseln jedoch viele Entwickler und Bibliotheken zur Verwendung funktionaler Komponenten.
Obwohl funktionale Komponenten inzwischen die Norm sind, werden Sie in älterem Code oft Klassenkomponenten finden.
Sie müssen sie nicht verwenden, aber wissen, wie Sie sie erkennen können.
Außerdem liefern sie eine verständliche Einführung in viele zukünftige Konzepte, wie z. B. Zustandsverwaltung.
In diesem Tutorial lernen Sie, sowohl Klassen- als auch Funktionskomponenten einzurichten.
Erstellen Sie zuerst eine neue Datei.
Der Konvention nach werden Komponentendateien großgeschrieben:
Öffnen Sie die Datei dann in Ihrem Texteditor:
Importieren Sie zuerst React und die Component-Klasse und exportieren Sie Instructions mit den folgenden Zeilen:
Durch Importieren von React wird die JSX konvertiert.
Component ist eine Basisklasse, die Sie zur Erstellung Ihrer Komponente erweitern werden.
Dazu haben Sie eine Klasse erstellt, die den Namen Ihrer Komponente (Instructions) trägt, und die Basis-Component mit der Zeile export erweitert.
Außerdem exportieren Sie diese Klasse mit export default am Anfang der Klassendeklaration als Standard.
Der Klassenname sollte großgeschrieben sein und mit dem Namen der Datei übereinstimmen.
Das ist wichtig, wenn Sie Debugging-Tools verwenden, die den Namen der Komponente anzeigen.
Wenn der Name mit der Dateistruktur übereinstimmt, ist es leichter, die entsprechende Komponente zu finden.
Die grundlegende Component-Klasse bietet verschiedene Methoden, die Sie in Ihrer benutzerdefinierten Klasse verwenden können.
Die wichtigste Methode und die einzige, die Sie in diesem Tutorial verwenden werden, ist die Methode render ().
Die Methode render () gibt den JSX-Code aus, den Sie im Browser anzeigen möchten.
Fügen Sie zuerst eine kleine Erklärung der App in einem < p > -Tag hinzu:
Zu diesem Zeitpunkt gibt es immer noch keine Änderung in Ihrem Browser.
Das liegt daran, dass Sie die neue Komponente noch nicht verwendet haben.
Um die Komponente zu nutzen, müssen Sie sie in eine weitere Komponente einfügen, die mit der root-Komponente verbunden ist.
In diesem Projekt ist < App > die Stammkomponente in index.js.
Um sie in Ihrer Anwendung sichtbar zu machen, müssen Sie sie der < App > -Komponente hinzufügen.
Öffnen Sie src / App.js in einem Texteditor:
Zuerst müssen Sie die Komponente importieren:
Da es sich dabei um den Standardimport handelt, können Sie mit jedem gewünschten Namen importieren.
Am besten ist es, die Namen für einfache Lesbarkeit konsistent zu halten: Der Import sollte mit dem Komponentennamen übereinstimmen, der mit dem Dateinamen übereinstimmen sollte. Die einzige feste Regel besteht darin, dass die Komponente mit einem Großbuchstaben beginnen muss.
So weiß React, dass es sich um eine React-Komponente handelt.
Nachdem Sie die Komponente importiert haben, fügen Sie sie jetzt zum Rest Ihres Codes hinzu, als wäre sie ein benutzerdefiniertes HTML-Element:
In diesem Code haben Sie die Komponente mit spitzen Klammern umschlossen.
Da diese Komponente keine untergeordneten Komponenten aufweist, kann sie sich selbst schließen, indem sie auf / > endet.
Wenn Sie dies tun, wird die Seite aktualisiert und Sie werden die neue Komponente sehen.
Browser mit Anweisungstext
Nachdem Sie über Text verfügen, können Sie nun ein Bild hinzufügen.
Laden Sie ein Emoji-Bild von Wikimedia herunter und speichern Sie es mit folgendem Befehl im Verzeichnis src als emoji.svg:
curl richtet die Anfrage an die URL und mit dem -o-Flag können Sie Datei als src / emoji.svg speichern.
Öffnen Sie anschließend Ihre Komponentendatei:
Importieren Sie das Emoji und fügen Sie es mit einem dynamischen Link Ihrer benutzerdefinierten Komponente hinzu:
Beachten Sie, dass Sie beim Importieren die Dateierweiterung .svg einfügen müssen.
Beim Import importieren Sie einen dynamischen Pfad, der von webpack bei der Kompilierung des Codes erstellt wird.
Weitere Informationen finden Sie unter Einrichten eines React-Projekts mit Create React App.
Außerdem müssen Sie die Tags < img > und < p > mit leeren Tags umschließen, um sicherzustellen, dass Sie ein einzelnes Element zurückgeben.
Wenn Sie neu laden, wird das Bild im Vergleich zum Rest des Inhalts sehr groß dargestellt:
Browserfenster mit großem Emoji-Bild
Um das Bild zu verkleinern, müssen Sie Ihrer benutzerdefinierten Komponente CSS und einen className hinzufügen.
Ändern Sie zuerst in Instructions.js die leeren Tags in ein div und vergeben Sie den className instructions:
Öffnen Sie als Nächstes App.css:
Erstellen Sie Regeln für den .instructions-Klassenselektor:
Wenn Sie ein display im Styling von flex hinzufügen, zentrieren Sie img und das p mit Flexbox.
Sie haben die Richtung geändert, sodass mit flex-direction: column; alles vertikal ausgerichtet ist.
Die Zeile align-items: center; wird die Elemente auf dem Bildschirm zentrieren.
Nachdem Ihre Elemente ausgerichtet sind, müssen Sie die Bildgröße ändern.
Geben Sie dem img innerhalb des div eine width (Breite) und height (Höhe) von 100px.
Der Browser wird neu geladen und Sie sehen, dass das Bild wesentlich kleiner ist:
Browserfenster mit kleinerem Bild
Nun haben Sie eine unabhängige und wiederverwendbare benutzerdefinierte Komponente erstellt.
Um zu sehen, wie sie sich wiederverwenden lässt, fügen Sie eine zweite Instanz zu App.js hinzu.
Fügen Sie in App.js eine zweite Instanz der Komponente hinzu:
Wenn der Browser neu geladen wird, sehen Sie die Komponente zweimal.
Browser mit zwei Instanzen der Instructions-Komponente
In diesem Fall benötigen Sie keine zwei Instanzen von Instructions, aber Sie können sehen, dass sich die Komponente effizient wiederverwenden lässt.
Wenn Sie benutzerdefinierte Schaltflächen oder Tabellen erstellen, verwenden Sie sie auf einer Seite wahrscheinlich mehrfach. Somit eignen sie sich perfekt für benutzerdefinierte Komponenten.
Zunächst können Sie das zusätzliche Bild-Tag löschen.
Löschen Sie in Ihrem Texteditor die zweiten < Instructions / > und speichern Sie die Datei:
Sie verfügen nun über eine wiederverwendbare, unabhängige Komponente, die Sie in einer übergeordneten Komponente mehrfach hinzufügen können.
Die Struktur, die Sie jetzt haben, funktioniert für eine kleine Anzahl von Komponenten; es gibt da aber ein kleines Problem.
Alle Dateien sind zusammengemischt.
Das Bild für < Instructions > befindet sich im gleichen Verzeichnis wie die Assets für < App >.
Außerdem vermischen Sie den CSS-Code für < App > mit dem CSS-Code für < Instructions >.
Im nächsten Schritt erstellen Sie eine Dateistruktur, die jeder Komponente Unabhängigkeit verleiht, indem ihre Funktionen, Stile und Abhängigkeiten gruppiert werden. Damit können Sie sie wie erforderlich verschieben.
Schritt 3 - Erstellen einer lesbaren Dateistruktur
In diesem Schritt erstellen Sie eine Dateistruktur, um Ihre Komponenten und ihre Assets (wie Bilder, CSS und andere JavaScript-Dateien) zu organisieren.
Sie werden Code nach Komponenten gruppieren, nicht nach Asset-Typ.
Anders ausgedrückt: Sie werden kein separates Verzeichnis für CSS, Bilder und JavaScript nutzen.
Stattdessen verfügen Sie über ein separates Verzeichnis für jede Komponente, das den entsprechenden CSS-Code, JavaScript und Bilder enthält.
In beiden Fällen sorgen Sie für eine Trennung von Bedenken.
Da Sie über eine unabhängige Komponente verfügen, benötigen Sie eine Dateistruktur, die den entsprechenden Code gruppiert.
Derzeit befindet sich alles im gleichen Verzeichnis.
Listen Sie die Elemente in Ihrem src-Verzeichnis auf:
Die Ausgabe zeigt, dass alles ziemlich durcheinander ist:
Sie sehen Code für die < App > -Komponente (App.css, App.js und App.test.js) neben Ihrer Stammkomponente (index.css und index.js) sowie Ihrer benutzerdefinierten Komponente Instructions.js.
React ist absichtlich agnostisch hinsichtlich der Dateistruktur.
React empfiehlt keine spezielle Struktur und das Projekt kann mit einer Vielzahl verschiedener Dateihierarchien arbeiten.
Wir empfehlen jedoch, für Ordnung zu sorgen, um ein Überladen Ihres Stammverzeichnisses mit Komponenten, CSS-Dateien und Bildern zu vermeiden. So können Sie die Navigation erleichtern.
Außerdem kann eine explizite Benennung leichter sichtbar machen, welche Teile Ihres Projekts verwandt sind.
Beispielsweise ist eine Bilddatei namens Logo.svg möglicherweise nicht eindeutig Teil einer Komponente namens Header.js.
Eine der einfachsten Strukturen besteht darin, ein Verzeichnis components mit einem separaten Verzeichnis für jede Komponente zu erstellen.
Dadurch können Sie Ihre Komponenten getrennt von Ihrem Konfigurationscode (wie z. B. serviceWorker) anordnen und die Assets mit den Komponenten gruppieren.
Erstellen eines Verzeichnisses namens Components
Erstellen Sie zuerst ein Verzeichnis namens Components:
Verschieben Sie als Nächstes die folgenden Komponenten und Code in das Verzeichnis: App.css, App.js, App.test.js, Instructions.js und emoji.svg:
Hier verwenden Sie einen Platzhalter (*) zur Auswahl aller mit App. beginnenden Dateien.
Nachdem Sie den Code verschoben haben, sehen Sie einen Fehler in Ihrem Terminal, in dem npm start ausgeführt wird.
Denken Sie daran, dass der gesamte Code unter Verwendung von relativen Pfaden importiert.
Wenn Sie den Pfad für einige Dateien ändern, müssen Sie den Code aktualisieren.
Dazu öffnen Sie index.js.
Ändern Sie dann den Pfad des App-Imports, um aus dem Verzeichnis components / zu importieren.
Ihr Skript wird die Änderungen erkennen und der Fehler verschwindet.
Nun verfügen Sie über Komponenten in einem separaten Verzeichnis.
Wenn Ihre Anwendungen komplexer werden, verfügen Sie möglicherweise über Verzeichnisse für API-Dienste, Datenspeicher und Dienstprogrammfunktionen.
Die Trennung von Komponentencode ist der erste Schritt, aber da gibt es noch CSS-Code für Instructions vermischt in der Datei App.css.
Um diese logische Trennung zu erstellen, werden Sie die Komponenten zuerst in separate Verzeichnisse verschieben.
Verschieben von Komponenten in einzelne Verzeichnisse
Erstellen Sie zuerst ein Verzeichnis speziell für die < App > -Komponente:
Verschieben Sie dann die verwandten Dateien in das neue Verzeichnis:
Wenn Sie das tun, erhalten Sie einen ähnlichen Fehler wie im letzten Abschnitt:
In diesem Fall müssen Sie zwei Dinge aktualisieren.
Zuerst müssen Sie den Pfad in index.js aktualisieren.
Öffnen Sie die Datei index.js:
Aktualisieren Sie dann den Importpfad für App, um auf die Komponente App im Verzeichnis App zu verweisen.
Die Anwendung wird immer noch nicht ausgeführt.
Ihnen wird ein Fehler angezeigt, der wie folgt aussieht:
Da sich < Instructions > nicht auf der gleichen Verzeichnisebene wie die < App > -Komponente befindet, müssen Sie den Importpfad ändern.
Erstellen Sie zuvor ein Verzeichnis für Instructions.
Erstellen Sie ein Verzeichnis namens Instructions im Verzeichnis src / components:
Verschieben Sie dann Instructions.js und emoji.svg in dieses Verzeichnis:
Nachdem das Komponentenverzeichnis Instructions nun erstellt wurde, können Sie die Aktualisierung der Dateipfade abschließen, um Ihre Komponente mit Ihrer App zu verbinden.
Aktualisieren von import-Pfaden
Nachdem sich die Komponenten in einzelnen Verzeichnissen befinden, können Sie nun den Importpfad in App.js anpassen.
Da der Pfad relativ ist, müssen Sie ein Verzeichnis -src / components - nach oben und dann in das Verzeichnis Instructions für Instructions.js gehen. Da dies jedoch eine JavaScript-Datei ist, benötigen Sie den letzten Import nicht.
Nachdem Ihre Importe nun den richtigen Pfad verwenden, wird Ihr Browser aktualisiert und zeigt die Anwendung an.
< $> note Anmerkung: Sie können die Stammdatei auch in jedem Verzeichnis-index.js aufrufen.
Anstelle von src / components / App / App.js könnten Sie beispielsweise src / components / App / index.js erstellen.
Der Vorteil dabei besteht darin, dass Ihre Importe etwas kleiner ausfallen.
Wenn der Pfad auf ein Verzeichnis verweist, wird der Import nach einer index.js-Datei suchen.
Der Import für src / components / App / index.js in der Datei src / index.js wäre import. / components / App.
Der Nachteil dieses Ansatzes besteht darin, dass Sie viele Dateien mit dem gleichen Namen erhalten. Das kann das Lesen in einigen Texteditoren erschweren.
Letztendlich ist es eine persönliche Entscheidung bzw. Entscheidung im Team; es wird jedoch empfohlen, konsistent zu sein.
Trennen von Code in freigegebenen Dateien
Nun verfügt jede Komponente über ein eigenes Verzeichnis, aber nicht alles ist vollkommen unabhängig voneinander.
Der letzte Schritt besteht darin, das CSS für Instructions in einer separaten Datei zu extrahieren.
Erstellen Sie zuerst eine CSS-Datei in src / components / Instructions:
Öffnen Sie die CSS-Datei dann in Ihrem Texteditor:
Fügen Sie das Instructions-CSS hinzu, den Sie in einem früheren Abschnitt erstellt haben:
Entfernen Sie anschließend das Instuctions-CSS aus src / components / App / App.css.
Entfernen Sie die Zeilen über .instructions.
Die endgültige Datei sieht so aus:
Importieren Sie abschließend das CSS in Instructions.js:
Importieren Sie das CSS unter Verwendung des relativen Pfads:
Ihr Browserfenster wird wie zuvor aussehen; allerdings sind jetzt alle Dateiressourcen im gleichen Verzeichnis angeordnet.
Sehen Sie sich am Ende noch einmal die Struktur an.
Zuerst das Verzeichnis src /:
Sie verfügen über die Stammkomponente index.js und das verwandten CSS index.css neben dem Verzeichnis components / und Dienstprogrammdateien wie serviceWorker.js und setupTests.js:
Sehen Sie anschließend in components nach:
Sie werden für jede Komponente ein Verzeichnis sehen:
Wenn Sie in den einzelnen Komponenten nachsehen, finden Sie den Komponentencode, CSS, Test und Bilddateien (so vorhanden).
Nun haben Sie eine solide Struktur für Ihr Projekt erstellt.
Sie haben viel Code verschoben. Jetzt verfügen Sie aber über eine Struktur, die das Skalieren einfacher macht.
Das ist nicht die einzige Methode, mit der Sie Ihre Struktur konzipieren können.
Einige Dateistrukturen können Codetrennung nutzen, indem ein Verzeichnis angegeben wird, das in verschiedene Pakete aufgeteilt wird.
Andere Dateistrukturen werden nach Route aufgeteilt und nutzen ein gemeinsames Verzeichnis für Komponenten, die über verschiedene Routen hinweg verwendet werden.
Beginnen Sie vorerst mit einem weniger komplexen Ansatz.
Wenn sich ein Bedarf nach einer anderen Struktur ergibt, ist es immer leichter, von einfach zu komplex zu gehen.
Wenn Sie mit einer komplexen Struktur anfangen, bevor Sie sie benötigen, erschwert das das Refactoring.
Nachdem Sie eine klassenbasierte Komponente erstellt und organisiert haben, werden Sie im nächsten Schritt eine funktionale Komponente erstellen.
Schritt 4 - Erstellen einer funktionalen Komponente
In diesem Schritt erstellen Sie eine funktionale Komponente.
Funktioale Komponenten sind die häufigsten Komponenten in aktuellem React-Code.
Diese Komponenten sind meist kürzer und können im Gegensatz zu klassenbasierten Komponenten React-Hooks verwenden. Dabei handelt es sich um eine neue Form von Zustands- und Ereignisverwaltung.
Eine funktionale Komponente ist eine JavaScript-Funktion, die JSX zurückgibt.
Sie muss nichts erweitern und es gibt keine speziellen Methoden, die Sie sich merken müssen.
Um < Instructions > als funktionale Komponente zu refaktorisieren, müssen Sie die Klasse in eine Funktion ändern und die Renderingmethode entfernen, damit nur noch die Return-Anweisung übrig bleibt.
Dazu öffnen Sie zuerst Instructions.js in einem Texteditor.
Ändern Sie die class-Deklaration in eine function-Deklaration:
Entfernen Sie anschließend den Import von {Component}:
Entfernen Sie abschließend die Methode render ().
Nun geben Sie nur JSX zurück.
Der Browser wird aktualisiert und Sie werden Ihre Seite wie zuvor sehen.
Sie könnten die Funktion auch als Pfeilfunktion neu schreiben, indem Sie die implizite Rückgabe verwenden.
Der Hauptunterschied besteht darin, dass Sie den Funktionsrumpf verlieren.
Außerdem müssen Sie die Funktion zuerst einer Variable zuweisen und dann die Variable exportieren:
Einfache funktionale Komponenten und klassenbasierte Komponenten sind sich sehr ähnlich.
Wenn Sie über eine einfache Komponente verfügen, die keinen Zustand speichert, ist es am besten, eine funktionale Komponente zu verwenden.
Der wahre Unterschied besteht darin, wie sie den Zustand einer Komponente speichern und Eigenschaften nutzen.
Klassenbasierte Komponenten verwenden Methoden und Eigenschaften, um den Zustand festzulegen, und sind meist etwas länger.
Funktionale Komponenten nutzen zum Speichern des Zustands oder Verwalten von Änderungen Hooks und sind meist etwas kürzer.
Nun verfügen Sie über eine kleine Anwendung mit voneinander unabhängigen Teilen.
Sie haben zwei verbreitete Arten von Komponenten erstellt: Funktionale und Klassen.
Sie haben Teile der Komponenten in Verzeichnisse aufgeteilt, damit Sie ähnliche Codestücke zusammen aufbewahren können.
Außerdem haben Sie die Komponenten importiert und wiederverwendet.
Mit einem guten Verständnis der Komponenten können Sie sich Ihre Anwendungen nun als Teile vorstellen, die Sie auseinander nehmen und wieder zusammensetzen können.
Projekte werden damit modular und austauschbar.
Die Fähigkeit, ganze Anwendungen als Reihe von Komponenten zu betrachten, ist ein wichtiger Denkschritt in React.
Wenn Sie weitere React-Tutorials konsultieren möchten, sehen Sie sich unsere React-Themenseite an oder kehren Sie zurück zur Seite der Reihe Codieren in React.js.
Installieren und Konfigurieren von Postfix als Send-Only-SMTP-Server unter Ubuntu 18.04
4103
Postfix ist ein Mail Transfer Agent (MTA), eine Anwendung zum Senden und Empfangen von E-Mail.
Er kann so konfiguriert werden, dass er sich nur zum Senden von E-Mails durch lokale Anwendungen verwenden lässt.
Das ist in Fällen nützlich, in denen Sie regelmäßig E-Mail-Benachrichtigungen von Ihren Anwendungen senden oder einfach über viel ausgehenden Datenverkehr verfügen, den E-Mail-Drittanbieter nicht zulassen.
Außerdem handelt es sich dabei um eine schlankere Alternative zur Ausführung eines kompletten SMTP-Servers, die dennoch die erforderliche Funktionalität bietet.
In diesem Tutorial installieren und konfigurieren Sie Postfix als Send-Only-SMTP-Server.
Außerdem werden Sie kostenlose TLS-Zertifikate von Let 's Encrypt für Ihre Domäne anfordern und die ausgehenden E-Mails damit verschlüsseln.
Ein Ubuntu 18.04-Server, der gemäß Ersteinrichtung eines Servers unter Ubuntu 18.04 eingerichtet wurde, einschließlich eines non-root user mit sudo-Berechtigungen.
Ein vollständig registrierter Domänenamen.
Dieses Tutorial verwendet in allen Bereichen < ^ > your _ domain < ^ >.
Ein DNS-Datensatz mit < ^ > your-domain < ^ >, der auf die öffentliche IP-Adresse Ihres Servers verweist.
< $> note Anmerkung: Der Hostname Ihres Servers und der Name Ihres Droplets müssen < ^ > your _ domain < ^ > entsprechen, da DigitalOcean anhand des Namens automatisch PTR-Einträge für die IP-Adresse des Droplets festlegt.
Sie können den Hostnamen des Servers überprüfen, indem Sie hostname in der Eingabeaufforderung eingeben.
Die Ausgabe sollte mit dem Namen übereinstimmen, den Sie dem Droplet bei der Erstellung gegeben haben.
Schritt 1 - Installieren von Postfix
In diesem Schritt installieren Sie Postfix.
Die schnellste Methode besteht aus der Installation des Pakets mailutils, in dem Postfix mit einigen zusätzlichen Programmen gebündelt ist, die Sie zum Testversand von E-Mails verwenden werden.
Aktualisieren Sie zuerst die Paketdatenbank:
Installieren Sie dann Postfix, indem Sie den folgenden Befehl ausführen:
Kurz vor Ende der Installation wird Ihnen das Postfix-Konfigurationsfenster angezeigt:
Wählen Sie "Internet Site" (Internetsite) aus dem Menü; drücken Sie zum Auswählen TAB
und dann ENTER.
Die Standardoption lautet Internet Site (Internetseite).
Das ist die empfohlene Option für Ihren Anwendungsfall. Drücken Sie also TAB und dann ENTER.
Wenn Sie nur den Beschreibungstext sehen, drücken Sie TAB, um OK zu wählen, und dann ENTER.
Wenn die Anzeige nicht automatisch erfolgt, führen Sie zum Starten den folgenden Befehl aus:
Danach erhalten Sie eine weitere Konfigurationsaufforderung in Bezug auf den System-E-Mail-Namen:
Geben Sie Ihren Domänenamen ein und drücken Sie zum Auswählen TAB sowie
ENTER.
Der System-E-Mail-Name muss gleich sein wie der Name, den Sie bei der Erstellung Ihres Servers zugewiesen haben. Wenn Sie damit fertig sind, drücken Sie TAB, gefolgt von ENTER.
Sie haben Postfix jetzt installiert und können mit der Konfiguration beginnen.
Schritt 2 - Konfigurieren von Postfix
Im diesem Schritt konfigurieren Sie Postfix so, dass E-Mails nur von dem Server gesendet und empfangen werden, auf dem Postfix ausgeführt wird - d. h. von localhost.
Dazu muss Postfix so konfiguriert werden, dass nur an der Loopback-Schnittstelle gelauscht wird; das ist die virtuelle Netzwerkschnittstelle, die der Server zur internen Kommunikation verwendet.
Um die Änderungen vorzunehmen, müssen Sie die Hauptkonfigurationsdatei von Postfix namens main.cf bearbeiten, die unter etc / postfix gespeichert ist.
Öffnen Sie sie zum Bearbeiten in Ihrem bevorzugten Texteditor:
Suchen Sie nach den folgenden Zeilen:
Setzen Sie den Wert von inet _ interfaces auf loopback-only:
Eine weitere Anweisung, die Sie ändern müssen, ist mydestination, die zur Angabe der Liste von Domänen dient, die über den Mailzustellungstransport local _ transport bereitgestellt werden.
Standardmäßig sehen die Werte etwa wie folgt aus:
Ändern Sie die Zeile, damit sie wie folgt aussieht:
Wenn Ihre Domäne in Wahrheit eine Subdomäne ist und Sie möchten, dass die E-Mail-Nachrichten aussehen, als ob sie von der Hauptdomäne gesendet wurden, können Sie am Ende von main.cf die folgende Zeile hinzufügen:
Die optionale Einstellung masquerade _ domains gibt an, bei welchen Domänen der Subdomänenteil in der E-Mail-Adresse entfernt wird.
Wenn Sie fertig sind, speichern und schließen Sie die Datei.
< $> note Anmerkung: Wenn Sie mehrere Domänen auf einem Server hosten, können die anderen Domänen mit der Anweisung mydestination ebenfalls an Postfix übergeben werden.
Starten Sie dann Postfix neu, indem Sie den folgenden Befehl ausführen:
Sie haben Postfix so konfiguriert, dass von Ihrem Server nur E-Mails gesendet werden.
Sie werden dies nun testen, indem Sie eine Beispielnachricht an eine E-Mail-Adresse senden.
Schritt 3 - Testen des SMTP-Servers
Im diesem Schritt testen Sie, ob Postfix E-Mails mit dem Befehl mail an ein externes E-Mail-Konto senden kann. Dieser Befehl ist Teil des Pakets mailutils, das Sie im ersten Schritt installiert haben.
Um eine Test-E-Mail zu senden, führen Sie den folgenden Befehl aus:
Sie können den Text und den Betreff der E-Mail nach Ihren Wünschen ändern.
Denken Sie daran, < ^ > your _ email _ address < ^ > durch eine gültige E-Mail-Adresse zu ersetzen, auf die Sie zugreifen können.
Überprüfen Sie nun die E-Mail-Adresse, an die Sie diese Nachricht gesendet haben.
Sie sollten die Nachricht in Ihrem Posteingang sehen.
Wenn Sie sie dort nicht finden können, sehen Sie in Ihrem Spam-Ordner nach.
Bislang sind alle von Ihnen gesendeten E-Mails unverschlüsselt, weswegen Dienstanbieter denken, dass es wahrscheinlich Spam-Nachrichten sind.
Im Schritt 5 richten Sie die Verschlüsselung ein.
Wenn Sie einen Fehler vom Befehl mail erhalten oder auch nach längerer Zeit keine Nachricht empfangen haben, dann vergewissern Sie sich, dass die von Ihnen bearbeitete Postfix-Konfiguration gültig ist und der Name sowie Hostname Ihres Servers auf Ihre Domäne festgelegt sind.
Achten Sie darauf, dass bei dieser Konfiguration die Adresse im Feld From für die von Ihnen gesendeten Test-E-Mails in Format < ^ > your _ user _ name < ^ > @ < ^ > your _ domain < ^ > vorliegt, wobei < ^ > your _ user _ name < ^ > der Benutzername des Serverbenutzers ist, als der Sie den Befehl ausgeführt haben.
Sie haben nun eine E-Mail von Ihrem Server gesendet und überprüft, ob sie erfolgreich empfangen wurde.
Im nächsten Schritt richten Sie die E-Mail-Weiterleitung für root ein.
Schritt 4 - Weiterleitung von System-E-Mail
Im diesem Schritt richten Sie eine E-Mail-Weiterleitung für den Benutzer root ein, damit systemgenerierte Nachrichten, die auf Ihrem Server an ihn gesendet werden, an eine externe E-Mail-Adresse weitergeleitet werden.
Die Datei / etc / aliases enthält eine Liste von alternativen Namen für E-Mail-Empfänger.
Öffnen Sie sie zum Bearbeiten:
Im Standardzustand sieht sie wie folgt aus:
Die einzige vorhandene Anweisung gibt an, dass systemgenerierte E-Mails an root gesendet werden.
Fügen Sie am Ende der Datei die folgende Zeile hinzu:
Mit dieser Zeile geben Sie an, dass an root gesendete E-Mails an eine E-Mail-Adresse weitergeleitet werden.
Denken Sie daran, < ^ > your _ email _ address < ^ > durch Ihre persönliche E-Mail-Adresse zu ersetzen.
Um die Änderung anzuwenden, führen Sie den folgenden Befehl aus:
Durch Ausführung von newaliases wird eine Datenbank mit Aliassen erstellt, die der Befehl mail verwendet. Die Aliasse werden aus der Konfigurationsdatei übernommen, die Sie gerade bearbeitet haben.
Testen Sie, ob E-Mails an root gesendet werden, indem Sie Folgendes ausführen:
Sie sollten die E-Mail unter Ihrer E-Mail-Adresse erhalten.
Im diesem Schritt haben Sie eine Weiterleitung systemgenerierter Nachrichten an Ihre E-Mail-Adresse eingerichtet.
Sie aktivieren jetzt die Nachrichtenverschlüsselung, damit alle E-Mails, die Ihr Server versendet, sicher vor Manipulation bei der Übertragung sind und als legitimer betrachtet werden.
Schritt 5 - Aktivieren von SMTP-Verschlüsselung
Sie aktivieren jetzt SMTP-Verschlüsselung, indem Sie ein kostenloses TLS-Zertifikat von Let "s Encrypt für Ihre Domäne anfordern (mit Certbot) und Postfix so konfigurieren, dass das Zertifikat zum Senden von Nachrichten verwendet wird.
Ubuntu enthält Certbot in seinem standardmäßigen Packet-Repository; es kann aber sein, dass Certbot nicht auf dem neuesten Stand ist.
Stattdessen fügen Sie das offizielle Repository hinzu, indem Sie den folgenden Befehl ausführen:
Drücken Sie, wenn Sie dazu aufgefordert werden, zum Akzeptieren ENTER.
Aktualisieren Sie dann den Cache des Paketmanagers auf Ihrem Server:
Installieren Sie abschließend die neueste Version von Certbot:
Im Rahmen der Ersteinrichtung des Servers in den Voraussetzungen haben Sie ufw, die unkomplizierte Firewall, installiert.
Sie müssen sie so konfigurieren, dass der HTTP-Port 80 zugelassen wird, damit die Verifizierung der Domäne abgeschlossen werden kann.
Führen Sie den folgenden Befehl aus, um ihn zu aktivieren:
Nachdem der Port nun geöffnet ist, führen Sie Certbot aus, um ein Zertifikat zu erhalten:
Dieser Befehl weist Certbot dazu an, Zertifikate mit einer RSA-Schlüsselgröße von 4096 Bits auszugeben, einen temporären Standalone-Webserver (--standalone) zur Verifizierung auszuführen und die Prüfung über Port 80 (--preferred-challenges http) vorzunehmen.
Denken Sie daran, < ^ > your _ domain < ^ > durch Ihre Domäne zu ersetzen, bevor Sie den Befehl ausführen, und geben Sie bei Aufforderung Ihre E-Mail-Adresse ein.
Wie in den Anmerkungen erwähnt, wurden Ihr Zertifikat und Ihre private Schlüsseldatei unter / etc / letsencrypt / live / < ^ > your _ domain < ^ > gespeichert.
Nachdem Sie über das Zertifikat verfügen, öffnen Sie nun main.cf zum Bearbeiten:
Suchen Sie nach dem folgenden Abschnitt:
Ändern Sie ihn, damit er folgendermaßen aussieht, indem Sie < ^ > your _ domain < ^ > ggf. durch Ihre Domäne ersetzen, um Ihre TLS-Einstellungen für Postfix zu aktualisieren:
Wenn Sie damit fertig sind, speichern und schließen Sie die Datei.
Wenden Sie die Änderungen durch Neustart von Postfix an:
Versuchen Sie nun, erneut eine E-Mail zu senden:
Überprüfen Sie dann die von Ihnen angegebene E-Mail-Adresse.
Es ist möglich, dass Sie die Nachricht sofort in Ihrem Posteingang sehen, da E-Mail-Anbieter verschlüsselte Nachrichten deutlich seltener als Spam markieren.
Sie können die technischen Informationen über die E-Mail-Nachricht in Ihrem Client prüfen, um zu sehen, ob die Nachricht tatsächlich verschlüsselt wurde.
Sie verfügen nun über einen Send-Only-E-Mail-Server, der von Postfix bereitgestellt wird.
Das Verschlüsseln aller ausgehenden Nachrichten ist ein guter erster Schritt, damit E-Mail-Anbieter Ihre Nachrichten nicht von vornherein als Spam markieren.
Wenn Sie das in einem Entwicklungsszenario tun, sollte diese Maßnahme ausreichen.
Wenn Ihr Anwendungsfall jedoch darin besteht, E-Mails an potenzielle Websitebenutzer zu senden (wie Bestätigungs-E-Mails für die Anmeldung bei einem Nachrichtenforum), sollten Sie sich mit der Einrichtung von SPF-Einträgen befassen, damit die E-Mails Ihres Servers mit noch höherer Wahrscheinlichkeit als legitim gelten.
Installieren und Verwenden von Linkerd mit Kubernetes
5429
Der Autor wählte den Tech Education Fund, um eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
Ein Service Mesh ist eine dedizierte Infrastrukturebene, mit der Administratoren die Kommunikation zwischen Diensten verwalten können.
Mit ihren vielen leistungsfähigen Tools können diese Service Meshes Ihr System sicherer, zuverlässiger und auch transparenter machen.
Ein Service Mesh wie zum Beispiel Linkerd kann Verbindungen automatisch verschlüsseln, wiederholte Anfragen und Timeouts verwalten, Telemetriedaten wie Erfolgsraten und Latenz liefern und vieles mehr.
In diesem Tutorial installieren Sie das Linkerd-Service Mesh in Ihrem Kubernetes-Cluster, stellen eine Beispielanwendung bereit und sehen sich dann das Dashboard von Linkerd an.
Nachdem Sie sich mit einigen dieser Informationen im Dashboard vertraut gemacht haben, konfigurieren Sie Linkerd, um timeout- und retry-Richtlinien für einen bestimmten Kubernetes-Pod durchzusetzen.
Erkunden Sie alternativ die One-Click-Installationsoption für Linkerd / Kubernetes von DigitalOcean.
Ein Kubernetes 1.12 + -Cluster.
In diesem Tutorial wird ein DigitalOcean Kubernetes-Cluster mit drei Knoten verwendet, aber Sie können auf Wunsch auch mit einer anderen Methode einen Cluster erstellen.
Das auf einem Entwicklungsserver installierte Befehlszeilentool kubectl, das zur Verbindung mit Ihrem Cluster konfiguriert ist.
Weitere Informationen zur Installation von kubectl finden Sie in der offiziellen Dokumentation.
Schritt 1 - Bereitstellen der Anwendung
Um Linkerd in Aktion zu sehen, müssen Sie eine Anwendung in Ihrem Cluster ausführen.
In diesem Schritt stellen Sie eine Anwendung namens emojivoto bereit, die das Linkerd-Team für diesen Zweck erstellt hat.
In diesem Repository können Sie den Code für die vier Dienste, aus denen die Anwendung besteht, sowie die Manifestdatei sehen, die Sie zur Bereitstellung dieser Dienste in Ihrem Kubernetes-Cluster verwenden werden.
Speichern Sie zuerst diese Manifestdatei lokal:
Sie verwenden curl, um die Datei abzurufen, und übergeben dann die Option --output, um mitzuteilen, wo die Datei gespeichert werden soll.
In diesem Fall erstellen Sie eine Datei namens manifest.yaml.
Um besser zu verstehen, was diese Datei tun wird, sehen Sie sich mit cat ihre Inhalte an oder öffnen Sie sie mit Ihrem bevorzugten Editor:
Drücken Sie SPACE, um durch die Anweisungen zu blättern.
Sie sehen, dass manifest.yaml einen Kubernetes-Namespace namens emojivoto erstellt, wo alles, was mit dieser Anwendung verbunden ist, ausgeführt wird. Hinzu kommen verschiedene Deployments und Services von Kubernetes.
Wenden Sie als Nächstes das Manifest in Ihrem Kubernetes-Cluster an:
Auch hier verwenden Sie kubectl apply mit dem Flag -f, um eine Datei zuzuweisen, die Sie anwenden möchten.
Dieser Befehl gibt eine Liste aller Ressourcen aus, die erstellt wurden:
Überprüfen Sie nun, ob die Dienste ausgeführt werden:
Sie verwenden kubectl, um alle Pods aufzulisten, die Sie in Ihrem Cluster ausführen, und übergeben dann das Flag -n, um anzugeben, welche Namespaces Sie verwenden möchten.
Sie übergeben den Namespace emojivoto, da Sie dort alle diese Dienste ausführen.
Wenn alle pods den Status Running aufweisen, sind Sie bereit für den nächsten Schritt:
Um die Anwendung abschließend in Ihrem Browser laufen zu sehen, verwenden Sie die integrierte Funktion kubectl, um lokale Anfragen an Ihren Remote-Cluster weiterzuleiten:
< $> note Anmerkung: Wenn Sie das nicht von Ihrem lokalen Rechner aus ausführen, müssen Sie das Flag --address 0.0.0.0 hinzufügen, um alle Adressen und nicht nur localhost aufzulisten.
Hier verwenden Sie kubectl erneut in den emojivoto-Namespaces, rufen aber nun den Unterbefehl port-forward auf und weisen ihn an, alle lokalen Anfragen an Port 8080 an den Kubernetes-Dienst web-svc weiterzuleiten. Das ist eine einfache Möglichkeit, auf Ihre Anwendung zuzugreifen, ohne dass Sie einen richtigen Lastausgleich einrichten müssen.
Besuchen Sie nun http: / / localhost: 8080 und Sie werden die Anwendung emojivoto sehen.
Emojivoto-Beispielanwendung
Drücken Sie STRG + C in Ihrem Terminal.
Nachdem nun eine Anwendung in Ihrem Cluster ausgeführt wird, sind Sie bereit, Linkerd zu installieren und zu sehen, wie es funktioniert.
Schritt 2 - Installieren von Linkerd
Nachdem Sie über eine ausgeführte Anwendung verfügen, installieren wir nun Linkerd.
Zur Installation in Ihrem Kubernetes-Cluster benötigen Sie zuerst die Linkerd-CLI.
Sie werden diese Befehlszeilenschnittstelle nutzen, um von Ihrem lokalen Rechner aus mit Linkerd zu interagieren.
Danach können Sie Linkerd in Ihrem Cluster installieren.
Installieren wir zuerst die CLI mit dem Skript, das das Linkerd-Team bereitgestellt hat:
Hier verwenden Sie curl zum Herunterladen des Installationsskripts und leiten die Ausgabe dann an sh weiter, wo das Skript automatisch ausgeführt wird.
Alternativ können Sie die CLI direkt von der Release-Seite von Linkerd herunterladen.
Wenn Sie das Skript verwenden, wird Linkerd unter ~ / .linkerd2 / bin installiert.
Überprüfen Sie nun, ob die CLI richtig funktioniert:
Der Befehl gibt etwa Folgendes aus:
Um die Ausführung der CLI einfacher zu gestalten, fügen Sie das Verzeichnis dann Ihrem $PATH hinzu:
Sie können die Befehle nun direkter ausführen, wie zum Beispiel den vorherigen:
Abschließend installieren wir Linkerd in Ihrem Kubernetes-Cluster.
Der Befehl linkerd install dient der Erstellung aller erforderlichen yaml-Manifeste, die zur Ausführung von Linkerd benötigt werden. Er wendet diese Manifeste jedoch nicht auf Ihren Cluster an.
Führen Sie diesen Befehl aus, um die Ausgabe zu überprüfen:
Sie werden eine lange Ausgabe sehen, in der alle yaml-Manifeste für Ressourcen aufgelistet werden, die Linkerd ausführen muss.
Um diese Manifeste auf Ihren Cluster anzuwenden, führen Sie Folgendes aus:
Durch Ausführung von linkerd install werden alle zuvor angezeigten Manifeste ausgegeben. | leitet diese Ausgabe dann direkt an kubectl apply weiter, wo sie angewendet werden.
Nachdem Sie diesen Befehl ausgeführt haben, gibt kubectl apply eine Liste aller Ressourcen aus, die erstellt wurden.
Um zu überprüfen, ob alles in Ihrem Cluster ausgeführt wird, führen Sie linkerd check aus:
Dadurch werden mehrere Prüfungen für Ihren Cluster ausgeführt, um zu sehen, ob alle erforderlichen Komponenten ausgeführt werden:
Führen Sie abschließend diesen Befehl aus, um das integrierte Linkerd-Dashboard in Ihrem Browser zu öffnen (vergessen Sie nicht, das Flag --address 0.0.0.0 anzugeben, wenn Sie es nicht von Ihrem lokalen Rechner ausführen):
Linkerd-Dashboard
Die meisten der Informationen, die Sie im Dashboard sehen, können Sie über die Linkerd-CLI aufrufen.
Führen Sie zum Beispiel diesen Befehl aus, um übergeordnete Statistiken für Bereitstellungen anzuzeigen:
Hier sagen Sie, dass Sie die Statistiken für Bereitstellungen, die im Namespace linkerd ausgeführt werden, abrufen möchten.
Das sind die eigenen Komponenten von Linkerd; interessanterweise können Sie Linkerd selbst zu ihrer Überwachung nutzen.
Sie können Statistiken wie Anfragen pro Sekunde (RPS), Erfolgsrate, Latenz und mehr anzeigen.
Außerdem können Sie eine Meshed-Spalte sehen, in der angegeben wird, wie viele pods Linkerd injiziert hat:
Probieren Sie diesen Befehl nun in Ihrem Namespace emojivoto aus:
Obwohl Sie Ihre vier Dienste sehen können, sind keine der zuvor erstellten Statistiken für diese Bereitstellungen verfügbar. In der Spalte "Meshed" können Sie sehen, dass 0 / 1 angegeben ist:
Die Ausgabe bedeutet, dass Linkerd immer noch nicht in die Anwendung injiziert wurde.
Das ist Ihr nächster Schritt.
Schritt 3 - Injizieren von Linkerd in Ihre Anwendung
Nachdem Sie Linkerd nun in Ihrem Cluster ausführen, sind Sie bereit, es in Ihre emojivoto-Anwendung zu injizieren.
Linkerd funktioniert, indem es einen Sidecar-Container in Ihren Kubernetes pods ausführt.
Das heißt, Sie injizieren einen Linkerd-Proxy-Container in jeden pod, der ausgeführt wird.
Jede Anfrage, die Ihre pods dann senden oder erhalten, werden über diesen sehr schlanken Proxyserver laufen, der Metriken (wie Erfolgsrate, Anfragen pro Sekunde und Latenz) sammeln und Richtlinien durchsetzen kann (wie für Timeouts und Wiederholungsversuche).
Mit diesem Befehl können Sie den Proxy von Linkerd manuell injizieren:
In diesem Befehl verwenden Sie kubectl zuerst, um alle Kubernetes deployments zu erhalten, die Sie im Namespace emojivoto ausführen, und geben dann an, ob Sie die Ausgabe im yaml-Format wünschen.
Dann senden Sie diese Ausgabe an den Befehl linkerd inject.
Dieser Befehl liest die yaml-Datei mit den aktuellen Manifesten, die ausgeführt werden, und modifiziert sie, um den linkerd-Proxy mit jedem deployment aufzunehmen.
Schließlich erhalten Sie das modifizierte Manifest und wenden es mit kubectl apply auf Ihren Cluster an.
Nach Ausführung dieses Befehls sehen Sie eine Meldung mit dem Hinweis, dass alle vier emojivoto-Dienste (emoji, vote-bot, voting und web) erfolgreich injiziert wurden.
Wenn Sie jetzt stats für emojivoto abrufen, sehen Sie, dass nun alle Ihre deployments vernetzt sind. Nach ein paar Sekunden sehen Sie die gleichen Statistiken, die für den Namespace linkerd angezeigt wurden:
Hier können Sie die Statistiken für alle vier Dienste sehen, aus denen die Anwendung emojivoto besteht, jeweils mit ihrer Erfolgsrate, Anfragen pro Sekunde und Latenz, ohne dass Sie Anwendungscode schreiben oder ändern müssen.
Der Dienst vote-bot zeigt keine Statistiken an, da er nur ein Bot ist, der Anfragen an andere Dienste sendet und daher keinen Datenverkehr empfängt. Das ist für sich eine wertvolle Information.
Nun sehen wir uns an, wie Sie über Ihre Dienste zusätzliche Informationen für Linkerd bereitstellen können, um das Verhalten anzupassen.
Schritt 4 - Definieren eines Dienstprofils
Nachdem Sie Linkerd in Ihre Anwendung injiziert haben, können Sie nun wertvolle Informationen darüber abrufen, wie sich Ihre einzelnen Dienste verhalten.
Außerdem haben Sie das geschafft, ohne benutzerdefinierte Konfigurationen schreiben oder den Code Ihrer Anwendung ändern zu müssen.
Wenn Sie Linkerd jedoch einige zusätzliche Informationen bereitstellen, kann es dann verschiedene Richtlinien durchsetzen, wie z. B. für Timeouts und Wiederholungsversuche.
Außerdem kann Linkerd Metriken für einzelne Routen liefern.
Diese Informationen werden über ein Dienstprofil bereitgestellt, das eine benutzerdefinierte Linkerd-Ressource ist, in der Sie die Routen in Ihren Anwendungen beschreiben und festhalten können, wie sich jede von ihnen verhalten wird.
Hier ist ein Beispiel dafür, wie das Manifest eines Dienstprofils aussieht:
Das Dienstprofil beschreibt eine Liste von Routen und definiert dann, wie sich Anfragen, die mit der angegebenen condition übereinstimmen, verhalten werden.
In diesem Beispiel sagen Sie, dass jede GET-Anfrage, die an / my / route / path gesendet wird, nach 100 ms ablaufen soll (Timeout); wenn sie fehlschlägt, kann es Wiederholungsversuche geben.
Erstellen wir nun ein Dienstprofil für einen Ihrer Dienste.
Wenn wir voting-svc als Beispiel nehmen, verwenden Sie zuerst die Linkerd-CLI, um die Routen zu überprüfen, die Sie für diesen Dienst definiert haben:
Hier verwenden Sie den Befehl linkerd routes, um alle Routen für den Dienst voting-svc im Namespace emojivoto aufzulisten:
Sie finden nur eine Route: [DEFAULT].
Hier werden alle Anfragen gruppiert, bis Sie das Dienstprofil definieren.
Öffnen Sie nun nano oder Ihren bevorzugten Editor, um eine service-profile.yaml-Datei zu erstellen:
Fügen Sie die folgende Dienstprofildefinition in diese Datei ein:
Speichern Sie nun die Datei und schließen Sie Ihren Editor.
Hier deklarieren Sie ein Dienstprofil für den Dienst voting-svc im Namespace emojivoto.
Sie haben eine Route namens VoteDoughnut definiert, die mit jeder POST-Anfrage an / emojivoto.v1 übereinstimmt.
Pfad VotingService / VoteDoughnut.
Wenn eine Anfrage, die mit diesen Kriterien übereinstimmt, länger als 100 ms benötigt, wird sie von Linkerd abgebrochen und der Client erhält eine 504-Antwort zurück.
Außerdem teilen Sie Linkerd mit, dass für die Anfrage, wenn sie fehlschlägt, Wiederholungsversuche vorgenommen werden dürfen.
Wenden Sie diese Datei nun auf Ihren Cluster an:
Nach ein paar Sekunden prüfen Sie die Routen erneut für diesen Dienst:
Sie sehen nun Ihre neu definierte VoteDoughnut-Route:
Sie können für diese spezielle Route verschiedene benutzerdefinierte Metriken anzeigen, wie Erfolgsrate, Anfragen pro Sekunde und Latenz.
Beachten Sie, dass der Endpunkt VoteDoughnut gezielt so konfiguriert ist, dass er immer einen Fehler zurückgibt und eine Erfolgsrate von 0% ausgibt, während die Route [DEFAULT] 100% ausgibt.
Nachdem Linkerd über einige Informationen zu Ihrem Dienst verfügt, haben Sie nun benutzerdefinierte Metriken pro Route sowie zwei Richtlinien, die durchgesetzt werden: für Timeouts und Wiederholungsversuche.
In diesem Artikel haben Sie Linkerd in Ihrem Kubernetes-Cluster installiert und zur Überwachung einer Beispielanwendung verwendet.
Sie haben nützliche Telemetriedaten wie Erfolgsrate, Durchsatz und Latenz extrahiert.
Außerdem haben Sie ein Linkerd-Dienstprofil konfiguriert, um routenspezifische Metriken abzurufen und zwei Richtlinien in der Anwendung emojivoto durchzusetzen.
Wenn Sie mehr über Linkerd erfahren möchten, können Sie die äußerst hilfreiche Dokumentationsseite konsultieren. Hier sehen sie, wie Sie Ihre Dienste sichern, eine verteilte Verfolgung konfigurieren, Canaryfreigaben automatisieren und vieles mehr erledigen können.
Von hier aus können Sie sich auch Istio ansehen, was ein anderer Service Mesh mit anderen Funktionen sowie Vor- und Nachteilen ist.
Installieren und Verwenden von PostgreSQL unter Ubuntu 20.04
5245
Relationale Datenbank-Managementsysteme sind ein wichtiger Bestandteil zahlreicher Websites und Anwendungen.
Sie stellen eine strukturierte Art und Weise zur Speicherung, Organisation und zum Zugriff auf Informationen bereit.
PostgreSQL oder Postgres ist ein relationales Datenbank-Managementsystem, das eine Implementierung der SQL-Abfragesprache ermöglicht.
Es ist standardkonform und verfügt über verschiedene erweiterte Funktionen wie zuverlässige Transaktionen und Gleichzeitigkeit ohne Lesesperren.
Dieser Leitfaden zeigt, wie Sie Postgres auf einem Ubuntu 20.04-Server installieren können.
Außerdem liefert er Anweisungen für die allgemeine Datenbankverwaltung.
Um diesem Tutorial zu folgen, benötigen Sie einen Ubuntu 20.04 Server, der im Sinne unseres Leitfadens Ersteinrichtung eines Servers für Ubuntu 20.04 konfiguriert wurde.
Nach Abschluss der Voraussetzungen dieses Tutorials sollte Ihr Server einen Benutzer ohne Root-, aber mit sudo-Berechtigung und eine einfache Firewall haben.
Schritt 1 - Installieren von PostgreSQL
Ubuntu Standard-Repositorys enthalten Postgres-Pakete, damit Sie diese mit dem apt-Paketsystem installieren können.
Wenn Sie es in letzter Zeit nicht getan haben, aktualisieren Sie den lokalen Paketindex Ihres Servers:
Installieren Sie dann das Postgres-Paket mit dem -contrib-Paket, das zusätzliche Hilfsprogramme und Funktionalität hinzufügt:
Nachdem die Software installiert ist, können wir nun ihre Funktionsweise besprechen und aufzeigen, wie sie sich von anderen relationalen Datenbank-Managementsystemen unterscheidet, die Sie möglicherweise verwendet haben.
Schritt 2 - Verwenden von PostgreSQL-Rollen und Datenbanken
Postgres verwendet zur Verwaltung von Authentifizierung und Autorisierung standardmäßig ein Konzept namens "Rollen".
Diese sind auf gewisse Art und Weise herkömmlichen Konten im Unix-Stil ähnlich, aber Postgres unterscheidet nicht zwischen Benutzern und Gruppen und bevorzugt eher den flexiblen Begriff "Rolle".
Nach der Installation verwendet Postgres ident-Authentifizierung. Das bedeutet, dass es Postgres-Rollen mit einem übereinstimmenden Unix / Linux-Systemkonto verknüpft.
Falls eine Rolle in Postgres existiert, kann sich ein Unix / Linux-Benutzername mit demselben Namen als diese Rolle anmelden.
Das Installationsverfahren hat ein Benutzerkonto namens Postgres erstellt, das mit der Standard-Postgres-Rolle verknüpft ist.
Um Postgres zu verwenden, können Sie sich bei diesem Konto anmelden.
Es gibt mehrere Möglichkeiten, um dieses Konto zum Zugriff auf Postgres zu verwenden.
Wechsel zum Postgres-Konto
Wechseln Sie auf Ihrem Server zum Postgres-Konto, indem Sie Folgendes eingeben:
Sie können jetzt auf eine PostgreSQL-Eingabeaufforderung zugreifen, indem Sie Folgendes eingeben:
Von dort aus können Sie nach Bedarf mit dem Datenbank-Managementsystem interagieren.
Beenden Sie die PostgreSQL-Eingabeaufforderung durch die Eingabe von:
Damit kehren Sie zur Postgres Linux-Eingabeaufforderung zurück.
Zugreifen auf eine Postgres-Eingabeaufforderung ohne Kontowechsel
Sie können den gewünschten Befehl auch mit dem Postgres-Konto direkt mit sudo ausführen.
So wurden Sie z. B. im letzten Beispiel angewiesen, zur Postgres-Eingabeaufforderung zu gelangen, indem Sie zunächst auf den Postgres-Benutzer wechseln und dann psql ausführen, um die Postgres-Eingabeaufforderung zu öffnen.
Sie könnten dies in einem Schritt ausführen, indem Sie den einzelnen psql-Befehl als Postgres-Benutzer mit sudo ausführen, wie zum Beispiel:
Damit melden Sie sich direkt in Postgres ohne die zwischengeschaltete Bash Shell an.
Sie können die interaktive Postgres-Sitzung beenden, indem Sie Folgendes eingeben:
Zahlreiche Anwendungsfälle erfordern mehr als eine Postgres-Rolle.
Lesen Sie weiter, um mehr zu deren Konfiguration zu erfahren.
Schritt 3 - Erstellen einer neuen Rolle
Sie haben momentan nur die Postgres-Rolle innerhalb der Datenbank konfiguriert.
Mit dem Befehl createrole können Sie neue Rollen von der Befehlszeile aus erstellen.
Die --interactive Flag fordert Sie dazu auf, den Namen der neuen Rolle einzugeben und fragt Sie auch, ob diese Superuser-Berechtigung erhalten sollte.
Wenn Sie als Postgres-Konto angemeldet sind, können Sie einen neuen Benutzer erstellen, indem Sie Folgendes eingeben:
Wenn Sie allerdings lieber sudo für jeden Befehl verwenden, ohne Ihr normales Konto zu wechseln, verwenden Sie bitte Folgendes:
Das Skript enthält einige Auswahlmöglichkeiten und führt die korrekten Postgres-Befehle basierend auf Ihren Antworten aus, um einen Benutzer im Sinne Ihrer Spezifikationen zu erstellen.
Sie können mehr Kontrolle erhalten, indem Sie einige zusätzliche Flags übergeben.
Sehen Sie sich die Optionen auf der man-Seite an:
Ihre Installation von Postgres hat jetzt einen neuen Benutzer, aber Sie haben noch keine Datenbanken hinzugefügt.
Der nächste Abschnitt beschreibt diesen Vorgang.
Schritt 4 - Erstellen einer neuen Datenbank
Eine andere, standardmäßige Annahme des Postgres-Authentifizierungssystems ist, dass mit jeder Rolle, die zur Anmeldung verwendet wird, eine gleichnamige Datenbank verknüpft ist, auf die diese Rolle zugreifen kann.
Wenn der von Ihnen im letzten Abschnitt erstellte Benutzer sammy heißt, wird diese Rolle demnach versuchen, eine Verbindung mit einer Datenbank herzustellen, die standardmäßig auch als "sammy" bezeichnet wird.
Sie können die entsprechende Datenbank mit dem Befehl createdb erstellen.
Wenn Sie als Postgres-Konto angemeldet sind, sollten Sie Folgendes eingeben:
Wenn Sie allerdings lieber sudo für jeden Befehl verwenden, ohne Ihr normales Konto zu wechseln, verwenden Sie bitte Folgendes:
Diese Flexibilität liefert mehrere Möglickkeiten, um je nach Bedarf Datenbanken zu erstellen.
Schritt 5 - Eröffnen einer Postgres-Eingabeaufforderung mit der neuen Rolle
Um sich mit der ident-basierten Authentifizierung anzumelden, benötigen Sie einen Linux-Benutzer mit dem gleichen Namen wie Ihre Postgres-Rolle und -Datenbank.
Wenn Sie keinen passenden Linux-Benutzer zur Verfügung haben, können Sie mit dem Befehl adduser einen erstellen.
Sie müssen dies von Ihrem non-root-Konto mit sudo-Berechtigungenvornehmen (d. h. Sie können nicht als postgres-Benutzer angemeldet sein):
Sobald dieses neue Konto verfügbar ist, können Sie entweder die Datenbank wechseln und sich mit ihr verbinden, indem Sie Folgendes eingeben:
Oder Sie können Folgendes Inline vornehmen:
Mit diesem Befehl melden Sie sich automatisch an, vorausgesetzt, dass alle Komponenten ordnungsgemäß konfiguriert wurden.
Wenn Ihr Benutzer sich mit einer anderen Datenbank verbinden soll, können Sie dies tun, indem Sie die Datenbank wie folgt angeben:
Sobald Sie angemeldet sind, können Sie Ihre aktuellen Verbindungsinformationen überprüfen, indem Sie Folgendes eingeben:
Das ist nützlich, wenn Sie eine Verbindung mit nicht standardmäßigen Datenbanken oder mit nicht standardmäßigen Benutzern vornehmen.
Schritt 6 - Erstellen und Löschen von Tabellen
Da Sie nun wissen, wie Sie eine Verbindung mit dem PostgreSQL -Datenbanksystem herstellen, können Sie einige grundlegende Postgres-Managementaufgaben lernen.
Die grundlegende Syntax zum Erstellen von Tabellen lautet wie folgt:
Wie Sie sehen, geben diese Befehle der Tabelle einen Namen, definieren dann die Spalten sowie den Spaltentyp und die maximale Länge der Felddaten. Sie können auch wahlweise Beschränkungen für jede Spalte angeben.
Sie können hier mehr über das Erstellen und Verwalten von Tabellen in Postgres erfahren.
Erstellen Sie zu Demonstrationszwecken die folgende Tabelle:
Mit diesem Befehl erstellen Sie eine Tabelle, mit der Spielplatzgeräte erfasst werden.
Die erste Spalte in der Tabelle enthält Geräte-ID-Nummern vom Typ serial, wobei es sich um eine sich automatisch erhöhende Ganzzahl handelt.
Diese Spalte weist außerdem die Einschränkung PRIMARY KEY (PRIMÄRSCHLÜSSEL) auf, was bedeutet, dass die darin enthaltenen Werte eindeutig sein müssen und nicht Null sein dürfen.
Die nächsten beiden Zeilen erstellen Spalten für type (Typ) bzw. color (Farbe) der Geräte, die beide nicht leer sein dürfen.
Die nächste Zeile erstellt eine location-Spalte für den Standort und eine Einschränkung, die vorschreibt, dass der Wert einer von acht möglichen Werten ist.
Die letzte Zeile erzeugt eine Datumsspalte, in der das Datum erfasst wird, an dem Sie die Geräte installiert haben.
Für zwei der Spalten (equip _ id und install _ date) gibt der Befehl keine Feldlänge vor.
Der Grund dafür ist, dass einige Datentypen keine festgelegte Länge erfordern, da Länge oder Format impliziert sind.
Zeigen Sie Ihre neue Tabelle an, indem Sie Folgendes eingeben:
Ihre Tabelle playground ist hier, aber Sie sehen auch etwas namens playground _ equip _ id _ seq, das vom Typ-sequence ist.
Damit wird der Serientyp angegeben, den Sie Ihrer Spalte equip _ id zugeordnet haben.
Damit wird die nächste Nummer in der Sequenz aufgezeichnet. Der Serientyp wird automatisch für diese Art von Spalten erstellt.
Wenn Sie nur die Tabelle, ohne die Sequenz, sehen möchten, können Sie Folgendes eingeben:
Mit einer einsatzbereiten Tabelle üben wir jetzt die Verwaltung von Daten.
Schritt 7 - Hinzufügen, Abfrage und Löschen von Daten in einer Tabelle
Nachdem Sie über eine Tabelle verfügen, können Sie nun Daten in sie einfügen. Fügen Sie zum Beispiel eine Rutsche und eine Schaukel hinzu, indem Sie die Tabelle aufrufen, der Sie Daten hinzufügen möchten, die Spalten benennen und dann Daten für jede Spalte eingeben (wie folgt):
Sie sollten bei der Eingabe der Daten darauf achten, einige häufige Fehler zu vermeiden.
Setzen Sie den Spaltennamen zum Beispiel nicht in Anführungszeichen, aber die Spaltenwerte müssen mit Anführungszeichen eingegeben werden.
Sie sollten auch bedenken, dass Sie keinen Wert für die Spalte equip _ id eingeben dürfen.
Das liegt daran, dass dieser automatisch generiert wird, wenn Sie der Tabelle eine neue Zeile hinzufügen.
Sie können die hinzugefügten Informationen mit der folgenden Eingabe abrufen:
Hier können Sie sehen, dass Ihre equip _ id erfolgreich ausgefüllt wurde und alle anderen Daten korrekt organisiert wurden.
Wenn die Rutsche auf dem Spielplatz beschädigt wird und Sie diese entfernen müssen, können Sie auch die Zeile aus der Tabelle entfernen, indem Sie Folgendes eingeben:
Führen Sie eine erneute Abfrage der Tabelle durch:
Beachten Sie, dass die Zeile slide nicht mehr Teil der Tabelle ist.
Schritt 8 - Hinzufügen und Löschen von Spalten in einer Tabelle
Nach dem Erstellen einer Tabelle können Sie diese ändern, indem Sie Spalten hinzufügen oder entfernen.
Fügen Sie eine Spalte hinzu, um den letzten Wartungsbesuch für jedes Gerät zu zeigen, indem Sie Folgendes eingeben:
Wenn Sie sich die Tabellendaten erneut ansehen, werden Sie sehen, dass eine neue Spalte hinzugefügt wurde (aber keine Daten eingegeben wurden):
Wenn Sie feststellen, dass Ihre Arbeitskräfte ein anderes Tool verwendet, um die Wartungsarbeiten aufzuzeichnen, können Sie die Spalte löschen, indem Sie Folgendes eingeben:
Damit werden die Spalte last _ maint und alle darin enthaltenen Werte gelöscht, aber alle anderen Daten bleiben intakt.
Schritt 9 - Aktualisieren der Daten in einer Tabelle
Bisher haben Sie gelernt, wie man einer Tabelle Einträge hinzufügen und aus ihr löschen kann, aber dieses Tutorial hat noch nicht erklärt, wie man bestehende Einträge ändern kann.
Sie können die Werte einer vorhandenen Eingabe aktualisieren, indem Sie den gewünschten Datensatz abfragen und die Spalte auf den Wert einstellen, den Sie verwenden möchten.
Sie können nach dem Eintrag swing abfragen (das sorgt für einen Treffer mit jeder Schaukel in Ihrer Tabelle) und deren Farbe in red (rot) ändern. Dies kann nützlich sein, wenn die Schaukel neu gestrichen wurde:
Sie können verifizieren, dass die Arbeiten erfolgreich waren, indem Sie die Daten erneut abfragen:
Wie Sie sehen, wird die Farbe der Schaukel jetzt als "red" (rot) angegeben.
Sie haben PostgreSQL nun auf Ihrem Ubuntu 20.04-Server eingerichtet.
Wenn Sie mehr über Postgres und seine Funktionsweise erfahren möchten, empfehlen wir Ihnen, die folgenden Leitfäden zu konsultieren:
Ein Vergleich von relationalen Datenbank-Managementsystemen
Übung zur Ausführung von Abfragen mit SQL
Installieren von Jitsi Meet unter Ubuntu 18.04
4104
Jitsi Meet ist eine Open-Source-basierte Videokonferenzlösung auf Grundlage von WebRTC.
Ein Jitsi Meet-Server bietet Videokonferenzräume für mehrere Personen. Sie benötigen lediglich Ihren Browser, um Funktionen zu erhalten, die mit denen einer Telefonkonferenz bei Zoom oder Skype vergleichbar sind.
Der Vorteil einer Jitsi-Konferenz besteht darin, dass alle Ihre Daten nur über Ihren Server übertragen werden; durchgängige TLS-Verschlüsselung sorgt zudem dafür, dass Anrufe von niemandem abgehört werden können.
Mit Jitsi können Sie sicherstellen, dass Ihre privaten Daten auch privat bleiben.
In diesem Tutorial installieren und konfigurieren Sie einen Jitsi Meet-Server unter Ubuntu 18.04.
Die Standardkonfiguration ermöglicht es beliebigen Benutzern, einen neuen Konferenzraum zu eröffnen.
Das ist nicht ideal bei Servern, die im Internet öffentlich verfügbar sind. Darum werden Sie Jitsi Meet so konfigurieren, dass nur registrierte Benutzer neue Konferenzräume erstellen können.
Nachdem Sie den Konferenzraum erstellt haben, können beliebige Benutzer teilnehmen, solange sie über die eindeutige Adresse und das optionale Passwort verfügen.
Einen Ubuntu-18.04-Server, der gemäß Ersteinrichtung eines Servers mit Ubuntu 18.04 eingerichtet wurde, einschließlich eines Nicht-root-Benutzers mit sudo-Berechtigungen.
Die Größe des Servers, den Sie benötigen, hängt meistens von der verfügbaren Bandbreite und der Anzahl der Teilnehmer ab, die den Server voraussichtlich verwenden werden.
Die folgende Tabelle bietet Ihnen eine Vorstellung davon, was benötigt wird.
In diesem Leitfaden wird der Beispieldomänenname < ^ > jitsi.your-domain < ^ > verwendet.
Wenn Sie einen Server für die Ausführung Ihrer Jitsi Meet-Instanz auswählen, müssen Sie die Systemressourcen berücksichtigen, die zum Hosten von Konferenzräumen benötigt werden.
Die folgenden Benchmark-Daten wurden von einer virtuellen Maschine (Single-Core) mit hochwertigen Videoeinstellungen gesammelt:
CPU
Serverbandbreite
Zwei Teilnehmer
3%
30 Kb / s (Up), 100 Kb / s (Down)
Drei Teilnehmer
15%
7 Mb / s (Up), 6,5 Mb / s (Down)
Der Sprung bei der Ressourcennutzung zwischen zwei und drei Teilnehmern entsteht dadurch, dass Jitsi den Anruf direkt zwischen den Clients routen wird, wenn es zwei von ihnen gibt.
Wenn mehr als zwei Clients vorhanden sind, werden Anrufdaten über den Jitsi Meet-Server geleitet.
Schritt 1 - Einrichten des System-Hostnamens
In diesem Schritt ändern Sie den Hostnamen des Systems so, dass er dem Domänennamen entspricht, den Sie für Ihre Jitsi Meet-Instanz verwenden, und diesen Hostnamen auf die Localhost-IP 127.0.0.1 auflöst.
Jitsi Meet nutzt bei der Installation beide dieser Einstellungen und generiert die Konfigurationsdateien.
Setzen Sie zunächst den Hostnamen des Systems auf den Domänennamen, den Sie für Ihre Jitsi-Instanz verwenden werden.
Der folgende Befehl legt den aktuellen Hostnamen fest und ändert den / etc / hostname, der zwischen Neustarts den Hostnamen des Systems enthält:
Der Befehl, den Sie ausgeführt haben, sieht wie folgt aus:
hostnamectl ist ein Dienstprogramm aus der Toolsuite systemd zur Verwaltung des System-Hostnamens.
set-hostname legt den Hostnamen des Systems fest.
Überprüfen Sie, ob das erfolgreich war, indem Sie Folgendes ausführen:
Dadurch wird der Hostname zurückgegeben, den Sie mit dem Befehl hostnamectl festgelegt haben:
Als Nächstes richten Sie eine lokale Zuordnung des Hostnamens des Servers zur Loopback-IP-Adresse 127.0.0.1 ein.
Öffnen Sie dazu die Datei / etc / hosts mit einem Texteditor:
Fügen Sie dann die folgende Zeile hinzu:
Durch Zuordnung des Domänennamens Ihres Jitsi Meet-Servers zu 127.0.0.1 kann Ihr Jitsi Meet-Server verschiedene Netzwerkprozesse nutzen, die an der IP-Adresse 127.0.0.1 lokale Verbindungen zueinander zulassen.
Diese Verbindungen werden mit einem TLS-Zertifikat, das auf Ihren Domänennamen registriert ist, authentifiziert und verschlüsselt.
Durch lokale Zuordnung des Domänennamens zu 127.0.0.1 ​ wird es möglich, das TLS-Zertifikat für diese lokalen Netzwerkverbindungen zu verwenden.
Ihr Server verfügt nun über den Hostnamen, den Jitsi zur Installation benötigt.
Im nächsten Schritt öffnen Sie die Firewall-Ports, die von Jitsi und dem TLS-Zertifikat-Installer benötigt werden.
Schritt 2 - Konfigurieren der Firewall
Wenn Sie dem Leitfaden Ersteinrichtung eines Servers unter Ubuntu 18.04 gefolgt sind, haben Sie die UFW-Firewall aktiviert und den SSH-Port geöffnet. Der Jitsi-Server benötigt einige geöffnete Ports, damit er mit den Anrufclients kommunizieren kann.
Außerdem ist der TLS-Installationsprozess auf einen geöffneten Port angewiesen, damit er die Zertifikatsanfrage authentifizieren kann.
Die Ports, die Sie öffnen werden, sind folgende:
80 / tcp, der in der TLS-Zertifikatanfrage verwendet wird.
443 / tcp, der für die Webseite zur Erstellung von Konferenzräumen verwendet wird.
4443 / tcp, 10000 / udp, die zur Übertragung und zum Empfang des verschlüsselten Anrufverkehrs dienen.
Führen Sie folgende ufw-Befehle aus, um diese Ports zu öffnen:
Überprüfen Sie mit dem Befehl ufw status, ob sie alle hinzugefügt wurden:
Sie sehen folgende Ausgabe, wenn diese Ports geöffnet sind:
Der Server ist nun bereit für die Installation von Jitsi, die Sie im nächsten Schritt vornehmen werden.
Schritt 3 - Installieren von Jitsi Meet
In diesem Schritt fügen Sie Ihrem Server das Jitsi-Stable-Repository hinzu und installieren dann das Jitsi Meet-Paket aus diesem Repository.
Dadurch wird sichergestellt, dass Sie stets das neueste stabile Jitsi Meet-Paket ausführen.
Laden Sie zuerst den Jitsi-GPG-Schlüssel mit dem Download-Dienstprogramm wget herunter:
Der Paketmanager apt verwendet diesen GPG-Schlüssel zur Validierung der Pakete, die Sie aus dem Jitsi-Repository herunterladen werden.
Fügen Sie als Nächstes mit dem Dienstprogramm apt-key den heruntergeladenen GPG-Schlüssel dem Keyring von apt hinzu:
Sie können die GPG-Schlüsseldatei nun löschen, da sie nicht mehr benötigt wird:
Nun fügen Sie Ihrem Server das Jitsi-Repository hinzu, indem Sie eine neue Quelldatei erstellen, die das Jitsi-Repository enthält.
Öffnen und erstellen Sie die neue Datei mit Ihrem Editor:
Fügen Sie der Datei für das Jitsi-Repository folgende Zeile hinzu:
Speichern und schließen Sie Ihren Editor.
Führen Sie abschließend eine Systemaktualisierung durch, um die Paketliste aus dem Jitsi-Repository zu erhalten, und installieren Sie dann das Paket jitsi-meet:
Während der Installation von jitsi-meet werden Sie aufgefordert, den Domänennamen einzugeben (z. B. jitsi.your-domain), den Sie für Ihre Jitsi Meet-Instanz verwenden möchten.
Bild mit dem Hostnamen-Dialogfeld bei der Installation von jitsi-meet
< $> note Anmerkung: Bewegen Sie den Cursor aus dem Hostnamenfeld, um die Schaltfläche
mit der Taste TAB auszuwählen.
Drücken Sie ENTER, wenn
ausgewählt ist, um den Hostnamen zu übermitteln.
Dann sehen Sie ein neues Dialogfeld, in dem Sie gefragt werden, ob Jitsi ein selbstsigniertes TLS-Zertifikat erstellen und verwenden oder ein bestehendes Zertifikat, das bereits vorhanden ist, nutzen soll:
Bild mit dem Zertifikat-Dialogfeld bei der Installation von jitsi-meet
Wenn Sie über kein TLS-Zertifikat für Ihre Jitsi-Domäne verfügen, wählen Sie die erste Option Generate a new self-signed certificate (Generieren eines neuen selbstsignierten Zertifikats).
Ihre Jitsi Meet-Instanz wurde nun mit einem selbstsignierten TLS-Zertifikat installiert.
Das führt dazu, dass Browserwarnungen angezeigt werden. Darum erhalten Sie im nächsten Schritt ein signiertes TLS-Zertifikat.
Schritt 4 - Abrufen eines signierten TLS-Zertifikats
Jitsi Meet verwendet TLS-Zertifikate, um den Anrufverkehr zu verschlüsseln, damit Ihre Anrufe bei der Übertragung über das Internet nicht abgehört werden können.
TLS-Zertifikate sind die gleichen Zertifikate, die von Websites zur Aktivierung von HTTPS-URLs verwendet werden.
Jitsi Meet bietet ein Programm zum automatischen Herunterladen eines TLS-Zertifikats für Ihren Domänennamen, das das Dienstprogramm Certbot nutzt.
Sie müssen dieses Programm installieren, bevor Sie das Zertifikatinstallationsskript ausführen.
Fügen Sie Ihrem System zuerst das Certbot-Repository hinzu, um sicherzustellen, dass Sie über die neueste Version von Certbot verfügen.
Führen Sie den folgenden Befehl aus, um das neue Repository hinzuzufügen und Ihr Systems zu aktualisieren:
Installieren Sie als Nächstes das Paket certbot:
Ihr Server ist nun bereit, das von Jitsi Meet zur Verfügung gestellte Installationsprogramm für TLS-Zertifikate auszuführen:
Wenn Sie das Skript ausführen, sehen Sie folgende Aufforderung zur Eingabe einer E-Mail-Adresse:
Diese E-Mail-Adresse wird dem Zertifikatsaussteller https: / / letsencrypt.org übermittelt und zur Meldung von Sicherheitsaspekten und anderen Fragen genutzt, die mit dem TLS-Zertifikat zusammenhängen.
Sie müssen hier eine E-Mail-Adresse eingeben, um mit der Installation fortzufahren.
Die Installation wird dann ohne weitere Eingabeaufforderungen abgeschlossen.
Nach der Fertigstellung ist Ihre Jitsi Meet-Instanz so konfiguriert, dass für Ihren Domänennamen ein signiertes TLS-Zertifikat verwendet wird.
Zertifikaterneuerungen erfolgen auch automatisch, da der Installer unter / etc / cron.weekly / letsencrypt-renew ein Erneuerungsskript platziert hat, das jede Woche ausgeführt wird.
Der TLS-Installer hat Port 80 verwendet, um zu verifizieren, dass Sie die Kontrolle über Ihren Domänennamen haben.
Nachdem Sie das Zertifikat nun erhalten haben, benötigt Ihr Server den geöffneten Port 80 nicht mehr, da Port 80 für regulären, nicht verschlüsselten HTTP-Verkehr genutzt wird.
Jitsi Meet stellt seine Website ausschließlich über HTTPS an Port 443 bereit.
Schließen Sie diesen Port in Ihrer Firewall mit dem folgenden ufw-Befehl:
Ihr Jitsi Meet-Server ist nun fertig und bereit zum Testen.
Öffnen Sie einen Browser und verweisen Sie ihn auf Ihren Domänennamen.
Sie können einen neuen Konferenzraum erstellen und andere dazu einladen, teilzunehmen.
Die Standardkonfiguration für Jitsi Meet lautet, dass jeder, der Ihre Jitsi Meet-Server-Homepage besucht, einen neuen Konferenzraum erstellen kann.
Dabei werden zur Ausführung des Konferenzraums die Systemressourcen Ihres Servers verwendet, was bei unbefugten Benutzern nicht erwünscht ist.
Im nächsten Schritt konfigurieren Sie Ihre Jitsi Meet-Instanz so, dass nur noch registrierte Benutzer Konferenzräume erstellen können.
Schritt 5 - Sperren der Erstellung von Konferenzräumen
Im nächsten Schritt konfigurieren Sie Ihren Jitsi Meet-Server so, dass nur registrierte Benutzer Konferenzräume erstellen können.
Die Dateien, die Sie bearbeiten werden, wurden vom Installer generiert und mit Ihrem Domänennamen konfiguriert.
Die Variable < ^ > your _ domain < ^ > wird in den folgenden Beispielen anstelle eines Domänennamens verwendet.
Öffnen Sie zuerst sudo nano / etc / prosody / conf.avail / your _ domain.cfg.lua mit einem Texteditor:
Bearbeiten Sie diese Zeile:
Tun Sie Folgendes:
Diese Konfiguration weist Jitsi Meet dazu an, eine Authentifizierung mit Benutzername und Passwort durchzusetzen, bevor neue Besucher einen Konferenzraum erstellen können.
Fügen Sie dann in der gleichen Datei folgenden Abschnitt am Ende der Datei hinzu:
Diese Konfiguration ermöglicht es anonymen Benutzern, Konferenzräumen beizutreten, die von einem authentifizierten Benutzer erstellt wurden.
Der Gast muss jedoch über eine eindeutige Adresse und ein optionales Passwort für das Betreten des Raums verfügen.
Hier haben Sie guest. am Anfang Ihres Domänennamens hinzugefügt.
Für jitsi.your-domain würden Sie zum Beispiel guest.jitsi.your-domain eingeben.
Den guest.-Hostnamen nutzt Jitsi Meet ausschließlich intern.
Sie werden ihn niemals in einen Browser eingeben oder einen DNS-Eintrag dafür erstellen müssen.
Öffnen Sie mit einem Texteditor eine weitere Konfigurationsdatei unter / etc / jitsi / meet / your _ domain-config.js:
Auch hier weist der Hostname guest. < ^ > your _ domain < ^ >, den Sie zuvor in dieser Konfiguration verwendet haben, Jitsi Meet an, welcher interne Hostname für die nicht authentifizierten Gäste verwendet werden soll.
Öffnen Sie als Nächstes / etc / jitsi / jicofo / sip-communicator.properties:
Und fügen Sie die folgende Zeile hinzu, um die Konfigurationsänderungen abzuschließen:
Diese Konfiguration verweist einen der Jitsi Meet-Prozesse auf den lokalen Server, der die jetzt obligatorische Benutzerauthentifizierung vornimmt.
Ihre Jitsi Meet-Instanz ist nun so konfiguriert, dass nur registrierte Benutzer Konferenzräume erstellen können.
Nach der Erstellung eines Konferenzraums können ihm sowohl registrierte als auch nicht registrierte Benutzer beitreten.
Sie benötigen lediglich die eindeutige Adresse des Konferenzraums und ein optionales Passwort, das ggf. vom Ersteller des Raums festgelegt wurde.
Nachdem Jitsi Meet nun so konfiguriert ist, dass nur authentifizierte Benutzer Räume erstellen können, müssen Sie diese Benutzer und ihre Passwörter registrieren.
Dazu verwenden Sie das Dienstprogramm prosodyctl.
Führen Sie den folgenden Befehl aus, um Ihrem Server einen Benutzer hinzuzufügen:
Der Benutzer, den Sie hier hinzufügen, ist kein Systembenutzer.
Sie können lediglich einen Konferenzraum erstellen und sich nicht über SSH bei Ihrem Server anmelden.
Starten Sie abschließend die Jitsi Meet-Prozesse neu, um die neue Konfiguration zu laden:
Die Jitsi Meet-Instanz wird nun mit einem Dialogfeld einen Benutzernamen und ein Passwort anfordern, sobald ein Konferenzraum erstellt wird.
Bild mit dem Jitsi-Dialogfeld für Benutzername und Passwort
Ihr Jitsi Meet-Server ist nun eingerichtet und sicher konfiguriert.
In diesem Artikel haben Sie einen Jitsi Meet-Server bereitgestellt, mit dem Sie sichere und private Videokonferenzräume hosten können.
Sie können Ihre Jitsi Meet-Instanz mit Anweisungen aus dem Jitsi Meet-Wiki erweitern.
Installieren des Linux-, Apache-, MySQL-, PHP- (LAMP-) Stacks unter Ubuntu 20.04
5239
Ein "LAMP" -Stack ist eine Gruppe von Open-Source-Software, die normalerweise zusammenhängend installiert wird, damit ein Server dynamische Websites und in PHP geschriebene Web-Apps hosten kann.
Die Daten dieser Site werden in einer MySQL-Datenbank gespeichert, und der dynamische Inhalt wird von PHP verarbeitet.
In diesem Leitfaden installieren wir einen LAMP Stack auf einem Ubuntu 20.04 Server.
Zum Absolvieren dieses Tutorials benötigen Sie einen Ubuntu 20.04 Server mit einem Nicht-root sudo-aktivierten Benutzerkonto und einer einfachen Firewall.
Dieser kann mit unserem Leitfaden zur Ersteinrichtung eines Servers für Ubuntu 20.04 konfiguriert werden.
Schritt 1 - Installieren von Apache und Aktualisieren der Firewall
Der Apache-Webserver gehört zu den weltweit beliebtesten Webservern.
Er ist gut dokumentiert, verfügt über eine aktive Benutzergemeinschaft und ist seit den Anfängen des Webs weit verbreitet. Dadurch eignet sich der Server gut für das Hosten von Websites.
Installieren Sie Apache mit Ubuntus Paketmanager apt:
Wenn Sie sudo in dieser Sitzung zum ersten Mal verwenden, werden Sie dazu aufgefordert, das Passwort Ihres Benutzers anzugeben, um zu bestätigen, dass Sie über die richtigen Berechtigungen zur Verwaltung von Systempaketen mit apt verfügen.
Außerdem werden Sie zur Bestätigung der Installation von Apache aufgefordert, indem Sie Y und dann die Eingabetaste drücken.
Sobald die Installation abgeschlossen ist, müssen Sie Ihre Firewalleinstellungen so anpassen, dass HTTP- und HTTPS-Verkehr erlaubt ist.
UFW verfügt über verschiedene Anwendungsprofile, die Sie dafür verwenden können.
Um alle aktuell verfügbaren UFW-Anwendungsprofile aufzulisten, können Sie Folgendes ausführen:
Das bedeuten die einzelnen Profile:
Apache: Dieses Profil öffnet nur Port 80 (normaler, unverschlüsselter Webverkehr).
Apache Full: Dieses Profil öffnet sowohl Port 80 (normaler, unverschlüsselter Webverkehr) als auch Port 443 (mit TLS / SSL verschlüsselter Verkehr).
Apache Secure: Dieses Profil öffnet nur Port 443 (mit TLS / SSL verschlüsselter Verkehr).
Erst einmal ist es am besten, nur Verbindungen an Port 80 zuzulassen, da es sich um eine neue Apache-Installation handelt und Sie noch kein TLS / SSL-Zertifikat konfiguriert haben, um HTTPS-Verkehr auf Ihrem Server zuzulassen.
Um Verkehr nur bei Port 80 zuzulassen, verwenden Sie das Apache-Profil:
Sie können die Änderung überprüfen mit:
Datenverkehr an Port 80 ist nun über die Firewall zugelassen.
Sie können eine Stichprobenkontrolle vornehmen, um zu verifizieren, dass alles wie geplant vorgenommen wurde, indem Sie die öffentliche IP-Adresse Ihres Servers im Webbrowser aufrufen (siehe Anmerkung unter der nächsten Rubrik, um zu erfahren, wie die öffentliche IP-Adresse lautet, sofern Sie diese Informationen noch nicht haben):
Es wird die Standard-Webseite für Ubuntu 20.04 Apache angezeigt, die Informations- und Testzwecken dient.
Sie sollte ungefähr wie folgt aussehen:
Ubuntu 20.04 Apache Standard
Wenn Sie diese Seite sehen, ist Ihr Webserver korrekt installiert und über Ihre Firewall zugänglich.
So finden Sie die öffentliche IP-Adresse Ihres Servers
Wenn Sie die öffentliche IP-Adresse Ihres Servers nicht kennen, haben Sie mehrere Möglichkeiten, sie zu finden. Es ist normalerweise die Adresse, die Sie zur Verbindung mit Ihrem Server über SSH verwenden.
Dies kann auf unterschiedliche Weise über die Befehlszeile erfolgen.
Sie können zunächst die iproute2-Tools verwenden, um Ihre IP-Adresse zu erhalten, indem Sie Folgendes eingeben:
Damit erhalten Sie zwei oder drei Zeilen.
Das sind alle korrekte Adressen, aber Ihr Computer kann ggf. nur eine davon verwenden. Probieren Sie daher jede aus.
Eine alternative Methode ist die Verwendung des Dienstprogramms curl, um einen Dritten zu kontaktieren, der Ihnen sagt, wie ihm Ihr Server angezeigt wird.
Dazu wird ein bestimmter Server nach Ihrer IP-Adresse gefragt:
Unabhängig davon, wie Sie Ihre IP-Adresse in Erfahrung bringen, geben Sie sie in die Adressleiste Ihres Webbrowsers ein, um die Standard-Apache-Seite zu sehen.
Schritt 2 - Installation von MySQL
Nachdem Sie nun einen funktionierenden Webserver eingerichtet haben, müssen Sie das Datenbanksystem installieren, um Daten für Ihre Website speichern und verwalten zu können.
MySQL ist ein beliebtes Datenbank-Managementsystem, das in PHP-Umgebungen zum Einsatz kommt.
Verwenden Sie auch hier wieder apt zur Beschaffung und Installation der Software:
Wenn Sie dazu aufgefordert werden, bestätigen Sie die Installation, indem Sie Y eingeben und dann ENTER drücken.
Sobald die Installation abgeschlossen ist, empfehlen wir, ein Sicherheitsskript auszuführen, das in MySQL vorinstalliert ist.
Sie werden gefragt, ob Sie das VALIDATE PASSWORD PLUGIN konfigurieren möchten.
< $> note Anmerkung: Die Aktivierung dieser Funktion bleibt Ihnen überlassen.
Sollten Sie sie aktivieren, werden Passwörter, die nicht den angegebenen Kriterien entsprechen, als Fehler von MySQL abgelehnt.
Sie können die Validierung deaktiviert lassen, aber sollten immer starke, eindeutige Passwörter für die Datenbankinformationen verwenden.
Geben Sie Y für Ja oder etwas Anderes ein, um ohne Aktivierung weiterzumachen.
Wenn Sie mit "ja" antworten, werden Sie dazu aufgefordert, eine Stufe der Passwortvalidierung zu wählen.
Denken Sie daran, dass die Auswahl von 2 als stärkste Validierungsstufe Fehler ergibt, wenn Sie ein Passwort ohne Zahlen, Buchstaben, Klein- oder Großbuchstaben und Sonderzeichen einrichten oder eines aus geläufigen Wörtern aus einem Wörterbuch.
Unabhängig davon, ob Sie sich für die Einrichtung des VALIDATE PASSWORD PLUGIN entschieden haben, wird der Server Sie als nächstes auffordern, ein Passwort für den MySQL root-Benutzer zu wählen und zu bestätigen.
Verwechseln Sie dies nicht mit dem Benutzer system root.
Zwar entbindet die Standardauthentifizierungsmethode für den MySQL root user von der Verwendung eines Passworts, selbst wenn eines festgelegt ist, doch sollten Sie hier als zusätzliche Sicherheitsmaßnahme ein starkes Passwort definieren.
Darüber werden wir in einem Moment sprechen.
Wenn Sie Passwortvalidierung aktiviert haben, wird Ihnen die Passwortstärke des soeben eingegebenen root-Passworts angezeigt und Sie werden gefragt, ob Sie das Passwort beibehalten möchten.
Wenn Sie mit Ihrem aktuellen Passwort zufrieden sind, geben Sie in der Eingabeaufforderung Y für "ja" ein:
Drücken Sie bei den restlichen Fragen auf Y und bei jeder Eingabeaufforderung die ENTER.
Wenn Sie damit fertig sind, testen Sie, ob Sie sich bei der MySQL-Konsole anmelden können, indem Sie Folgendes eingeben:
Damit wird eine Verbindung zum MySQL-Server als administrativer Datenbank-Benutzer root hergestellt, was durch Nutzung von sudo abgeleitet wird, wenn dieser Befehl ausgeführt wird.
Um die MySQL-Konsole zu beenden, geben Sie Folgendes ein:
Beachten Sie, dass Sie kein Passwort angeben mussten, um als root-Benutzer eine Verbindung herzustellen, obwohl Sie bei der Ausführung des Skripts mysql _ secure _ installation ein Passwort festgelegt haben.
Das liegt daran, dass die standardmäßige Authentifizierungsmethode für den administrativen MySQL-Benutzer unix _ socket lautet und nicht password.
Auch wenn es zunächst wie ein Sicherheitsproblem aussehen mag, wird der Datenbankserver dadurch sicherer, da sich nur die Systembenutzer mit sudo-Berechtigungen über die Konsole oder über eine Anwendung, die mit den gleichen Berechtigungen ausgeführt wird, als root-MySQL-Benutzer anmelden dürfen.
Das Einrichten eines Passworts für das root-MySQL-Konto funktioniert als Schutz für den Fall, dass die Standardauthentifizierungsmethode von unix _ socket in passwort geändert wird.
< $> note Anmerkung: Zum Zeitpunkt der Verfassung dieses Dokuments unterstützt die native MySQL-PHP-Bibliothek mysqlnd keine caching _ sha2 _ authentication, die standardmäßige Authentifizierungsmethode für MySQL 8. Wenn Sie Datenbankbenutzer für PHP-Anwendungen unter MySQL 8 erstellen, müssen Sie sie daher so konfigurieren, dass sie stattdessen mysql _ native _ password verwenden.
In Schritt 6 zeigen wir, wie das geht. < $>
Ihr MySQL-Server ist nun installiert und gesichert.
Als Nächstes installieren wir PHP, die letzte Komponente im LAMP-Stack.
Sie haben Apache zur Bereitstellung Ihrer Inhalte und MySQL zur Speicherung und Verwaltung Ihrer Daten installiert. PHP ist die Komponente unserer Einrichtung, die Code verarbeitet, um dynamische Inhalte für den Endbenutzer anzuzeigen.
Zusätzlich zum php-Paket benötigen Sie php-mysqlnd, ein PHP-Modul, das PHP die Kommunikation mit MySQL-basierten Datenbanken ermöglicht.
Außerdem benötigen Sie libapache2-mod-php, damit Apache PHP-Dateien verarbeiten kann.
Um diese Pakete zu installieren, führen Sie Folgendes aus:
Sobald die Installation abgeschlossen ist, können Sie folgenden Befehl ausführen, um Ihre PHP-Version zu prüfen:
Nun ist Ihr LAMP-Stack voll einsatzbereit. Bevor Sie aber Ihre Einrichtung mit einem PHP-Skript testen können, ist es am besten, einen richtigen virtuellen Apache-Host für die Speicherung der Dateien und Ordner Ihrer Website einzurichten.
Das tun wir im nächsten Schritt.
Schritt 4 - Erstellen eines virtuellen Hosts für Ihre Website
Wenn Sie den Apache-Webserver verwenden, können Sie virtuelle Hosts (ähnlich wie Serverblocks in Nginx) einrichten, um Konfigurationsdaten einzuschließen und mehr als eine Domäne auf einem Server zu hosten.
In diesem Leitfaden richten wir eine Domäne namens your _ domain ein, aber Sie sollten diesen Namen durch Ihren eigenen Domänenamen ersetzen.
Apache unter Ubuntu 20.04 hat einen Serverblock, der standardmäßig aktiviert und so konfiguriert ist, dass er Dokumente aus dem / var / www / html-Verzeichnis bereitstellt.
Das eignet sich gut für eine Site, kann aber umständlich werden, wenn Sie mehrere Sites hosten.
Statt / var / www / html zu ändern, erstellen wir eine Verzeichnisstruktur innerhalb von / var / www für die Site your _ domain und belassen dabei / var / www / html als Standardverzeichnis, das genutzt wird, wenn eine Clientanfrage keine übereinstimmenden Sites ergibt.
Erstellen Sie das Verzeichnis für your _ domain wie folgt:
Als Nächstes weisen Sie die Eigentumsrechte des Verzeichnisses mit der Umgebungsvariablen $USER zu, die auf Ihren aktuellen Systembenutzer verweisen wird:
Öffnen Sie dann mit Ihrem bevorzugten Befehlszeileneditor eine neue Konfigurationsdatei im Verzeichnis sites-available von Apache.
Wir verwenden hier nano:
Dadurch wird eine neue Leerdatei erstellt.
Fügen Sie die folgende Basiskonfiguration ein:
Mit dieser VirtualHost-Konfiguration weisen wir Apache an, < ^ > your _ domain < ^ > mit / var / www / < ^ > your _ domain < ^ > als Web-Stammverzeichnis bereitzustellen.
Wenn Sie Apache ohne Domänennamen testen möchten, können Sie die Optionen ServerName und ServerAlias entfernen oder auskommentieren, indem Sie am Anfang der Zeilen jeder Option ein # -Zeichen hinzufügen.
Sie können nun a2ensite verwenden, um den neuen virtuellen Host zu aktivieren:
Vielleicht möchten Sie die Standard-Website, die mit Apache installiert wird, deaktivieren.
Das ist erforderlich, wenn Sie keinen benutzerdefinierten Domänennamen verwenden, da in dem Fall die Standardkonfiguration von Apache Ihren virtuellen Host überschreiben würde.
Um die Standard-Website von Apache zu deaktivieren, geben Sie Folgendes ein:
Um sicherzustellen, dass Ihre Konfigurationsdatei keine Syntaxfehler enthält, führen Sie Folgendes aus:
Schließlich laden Sie Apache neu, damit diese Änderungen wirksam werden:
Ihre neue Website ist nun aktiv, aber der Web-Stamm / var / www / < ^ > your _ domain < ^ > ist immer noch leer.
Erstellen Sie an diesem Ort eine index.html-Datei, damit wir testen können, ob der virtuelle Host wie erwartet funktioniert:
Fügen Sie in die Datei folgenden Inhalt ein:
Greifen Sie jetzt über Ihren Browser erneut auf den Domänenamen oder die IP-Adresse Ihres Servers zu:
Test für einen virtuellen Apache Host
Wenn Sie diese Seite sehen, bedeutet das, dass Ihr virtueller Apache-Host wie erwartet funktioniert.
Sie können diese Datei als temporäre Zielseite für Ihre Anwendung so lange belassen, bis Sie sie durch eine index.php-Datei ersetzen. Vergessen Sie in dem Fall nicht, die Datei index.html aus Ihrem Dokumentstamm zu entfernen oder umzubenennen, da sie standardmäßig Vorrang gegenüber einer index.php-Datei erhalten würde.
Hinweis zu DirectoryIndex in Apache
Mit den Standardeinstellungen von DirectoryIndex in Apache wird eine Datei namens index.html immer Vorrang gegenüber einer index.php-Datei erhalten.
Das ist für die Einrichtung von Wartungsseiten in PHP-Anwendungen nützlich, da eine temporäre Datei index.html erstellt wird, die eine Informationsmeldung für Besucher enthält.
Da diese Seite Vorrang gegenüber der Seite index.php hat, wird sie dann zur Zielseite für die Anwendung.
Sobald die Wartung abgeschlossen ist, wird die Datei index.html umbenannt oder aus dem Dokumentenstamm entfernt. Damit kehrt die regelmäßige Anwendungsseite zurück.
Wenn Sie dieses Verhalten ändern möchten, müssen Sie die Datei / etc / apache2 / mods-enabled / dir.conf bearbeiten und die Reihenfolge ändern, in der die Datei index.php in der Anweisung DirectoryIndex aufgeführt ist:
Nach dem Speichern und Schließen der Datei müssen Sie Apache neu laden, damit die Änderungen wirksam werden:
Im nächsten Schritt erstellen wir ein PHP-Skript, um zu testen, ob PHP auf Ihrem Server korrekt installiert und konfiguriert wurde.
Schritt 5 - Test der PHP-Verarbeitung auf Ihrem Webserver
Nachdem Sie nun über einen benutzerdefinierten Speicherort zum Hosten der Dateien und Ordner Ihrer Website verfügen, erstellen wir ein PHP-Testskript, um zu prüfen, ob Apache Anfragen nach PHP-Dateien bewältigen und verarbeiten kann.
Erstellen Sie eine neue Datei namens info.php in Ihrem benutzerdefinierten Web-Stammordner:
Damit wird eine leere Datei geöffnet.
Fügen Sie der Datei den folgenden Text, d. h. den gültigen PHP-Code, hinzu:
Um das Skript zu testen, rufen Sie in Ihrem Webbrowser den Domänenamen oder die IP-Adresse Ihres Servers auf, gefolgt vom Skriptnamen (in diesem Fall info.php):
Ubuntu 20.04 PHP-Informationen
Diese Seite liefert Informationen über den Server aus der Sicht von PHP.
Sie ist zum Debuggen nützlich und um sicherzustellen, dass Ihre Einstellungen korrekt angewendet werden.
Wenn Sie diese Seite in Ihrem Browser sehen können, funktioniert Ihre PHP-Installation wie erwartet.
Nachdem Sie über diese Seite die relevanten Informationen zu Ihrem PHP-Server überprüft haben, ist es am besten, die von Ihnen erstellte Datei zu entfernen, da sie sensible Informationen über Ihre PHP-Umgebung und Ihren Ubuntu-Server enthält.
Sie können dazu rm verwenden:
Sie können diese Seite immer wieder neu erstellen, wenn Sie später auf die Informationen zugreifen müssen.
Schritt 6 - Testen der Datenbankverbindung von PHP (optional)
Wenn Sie testen möchten, ob PHP eine Verbindung mit MySQL herstellen und Datenbankabfragen ausführen kann, können Sie eine Testtabelle mit Pseudodaten erstellen und die Inhalte mit einem PHP-Skript abfragen.
Zuvor müssen wir eine Testdatenbank und einen neuen MySQL-Benutzer erstellen, der für den Zugriff richtig konfiguriert ist.
Zum Zeitpunkt der Verfassung dieses Dokuments unterstützt die native MySQL-PHP-Bibliothek mysqlnd nicht caching _ sha2 _ authentication, die standardmäßige Authentifizierungsmethode für MySQL 8. Wir müssen einen neuen Benutzer mit der Authentifizierungsmethode mysql _ native _ password erstellen, um über PHP eine Verbindung zur MySQL-Datenbank herzustellen.
Wir erstellen eine Datenbank namens example\ _ database und einen Benutzer namens example\ _ user. Sie können diese Namen jedoch durch andere Werte ersetzen.
Stellen Sie zuerst unter Verwendung des root-Kontos eine Verbindung zur MySQL-Konsole her:
Um eine neue Datenbank zu erstellen, führen Sie den folgenden Befehl über Ihre MySQL-Konsole aus:
Der folgende Befehl erstellt einen neuen Benutzer namens < ^ > example _ user < ^ >, wobei mysql _ native _ password als standardmäßige Authentifizierungsmethode dient.
Wir definieren das Passwort dieses Benutzers als < ^ > password < ^ >, aber Sie sollten diesen Wert durch ein sicheres Passwort Ihrer Wahl ersetzen:
Nun müssen wir diesem Benutzer eine Berechtigung für die Datenbank example _ database erteilen:
Beenden Sie nun die MySQL-Shell mit:
Sie können testen, ob der neue Benutzer die richtigen Berechtigungen hat, indem Sie sich erneut bei der MySQL-Konsole anmelden, diesmal mit den benutzerdefinierten Anmeldedaten:
Beachten Sie das -p-Flag in diesem Befehl, das Sie nach dem Passwort fragt, das Sie bei der Erstellung des Benutzers example\ _ user gewählt haben.
Nach der Anmeldung bei der MySQL-Konsole bestätigen Sie, dass Sie Zugriff auf die Datenbank example\ _ database haben:
Als Nächstes erstellen wir eine Testtabelle namens todo _ list.
Führen Sie die folgende Anweisung in der MySQL-Konsole aus:
Geben Sie einige Zeilen an Inhalt in die Testtabelle ein.
Nachdem Sie bestätigt haben, dass Sie gültige Daten in Ihrer Testtabelle haben, können Sie die MySQL-Konsole verlassen:
Sie können nun das PHP-Skript erstellen, das sich mit MySQL verbindet, und Ihre Inhalte abfragen.
Das folgende PHP-Skript verbindet sich mit der MySQL-Datenbank und fragt den Inhalt der Tabelle todo _ list ab, wobei die Ergebnisse in einer Liste angezeigt werden.
Wenn ein Problem mit der Datenbankverbindung besteht, wird eine Ausnahme ausgelöst.
Kopieren Sie diesen Inhalt in Ihr todo _ list.php-Skript:
Sie können diese Seite nun in Ihrem Webbrowser aufrufen, indem Sie den Domänenamen oder die öffentliche IP-Adresse für Ihre Website aufrufen, gefolgt von / todo _ list.php:
Das bedeutet, dass Ihre PHP-Umgebung bereit dazu ist, sich mit Ihrem MySQL-Server zu verbinden und zu interagieren.
In diesem Leitfaden haben wir eine flexible Basis für die Bereitstellung von PHP-Webseiten und -Anwendungen für Ihre Besucher eingerichtet, wobei Apache als Webserver und MySQL als Datenbanksystem dienen.
Installieren von Python 3 und Einrichten einer Programmierumgebung auf einem Ubuntu 20.04-Server
5300
Die Python-Programmiersprache ist eine zunehmend beliebte Wahl für Anfänger und auch erfahrene Entwickler.
Python ist flexibel und vielseitig und bietet Vorteile in den Bereichen Skripterstellung, Automatisierung, Datenanalysen, maschinelles Lernen und Backend-Entwicklung.
Python wurde 1991 erstmals veröffentlicht; der Name ist von der britischen Komikergruppe Monty Python inspiriert. Das Entwicklerteam wollte mit Python eine Sprache schaffen, deren Verwendung Spaß macht.
In diesem Tutorial richten Sie Ihren Ubuntu 20.04-Server mit einer Python 3-Programmierumgebung ein.
Das Programmieren auf einem Server bietet viele Vorteile und fördert die Zusammenarbeit bei Entwicklungsprojekten.
Die allgemeinen Grundsätze dieses Tutorials gelten für alle Distributionen von Debian Linux.
Um dieses Tutorial absolvieren zu können, benötigen Sie einen non-root user mit sudo-Berechtigungen auf einem Ubuntu 20.04-Server.
Um zu erfahren, wie Sie diese Einrichtung erreichen, befolgen Sie unseren Leitfaden zur Ersteinrichtung des Servers.
Wenn Sie noch nicht mit einer Terminalumgebung vertraut sind, können Sie im Artikel "Eine Einführung in das Linux-Terminal" (https: / / www.digitalocean.com / community / tutorials / an-introduction-to-the-linux-terminal) mehr über das Terminal erfahren.
Nach der Einrichtung Ihres Servers und Benutzers können Sie loslegen.
Schritt 1 - Einrichten von Python 3
Ubuntu 20.04 und andere Versionen von Debian Linux werden mit vorinstalliertem Python 3 ausgeliefert.
Um sicherzustellen, dass unsere Versionen aktuell sind, aktualisieren wir das System mit dem Befehl apt, um das Advanced Packaging Tool von Ubuntu zu nutzen:
Das Flag -y bestätigt, dass wir mit der Installation aller Elemente einverstanden sind. Je nach Ihrer Linux-Version müssen Sie aber ggf. zusätzliche Eingabeaufforderungen bei Aktualisierungen und Upgrades Ihres Systems bestätigen.
Nach Abschluss des Verfahrens können wir die im System installierte Version von Python 3 überprüfen, indem wir Folgendes eingeben:
Sie erhalten eine Ausgabe im Terminalfenster, in der die Versionsnummer steht.
Zwar kann Ihre Zahl anders sein, die Ausgabe wird aber etwa wie folgt aussehen:
Um Softwarepakete für Python zu verwalten, installieren wir pip, ein Tool, das Programmierpakete installieren und verwalten wird, die wir möglicherweise in unseren Entwicklungsprojekten verwenden möchten.
Sie können mehr über Module oder Pakete erfahren, die Sie mit pip installieren können, indem Sie "Importieren von Modulen in Python 3" lesen.
Python-Pakete lassen sich installieren, indem Sie Folgendes eingeben:
Hier kann sich < ^ > package _ name < ^ > auf beliebige Python-Pakete oder Bibliotheken beziehen, wie Django für die Webentwicklung oder NumPy für wissenschaftliches Rechnen.
Wenn Sie NumPy installieren möchten, können Sie dies mit dem Befehl pip3 install numpy tun.
Es sind noch einige weitere Pakete und Entwicklungstools zu installieren, um sicherzustellen, dass wir eine robuste Einrichtung für unsere Programmierumgebung haben:
Nach der Einrichtung von Python und der Installation von pip und anderen Tools können wir eine virtuelle Umgebung für unsere Entwicklungsprojekte einrichten.
Schritt 2 - Einrichten einer virtuellen Umgebung
Mit virtuellen Umgebungen können Sie einen isolierten Bereich auf Ihrem Server für Python-Projekte schaffen, sodass jedes Ihrer Projekte einen eigenen Satz von Abhängigkeiten aufweist, der andere Projekte nicht stört.
Das Einrichten einer Programmierumgebung bietet eine größere Kontrolle über Python-Projekte und die Handhabung verschiedener Versionen von Paketen.
Das ist beim Einsatz von Paketen anderer Anbieter besonders wichtig.
Sie können so viele Python-Programmierumgebungen einrichten wie nötig.
Jede Umgebung ist im Grunde genommen ein Verzeichnis oder Ordner auf Ihrem Server, das bzw. der Skripte zum Ausführen einer Umgebung enthält.
Es gibt zwar verschiedene Möglichkeiten, um eine Programmierumgebung in Python einzurichten, aber wir verwenden hier das Modul venv, das Teil der Standardbibliothek von Python 3 ist.
Wir installieren venv, indem wir Folgendes eingeben:
Nach der Installation sind wir bereit, Umgebungen zu erstellen.
Wählen Sie entweder ein Verzeichnis, in das Sie Ihre Python-Programmierumgebungen einfügen möchten, oder erstellen Sie mit mkdir wie folgt ein neues Verzeichnis:
Sobald Sie sich im Verzeichnis befinden, in dem Sie die Umgebungen einrichten möchten, können Sie eine Umgebung erstellen, indem Sie den folgenden Befehl ausführen:
Im Grunde genommen richtet pyvenv ein neues Verzeichnis ein, das einige Elemente enthält, die wir mit dem Befehl ls anzeigen können:
Zusammen sorgen diese Dateien dafür, dass Ihre Projekte vom breiteren Kontext Ihres Servers isoliert werden, sodass sich Systemdateien und Projektdateien nicht vermischen.
Dies ist eine gute Praxis für die Versionsverwaltung und sorgt dafür, dass jedes Ihrer Projekte Zugriff auf die jeweils benötigten Pakete hat.
Python Wheels, ein für Python entwickeltes Paketformat, das Ihre Software-Produktion beschleunigen kann, indem es die Anzahl der benötigten Kompilierungen reduziert, wird sich im Verzeichnis share von Ubuntu 20.04 befinden.
Um diese Umgebung zu verwenden, müssen Sie sie aktivieren. Das können Sie tun, indem Sie den folgenden Befehl eingeben, der das Skript activate aufruft:
Ihrer Eingabeaufforderung wird nun der Name Ihrer Umgebung vorangestellt. In diesem Fall heißt sie < ^ > my _ env < ^ >:
Je nach der Version von Debian Linux, die Sie ausführen, sieht Ihr Präfix möglicherweise etwas anders aus, der Name Ihrer Umgebung in Klammern sollte jedoch das Erste sein, das Sie in der Zeile sehen:
Dieses Präfix lässt uns wissen, dass die Umgebung < ^ > my _ env < ^ > gegenwärtig aktiv ist. Das bedeutet, dass bei der Erstellung von Programmen nur die Einstellungen und Pakete dieser spezifischen Umgebung verwendet werden.
< $> note Anmerkung: Innerhalb der virtuellen Umgebung können Sie auf Wunsch den Befehl python anstelle von python3 und pip anstelle von pip3 verwenden.
Wenn Sie Python 3 auf Ihrem Rechner außerhalb einer Umgebung verwenden, können Sie ausschließlich die Befehle python3 und pip3 verwenden.
Nach den folgenden Schritten ist Ihre virtuelle Umgebung bereit zur Verwendung.
Schritt 3 - Erstellen eines "Hello, World" -Programms
Nachdem wir unsere virtuelle Umgebung eingerichtet haben, erstellen wir nun das traditionelle
Dadurch können wir unsere Umgebung testen und erhalten die Möglichkeit, uns mit Python besser vertraut zu machen, wenn wir es nicht bereits sind.
Dazu öffnen wir einen Befehlszeileneditor wie nano und erstellen eine neue Datei:
Sobald die Textdatei im Terminalfenster geöffnet ist, geben wir unser Programm ein:
Beenden Sie nano, indem Sie die Tasten STRG und X drücken. Wenn Sie zum Speichern der Datei aufgefordert werden, drücken Sie y.
Sobald Sie nano beendet haben und zu Ihrer Shell zurückgekehrt sind, führen Sie das Programm aus:
Das gerade erstellte Programm hello.py sollte dazu führen, dass im Terminal die folgende Ausgabe angezeigt wird:
Geben Sie den Befehl deactivate ein, um die Umgebung zu verlassen und in das Originalverzeichnis zurückkehren.
Herzlichen Glückwunsch!
Sie haben auf Ihrem Ubuntu Linux-Server eine Python-3-Programmierumgebung eingerichtet und können nun mit dem Codieren loslegen!
Wenn Sie einen lokalen Rechner anstelle eines Servers verwenden, konsultieren Sie das Tutorial, das für Ihr Betriebssystem relevant ist, in unserer Reihe Installieren und Einrichten einer lokalen Programmierumgebung für Python 3.
Da Ihr Server nun bereit für die Softwareentwicklung ist, können Sie mehr über das Codieren in Python erfahren, indem Sie unser kostenloses E-Book Codieren in Python 3 lesen oder unsere Python-Tutorials konsultieren.
Einrichtung von SSH-Schlüsseln unter Ubuntu 20.04
5247
Wenn Sie mit einem Ubuntu-Server arbeiten, verbringen Sie wahrscheinlich die meiste Zeit in einer Terminalsitzung, die über SSH mit Ihrem Server verbunden ist.
In dieser Anleitung geht es um die Einrichtung von SSH-Schlüsseln für eine Ubuntu 20.04 Installation.
SSH-Schlüssel bieten eine einfache und sichere Möglichkeit, sich bei Ihrem Server anzumelden. Sie werden allen Benutzern empfohlen.
Schritt 1 - Erstellen des Schlüsselpaars
Der erste Schritt besteht darin, ein Schlüsselpaar auf dem Client-Computer (normalerweise Ihrem Computer) zu erstellen:
Standardmäßig erstellen die neuesten Versionen von ssh-keygen ein 3072-Bit-RSA-Schlüsselpaar, das für die meisten Anwendungsfälle sicher genug ist. (Optional können Sie über Flag -b 4096 einen größeren 4096-Bit-Schlüssel erstellen lassen.)
Nach Eingabe des Befehls sollte die folgende Ausgabe angezeigt werden:
Drücken Sie die Eingabetaste, um das Schlüsselpaar im Unterverzeichnis .ssh / in Ihrem Stammverzeichnis zu speichern, oder geben Sie einen alternativen Pfad an.
Hier können Sie optional eine sichere Passphrase eingeben, die dringend empfohlen wird.
Mit einer Passphrase wird eine zusätzliche Sicherheitsebene hinzugefügt, um zu verhindern, dass sich nicht autorisierte Benutzer anmelden.
Weitere Informationen zur Sicherheit finden Sie in unserem Tutorial unter Konfiguration einer auf SSH-Schlüsseln basierten Authentifizierung auf einem Linux-Server.
Sie sollten dann eine Ausgabe ungefähr wie folgt sehen:
Der nächste Schritt besteht darin, den öffentlichen Schlüssel auf Ihrem Server abzulegen, damit Sie sich mithilfe der SSH-Schlüsselauthentifizierung anmelden können.
Schritt 2 - Kopieren des öffentlichen Schlüssels auf Ihren Ubuntu-Server
Die schnellste Möglichkeit, Ihren öffentlichen Schlüssel auf den Ubuntu-Host zu kopieren, ist die Verwendung eines Utilitys namens ssh-copy-id.
Aufgrund der Einfachheit wird diese Methode dringend empfohlen, falls sie verfügbar ist.
Wenn Sie auf Ihrem Client-Computer nicht über ssh-copy-id verfügen, können Sie eine der beiden in diesem Abschnitt beschriebenen alternativen Methoden verwenden (Kopieren über passwortbasiertes SSH oder manuelles Kopieren des Schlüssels).
Kopieren des öffentlichen Schlüssels mit ssh-copy-id
Um dieses Dienstprogramm zu verwenden, müssen Sie nur den Remote-Host, zu dem Sie eine Verbindung herstellen möchten, und das Benutzerkonto angeben, zu dem Sie einen passwortbasierten SSH-Zugang haben.
Auf dieses Konto wird Ihr öffentlicher SSH-Schlüssel kopiert.
Die Syntax lautet:
Geben Sie "yes" ein und drücken Sie ENTER, um fortzufahren.
Geben Sie das Passwort ein (Ihre Eingabe wird aus Sicherheitsgründen nicht angezeigt) und drücken Sie ENTER.
Anschließend wird der Inhalt Ihres Schlüssels ~ / .ssh / id _ rsa.pub in eine Datei im Stammverzeichnis ~ / .ssh des Remote-Kontos namens authorized _ keys kopiert.
Kopieren des öffentlichen Schlüssels mit SSH
Wenn Sie nicht über ssh-copy-id verfügen, aber einen passwortbasierten SSH-Zugriff auf ein Konto auf Ihrem Server haben, können Sie Ihre Schlüssel mit einer herkömmlichen SSH-Methode hochladen.
Wir verwenden das Umleitungssymbol > >, um den Inhalt anzuhängen, anstatt ihn zu überschreiben. Dadurch können wir Schlüssel hinzufügen, ohne zuvor hinzugefügte Schlüssel zu zerstören.
Manuelles Kopieren des öffentlichen Schlüssels
Greifen Sie mit der jeweils verfügbaren Methode auf Ihren Remote-Host zu.
Wir können jetzt eine passwortlose Authentifizierung mit unserem Ubuntu-Server versuchen.
Schritt 3 - Authentifizierung bei Ihrem Ubuntu-Server mit SSH-Schlüsseln
Wenn Sie eines der oben genannten Verfahren erfolgreich abgeschlossen haben, sollten Sie sich ohne das Passwort des Remote-Kontos beim Remote-Host anmelden können.
Der grundlegende Prozess ist der gleiche:
Geben Sie "yes" ein und drücken Sie dann ENTER, um fortzufahren.
Wenn Sie keine Passphrase für Ihren privaten Schlüssel angegeben haben, werden Sie sofort angemeldet.
Wenn Sie beim Erstellen des Schlüssels eine Passphrase für den privaten Schlüssel eingegeben haben, werden Sie aufgefordert, diese jetzt einzugeben (beachten Sie, dass Ihre Tastatureingaben aus Sicherheitsgründen nicht in der Terminalsitzung angezeigt werden).
Nach der Authentifizierung sollte eine neue Shell-Sitzung mit dem konfigurierten Konto auf dem Ubuntu-Server für Sie geöffnet werden.
Wenn die schlüsselbasierte Authentifizierung erfolgreich war, fahren Sie fort, um sich zu informieren, wie Sie Ihr System durch Deaktivieren der Passwortauthentifizierung weiter sichern können.
Schritt 4 - Deaktivieren der Passwortauthentifizierung auf Ihrem Server
Wenn Sie sich mit SSH ohne Passwort bei Ihrem Konto anmelden konnten, haben Sie die auf SSH-Schlüssel basierte Authentifizierung für Ihr Konto erfolgreich konfiguriert.
Diese Zeile kann mit einem # am Anfang der Zeile kommentiert werden.
Entfernen Sie das Kommentarzeichen # und setzen Sie den Wert auf no. Dies deaktiviert Ihre Möglichkeit, sich über SSH mit Kontopasswörtern anzumelden:
Speichern und schließen dann noch die Datei durch Drücken von STRG + X, anschließend von Y, um das Speichern der Datei zu bestätigen und abschließend durch Drücken von ENTER, um nano zu verlassen.
Um diese Änderungen zu aktivieren, müssen wir den sshd Dienst neu starten:
Nachdem Sie überprüft haben, dass der SSH Dienst korrekt funktioniert, können Sie alle aktuellen Serversitzungen sicher schließen.
Der SSH-Daemon auf Ihrem Ubuntu-Server reagiert jetzt nur noch auf eine Authentifizierung mit einem SSH-Schlüssel.
Eine Anmeldung per Passwort ist deaktiviert.
So installieren und konfigurieren Sie Elasticsearch unter Ubuntu 20.04
5416
Elasticsearch ist eine Plattform für verteilte Suche und Analyse von Daten in Echtzeit.
Aufgrund hoher Benutzerfreundlichkeit, leistungsfähiger Funktionen und Skalierbarkeit ist sie eine beliebte Wahl.
In diesem Artikel installieren Sie Elasticsearch, konfigurieren es für Ihren Anwendungsfall, sichern Ihre Installation und unternehmen die ersten Schritte mit Ihrem Elasticsearch-Server.
Bevor Sie dieses Tutorial absolvieren, benötigen Sie Folgendes:
Einen Ubuntu 20.04-Server mit 4 GB RAM und 2 CPUs, eingerichtet mit einem non-root sudo user.
Sie können hierzu auch der Ersteinrichtung eines Servers unter Ubuntu 20.04 folgen
Installiertes OpenJDK 11
In diesem Tutorial arbeiten wir mit der Mindestmenge von CPUs und RAM, die zur Ausführung von Elasticsearch benötigt werden.
Beachten Sie, dass die Menge von CPUs, RAM und Speicher, die Ihr Elasticsearch-Server benötigt, von der Menge der Protokolle abhängt, die Sie erwarten.
Schritt 1 - Installieren und Konfigurieren von Elasticsearch
Die Elasticsearch-Komponenten sind in Standard-Paket-Repositorys von Ubuntu nicht verfügbar.
Sie können jedoch mit APT installiert werden, nachdem Sie die Paketquellliste von Elastic hinzugefügt haben.
Alle Pakete werden mit dem Signierschlüssel von Elasticsearch signiert, um das System vor Paket-Spoofing zu schützen.
Pakete, die mit dem Schlüssel authentifiziert wurden, werden von Ihrem Paketmanager als vertrauenswürdig eingestuft.
In diesem Schritt importieren Sie den öffentlichen GPG-Schlüssel von Elasticsearch und fügen die Paketquellliste von Elasticsearch hinzu, um Elasticsearch zu installieren.
Verwenden Sie zunächst cURL, das Befehlszeilentool zur Übertragung von Daten mit URLs, um den öffentlichen GPG-Schlüssel von Elasticsearch in APT zu importieren.
Beachten Sie, dass wir die Argumente -fsSL nutzen, um alle Fortschritte und möglichen Fehler stumm zu schalten (ausgenommen Serverfehler) und um zuzulassen, dass cURL bei einer Umleitung eine Anfrage an einem neuen Ort stellt.
Leiten Sie die Ausgabe des cURL-Befehls in das APT-Schlüsselprogramm weiter, das den öffentlichen GPG-Schlüssel zu APT hinzufügt.
Fügen Sie als Nächstes die Elastic-Quellliste in das Verzeichnis sources.list.d ein, in dem APT nach neuen Quellen sucht:
Aktualisieren Sie als Nächstes Ihre Paketlisten, damit APT die neue Elastic-Quelle liest:
Installieren Sie dann Elasticsearch mit diesem Befehl:
Elasticsearch ist nun installiert und bereit für die Konfiguration.
Schritt 2 - Konfigurieren von Elasticsearch
Um Elasticsearch zu konfigurieren, bearbeiten wir die Hauptkonfigurationsdatei elasticsearch.yml, in der die meisten Konfigurationsoptionen gespeichert sind.
Diese Datei befindet sich im Verzeichnis / etc / elasticsearch.
Verwenden Sie zur Bearbeitung der Konfigurationsdatei von Elasticsearch Ihren bevorzugten Texteditor.
< $> note Anmerkung: Die Konfigurationsdatei von Elasticsearch liegt im YAML-Format vor. Das bedeutet, dass wir das Einrückungsformat beibehalten müssen.
Achten Sie darauf, dass Sie beim Bearbeiten der Datei keine zusätzlichen Leerzeichen hinzufügen.
Die Datei elasticsearch.yml bietet Konfigurationsoptionen für Cluster, Knoten, Pfade, Arbeitsspeicher, Netzwerk, Suche und Gateway.
Die meisten dieser Optionen sind in der Datei vorkonfiguriert, aber Sie können sie je nach Ihren Bedürfnissen ändern.
Im Sinne unserer Demonstration einer Konfiguration mit nur einem Server werden wir nur die Einstellungen für den Netzwerkhost anpassen.
Elasticsearch lauscht an Port 9200 auf Verkehr von überall.
Sie werden externen Zugriff auf Ihre Elasticsearch-Instanz einschränken wollen, um zu verhindern, dass externe Personen Ihre Daten lesen oder Ihren Elasticsearch-Cluster mit der REST-API (https: / / de.wikipedia.org / wiki / Representational _ State _ Transfer) herunterfahren.
Um Zugriff zu beschränken und damit die Sicherheit zu erhöhen, suchen Sie nach der Zeile, die network.host angibt, heben Sie die Kommentierung auf und ersetzen den Wert durch localhost, sodass die Zeile wie folgt aussieht:
Wir haben localhost angegeben, damit Elasticsearch an allen Schnittstellen und gebundenen IPs lauscht.
Wenn Sie möchten, dass nur an einer bestimmten Schnittstelle gelauscht werden soll, können Sie deren IP-Adresse an Stelle von localhost angeben.
Speichern und schließen Sie elasticsearch.yml.
Wenn Sie nano verwenden, können Sie dazu STRG + X drücken, gefolgt von Y und dann ENTER.
Das sind die Mindesteinstellungen, mit denen Sie beginnen können, um Elasticsearch zu verwenden.
Sie können Elasticsearch jetzt zum ersten Mal starten.
Starten Sie den Elasticsearch-Dienst mit systemctl.
Geben Sie Elasticsearch einige Momente zum Starten.
Andernfalls erhalten Sie möglicherweise Fehlermeldungen, dass Sie keine Verbindung herstellen können.
Führen Sie als Nächstes den folgenden Befehl aus, damit Elasticsearch bei jedem Server-Boot gestartet wird:
Nachdem Elasticsearch beim Starten nun aktiviert ist, fahren wir mit dem nächsten Schritt fort und besprechen das Thema Sicherheit.
Schritt 3 - Sichern von Elasticsearch
Standardmäßig kann Elasticsearch von jedem kontrolliert werden, der auf die HTTP-API zugreifen kann.
Das ist nicht immer ein Sicherheitsproblem, da Elasticsearch nur an der Loopback-Schnittstelle lauscht (d. h. 127.0.0.1), auf die nur lokal zugegriffen werden kann.
So ist kein öffentlicher Zugriff möglich und solange alle Serverbenutzer vertrauenswürdig sind, ist Sicherheit möglicherweise kein großes Problem.
Wenn Sie Remotezugriff auf die HTTP-API zulassen müssen, können Sie das Gefahrenpotenzial im Netzwerk mit der Standard-Firewall von Ubuntu (UFW) reduzieren.
Diese Firewall sollte bereits aktiviert sein, wenn Sie die Schritte im Tutorial Ersteinrichtung eines Servers unter Ubuntu 20.04 ausgeführt haben.
Wir konfigurieren nun die Firewall, um Zugriff auf den standardmäßigen HTTP-API-Port (TCP 9200) für den vertrauenswürdigen Remote-Host zuzulassen. Das ist in der Regel der Server, den Sie in einer Einrichtung mit einem einzelnen Server verwenden, wie zum Beispiel < ^ > 198.51.100.0 < ^ >.
Geben Sie, um Zugriff zuzulassen, den folgenden Befehl ein:
Danach können Sie UFW mit folgendem Befehl aktivieren:
Überprüfen Sie abschließend mit folgendem Befehl den Status von UFW:
Wenn Sie die Regeln richtig angegeben haben, sollten Sie eine Ausgabe wie diese erhalten:
Die UFW sollte jetzt aktiviert und zum Schutz von Elasticsearch Port 9200 eingerichtet ein.
Wenn Sie in zusätzlichen Schutz investieren möchten, bietet Elasticsearch das kommerzielle Shield-Plugin zum Kauf an.
Schritt 4 - Testen von Elasticsearch
Jetzt sollte Elasticsearch an Port 9200 ausgeführt werden.
Sie können das mit cURL und einer GET-Anfrage testen.
Sie sollten die folgende Antwort erhalten:
Wenn Sie eine Antwort sehen, die der obigen ähnelt, funktioniert Elasticsearch richtig.
Wenn nicht, stellen Sie sicher, dass Sie die Installationsanweisungen richtig befolgt und Elasticsearch Zeit zum vollständigen Starten gegeben haben.
Um eine gründlichere Prüfung von Elasticsearch vorzunehmen, führen Sie den folgenden Befehl aus:
In der Ausgabe des obigen Befehls können Sie alle aktuellen Einstellungen für Knoten, Cluster, Anwendungspfade, Module und mehr überprüfen.
Schritt 5 - Verwenden von Elasticsearch
Um mit der Verwendung von Elasticsearch zu beginnen, fügen wir zuerst Daten hinzu. Elasticsearch nutzt eine RESTful-API, die auf die üblichen CRUD-Befehle reagiert: create, read, update und delete.
Dazu verwenden wir erneut den Befehl cURL.
Sie können Ihren ersten Eintrag wie folgt hinzufügen:
Mit cURL haben wir eine HTTP-POST-Anfrage an den Elasticsearch-Server gesendet.
Der URI der Anfrage war / tutorial / helloworld / 1 mit mehreren Parametern:
tutorial ist der Index der Daten in Elasticsearch.
helloworld ist der Typ.
1 ist die ID unseres Eintrags unter dem obigen Index und Typ.
Sie können diesen ersten Eintrag mit einer HTTP-GET-Anfrage abrufen.
Die Ausgabe sollte wie folgt aussehen:
Um einen bestehenden Eintrag zu ändern, können Sie eine HTTP-PUT-Anfrage verwenden.
Elasticsearch sollte die erfolgreiche Änderung wie folgt bestätigen:
Im obigen Beispiel haben wir die message des ersten Eintrags in "Hello, People!" geändert.
Damit wurde die Versionsnummer automatisch auf 2 erhöht.
Vielleicht haben Sie in der obigen Anfrage das zusätzliche Argument pretty bemerkt.
Das ermöglicht ein visuell lesbares Format, sodass Sie jedes Datenfeld in eine neue Zeile schreiben können.
Sie können Ihre Ergebnisse beim Abruf von Daten auch "verschönern", um eine besser lesbare Ausgabe zu erhalten, indem Sie den folgenden Befehl eingeben:
Nun wird die Antwort so formatiert, dass sie visuell analysiert werden kann:
Wir haben jetzt Daten in Elasticsearch hinzugefügt und abgefragt.
Um mehr über die anderen Operationen zu erfahren, konsultieren Sie bitte die API-Dokumentation.
Sie haben Elasticsearch jetzt installiert, konfiguriert und erstmals verwendet.
Um die Funktionalität von Elasticsearch weiter zu erkunden, lesen Sie bitte die offizielle Elasticsearch-Dokumentation.
So installieren Sie den Apache Webserver unter CentOS 8
5243
Der Apache-HTTP-Server ist der am häufigsten verwendete Webserver der Welt.
Er bietet viele leistungsstarke Funktionen, darunter dynamisch ladbare Module, robuste Medienunterstützung und eine umfassende Integration mit anderer gängiger Software.
In diesem Leitfaden installieren Sie einen Apache-Webserver mit virtuellen Hosts auf Ihrem CentOS 8-Server.
Zur Absolvierung dieses Leitfadens benötigen Sie Folgendes:
Einen auf Ihrem Server konfigurierten non-root user mit sudo-Berechtigungen. Sie können hierzu dem Leitfaden Ersteinrichtung des Servers unter CentOS 8 folgen.
Stellen Sie sicher, dass eine grundlegende Firewall konfiguriert ist, indem Sie Schritt 4 der Ersteinrichtung des Servers unter CentOS 8 im obigen Leitfaden folgen (empfohlen).
Schritt 1 - Installieren von Apache
Apache ist in den Standard-Software-Repositorys von CentOS verfügbar und kann mit dem dnf-Paketmanager installiert werden.
Installieren Sie das Apache-Paket als der non-root sudo-user, der in den Voraussetzungen konfiguriert wurde:
Nach der Bestätigung der Installation installiert dnf Apache und alle erforderlichen Abhängigkeiten.
Wenn Sie den im Abschnitt Voraussetzungen erwähnten Schritt 4 des Leitfadens Ersteinrichtung des Servers unter CentOS 8 gefolgt sind, haben Sie bereits firewalld auf Ihrem Server installiert, um Anforderungen über HTTP zu bedienen.
Wenn Sie Apache so konfigurieren möchten, dass er auch Inhalte über HTTPS bedient, sollten Sie zudem Port 443 öffnen, indem Sie den https-Dienst aktivieren:
Laden Sie als Nächstes die Firewall neu, um diese neuen Regeln anzuwenden:
Nach dem Neuladen der Firewall können Sie den Dienst starten und den Webserver überprüfen.
Schritt 2 - Prüfen Ihres Webservers
Apache startet beim Abschluss der Installation nicht automatisch auf CentOS, sodass Sie den Apache-Prozess manuell starten müssen:
Überprüfen Sie mit dem folgenden Befehl, ob der Dienst ausgeführt wird:
Wenn der Dienst ausgeführt wird, erhalten Sie einen active-Status:
Die Ausgabe zeigt an, dass der Dienst erfolgreich gestartet wurde.
Der beste Weg, dies zu testen, besteht jedoch darin, eine Seite von Apache anzufordern.
Sie können auf die Standardstartseite von Apache zugreifen, um zu bestätigen, dass die Software ordnungsgemäß über Ihre IP-Adresse ausgeführt wird.
Wenn Sie die IP-Adresse Ihres Servers nicht kennen, können Sie sie von der Befehlszeile auf verschiedene Arten abrufen.
Geben Sie q ein, um zur Eingabeaufforderung zurückzukehren, und geben Sie dann Folgendes ein:
Dieser Befehl zeigt alle Netzwerkadressen des Hosts an, wodurch Sie einige IP-Adressen erhalten, die durch Leerzeichen getrennt sind.
Sie können jede in Ihrem Webbrowser ausprobieren, um zu sehen, ob sie funktionieren.
Alternativ können Sie curl verwenden, um Ihre IP von icanhazip.com anzufordern. Dadurch erhalten Sie Ihre öffentliche IPv4-Adresse, wie sie von einem anderen Ort im Internet gelesen wird:
Wenn Sie über die IP-Adresse Ihres Servers verfügen, geben Sie sie in die Adressleiste Ihres Browsers ein:
Sie sehen dann die Standardwebsite von CentOS 8 Apache:
Standardseite Apache für CentOS 8
Diese Seite zeigt an, dass Apache richtig funktioniert.
Sie enthält auch einige grundlegende Informationen zu wichtigen Apache-Dateien und Verzeichnispositionen.
Schritt 3 - Verwalten des Apache-Prozesses
Nachdem der Dienst nun installiert ist und ausgeführt wird, können Sie jetzt verschiedene systemctl-Befehle verwenden, um den Dienst zu verwalten.
Um Ihren Webserver anzuhalten, geben Sie Folgendes ein:
Um den Webserver zu starten, wenn er angehalten wurde, geben Sie Folgendes ein:
Um den Dienst anzuhalten und erneut zu starten, geben Sie Folgendes ein:
Wenn Sie nur Konfigurationsänderungen vornehmen, kann Apache oft neu geladen werden, ohne Verbindungen zu trennen.
Dazu verwenden Sie diesen Befehl:
Standardmäßig ist Apache so konfiguriert, dass er beim Booten des Servers automatisch startet.
Wenn Sie dieses Verhalten nicht wünschen, können Sie es durch folgende Eingabe deaktivieren:
Geben Sie Folgendes ein, um den Dienst beim Booten wieder zu aktivieren:
Apache wird jetzt automatisch gestartet, wenn der Server erneut gebootet wird.
Die Standardkonfiguration für Apache ermöglicht Ihrem Server, eine einzelne Website zu hosten.
Wenn Sie mehrere Domänen auf Ihrem Server hosten möchten, müssen Sie virtuelle Hosts auf Ihrem Apache-Webserver konfigurieren.
Schritt 4 - Einrichten eines virtuellen Hosts (Empfohlen)
Bei Verwendung des Apache-Webservers können Sie virtuelle Hosts verwenden (diese sind ähnlich wie Serverblocks in Nginx), um Konfigurationsdetails einzuschließen und mehr als eine Domäne auf einem einzelnen Server zu hosten.
In diesem Schritt richten wir eine Domäne namens example.com ein, aber Sie sollten diesen Namen durch Ihren eigenen Domänennamen ersetzen.
Wenn Sie mit DigitalOcean einen Domainnamen einrichten, lesen Sie bitte unsere Netzwerkdokumentation.
Apache unter CentOS 8 hat einen virtuellen Host, der standardmäßig aktiviert und so konfiguriert ist, dass er Dokumente aus dem / var / www / html-Verzeichnis bereitstellt.
Statt / var / www / html zu ändern, erstellen Sie eine Verzeichnisstruktur innerhalb von / var / www für die Site example.com und lassen dabei / var / www / html als Standardverzeichnis stehen, das bereitgestellt wird, wenn eine Client-Anforderung keine übereinstimmenden Sites hat.
Erstellen Sie das html-Verzeichnis für example.com wie folgt und verwenden Sie das -p-Flag, um alle notwendigen übergeordneten Verzeichnisse zu erstellen:
Erstellen Sie ein zusätzliches Verzeichnis, um Protokolldateien für die Site zu speichern:
Weisen Sie als Nächstes das Eigentum am html-Verzeichnis mit der Umgebungsvariablen $USER zu:
Stellen Sie sicher, dass Ihre Webroot über die Standardberechtigungen verfügt:
Erstellen Sie als Nächstes eine index.html-Seite durch die Verwendung von vi oder Ihrem bevorzugten Texteditor:
Drücken Sie i, um zum INSERT-Modus zu wechseln, und fügen Sie der Datei folgende Beispiel-HTML hinzu:
Speichern und schließen Sie die Datei, indem Sie ESC drücken,: wq eingeben und ENTER drücken.
Mit dem fertig erstellten Siteverzeichnis und der Beispiel-Indexdatei sind Sie nun fast bereit, die virtuellen Host-Dateien zu erstellen.
Virtuelle Host-Dateien legen die Konfiguration Ihrer einzelnen Sites fest und weisen den Apache-Webserver an, wie er auf verschiedene Domänenanforderungen reagieren soll.
Vor der Erstellung Ihrer virtuellen Hosts müssen Sie ein sites-available-Verzeichnis erstellen, in der diese gespeichert werden.
Sie erstellen auch ein sites-enabled-Verzeichnis, welches Apache anweist, dass ein virtueller Host verfügbar ist, um Besucher zu bedienen.
Das sites-enabled-Verzeichnis enthält symbolische Links zu virtuellen Hosts, die wir veröffentlichen möchten.
Erstellen Sie beide Verzeichnisse mit dem folgenden Befehl:
Als Nächstes weisen Sie Apache an, im sites-enabled-Verzeichnis nach virtuellen Hosts zu suchen.
Bearbeiten Sie dazu die Hauptkonfigurationsdatei von Apache mit vi oder Ihrem bevorzugten Texteditor und fügen Sie eine Zeile hinzu, die ein optionales Verzeichnis für zusätzliche Konfigurationsdateien deklariert:
Drücken Sie den Großbuchstaben G, um zum Ende der Datei zu navigieren.
Drücken Sie dann i, um zum INSERT-Modus zu wechseln, und fügen Sie die folgende Zeile am Ende der Datei hinzu:
Nach dem Hinzufügen der Zeile speichern und schließen Sie die Datei.
Nachdem Sie nun die virtuellen Host-Verzeichnisse eingerichtet haben, erstellen Sie die virtuelle Host-Datei.
Zuerst erstellen Sie eine neue Datei im sites-available-Verzeichnis:
Fügen Sie folgenden Konfigurationsblock hinzu und ändern Sie example.com auf Ihren Domänennamen:
Hierdurch wird Apache mitgeteilt, wo das Rootverzeichnis mit den öffentlich zugänglichen Webdokumenten zu finden ist.
Außerdem wird Apache angewiesen, wo die Fehler- und Anforderungsprotokolle für die jeweilige Site zu speichern sind.
Die virtuellen Host-Dateien sind nun erstellt und können aktiviert werden, damit Apache sie für Besucher bereitstellen kann.
Erstellen Sie hierzu einen symbolischen Link für jeden virtuellen Host im sites-enabled-Verzeichnis:
Ihr virtueller Host ist nun konfiguriert und bereit, Inhalte bereitzustellen.
Vor dem Neustart des Apache-Dienstes prüfen wir, ob SELinux über die richtigen Richtlinien für Ihre virtuellen Hosts verfügt.
Schritt 5 - Anpassen von SELinux für virtuelle Hosts (empfohlen)
SELinux ist ein Sicherheitsmodul des Linux-Kernels, das eine höhere Sicherheit für Linux-Systeme bietet.
CentOS 8 ist mit SELinux ausgestattet und konfiguriert, um mit der Apache-Konfiguration zu arbeiten.
Da Sie durch Einrichten eines benutzerdefinierten Protokollverzeichnisses in der Konfigurationsdatei der virtuellen Hosts die Standardkonfiguration geändert haben, erhalten Sie beim Versuch, den Apache-Dienst zu starten, eine Fehlermeldung.
Zur Lösung des Problems müssen Sie die Richtlinien von SELinux aktualisieren, damit Apache in die erforderlichen Dateien schreiben kann.
Es gibt verschiedene Möglichkeiten, Richtlinien entsprechend den Anforderungen Ihrer Umgebung festzulegen, da SELinux Ihnen die Möglichkeit bietet, Ihre Sicherheitsstufe anzupassen.
Dieser Schritt behandelt zwei Methoden zur Anpassung der Richtlinien von Apache: universell und in einem bestimmten Verzeichnis.
Da es sicherer ist, Richtlinien von Verzeichnissen anzupassen, wird dieser Ansatz empfohlen.
Universelles Anpassen der Apache-Richtlinien
Wenn die Apache-Richtlinien universell festgelegt werden, wird SELinux angewiesen, alle Apache-Prozesse mit dem booleschen Argument httpd _ unified identisch zu behandeln.
Dieser Ansatz ist zwar bequemer, gibt Ihnen aber nicht das gleiche Maß an Kontrolle wie ein Ansatz, der sich auf eine Datei- oder Verzeichnisrichtlinie konzentriert.
Führen Sie den folgenden Befehl aus, um universelle Apache-Richtlinien festzulegen:
Der Befehl setsebool ändert boolesche SELinux-Werte.
Das -P-Flag aktualisiert den Bootzeitwert, sodass diese Änderung auch bei Neustarts erhalten bleibt. httpd _ unified ist das boolesche Argument, das SELinux anweist, alle Apache-Prozesse als den gleichen Typ zu behandeln. Sie haben es mit dem Wert von 1 aktiviert.
Anpassen von Apache-Richtlinien in einem Verzeichnis
Das individuelle Einrichten von SELinux-Berechtigungen für das Verzeichnis / var / www / < ^ > example.com < ^ > / log gibt Ihnen mehr Kontrolle über die Apache-Richtlinien, kann aber wartungsintensiver sein.
Da es sich bei dieser Option nicht um eine universelle Festlegung von Richtlinien handelt, müssen Sie den Kontexttyp für alle neuen Protokollverzeichnisse, die in Ihren virtuellen Host-Konfigurationen angegeben sind, manuell festlegen.
Überprüfen Sie zunächst den Kontexttyp, den SELinux dem Verzeichnis / var / www / example.com / log gegeben hat:
Dieser Befehl listet den SELinux-Kontext des Verzeichnisses auf und gibt ihn aus.
Der aktuelle Kontext ist httpd _ sys _ content _ t, der SELinux mitteilt, dass der Apache-Prozess nur Dateien lesen kann, die in diesem Verzeichnis erstellt wurden.
In diesem Leitfaden ändern Sie den Kontexttyp des Verzeichnisses / var / www / < ^ > example.com < ^ > / log in httpd _ log _ t.
Dieser Typ ermöglicht Apache, Protokolldateien von Web-Anwendungen zu generieren und an diese anzuhängen:
Verwenden Sie als Nächstes den Befehl restorecon, um diese Änderungen anzuwenden und sie bei Neustarts beizubehalten:
Das Flag -R führt diesen Befehl rekursiv aus. Das bedeutet, dass alle vorhandenen Dateien aktualisiert werden, um den neuen Kontext zu verwenden.
Das Flag -v gibt die Kontextänderungen aus, die der Befehl vorgenommen hat.
Sie erhalten die folgende Ausgabe, die die Änderungen bestätigt:
Sie können die Kontexte erneut auflisten, um die Änderungen zu sehen:
Die Ausgabe zeigt den aktualisierten Kontexttyp an:
Das Verzeichnis / var / www / < ^ > example.com < ^ > / log verwendet nun den Typ httpd _ log _ t ​ ​ ​ ​ ​ ​ und Sie können jetzt die Konfiguration des virtuelles Hosts testen.
Schritt 6 - Testen des virtuellen Hosts (empfohlen)
Sobald der SELinux-Kontext mit einer der beiden Methoden aktualisiert wurde, kann Apache in das Verzeichnis / var / www / example.com / log schreiben.
Sie können jetzt den Apache-Dienst erfolgreich neu starten:
Listen Sie den Inhalt des Verzeichnisses / var / www / < ^ > example.com < ^ > / log auf, um zu sehen, ob Apache die Protokolldateien erstellt hat:
Sie erhalten eine Bestätigung, dass Apache die in der Konfiguration des virtuellen Hosts angegebenen Dateien error.log und requests.log erstellen konnte:
Nachdem Sie nun Ihren virtuellen Host eingerichtet und die Berechtigungen von SELinux aktualisiert haben, stellt Apache Ihren Domänennamen bereit.
Sie können dies testen, indem Sie zu http: / / < ^ > example.com < ^ > navigieren, wo Sie etwas Ähnliches sehen sollten wie:
Erfolg!
Der virtuelle Host example.com funktioniert!
Das bestätigt, dass Ihr virtueller Host erfolgreich konfiguriert ist und Inhalte bereitstellt.
Wiederholen Sie die Schritte 4 und 5, um neue virtuelle Hosts mit SELinux-Berechtigungen für zusätzliche Domänen zu erstellen.
In diesem Leitfaden haben Sie den Apache-Webserver installiert und verwaltet.
Nachdem Sie Ihren Webserver nun installiert haben, haben Sie viele Möglichkeiten für die Art des Inhalts und die Technologien, die Sie verwenden möchten, um eine umfassendere Benutzererfahrung zu erzielen.
Wenn Sie einen vollständigeren Anwendungsstapel erstellen möchten, lesen Sie den Artikel Konfigurieren eines LAMP-Stacks unter CentOS 8.
So installieren Sie den Apache-Webserver unter Ubuntu 20.04 Schnellstart
5388
Er bietet viele leistungsfähige Funktionen, einschließlich dynamisch belastbarer Module, robuster Medienunterstützung und umfassende Integration mit anderen beliebten Programmen.
In diesem Leitfaden erklären wir, wie Sie einen Apache-Webserver auf Ihrem Ubuntu 20.04-Server installieren können.
Eine ausführlichere Version dieses Tutorials finden Sie in Installieren des Apache Webservers unter Ubuntu 20.04.
Bevor Sie mit diesem Leitfaden beginnen, benötigen Sie Folgendes:
Einen Ubuntu 20.04-Server und einen regulären non-root user mit Sudo-Berechtigungen.
Außerdem müssen Sie eine grundlegende Firewall aktivieren, um nicht wesentliche Ports zu blockieren.
Sie können lernen, wie Sie ein reguläres Benutzerkonto konfigurieren und eine Firewall für Ihren Server einrichten, indem Sie unserem Leitfaden zur Ersteinrichtung des Servers für Ubuntu 20.04 folgen.
Wenn Sie über ein Konto verfügen, melden Sie sich zunächst als Nicht-root-Benutzer an.
Apache ist in den Standard-Software-Repositorys von Ubuntu verfügbar, sodass Sie es mit herkömmlichen Paketverwaltungstools installieren können.
Aktualisieren Sie Ihren lokalen Paketindex:
Installieren Sie das Paket apache2:
Schritt 2 - Anpassen der Firewall
Überprüfen Sie die verfügbaren ufw-Anwendungsprofile:
Aktivieren Sie das restriktivste Profil, das den von Ihnen konfigurierten Datenverkehr weiterhin und den Datenverkehr auf Port 80 zulässt (normaler, unverschlüsselter Webdatenverkehr):
Überprüfen Sie die Änderung:
Schritt 3 - Testen Ihres Webservers
Überprüfen Sie mit dem systemd init-System, um sicherzustellen, dass der Dienst ausgeführt wird, indem Sie Folgendes eingeben:
Öffnen Sie die Standardstartseite von Nginx, um zu bestätigen, dass die Software über Ihre IP-Adresse ordnungsgemäß ausgeführt wird.
Sie sollten die Standard-Webseite von Apache für Ubuntu 20.04 erhalten:
Apache Standardseite
Wenn Sie den Apache-Webserver verwenden, können Sie virtuelle Hosts (ähnlich wie Server Blocks in Nginx) verwenden, um Konfigurationsinformationen mit einzuschließen und mehr als eine Domäne auf einem einzigen Server zu hosten.
Wir richten eine Domäne namens your _ domain ein, aber Sie sollten diesen Namen durch Ihren eigenen Domänennamen ersetzen.
Um mehr über das Einrichten eines Domänennamens mit DigitalOcean zu erfahren, lesen Sie bitte unsere Einführung zu DigitalOcean DNS.
Erstellen Sie das Verzeichnis für < ^ > your _ domain < ^ >:
Erteilen Sie den Besitz des Verzeichnisses:
Die Berechtigungen Ihrer Web-Roots sollten korrekt sein, wenn Sie Ihren unmask-Wert nicht geändert haben, aber Sie können das durch die folgende Eingabe prüfen:
Erstellen Sie als Nächstes eine index.html Beispielsseite durch die Verwendung von nano oder Ihrem bevorzugten Texteditor:
Fügen Sie dann das folgende HTML-Beispiel hinzu:
Erstellen Sie eine neue virtuelle Host-Datei in / etc / apache2 / sites-available / < ^ > your _ domain < ^ > .conf
Fügen Sie den folgenden Konfigurationsblock ein, der für unser neues Verzeichnis und den Domänennamen aktualisiert wird:
Aktivieren Sie die Datei mit a2ensite:
Deaktivieren Sie die unter 000-default.conf definierte Standard-Site:
Testen Sie die Konfigurationsfehler:
Sie sollten die folgende Ausgabe sehen:
Starten Sie Apache neu, um Ihre Änderungen zu implementieren.
Apache sollte jetzt für Ihren Domänennamen eingerichtet sein.
Sie können dies testen, indem Sie zu http: / / < ^ > your _ domain < ^ > navigieren, wo Sie etwas Ähnliches wie Folgendes sehen sollten:
Beispiel für einen virtuellen Apache-Host
Nachdem Sie Ihren Webserver installiert haben, haben Sie viele Optionen für die Art des Inhalts und die Technologien, die Sie verwenden möchten, um eine umfassendere Benutzererfahrung zu erzielen.
Wenn Sie einen vollständigeren Anwendungsstapel erstellen möchten, lesen Sie diesen Artikel unter So konfiguriert man einen LAMP-Stapel unter Ubuntu 20.04
Verwenden von Puffern in Node.js.
5391
Ein Puffer ist ein Speicherplatz (normalerweise RAM), in dem Binärdaten gespeichert werden. In Node.js können wir mit der integrierten Puffer-Klasse auf diese Speicherbereiche zugreifen.
Puffer speichern eine Folge von Ganzzahlen, ähnlich einer Anordnung in JavaScript.
Im Gegensatz zu Arrays können Sie die Größe eines Puffers nach seiner Erstellung nicht mehr ändern.
Möglicherweise haben Sie implizit Puffer verwendet, wenn Sie bereits den Node.js-Code geschrieben haben.
Wenn Sie beispielsweise mit fs.readFile () aus einer Datei lesen, sind die an den Rückruf oder das Versprechen zurückgegebenen Daten ein Pufferobjekt.
Wenn HTTP-Anforderungen in Node.js gestellt werden, geben sie außerdem Datenströme zurück, die vorübergehend in einem internen Puffer gespeichert sind, wenn der Client den Stream nicht auf einmal verarbeiten kann.
Puffer sind nützlich, wenn Sie mit Binärdaten interagieren, normalerweise auf niedrigeren Netzwerkebenen.
Sie bieten Ihnen auch die Möglichkeit, feinkörnige Datenmanipulationen in Node.js durchzuführen.
In diesem Tutorial verwenden Sie Node.js REPL, um verschiedene Beispiele für Puffer durchzugehen, z. B. das Erstellen von Puffern, das Lesen aus Puffern, das Schreiben in und das Kopieren aus Puffern sowie die Verwendung von Puffern zum Konvertieren zwischen binären und codierten Daten. Am Ende des Tutorials haben Sie gelernt, wie Sie mit der Puffer-Klasse mit Binärdaten arbeiten.
Sie müssen Node.js auf Ihrem Entwicklungscomputer installiert haben.
Dieses Tutorial verwendet die Version 10.19.0.
In diesem Tutorial interagieren Sie mit Puffern in der Node.js REPL (Read-Evaluate-Print-Loop).
Wenn Sie eine Auffrischung zur effektiven Verwendung von Node.js REPL wünschen, lesen Sie unseren Leitfaden zur Verwendung von Node.js REPL.
Für diesen Artikel erwarten wir, dass der Benutzer mit grundlegendem JavaScript und seinen Datentypen vertraut ist.
Sie können diese Grundlagen mit unserer Codieren in JavaScript-Reihe lernen.
Schritt 1 - Erstellen eines Puffers
Dieser erste Schritt zeigt Ihnen die beiden wichtigsten Möglichkeiten zum Erstellen eines Pufferobjekts in Node.js.
Um zu entscheiden, welche Methode verwendet werden soll, müssen Sie die folgende Frage beantworten: Möchten Sie einen neuen Puffer erstellen oder einen Puffer aus vorhandenen Daten extrahieren?
Wenn Sie Daten im Speicher speichern möchten, die Sie noch nicht empfangen haben, sollten Sie einen neuen Puffer erstellen.
In Node.js verwenden wir dazu die Funktion alloc () der Puffer-Klasse.
Öffnen wir die Node.js REPL, um uns selbst davon zu überzeugen.
Geben Sie in Ihrem Terminal den Befehl node ein:
Sie sehen, dass die Eingabeaufforderung mit > beginnt.
Die Funktion alloc () verwendet die Größe des Puffers als erstes und einziges erforderliches Argument.
Die Größe ist eine Ganzzahl, die angibt, wie viele Speicherbytes das Pufferobjekt verwendet.
Wenn wir beispielsweise einen Puffer mit einer Größe von 1 KB (Kilobyte) erstellen möchten, der 1024 Byte entspricht, geben Sie dies in die Konsole ein:
Um einen neuen Puffer zu erstellen, haben wir die global verfügbare Puffer-Klasse verwendet, die über die Methode alloc () verfügt.
Durch die Angabe von 1024 als Argument für alloc () haben wir einen Puffer mit einer Größe von 1 KB erstellt.
Wenn Sie einen Puffer mit alloc () initialisieren, wird der Puffer standardmäßig mit binären Nullen als Platzhalter für festgelegte Datenmengen gefüllt. Wir können dennoch den Standardwert ändern, wenn wir möchten.
Wenn wir einen neuen Puffer mit Einsen anstelle von Nullen erstellen möchten, setzen wir den zweiten Parameter der Funktion alloc () - fill.
Erstellen Sie in Ihrem Terminal an der REPL-Eingabeaufforderung einen neuen Puffer, der mit Einsen gefüllt ist:
Wir haben gerade ein neues Pufferobjekt erstellt, das auf einen Speicherplatz im Speicher verweist, in dem 1 KB Einsen gespeichert sind.
Obwohl wir eine Ganzzahl eingegeben haben, sind alle in einem Puffer gespeicherten Daten Binärdaten.
Binärdaten können in vielen verschiedenen Formaten vorliegen.
Betrachten wir beispielsweise eine Binärsequenz, die ein Datenbyte darstellt: 01110110.
Wenn diese Binärsequenz eine Zeichenfolge in Englisch unter Verwendung des ASCII-Codierungsstandards darstellen würde, wäre dies der Buchstabe v. Wenn unser Computer jedoch ein Bild verarbeitet, könnte diese Binärsequenz Informationen über die Farbe eines Pixels enthalten.
Der Computer kann sie unterschiedlich verarbeiten, da die Bytes unterschiedlich codiert sind.
Die Bytecodierung ist das Format des Bytes.
Ein Puffer in Node.js verwendet standardmäßig das UTF-8-Codierungsschema, wenn er mit Zeichenfolgendaten initialisiert wird. Ein Byte in UTF-8 repräsentiert eine Zahl, einen Buchstaben (in Englisch und in anderen Sprachen) oder ein Symbol.
UTF-8 ist eine Obermenge von ASCII, dem amerikanischen Standardcode für den Informationsaustausch.
ASCII kann Bytes mit englischen Groß- und Kleinbuchstaben, den Zahlen 0-9 und einigen anderen Symbolen wie dem Ausrufezeichen (!) codieren
oder mit dem kaufmännischen Und-Zeichen (&).
Wenn wir ein Programm schreiben würden, das nur mit ASCII-Zeichen arbeiten könnte, könnten wir die von unserem Puffer verwendete Codierung mit dem dritten Argument der Funktion alloc () ändern - der Codierung.
Erstellen wir einen neuen Puffer, der fünf Byte lang ist und nur ASCII-Zeichen speichert:
Der Puffer wird mit fünf Bytes des Zeichens a unter Verwendung der ASCII-Darstellung initialisiert.
< $> Hinweis Hinweis: Node.js unterstützt standardmäßig die folgenden Zeichencodierungen:
ASCII, dargestellt als ascii
UTF-8, dargestellt als utf-8 oder utf8
UTF-16, dargestellt als utf-16le oder utf16le
UCS-2, dargestellt als ucs-2 oder ucs2
Base64, dargestellt als base64
Hexadezimal, dargestellt als hex
ISO / IEC 8859-1, dargestellt als latin1 oder binär
Alle diese Werte können in Pufferklassenfunktionen verwendet werden, die einen Codierungsparameter akzeptieren.
Daher sind diese Werte alle für die Methode alloc () gültig.
Bisher haben wir mit der Funktion alloc () neue Puffer erstellt.
Manchmal möchten wir jedoch einen Puffer aus bereits vorhandenen Daten erstellen, z. B. einer Zeichenfolge oder einer Anordnung.
Um einen Puffer aus bereits vorhandenen Daten zu erstellen, verwenden wir die Methode from ().
Wir können diese Funktion verwenden, um Puffer zu erstellen aus:
Eine Anordnung von Ganzzahlen: die ganzzahligen Werte können zwischen 0 und 255 liegen.
Ein ArrayBuffer: Dies ist ein JavaScript-Objekt, das eine festgesetzte Länge von Bytes speichert.
Eine Zeichenfolge.
Ein weiterer Puffer.
Andere JavaScript-Objekte mit einer Symbol.toPrimitive-Eigenschaft.
Diese Eigenschaft teilt JavaScript mit, wie das Objekt in einen primitiven Datentyp konvertiert werden soll: Boolescher Wert, Nullwert, undefiniert, Zahl, Zeichenfolge oder Symbol.
Weitere Informationen zu Symbolen finden Sie in der JavaScript-Dokumentation von Mozilla.
Sehen wir uns an, wie wir einen Puffer aus einer Zeichenfolge erstellen können.
Geben Sie in der Node.js Folgendes ein:
Wir haben jetzt ein Pufferobjekt aus der Zeichenfolge My name is Paul ​ ​ ​ erstellt.
Erstellen wir einen neuen Puffer aus einem anderen Puffer, den wir zuvor erstellt haben:
Wir haben jetzt einen neuen Puffer asciiCopy erstellt, der dieselben Daten wie asciiBuf enthält.
Nachdem wir die Erstellung von Puffern erlebt haben, können wir uns mit Beispielen zum Lesen ihrer Daten befassen.
Schritt 2 - Lesen aus einem Puffer
Es gibt viele Möglichkeiten, auf Daten in einem Puffer zuzugreifen.
Wir können auf ein einzelnes Byte in einem Puffer zugreifen oder den gesamten Inhalt extrahieren.
Um auf ein Byte eines Puffers zuzugreifen, übergeben wir den Index oder die Position des gewünschten Bytes.
Puffer speichern Daten sequentiell wie Arrays.
Sie indizieren ihre Daten auch wie Arrays, beginnend bei 0. Wir können die Array-Notation für das Pufferobjekt verwenden, um ein einzelnes Byte zu erhalten.
Sehen wir uns an, wie dies aussieht, indem Sie einen Puffer aus einer Zeichenfolge in der REPL erstellen:
Lesen wir nun das erste Byte des Puffers:
Wenn Sie ENTER drücken, zeigt die REPL Folgendes:
Die Ganzzahl 72 entspricht der UTF-8-Darstellung für den Buchstaben H.
< $> Hinweis Hinweis: Die Werte für Bytes können Zahlen zwischen 0 und 255 sein. Ein Byte ist eine Folge von 8 Bits.
Ein Bit ist binär und kann daher nur einen von zwei Werten haben: 0 oder 1. Wenn wir eine Folge von 8 Bits und zwei mögliche Werte pro Bit haben, haben wir maximal 2 ⁸ mögliche Werte für ein Byte.
Das führt zu einer maximalen Größe von 256 Werten.
Da wir ab Null zählen, bedeutet dies, dass unsere höchste Zahl 255 ist. < $>
Machen wir dasselbe für das zweite Byte.
Geben Sie Folgendes in die REPL ein:
Die REPL gibt 105 zurück, was den Kleinbuchstaben i darstellt.
Wir erhalten schließlich das dritte Zeichen:
In der REPL wird 33 angezeigt, was! entspricht.
Versuchen wir, ein Byte aus einem ungültigen Index abzurufen:
Die REPL wird Folgendes ausgeben:
Dies ist genau so, als hätten wir versucht, auf ein Element in einer Anordnung mit einem falschen Index zuzugreifen.
Nachdem wir nun gesehen haben, wie einzelne Bytes eines Puffers gelesen werden, sehen wir uns unsere Optionen zum gleichzeitigen Abrufen aller in einem Puffer gespeicherten Daten an.
Das Pufferobjekt wird mit den Methoden toString () und toJSON () bereitgestellt, die den gesamten Inhalt eines Puffers in zwei verschiedenen Formaten zurückgeben.
Wie der Name schon sagt, konvertiert die toString () -Methode die Bytes des Puffers in eine Zeichenfolge und gibt sie an den Benutzer zurück.
Wenn wir diese Methode auf hiBuf verwenden, erhalten wir die Zeichenfolge Hi!.
Versuchen wir es!
Geben Sie in der Eingabeaufforderung Folgendes ein:
Dieser Puffer wurde aus einer Zeichenfolge erstellt.
Sehen wir uns an, was passiert, wenn wir toString () in einem Puffer verwenden, der nicht aus Zeichenfolgendaten hergestellt wurde.
Erstellen wir einen neuen, leeren Puffer, der 10 Bytes groß ist:
Verwenden wir nun die toString () -Methode:
Wir sehen das folgende Ergebnis:
Die Zeichenfolge\ u0000 ist das Unicode-Zeichen für NULL.
Sie entspricht der Zahl 0. Wenn die Daten des Puffers nicht als Zeichenfolge codiert sind, gibt die Methode toString () die UTF-8-Codierung der Bytes zurück.
ToString () verfügt über einen optionalen Parameter, die Codierung.
Mit diesem Parameter können wir die Codierung der zurückgegebenen Pufferdaten ändern.
Wenn Sie beispielsweise die hexadezimale Codierung für hiBuf wünschen, geben Sie an der Eingabeaufforderung Folgendes ein:
Diese Aussage wird folgendermaßen bewertet:
486921 ist die hexadezimale Darstellung für die Bytes, die die Zeichenfolge Hi! darstellen.
Wenn Benutzer in Node.js die Codierung von Daten von einem Formular in ein anderes konvertieren möchten, legen sie die Zeichenfolge normalerweise in einen Puffer und rufen toString () mit der gewünschten Codierung auf.
Die toJSON () -Methode verhält sich anders.
Unabhängig davon, ob der Puffer aus einer Zeichenfolge erstellt wurde oder nicht, werden die Daten immer als ganzzahlige Darstellung des Bytes zurückgegeben.
Lassen Sie uns die Puffer hiBuf und tenZeroes erneut verwenden, um die Verwendung von toJSON () zu üben.
Geben Sie bei der Eingabeaufforderung Folgendes ein:
Das JSON-Objekt verfügt über eine type-Eigenschaft, die immer Puffer ist.
Auf diese Weise können Programme diese JSON-Objekte von anderen JSON-Objekten unterscheiden.
Die Daten-Eigenschaft enthält eine Anordnung der ganzzahligen Darstellung der Bytes.
Möglicherweise haben Sie bemerkt, dass 72, 105 und 33 den Werten entsprechen, die wir beim einzelnen Abrufen der Bytes erhalten haben.
Versuchen wir die toJSON () -Methode mit tenZeroes:
In der REPL sehen Sie Folgendes:
Der Typ ist der gleiche wie zuvor angegeben.
Die Daten sind jedoch jetzt eine Anordnung mit zehn Nullen.
Nachdem wir uns nun mit den wichtigsten Möglichkeiten zum Lesen aus einem Puffer befasst haben, schauen wir uns an, wie wir den Inhalt eines Puffers ändern.
Schritt 3 - Modifizieren eines Puffers
Es gibt viele Möglichkeiten, um ein bestehendes Pufferobjekt zu ändern.
Ähnlich wie beim Lesen können wir Pufferbytes mithilfe der Array-Syntax einzeln ändern.
Wir können auch neue Inhalte in einen Puffer schreiben und die vorhandenen Daten ersetzen.
Lassen Sie uns zunächst untersuchen, wie wir einzelne Bytes eines Puffers ändern können.
Erinnern Sie sich an unsere Puffervariable hiBuf, die den String Hi! enthält.
Lassen Sie uns jedes Byte so ändern, dass es stattdessen Hey enthält.
Versuchen wir in der REPL zunächst, das zweite Element von hiBuf auf e zu setzen:
Lassen Sie uns diesen Puffer nun als Zeichenfolge betrachten, um zu bestätigen, dass die richtigen Daten gespeichert werden. Rufen Sie anschließend die toString () -Methode auf:
Sie wird bewertet als:
Wir haben diese seltsame Ausgabe erhalten, weil der Puffer nur einen ganzzahligen Wert akzeptieren kann.
Wir können ihn nicht dem Buchstaben e zuordnen; vielmehr müssen wir ihm die Zahl zuweisen, deren binäres Äquivalent e darstellt:
Wenn wir nun die toString () -Methode aufrufen:
In der REPL erhalten wir diese Ausgabe:
Um das letzte Zeichen im Puffer zu ändern, müssen wir das dritte Element auf die Ganzzahl setzen, die dem Byte für y entspricht:
Lassen Sie uns dies noch einmal mit der toString () -Methode bestätigen:
Ihre REPL zeigt:
Wenn wir versuchen, ein Byte zu schreiben, das außerhalb des Bereichs des Puffers liegt, wird es ignoriert und der Inhalt des Puffers ändert sich nicht.
Versuchen wir beispielsweise, das nicht vorhandene vierte Element des Puffers auf o zu setzen:
Wir können bestätigen, dass der Puffer mit der toString () -Methode unverändert bleibt:
Die Ausgabe ist immer noch:
Wenn wir den Inhalt des gesamten Puffers ändern möchten, können wir die write () -Methode verwenden.
Die write () -Methode akzeptiert eine Zeichenfolge, die den Inhalt eines Puffers ersetzt.
Verwenden wir die write () -Methode, um den Inhalt von hiBuf wieder in Hi! zu ändern.
Geben Sie in Ihrer Node.js-Shell bei der Eingabeaufforderung den folgenden Befehl ein:
Die write () -Methode hat 3 in der REPL zurückgegeben.
Dies liegt daran, dass drei Datenbytes geschrieben wurden. Jeder Buchstabe hat eine Byte-Größe, da dieser Puffer eine UTF-8-Codierung verwendet, bei der für jedes Zeichen ein Byte verwendet wird.
Wenn der Puffer eine UTF-16-Codierung verwendet hätte, die mindestens zwei Bytes pro Zeichen enthält, hätte die Funktion write () 6 zurückgegeben.
Überprüfen Sie nun den Inhalt des Puffers mit toString ():
Die REPL wird Folgendes ausgeben:
Dies ist schneller, als jedes Element byteweise ändern zu müssen.
Wenn Sie versuchen, mehr Bytes als die Größe eines Puffers zu schreiben, akzeptiert das Pufferobjekt nur, welche Bytes passen.
Erstellen wir zur Veranschaulichung einen Puffer, in dem drei Bytes gespeichert sind:
Versuchen wir nun, Cats darauf zu schreiben:
Wenn der Aufruf von write () ausgewertet wird, gibt die REPL 3 zurück und zeigt an, dass nur drei Bytes in den Puffer geschrieben wurden.
Bestätigen Sie nun, dass der Puffer die ersten drei Bytes enthält:
Die REPL gibt Folgendes aus:
Die Funktion write () fügt die Bytes in sequentieller Reihenfolge hinzu, sodass nur die ersten drei Bytes in den Puffer gestellt wurden.
Im Gegensatz dazu erstellen wir einen Puffer, der vier Bytes speichert:
Schreiben Sie den gleichen Inhalt darauf:
Fügen Sie dann einige neue Inhalte hinzu, die weniger Platz beanspruchen als der ursprüngliche Inhalt:
Da Puffer nacheinander ab 0 schreiben, wenn wir den Inhalt des Puffers drucken:
Wir würden begrüßt werden mit:
Die ersten beiden Zeichen werden überschrieben, der Rest des Puffers bleibt jedoch unberührt.
Manchmal befinden sich die Daten, die wir in unserem bereits vorhandenen Puffer haben möchten, nicht in einer Zeichenfolge, sondern in einem anderen Pufferobjekt.
In diesen Fällen können wir die copy () -Funktion verwenden, um zu ändern, was unser Puffer speichert.
Erstellen wir zwei neue Puffer:
Die Puffer wordsBuf und catchphraseBuf enthalten beide Zeichenfolgendaten. Wir möchten catchphraseBuf so ändern, dass es Nananana Turtle! speichert
statt Not sure Turtle! ​ ​ ​.
Wir werden copy () verwenden, um Nananana von wordsBuf zu catchphraseBuf zu bekommen.
Um Daten von einem Puffer in den anderen zu kopieren, verwenden wir die copy () -Methode für den Puffer, der die Informationsquelle darstellt.
Da wordsBuf die zu kopierenden Zeichenfolgendaten enthält, müssen wir diese wie folgt kopieren:
Der Zielparameter ist in diesem Fall der catchphraseBuf-Puffer.
Wenn wir das in die REPL eingeben, gibt es 15 zurück, was anzeigt, dass 15 Bytes geschrieben wurden.
Die Zeichenfolge Nananana verwendet nur 8 Datenbytes, sodass wir sofort wissen, dass unsere Kopie nicht wie beabsichtigt verlaufen ist.
Verwenden Sie die toString () -Methode, um den Inhalt von catchphraseBuf anzuzeigen:
Standardmäßig hat copy () den gesamten Inhalt von wordsBuf übernommen und in catchphraseBuf abgelegt.
Wir müssen selektiver für unser Ziel sein und nur Nananana kopieren.
Lassen Sie uns den ursprünglichen Inhalt von catchphraseBuf neu schreiben, bevor wir fortfahren:
Die Funktion copy () verfügt über einige weitere Parameter, mit denen wir anpassen können, welche Daten in den anderen Puffer kopiert werden.
Hier ist eine Liste aller Parameter dieser Funktion:
target - Dies ist der einzige erforderliche Parameter von copy ().
Wie wir aus unserer vorherigen Verwendung gesehen haben, ist dies der Puffer, in den wir kopieren möchten.
targetStart - Dies ist der Index der Bytes im Zielpuffer, in den mit dem Kopieren begonnen werden soll.
Standardmäßig ist es 0, d. h. es werden Daten kopiert, die am Anfang des Puffers beginnen.
sourceStart - Dies ist der Index der Bytes im Quellpuffer, aus denen kopiert werden soll.
sourceEnd - Dies ist der Index der Bytes im Quellpuffer, in den das Kopieren beendet werden soll.
Standardmäßig ist es die Länge des Puffers.
Um Nananana aus wordsBuf in catchphraseBuf zu kopieren, sollte unser Ziel catchphraseBuf wie zuvor sein.
Der targetStart wäre 0, da Nananana am Anfang von catchphraseBuf erscheinen soll.
Der sourceStart sollte 7 sein, da dies der Index ist, in dem Nananana in wordsBuf beginnt.
Das sourceEnd würde weiterhin die Länge der Puffer sein.
Kopieren Sie bei der REPL-Eingabeaufforderung den Inhalt von wordsBuf folgendermaßen:
Die REPL bestätigt, dass 8 Bytes geschrieben wurden.
Beachten Sie, wie wordsBuf.length als Wert für den Parameter sourceEnd verwendet wird.
Wie bei Arrays gibt uns die Eigenschaft length die Größe des Puffers an.
Nun sehen wir uns den Inhalt von catchphraseBuf an:
Wir konnten die Daten von catchphraseBuf ändern, indem wir den Inhalt von wordsBuf kopiert haben.
Sie können die Node.js REPL beenden, wenn Sie dies möchten.
Beachten Sie, dass alle erstellten Variablen nicht mehr verfügbar sind, wenn Sie Folgendes tun:
In diesem Tutorial haben Sie gelernt, dass Puffer Zuordnungen fester Länge im Speicher sind, in denen Binärdaten gespeichert werden. Sie haben zuerst Puffer erstellt, indem Sie ihre Größe im Speicher definiert und sie mit bereits vorhandenen Daten initialisiert haben. Anschließend lasen Sie Daten aus einem Puffer, indem Sie die einzelnen Bytes untersucht und die Methoden toString () und toJSON () verwendet haben.
Schließlich haben Sie die von einem Puffer gespeicherten Daten geändert, indem Sie die einzelnen Bytes geändert und die Methoden write () und copy () verwendet haben.
Puffer geben Ihnen einen guten Einblick, wie Binärdaten von Node.js manipuliert werden.
Jetzt, da Sie mit Puffern interagieren können, können Sie die verschiedenen Auswirkungen der Zeichenkodierung auf die Speicherung von Daten beobachten.
Sie können beispielsweise Puffer aus Zeichenfolgendaten erstellen, die keine UTF-8- oder ASCII-Codierung aufweisen, und deren Größenunterschied beobachten.
Sie können auch einen Puffer mit UTF-8 und toString () verwenden, um ihn in andere Codierungsschemata zu konvertieren.
Informationen zu Puffern in Node.js finden Sie in der Node.js-Dokumentation zum Puffer-Objekt.
Wenn Sie Node.js weiter lernen möchten, können Sie zur Codieren in Node-Reihe zurückkehren oder Programmierprojekte und -einstellungen auf unserer Node-Themenseite durchsuchen.
Hinzufügen von Auslagerungsspeicher unter Ubuntu 20.04
5463
Eine Möglichkeit, sich vor Fehlern aufgrund von Speichermangel in Anwendungen zu schützen, besteht darin, Ihrem Server Swap-Speicherplatz hinzuzufügen.
In diesem Leitfaden erfahren Sie, wie Sie einem Ubuntu 20.04-Server eine Swap-Datei hinzufügen.
< $> warning Warnung: Obwohl die Auslagerung im Allgemeinen für Systeme empfohlen wird, die herkömmliche rotierende Festplatten verwenden, kann das Platzieren der Auslagerung auf SSDs im Laufe der Zeit zu Problemen im Hinblick auf die Hardwareverschlechterung führen.
Daher empfehlen wir, die Auslagerung bei DigitalOcean oder anderen Anbietern, die SSD-Speicher verwenden, nicht zu aktivieren.
Was ist Swap?
Unter dem Begriff Auslagerung, hier auch als Swap bezeichnet, versteht man einen Teil des Festplattenspeichers, der für das Betriebssystem reserviert wurde, um vorübergehend Daten zu speichern, die es nicht mehr im RAM aufbewahren kann.
Auf diese Weise können Sie die Informationsmenge erhöhen, die Ihr Server mit einigen Einschränkungen im Arbeitsspeicher speichern kann.
Der Auslagerungsspeicher auf der Festplatte wird hauptsächlich verwendet, wenn im RAM nicht mehr genügend Speicherplatz vorhanden ist, um Anwendungsdaten zu speichern.
Die auf die Festplatte geschriebenen Informationen sind erheblich langsamer als die im RAM gespeicherten Informationen. Das Betriebssystem zieht es jedoch vor, weiterhin Anwendungsdaten im Speicher auszuführen und die Auslagerung für die älteren Daten zu verwenden. Insgesamt kann der Auslagerungsspeicher als Ersatz für die Erschöpfung des Arbeitsspeichers Ihres Systems ein gutes Sicherheitsnetz gegen Ausnahmen aufgrund von Speichermangel auf Systemen mit nicht verfügbarem SSD-Speicher sein.
Schritt 1 - Überprüfen des Systems auf Swap-Informationen
Bevor wir beginnen, können wir überprüfen, ob auf dem System bereits Auslagerungsspeicher verfügbar ist.
Es ist möglich, mehrere Auslagerungsdateien oder -partitionen zu haben, aber im Allgemeinen sollte eine ausreichen.
Wir können sehen, ob das System eine konfigurierte Auslagerungsdatei oder -partition hat, indem wir Folgendes eingeben:
Wenn Sie keine Ausgabe zurückerhalten, bedeutet dies, dass auf Ihrem System derzeit kein Auslagerungsspeicher verfügbar ist.
Mit dem Dienstprogramm free können Sie überprüfen, dass keine aktive Auslagerung vorhanden ist:
Wie Sie in der Swap-Zeile der Ausgabe sehen können, ist auf dem System keine Auslagerung aktiv.
Schritt 2 - Überprüfen des verfügbaren Speicherplatzes auf der Festplattenpartition
Bevor wir unsere Auslagerungsdatei erstellen, überprüfen wir unsere aktuelle Festplattennutzung, um sicherzustellen, dass genügend Speicherplatz vorhanden ist.
Tun Sie dies, indem Sie Folgendes eingeben:
Das Gerät mit / in der Spalte Mounted on ist in diesem Fall unsere Festplatte.
In diesem Beispiel steht ausreichend Speicherplatz zur Verfügung (nur 1,4 G werden verwendet).
Ihre Auslastung wird wahrscheinlich anders sein.
Obwohl es viele Meinungen über die angemessene Größe eines Auslagerungsspeichers gibt, hängt dies wirklich von Ihren persönlichen Vorlieben und Ihren Anwendungsanforderungen ab.
Im Allgemeinen ist eine Größe, die dem RAM-Speicher Ihres Systems entspricht oder diesen verdoppelt, ein guter Ausgangspunkt.
Eine andere gute Faustregel ist, dass alles über 4 GB Auslagerung wahrscheinlich unnötig ist, wenn Sie es nur als RAM-Ausweichlösung verwenden.
Schritt 3 - Erstellen einer Auslagerungsdatei
Nachdem wir unseren verfügbaren Festplattenspeicher kennen, können wir eine Auslagerungsdatei auf unserem Dateisystem erstellen.
Wir werden eine Datei der gewünschten Größe namens swapfile in unserem Stammverzeichnis (/) zuweisen.
Der beste Weg, eine Auslagerungsdatei zu erstellen, ist das fallocate-Programm.
Dieser Befehl erstellt sofort eine Datei mit der angegebenen Größe.
Da der Server in unserem Beispiel über 1 GB RAM verfügt, erstellen wir in diesem Leitfaden eine 1 GB-Datei.
Passen Sie dies an die Anforderungen Ihres eigenen Servers an:
Wir können überprüfen, ob die korrekte Speicherplatzgröße reserviert wurde, indem wir Folgendes eingeben:
Unsere Datei wurde mit der richtigen Speicherplatzgröße erstellt.
Schritt 4 - Aktivieren der Auslagerungsdatei
Nachdem wir eine Datei mit der richtigen Größe zur Verfügung haben, müssen wir diese tatsächlich in Auslagerungsspeicher umwandeln.
Zunächst müssen wir die Berechtigungen der Datei sperren, damit nur Benutzer mit root-Berechtigungen den Inhalt lesen können.
Dies verhindert, dass normale Benutzer auf die Datei zugreifen können, was erhebliche Auswirkungen auf die Sicherheit haben würde.
Machen Sie die Datei nur für root zugänglich, indem Sie Folgendes eingeben:
Verifizieren Sie die Änderung der Berechtigungen, indem Sie Folgendes eingeben:
Wie Sie sehen können, sind die Lese- und Schreibflags nur für den Benutzer root aktiviert.
Wir können jetzt die Datei als Auslagerungsspeicher markieren, indem wir Folgendes eingeben:
Nach dem Markieren der Datei können wir die Auslagerungsdatei aktivieren, sodass unser System sie verwenden kann:
Verifizieren Sie, ob die Auslagerung verfügbar ist, indem Sie Folgendes eingeben:
Wir können die Ausgabe des Dienstprogramms free erneut überprüfen, um unsere Ergebnisse zu bestätigen:
Unsere Auslagerung wurde erfolgreich eingerichtet und wird von unserem Betriebssystem bei Bedarf verwendet.
Schritt 5 - Permanente Bereitstellung der Auslagerungsdatei
Unsere letzten Änderungen haben die Auslagerungsdatei für die aktuelle Sitzung aktiviert.
Wenn wir jedoch neu starten, wird der Server die Auslagerungseinstellungen nicht automatisch beibehalten.
Wir können dies ändern, indem wir die Auslagerungsdatei zu unserer / etc / fstab-Datei hinzufügen.
Sichern Sie die / etc / fstab-Datei, falls etwas schief geht:
Fügen Sie die Informationen zur Auslagerungsdatei am Ende Ihrer Datei / etc / fstab hinzu, indem Sie Folgendes eingeben:
Als nächstes werden wir einige Einstellungen überprüfen, die wir aktualisieren können, um unseren Auslagerungsspeicher zu optimieren.
Schritt 6 - Optimieren Ihrer Swap-Einstellungen
Es gibt einige Optionen, die Sie konfigurieren können und die sich auf die Leistung Ihres Systems auswirken, wenn Sie mit Swap arbeiten.
Anpassen der Swappiness-Eigenschaft
Der swappiness-Parameter konfiguriert, wie oft Ihr System Daten aus dem RAM in den Auslagerungsspeicher überträgt.
Dies ist ein Wert zwischen 0 und 100, der einen Prozentsatz darstellt.
Bei Werten nahe Null lagert der Kernel keine Daten auf die Festplatte aus, es sei denn, dies ist unbedingt erforderlich.
Denken Sie daran, dass Interaktionen mit der Auslagerungsdatei insofern "teuer" sind, da sie viel länger dauern als Interaktionen mit dem RAM und eine erhebliche Leistungsminderung verursachen können.
Wenn Sie dem System sagen, dass es sich nicht zu sehr auf die Auslagerung verlassen soll, wird Ihr System im Allgemeinen schneller.
Werte, die näher an 100 liegen, versuchen, mehr Daten auszulagern, um mehr RAM-Speicherplatz freizuhalten.
Abhängig vom Speicherprofil Ihrer Anwendungen oder davon, wofür Sie Ihren Server verwenden, ist dies in einigen Fällen möglicherweise besser.
Wir können den aktuellen Swappiness-Wert sehen, indem wir Folgendes eingeben:
Für einen Desktop ist eine Swappiness-Einstellung von 60 kein schlechter Wert.
Für einen Server möchten Sie ihn möglicherweise näher an 0 festlegen.
Mit dem Befehl sysctl können wir die Swappiness auf einen anderen Wert setzen.
Um beispielsweise die Swappiness auf 10 zu setzen, könnten wir Folgendes eingeben:
Diese Einstellung bleibt bis zum nächsten Neustart bestehen.
Wir können diesen Wert beim Neustart automatisch festlegen, indem wir die Zeile zu unserer / etc / sysctl.conf-Datei hinzufügen:
Am Ende können Sie Folgendes hinzufügen:
Anpassen der Cache-Druckeinstellung
Ein weiterer verwandter Wert, den Sie möglicherweise ändern möchten, ist vfs _ cache _ pressure.
Diese Einstellung konfiguriert, in welchem Umfang das System inode- und dentry-Informationen über andere Daten zwischenspeichert.
Grundsätzlich handelt es sich hierbei um Zugriffsdaten zum Dateisystem.
Das Nachschlagen ist im Allgemeinen sehr kostspielig und wird sehr häufig angefordert. Daher ist die Zwischenspeicherung für Ihr System eine hervorragende Sache.
Sie können den aktuellen Wert anzeigen, indem Sie das proc-Dateisystem erneut abfragen:
In der aktuellen Konfiguration entfernt unser System inode-Informationen zu schnell aus dem Cache.
Wir können dies auf eine konservativere Einstellung wie 50 einstellen, indem wir Folgendes eingeben:
Dies gilt wiederum nur für unsere aktuelle Sitzung.
Wir können dies ändern, indem wir es unserer Konfigurationsdatei hinzufügen, wie wir es mit unserer Swappiness-Einstellung getan haben:
Fügen Sie unten die Zeile hinzu, die Ihren neuen Wert angibt:
Wenn Sie die Schritte in diesem Leitfaden befolgen, haben Sie in Fällen, in denen sonst Ausnahmen aufgrund von Speichermangel auftreten würden, etwas Luft zum Atmen.
Auslagerungsspeicher kann unglaublich nützlich sein, um einige dieser häufigen Probleme zu vermeiden.
Wenn Sie auf OOM-Fehler (nicht genügend Speicher) stoßen oder feststellen, dass Ihr System die von Ihnen benötigten Anwendungen nicht verwenden kann, besteht die beste Lösung darin, Ihre Anwendungskonfigurationen zu optimieren oder Ihren Server zu aktualisieren.
Einrichten und Konfigurieren eines OpenVPN-Servers unter Ubuntu 20.04
5390
Ein Virtual Private Network (VPN) ermöglicht Ihnen die Nutzung nicht vertrauenswürdiger Netzwerke, als ob Sie sich in einem privaten Netzwerk befinden würden.
Es bietet Ihnen die Möglichkeit, mit Ihrem Smartphone oder Laptop sicher auf das Internet zuzugreifen, wenn Sie mit einem nicht vertrauenswürdigen Netzwerk, wie dem WLAN in einem Hotel oder Café, verbunden sind.
Sie können geografische Beschränkungen und Zensuren umgehen und Ihren Ort und jeglichen unverschlüsselten HTTP-Verkehr vom nicht vertrauenswürdigen Netzwerk abschirmen.
OpenVPN ist eine vollständige Open-Source-VPN-Lösung mit Transport Layer Security (TLS), die eine breite Palette von Konfigurationen ermöglicht.
In diesem Tutorial richten Sie OpenVPN auf einem Ubuntu 20.04-Server ein und konfigurieren es dann so, dass es von einem Client-Computer aus zugänglich ist.
< $> note Anmerkung: Wenn Sie planen, einen OpenVPN-Server auf einem DigitalOcean-Droplet einzurichten, sollten Sie sich bewusst sein, dass wir, wie viele Hosting-Anbieter, Zusatzgebühren für das Überschreiten des Bandbreitenlimits verlangen können.
Einen Ubuntu 20.04-Server mit einem sudo Nicht-root-Benutzer und eine aktivierte Firewall.
Um dies einzurichten, können Sie unserem Tutorial Ersteinrichtung des Servers unter Ubuntu 20.04 folgen.
In diesem Leitfaden wird dieser als OpenVPN-Server bezeichnet.
Einen separaten Ubuntu 20.04-Server, der als private Zertifizierungsstelle (" Certificate Authority ", CA) eingerichtet ist. Diesen bezeichnen wird in diesem Leitfaden als CA-Server.
Um das einzurichten, können Sie nach Ausführung der Schritte aus dem Leitfaden Ersteinrichtung des Servers in diesem Server den Schritten 1 bis 3 unseres Leitfadens Einrichten und Konfigurieren einer Zertifizierungsstelle (CA) unter Ubuntu 20.04 folgen.
< $> note Anmerkung: Obwohl es technisch möglich ist, Ihren OpenVPN-Server oder Ihren lokalen Computer als Ihre CA zu verwenden, wird dies nicht empfohlen, da es Ihr VPN für einige Sicherheitslücken öffnet.
Gemäß der offiziellen OpenVPN-Dokumentation sollten Sie Ihre CA auf einem eigenständigen Computer ablegen, der für das Importieren und Signieren von Zertifikatsanforderungen bestimmt ist.
Aus diesem Grund wird in diesem Leitfaden davon ausgegangen, dass sich Ihre CA auf einem separaten Ubuntu 20.04-Server befindet, der auch einen non-root user mit sudo-Berechtigungen und eine einfache Firewall aufweist.
Zusätzlich benötigen Sie einen Client-Computer, den Sie für die Verbindung mit Ihrem OpenVPN-Server verwenden.
In diesem Leitfaden nennen wir ihn OpenVPN-Client.
Für die Zwecke dieses Tutorials wird empfohlen, dass Sie Ihren lokalen Computer als OpenVPN-Client verwenden.
Wenn diese Voraussetzungen erfüllt sind, können Sie mit der Einrichtung und Konfiguration eines OpenVPN-Servers unter Ubuntu 20.04 beginnen.
< $> note Anmerkung: Bitte beachten Sie, dass Sie später in diesem Leitfaden beim Übertragen von Dateien zwischen den Servern Schwierigkeiten haben könnten, wenn Sie beim Konfigurieren der Server die Passwort-Authentifizierung deaktivieren.
Alternativ könnten Sie für jeden Server ein SSH-Schlüsselpaar erstellen, dann den öffentlichen SSH-Schlüssel des OpenVPN-Servers zur Datei authorized _ keys des CA-Computers hinzufügen und umgekehrt.
Anweisungen zur Ausführung dieser Lösungen finden Sie unter Einrichten von SSH-Schlüsseln unter Ubuntu 20.04.
Schritt 1 - Installieren von OpenVPN und Easy-RSA
Der erste Schritt in diesem Tutorial ist die Installation von OpenVPN und Easy-RSA.
Easy-RSA ist ein Verwaltungs-Tool mit öffentlicher Schlüsselinfrastruktur (" public key infrastructure ", PKI), das Sie auf dem OpenVPN-Server zum Erzeugen einer Zertifikatsanforderung verwenden, die Sie dann auf dem CA-Server verifizieren und signieren.
Aktualisieren Sie zunächst den Paketindex Ihres OpenVPN-Servers und installieren Sie OpenVPN und Easy-RSA.
Beide Pakete sind in Standard-Repositorys von Ubuntu verfügbar, sodass Sie apt für die Installation verwenden können:
Als Nächstes müssen Sie ein neues Verzeichnis auf dem OpenVPN-Server als non-root user namens ~ / easy-rsa erstellen:
Jetzt müssen Sie einen Symlink aus dem easyrsa-Skript erstellen, den das Paket in das gerade von Ihnen erstellte Verzeichnis ~ / easy-rsa installiert hat:
< $> note Anmerkung: Während andere Leitfäden Sie möglicherweise anweisen, die Dateien des easy-rsa-Pakets in Ihr PKI-Verzeichnis zu kopieren, verfolgt dieses Tutorial einen Symlink-Ansatz.
Infolgedessen werden alle Aktualisierungen des easy-rsa-Pakets automatisch in den Skripten Ihrer PKI wiedergegeben.
Stellen Sie abschließend sicher, dass der Eigentümer des Verzeichnisses Ihr non-root sudo-user ist und schränken Sie mit chmod den Zugriff für diesen User ein:
Sobald diese Programme installiert sind und an die richtigen Orte auf Ihrem System verschoben wurden, besteht der nächste Schritt darin, eine öffentliche Schlüsselinfrastruktur (PKI) auf dem OpenVPN-Server zu erstellen, damit Sie TLS-Zertifikate für Clients und andere Server, die sich mit Ihrem VPN verbinden werden, anfordern und verwalten können.
Schritt 2 - Erstellen einer PKI für OpenVPN
Bevor Sie den privaten Schlüssel und das Zertifikat Ihres OpenVPN-Servers erstellen können, müssen Sie ein lokales Verzeichnis der öffentlichen Schlüsselinfrastruktur auf Ihrem OpenVPN-Server erstellen.
Sie verwenden dieses Verzeichnis, um die Zertifikatsanforderungen des Servers und der Clients zu verwalten, anstatt sie direkt auf Ihrem CA-Server zu erstellen.
Um ein PKI-Verzeichnis auf Ihrem OpenVPN-Server zu erstellen, müssen Sie eine Datei namens vars mit einigen Standardwerten füllen.
Zuerst wechseln Sie mit cd in das Verzeichnis easy-rsa, dann werden Sie die Datei vars mit nano oder Ihrem bevorzugten Texteditor erstellen und bearbeiten:
Sobald die Datei geöffnet ist, fügen Sie die folgenden beiden Zeilen ein:
Das sind die einzigen Zeilen, die Sie in dieser vars-Datei auf Ihrem OpenVPN-Server benötigen, da er nicht als Zertifizierungsstelle verwendet wird.
Diese Zeilen stellen sicher, dass Ihre privaten Schlüssel und Zertifikatsanforderungen so konfiguriert sind, dass sie moderne Elliptische-Kurven-Kryptografie (" Elliptic Curve Cryptography ", ECC) nutzen, um Schlüssel und sichere Signaturen für Ihre Clients und OpenVPN-Server zu erzeugen.
Wenn Sie Ihre OpenVPN- und CA-Server für die Verwendung von ECC konfigurieren, können Client und Server beim Versuch, einen gemeinsamen symmetrischen Schlüssel zu erstellen, Algorithmen für elliptische Kurven für den Austausch verwenden.
Die Verwendung von ECC für einen Schlüsselaustausch ist wesentlich schneller als die Verwendung von einfachem Diffie-Hellman mit dem klassischen RSA-Algorithmus, da die Zahlen viel kleiner und die Berechnungen schneller sind.
< $> note Hintergrund: Wenn Clients sich mit OpenVPN verbinden, verwenden sie asymmetrische Verschlüsselung (auch bekannt als öffentlicher / privater Schlüssel), um einen TLS-Handshake auszuführen.
Wenn der Server und Clients jedoch verschlüsselten VPN-Verkehr übertragen, verwenden sie symmetrische Verschlüsselung, die auch als geteilte Schlüssel-Verschlüsselung bekannt ist.
Im Vergleich zu asymmetrischer Verschlüsselung ist der Rechenaufwand bei symmetrischer Verschlüsselung viel geringer: Die verwendeten Zahlen sind wesentlich kleiner und moderne CPUs integrieren Anweisungen für die Ausführung optimierter symmetrischer Verschlüsselungsoperationen.
Für den Wechsel von asymmetrischer zu symmetrischer Verschlüsselung verwenden der OpenVPN-Server und der Client den Algorithmus Elliptic Curve Diffie-Hellman (ECDH), um so schnell wie möglich einen geteilten geheimen Schlüssel zu akzeptieren.
Sobald Sie die vars-Datei gefüllt haben, können Sie mit der Erstellung des PKI-Verzeichnisses fortfahren.
Führen Sie dazu das Skript easyrsa mit der Option init-pki aus.
Obwohl Sie diesen Befehl als Teil der Voraussetzungen bereits auf dem CA-Server ausgeführt haben, ist es notwendig, diesen auch hier auszuführen, da Ihr OpenVPN-Server und der CA-Server separate PKI-Verzeichnisse haben:
Beachten Sie, dass auf Ihrem OpenVPN-Server keine Zertifizierungsstelle erstellt werden muss.
Ausschließlich Ihr CA-Server ist für die Validierung und Signierung von Zertifikaten zuständig.
Die PKI auf Ihrem VPN-Server dient nur als praktischer und zentralisierter Ort zum Speichern von Zertifikatsanforderungen und öffentlichen Zertifikaten.
Nachdem Sie Ihre PKI auf dem OpenVPN-Server initialisiert haben, können Sie zum nächsten Schritt, dem Erstellen einer OpenVPN-Server-Zertifikatsanforderung und eines privaten Schlüssels, übergehen.
Schritt 3 - Erstellen einer OpenVPN-Server-Zertifikatsanforderung und eines privaten Schlüssels
Nachdem Ihr OpenVPN-Server nun alle Voraussetzungen installiert hat, besteht der nächste Schritt darin, auf Ihrem OpenVPN-Server einen privaten Schlüssel und eine Zertifikatsignierungsanforderung (" Certificate Signing Request ", CSR) zu erstellen.
Danach übertragen Sie die Anforderung zum Signieren an Ihre CA, sodass das erforderliche Zertifikat erstellt wird.
Sobald Sie ein signiertes Zertifikat haben, übertragen Sie es zurück zum OpenVPN-Server und installieren es für die Nutzung durch den Server.
Navigieren Sie zunächst als Ihr non-root user zum ~ / easy-rsa-Verzeichnis auf Ihrem OpenVPN-Server:
Nun rufen Sie das easyrsa mit der Option gen-req auf, gefolgt von einem geläufigen Namen, dem Common Name (CN,) für den Computer.
Sie können den CN frei auswählen, aber es kann hilfreich sein, eine aussagekräftige Bezeichnung zu wählen.
In diesem Tutorial ist der CN des OpenVPN-Servers server.
Wenn das nicht geschieht, wird die Anforderungsdatei passwortgeschützt, was später zu Berechtigungsproblemen führen könnte.
< $> note Anmerkung: Wenn Sie hier einen anderen Namen als server wählen, müssen Sie einige der nachstehenden Anweisungen anpassen.
Kopieren Sie den Serverschlüssel in das Verzeichnis / etc / openvpn / server:
Nach dem Ausführen dieser Schritte haben Sie erfolgreich einen privaten Schlüssel für Ihren OpenVPN-Server erstellt.
Sie haben auch eine Zertifikatsignierungsanforderung für den OpenVPN-Server generiert.
Die CSR ist nun zur Signierung durch Ihre CA bereit.
Im nächsten Abschnitt dieses Tutorials lernen Sie, wie eine CSR mit dem privaten Schlüssel Ihres CA-Servers signiert wird.
Schritt 4 - Signieren der Zertifikatsanforderung des OpenVPN-Servers
Im vorherigen Schritt haben Sie eine Zertifikatsignierungsanforderung (CSR) und einen privaten Schlüssel für den OpenVPN-Server erstellt.
Nun muss der CA-Server von dem server-Zertifikat erfahren und es validieren. Sobald die CA das Zertifikat validiert und an den OpenVPN-Server zurückleitet, können Clients, die Ihrer CA vertrauen, auch dem OpenVPN-Server vertrauen.
Verwenden Sie als non-root user SCP oder eine andere Übertragungsmethode auf dem OpenVPN-Server, um die Zertifikatsanforderung server.req zur Signierung an den CA-Server zu kopieren:
Wenn Sie der Voraussetzung Einrichten und Konfigurieren einer Zertifizierungsstelle (CA) unter Ubuntu 20.04 gefolgt sind, besteht der nächste Schritt darin, sich beim CA-Server als Nicht-root-Benutzer anzumelden, den Sie für die Verwaltung Ihrer CA erstellt haben.
Sie wechseln mit cd in das Verzeichnis ~ / easy-rsa, in dem Sie Ihren PK erstellt haben, und importieren dann die Zertifikatsanforderung mit dem Skript easyrsa:
Signieren Sie die Anforderung, indem Sie das easyrsa-Skript mit der Option sign-req ausführen, gefolgt vom Anforderungstyp und dem geläufigen Namen.
Der Anforderungstyp kann entweder client oder server sein.
Da wir mit der Zertifikatsanforderung des OpenVPN-Servers arbeiten, muss der Anforderungstyp server verwendet werden:
In der Ausgabe werden Sie zur Überprüfung aufgefordert, ob die Anfrage von einer vertrauenswürdigen Quelle stammt.
Geben Sie yes ein und drücken Sie ENTER zur Bestätigung:
Beachten Sie, dass Sie an dieser Stelle zur Eingabe Ihres Passworts aufgefordert werden, falls Sie Ihren privaten CA-Schlüssel verschlüsselt haben.
Nach Abschluss dieser Schritte haben Sie die Zertifikatsanforderung des OpenVPN-Servers mit dem privaten Schlüssel des CA-Servers signiert.
Die resultierende Datei sammy-server.crt enthält den öffentlichen Verschlüsselungs-Schlüssel des OpenVPN-Servers sowie eine Signatur des CA-Servers.
Der Sinn der Signatur besteht darin, jedem, der dem CA-Server vertraut, mitzuteilen, dass er auch dem OpenVPN-Server vertrauen kann, wenn er sich mit ihm verbindet.
Um die Konfiguration der Zertifikate abzuschließen, kopieren Sie die Dateien server.crt und ca.crt vom CA-Server auf den OpenVPN-Server:
Zurück auf Ihrem OpenVPN-Server kopieren Sie die Dateien nun von / tmp nach / etc / openvpn / server:
Nun ist Ihr OpenVPN-Server fast bereit, Verbindungen zu akzeptieren.
Als Nächstes führen Sie einige zusätzliche Schritte aus, um die Sicherheit des Servers zu erhöhen.
Schritt 5 - Konfigurieren von kryptografischem OpenVPN-Material
Für eine Extraportion Sicherheit fügen wir einen zusätzlichen gemeinsamen geheimen Schlüssel hinzu, den der Server und alle Clients mit der Anweisung tls-crypt des OpenVPN verwenden.
Diese Option wird genutzt, um das TLS-Zertifikat zu verdecken, das bei der Erstverbindung eines Servers und Clients verwendet wird.
Sie wird auch vom OpenVPN-Server dazu genutzt, schnelle Kontrollen bei eingehenden Paketen durchzuführen: Wenn ein Paket mit dem gemeinsamen Schlüssel signiert ist, wird es vom Server verarbeitet. Wenn es nicht signiert ist, weiß der Server, dass es aus einer nicht vertrauenswürdigen Quelle stammt und kann es verwerfen, ohne es zusätzlich entschlüsseln zu müssen.
Diese Option hilft sicherzustellen, dass Ihr OpenVPN-Server in der Lage ist, mit nicht authentifiziertem Datenverkehr, Port-Scans und Denial-of-Service-Angriffen umzugehen, die Serverressourcen binden können.
Auch die Identifizierung des Netzwerkverkehrs mit OpenVPN wird hierdurch erschwert.
Um den gemeinsamen tls-crypt-Schlüssel zu generieren, führen Sie Folgendes auf dem OpenVPN-Server im Verzeichnis ~ / easy-rsa aus:
Das Ergebnis ist eine Datei namens ta.key.
Kopieren Sie diese in das Verzeichnis / etc / openvpn / server /:
Nachdem diese Dateien auf dem OpenVPN-Server vorhanden sind, können Sie Client-Zertifikate und Schlüsseldateien für Ihre Benutzer erstellen, die Sie für die Verbindung mit dem VPN verwenden.
Schritt 6 - Generieren eines Client-Zertifikats und eines Schlüsselpaars
Sie können zwar einen privaten Schlüssel und eine Zertifikatsanforderung auf Ihrem Client-Computer erstellen und dann zwecks Signierung zur CA senden, aber dieser Leitfaden beschreibt einen Prozess zum Generieren der Zertifikatsanforderung auf dem Server.
Der Vorteil besteht darin, dass wir ein Skript erstellen können, das automatisch Client-Konfigurationsdateien generiert, die alle benötigten Schlüssel und Zertifikate enthalten.
Kopieren Sie dann die Datei client1.key in das zuvor erstellte Verzeichnis ~ / client-configs / keys /:
Als Nächstes übertragen Sie die Datei client1.req mit einer sicheren Methode auf Ihren CA-Server:
Melden Sie sich nun bei Ihrem CA-Server an.
Navigieren Sie dann zum EasyRSA-Verzeichnis und importieren Sie die Zertifikatsanforderung:
Signieren Sie dann die Anforderung, wie Sie dies im vorherigen Schritt für den Server getan haben.
Geben Sie bei der Eingabeaufforderung yes ein, um zu bestätigen, dass Sie beabsichtigen, die Zertifikatsanforderung zu signieren, und dass sie aus einer vertrauenswürdigen Quelle stammt:
Zurück auf Ihrem OpenVPN-Server kopieren Sie das Client-Zertifikat in das Verzeichnis ~ / client-configs / keys /:
Kopieren Sie dann auch die Dateien ca.crt und ta.key in das Verzeichnis ~ / client-configs / keys / und legen Sie die entsprechenden Berechtigungen für Ihren sudo-user fest:
Damit wurden alle Zertifikate des Servers und Clients sowie alle Schlüssel generiert und in den entsprechenden Verzeichnissen auf Ihrem OpenVPN-Server gespeichert.
Vorerst können Sie mit der Konfiguration des OpenVPN fortfahren.
Schritt 7 - Konfigurieren von OpenVPN
Wie viele andere weit verbreitete Open-Source-Tools bietet auch OpenVPN eine Vielzahl von Konfigurationsoptionen an, mit denen Sie Ihren Server Ihren spezifischen Bedürfnissen anpassen können.
In diesem Abschnitt erhalten Sie Anweisungen zum Einrichten eines OpenVPN auf der Grundlage einer der Beispielkonfigurationsdateien, die in der Dokumentation dieser Software enthalten sind.
Kopieren Sie zunächst die Beispieldatei server.conf als Ausgangspunkt für Ihre eigene Konfigurationsdatei:
Öffnen Sie die neue Datei zum Bearbeiten mit dem Texteditor Ihrer Wahl.
Wir verwenden in unserem Beispiel nano:
Wir müssen einige Zeilen in dieser Datei ändern.
Finden Sie zunächst den HMAC-Abschnitt der Konfiguration, indem Sie nach der Anweisung tls-auth suchen.
Diese Zeile sollte unkommentiert sein.
Kommentieren Sie sie aus, indem Sie am Anfang der Zeile ein; einfügen.
Fügen Sie dann hinter diese eine neue Zeile hinzu, die nur den Wert tls-crypt ta.key enthält:
Als Nächstes lokalisieren Sie den Abschnitt über kryptografische Chiffren, indem Sie nach den cipher-Zeilen suchen.
Der Standardwert ist auf AES-256-CBC eingestellt. Die Chiffrierung mit AES-256-GCM bietet jedoch ein höheres Niveau an Verschlüsselung und Leistung und wird von modernen OpenVPN-Clients gut unterstützt.
Wir kommentieren den Standardwert aus, indem wir am Anfang dieser Zeile ein; -Zeichen einfügen. Nach dieser fügen wir eine weitere Zeile hinzu, die den aktualisierten Wert von AES-256-GCM enthält:
Fügen Sie direkt hinter dieser Zeile eine auth-Anweisung hinzu, um den Digestalgorithmus der HMAC-Nachricht auszuwählen.
Suchen Sie als Nächstes nach der Zeile mit der Anweisung dh, die Diffie-Hellman-Parameter definiert.
Da wir alle Zertifikate so konfiguriert haben, dass sie die Elliptische-Kurven-Kryptografie verwenden, ist eine Diffie-Hellman-Seed-Datei nicht erforderlich.
Kommentieren Sie die bestehende Zeile aus, die wie dh dh2048.pem oder dh dh dh.pem aussieht.
Der Dateiname für den Diffie-Hellman-Schlüssel kann anders sein als die in der Beispielkonfigurationsdatei des Servers.
Fügen Sie dann nach dieser eine Zeile mit dem Inhalt dh none hinzu:
Als Nächstes möchten wir, dass OpenVPN nach seinem Start ohne Berechtigungen läuft. Daher müssen wir ihm sagen, dass es mit einem Benutzer nobody und einer Gruppe nobody läuft.
Um dies zu aktivieren, finden Sie die Zeilen mit user nobody und group nogroup und heben Sie die Auskommentierung auf, indem Sie das; -Zeichen am Anfang jeder Zeile entfernen:
Mit den obigen Einstellungen wird die VPN-Verbindung zwischen Ihrem Client und Server erstellt, die Verbindungen werden allerdings nicht zur Nutzung des Tunnels gezwungen.
Wenn Sie das VPN verwenden möchten, um Ihren gesamten Client-Verkehr über das VPN zu leiten, sollten Sie wahrscheinlich einige zusätzlichen Einstellungen mithilfe von Push an die Client-Computer übertragen.
Finden Sie hierzu die Zeile mit push "redirect-gateway def1 bypass-dhcp" und kommentieren Sie sie aus.
Dadurch wird Ihr Client angewiesen, seinen gesamten Verkehr über Ihren OpenVPN-Server umzuleiten.
Beachten Sie, dass die Aktivierung dieser Funktionalität Verbindungsprobleme mit anderen Netzwerkdiensten wie SSH verursachen kann:
Direkt unter dieser Zeile finden Sie den Abschnitt dhcp-option.
Entfernen Sie erneut das "; "zu Beginn der beiden Zeilen, um sie auszukommentieren:
Diese Zeilen weisen Ihren Client am, die kostenlosen OpenDNS-Resolver unter den aufgelisteten IP-Adressen zu verwenden.
Wenn Sie andere DNS-Resolver bevorzugen, können Sie diese an Stelle der hervorgehobenen IPs einsetzen.
Dadurch werden die Clients bei der Neukonfigurierung ihrer DNS-Einstellungen unterstützt, damit der VPN-Tunnel als Standard-Gateway verwendet werden kann.
Um OpenVPN so zu ändern, dass es auf Port 443 lauscht, öffnen Sie die Datei server.conf und suchen Sie die Zeile, die wie folgt aussieht:
Bearbeiten Sie sie entsprechend, damit der Port 443 ist:
Finden Sie in diesem Fall die proto-Zeile unterhalb der port-Zeile und ändern Sie das Protokoll von udp zu tcp:
Wenn dies während der Verwendung von TCP nicht befolgt wird, treten beim Starten des OpenVPN-Dienstes Fehler auf.
Finden Sie die Zeile explicit-exit-notify am Ende der Datei und ändern Sie den Wert auf 0:
Wenn Sie keinen anderen Port und kein anderes Protokoll verwenden müssen, ist es am besten, diese Einstellungen in ihren Standardeinstellungen zu belassen.
Wenn Sie zuvor beim Befehl. / easyrsa gen-req server einen anderen Namen gewählt haben, ändern Sie die Zeilen cert und key in der Konfigurationsdatei server.conf, damit sie auf die entsprechenden Dateien .crt und .key verweisen.
Wenn Sie den Standardnamen, server, verwendet haben, ist dies bereits korrekt festgelegt:
Sie haben nun die Konfiguration der allgemeinen Einstellungen Ihres OpenVPN abgeschlossen.
Im nächsten Schritt passen wir die Netzwerkoptionen des Servers an.
Schritt 8 - Anpassen der Netzwerkkonfiguration des Open VPN-Servers
Es gibt einige Aspekte der Netzwerkkonfiguration des Servers, die optimiert werden müssen, damit OpenVPN den Verkehr korrekt durch das VPN leiten kann.
Um die standardmäßige IP-Weiterleitungseinstellung Ihres OpenVPN-Servers anzupassen, öffnen Sie die Datei / etc / sysctl.conf mit nano oder Ihrem bevorzugten Editor:
Fügen Sie dann am Ende der Datei folgende Zeile hinzu:
Um die Datei zu lesen und die Werte für die aktuelle Sitzung zu laden, geben Sie Folgendes ein:
Jetzt kann Ihr OpenVPN-Server eingehenden Datenverkehr von einem Ethernet-Gerät auf ein anderes weiterleiten.
Diese Einstellung stellt sicher, dass der Server den Datenverkehr von Clients, die sich über die virtuelle VPN-Schnittstelle verbinden, über seine anderen physischen Ethernet-Geräte hinausleiten kann.
Diese Konfiguration leitet den gesamten Webverkehr von Ihrem Client über die IP-Adresse Ihres Servers und die öffentliche IP-Adresse Ihres Clients wird effektiv verborgen.
Im nächsten Schritt müssen Sie einige Firewall-Regeln konfigurieren, um sicherzustellen, dass der Verkehr zu und von Ihrem OpenVPN-Server korrekt fließt.
Schritt 9 - Konfigurieren der Firewall
Bis hierher haben Sie OpenVPN auf Ihrem Server installiert, konfiguriert und die Schlüssel und Zertifikate generiert, die für Ihren Client für den Zugriff auf das VPN benötigt werden.
Sie haben OpenVPN jedoch noch keine Anweisungen gegeben, wohin eingehender Webverkehr von Clients gesendet werden soll.
Sie können festlegen, wie der Server mit dem Client-Verkehr umgehen soll, indem Sie einige Firewall-Regeln und Leitungskonfigurationen festlegen.
Wenn Sie den Voraussetzungen am Anfang dieses Tutorials gefolgt sind, sollte ufw bereits installiert sein und auf Ihrem Server laufen.
Um OpenVPN über die Firewall zuzulassen, müssen Sie Masquerading aktivieren, ein iptables-Konzept, das Network Address Translation (NAT) bei Bedarf bereitstellt, um Client-Verbindungen korrekt weiterzuleiten.
Ihre öffentliche Schnittstelle ist die in der Ausgabe dieses Befehls enthaltene Zeichenfolge, die dem Wort "dev" folgt.
Dieses Ergebnis zeigt beispielsweise die im Folgenden hervorgehobene Schnittstelle eth0 an:
Denken Sie daran, < ^ > eth0 < ^ > in der nachstehenden Zeile -A POSTROUTING durch die im obigen Befehl angegebene Schnittstelle zu ersetzen:
Wenn die Firewall-Regeln eingerichtet sind, können wir den OpenVPN-Dienst auf dem Server starten.
Schritt 10 - Starten des OpenVPN
OpenVPN läuft als systemd-Dienst, sodass wir systemctl verwenden können, um es zu verwalten. Wir konfigurieren OpenVPN, um beim Booten zu starten, sodass Sie sich jederzeit mit Ihrem VPN verbinden können, solange Ihr Server läuft.
Aktivieren Sie dazu den OpenVPN-Dienst, indem Sie ihn zu systemctl hinzufügen:
Starten Sie dann den OpenVPN-Dienst:
Überprüfen Sie nochmals mit dem folgenden Befehl, ob der OpenVPN-Dienst aktiv ist.
Sie sollten in der Ausgabe active (running) sehen:
Wir haben nun die serverseitige Konfiguration für OpenVPN abgeschlossen.
Als Nächstes konfigurieren Sie Ihren Client-Computer und verbinden sich mit dem OpenVPN-Server.
Schritt 11 - Erstellen der Client-Konfigurationsinfrastruktur
Öffnen Sie diese neue Datei mit nano oder Ihrem bevorzugten Texteditor:
Als Nächstes kommentieren Sie die Anweisungen user und group aus, indem Sie das; -Zeichen am Anfang jeder Zeile entfernen:
Kommentieren Sie in ähnlicher Weise die Anweisung tls-auth aus, da Sie ta.key direkt in die Client-Konfigurationsdatei einfügen werden (und der Server zur Nutzung von tls-crypt eingerichtet ist):
Verwenden Sie dieselben Einstellungen für cipher und auth, die Sie in der Datei / etc / openvpn / server / server.conf festgelegt haben:
Fügen Sie abschließend ein paar auskommentierte Zeilen hinzu, um verschiedene Methoden zu verwalten, die Linux-basierte VPN-Clients zur DNS-Auflösung verwenden.
Sie fügen zwei ähnliche, aber separate Sätze von auskommentierten Zeilen hinzu.
Das erste Satz ist für Clients bestimmt, die zur Verwaltung von DNS nicht systemd-resolved verwenden.
Diese Clients verlassen sich auf das Dienstprogramm resolvconf, um DNS-Informationen für Linux-Clients zu aktualisieren.
Fügen Sie jetzt ein weiteres Zeilensatz für Clients hinzu, die für DNS-Auflösung systemd-resolved verwenden:
Später in Schritt 13 - Installieren der Client-Konfiguration dieses Tutorials lernen Sie, wie DNS-Auflösung bei Linux-Clients funktioniert und für welchen Abschnitt die Auskommentierung aufgehoben werden muss.
Als Nächstes erstellen wir ein Skript, das Ihre Basiskonfiguration mit dem entsprechenden Zertifikat, Schlüssel und den Verschlüsselungsdateien kompiliert und platzieren die generierte Konfiguration dann in das Verzeichnis ~ / client-configs / files.
Der Vorteil besteht darin, dass Sie, falls Sie einmal einen Client hinzufügen müssen, einfach dieses Skript ausführen können, um rasch die neue Konfigurationsdatei zu erstellen. Dabei wird sichergestellt, dass alle wichtigen Informationen an einem einzigen, einfach zugänglichen Ort gespeichert werden.
Schritt 12 - Generieren von Client-Konfigurationen
Durch Befolgung der Anweisungen im Leitfaden haben Sie in Schritt 6 ein Client-Zertifikat und einen Schlüssel namens client1.crt bzw. client1.key erstellt. Sie können eine Konfigurationsdatei für diese Anmeldedaten generieren, indem Sie in das Verzeichnis ~ / client-configs wechseln und das am Ende des vorherigen Schrittes erstellte Skript ausführen:
Hier sehen Sie das Beispiel eines SFTP-Befehls, den Sie von Ihrem lokalen Computer (macOS oder Linux) ausführen können.
Damit wird die Datei < ^ > client1.ovpn < ^ > kopiert, die wir im letzten Schritt in Ihrem Stammverzeichnis erstellt haben:
Hier sind einige Tools und Tutorials für die sichere Übertragung von Dateien vom OpenVPN-Server auf einen lokalen Computer:
Schritt 13 - Installieren der Client-Konfiguration
< $> note Anmerkung: OpenVPN benötigt für die Installation Administratorrechte.
Beim Starten von OpenVPN lokalisiert es automatisch das Profil und stellt es zur Verfügung.
Konfigurieren von Clients, die systemd-resolved verwenden
Ermitteln Sie zuerst, ob Ihr System systemd-resolved verwendet, um DNS-Auflösung zu verwalten, indem Sie die Datei / etc / resolv.conf überprüfen:
Wenn Ihr System so konfiguriert ist, dass für DNS-Auflösung systemd-resolved verwendet wird, lautet die IP-Adresse nach der Option nameserver 127.0.0.53.
Es sollte in der Datei auch Kommentare geben (wie in der angezeigten Ausgabe), die erklären, wie systemd-resolved die Datei verwaltet.
Wenn Sie eine andere IP-Adresse haben als 127.0.0.53, dann nutzt Ihr System systemd-resolved wahrscheinlich nicht und Sie können mit dem nächsten Abschnitt zum Konfigurieren von Linux-Clients fortfahren, die stattdessen ein Skript update-resolv-conf haben.
Um diese Clients zu unterstützen, installieren Sie zuerst das Paket openvpn-systemd-resolved.
Es bietet Skripte, die systemd-resolved dazu zwingen, den VPN-Server für die DNS-Auflösung zu verwenden.
Nachdem das Paket installiert ist, konfigurieren Sie den Client so, dass er es verwendet und alle DNS-Abfragen über die VPN-Schnittstelle sendet.
Öffnen Sie die VPN-Datei des Clients:
Heben Sie jetzt die Auskommentierung der folgenden Zeilen, die Sie zuvor hinzugefügt haben, auf:
Konfigurieren von Clients, die update-resolv-conf verwenden
Wenn Ihr System nicht systemd-resolved verwendet, um DNS zu verwalten, überprüfen Sie, ob Ihre Distribution ein Skript / etc / openvpn / update-resolv-conf enthält:
Wenn Ihr Client die Datei update-resolv-conf enthält, bearbeiten Sie als Nächstes die OpenVPN-Client-Konfigurationsdatei, die Sie zuvor übertragen haben:
Heben Sie die Auskommentierung der drei Zeilen, die Sie hinzugefügt haben, auf, um die DNS-Einstellungen anzupassen:
* * Verbindung wird hergestellt * *
< $> note Anmerkung: Wenn Ihr Client systemd-resolved zum Verwalten von DNS verwendet, überprüfen Sie, ob die Einstellungen korrekt angewendet werden, indem Sie den Befehl systemd-resolve --status wie folgt ausführen:
Sie sollten eine Ausgabe wie die folgende sehen:
Wenn Sie die IP-Adressen der DNS-Server, die Sie auf dem OpenVPN-Server konfiguriert haben, zusammen mit der ~ .-Einstellung für DNS Domain in der Ausgabe sehen, dann haben Sie Ihren Client richtig so konfiguriert, dass er die DNS-Auflösung des VPN-Servers verwendet.
Sie können auch überprüfen, ob Sie DNS-Abfragen über das VPN senden, indem Sie eine Site wie DNS leak test.com verwenden.
Ziehen Sie die Datei .ovpn in das Fenster OpenVPN-Dokumente. ITunes zeigt das zum Laden bereite VPN-Profil auf dem iPhone
Die OpenVPN iOS-App zeigt ein neues Profil an, das zum Import bereit ist Verbindung wird hergestellt
< $> note Anmerkung: Der VPN-Switch unter Einstellungen kann nicht zur Verbindung mit dem VPN verwendet werden.
Starten Sie die OpenVPN-App und tippen Sie auf das Menü FILE, um das Profil zu importieren.
Navigieren Sie dann zum Speicherort des Profils (im Screenshot wird / storage / emulated / 0 / openvpn verwendet) und wählen Sie Ihre Datei .ovpn aus.
Tippen Sie auf die Schalttaste IMPORT, um das Importieren dieses Profils zu beenden.
Verbindung wird hergestellt Sobald das Profil hinzugefügt wurde, sehen Sie einen Bildschirm wie diesen:
Die OpenVPN-App für Android mit dem neu hinzugefügten Profil
Tippen Sie zum Verbinden auf die Umschalttaste in der Nähe des Profils, das Sie verwenden möchten.
Sie sehen Echtzeitdaten zu Ihrer Verbindung sowie zu dem über Ihren OpenVPN-Server geleiteten Datenverkehr: Die OpenVPN-App für Android, die mit dem VPN verbunden ist
Zum Trennen der Verbindung tippen Sie einfach erneut auf die Umschalttaste oben links.
Sie werden aufgefordert, zu bestätigen, dass Sie die Verbindung zu Ihrem VPN trennen möchten.
Schritt 14 - Testen Ihrer VPN-Verbindung (optional)
< $> note Anmerkung: Diese Methode zum Testen Ihrer VPN-Verbindung funktioniert nur dann, wenn Sie sich im Schritt 7 bei der Bearbeitung der Datei server.conf für OpenVPN dazu entschieden haben, den gesamten Verkehr über das VPN zu leiten.
Schritt 15 - Sperren von Client-Zertifikaten
Folgen Sie dazu dem Beispiel im Abschnitt Sperren eines Zertifikats im voraussetzenden Tutorial Erstellen und Konfigurieren einer Zertifizierungsstelle unter Ubuntu 20.04.
Sobald Sie ein Zertifikat für einen Client mit diesen Anweisungen widerrufen haben, müssen Sie die generierte Datei crl.pem in das Verzeichnis / etc / openvpn / server ihres OpenVPN-Servers kopieren:
Übertragen Sie die neue Datei crl.pem auf Ihren OpenVPN-Server und kopieren Sie sie in das Verzeichnis / etc / openvpn / server /, um die alte Liste zu überschreiben
Sie sollten nun über ein voll funktionsfähiges virtuelles privates Netzwerk verfügen, das auf Ihrem OpenVPN-Server läuft.
Sie können im Internet surfen und Inhalte herunterladen, ohne sich Sorgen machen zu müssen, dass böswillige Akteure Ihre Aktivitäten verfolgen.
Sie können noch verschiedene weitere Schritte ausführen, um Ihr OpenVPN anzupassen. Beispielsweise können Sie Ihren Client so konfigurieren, dass er sich automatisch mit dem VPN verbindet oder client-spezifische Regeln und Zugriffsrichtlinien konfigurieren.
Konsultieren Sie für diese und andere Anpassungen des OpenVPNs die offizielle OpenVPN-Dokumentation.
Um weitere Clients zu konfigurieren, müssen Sie nur den Schritten 6 und 11 - 13 für jedes zusätzliche Gerät folgen.
Um den Zugriff auf Clients zu sperren, führen Sie einfach Schritt 15 aus.
Verwenden des MySQL BLOB-Datentyps zum Speichern von Bildern mit PHP unter Ubuntu 20.04
5464
Der Autor hat Girls Who Code dazu ausgewählt, im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
Ein Binary Large Object (BLOB) ist ein MySQL-Datentyp, der Binärdaten wie Bild-, Multimedia- und PDF-Dateien speichern kann.
Bei der Erstellung von Anwendungen, die eine eng gekoppelte Datenbank erfordern, in der Bilder mit verwandten Daten synchronisiert sein sollen (z. B. ein Mitarbeiterportal, eine Studentendatenbank oder eine Finanzanwendung), finden Sie es ggf. praktisch, Bilder wie Passfotos und Unterschriften von Studenten in einer MySQL-Datenbank neben anderen Informationen zu speichern.
Hier kommt der Datentyp MySQL BLOB ins Spiel.
Dieser Programmieransatz beseitigt die Notwendigkeit, ein separates Dateisystem zum Speichern von Bildern einzurichten.
Außerdem zentralisiert das Schema die Datenbank, wodurch sie portabler ist und sicherer wird, da die Daten vom Dateisystem isoliert werden.
Das Erstellen von Backups erfolgt zudem nahtloser, da Sie eine einzelne MySQL Dump-Dateien erstellen können, die alle Ihre Daten enthält.
Das Abrufen von Daten ist schneller und beim Erstellen von Datensätzen können Sie sicher sein, dass Datenvalidierungsregeln und referenzielle Integrität beachtet werden - besonders bei der Verwendung von MySQL-Transaktionen.
In diesem Tutorial verwenden Sie den MySQL BLOB-Datentyp, um Bilder mit PHP unter Ubuntu 18.04 zu speichern.
Um diesem Leitfaden zu folgen, benötigen Sie Folgendes:
Einen Ubuntu 18.04-Server, der mit der Ersteinrichtung des Servers unter Ubuntu 18.04 und einem Nicht-root-Benutzer mit sudo-Berechtigungen konfiguriert wurde.
Apache, MySQL und PHP, die gemäß dem Leitfaden Installieren des Linux, Apache, MySQL, PHP (LAMP) -Stacks unter Ubuntu 18.04 eingerichtet wurden.
Für dieses Tutorial ist es nicht erforderlich, virtuelle Hosts zu erstellen, sodass Sie Schritt 4 überspringen können.
Schritt 1 - Erstellen einer Datenbank
Sie beginnen mit der Erstellung einer Beispieldatenbank für Ihr Projekt.
Dazu stellen Sie eine SSH-Verbindung zu Ihrem Server her und führen dann den folgenden Befehl aus, um sich bei Ihrem MySQL-Server als root anzumelden:
Geben Sie das root-Passwort Ihrer MySQL-Datenbank ein und drücken Sie ENTER, um fortzufahren.
Führen Sie dann den folgenden Befehl aus, um eine Datenbank zu erstellen.
In diesem Tutorial nennen wir sie test _ company:
Sobald die Datenbank erstellt wurde, sehen Sie die folgende Ausgabe:
Erstellen Sie als Nächstes auf dem MySQL-Server ein Konto namens test _ user und denken Sie daran, PASSWORD durch ein starkes Passwort zu ersetzen:
Um test _ user in der Datenbank test _ company volle Berechtigungen zu erteilen, führen Sie Folgendes aus:
Stellen Sie sicher, dass Sie die folgende Ausgabe erhalten:
Leeren Sie abschließend die Berechtigungstabelle, damit MySQL die Berechtigungen neu lädt:
Stellen Sie sicher, dass Sie die folgende Ausgabe sehen:
Nachdem die Datenbank test _ company und test _ user nun bereit sind, fahren Sie mit dem Erstellen einer Tabelle namens products fort, um dort Beispielprodukte zu speichern.
Sie verwenden diese Tabelle später, um Datensätze einzufügen und abzurufen und auszuprobieren, wie MySQL BLOB funktioniert.
Melden Sie sich dann erneut mit den Anmeldedaten des test _ user an, den Sie erstellt haben:
Geben Sie auf Aufforderung das Passwort für den test _ user ein und drücken Sie ENTER, um fortzufahren.
Wechseln Sie als Nächstes zur Datenbank test _ company, indem Sie Folgendes eingeben:
Sobald die Datenbank test _ company ausgewählt ist, zeigt MySQL Folgendes an:
Erstellen Sie als Nächstes eine Tabelle namens products, indem Sie Folgendes ausführen:
Dieser Befehl erstellt eine Tabelle namens products.
Die Tabelle hat vier Spalten:
product _ id: Diese Spalte verwendet einen BIGINT-Datentyp, um eine große Liste von Produkten mit bis zu 2 ⁶ ³ -1 Elementen aufzunehmen.
Sie haben die Spalte als PRIMARY KEY markiert, um Produkte eindeutig zu identifizieren.
Damit MySQL die Erzeugung neuer Kennungen für eingefügte Spalten verwaltet, haben Sie das Schlüsselwort AUTO _ INCREMENT verwendet.
product _ name: Diese Spalte enthält die Namen der Produkte.
Sie haben den Datentyp VARCHAR verwendet, da dieses Feld im Allgemeinen alphanumerische Zeichenfolgen mit bis zu 50 Zeichen verwaltet. Die Grenze von 50 ist nur ein hypothetischer Wert, der für den Zweck dieses Tutorials verwendet wird.
price: Für Demonstrationszwecke enthält Ihre Tabelle namens products eine Spalte namens price, in der der Einzelhandelspreis von Produkten gespeichert wird.
Da einige Produkte möglicherweise über veränderliche Werte verfügen (z. B. 23,69, 45,36, 102,99), haben Sie den Datentyp DOUBLE verwendet.
product _ image: Diese Spalte verwendet einen BLOB-Datentyp, um die tatsächlichen Binärdaten der Produktbilder zu speichern.
Sie haben die InnoDB-Speicher-ENGINE für die Tabelle verwendet, um eine breite Palette von Funktionen einschließlich MySQL-Transaktionen zu unterstützen.
Nach der Ausführung zum Erstellen der Tabelle products sehen Sie die folgende Ausgabe:
Melden Sie sich von Ihrem MySQL-Server ab:
Sie erhalten folgende Ausgabe:
Die Tabelle products ist jetzt bereit dazu, Datensätze zu speichern, einschließlich der Bilder von Produkten. Sie werden sie im nächsten Schritt mit einigen Produkten füllen.
Schritt 2 - Erstellen von PHP-Skripten zum Verbinden und Füllen der Datenbank
In diesem Schritt erstellen Sie ein PHP-Skript, das eine Verbindung mit der MySQL-Datenbank herstellt, die Sie in Schritt 1 erstellt haben. Das Skript bereitet drei Beispielprodukte vor und fügt sie in die Tabelle products ein.
Um den PHP-Code zu erstellen, öffnen Sie mit Ihrem Texteditor eine neue Datei:
Geben Sie dann die folgenden Daten in die Datei ein und ersetzen Sie < ^ > PASSWORD < ^ > durch das Passwort des test _ user, das Sie in Schritt 1 erstellt haben:
In dieser Datei haben Sie vier PHP-Konstanten verwendet, um sich mit der MySQL-Datenbank zu verbinden, die Sie in Schritt 1 erstellt haben:
DB _ NAME: Diese Konstante enthält den Namen der Datenbank test _ company.
DB _ USER: Diese Variable enthält den Benutzernamen des test _ user.
DB _ PASSWORD: Diese Konstante speichert das MySQL PASSWORD des Kontos test _ user.
DB _ HOST: Gibt den Server an, auf dem sich die Datenbank befindet.
In diesem Fall verwenden Sie den Server localhost.
Die folgende Zeile in Ihrer Datei initiiert ein PHP Data Object (PDO) und stellt eine Verbindung zur MySQL-Datenbank her:
Am Ende der Datei haben Sie einige PDO-Attribute festgelegt:
ATTR _ ERRMODE, PDO:: ERRMODE _ EXCEPTION: Dieses Attribut weist PDO an, eine Ausnahme auszulösen, die für Debugging-Zwecke protokolliert werden kann.
ATTR _ EMULATE _ PREPARES, false: Diese Option erhöht die Sicherheit, indem die MySQL-Datenbank-Engine dazu angewiesen wird, die Vorbereitung anstelle von PDO durchzuführen.
Sie schließen die Datei / var / www / html / config.php in zwei PHP-Skripte ein, die Sie als Nächstes zum Einfügen bzw. Abrufen von Daten erstellen werden.
Erstellen Sie zuerst das PHP-Skript / var / www / html / insert _ products.php zum Einfügen von Datensätzen in die Tabelle products:
Fügen Sie dann folgende Informationen in die Datei / var / www / html / insert _ products.php ein:
In der Datei haben Sie oben die Datei config.php aufgenommen.
Dies ist die erste Datei, die Sie zum Definieren der Datenbankvariablen und Verbinden mit der Datenbank erstellt haben.
Die Datei initiiert außerdem ein PDO-Objekt und speichert es in einer Variable $pdo.
Als Nächstes haben Sie ein Array mit Produktdaten erstellt, die in die Datenbank eingefügt werden sollen.
Neben dem product _ name und price, die als Zeichenfolgen bzw. numerische Werte vorbereitet sind, verwendet das Skript die integrierte file _ get _ contents-Funktion von PHP, um Bilder aus einer externen Quelle zu lesen und als Zeichenfolgen an die Spalte product _ image zu übergeben.
Als Nächstes haben Sie eine SQL-Anweisung vorbereitet und die PHP-Anweisung foreach {...} verwendet, um jedes Produkt in die Datenbank einzufügen.
Um die Datei / var / www / html / insert _ products.php auszuführen, führen Sie sie mit der folgenden URL in Ihrem Browserfenster aus.
Denken Sie daran, < ^ > your-server-IP < ^ > durch die öffentliche IP-Adresse Ihres Servers zu ersetzen:
Nach der Ausführung der Datei sehen Sie eine Erfolgsmeldung in Ihrem Browser, die bestätigt, dass Datensätze in die Datenbank eingefügt wurden.
Eine Erfolgsmeldung zeigt, dass Datensätze in Datenbank eingefügt wurden
Sie haben erfolgreich drei Datensätze mit Produktbildern in die Tabelle products eingefügt.
Im nächsten Schritt erstellen Sie ein PHP-Skript zum Abrufen und Anzeigen dieser Datensätze in Ihrem Browser.
Schritt 3 - Anzeigen von Produktinformationen aus der MySQL-Datenbank
Mit den Informationen und Bildern der Produkte in der Datenbank werden Sie jetzt ein anderes PHP-Skript codieren, das die Produktinformationen abfragt und in Ihrem Browser in einer HTML-Tabelle anzeigt.
Um die Datei zu erstellen, geben Sie Folgendes ein:
Geben Sie dann folgende Informationen in die Datei ein:
Speichern Sie die Änderungen in der Datei und schließen Sie sie.
Hier haben Sie erneut die Datei config.php hinzugefügt, um eine Verbindung zur Datenbank herzustellen.
Dann haben Sie mit PDO eine SQL-Anweisung vorbereitet und ausgeführt, um alle Elemente aus der Tabelle products mit dem Befehl SELECT * FROM products abzurufen.
Als Nächstes haben Sie eine HTML-Tabelle erstellt und unter Verwendung der PHP-Anweisung while () {...} gefüllt.
Die Zeile $row = $stmt- > fetch (PDO:: FETCH _ ASSOC) fragt die Datenbank ab und speichert das Ergebnis in der Variable $row als multidimensionales Array, das Sie dann mit der Syntax $row ['column _ name'] in einer HTML-Tabellenspalte angezeigt haben.
Die Bilder aus der Spalte product _ image sind in den < img src = "" > -Tags eingeschlossen.
Sie haben die Attribute width und height verwendet, um die Bilder in eine kleinere Größe zu ändern, die in die HTML-Tabellenspalte passt.
Um die im BLOB-Datentyp enthaltenen Daten wieder in Bilder zu konvertieren, haben Sie die native PHP-Funktion base64 _ encode und die folgende Syntax für das Data URI-Schema verwendet:
In diesem Fall ist image / png der media _ type und die Base64-codierte Zeichenfolge aus der Spalte product _ image ist base _ 64 _ encoded _ data.
Führen Sie als Nächstes die Datei display _ products.php in einem Webbrowser aus, indem Sie die folgende Adresse eingeben:
Nachdem die Datei display _ products.php in Ihrem Browser ausgeführt wurde, sehen Sie eine HTML-Tabelle mit einer Liste von Produkten und zugehörigen Bildern.
Liste von Produkten aus der MySQL-Datenbank
Dadurch wird bestätigt, dass das PHP-Skript zum Abrufen von Bildern aus MySQL wie erwartet funktioniert.
In diesem Leitfaden haben Sie den MySQL BLOB-Datentyp verwendet, um Bilder mit PHP unter Ubuntu 18.04 zu speichern.
Sie haben auch die grundlegenden Vorteile des Speicherns von Bildern in einer Datenbank anstelle eines Dateisystems kennengelernt.
Diese schließen Portabilität, Sicherheit und einfache Backups ein.
Wenn Sie eine Anwendung wie ein Studentenportal oder oder eine Mitarbeiterdatenbank erstellen, die verlangt, dass Informationen und zugehörige Bilder zusammen gespeichert werden, kann diese Technologie sehr nützlich sein.
Weitere Informationen zu den unterstützten Datentypen in MySQL finden Sie im Leitfaden zu MySQL-Datentypen.
Wenn Sie mehr über MySQL und PHP erfahren möchten, lesen Sie die folgenden Tutorials:
E-Book: Von Containern zu Kubernetes mit Node.js
5504
< $> note label Das komplette E-Book herunterladen!
Von Containern bis Kubernetes mit Node.js E-Book im EPUB-Format
Von Containern bis Kubernetes mit Node.js E-Book als PDF
E-Book-Vorwort
Dieses Buch dient als Einführung in die Nutzung von Containern und Kubernetes für die Full-Stack-Entwicklung.
Sie lernen, wie Sie eine Full-Stack-Anwendung mit Node.js und MongoDB entwickeln und verwalten - zuerst mit Docker, dann mit Docker Compose und schließlich mit Kubernetes.
Dieses Buch basiert auf der Tutorial-Reihe Von Containern bis Kubernetes mit Node.js, die Sie in der DigitalOcean Community finden.
Darin werden folgende Themen behandelt:
Erstellen einer Node.js-Anwendung mit Docker für die Entwicklung
Integrieren einer NoSQL-Datenbank in Ihre Node.js-Anwendung mit MongoDB
Verwalten Ihrer Entwicklungsumgebung mit Docker Compose
Migrieren Ihres Docker Compose-Workflows auf Kubernetes
Skalieren Ihrer Node.js- und MongoDB-Anwendung mit Helm und Kubernetes
Sichern Ihrer containerisierten Node.js-Anwendung mit Nginx, Let "s Encrypt und Docker Compose
Von Anfang an ist jedes Kapitel so konzipiert, dass es auf dem Vorherigem aufbaut.
Wenn Sie jedoch mit einem Thema bereits vertraut sind oder mehr Interesse an einem bestimmten Abschnitt haben, können Sie zu dem Kapitel springen, das das von Ihnen gewünschte Thema behandelt.
E-Book herunterladen
Sie können das E-Book im EPUB- oder PDF-Format unter den folgenden Links herunterladen.
Weitere Informationen über die App-Entwicklung mit Node.js erhalten Sie in der DigitalOcean Community im Abschnitt Node.js (https: / / www.digitalocean.com / community / tags / node-js).
Oder falls Sie mehr über Container, Docker und Kubernetes lernen möchten, interessieren Sie sich womöglich auch für den selbstständig durchführbaren Kurs Kubernetes für Full-Stack-Entwickler.
Installation von Java mit Apt auf Ubuntu 20.04
5503
Java und die JVM (Java Virtual Machine) werden für viele Arten von Software benötigt, einschließlich Tomcat, Jetty, Glassfish, Cassandra und Jenkins.
In diesem Leitfaden werden Sie verschiedene Versionen der Java Runtime Environment (JRE) und des Java Developer Kit (JDK) mit apt installieren.
Sie installieren OpenJDK sowie das offizielle JDK von Oracle.
Anschließend wählen Sie die Version aus, die Sie für Ihre Projekte verwenden möchten.
Wenn Sie fertig sind, können Sie mit JDK Software entwickeln oder mit Java Runtime Software ausführen.
Einen Ubuntu 20.04-Server, der gemäß des Leitfadens zur Ersteinrichtung eines Ubuntu 20.04 Servers eingerichtet wurde, einschließlich eines sudo-Nicht-root-Benutzers ohne Rootberechtigung und einer Firewall.
Installation von Standard-JRE / JDK
Der einfachste Weg der Installation von Java besteht darin, die im Ubuntu Paket enthaltene Version zu verwenden.
Ubuntu 20.04 enthält standardmäßig Open JDK 11, eine Open-Source-Variante der JRE und des JDK.
Aktualisieren Sie zuerst den Paketindex, um diese Version zu installieren:
Überprüfen Sie anschließend, ob Java bereits installiert ist:
Fall Java aktuell nicht installiert ist, so wird die folgende Ausgabe angezeigt:
Führen Sie den folgenden Befehl aus, um die standardmäßige Java-Laufzeitumgebung (JRE) zu installieren, die das JRE von OpenJDK 11 installiert:
Die JRE ermöglicht Ihnen, fast alle Java-Software auszuführen.
Überprüfen Sie die Installation mit:
Möglicherweise benötigen Sie das Java Development Kit (JDK) zusätzlich zur JRE, um bestimmte Java-basierte Software zu kompilieren und auszuführen.
Führen Sie zum Installieren des JDK den folgenden Befehl aus, mit dem auch die JRE installiert wird:
Überprüfen Sie, ob das JDK installiert ist, indem Sie die Version von javac, dem Java-Compiler, überprüfen:
Schauen wir uns nun an, wie Sie das offizielle JDK und JRE von Oracle installieren.
Installation von Oracle JDK 11
Die Lizenzvereinbarung von Oracle für Java erlaubt keine automatische Installation durch Paketmanager.
Um das Oracle JDK zu installieren, das die offizielle Version von Oracle ist, müssen Sie ein Oracle-Konto erstellen und das JDK manuell herunterladen, um ein neues Paket-Repository für die Version hinzuzufügen, die Sie verwenden möchten.
Dann können Sie apt verwenden, um es mithilfe eines Drittanbieter-Skripts zu installieren.
Die Version des JDK von Oracle, das Sie herunterladen sollen, muss der Skript-Version des Installationsprogramms entsprechen.
Um zu erfahren, welche Version Sie benötigen, gehen Sie auf die Seite oracle-java11-installer.
Suchen Sie das Paket für Focal, wie in der folgenden Abbildung dargestellt:
Installationspaket für Ubuntu 2.04
In diesem Bild sehen Sie die Version des Skripts 11.0.7.
In diesem Fall benötigen Sie Oracle JDK 11.0.7.
Von dieser Seite müssen Sie nichts herunterladen. Sie laden das Installationsskript in Kürze über apt herunter.
Besuchen Sie die Seite Downloads und suchen Sie die Version, die der gewünschten Variante entspricht.
Oracle Java 11
Klicken Sie auf die Schaltfläche JDK Download und Sie werden auf einen Bildschirm geführt, der die verfügbaren Versionen anzeigt.
Klicken Sie auf das Paket .tar.gz für Linux.
Linux-Download ​ ​ ​
Daraufhin wird Ihnen ein Bildschirm angezeigt, auf dem Sie aufgefordert werden, die Lizenzvereinbarung von Oracle zu akzeptieren.
Markieren Sie das Kontrollkästchen, um die Lizenzvereinbarung zu akzeptieren und klicken Sie auf die Schaltfläche Download.
Nun startet Ihr Download.
Möglicherweise müssen Sie sich noch einmal bei Ihrem Oracle-Konto anmelden, bevor der Download beginnt.
Sobald die Datei heruntergeladen wurde, müssen Sie sie auf Ihren Server übertragen.
Laden Sie die Datei von Ihrem lokalen Rechner auf Ihren Server hoch.
Verwenden Sie auf MacOS, Linux oder Windows mit dem Windows Subsystem für Linux den Befehl scp, um die Datei in das Stammverzeichnis Ihres Benutzers < ^ > sammy < ^ > zu übertragen.
Beim folgenden Befehl wird davon ausgegangen, dass Sie die Oracle JDK-Datei in den Download-Ordner Ihres lokalen Rechners gespeichert haben:
Sobald der Upload der Datei abgeschlossen ist, kehren Sie zu Ihrem Server zurück und fügen das Drittanbieter-Repository hinzu, das Sie bei der Installation der Oracle-Software Java unterstützt.
Installieren Sie das Paket software-properties-common, das Ihrem System den Befehl add-apt-repository hinzufügt:
Importieren Sie als Nächstes den Signierschlüssel, der zur Überprüfung der Software verwendet wird, die Sie installieren möchten:
Sie sehen diese Ausgabe:
Verwenden Sie dann den Befehl add-apt-repository, um das Repo zu Ihrer Paketquellenliste hinzuzufügen:
Sie sehen diese Meldung:
Drücken Sie ENTER, um die Installation fortzusetzen.
Eventuell sehen Sie eine Meldung, die besagt, dass keine gültigen OpenPGP-Daten gefunden wurden. Diese können Sie jedoch ignorieren.
Aktualisieren Sie Ihre Paketliste, um die neue Software für die Installation bereitzustellen:
Das Installationsprogramm sucht nach dem Oracle JDK, das Sie heruntergeladen haben, unter / var / cache / oracle-jdk11-installer-local ​ ​ ​ 1 ​ ​ ​.
Erstellen Sie dieses Verzeichnis und verschieben Sie das Oracle JDK dorthin:
Installieren Sie schließlich das Paket:
Das Installationsprogramm fordert Sie zunächst dazu auf, die Lizenzvereinbarung von Oracle zu akzeptieren.
Wenn Sie die Vereinbarung akzeptieren, wird das Installationsprogramm das Java-Paket extrahieren und installieren.
Schauen wir uns nun an, wie Sie auswählen, welche Java-Version Sie verwenden möchten.
Java-Management
Es können sich mehrere Java-Installationen auf einem Server befinden.
Mit dem Befehl update-alternatives können Sie konfigurieren, welche Version standardmäßig in der Befehlszeile verwendet wird.
So würde das Ergebnis aussehen, wenn Sie gemäß diesem Tutorial alle Java-Versionen installiert haben:
Wählen Sie die mit der Java-Version verknüpfte Zahl aus, um sie als Standard zu verwenden, oder drücken Sie die ENTER, um die aktuellen Einstellungen zu übernehmen.
Sie können dies für andere Java-Befehle tun, etwa für den Compiler (javac):
Weitere Befehle, für die dieser Befehl verwendet werden kann, sind unter anderem: keytool, javadoc und jarsigner.
Einstellen der Umgebungsvariablen JAVA _ HOME
Viele mit Java geschriebene Programme verwenden die Umgebungsvariable JAVA _ HOME, um den Java-Installationsort zu bestimmen.
Zur Einstellung der Umgebungsvariable müssen Sie zunächst bestimmen, wo Java installiert ist.
Verwenden Sie den Befehl update-alternatives:
Dieser Befehl zeigt jede Java-Installation sowie deren Installationspfad an:
In diesem Fall lauten die Installationspfade wie folgt:
OpenJDK 11 befindet sich unter / usr / lib / jvm / java-11-openjdk-amd64 / bin / java.
Oracle Java befindet sich unter / usr / lib / jvm / java-11-oracle / jre / bin / java.
Kopieren Sie den Pfad Ihrer bevorzugten Installation.
Öffnen Sie dann / etc / environment mit nano oder Ihrem bevorzugten Texteditor:
Fügen Sie am Ende dieser Datei die folgende Zeile hinzu und stellen Sie sicher, dass der hervorgehobene Pfad durch Ihren eigenen kopierten Pfad ersetzt wird, aber nicht den Teil bin / des Pfades beinhaltet:
Durch das Ändern dieser Datei wird der Pfad JAVA _ HOME für alle Benutzer Ihres Systems festgelegt.
Speichern Sie die Datei und beenden Sie den Editor.
Laden Sie nun diese Datei neu, um die Änderungen auf Ihre aktuelle Sitzung anzuwenden:
Überprüfen Sie, dass die Umgebungsvariable eingestellt ist:
Sie sehen den Pfad, den Sie gerade eingestellt haben:
Andere Benutzer müssen den Befehl source / etc / environment ausführen oder sich ab- und erneut anmelden, um diese Einstellung zu übernehmen.
In dieser Anleitung haben Sie mehrere Versionen von Java installiert und gelernt, wie man diese verwaltet.
Sie können jetzt Software installieren, die auf Java ausgeführt wird, etwa Tomcat, Jetty, Glassfish, Cassandra oder Jenkins.
Installieren von MariaDB unter Ubuntu 20.04 Quickstart
5568
Eine frühere Version dieses Tutorials wurde von Brian Boucheron geschrieben
MariaDB ist ein Open-Source-basiertes relationales Datenbank-Managementsystem, das häufig als Alternative für den MySQL-Datenbankteil des beliebten LAMP-Stacks (Linux, Apache, MySQL, PHP / Python / Perl) verwendet wird.
Dieses Quickstart-Tutorial beschreibt, wie Sie MariaDB auf einem Ubuntu 20.04-Server installieren und es mit einer sicheren Erstkonfiguration einrichten.
Es behandelt außerdem, wie Sie ein zusätzliches Administratorkonto für den Passwortzugriff einrichten.
Um diesem Tutorial folgen zu können, benötigen Sie einen Server, auf dem Ubuntu 20.04 ausgeführt wird.
Dieser Server sollte über einen administrativen non-root user und eine mit UFW konfigurierte Firewall verfügen.
Sie können dies einrichten, indem Sie unserem Leitfaden zur Ersteinrichtung des Servers für Ubuntu 20.04 folgen.
Bevor Sie MariaDB installieren, aktualisieren Sie mit apt den Paketindex auf Ihrem Server:
Bei Installation aus den Standard-Repositorys startet die Ausführung von MariaDB automatisch.
Führen Sie das Sicherheitsskript aus, das mit MariaDB installiert wurde.
Dieses Skript führt Sie durch eine Reihe von Aufforderungen, in denen Sie verschiedene Änderungen an den Sicherheitseinstellungen Ihrer MySQL-Installation vornehmen können:
Da wir noch kein Passwort eingerichtet haben, drücken Sie ENTER, um "none" (keines) anzugeben.
Damit werden einige anonyme Benutzer und die Testdatenbank entfernt, Remote-root-Logins deaktiviert und dann diese neuen Regeln geladen.
Schritt 3 - (Optional) Erstellen eines administrativen Benutzers, der Passwortauthentifizierung verwendet
Bei Ubuntu mit MariaDB 10.3 ist der root-MariaDB-Benutzer standardmäßig so eingerichtet, dass die Authentifizierung mit dem unix _ socket-Plugin und nicht mit einem Passwort vorgenommen wird.
Stattdessen empfehlen die Paketverwalter die Erstellung eines separaten Administratorkontos für passwortbasierten Zugriff.
Öffnen Sie dazu die MariaDB-Eingabeaufforderung in Ihrem Terminal:
Erstellen Sie dann einen neuen Benutzer mit root-Berechtigungen und passwortbasiertem Zugriff.
Achten Sie darauf, den Benutzernamen und das Passwort entsprechend Ihren Präferenzen zu ändern:
Sie können diesen neuen Benutzer mit dem Tool mysqladmin testen. Das ist ein Client, der Sie administrative Befehle ausführen lässt.
Der folgende mysqladmin-Befehl stellt eine Verbindung zu MariaDB als admin-Benutzer her und gibt die Versionsnummer aus, nachdem der Benutzer zur Eingabe seines Passworts aufgefordert wurde:
Sie erhalten eine ähnliche Ausgabe wie diese:
In diesem Leitfaden haben Sie das relationale Datenbankmanagementsystem MariaDB installiert und es mit dem Skript mysql _ secure _ installation gesichert, das mit dem System installiert wird.
Sie hatten die Möglichkeit, einen neuen administrativen Benutzer zu erstellen, der die Passwortauthentifizierung verwendet.
Importieren und Exportieren von Datenbanken
Ausführen von SQL-Anfragen
Einbinden von MariaDB in einen größeren Anwendungs-Stack
Installieren von Node.js unter Ubuntu 20.04
5508
Node.js ist eine JavaScript-Laufzeitumgebung für serverseitige Programmierung.
Sie ermöglicht Entwicklern die Erstellung von skalierbaren Backend-Funktionen mit JavaScript, einer Sprache, die viele aus der Browser-basierten Webentwicklung bereits kennen.
In diesem Leitfaden stellen wir Ihnen drei verschiedene Möglichkeiten vor, um Node.js auf einem Ubuntu 20.04-Server zu installieren:
Verwendung von apt zum Installieren des nodejs-Pakets aus Ubuntus Standard-Software-Repository
Verwendung von apt mit einem alternativen PPA-Software-Repository zur Installation bestimmter Versionen des nodejs-Pakets
Installation von nvm, dem Node Version Manager, und dessen Verwendung zum Installieren und Verwalten von verschiedenen Node.js-Versionen
Für viele Benutzer ist die Verwendung von apt mit dem Standard-Repo ausreichend.
Wenn Sie bestimmte neuere (oder ältere) Versionen von Node benötigen, sollten Sie das PPA-Repository verwenden.
Wenn Sie aktiv Node-Anwendungen entwickeln und häufig zwischen node-Versionen wechseln müssen, wählen Sie die nvm-Methode.
Dieser Leitfaden geht davon aus, dass Sie Ubuntu 20.04 verwenden.
Bevor Sie beginnen, sollten Sie das Benutzerkonto eines non-root users mit sudo-Privilegien auf Ihrem System eingerichtet haben.
Dazu können Sie sich den Leitfaden zur Ersteinrichtung des Servers unter Ubuntu 20.04 ansehen.
Option 1 - Installieren von Node.js mit Apt aus den Standard-Repositorys
Ubuntu 20.04 enthält eine Version von Node.js in seinen Standard-Repositorys, die verwendet werden kann, um eine konsistente Erfahrung über mehrere Systeme hinweg zu gewährleisten.
Zum Zeitpunkt des Verfassens dieses Artikels handelt es sich dabei um die Version 10.19. der Repositorys.
Das ist wahrscheinlich nicht die neueste Version, aber sollte stabil und ausreichend sein, um kurze Experimente mit der Sprache zu ermöglichen.
Um diese Version zu erhalten, können Sie den apt-Paketmanager verwenden.
Aktualisieren Sie Ihren lokalen Paketindex durch die Eingabe von:
Installieren Sie anschließend Node.js:
Überprüfen Sie, ob die Installation erfolgreich war, indem Sie node nach seiner Versionsnummer abfragen:
Wenn das Paket in den Repositorys Ihren Bedürfnissen entspricht, müssen Sie nichts weiter tun, um Node.js einzurichten.
In den meisten Fällen sollten Sie auch den Node.js-Paketmanager npm installieren.
Sie können dies tun, indem Sie das npm-Paket mit apt installieren:
Damit können Sie Module und Pakete zur Verwendung mit Node.js installieren.
Nun haben Sie Node.js und npm erfolgreich mit apt und den Standard-Software-Repositorys von Ubuntu installiert.
Im nächsten Abschnitt wird gezeigt, wie Sie ein alternatives Repository zum Installieren verschiedener Versionen von Node.js verwenden.
Option 2 - Installieren von Node.js mit Apt unter Verwendung des NodeSource PPA
Um eine andere Version von Node.js zu installieren, können Sie ein PPA (persönliches Paketarchiv) verwenden, das von NodeSource unterhalten wird.
Diese PPAs verfügen über mehr Versionen von Node.js als die offiziellen Ubuntu-Repositorys.
Node.js v10, v12, v13, und v14 sind zum Zeitpunkt des Verfassens dieses Artikels verfügbar.
Zuerst installieren wir das PPA, um Zugriff auf seine Pakete zu erhalten.
Verwenden Sie curl aus Ihrem Stammverzeichnis, um das Installationsskript für Ihre bevorzugte Version abzurufen, und achten Sie darauf, < ^ > 14.x < ^ > durch Ihre bevorzugte Versionszeichenfolge zu ersetzen (sofern sich diese unterscheidet).
Konsultieren Sie die NodeSource-Dokumentation für weitere Informationen zu den verfügbaren Versionen.
Überprüfen Sie den Inhalt des heruntergeladenen Skripts mit nano (oder Ihrem bevorzugten Texteditor):
Wenn Sie sich davon überzeugt haben, dass das Skript sicher ausgeführt werden kann, verlassen Sie Ihren Editor und führen Sie das Skript mit sudo aus:
Das PPA wird Ihrer Konfiguration hinzugefügt und Ihr lokaler Paket-Cache automatisch aktualisiert.
Sie können das Node.js-Paket jetzt in der gleichen Weise installieren, wie Sie es im vorherigen Abschnitt getan haben:
Überprüfen Sie, ob die neue Version installiert wurde, indem Sie node mit dem Versions-Flag -v ausführen:
Das nodejs-Paket von NodeSource enthält sowohl die node-Binary als auch npm, sodass Sie npm nicht getrennt installieren müssen.
Nun haben Sie Node.js und npm erfolgreich mit apt und dem NodeSource-PPA installiert.
Im nächsten Abschnitt wird gezeigt, wie Sie den Node Version Manager zum Installieren und Verwalten von mehreren Node.js-Versionen verwenden.
Option 3 - Installieren von Node mit dem Node Version Manager
Eine weitere Möglichkeit zur Installation von Node.js, die besonders flexibel ist, ist die Verwendung von nvm, dem Node Version Manager.
Mit dieser Software können Sie verschiedene unabhängige Versionen von Node.js und ihre zugehörigen Node-Pakete auf einmal installieren und verwalten.
Um NVM auf Ihrem Ubuntu 20.04-Computer zu installieren, besuchen Sie die GitHub-Seite des Projekts.
Kopieren Sie den Befehl curl aus der README-Datei, die auf der Hauptseite angezeigt wird.
Dadurch erhalten Sie die neueste Version des Installationsskripts.
Bevor Sie den Befehl an bash weiterleiten, ist es immer eine gute Idee, das Skript zu prüfen, um sicherzustellen, dass es nichts tut, was es nicht tun sollte.
Entfernen Sie dazu das Segment | bash am Ende des Befehls curl:
Sehen Sie nach und vergewissern Sie sich, dass Sie mit den vorgenommenen Änderungen einverstanden sind.
Wenn Sie zufrieden sind, führen Sie den Befehl mit | bash am Ende angehängt erneut aus.
Die von Ihnen verwendete URL ändert sich je nach der neuesten Version von NVM; ab jetzt aber kann das Skript durch folgende Eingabe heruntergeladen und ausgeführt werden:
Dadurch wird das nvm-Skript in Ihrem Benutzerkonto installiert.
Um es zu verwenden, müssen Sie zunächst Ihre .bashrc-Datei bereitstellen:
Jetzt können Sie NVM fragen, welche Versionen von Node verfügbar sind:
Das ist eine sehr lange Liste!
Sie können eine Version von Node installieren, indem Sie eine der freigegebenen Versionen eingeben.
Um zum Beispiel Version v13.6.0 zu erhalten, können Sie Folgendes eingeben:
Sie können die verschiedenen Versionen sehen, die Sie installiert haben, indem Sie Folgendes eingeben:
Dadurch wird die gerade aktive Version in der ersten Zeile angezeigt (- > v13.6.0), gefolgt von einigen benannten Aliassen und den Versionen, auf die diese Aliasse verweisen.
< $> note Hinweis: Wenn Sie auch eine Version von Node.js über apt installiert haben, sehen Sie hier gegebenenfalls einen system-Eintrag.
Sie können mit nvm use system immer die systeminstallierte Version von Node aktivieren.
Zusätzlich sehen Sie Aliasse für die verschiedenen Langzeitsupport- (oder LTS) -Versionen von Node:
Wir können auch eine auf diesen Aliassen basierende Version installieren.
Um beispielsweise die neueste Langzeitsupport-Version erbium zu installieren, führen Sie Folgendes aus:
Sie können mit nvm use zwischen installierten Versionen wechseln:
Sie können überprüfen, ob die Installation erfolgreich war, indem Sie das gleiche Verfahren aus den anderen Abschnitten anwenden und Folgendes eingeben:
Die korrekte Version von Node wurde wie erwartet auf unserem Computer installiert.
Es ist auch eine kompatible Version von npm verfügbar.
Sie haben verschiedene Möglichkeiten, mit Node.js die Arbeit auf Ihrem Ubuntu 20.04-Server aufzunehmen.
Ihre Umstände bestimmen, welche der obigen Methoden am besten zu Ihnen passen.
Obwohl die Paketversion in Ubuntus Repository die einfachste Methode ist, bieten nvm oder ein NodeSource-PPA zusätzliche Flexibilität.
Weitere Informationen zum Programmieren mit Node.js finden Sie in unserer Tutorial-Reihe Codieren in Node.js.
Aktualisieren auf Ubuntu 20.04 Focal Fossa
5569
Die neueste Langzeitsupport-Version (LTS) des Betriebssystems Ubuntu, Ubuntu 20.04 (Focal Fossa), wurde am 23. April 2020. veröffentlicht.
Dieser Leitfaden erklärt, wie Sie ein Ubuntu-System der Version 18.04 oder höher auf Ubuntu 20.04 aktualisieren.
< $> warning Warnung: Wie bei fast jeder Aktualisierung zwischen Hauptversionen eines Betriebssystems besteht bei diesem Prozess das inhärente Risiko eines Ausfalls, eines Datenverlusts oder einer fehlerhaften Softwarekonfiguration.
Umfassende Datensicherung und umfangreiche Tests werden dringend empfohlen.
Um diese Probleme zu vermeiden, empfehlen wir, auf einen neuen Ubuntu 20.04-Server zu migrieren, anstatt eine Aktualisierung an Ort und Stelle durchzuführen.
Möglicherweise müssen Sie beim Aktualisieren noch Unterschiede in der Softwarekonfiguration überprüfen, aber das Kernsystem wird wahrscheinlich eine größere Stabilität aufweisen.
Sie können unsere Serie zur Migration auf einen neuen Linux-Server verfolgen, um zu erfahren, wie Sie zwischen Servern migrieren.
Dieser Leitfaden geht davon aus, dass Sie Ubuntu 18.04 oder ein höheres System mit einem sudo-fähigen Nicht-root Benutzer konfiguriert haben.
Potenzielle Fallstricke
Viele Systeme können an Ort und Stelle ohne Zwischenfälle aktualisiert werden. Jedoch ist es bei der Migration auf eine neue Hauptversion oft sicherer und berechenbarer, die Distribution von Grund auf neu zu installieren, die Dienste dabei mit sorgfältigen Tests zu konfigurieren und die Anwendungs- oder Benutzerdaten in einem separaten Schritt zu migrieren.
Aktualisieren Sie ein Produktionssystem niemals, ohne zuerst die Software und die Dienste, die von Ihnen bereitgestellt wurden, hinsichtlich der Aktualisierung in einer Staging-Umgebung zu testen.
Beachten Sie, dass sich Bibliotheken, Sprachen und Systemdienste möglicherweise erheblich geändert haben.
Lesen Sie hierzu vor der Aktualisierung am besten die Versionshinweise zu Focal Fossa.
Schritt 1 - Sichern der Systemdaten
Vor einer wesentlichen Aktualisierung jedes Systems sollten Sie sicherstellen, dass Sie keine Daten verlieren, falls die Aktualisierung fehlschlägt.
Die beste Möglichkeit hierzu ist eine Datensicherung Ihres gesamten Dateisystems.
Falls dies nicht möglich ist, stellen Sie sicher, dass Sie über Kopien von Benutzerstammverzeichnissen, allen benutzerdefinierten Konfigurationsdateien und Daten verfügen, die von Diensten wie relationalen Datenbanken gespeichert werden.
In einem DigitalOcean Droplet besteht ein Ansatz darin, das System herunterzufahren und einen Snapshot zu erstellen (durch das Herunterfahren wird das Dateisystem konsistenter).
Weitere Informationen zum Snapshot-Prozess finden Sie in Erstellen von Snapshots in Droplets.
Nachdem Sie geprüft haben, dass die Ubuntu-Aktualisierung erfolgreich war, können Sie den Snapshot löschen, damit Ihnen seine Speicherung nicht länger in Rechnung gestellt wird.
Weitere Datenspeicherungsmethoden, die auf den meisten Ubuntu-Systemen funktionieren, finden Sie in Auswählen einer effektiven Datensicherungsstrategie für Ihren VPS.
Schritt 2 - Aktualisieren derzeitig installierter Pakete
Vor der Aktualisierung der Version ist es am sichersten, alle Pakete der derzeitigen Version auf deren neueste Versionen zu aktualisieren.
Aktualisieren Sie zuerst die Paketliste:
Aktualisieren Sie als Nächstes installierte Pakete auf ihre neuesten verfügbaren Versionen:
Ihnen wird eine Liste der Aktualisierungen angezeigt und Sie werden dazu aufgefordert, fortzufahren.
Geben Sie y für ja ein und drücken Sie ENTER.
Dieser Prozess kann einige Zeit in Anspruch nehmen.
Nach dessen Beendung verwenden Sie den Befehl dist-upgrade mit apt-get. Dadurch werden alle zusätzlichen Aktualisierungen ausgeführt, bei denen bei Bedarf Abhängigkeiten geändert sowie neue Pakete hinzugefügt oder entfernt werden.
Hierdurch werden eine Reihe von Aktualisierungen gehandhabt, die möglicherweise im vorherigen Schritt mit apt upgrade zurückgehalten wurden:
Geben Sie bei der Eingabeaufforderung zum Fortfahren erneut y ein und warten Sie, bis die Aktualisierungen abgeschlossen sind.
Nachdem Sie nun über eine aktuelle Installation von Ubuntu verfügen, können Sie do-release-upgrade zum Aktualisieren auf die Version 20.04 verwenden.
Schritt 3 - Aktualisieren mit dem Ubuntu-Tool do-release-upgrade
Traditionell waren Ubuntu-Veröffentlichungen aktualisierbar, indem die / etc / apt / sources.list von Apt - die die Paket-Repositorys angibt - geändert und apt-get dist-upgrade verwendet wurde, um das Upgrade selbst durchzuführen.
Obwohl dieser Prozess wahrscheinlich immer noch funktioniert, bietet Ubuntu ein Tool namens do-release-upgrade, um das Aktualisieren sicherer und einfacher zu machen.
do-release-upgrade handhabt die Suche nach einer neuen Version, die Aktualisierung der sources.list sowie eine Reihe anderer Aufgaben und ist der offiziell empfohlene Aktualisierungspfad, der über eine Remote-Verbindung ausgeführt werden muss.
Beginnen Sie mit der Ausführung von do-release-upgrade ohne Optionen:
Wenn die neue Version von Ubuntu noch nicht offiziell veröffentlicht wurde, erhalten Sie möglicherweise die folgende Ausgabe:
Beachten Sie, dass auf Ubuntu Server die neue LTS-Version erst mit der ersten Punktversion, in diesem Fall 20.04.1, für ein do-release-upgrade zur Verfügung gestellt wird.
Dies erfolgt in der Regel einige Monate nach dem Datum der Erstveröffentlichung.
Wenn Sie keine verfügbare Version sehen, fügen Sie die Option -d hinzu, um auf die Version development zu aktualisieren:
Wenn Sie mit Ihrem System über SSH verbunden sind, werden Sie gefragt, ob Sie fortfahren möchten.
Bei virtuellen Rechnern oder verwalteten Servern sollten Sie bedenken, dass der Verlust der SSH-Konnektivität ein Risiko darstellt, insbesondere wenn Sie keine andere Möglichkeit haben, eine Remote-Verbindung zur Konsole des Systems herzustellen (wie beispielsweise eine webbasierte Konsolenfunktion).
Bei anderen Systemen unter Ihrer Kontrolle sollten Sie bedenken, dass es am sichersten ist, größere Betriebssystemaktualisierungen nur dann durchzuführen, wenn Sie direkten physischen Zugriff auf den Rechner haben.
Geben Sie der Eingabeaufforderung y ein und drücken Sie ENTER, um fortzufahren:
Als Nächstes werden Sie informiert, dass do-release-upgrade eine neue Instanz von sshd auf Port 1022 startet:
Drücken Sie Enter.
Anschließend erhalten Sie möglicherweise eine Warnung, dass kein Spiegeleintrag gefunden wurde.
Auf DigitalOcean-Systemen ist es sicher, diese Warnung zu ignorieren und mit der Aktualisierung fortzufahren, da ein lokaler Spiegel für 20.04 in Wirklichkeit verfügbar ist.
Geben Sie y ein:
Sobald die neuen Paketlisten heruntergeladen und Änderungen berechnet wurden, werden Sie gefragt, ob Sie die Aktualisierung starten möchten.
Geben Sie erneut y ein, um fortzufahren:
Nun werden neue Pakete abgerufen, entpackt und installiert.
Auch wenn Ihr System über eine schnelle Verbindung verfügt, dauert das eine Weile.
Während der Installation werden Ihnen möglicherweise interaktive Dialoge für verschiedene Fragen präsentiert.
Zum Beispiel könnten Sie gefragt werden, ob Sie bei Bedarf die Dienste automatisch neu starten möchten:
Dialog für den Neustart des Dienstes
In diesem Fall ist es sicher, mit Yes zu antworten.
In anderen Fällen könnten Sie gefragt werden, ob Sie eine Konfigurationsdatei ersetzen möchten, die Sie geändert haben.
Dies ist oft eine Ermessensentscheidung und erfordert wahrscheinlich Kenntnisse über spezifische Software, die außerhalb des Rahmens dieses Tutorials liegen.
Sobald die Installierung neuer Pakete abgeschlossen ist, werden Sie gefragt, ob Sie bereit sind, obsolete Pakete zu entfernen.
Bei einem Lagersystem ohne benutzerdefinierte Konfiguration sollte es sicher sein, hier y einzugeben.
Haben Sie das System stark geändert, sollten Sie möglicherweise d eingeben und die Liste der zu entfernenden Pakete für den Fall einsehen, dass sie etwas enthält, das Sie später neu installieren müssen.
Unter der Annahme, dass alles gut verlaufen ist, werden Sie abschließend informiert, dass die Aktualisierung abgeschlossen wurde und ein Neustart erforderlich ist.
Geben Sie y ein, um fortzufahren:
In einer SSH-Sitzung sehen Sie wahrscheinlich etwa Folgendes:
Möglicherweise müssen Sie hier eine Taste drücken, um zu Ihrer lokalen Eingabeaufforderung zu gelangen, da Ihre SSH-Sitzung auf der Server-Seite beendet ist.
Warten Sie einen Moment, bis Ihr Server neu gestartet ist, und verbinden Sie sich dann erneut.
Bei der Anmeldung sollten Sie eine Meldung empfangen werden, die bestätigt, dass Sie jetzt auf Focal Fossa sind:
Sie sollten nun eine funktionierende Installation von Ubuntu 20.04 haben.
Ab hier müssen Sie wahrscheinlich notwendige Konfigurationsänderungen an Diensten und bereitgestellten Anwendungen untersuchen.
Weitere 20.04-Tutorials und -Fragen finden Sie auf unserer Registerseite der Tutorials zu Ubuntu 20.04.
eBook "Codieren in Go"
5839
Codieren in Go eBook im EPUB-Format
Codieren in Go eBook im PDF-Format < $>
Dieses Buch soll Sie mit dem Schreiben von Programmen mit der Programmiersprache Go vertraut machen.
Sie werden erfahren, wie Sie nützliche Tools und Anwendungen schreiben können, die sich auf Remoteservern oder in lokalen Windows-, macOS- und Linux-Systemen für die Entwicklung ausführen lassen.
Dieses Buch basiert auf der Tutorial-Reihe Codieren in Go, die Sie in der DigitalOcean Community finden können.
Installieren und Einrichten einer lokalen Go-Entwicklungsumgebung in Windows-, MacOS- und Linux-Systemen
Gestalten Ihrer Programme mit bedingter Logik, einschließlich Switch-Anweisungen zur Steuerung des Programmflusses
Definieren eigener Datenstrukturen und Erstellen von Schnittstellen zu ihnen für wiederverwendbaren Code
Schreiben benutzerdefinierter Fehlerbehandlungsfunktionen
Erstellen und Installieren Ihrer Go-Programme, damit diese in verschiedenen Betriebssystemen und CPU-Architekturen ausgeführt werden können
Verwenden von Flags zur Übergabe von Argumenten an Ihre Programme, um Standardoptionen zu überschreiben
Jedes Kapitel kann für sich allein gelesen oder als Referenz verwendet werden; alternativ können Sie die Kapitel von Anfang bis Ende lesen.
Wechseln Sie nach Belieben zwischen Kapiteln, die für Ihre aktuelle Aufgabe relevant sind, während Sie Go mit diesem Buch erlernen.
Nachdem Sie das Buch gelesen haben, können Sie mehr über die Erstellung von Tools und Anwendungen mit Go erfahren. Besuchen Sie dazu den Bereich Go der DigitalOcean Community (https: / / www.digitalocean.com / community / tags / go).
Bereitstellen von Laravel 7 und MySQL in Kubernetes mithilfe von Helm
6029
Der Autor hat den Diversity in Tech Fund dazu ausgewählt, eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
Laravel ist heute eines der beliebtesten Open-Source-basierten PHP-Anwendungsframeworks.
Es wird oft mit einer MySQL-Datenbank bereitgestellt, kann aber so konfiguriert werden, dass verschiedene Backend-Datenspeicheroptionen zum Einsatz kommen.
Laravel ist stolz darauf, viele moderne Funktionen und das umfangreiche Paketökosystem von PHP zu nutzen.
Kubernetes ist eine Plattform zur Orchestrierung von Containern, die in DigitalOcean Kubernetes-Clustern gehostet werden kann, um einen Großteil der Verwaltungsaufgaben bei der Einrichtung und Ausführung von Containern in der Produktion zu übernehmen.
Helm ist ein Kubernetes-Paketmanager, der das Konfigurieren und Installieren von Diensten und Pods vereinfacht.
In diesem Leitfaden erstellen Sie eine Laravel PHP-Anwendung, erstellen Ihre App in einem Docker-Image und stellen das Image mithilfe des LAMP Helm Chart in einem DigitalOcean Kubernetes-Cluster bereit.
Als Nächstes richten Sie einen Ingress Controller ein, um Ihrer App SSL und einen benutzerdefinierten Domänennamen hinzuzufügen.Danach verfügen Sie eine funktionierende Laravel-Anwendung, die mit einer MySQL-Datenbank verbunden ist, die in einem Kubernetes-Cluster ausgeführt wird.
Docker, installiert auf dem Computer, von dem aus Sie auf Ihren Cluster zugreifen werden.
Detaillierte Anweisungen zum Installieren von Docker für die meisten Linux-Distributionen finden Sie hier oder für andere Betriebssysteme auf der Website von Docker.
Ein Konto bei Docker Hub zur Speicherung von Docker-Images, die Sie in diesem Tutorial erstellen werden.
Einen DigitalOcean Kubernetes 1.17 + -Cluster, bei dem Ihre Verbindung als der kubectl-Standard konfiguriert ist.
Um zu erfahren, wie Sie einen Kubernetes-Cluster in DigitalOcean erstellen können, lesen Sie unser Dokument Kubernetes Schnellstart.
Um zu erfahren, wie Sie eine Verbindung zum Cluster herstellen können, konsultieren Sie Herstellen einer Verbindung zu einem DigitalOcean Kubernetes-Cluster.
Helm 3-Paketmanager, auf Ihrem lokalen Rechner installiert.
Führen Sie den ersten Schritt aus und fügen Sie das stable-Repository aus dem zweiten Schritt des Tutorials Installieren von Software in Kubernetes-Clustern mit dem Helm 3-Paketmanager hinzu.
Einen vollständig registrierten Domänennamen mit einem verfügbaren A-Eintrag.
Sie müssen sich erst einmal keine Gedanken um eine Verknüpfung des A-Eintrags Ihrer Domäne mit einer IP-Adresse machen.
Sobald Sie Schritt 5 erreichen und Ihr Ingress Controller verfügbar ist, werden Sie < ^ > your _ domain < ^ > mit der richtigen IP-Adresse verbinden.
Schritt 1 - Erstellen einer neuen Laravel-Anwendung
In diesem Schritt verwenden Sie Docker, um eine neue Laravel-7-Anwendung zu erstellen. Sie sollten jedoch mit einer bestehenden Laravel-Anwendung, die MySQL als Backing-Datenbank nutzt, das gleiche Verfahren nutzen können.
Die neu erstellte Anwendung wird überprüfen, ob Laravel mit der Datenbank verbunden ist, und den Namen der Datenbank anzeigen.
Wechseln Sie zunächst in Ihr Stammverzeichnis und erstellen Sie dann eine neue Laravel-Anwendung mit einem Docker-Container vom Typ composer:
Nach der Fertigstellung des Containers und der Installation aller Composer-Pakete sollten Sie eine neue Installation von Laravel in Ihrem aktuellen Verzeichnis namens laravel-kubernetes / sehen.
Navigieren Sie zu diesem Ordner:
Von hier führen Sie die restlichen Befehle dieses Tutorials aus.
Zweck dieser Anwendung ist es, Ihre Datenbankverbindung zu testen und den Namen der Datenbank in Ihrem Browser anzuzeigen.
Öffnen Sie die Datei. / resources / views / welcome.blade.php in einem Texteditor, um die Datenbankverbindung zu testen:
Suchen Sie nach dem Abschnitt < div class = "links" >... < / div > und ersetzen Sie den Inhalt durch Folgendes:
Weitere Anpassungen müssen Sie in diesem Tutorial an der standardmäßigen Laravel-Anwendung nicht vornehmen.
Nun wird dieser kurze PHP-Abschnitt Ihre Datenbankverbindung testen und den Namen der Datenbank im Laravel-Begrüßungsbildschirm in Ihrem Webbrowser anzeigen.
Im nächsten Schritt verwenden Sie Docker, um ein Image zu erstellen, das diese Laravel-Anwendung und Docker Compose enthält, um zu testen, ob sie lokal ausgeführt wird und eine Verbindung zu einer MySQL-Datenbank hergestellt wird.
Schritt 2 & mdash; Containerisieren Ihrer Laravel-Anwendung
Nachdem Sie eine neue Laravel-Anwendung erzeugt haben, müssen Sie nun Ihren Code in ein Docker-Image integrieren und das Image dann mit Docker Compose testen.
Zwar ist das Ziel dieses Tutorials, Ihre Anwendung in einem Kubernetes-Cluster bereitzustellen, doch ist Docker Compose eine praktische Option, um Ihr Docker-Image und Ihre Konfiguration vor Ort zu testen, bevor Sie sie in der Cloud ausführen.
Die schnelle Feedbackschleife kann nützlich sein, um kleine Änderungen vorzunehmen und zu testen.
Erstellen Sie zunächst mit nano oder Ihrem bevorzugten Texteditor im Stammverzeichnis Ihrer Laravel-Anwendung eine Datei namens Dockerfile:
Docker wird diese Datei verwenden, um Ihren Code in ein Image zu integrieren:
Diese Dockerfile-Datei startet mit dem in Docker Hub gefundenen PHP 7.4 Apache Docker-Image und installiert dann mehrere Linux-Pakete, die allgemein von Laravel-Anwendungen benötigt werden.
Als Nächstes erstellt sie Apache-Konfigurationsdateien und ermöglicht das Umschreiben von Headern.
Die Dockerfile-Datei installiert mehrere gängige PHP-Erweiterungen und fügt eine Umgebungsvariable hinzu, um sicherzustellen, dass die Protokolle von Laravel über stderr an den Container gestreamt werden.
So können Sie Laravel-Protokolle sehen, indem Sie Ihre Docker Compose- oder Kubernetes-Protokolle durchsehen.
Schließlich kopiert die Dockerfile-Datei den gesamten Code in Ihrer Laravel-Anwendung nach / var / www / tmp und installiert die Abhängigkeiten von Composer.
Dann setzt sie einen ENTRYPOINT. Sie müssen diese Datei aber noch erstellen, was wir als Nächstes tun werden.
Erstellen Sie im Stammverzeichnis Ihres Projekts eine neue Datei namens docker-entrypoint.sh.
Diese Datei wird ausgeführt, wenn Ihr Container lokal oder im Kubernetes Cluster ausgeführt wird. Außerdem wird Ihr Laravel-Anwendungscode vom Verzeichnis / var / www / tmp in / var / www / html kopiert, wo Apache ihn bereitstellen kann.
Fügen Sie nun folgendes Skript hinzu:
Die abschließende Zeile exec "$@" weist das Shell an, jeden Befehl auszuführen, der als Nächstes als Eingabeargumenttext übergeben wurde.
Dies ist wichtig, da Docker nach Ausführung dieses Skripts den Apache run-Befehl (apache2-foreground) weiter ausführen soll.
Erstellen Sie als Nächstes im Stammverzeichnis Ihrer Anwendung eine Datei namens .dockerignore.
Diese Datei sorgt dafür, dass Ihr Docker-Image beim Erstellen nicht mit Paketen oder Umgebungsdateien verschmutzt wird, die nicht hinein kopiert werden sollen:
Die letzte Datei, die Sie erstellen müssen, bevor Sie Ihre Anwendung mit Docker Compose lokal ausführen können, ist eine docker-compose.yml-Datei.
Bei der Konfiguration dieser YAML-Datei müssen Sie jedoch den APP _ KEY eingeben, den Laravel bei der Installation generiert hat.
Um ihn zu finden, öffnen und durchsuchen Sie die Datei.
/ .env oder führen die Sie die folgenden Befehle cat und grep aus:
Sie werden eine Ausgabe wie diese sehen:
Kopieren Sie Ihren Schlüssel in die Zwischenablage.
Vergewissern Sie sich, dass Sie das Präfix base64: einschließen.
Erstellen Sie nun im Stammverzeichnis Ihrer Anwendung die Datei namens docker-compose.yml:
Hier werden wir das PHP-Image Ihrer Laravel-Anwendung sowie einen MySQL-Container für die Ausführung Ihrer Datenbank einschließen.
Fügen Sie den folgenden Inhalt hinzu:
Verwenden Sie die Variable APP _ KEY, die Sie in Ihre Zwischenablage kopiert haben, für die Variable < ^ > your _ laravel _ app _ key < ^ > und Ihren Docker Hub-Benutzernamen für die Variable < ^ > your _ docker _ hub _ username < ^ >.
Sie werden das erste Image lokal mit docker build erstellen.
Das zweite Image ist das offizielle MySQL Docker-Image, das in Docker Hub verfügbar ist.
Beide benötigen verschiedene Umgebungsvariablen, die Sie bei Ausführung der Container einschließen werden.
Um das Docker-Image mit Ihrer Laravel-Anwendung zu erstellen, führen Sie folgenden Befehl aus.
Ersetzen Sie < ^ > your _ docker _ hub _ username < ^ > durch Ihren Benutzernamen oder den Benutzernamen Ihres Teams bei Docker Hub, wo dieses Image gespeichert werden soll:
Als Nächstes können Sie die beiden Container unter Verwendung von Docker Compose mit den erforderlichen Datenbankanmeldedaten ausführen:
Die hier verwendeten vier Umgebungsvariablen (DB _ ROOT _ PASSWORD, DB _ DATABASE, DB _ USERNAME, DB _ PASSWORD) können bei Bedarf geändert werden; da Sie Ihre Anwendung jedoch nur vor Ort testen, müssen Sie sich noch nicht um ihre Sicherheit kümmern.
Es kann bis zu 30 Sekunden dauern, bis Ihre MySQL-Datenbank initialisiert ist und die Container einsatzbereit sind.
Sobald das der Fall ist, können Sie Ihre Laravel-Anwendung auf Ihrem Computer unter localhost: 8000 anzeigen.
Die Laravel-Anwendung, die mit Docker Compose lokal ausgeführt wird
Ihre PHP-Anwendung wird sich mit Ihrer MySQL-Datenbank verbinden.
Nach erfolgreicher Verbindungsherstellung wird unter dem Laravel-Logo der Text "Database Connected: local _ db" (Datenbank verbunden: local _ db) angezeigt.
Nachdem Sie Ihr Docker-Image mit Docker Compose lokal getestet haben, können Sie die Container mit docker-compose down nun herunterfahren:
Im nächsten Abschnitt pushen Sie Ihr Docker-Image an Docker Hub, damit es Ihr Helm Chart nutzen kann, um die Anwendung in Ihrem Kubernetes-Cluster bereitzustellen.
Schritt 3 & mdash; Pushen Ihres Docker-Image an Docker Hub
Das LAMP Helm Chart, das Sie zur Bereitstellung Ihres Codes an Kubernetes verwenden werden, erfordert, dass Ihr Code in einer Container-Registry verfügbar ist.
Zwar können Sie Ihr Image in eine private oder selbst gehostete Registry pushen, doch verwenden Sie in diesem Tutorial eine öffentlich verfügbare und kostenlose Docker-Registry in Docker Hub.
Greifen Sie mit Ihrem Webbrowser auf Ihr Konto in Docker Hub zu und erstellen Sie dann ein neues Repository namens laravel-kubernetes.
Erstellen eines neuen Repository in Docker Hub
Wenn Sie von Ihrem lokalen Computer noch keine Verbindung zu Docker Hub hergestellt haben, müssen Sie sich bei Docker Hub anmelden.
Sie können dies über die Befehlszeile tun:
Geben Sie Ihre Anmeldedaten ein, wenn Sie dazu aufgefordert werden.
Dies muss normalerweise nur einmal pro Computer erfolgen, da Docker Ihre Anmeldedaten in Ihrem Stammverzeichnis in ~ / .docker / config.json speichert.
Abschließend pushen Sie Ihr Image an Docker Hub:
Je nach Verbindungsgeschwindigkeit kann es einige Minuten dauern, bis Ihre Anwendung hochgeladen ist. Sobald Docker fertig ist, sehen Sie ein endgültiges Digest-Hash und die Größe Ihres Images im Terminal.
Nachdem Sie Ihre Laravel-Anwendung containerisiert und ein Image an Docker Hub gepusht haben, können Sie das Image nun in einer Helm Chart- oder Kubernetes-Bereitstellung verwenden.
Im nächsten Schritt werden Sie basierend auf dem LAMP Helm Chart benutzerdefinierte Werte festlegen und die Anwendung in Ihrem DigitalOcean Kubernetes-Cluster bereitstellen.
Schritt 4 & mdash; Konfigurieren und Bereitstellen der Anwendung mit dem LAMP Helm Chart
Helm bietet eine Reihe von Charts, um Ihnen mit voreingestellten Kombinationen von Tools bei der Einrichtung von Kubernetes-Anwendungen zu helfen.
Sie können zwar eigene Kubernetes-Dienstdateien schreiben, um eine eine ähnliche Bereitstellung zu erhalten, doch werden Sie in diesem Bereich sehen, warum die Verwendung eines Helm Chart die Konfiguration deutlich erleichtert.
Zuerst benötigen Sie ein Verzeichnis, in dem alle Ihre Helm-Konfigurationsdateien gespeichert werden.
Erstellen Sie im Stammverzeichnis Ihres Laravel-Projekts ein neues Verzeichnis namens helm /:
Im Verzeichnis helm / werden Sie zwei neue Dateien erstellen: values.yml und secrets.yml.
Erstellen und öffnen Sie zunächst values.yml:
Die Datei values.yml enthält nicht-geheime Konfigurationsoptionen, die die Standardwerte im LAMP Helm Chart überschreiben werden.
Fügen Sie folgende Konfigurationen hinzu und stellen Sie sicher, < ^ > your _ docker _ hub _ username < ^ > durch Ihren eigenen Benutzernamen zu ersetzen:
Erstellen Sie nun eine Datei namens secrets.yml:
secrets.yml wird nicht in der Versionskontrolle geprüft.
Sie enthält sensible Konfigurationsdaten wie Ihr Datenbankpasswort und den Laravel-App Key.
Fügen Sie die folgenden Konfigurationen hinzu und nehmen Sie gegebenenfalls Anpassungen für Ihre Anmeldedaten vor:
Verwenden Sie für Ihre Produktionsdatenbank starke Benutzername- und Passwortkombinationen und nutzen Sie den gleichen < ^ > your _ laravel _ app _ key < ^ > wie oben; öffnen Sie alternativ ein neues Terminalfenster und generieren Sie einen neuen App Key, indem Sie folgenden Befehl ausführen.
Dann können Sie den neuen Wert, den Laravel festlegt, in Ihre .env-Datei kopieren:
Speichern und schließen Sie secrets.yml.
Um zu verhindern, dass Ihre secrets.yml-Datei in das Docker-Image integriert oder in der Versionskontrolle gespeichert wird, sollten Sie als Nächstes sowohl der Datei .dockerignore als auch der Datei .gitignore die folgende Zeile hinzufügen.
Öffnen und fügen Sie jeder Datei / helm / secrets.yml an oder führen Sie den folgenden Befehl aus, um beide hinzuzufügen:
Nachdem Sie Helm-Konfigurationsdateien für Ihre Anwendung und das Docker-Image erstellt haben, können Sie dieses Helm Chart nun als neue Version in Ihrem Kubernetes-Cluster installieren.
Installieren Sie Ihr Chart im Stammverzeichnis Ihrer Anwendung:
Ihre Anwendung wird eine oder zwei Minuten brauchen, bis sie verfügbar ist. Sie können jedoch folgenden Befehl ausführen, um die Kubernetes-Dienste in Ihrem Cluster zu überwachen:
Suchen Sie nach dem Namen Ihrer Anwendung:
Wenn Ihr neuer Dienst laravel-kubernetes-lamp unter EXTERNAL-IP eine IP-Adresse anzeigt, können Sie < ^ > your _ external _ ip < ^ > aufrufen, um die in Ihrem Kubernetes-Cluster ausgeführte Anwendung anzuzeigen.
Ihre Anwendung wird sich mit Ihrer Datenbank verbinden und Sie werden den Namen der Datenbank unterhalb des Laravel-Logos sehen (genauso wie bei der lokalen Ausführung Ihrer Anwendung in Docker Compose).
Die Laravel-Anwendung, die in Kubernetes mit dem LAMP Helm Chart ausgeführt wird
Das Ausführen einer Webanwendung an einer ungesicherten IP-Adresse kann für einen Konzeptnachweis in Ordnung sein, Ihre Website ist jedoch ohne SSL-Zertifikat und einen benutzerdefinierten Domänennamen nicht bereit für die Produktion.
Bevor Sie dies im nächsten Schritt einrichten, deinstallieren Sie Ihre Version über die Befehlszeile:
Im nächsten Schritt werden Sie auf Grundlage dieser ersten Helm-Konfiguration Ihrer Laravel-Anwendung einen Ingress Controller, ein SSL-Zertifikat und eine benutzerdefinierte Domäne hinzuzufügen.
Schritt 5 & mdash; Hinzufügen von Ingress Controller und SSL zu Ihrem Kubernetes Cluster
Ein Ingress Controller ist in Kubernetes dafür verantwortlich, die Dienste Ihrer Anwendung im Internet zu verfügbar zu machen.
Im vorherigen Schritt hat das LAMP Helm Chart einen DigitalOcean Load Balancer erstellt und Ihre Anwendung direkt über die IP-Adresse des Load Balancer verfügbar gemacht.
Sie könnten SSL und Ihren Domänennamen direkt im Load Balancer terminieren; da Sie jedoch in Kubernetes arbeiten, kann es praktischer sein, alles an einem Ort zu verwalten.
Deutlich ausführlichere Informationen zu Ingress Controllern und Details zu den folgenden Schritten finden Sie unter Verwenden eines Nginx Ingress in DigitalOcean Kubernetes mit Helm.
Das LAMP Helm Chart enthält eine Konfigurationsoption zur Unterstützung von Ingress.
Öffnen Sie die Datei helm / values.yml:
Fügen Sie jetzt die folgenden Zeilen hinzu:
Dadurch wird Ihre Bereitstellung angewiesen, keinen Load Balancer zu installieren und die Anwendung stattdessen an Port 80 des Kubernetes-Clusters verfügbar zu machen, wo der Ingress Controller sie im Internet verfügbar macht.
Speichern und schließen Sie values.yml.
Führen Sie nun den Befehl helm install aus, den Sie zuvor ausgeführt haben, damit Ihre Laravel-Anwendung wieder ausgeführt wird.
Stellen Sie sicher, dass Sie den Befehl aus dem Stammverzeichnis Ihrer Anwendung ausführen:
Installieren Sie als Nächstes den Controller nginx-ingress in Ihrem Kubernetes-Cluster mit dem von Kubernetes verwalteten Nginx Ingress Controller:
Nach der Installation werden Sie eine Ausgabe wie diese sehen:
Außerdem benötigen Sie eine Ingress-Ressource, um die Bereitstellung Ihrer Laravel-Anwendung verfügbar zu machen.
Erstellen Sie im Stammverzeichnis Ihrer Anwendung eine neue Datei namens ingress.yml:
Diese Datei legt den Host, den SSL-Zertifikatmanager und den Backend-Dienst und Port-Namen der Anwendung fest.
Fügen Sie die folgenden Konfigurationen hinzu, wobei Sie < ^ > your _ domain < ^ > durch die Domäne Ihrer Wahl ersetzen:
Als Nächstes sollten Sie Cert-Manager installieren und einen Aussteller erstellen, mit dem Sie mit Let 's Encrypt SSL-Zertifikate für die Produktion erzeugen können.
Cert-Manager erfordert benutzerdefinierte Ressourcendefinitionen, die Sie aus dem Cert-Manager-Repository über die Befehlszeile anwenden können:
Dadurch wird eine Reihe von Kubernetes-Ressourcen erstellt, die in der Befehlszeile angezeigt werden:
Außerdem benötigt Cert-Manager einen Namespace zur Isolation in Ihrem Kubernetes-Cluster:
Sie sehen diese Ausgabe:
Da Cert-Manager von Jetstack keines der von Kubernetes verwalteten Charts ist, müssen Sie auch das Jetstack Helm-Repository hinzufügen.
Führen Sie folgenden Befehl aus, um es in Helm verfügbar zu machen:
Bei erfolgreicher Ergänzung erhalten Sie folgende Ausgabe:
Jetzt können Sie Cert-Manager in Ihrem Kubernetes-Cluster im Namespace cert-manager installieren:
Nach Abschluss sehen Sie eine Zusammenfassung der Bereitstellung, die in etwa wie folgt aussieht:
Die letzte Datei, die Sie dem Stammverzeichnis Ihrer Laravel-Anwendung hinzufügen müssen, ist eine Kubernetes-Konfigurationsdatei namens production _ issuer.yml.
Erstellen Sie die Datei:
Fügen Sie nun Folgendes hinzu:
Let 's Encrypt sendet an < ^ > your _ email _ address < ^ > sämtliche wichtigen Hinweise und Warnungen zum Ablauf von Zertifikaten; darum sollten Sie eine Adresse hinzufügen, die Sie regelmäßig prüfen.
Speichern Sie diese Datei und erstellen Sie eine neue Ressource sowohl für Ihre Ingress-Ressource als auch den Produktionsaussteller in Ihrem Kubernetes-Cluster:
Aktualisieren Sie schließlich die DNS-Einträge Ihres Domänennamens so, dass ein A-Eintrag auf die IP-Adresse Ihres Load Balancer verweist.
Um nach der IP-Adresse Ihres Ingress Controller zu suchen, geben Sie Folgendes ein:
Verwenden Sie die Adresse < ^ > your _ external _ ip < ^ > als IP-Adresse für Ihren DNS A-Eintrag.
Das Verfahren zur Aktualisierung Ihrer DNS-Einträge variiert, je nachdem wo Sie Ihre Domänennamen und das DNS-Hosting verwalten. Wenn Sie DigitalOcean verwenden, können Sie jedoch unseren Leitfaden zum Verwalten von DNS-Einträgen konsultieren.
Sobald Ihre DNS-Einträge aktualisiert und das SSL-Zertifikat generiert wurden, wird Ihre Anwendung in < ^ > your _ domain < ^ > verfügbar und SSL aktiviert.
Die Laravel-Anwendung mit SSL-Terminierung und einem benutzerdefinierten Domänennamen
Zwar sind Ihre PHP-Anwendung und Ihre Datenbank bereits miteinander verbunden, doch müssen Sie noch Datenbankmigrationen ausführen.
Im letzten Schritt erfahren Sie, wie Sie Artisan-Befehle in Ihrem Kubernetes-Pod ausführen können, um Datenbankmigrationen und andere häufige Wartungsaufgaben durchzuführen.
Schritt 6 & mdash; Ausführen von Remotebefehlen
Zwar wird Ihre Laravel-Anwendung ausgeführt und ist mit der MySQL-Datenbank in Kubernetes verbunden, doch gibt es mehrere gängige Aufgaben, die Sie in einer neuen Laravel-Installation erledigen sollten.
Eine gängige Aufgabe, die Sie ausführen sollten, sind Datenbankmigrationen.
Bevor Sie in Ihrer Laravel-Anwendung einen Artisan-Befehl ausführen können, müssen Sie den Namen des Pods kennen, das Ihren Laravel-Anwendungscontainer ausführt.
Mit der Befehlszeile können Sie alle Pods in Ihrem Kubernetes-Cluster anzeigen:
Wählen Sie das Pod für Ihre laravel-kubernetes-lamp-... -Bereitstellung aus.
Stellen Sie sicher, dass Sie den Namen in Ihrer Ausgabe verwenden und nicht den oben aufgeführten Namen.
Jetzt können Sie kubectl exec dafür ausführen. Beispielsweise führen Sie eine Datenbankmigration mit dem Befehl artisan migrate aus.
Sie fügen das Flag --force hinzu, da Sie das Pod in der Produktion ausführen:
Dieser Befehl wird eine Ausgabe erzeugen:
Sie haben Laravel 7 und MySQL nun erfolgreich in Kubernetes bereitgestellt und eine wichtige Aufgabe zur Datenbankwartung durchgeführt.
In diesem Tutorial haben Sie gelernt, wie Sie eine Laravel PHP-Anwendung containerisieren, mit einer MySQL-Datenbank verbinden, ein Docker-Image mit Ihrem Code an Docker Hub pushen und dann ein Helm Chart nutzen, um das Image in einem DigitalOcean Kubernetes-Cluster bereitzustellen.
Schließlich haben Sie SSL und einen benutzerdefinierten Domänennamen hinzugefügt und erfahren, wie Sie in Ihren laufenden Pods Befehlszeilentools ausführen.
Kubernetes and Helm bieten Ihnen eine Reihe von Vorteilen gegenüber dem herkömmlichen LAMP-Stack-Hosting: Skalierbarkeit, die Fähigkeit, Dienste ohne direkte Anmeldung bei Ihrem Server auszutauschen, Tools zur Durchführung von rollierenden Upgrades und Kontrolle über Ihre Hostingumgebung.
Es muss jedoch gesagt werden, dass die Komplexität der anfänglichen Containerisierung und Konfiguration Ihrer Anwendung am Anfang eine relativ hohe Barriere darstellt.
Mit diesem Leitfaden als Ausgangspunkt wird die Bereitstellung von Laravel in Kubernetes jedoch verständlicher.
Vielleicht wollen Sie nun mehr über die Vorzüge von Laravel oder das Hinzufügen von Überwachungstools (wie Linkerd) zu Kubernetes zu erfahren (manuell installierbar mit unserem Leitfaden oder mit einem DigitalOcean 1-Click).
Erstellen eines Slackbot in Python unter Ubuntu 20.04
6041
Slack ist eine Kommunikationsplattform, die für die Produktivität am Arbeitsplatz ausgelegt ist.
Es enthält Funktionen wie Direktnachrichten, öffentliche und private Kanäle, Sprach- und Videoanrufe und Bot-Integrationen.
Ein Slackbot ist ein automatisiertes Programm, das eine Vielzahl von Funktionen in Slack ausführen kann, von dem Senden von Nachrichten über das Auslösen von Aufgaben bis hin zu Warnungen bei bestimmten Ereignissen.
In diesem Tutorial erstellen Sie in der Programmiersprache Python einen Slackbot.
Python ist eine beliebte Sprache, die sich durch Einfachheit und Lesbarkeit auszeichnet.
Slack bietet eine reichhaltige Python Slack API zur Integration mit Slack, um allgemeine Aufgaben wie das Senden von Nachrichten, das Hinzufügen von Emojis zu Nachrichten und vieles mehr durchzuführen.
Slack bietet auch eine Python Slack Ereignisse API zur Integration mit Ereignissen in Slack, sodass Sie Aktionen für Ereignisse wie Nachrichten und Erwähnungen ausführen können.
Als spaßigen Proof-of-Concept, der die Leistungsfähigkeit von Python und seinen Slack-Apis demonstriert, erstellen Sie einen CoinBot & mdash; einen Slackbot, der einen Kanal überwacht und, wenn er ausgelöst wird, eine Münze für Sie wirft.
Sie können Ihren CoinBot dann so modifizieren, dass er eine beliebige Anzahl von etwas praktischeren Anwendungen erfüllt.
Beachten Sie, dass dieses Tutorial Python 3 verwendet und nicht mit Python 2 kompatibel ist.
Um diesen Leitfaden auszuführen, benötigen Sie:
Einen Slack Workspace, in dem Sie Anwendungen installieren können.
Wenn Sie den Arbeitsbereich erstellt haben, haben Sie diese Fähigkeit.
Wenn Sie nicht bereits über einen solchen verfügen, können Sie einen auf der Slack Website erstellen.
(Optional) Einen Server oder einen Computer mit einer öffentlichen IP-Adresse für die Entwicklung.
Wir empfehlen eine Neuinstallation von Ubuntu 20.04, einen Nicht-Root-Benutzer mit Sudo-Berechtigungen und aktiviertem SSH.
< $> note Sie möchten dieses Tutorial möglicherweise auf einem Server testen, der eine öffentliche IP-Adresse hat.
Slack muss in der Lage sein, Ereignisse wie Nachrichten an Ihren Bot zu senden. Wenn Sie auf einem lokalen Rechner testen, müssen Sie Datenverkehr über Ihre Firewall auf Ihr lokales System portieren.
Wenn Sie nach einer Möglichkeit suchen, auf einem Cloud-Server zu entwickeln, sehen Sie sich dieses Tutorial zur Verwendung des Visual Studio Code für die Remote-Entwicklung über das Remote-SSH-Plugin an.
Schritt 1 & mdash; Erstellen des Slackbots in der Slack-Benutzeroberfläche
Erstellen Sie zunächst Ihre Slack Anwendung in dem Slack API Bedienfeld.
Melden Sie sich über einen Webbrowser bei Ihrem Arbeitsbereich in Slack an und navigieren Sie zum API Bedienfeld.
Klicken Sie nun auf die Schaltfläche Create an App.
Erstellen Sie Ihre Slack-Anwendung
Als Nächstes werden Sie aufgefordert, den Namen Ihrer Anwendung einzugeben und einen Slack-Arbeitsbereich für die Entwicklung auszuwählen.
Benennen Sie für dieses Tutorial Ihre Anwendung < ^ > CoinBot < ^ > und wählen Sie einen Arbeitsbereich, auf den Sie Admin-Zugriff haben.
Sobald Sie dies getan haben, klicken Sie auf die Schaltfläche Create App.
Benennen Sie Ihre Slack-App und wählen Sie einen Arbeitsbereich aus
Sobald Ihre Anwendung erstellt ist, wird Ihnen das folgende Standard-Dashboard der Anwendung angezeigt.
In diesem Dashboard verwalten Sie Anwendung, indem Sie Berechtigungen festlegen, Ereignisse abonnieren, die Anwendung in Arbeitsbereichen installieren, und vieles mehr.
Standard Slack-Anwendungsbereich
Damit Ihre Anwendung Nachrichten an einen Kanal senden kann, müssen Sie der Anwendung Berechtigungen zum Senden von Nachrichten erteilen.
Klicken Sie dazu auf die Schaltfläche Permissions im Bedienfeld.
Wählen Sie die Schaltfläche "Permissions" im Bedienfeld
Wenn Sie auf der Seite OAuth & Permissions ankommen, scrollen Sie nach unten, bis Sie den Abschnitt Scopes der Seite finden.
Suchen Sie dann in dem Bereich den Unterabschnitt Bot Token Scopes und klicken Sie auf die Schaltfläche Add an OAuth Scope.
Wählen Sie die Schaltfläche "Add OAuth Scope"
Klicken Sie auf diese Schaltfläche und geben Sie dann chat: write ein.
Wählen Sie diese Berechtigung aus, um sie zu Ihrem Bot hinzuzufügen. Dadurch kann die Anwendung Nachrichten an Kanäle senden, auf die sie zugreifen kann.
Weitere Informationen zu den verfügbaren Berechtigungen finden Sie in der Dokumentation von Slack.
Fügen Sie die Berechtigung chat: write hinzu
Nachdem Sie die entsprechende Berechtigung hinzugefügt haben, ist es an der Zeit, Ihre Anwendung in Ihren Slack-Arbeitsbereich zu installieren.
Scrollen Sie auf der Seite OAuth & Permissions nach oben und klicken Sie oben auf die Schaltfläche Install App to Workspace.
Installieren der Anwendung in den Arbeitsbereich
Klicken Sie auf diese Schaltfläche und überprüfen Sie die Aktionen, die die Anwendung im Kanal ausführen kann.
Sobald Sie zufrieden sind, klicken Sie auf die Schaltfläche Allow, um die Installation zu beenden.
Sobald der Bot installiert ist, erhalten Sie ein Bot User OAuth Access Token für Ihre Anwendung, das Sie verwenden können, wenn Sie das Ausführen von Aktionen im Arbeitsbereich versuchen möchten.
Kopieren Sie dieses Token, da Sie es später benötigen.
Speichern Sie das Access Token
Fügen Sie Ihren neu installierten Bot abschließend in einen Kanal in Ihren Arbeitsbereich hinzu.
Wenn Sie noch keinen Kanal erstellt haben, können Sie den Kanal # general verwenden, der standardmäßig in Ihrem Slack-Arbeitsbereich erstellt wird.
Suchen Sie die Anwendung im Abschnitt Apps der Navigationsleiste in Ihrem Slack-Client und klicken Sie darauf. Öffnen Sie anschließend das Menü Details oben rechts.
Wenn Ihr Slack-Client nicht vollständig angezeigt wird, sieht er wie ein i in einem Kreis aus.
Klicken Sie auf das Detailsymbol der Anwendung
Um das Hinzufügen Ihrer Anwendung zu einem Kanal abzuschließen, klicken Sie auf die Schaltfläche More, die durch drei Punkte auf der Detailseite dargestellt wird, und wählen Sie Add this app to a channel....
Geben Sie Ihren Kanal in das angezeigte Modal ein und klicken Sie auf Add.
Fügen Sie die Anwendung einem Kanal hinzu
Sie haben Ihre Anwendung nun erfolgreich erstellt und sie zu einem Kanal in Ihrem Slack-Arbeitsbereich hinzugefügt.
Nachdem Sie den Code für Ihre Anwendung geschrieben haben, kann sie Nachrichten in diesem Kanal posten.
Im nächsten Abschnitt beginnen Sie das Schreiben des Python-Codes, der CoinBot betreiben wird.
Schritt 2 & mdash; Einrichten Ihrer Python Entwicklerumgebung
Erstellen Sie zunächst Ihre Python-Umgebung, damit Sie den Slackbot entwickeln können.
Öffnen Sie ein Terminal und installieren Sie python3 und die entsprechenden Tools auf Ihrem System:
Als Nächstes erstellen Sie eine virtuelle Umgebung, um Ihre Python-Pakete von der Systeminstallation von Python zu isolieren.
Erstellen Sie dazu zunächst ein Verzeichnis, in dem Sie Ihre virtuelle Umgebung erstellen werden.
Erstellen Sie ein neues Verzeichnis unter ~ / .venvs:
Erstellen Sie nun Ihre virtuelle Python-Umgebung:
Aktivieren Sie als Nächstes Ihre virtuelle Umgebung, damit Sie die Python-Installation verwenden und Pakete installieren können:
Ihre Shell-Eingabeaufforderung zeigt die virtuelle Umgebung nun in Klammern an.
Verwenden Sie nun pip zum Installieren der erforderlichen Python-Pakete in Ihre virtuelle Umgebung:
slackclient und slackeventsapi erleichtern die Interaktion von Python mit den APIs von Slack.
Flask ist ein beliebtes Mikro-Web-Framework, das Sie für die Bereitstellung Ihrer Anwendung verwenden werden:
Nachdem Sie Ihre Entwicklungsumgebung eingerichtet haben, können Sie nun mit dem Schreiben Ihres Python Slackbots beginnen:
Schritt 3 & mdash; Erstellen der Slackbot-Nachrichtenklasse in Python
Nachrichten in Slack werden über eine speziell formatierte JSON-Nutzlast gesendet.
Dies ist ein Beispiel für die JSON, die Ihr Slackbot erstellen und als Nachricht senden wird:
Sie könnten diese JSON manuell erstellen und senden, aber stattdessen erstellen wir eine Python-Klasse, die nicht nur diese Nutzlast erstellt, sondern auch einen Münzwurf simuliert.
Verwenden Sie zunächst den Befehl touch zum Erstellen einer Datei namens coinbot.py:
Öffnen Sie als Nächstes die Datei mit nano oder Ihrem bevorzugten Texteditor:
Fügen Sie nun die folgenden Codezeilen hinzu, um die entsprechenden Bibliotheken für Ihre Anwendung zu importieren. Die einzige Bibliothek, die Sie für diese Klasse benötigen, ist die Bibliothek random aus der Python-Standardbibliothek.
Diese Bibliothek ermöglicht es uns, einen Münzwurf zu simulieren.
Fügen Sie die folgenden Zeilen zu coinbot.py hinzu, um alle erforderlichen Bibliotheken zu importieren:
Erstellen Sie als Nächstes Ihre Klasse CoinBot und eine Instanz dieser Klasse, um die Nachrichten-Nutzlast zu erstellen.
Fügen Sie die folgenden Zeilen zu coinbot.py hinzu, um die Klasse CoinBot zu erstellen:
Rücken Sie nun um eins ein und erstellen Sie die für Ihre Klasse erforderlichen Konstanten, Konstruktoren und Methoden.
Erstellen Sie zunächst die Konstante, die die Basis Ihrer Nachrichten-Nutzlast enthält.
Dieser Abschnitt gibt an, dass diese Konstante von Abschnitt-Typ ist und der Text über Markdown formatiert wird.
Außerdem gibt er an, welchen Text Sie anzeigen möchten.
Weitere Informationen zu den verschiedenen Nutzlast-Optionen finden Sie in der offiziellen Slack-Dokumentation zur Nachrichten-Nutzlast.
Fügen Sie die folgenden Zeilen an coinbot.py an, um die Grundvorlage für die Nutzlast zu erstellen:
Erstellen Sie als Nächstes einen Konstruktor für Ihre Klasse, damit Sie für jede Anfrage eine separate Instanz Ihres Bots erstellen können.
Machen Sie sich hier keine Sorgen über den Speicher-Overhead. Der Python Garbage Collector wird diese Instanzen bereinigen, sobald sie nicht mehr benötigt werden.
Dieser Code setzt den Empfängerkanal basierend auf einem Parameter, der an den Konstruktor übergeben wird.
Fügen Sie die folgenden Zeilen an coinbot.py an, um den Konstruktor zu erstellen:
Schreiben Sie nun den Code, der das Werfen einer Münze simuliert.
Wir erzeugen nach dem Zufallsprinzip eine Eins oder eine Null, die Kopf bzw. Zahl darstellen.
Fügen Sie die folgenden Zeilen an coinbot.py an, um den Münzwurf zu simulieren und die erstellte Nutzlast zurückzugeben:
Erstellen Sie schließlich eine Methode, die die gesamte Nachrichten-Nutzlast, einschließlich der Daten aus Ihrem Konstruktor, handhabt und zurückgibt, indem Sie Ihre Methode _ flip _ coin aufrufen.
Fügen Sie die folgenden Zeilen an coinbot.py an, um die Methode zu erstellen, die die fertige Nutzlast generiert:
Sie sind nun mit der Klasse CoinBot fertig und sie ist zum Testen bereit.
Bevor Sie fortfahren, überprüfen Sie, ob Ihre fertige Datei, coinbot.py, Folgendes enthält:
Nachdem Sie nun eine Python-Klasse haben, die bereit ist, die Arbeit für Ihren Slackbot zu erledigen, stellen wir sicher, dass diese Klasse eine nützliche Nachrichten-Nutzlast erzeugt und dass Sie diese an Ihren Arbeitsbereich senden können.
Schritt 4 & mdash; Testen Ihrer Nachricht
Jetzt testen wir, ob diese Klasse eine angemessene Nutzlast erzeugt.
Erstellen Sie eine Datei namens coinbot _ test.py:
Fügen Sie jetzt den folgenden Code hinzu.
Achten Sie darauf, den Namen des Kanals in der Instanziierung der Coinbot-Klasse coin _ bot = coinbot (" # < ^ > YOUR _ CHANNEL _ HERE < ^ > ") zu ändern.
Dieser Code erstellt einen Slack-Client in Python, der eine Nachricht an den von Ihnen angegebenen Kanal sendet, in dem Sie die Anwendung bereits installiert haben:
Bevor Sie diese Datei ausführen können, müssen Sie das Slack-Token, das Sie in Schritt 1 als Umgebungsvariablen gespeichert haben, exportieren:
Testen Sie nun diese Datei und überprüfen Sie, ob die Nutzlast erzeugt und gesendet wird, indem Sie das folgende Skript in Ihrem Terminal ausführen.
Stellen Sie sicher, dass Ihre virtuelle Umgebung aktiviert ist.
Sie können dies überprüfen, indem Sie den Text (slackbot) am Anfang Ihrer Bash-Eingabeaufforderung sehen.
Wenn Sie diesen Befehl ausführen, erhalten Sie eine Nachricht von Ihrem Slackbot mit den Ergebnissen eines Münzwurfs:
Überprüfen Sie den Kanal, in dem Sie Ihre Anwendung installiert haben und vergewissen Sie sich, dass Ihr Bot die Münzwurf-Nachricht tatsächlich gesendet hat.
Ihr Ergebnis wird Köpfe oder Zahlen sein.
Münzwurftest
Nachdem Sie nun bestätigt haben, dass Ihr Slackbot eine Münze werfen, eine Nachricht erstellen und die Nachricht übermitteln kann, erstellen wir einen Flask, um diese Anwendung ständig auszuführen und sie einen Münzwurf simulieren und die Ergebnisse teilen zu lassen, wenn sie einen bestimmten Text in den im Kanal gesendeten Nachrichten sieht.
Schritt 5 & mdash; Erstellen einer Flask-Anwendung zum Ausführen Ihres Slackbots
Nachdem Sie nun über eine funktionierende Anwendung verfügen, die Nachrichten an Ihren Slack-Arbeitsbereich senden kann, müssen Sie einen lang laufenden Prozess erstellen, damit Ihr Bot die im Kanal gesendeten Nachrichten abhören und darauf antworten kann, wenn der Text bestimmte Kriterien erfüllt.
Sie verwenden das Python-Web-Framework Flask, um diesen Prozess auszuführen nach Ergebnissen in Ihrem Kanal zu lauschen.
< $> note In diesem Abschnitt führen Sie Ihre Flask-Anwendung von einem Server mit einer öffentlichen IP-Adresse aus, damit die Slack-API Ihnen Ereignisse senden kann.
Wenn Sie dies lokal auf Ihrer persönlichen Workstation ausführen, müssen Sie den Port von Ihrer persönlichen Firewall an den Port weiterleiten, der auf Ihrer Workstation ausgeführt wird.
Diese Ports können die gleichen sein und dieses Tutorial wird so eingerichtet, dass Port 3000 verwendet wird.
Passen Sie zunächst Ihre Firewall-Einstellungen so an, dass der Verkehr über Port 3000 zugelassen wird:
Überprüfen Sie nun den Status von ufw:
Erstellen Sie nun die Datei für Ihre Flask-Anwendung. Benennen Sie diese Datei app.py:
Öffnen Sie als Nächstes diese Datei in Ihrem bevorzugten Texteditor:
Fügen Sie nun die folgenden Import-statements hinzu.
Sie werden die folgenden Bibliotheken aus folgenden Gründen importieren:
import os - um auf Umgebungsvariablen zuzugreifen
import logging - um die Ereignisse der Anwendung zu protokollieren
from flask import Flask - um eine Flask-Anwendung zu erstellen
from slack import WebClient - um Nachrichten über Slack zu senden
from slackeventsapi import SlackEventAdapter - um Ereignisse von Slack zu empfangen und sie zu verarbeiten
from coinbot import CoinBot - um eine Instanz Ihres CoinBot zu erstellen und die Nachrichten-Nutzlast zu generieren.
Fügen Sie die folgenden Zeilen zu appy.py hinzu, um alle erforderlichen Bibliotheken zu importieren:
Erstellen Sie nun Ihre Flask-Anwendung und registrieren Sie einen Slack Ereignisadapter für Ihre Slack-Anwendung am Endpunkt / slack / events.
Dadurch wird eine Route in Ihrer Slack-Anwendung erstellt, über die Slack-Ereignisse gesendet und aufgenommen werden.
Dazu müssen Sie ein weiteres Token von Ihrer Slack-Anwendung erhalten, was Sie später im Tutorial tun werden.
Sobald Sie diese Variable erhalten, exportieren Sie sie als Umgebungsvariable namens SLACK _ EVENTS _ TOKEN.
Fahren Sie fort und schreiben Sie Ihren Code, um ihn beim Erstellen des SlackEventAdapters einzulesen, auch wenn Sie das Token noch nicht gesetzt haben.
Fügen Sie die folgenden Zeilen an app.py an, um die Flask-Anwendung zu erstellen und den Ereignisadapter in dieser Anwendung zu registrieren:
Erstellen Sie als Nächstes ein Web-Client-Objekt, mit dem Ihre Anwendung Aktionen im Arbeitsbereich ausführen kann, insbesondere um Nachrichten zu senden.
Dies ist vergleichbar mit dem, was Sie getan haben, als Sie zuvor Ihre Datei coinbot.py getestet haben.
Fügen Sie die folgende Zeile an app.py an, um diesen slack _ web _ client zu erstellen:
Erstellen Sie nun eine aufrufbare Funktion, die eine Instanz von CoinBot erzeugt, und verwenden Sie dann diese Instanz, um eine Nachrichten-Nutzlast zu erstellen und die Nachrichten-Nutzlast an den Slack Web-Client zur Zustellung zu übergeben.
Fügen Sie die folgenden Zeilen an app.py an, um diese Funktion zu erstellen:
Nachdem Sie nun eine Funktion zur Behandlung der Nachrichten-Aspekte Ihrer Anwendung erstellt haben, erstellen Sie eine Funktion, die Slack-Ereignisse für eine bestimmte Aktion überwacht und dann Ihren Bot ausführt. Sie werden Ihre Anwendung so konfigurieren, dass sie mit den Ergebnissen eines simulierten Münzwurfs reagiert, wenn sie den Satz "Hey Sammy, Flip a coin" (Hey Sammy, wirf eine Münze) sieht.
Sie werden jede Version dieses Falls akzeptieren - was die Anwendung nicht daran hindert, zu reagieren.
Dekorieren Sie zunächst Ihre Funktion mit der Syntax @ slack _ events _ adapter.on, die es Ihrer Funktion ermöglicht, Ereignisse zu empfangen.
Geben Sie an, dass Sie nur die Nachrichten-Ereignisse empfangen wollen, und lassen Sie Ihre Funktion einen Nutzlastparameter akzeptieren, der alle erforderlichen Slack-Informationen enthält.
Sobald Sie diese Nutzlast haben, parsen Sie den Text aus und analysieren ihn. Wenn er dann die Aktivierungs-Phrase erhält, sendet Ihre Anwendung die Ergebnisse eines simulierten Münzwurfs.
Fügen Sie den folgenden Code an app.py an, um eingehende Nachrichten zu empfangen, zu analysieren und darauf zu reagieren:
Erstellen Sie schließlich einen Hauptabschnitt, der einen Logger erstellt, damit Sie die Interna Ihrer Anwendung sehen und die Anwendung auf Ihrer externen IP-Adresse auf Port 3000 starten können.
Um die Ereignisse von Slack aufzunehmen, z. B. wenn eine neue Nachricht gesendet wird, müssen Sie Ihre Anwendung auf einer öffentlich zugänglichen IP-Adresse testen.
Fügen Sie die folgenden Zeilen an app.py an, um Ihren Hauptabschnitt einzurichten:
Sie sind nun mit der Flask-Anwendung fertig und sie ist zum Testen bereit.
Bevor Sie fortfahren, überprüfen Sie, dass Ihre fertige Datei app.py Folgendes enthält:
Nachdem Ihre Flask-Anwendung nun bereit ist, Ihrer Anwendung zu bedienen, testen wir sie.
Schritt 6 & mdash; Ausführen Ihrer Flask-Anwendung
Bringen Sie abschließend alles zusammen und führen Sie Ihre Anwendung aus.
Fügen Sie zunächst Ihre laufende Anwendung als autorisierten Handler für Ihren Slackbot hinzu.
Navigieren Sie zum Abschnitt Basic Information Ihrer Anwendung in der Slack-Benutzeroberfläche.
Scrollen Sie nach unten, bis Sie den Abschnitt App Credentials finden.
Slack Signing Secret (Unterschriftengeheimnis)
Kopieren Sie das Signing Secret und exportieren Sie es als Umgebungsvariable SLACK _ EVENTS _ TOKEN:
Damit haben Sie alle erforderlichen API-Token, um Ihre Anwendung auszuführen. Siehe Schritt 1, wenn Sie eine Auffrischung benötigen, wie Sie Ihre SLACK _ TOKEN exportieren können.
Jetzt können Sie Ihre Anwendung starten und überprüfen, ob sie tatsächlich ausgeführt wird.
Stellen Sie sicher, dass Ihre virtuelle Umgebung aktiviert ist und führen Sie folgenden Befehl aus, um Ihre Flask-Anwendung zu starten:
Um zu überprüfen, ob Ihre Anwendung aktiviert ist, öffnen Sie ein neues Terminal-Fenster und führen curl für die IP-Adresse Ihres Servers mit dem richtigen Port aus unter / slack / events:
curl gibt folgendes zurück:
Der Empfang der Nachricht Theses are not the slackbots you 're looking for (Dies sind nicht die Slackbots, nach denen Sie suchen) zeigt an, dass Ihre Anwendung aktiviert ist und ausgeführt wird.
Lassen Sie diese Flask-Anwendung nun laufen, während Sie die Konfiguration Ihrer Anwendung in der Slack-Benutzeroberfläche beenden.
Erteilen Sie Ihrer Anwendung zunächst die entsprechenden Berechtigungen, damit sie auf Nachrichten hören und entsprechend reagieren kann.
Klicken Sie auf Event Subscriptions in der Seitenleiste der Benutzeroberfläche und schalten Sie die Optionsschaltfläche Enable Events um.
Schaltfläche "Ereignisse aktivieren"
Geben Sie anschließend Ihre IP-Adresse, den Port und / slack / events-Endpunkte in das Feld Request URL ein.
Vergessen Sie nicht das HTTP-Protokoll-Präfix.
Slack wird versuchen, sich mit Ihrem Endpunkt zu verbinden.
Sobald dies erfolgreich abgeschlossen ist, sehen Sie ein grünes Häkchen mit dem Wort Verified daneben.
Ereignis-Abonnements URL anfordern
Erweitern Sie als Nächstes Subscribe to bot events und fügen Sie die Berechtigung message.channels zu Ihrer Anwendung hinzu. Dadurch kann Ihre Anwendung Nachrichten von Ihrem Kanal empfangen und verarbeiten.
Berechtigung "Bot-Ereignisse abonnieren"
Sobald Sie dies getan haben, sehen Sie das Ereignis in Ihrem Abschnitt Subscribe to bot events aufgelistet.
Klicken Sie als Nächstes auf die grüne Schaltfläche Save Changes in der rechten unteren Ecke.
Änderungen bestätigen und speichern
Sobald Sie dies getan haben, sehen Sie oben auf dem Bildschirm ein gelbes Banner, das Sie darauf hinweist, dass Sie Ihre Anwendung neu installieren müssen, damit die folgenden Änderungen wirksam werden.
Jedes Mal, wenn Sie Berechtigungen ändern, müssen Sie Ihre Anwendung neu installieren. Klicken Sie in diesem Banner auf den Link reinstall your app, um Ihre Anwendung neu zu installieren.
Banner "Neuinstallation Ihrer Anwendung"
Es wird ein Bestätigungsbildschirm angezeigt, der die Berechtigungen Ihres Bots zusammenfasst und Sie fragt, ob Sie die Installation zulassen möchten.
Klicken Sie auf die grüne Schaltfläche Allow, um den Installationsvorgang abzuschließen.
Bestätigung der Neuinstallation
Danach sollte Ihre Anwendung bereit sein.
Gehen Sie zurück zu dem Kanal, in dem Sie CoinBot installiert haben, und senden Sie eine Nachricht mit dem Inhalt Hey Sammy, Flip a coin. Ihr Bot wird eine Münze werfen und mit den Ergebnissen antworten.
Herzlichen Glückwunsch!
Sie haben einen Slackbot erstellt!
Hey Sammy, wirf eine Münze
Wenn Sie mit der Entwicklung Ihrer Anwendung fertig sind und sie in Produktion gehen kann, müssen Sie sie auf einem Server bereitstellen.
Dies ist notwendig, da der Flask-Entwicklungsserver keine sichere Produktionsumgebung ist.
Es ist besser, wenn Sie Ihre Anwendung über ein WSGI bereitstellen und eventuell sogar einen Domänennamen sichern und Ihrem Server einen DNS-Eintrag geben.
Es gibt viele Optionen zum Bereitstellen von Flask-Anwendungen, von denen einige unten aufgeführt sind:
Bereitstellen Ihrer Flask-Anwendung unter Ubuntu 20.04 mit Gunicorn und Nginx
Bereitstellen Ihrer Flask-Anwendung unter Ubuntu 20.04 mit uWSGI und Nginx
Bereitstellen Ihrer Flask-Anwendung unter Ubuntu mit Docker
Es gibt noch viele weitere Möglichkeiten, Ihre Anwendung bereitzustellen, als nur diese.
Wie immer, wenn es um Bereitstellungen und Infrastruktur geht, tun Sie das, was für Sie am besten funktioniert.
In jedem Fall verfügen Sie nun über einen Slackbot, mit dem Sie eine Münze werfen können, um Ihnen bei Entscheidungen zu helfen, wie zum Beispiel, was Sie zu Mittagessen essen möchten.
Sie können diesen Basiscode auch nehmen und ihn an Ihre Bedürfnisse anpassen, sei es automatisierte Unterstützung, Ressourcenmanagement, Bilder von Katzen oder was immer Ihnen einfällt.
Sie können die vollständigen Python Slack API-Dokumente hier einsehen.
Einrichten von Django mit Postgres, Nginx und Gunicorn unter Ubuntu 20.04
6043
Django ist ein leistungsfähiges Web-Framework, dass Ihnen dabei helfen kann, Ihre Python-Anwendung oder Website bereitzustellen.
Django umfasst einen vereinfachten Entwicklungsserver zum lokalen Testen Ihres Codes; für alles, was auch nur ansatzweise mit der Produktion zu tun hat, wird ein sicherer und leistungsfähiger Webserver benötigt.
In diesem Leitfaden zeigen wir, wie sich bestimmte Komponenten zum Unterstützen und Bereitstellen von Django-Anwendungen in Ubuntu 20.04 installieren und konfigurieren lassen.
Wir werden anstelle der standardmäßigen SQLite-Datenbank eine PostgreSQL-Datenbank einrichten.
Wir werden den Gunicorn-Anwendungsserver als Schnittstelle zu unseren Anwendungen konfigurieren.
Dann werden wir Nginx als Reverseproxy für Gunicorn einrichten, damit wir beim Bereitstellen unserer Anwendungen auf dessen Sicherheits- und Leistungsmerkmale zugreifen können.
Voraussetzungen und Ziele
Um diesen Leitfaden erfolgreich zu absolvieren, sollten Sie eine neue Ubuntu 20.04-Serverinstanz mit einer einfachen Firewall und einem Nicht-root-Benutzer mit sudo-Berechtigungen konfiguriert haben.
In unserem Leitfaden zur Ersteinrichtung des Servers erfahren Sie, wie Sie die Einrichtung vornehmen.
Wir werden Django in einer virtuellen Umgebung installieren.
Wenn Sie Django in einer für Ihr Projekt spezifischen Umgebung installieren, können Sie Ihre Projekte und deren Anforderungen separat verwalten.
Sobald wir über unsere Datenbank verfügen und die Anwendung ausgeführt wird, installieren und konfigurieren wir den Gunicorn-Anwendungsserver.
Dieser wird als Schnittstelle zu unserer Anwendung dienen und Clientanfragen aus HTTP in Python-Aufrufe übersetzen, die unsere Anwendung verarbeiten kann.
Dann richten wir Nginx vor Gunicorn ein, um dessen leistungsfähige Verbindungsverwaltungsmechanismen und einfach zu implementierenden Sicherheitsfunktionen nutzen zu können.
Fangen wir an.
Installieren der Pakete aus den Ubuntu-Repositorys
Zu Beginn laden wir alle Elemente, die wir benötigen, aus den Ubuntu-Repositorys herunter und installieren sie.
Ein wenig später werden wir mit dem Python-Paketmanager pip zusätzliche Komponenten installieren.
Wir müssen zuerst den lokalen Paketindex apt aktualisieren und dann die Pakete herunterladen und installieren:
Die installierten Pakete hängen davon ab, welche Version von Python Sie für Ihr Projekt verwenden werden.
Wenn Sie Django mit Python 3 nutzen, geben Sie Folgendes ein:
Django 1.11 ist die letzte Version von Django, die Python 2 unterstützt. Wenn Sie neue Projekte starten, wird dringend empfohlen, Python 3 zu wählen. Wenn Sie Python 2 weiter verwenden müssen, geben Sie Folgendes ein:
Dadurch werden pip, die Python-Entwicklungsdateien, die zum späteren Erstellen von Gunicorn benötigt werden, das Postgres-Datenbankysystem, die zum Interagieren damit erforderlichen Bibliotheken sowie der Nginx-Webserver installiert.
Erstellen der PostgreSQL-Datenbank und des Benutzers
Wir legen sofort los und erstellen für unsere Django-Anwendung eine Datenbank und einen Datenbankbenutzer.
Standardmäßig nutzt Postgres ein Authentifizierungschema namens "Peer Authentication" für lokale Verbindungen.
Im Grunde bedeutet dies, dass sich der Benutzer ohne weitere Authentifizierung anmelden kann, wenn der Benutzername des Benutzers im Betriebssystem mit einem gültigen Postgres-Benutzernamen übereinstimmt.
Während der Postgres-Installation wurde ein Betriebssystembenutzer namens postgres erstellt, der dem administrativen PostgreSQL-Benutzer postgres entspricht.
Wir benötigen diesen Benutzer zur Erledigung administrativer Aufgaben.
Wir können sudo verwenden und den Benutzernamen mit der Option -u übergeben.
Melden Sie sich in einer interaktiven Postgres-Sitzung an, indem Sie Folgendes eingeben:
Ihnen wird eine PostgreSQL-Eingabeaufforderung angezeigt, in der Sie Ihre Anforderungen einrichten können.
Erstellen Sie zunächst eine Datenbank für Ihr Projekt:
< $> note Anmerkung: Jede Postgres-Anweisung muss mit einem Semikolon enden; stellen Sie sicher, dass Ihr Befehl auf ein Semikolon endet, falls Probleme auftreten.
Erstellen Sie als Nächstes einen Datenbankbenutzer für das Projekt.
Wählen Sie unbedingt ein sicheres Passwort:
Anschließend ändern wir einige der Verbindungsparameter für den gerade erstellten Benutzer.
Dadurch werden Datenbankoperationen beschleunigt, sodass die richtigen Werte nicht jedes Mal abgefragt und festgelegt werden müssen, wenn eine Verbindung hergestellt wird.
Wir legen die Standardkodierung auf UTF-8 fest, was Django erwartet.
Außerdem legen wir das standardmäßige Transaktionsisolierungsschema auf "read committed" fest, um das Lesen von Blöcken aus Transaktionen ohne Commit zu blockieren.
Schließlich legen wir die Zeitzone fest.
Standardmäßig werden unsere Django-Projekte die Zeitzone UTC verwenden.
Dies sind alles Empfehlungen aus dem Django-Projekt selbst:
Jetzt können wir unserem neuen Benutzer Zugriff zum Verwalten unserer neuen Datenbank gewähren:
Wenn Sie damit fertig sind, beenden Sie die PostgreSQL-Eingabeaufforderung durch folgende Eingabe:
Postgres nun so eingerichtet, dass Django eine Verbindung herstellen und dessen Datenbankinformationen verwalten kann.
Erstellen einer virtuellen Python-Umgebung für Ihr Projekt
Nachdem wir unsere Datenbank erstellt haben, können wir nun damit beginnen, die restlichen Projektanforderungen zu erfüllen.
Wir werden unsere Python-Anforderungen zur einfacheren Verwaltung in einer virtuellen Umgebung installieren.
Dazu benötigen wir zunächst Zugriff auf den Befehl virtualenv.
Wir können ihn mit pip installieren.
Wenn Sie Python 3 verwenden, aktualisieren Sie pip und installieren Sie das Paket durch folgende Eingabe:
Wenn Sie Python 2 verwenden, aktualisieren Sie pip und installieren Sie das Paket durch folgende Eingabe:
Nach der Installation von virtualenv können wir mit der Gestaltung unseres Projekts beginnen.
Erstellen und wechseln Sie in ein Verzeichnis, in dem wir unsere Projektdateien speichern können.
Erstellen Sie im Projektverzeichnis durch folgende Eingabe eine virtuelle Python-Umgebung:
Auf diese Weise wird ein Verzeichnis mit dem Namen < ^ > myprojectenv < ^ > in Ihrem Verzeichnis < ^ > myprojectdir < ^ > erstellt.
Darin wird eine lokale Version von Python und eine lokale Version von pip installiert.
Mit ihrer Hilfe können wir für unser Projekt eine isolierte Python-Umgebung installieren und konfigurieren.
Bevor wir die Python-Anforderungen unseres Projekts installieren, müssen wir die virtuelle Umgebung aktivieren.
Geben Sie hierzu Folgendes ein:
Ihre Eingabeaufforderung ändert sich und zeigt an, dass Sie jetzt innerhalb einer virtuellen Python-Umgebung arbeiten.
Sie sieht etwa wie folgt aus: (< ^ > myprojectenv < ^ >) < ^ > user < ^ > @ < ^ > host < ^ >: ~ / < ^ > myprojectdir < ^ > $.
Bei aktivierter virtueller Umgebung installieren Sie Django, Gunicorn und den PostgreSQL-Adapter psycopg2 mit der lokalen Instanz von pip:
< $> note Anmerkung: Wenn die virtuelle Umgebung aktiviert ist (wenn Ihrer Eingabeaufforderung (myprojectenv) voransteht), verwenden Sie pip anstelle von pip3, selbst wenn Sie Python 3 verwenden. Die Kopie des Tools der virtuellen Umgebung heißt immer pip - unabhängig von der Python-Version.
Sie sollten nun die gesamte für den Start eines Django-Projekts erforderliche Software haben.
Erstellen und Konfigurieren eines neuen Django-Projekts
Nach der Installation unserer Python-Komponenten können wir die eigentlichen Django-Projektdateien erstellen.
Erstellen des Django-Projekts
Da wir bereits über ein Projektverzeichnis verfügen, werden wir Django anweisen, die Dateien dort zu installieren.
Es wird ein Verzeichnis der zweiten Ebene mit dem tatsächlichen Code erstellt, was normal ist, und ein Managementskript in diesem Verzeichnis platziert.
Der Schlüssel dazu besteht darin, dass wir das Verzeichnis explizit definieren, anstatt Django Entscheidungen in Bezug auf unser aktuelles Verzeichnis zu ermöglichen:
An diesem Punkt sollte Ihr Projektverzeichnis (in unserem Fall ~ / < ^ > myprojectdir < ^ >) folgenden Inhalt haben:
~ / myprojectdir / manage.py: Ein Django-Projektmanagement-Skript.
~ / myprojectdir / myproject /: Das Django-Projektpaket.
Dieses sollte die Dateien _ _ init _ _ .py, settings.py, urls.py, asgi.py und wsgi.py enthalten.
~ / myprojectdir / myprojectenv /: Das Verzeichnis der virtuellen Umgebung, die wir zuvor erstellt haben.
Anpassen der Projekteinstellungen
Das Erste, was wir mit unseren neu erstellten Projektdateien tun sollten, ist das Anpassen der Einstellungen.
Öffnen Sie die Einstellungsdatei in Ihrem Texteditor:
Suchen Sie zunächst nach der Direktive ALLOWED _ HOSTS.
Dadurch wird eine Liste der Adressen oder Domänennamen des Servers definiert, die zum Herstellen einer Verbindung mit der Django-Instanz genutzt werden können.
Alle eingehenden Anfragen mit einem Host, der sich nicht in dieser Liste befindet, werden eine Ausnahme auslösen.
Django verlangt, dass Sie dies so festlegen, um eine bestimmte Art von Sicherheitslücke zu verhindern.
In den quadratischen Klammern listen Sie die IP-Adressen oder Domänennamen auf, die mit Ihrem Django-Projekt verknüpft sind.
Jedes Element sollte in Anführungszeichen aufgelistet werden, wobei Einträge durch ein Komma getrennt werden.
Wenn Sie Anfragen für eine ganze Domäne und Subdomänen wünschen, stellen Sie dem Anfang des Eintrags einen Punkt voran.
Im folgenden Snippet befinden sich einige auskommentierte Beispiele:
< $> note Anmerkung: Stellen Sie sicher, dass localhost als eine der Optionen eingeschlossen ist, da wir Verbindungen über eine lokale Nginx-Instanz vermitteln werden.
Als Nächstes suchen Sie nach dem Bereich, der Datenbankzugriff konfiguriert.
Er beginnt mit DATABASES.
Die Konfiguration in der Datei dient für eine SQLite-Datenbank.
Wir haben für unser Projekt bereits eine PostgreSQL-Datenbank erstellt; daher müssen wir die Einstellungen anpassen.
Ändern Sie die Einstellungen mit Ihren PostgreSQL-Datenbankinformationen.
Wir weisen Django an, den mit pip installierten psycopg2-Adapter zu verwenden.
Wir müssen den Datenbanknamen, den Namen des Datenbankbenutzers und das Passwort des Datenbankbenutzers angeben. Dann geben wir an, dass sich die Datenbank auf dem lokalen Computer befindet.
Sie können die PORT-Einstellung als leere Zeichenfolge belassen:
Als Nächstes bewegen Sie sich nach unten zum Ende der Datei und fügen eine Einstellung hinzu, die angibt, wo die statischen Dateien platziert werden sollen.
Das ist notwendig, damit Nginx Anfragen für diese Elemente verwalten kann.
In der folgenden Zeile wird Django angewiesen, sie im grundlegenden Projektverzeichnis in einem Verzeichnis mit dem Namen static zu platzieren:
Abschließen der anfänglichen Projekteinrichtung
Jetzt können wir das erste Datenbankschema mit dem Managementskript in unsere PostgreSQL-Datenbank migrieren:
Erstellen Sie durch folgende Eingabe einen administrativen Benutzer für das Projekt:
Sie müssen einen Benutzernamen auswählen, eine E-Mail-Adresse angeben und ein Passwort bestätigen.
Wir können alle statischen Inhalte am von uns konfigurierten Verzeichnisort sammeln, indem wir Folgendes eingeben:
Sie müssen die Operation bestätigen.
Die statischen Dateien werden dann in Ihrem Projektverzeichnis in einem Verzeichnis mit dem Namen static platziert.
Wenn Sie den Leitfaden zur Ersteinrichtung des Servers befolgt haben, sollte Ihre UFW-Firewall Ihren Server schützen.
Um den Entwicklungsserver zu testen, müssen wir Zugriff auf den Port gewähren, den wir dazu verwenden möchten.
Erstellen Sie durch folgende Eingabe eine Ausnahme für Port 8000:
Schließlich können Sie Ihr Projekt durch Starten des Django-Entwicklungsservers mit diesem Befehl testen:
Rufen Sie in Ihrem Webbrowser den Domänennamen oder die IP-Adresse Ihres Servers auf, gefolgt von: 8000:
Sie sollten nun die Indexseite von Django erhalten:
Django-Indexseite
Wenn Sie am Ende der URL in der Adressleiste / admin anfügen, werden Sie zur Eingabe des administrativen Benutzernamens und Passworts aufgefordert, die Sie mit dem Befehl createsuperuser erstellt haben:
Django-Admin-Anmeldung
Nach der Authentifizierung können Sie auf die standardmäßige Admin-Schnittstelle von Django zugreifen:
Django-Admin-Schnittstelle
Wenn Sie mit der Erkundung fertig sind, klicken Sie im Terminalfenster auf Strg + C, um den Entwicklungsserver herunterzufahren.
Testen der Fähigkeit von Gunicorn zum Bereitstellen des Projekts
Das Letzte, was wir tun möchten, bevor wir unsere virtuelle Umgebung verlassen, ist Gunicorn zu testen. Dadurch wollen wir sicherstellen, dass die Anwendung bereitgestellt werden kann.
Wir geben dazu unser Projektverzeichnis ein und verwenden gunicorn zum Laden des WSGI-Moduls des Projekts:
Dadurch wird Gunicorn an der gleichen Schnittstelle gestartet, an der auch der Django-Server ausgeführt wurde.
Sie können zurückgehen und die Anwendung erneut testen.
< $> note Anmerkung: Die Admin-Schnittstelle wird keinen der verwendeten Stile verwenden, da Gunicorn nicht weiß, wo der dafür zuständige statische CSS-Inhalt zu finden ist.
Wir haben Gunicorn einem Modul übergeben, indem wir mithilfe der Modulsyntax von Python den relativen Verzeichnispfad zur Datei wsgi.py von Django angegeben haben. Diese ist der Einstiegspunkt für unsere Anwendung.
In dieser Datei ist eine Funktion namens application definiert, die zur Kommunikation mit der Anwendung dient.
Um mehr über die WSGI-Spezifikation zu erfahren, klicken Sie hier.
Wenn Sie mit dem Testen fertig sind, drücken Sie im Terminalfenster auf Strg + C, um Gunicorn anzuhalten.
Wir sind nun fertig mit der Konfiguration unserer Django-Anwendung.
Wir können unsere virtuelle Umgebung durch folgende Eingabe verlassen:
Der Indikator für die virtuelle Umgebung in Ihrer Eingabeaufforderung wird entfernt.
Erstellen von systemd-Socket- und Service-Dateien für Gunicorn
Wir haben getestet, ob Gunicorn mit unserer Django-Anwendung interagieren kann. Wir sollten jedoch eine effektivere Methode zum Starten und Anhalten des Anwendungsservers implementieren.
Dazu erstellen wir systemd-Service- und Socket-Dateien.
Das Gunicorn-Socket wird beim Booten erstellt und nach Verbindungen lauschen.
Wenn eine Verbindung hergestellt wird, startet systemd automatisch den Gunicorn-Prozess, um die Verbindung zu verwalten.
Erstellen und öffnen Sie zunächst eine systemd-Socket-Datei für Gunicorn mit sudo-Berechtigungen:
Darin erstellen wir einen Abschnitt [Unit], um das Socket zu beschreiben, einen Abschnitt [Socket], um den Ort des Sockets zu definieren, und einen Abschnitt [Install], um sicherzustellen, dass das Socket zum richtigen Zeitpunkt erstellt wird:
Erstellen und öffnen Sie in Ihrem Texteditor als Nächstes eine systemd-Service-Datei für Gunicorn mit sudo-Berechtigungen.
Der Name der Service-Datei sollte mit Ausnahme der Erweiterung mit dem Namen der Socket-Datei übereinstimmen:
Beginnen Sie mit dem Abschnitt [Unit], mit dem Metadaten und Abhängigkeiten angegeben werden.
Wir werden hier eine Beschreibung unseres Diensts eingeben und das Init-System anweisen, ihn erst zu starten, nachdem das Netzwerkziel erreicht wurde.
Da sich unser Dienst auf das Socket aus der Socket-Datei bezieht, müssen wir eine Requires-Direktive einschließen, um diese Beziehung anzugeben:
Als Nächstes werden wir den Abschnitt [Service] öffnen.
Wir werden den Benutzer und die Gruppe angeben, unter denen wir den Prozess ausführen möchten.
Wir wollen unserem regulären Benutzerkonto die Prozessverantwortung übergeben, da es alle relevanten Dateien besitzt.
Wir werden der Gruppe www-data die Gruppenverantwortung übergeben, damit Nginx einfach mit Gunicorn kommunizieren kann.
Dann werden wir das Arbeitsverzeichnis zuordnen und den Befehl zum Starten des Diensts angeben.
In diesem Fall müssen wir den vollständigen Pfad zur ausführbaren Gunicorn-Datei angeben, die in unserer virtuellen Umgebung installiert ist.
Wir werden den Prozess mit dem im Verzeichnis / run erstellten Unix-Socket verknüpfen, damit der Prozess mit Nginx kommunizieren kann.
Wir protokollieren alle Daten in der Standardausgabe, damit der journald-Prozess die Gunicorn-Protokolle erfassen kann.
Außerdem können wir hier optionale Gunicorn-Optimierungen angeben.
Beispielsweise haben wir in diesem Fall drei Workerprozesse angegeben:
Schließlich werden wir einen Abschnitt [Install] hinzufügen.
Dies teilt systemd mit, womit dieser Dienst verknüpft werden soll, wenn wir festlegen, dass er während des Startvorgangs starten soll.
Wir wollen, dass dieser Dienst startet, wenn das normale Mehrbenutzersystem arbeitet.
Damit ist unsere systemd-Dienstdatei fertiggestellt.
Speichern und schließen Sie diese jetzt.
Wir können nun starten und das Gunicorn-Socket aktivieren.
Dadurch wird die Socket-Datei nun unter / run / gunicorn.sock und beim Booten erstellt.
Wenn eine Verbindung zu diesem Socket hergestellt wird, startet systemd automatisch den gunicorn.service, um die Verbindung zu verwalten:
Wir können uns vergewissern, dass der Vorgang erfolgreich war, indem wir nach der Socket-Datei suchen.
Suchen nach der Gunicorn-Socket-Datei
Überprüfen Sie den Status des Prozesses, um herauszufinden, ob er gestartet werden konnte:
Sie sollten eine Ausgabe wie diese erhalten:
Überprüfen Sie als Nächstes, ob die Datei gunicorn.sock im Verzeichnis / run vorhanden ist:
Wenn der Befehl systemctl status angegeben hat, dass ein Fehler aufgetreten ist, oder Sie die Datei gunicorn.sock im Verzeichnis nicht finden können, ist dies ein Hinweis darauf, dass das Gunicorn-Socket nicht richtig erstellt wurde.
Überprüfen Sie die Protokolle des Gunicorn-Sockets durch folgende Eingabe:
Werfen Sie einen erneuten Blick auf Ihre Datei / etc / systemd / system / gunicorn.socket, um alle vorhandenen Probleme zu beheben, bevor Sie fortfahren.
Testen der Socket-Aktivierung
Wenn Sie derzeit nur die Einheit gunicorn.socket gestartet haben, wird der gunicorn.service aktuell noch nicht aktiv sein, da das Socket noch keine Verbindungen erhalten hat.
Geben Sie zum Überprüfen Folgendes ein:
Zum Testen des Socket-Aktivierungsverfahrens können wir über curl eine Verbindung an das Socket senden, indem wir Folgendes eingeben:
Sie sollten die HTML-Ausgabe von Ihrer Anwendung im Terminal erhalten.
Das bedeutet, dass Gunicorn gestartet wurde und Ihre Django-Anwendung bereitstellen konnte.
Sie können überprüfen, ob der Gunicorn-Service ausgeführt wird, indem Sie Folgendes eingeben:
Wenn die Ausgabe von curl oder die Ausgabe von systemctl status anzeigt, dass ein Problem aufgetreten ist, prüfen Sie die Protokolle auf weitere Details:
Überprüfen Sie Ihre Datei / etc / systemd / system / gunicorn.service auf Probleme.
Wenn Sie Änderungen an der Datei / etc / systemd / system / gunicorn.service vornehmen, laden Sie das Daemon neu, um die Servicedefinition neu zu lesen, und starten Sie den Gunicorn-Prozess neu, indem Sie Folgendes eingeben:
Sorgen Sie dafür, dass die oben genannten Probleme behoben werden, bevor Sie fortfahren.
Konfigurieren von Nginx zur Proxy-Übergabe an Gunicorn
Nachdem Gunicorn eingerichtet ist, müssen wir nun Nginx so konfigurieren, dass Datenverkehr an den Prozess übergeben wird.
Erstellen und öffnen Sie zunächst einen neuen Serverblock im Verzeichnis sites-available von Nginx:
Öffnen Sie darin einen neuen Serverblock.
Wir geben zunächst an, dass dieser Block am normalen Port 80 lauschen und auf den Domänennamen oder die IP-Adresse unseres Servers reagieren soll:
Als Nächstes weisen wir Nginx an, alle Probleme bei der Suche nach einem Favicon zu ignorieren.
Außerdem teilen wir Nginx mit, wo die statischen Assets zu finden sind, die wir in unserem Verzeichnis ~ / < ^ > myprojectdir < ^ > / static gesammelt haben.
Alle diese Dateien verfügen über das Standard-URI-Präfix "/ static "; daher können wir einen Ortsblock erstellen, um diese Anfragen abzugleichen:
Schließlich erstellen wir einen location / {} -Block zum Abgleichen aller anderen Anforderungen.
In diesen Block werden wir die standardmäßige Datei proxy _ params einschließen, die in der Nginx-Installation enthalten ist, und dann den Datenverkehr direkt an das Gunicorn-Socket übergeben.
Jetzt können wir die Datei aktivieren, indem wir sie mit dem Verzeichnis sites-enabled verknüpfen:
Prüfen Sie Ihre Nginx-Konfiguration durch folgende Eingabe auf Syntaxfehler:
Wenn keine Fehler gemeldet werden, fahren Sie fort und starten Sie Nginx neu, indem Sie Folgendes eingeben:
Schließlich müssen wir unsere Firewall an Port 80 für normalen Datenverkehr öffnen. Da wir keinen Zugriff mehr auf den Entwicklungsserver benötigen, können wir außerdem die Regel zum Öffnen von Port 8000 entfernen:
Sie sollten nun in der Lage sein, die Domäne oder IP-Adresse Ihres Servers aufzurufen, um Ihre Anwendung anzuzeigen.
< $> note Anmerkung: Nach der Konfiguration von Nginx sollte der nächste Schritt aus dem Sichern des Datenverkehrs zum Server mit SSL / TLS bestehen.
Das ist äußerst wichtig, da ohne SSL / TLS alle Informationen, einschließlich Passwörter, in Klartext über das Netzwerk gesendet werden.
Wenn Sie einen Domänennamen haben, ist Let 's Encrypt der einfachste Weg, um sich ein SSL-Zertifikat zum Sichern Ihres Datenverkehrs zu verschaffen.
Folgen Sie diesem Leitfaden zum Einrichten von Let 's Encrypt mit Nginx unter Ubuntu 20.04.
Befolgen Sie das Verfahren mit dem Nginx-Serverblock, den wir in diesem Leitfaden erstellt haben.
Fehlerbehebung bei Nginx und Gunicorn
Wenn Ihre Anwendung in diesem letzten Schritt nicht angezeigt wird, müssen Sie Probleme mit Ihrer Installation beheben.
Nginx zeigt anstelle der Django-Anwendung die Standardseite an
Wenn Nginx die Standardseite und nicht einen Proxy zu Ihrer Anwendung anzeigt, bedeutet das in der Regel, dass Sie den server _ name in der Datei / etc / nginx / sites-available / < ^ > myproject < ^ > so ändern müssen, dass er auf die IP-Adresse oder den Domänennamen Ihres Servers verweist.
Nginx verwendet den server _ name, um zu bestimmen, welcher Serverblock für Antworten auf Anfragen verwendet werden soll.
Wenn Sie die Nginx-Standardseite erhalten, ist das ein Zeichen dafür, dass Nginx die Anfrage nicht explizit mit einem Serverblock abgleichen konnte. Darum greift Nginx auf den Standardblock zurück, der in / etc / nginx / sites-available / default definiert ist.
Der server _ name im Serverblock Ihres Projekts muss spezifischer sein als der im auszuwählenden standardmäßigen Serverblock.
Nginx zeigt anstelle der Django-Anwendung einen 502-Fehler (Ungültiges Gateway) an
Ein 502-Fehler zeigt, dass Nginx die Anfrage per Proxy nicht erfolgreich vermitteln kann.
Eine Vielzahl von Konfigurationsproblemen äußern sich in einem 502-Fehler; für eine angemessene Fehlerbehebung sind also weitere Informationen erforderlich.
Der primäre Ort zur Suche nach weiteren Informationen sind die Fehlerprotokolle von Nginx.
Generell werden sie Ihnen mitteilen, welche Bedingungen während des Proxying-Ereignisses Probleme verursacht haben.
Folgen Sie den Nginx-Fehlerprotokollen, indem Sie Folgendes eingeben:
Erstellen Sie nun in Ihrem Browser eine weitere Anfrage zur Generierung eines neuen Fehlers (versuchen Sie, die Seite zu aktualisieren).
Sie sollten eine neue Fehlermeldung erhalten, die in das Protokoll geschrieben wird. Wenn Sie sich die Nachricht ansehen, sollten Sie das Problem eingrenzen können.
Sie erhalten möglicherweise die folgende Meldung:
connect () to unix: / run / gunicorn.sock failed (2: No such file or directory)
Das bedeutet, dass Nginx die Datei gunicorn.sock am angegebenen Ort nicht finden konnte.
Sie sollten den proxy _ pass-Ort, der in der Datei / etc / nginx / sites-available / myproject definiert ist, mit dem tatsächlichen Ort der Datei gunicorn.sock vergleichen, die von der Datei gunicorn.socket generiert wurde.
Wenn Sie keine gunicorn.sock-Datei im Verzeichnis / run finden können, bedeutet das im Allgemeinen, dass die systemd-Socket-Datei sie nicht erstellen konnte. Kehren Sie zurück zum Abschnitt zum Suchen nach der Gunicorn-Socket-Datei, um die Schritte zur Fehlerbehebung für Gunicorn zu durchlaufen.
connect () to unix: / run / gunicorn.sock failed (13: Permission denied)
Das bedeutet, dass Nginx aufgrund von Berechtigungsproblemen keine Verbindung zum Gunicorn-Socket herstellen konnte.
Dies kann geschehen, wenn das Verfahren mit dem root user anstelle eines sudo-Benutzers ausgeführt wird.
Zwar kann systemd die Gunicorn-Socket-Datei erstellen, doch kann Nginx nicht darauf zugreifen.
Dies kann geschehen, wenn es an einem beliebigen Punkt zwischen dem root-Verzeichnis (/) der Datei gunicorn.sock begrenzte Berechtigungen gibt.
Wir können die Berechtigungen und Verantwortungswerte der Socket-Datei und jedes der übergeordneten Verzeichnisse überprüfen, indem wir den absoluten Pfad zu unserer Socket-Datei dem Befehl namei übergeben:
Die Ausgabe zeigt die Berechtigungen der einzelnen Verzeichniskomponenten an.
Indem wir uns die Berechtigungen (erste Spalte), den Besitzer (zweite Spalte) und den Gruppenbesitzer (dritte Spalte) ansehen, können wir ermitteln, welche Art des Zugriffs auf die Socket-Datei erlaubt ist.
Im obigen Beispiel haben die Socket-Datei und die einzelnen Verzeichnisse, die zur Socket-Datei führen, globale Lese- und Ausführungsberechtigungen (die Berechtigungsspalte für die Verzeichnisse endet mit r-x anstelle von ---).
Der Nginx-Prozess sollte erfolgreich auf das Socket zugreifen können.
Wenn eines der Verzeichnisse, das zum Socket führt, keine globale Lese- und Ausführungsberechtigung aufweist, kann Nginx nicht auf das Socket zugreifen, ohne globale Lese- und Ausführungsberechtigungen zuzulassen oder sicherzustellen, dass die Gruppenverantwortung einer Gruppe erteilt wird, zu der Nginx gehört.
Django zeigt an: "could not connect to server: Connection refused"
Eine Meldung, die Sie bei dem Versuch, im Webbrowser auf Teile der Anwendung zuzugreifen, von Django erhalten können, lautet:
Das bedeutet, dass Django keine Verbindung zur Postgres-Datenbank herstellen kann.
Vergewissern Sie sich durch folgende Eingabe, dass die Postgres-Instanz ausgeführt wird:
Wenn nicht, können Sie sie starten und so aktivieren, dass sie beim Booten automatisch gestartet wird (wenn sie nicht bereits entsprechend konfiguriert ist); geben Sie dazu Folgendes ein:
Wenn Sie weiter Probleme haben, stellen Sie sicher, dass die in der Datei ~ / myprojectdir / myproject / settings.py definierten Datenbankeinstellungen korrekt sind.
Weitere Fehlerbehebung
Bei der weiteren Fehlerbehebung können die Protokolle dazu beitragen, mögliche Ursachen einzugrenzen.
Prüfen Sie sie nacheinander und suchen Sie nach Meldungen, die auf Problembereiche hinweisen.
Folgende Protokolle können hilfreich sein:
Prüfen Sie die Nginx-Prozessprotokolle, indem Sie Folgendes eingeben: sudo journalctl -u nginx
Prüfen Sie die Nginx-Zugangsprotokolle, indem Sie Folgendes eingeben: sudo less / var / log / nginx / access.log
Prüfen Sie die Nginx-Fehlerprotokolle, indem Sie Folgendes eingeben: sudo less / var / log / nginx / error.log
Prüfen Sie die Gunicorn-Anwendungsprotokolle, indem Sie Folgendes eingeben: sudo journalctl -u gunicorn
Prüfen Sie die Gunicorn-Socket-Protokolle, indem Sie Folgendes eingeben: sudo journalctl -u gunicorn.socket
Wenn Sie Ihre Konfiguration oder Anwendung aktualisieren, müssen Sie die Prozesse wahrscheinlich neu starten, damit Ihre Änderungen aktiv werden.
Wenn Sie Ihre Django-Anwendung aktualisieren, können Sie den Gunicorn-Prozess durch folgende Eingabe neu starten, um die Änderungen zu erfassen:
Wenn Sie Gunicorn-Socket- oder Service-Dateien ändern, laden Sie das Daemon neu und starten Sie den Prozess neu, indem Sie Folgendes eingeben:
Wenn Sie die Konfiguration des Nginx-Serverblocks ändern, testen Sie die Konfiguration und dann Nginx, indem Sie Folgendes eingeben:
Diese Befehle sind hilfreich, um Änderungen zu erfassen, während Sie Ihre Konfiguration anpassen.
In diesem Leitfaden haben wir ein Django-Projekt in seiner eigenen virtuellen Umgebung eingerichtet.
Wir haben Gunicorn so konfiguriert, dass Clientanfragen übersetzt werden, damit Django sie verwalten kann.
Anschließend haben wir Nginx als Reverseproxy eingerichtet, um Clientverbindungen zu verwalten und je nach Clientanfrage das richtige Projekt bereitzustellen.
Django macht die Erstellung von Projekten und Anwendungen durch Bereitstellung vieler der gängigen Elemente besonders einfach; so können Sie sich ganz auf die individuellen Elemente konzentrieren.
Durch Nutzung der in diesem Artikel beschriebenen allgemeinen Tool Chain können Sie die erstellten Anwendungen bequem über einen einzelnen Server bereitstellen.
Zentralisieren von Protokollen mit Journald unter Ubuntu 20.04
6061
Systemprotokolle sind ein äußerst wichtiger Bestandteil für die Verwaltung von Linux-Systemen.
Sie bieten einen unschätzbaren Einblick in die Funktionsweise und Verwendung der Systeme, da sie neben Fehlern auch Betriebsinformationen wie Sicherheitsereignisse aufzeichnen.
Die Standardkonfiguration für Linux-Systeme besteht darin, ihre Protokolle lokal auf demselben System zu speichern, auf dem sie aufgetreten sind.
Dies funktioniert für eigenständige Systeme, wird jedoch mit zunehmender Anzahl von Systemen schnell zu einem Problem.
Die Lösung für die Verwaltung all dieser Protokolle besteht darin, einen zentralen Protokollierungsserver zu erstellen, auf dem jeder Linux-Host seine Protokolle in Echtzeit an einen dedizierten Protokollverwaltungsserver sendet.
Eine zentralisierte Protokollierungslösung bietet im Vergleich zum Speichern von Protokollen auf jedem Host mehrere Vorteile:
Reduziert den Speicherplatz, der auf jedem Host zum Speichern von Protokolldateien benötigt wird.
Protokolle können länger aufbewahrt werden, da der dedizierte Protokollserver mit mehr Speicherkapazität konfiguriert werden kann.
Es kann eine erweiterte Protokollanalyse durchgeführt werden, die Protokolle von mehreren Systemen und mehr Rechenressourcen erfordert, als auf den Hosts verfügbar sind.
Systemadministratoren können auf die Protokolle aller ihrer Systeme zugreifen, bei denen sie sich aus Sicherheitsgründen möglicherweise nicht direkt anmelden können.
In diesem Leitfaden konfigurieren Sie eine Komponente der Tool-Suite systemd, um Protokollnachrichten von Client-Systemen an einen zentralen Protokollsammlungsserver weiterzuleiten.
Sie konfigurieren den Server und den Client so, dass TLS-Zertifikate verwendet werden, um die Protokollnachrichten zu verschlüsseln, wenn sie über unsichere Netzwerke wie das Internet übertragen werden, und um sich gegenseitig zu authentifizieren.
Zwei Ubuntu 20.04-Server.
Einen Nicht-root-Benutzer mit Sudo-Berechtigungen auf beiden Servern.
Anweisungen dazu finden Sie im Leitfaden zur Ersteinrichtung des Servers mit Ubuntu 20.04.
Sie sollten auch die UFW-Firewall auf beiden Servern konfigurieren, wie im Leitfaden erläutert.
Zwei Hostnamen, die auf Ihre Server verweisen.
Ein Hostname für das Client-System, das die Protokolle generiert, und ein anderer für den Protokollsammlungsserver.
In der Domains- und DNS-Dokumentation erfahren Sie, wie Sie Hostnamen auf DigitalOcean Droplets verweisen.
In diesem Leitfaden werden die folgenden zwei Beispiel-Hostnamen verwendet:
< ^ > client.your _ domain < ^ >: Das Client-System, das die Protokolle generiert.
< ^ > server.your _ domain < ^ >: Der Protokollsammlungsserver.
Melden Sie sich sowohl beim Client als auch beim Server in separaten Terminals über SSH als Nicht-root-sudo-Benutzer an, um dieses Tutorial zu starten.
< $> note Hinweis: Während des gesamten Tutorials werden Befehlsblöcke mit dem Namen des Servers (Client oder Server) gekennzeichnet, auf dem der Befehl ausgeführt werden soll.
Schritt 1 - Installieren von systemd-journal-remote
In diesem Schritt installieren Sie das Paket systemd-journal-remote auf dem Client und dem Server.
Dieses Paket enthält die Komponenten, die der Client und der Server zum Weiterleiten von Protokollnachrichten verwenden.
Führen Sie zunächst sowohl auf dem Client als auch auf dem Server ein Systemupdate aus, um sicherzustellen, dass die Paketdatenbank und das System aktuell sind:
Installieren Sie das Paket systemd-journal-remote:
Aktivieren und starten Sie auf dem Server die beiden systemd-Komponenten, die zum Empfangen von Protokollnachrichten benötigt werden, mit dem folgenden Befehl:
Die Option --now im ersten Befehl startet die Dienste sofort.
Sie haben es im zweiten Befehl nicht verwendet, da dieser Dienst erst gestartet wird, wenn er über TLS-Zertifikate verfügt, die Sie im nächsten Schritt erstellen.
Aktivieren Sie auf dem Client die Komponente, mit der systemd die Protokollnachrichten an den Server sendet:
Öffnen Sie anschließend auf dem Server die Ports 19532 und 80 in der UFW-Firewall.
Dadurch kann der Server die Protokollnachrichten vom Client empfangen.
Port 80 ist der Port, über den Certbot das TLS-Zertifikat generiert.
Die folgenden Befehle öffnen diese Ports:
Auf dem Client müssen Sie Port 80 nur mit diesem Befehl öffnen:
Sie haben jetzt die erforderlichen Komponenten installiert und die Basissystemkonfiguration auf dem Client und dem Server abgeschlossen.
Bevor Sie diese Komponenten so konfigurieren können, dass sie mit der Weiterleitung von Protokollnachrichten beginnen, registrieren Sie die Let 's Encrypt TLS-Zertifikate für den Client und den Server mithilfe des Dienstprogramms certbot.
Schritt 2 - Installieren von Certbot und Registrieren von Zertifikaten
Let 's Encrypt ist eine Zertifizierungsstelle, die kostenlose TLS-Zertifikate ausstellt.
Mit diesen Zertifikaten können Computer sowohl die zwischen ihnen gesendeten Daten verschlüsseln als auch die Identität des anderen überprüfen.
Mit diesen Zertifikaten können Sie Ihr Surfen im Internet mit HTTPS sichern.
Dieselben Zertifikate können von jeder anderen Anwendung verwendet werden, die dieselbe Sicherheitsstufe wünscht.
Der Prozess der Registrierung des Zertifikats ist der gleiche, unabhängig davon, wofür Sie es verwenden.
In diesem Schritt installieren Sie das Dienstprogramm certbot und registrieren damit die Zertifikate.
Außerdem wird es automatisch darum kümmern, die Zertifikate zu erneuern, wenn sie ablaufen.
Der Registrierungsvorgang ist hier der gleiche auf dem Client und dem Server.
Sie müssen nur den Hostnamen ändern, um ihn an den Host anzupassen, auf dem Sie den Registrierungsbefehl ausführen.
Aktivieren Sie zunächst das universe-Repository von Ubuntu, da sich das Dienstprogramm certbot im universe-Repository befindet.
Wenn Sie das universe-Repository bereits aktiviert haben, wird eine Ausführung dieser Befehle in Ihrem System nichts verändern und ist somit sicher:
Als Nächstes installieren Sie certbot auf beiden Hosts:
Nachdem Sie certbot installiert haben, führen Sie den folgenden Befehl aus, um die Zertifikate auf dem Client und Server zu registrieren:
Die Optionen in diesem Befehl bedeuten Folgendes:
certonly: Registrieren Sie das Zertifikat und führen Sie keine anderen Änderungen am System vor.
--standalone: Verwenden Sie den integrierten Webserver von certbot zur Verifizierung der Zertifikatsanforderung.
--agree-tos: Stimmen Sie automatisch den Nutzungsbedingungen von Let 's Encrypt zu.
--email < ^ > your _ email < ^ >: Dies ist die E-Mail-Adresse, mit der Let 's Encrypt Sie über den Ablauf des Zertifikats und andere wichtige Informationen benachrichtigt.
-d < ^ > your _ domain < ^ >: Der Hostname, für den das Zertifikat registriert wird.
Dies muss dem System entsprechen, in dem Sie es ausführen.
Wenn Sie diesen Befehl ausführen, werden Sie gefragt, ob Sie die E-Mail-Adresse mit Let 's Encrypt teilen möchten, damit diese Ihnen Nachrichten und andere Informationen zu ihrer Arbeit per E-Mail senden können.
Dies ist optional. Wenn Sie Ihre E-Mail-Adresse nicht teilen, wird die Zertifikatsregistrierung weiterhin normal abgeschlossen.
Wenn der Zertifikatregistrierungsprozess abgeschlossen ist, werden die Zertifikat- und Schlüsseldateien in / etc / letsencrypt / live / < ^ > your _ domain < ^ > / abgelegt, wobei your _ domain der Hostname ist, für den Sie das Zertifikat registriert haben.
Schließlich müssen Sie eine Kopie der Let 's Encrypt-Zertifizierungsstelle und der Zwischenzertifikate herunterladen und in dieselbe Datei einfügen. journald verwendet diese Datei, um die Authentizität der Zertifikate auf dem Client und dem Server zu überprüfen, wenn diese miteinander kommunizieren.
Mit dem folgenden Befehl werden die beiden Zertifikate von der Let 's Encrypt-Website heruntergeladen und in einer einzigen Datei mit dem Namen letsencrypt-combined-certs.pem im Home-Verzeichnis Ihres Benutzers abgelegt.
Führen Sie diesen Befehl auf dem Client und dem Server aus, um die Zertifikate herunterzuladen und die kombinierte Datei zu erstellen:
Verschieben Sie als Nächstes diese Datei in das Verzeichnis Let 's Encrypt, das die Zertifikate und Schlüssel enthält:
Sie haben nun die Zertifikate und Schlüssel registriert.
Im nächsten Schritt konfigurieren Sie den Protokollsammlungsserver so, dass er Protokollnachrichten vom Client abhört und speichert.
Schritt 3 - Konfigurieren des Servers
In diesem Schritt konfigurieren Sie den Server so, dass er die im letzten Schritt generierten Zertifikat- und Schlüsseldateien verwendet, damit er Protokollnachrichten vom Client akzeptieren kann.
systemd-journal-remote ist die Komponente, die auf Protokollnachrichten wartet.
Öffnen Sie ihre Konfigurationsdatei unter / etc / systemd / journal-remote.conf mit einem Texteditor, um die Konfiguration auf dem Server zu starten:
Entfernen Sie anschließend die Kommentare in allen Zeilen im Abschnitt [Remote] und legen Sie die Pfade so fest, dass sie auf die soeben erstellten TLS-Dateien verweisen:
Hier sind die Optionen, die Sie hier verwendet haben:
Seal = false: Melden Sie die Protokolldaten im Journal an.
Aktivieren Sie diese Option, wenn Sie maximale Sicherheit benötigen; andernfalls können Sie es als false belassen.
Wenn Sie möchten, dass alle Protokolle einer einzelnen Datei hinzugefügt werden, setzen Sie dies auf SplitMode = false.
ServerKeyFile: Die private Schlüsseldatei des Servers.
ServerCertificateFile: Die Zertifikatdatei des Servers.
TrustedCertificateFile: Die Datei mit den Let 's Encrypt CA-Zertifikaten.
Jetzt müssen Sie die Berechtigungen für die Let 's Encrypt-Verzeichnisse ändern, die die Zertifikate und den Schlüssel enthalten, damit die systemd-journal-remote sie lesen und verwenden kann.
Ändern Sie zunächst die Berechtigungen so, dass das Zertifikat und der private Schlüssel lesbar sind:
Jetzt können Sie systemd-journal-remote starten:
Ihr Protokollsammlungsserver wird jetzt ausgeführt und ist bereit, Protokollnachrichten von einem Client zu akzeptieren.
Im nächsten Schritt konfigurieren Sie den Client so, dass die Protokolle an Ihren Sammlungsserver weitergeleitet werden.
Schritt 4 - Konfigurieren des Clients
In diesem Schritt konfigurieren Sie die Komponente, die die Protokollnachrichten an den Protokollsammlungsserver weiterleitet.
Diese Komponente wird systemd-journal-upload genannt.
Die Standardkonfiguration für systemd-journal-upload ist, dass ein temporärer Benutzer verwendet wird, der nur vorhanden ist, während der Prozess ausgeführt wird.
Dies erschwert das Lesen der TLS-Zertifikate und -Schlüssel durch systemd-journal-upload.
Um dies zu beheben, erstellen Sie einen neuen Systembenutzer mit demselben Namen wie der temporäre Benutzer, der an seiner Stelle verwendet wird.
Erstellen Sie zunächst den neuen Benutzer mit dem Namen systemd-journal-upload auf dem Client mit dem folgenden adduser-Befehl:
Diese Optionen für den Befehl lauten:
--system: Erstellen Sie den neuen Benutzer als Systembenutzer.
Dadurch wird dem Benutzer eine UID-Nummer (Benutzerkennung) unter 1000 gegeben.
UIDs über 1000 werden normalerweise an Benutzerkonten vergeben, mit denen sich ein Mensch anmeldet.
--home / run / systemd: Legen Sie / run / systemd als Home-Verzeichnis für diesen Benutzer fest.
--no-create-home: Erstellen Sie das Home-Verzeichnis nicht, da es bereits vorhanden ist.
--disabled-login: Der Benutzer kann sich beispielsweise nicht über SSH beim Server anmelden.
--group: Erstellen Sie eine Gruppe mit demselben Namen wie der Benutzer.
Legen Sie als Nächstes die Berechtigungen und den Besitz der Let 's Encrypt-Zertifikatdateien fest:
Bearbeiten Sie nun die Konfiguration für systemd-journal-upload unter / etc / systemd / journal-upload.conf.
Öffnen Sie diese Datei mit einem Texteditor:
Bearbeiten Sie diese Datei, damit sie wie folgt aussieht:
Ihr Client ist jetzt eingerichtet und wird ausgeführt und sendet seine Protokollnachrichten an den Protokollsammlungsserver.
Im nächsten Schritt überprüfen Sie, ob die Protokolle korrekt gesendet und aufgezeichnet werden.
Schritt 5 - Testen des Clients und des Servers
In diesem Schritt testen Sie, ob der Client Protokollnachrichten an den Server weiterleitet und ob der Server sie korrekt speichert.
Der Protokollsammlungsserver speichert die Protokolle von den Clients in einem Verzeichnis unter / var / log / journal / remote /.
Wenn Sie den Client am Ende des letzten Schritts neu gestartet haben, wurden Protokollnachrichten gesendet, sodass sich jetzt eine Protokolldatei in / var / log / journal / remote / befindet.
Die Datei wird nach dem Hostnamen, den Sie für das TLS-Zertifikat verwenden, benannt.
Verwenden Sie den Befehl Is, um zu überprüfen, ob die Protokolldatei des Clients auf dem Server vorhanden ist:
Dadurch wird der Verzeichnisinhalt mit der Protokolldatei gedruckt:
Schreiben Sie als Nächstes eine Protokollnachricht auf den Client, um zu überprüfen, ob der Server die Nachrichten des Clients wie erwartet empfängt.
Mit dem Dienstprogramm logger erstellen Sie eine benutzerdefinierte Protokollnachricht auf dem Client.
Wenn alles funktioniert, leitet systemd-journal-upload diese Nachricht an den Server weiter.
Führen Sie auf dem Client den folgenden logger-Befehl aus:
-p syslog.debug in diesem Befehl legt Funktion und Schweregrad der Nachricht fest.
Wenn man dies auf syslog.debug setzt, wird deutlich, dass es sich um eine Testnachricht handelt.
Lesen Sie als Nächstes die Journaldatei des Clients auf dem Server, um zu überprüfen, ob die Protokollnachrichten vom Client eingehen.
Diese Datei ist eine binäre Protokolldatei, sodass Sie sie mit Tools wie less nicht lesen können.
Lesen Sie die Datei stattdessen mit journalctl mit der Option --file =, mit der Sie eine benutzerdefinierte Journaldatei angeben können:
Die Protokollnachricht wird wie folgt angezeigt:
Ihr Protokollzentralisierungsserver sammelt jetzt erfolgreich Protokolle von Ihrem Client-System.
In diesem Artikel haben Sie einen zentralen Protokollsammlungsserver eingerichtet und einen Client so konfiguriert, dass eine Kopie seiner Systemprotokolle an den Server weitergeleitet wird.
Mit den hier verwendeten Client-Konfigurationsschritten können Sie so viele Clients konfigurieren, wie Sie zum Weiterleiten von Nachrichten an den Protokollsammlungsserver benötigen.
Installieren von Elasticsearch, Logstash und Kibana (Elastic Stack) unter Ubuntu 20.04
5823
Der Elastic Stack (früher ELK Stack genannt) ist eine Sammlung von Open-Source-Software, die von Elastic produziert wird. Damit können Sie Protokolle, die aus beliebigen Quellen in beliebigen Formaten generiert werden, durchsuchen, analysieren und visualisieren.
Zentrale Protokollierung kann dabei helfen, Probleme mit Ihren Servern oder Anwendungen zu identifizieren, da Sie so alle Protokolle an einem Ort durchsuchen können.
Außerdem können Sie Probleme ermitteln, die mehrere Server umfassen, indem Sie ihre Protokolle für einen bestimmten Zeitraum miteinander korrelieren.
Der Elastic Stack verfügt über vier Hauptkomponenten:
Elasticsearch: eine verteilte RESTful-Suchmaschine, die alle erfassten Daten speichert.
Logstash: die Datenverarbeitungskomponente von Elastic Stack, die eingehende Daten an Elasticsearch sendet.
Kibana: eine Weboberfläche zum Durchsuchen und Visualisieren von Protokollen.
Beats: schlanke Datenversender mit einem Zweck, die Daten von Hunderten oder Tausenden von Geräten an Logstash oder Elasticsearch senden können.
In diesem Tutorial installieren Sie den Elastic Stack auf einem Ubuntu 20.04-Server.
Sie erfahren, wie Sie alle Komponenten des Elastic Stacks installieren - darunter Filebeat, ein Beat, das zum Weiterleiten und Zentralisieren von Protokollen und Dateien dient - und so konfigurieren, dass Systemprotokolle gesammelt und visualisiert werden.
Da Kibana in der Regel nur auf dem localhost verfügbar ist, verwenden wir außerdem Nginx als Proxy, sodass es über einen Webbrowser aufrufbar ist.
Wir installieren alle diese Komponenten auf einem einzigen Server, den wir als Elastic Stack-Server bezeichnen werden.
< $> note Hinweis: Bei der Installation des Elastic Stacks müssen Sie im gesamten Stack die gleiche Version verwenden.
In diesem Tutorial installieren wir die neuesten Versionen des gesamten Stacks (zum Zeitpunkt der Verfassung dieses Textes Elasticsearch 7.7.1, Kibana 7.7.1, Logstash 7.7.1 und Filebeat 7.7.1).
Sie können hierzu der Ersteinrichtung eines Servers mit Ubuntu 20.04 folgen.
Installiertes OpenJDK 11.
Siehe Abschnitt Installieren der standardmäßigen JRE / JDK (https: / / www.digitalocean.com / community / tutorials / how-to-install-java-with-apt-on-ubuntu-20-04 # installing-the-default-jrejdk in our guide) Installieren von Java mit Apt unter Ubuntu 20.04, um die Einrichtung vorzunehmen.
Nginx, installiert auf Ihrem Server; Nginx werden wir in diesem Leitfaden später als Reverseproxy für Kibana konfigurieren.
Folgen Sie unserem Leitfaden Installieren von Nginx unter Ubuntu 20.04, um die Einrichtung vorzunehmen.
Da der Elastic Stack dazu dient, wertvolle Informationen über Ihren Server aufzurufen, auf die nicht autorisierte Benutzer nicht zugreifen sollen, sollten Sie Ihren Server unbedingt schützen, indem Sie ein TLS / SSL-Zertifikat installieren.
Dieser Schutz ist optional, wird jedoch ausdrücklich empfohlen.
Da Sie im Laufe des Leitfadens jedoch Änderungen an Ihrem Nginx-Serverblock vornehmen werden, ist es wahrscheinlich sinnvoller, dem Leitfaden Let 's Encrypt unter Ubuntu 20.04 am Ende des zweiten Schritts dieses Tutorials zu folgen.
Wenn Sie vor diesem Hintergrund planen, Let 's Encrypt auf Ihrem Server zu konfigurieren, benötigen Sie dazu Folgendes:
Einen vollständig qualifizierten Domänennamen (FQDN).
Einen A-Datensatz mit < ^ > your-domain < ^ >, der auf die öffentliche IP-Adresse Ihres Servers verweist.
Einen A-Eintrag mit www. < ^ > your _ domain < ^ >, der auf die öffentliche IP-Adresse Ihres Servers verweist.
Verwenden Sie zur Bearbeitung der Konfigurationsdatei von Elasticsearch (elasticsearch.yml) Ihren bevorzugten Texteditor.
Um Zugriff zu beschränken und damit die Sicherheit zu erhöhen, suchen Sie nach der Zeile, die network.host angibt, heben Sie die Kommentierung auf und ersetzen den Wert durch localhost, sodass die Zeile wie folgt aussieht:
Sie können testen, ob Ihr Elasticsearch-Dienst ausgeführt wird, indem Sie eine HTTP-Anfrage senden:
Sie erhalten eine Antwort, die einige grundlegende Informationen zu Ihrem lokalen Knoten enthält und in etwa wie folgt aussieht:
Nachdem Elasticsearch ausgeführt wird, installieren wir nun Kibana, die nächste Komponente des Elastic Stacks.
Schritt 2 - Installieren und Konfigurieren des Kibana-Dashboards
Laut der offiziellen Dokumentation sollten Sie Kibana erst nach der Installation von Elasticsearch installieren.
Durch eine Installation in dieser Reihenfolge wird sichergestellt, dass die Komponenten, auf die die einzelnen Produkte angewiesen sind, korrekt eingerichtet werden.
Da Sie die Elastic-Paketquelle bereits im vorherigen Schritt hinzugefügt haben, können Sie die verbleibenden Komponenten des Elastic Stacks einfach mit apt installieren:
Aktivieren und starten Sie dann den Kibana-Dienst:
Da Kibana so konfiguriert ist, dass nur an localhost gelauscht wird, müssen wir einen Reverseproxy einrichten, um externen Zugriff darauf zu ermöglichen. Dazu verwenden wir Nginx, das bereits auf Ihrem Server installiert sein sollte.
Verwenden Sie zunächst den Befehl openssl, um einen administrativen Kibana-Benutzer zu erstellen, den Sie für den Zugriff auf die Kibana-Weboberfläche verwenden werden.
Als Beispiel nennen wir dieses Konto < ^ > kibanaadmin < ^ >; für mehr Sicherheit empfehlen wir aber, einen nicht standardmäßigen Namen für Ihren Benutzer zu wählen, der sich nur schwer erraten lässt.
Mit dem folgenden Befehl werden der administrative Kibana-Benutzer und das Passwort erstellt und in der Datei htpasswd.users gespeichert.
Sie werden Nginx so konfigurieren, dass dieser Benutzername und das Passwort erforderlich sind und die Datei vorübergehend gelesen wird:
Geben Sie in der Eingabeaufforderung ein Passwort ein und bestätigen Sie es.
Merken oder notieren Sie sich die Anmeldedaten, da Sie sie benötigen werden, um auf die Kibana-Weboberfläche zuzugreifen.
Als Nächstes erstellen wir eine Nginx-Serverblockdatei.
Als Beispiel werden wir diese Datei als < ^ > your _ domain < ^ > bezeichnen; vielleicht finden Sie es jedoch hilfreich, Ihrer Datei einen beschreibenden Namen zu geben.
Wenn Sie für diesen Server beispielsweise einen FQDN und DNS-Einträge eingerichtet haben, könnten Sie diese Datei nach Ihrem FQDN benennen.
Erstellen Sie mit nano oder Ihrem bevorzugten Texteditor die Nginx-Serverblockdatei:
Fügen Sie der Datei den folgenden Codeblock hinzu; vergessen Sie dabei nicht, < ^ > your _ domain < ^ > durch den FQDN oder die öffentliche IP-Adresse Ihres Servers zu ersetzen.
Dieser Code konfiguriert Nginx so, dass HTTP-Datenverkehr Ihres Servers an die Kibana-Anwendung geleitet wird, die an localhost: 5601 lauscht.
Außerdem konfiguriert er Nginx so, dass die Datei htpasswd.users gelesen und eine grundlegende Authentifizierung vorgeschrieben wird.
Hinweis: Wenn Sie das Nginx-Voraussetzungstutorial bis zum Ende befolgt haben, haben Sie diese Datei möglicherweise bereits erstellt und mit Inhalten ausgefüllt.
Löschen Sie in diesem Fall alle vorhandenen Inhalte in der Datei, bevor Sie Folgendes hinzufügen:
Wenn Sie fertig sind, speichern und schließen Sie die Datei.
Aktivieren Sie als Nächstes die neue Konfiguration, indem Sie eine symbolische Verknüpfung zum Verzeichnis sites-enabled erstellen.
Wenn Sie bereits eine Serverblockdatei mit dem gleichen Namen in der Nginx-Voraussetzung erstellt haben, müssen Sie diesen Befehl nicht ausführen:
Prüfen Sie die Konfiguration dann auf Syntaxfehler:
Wenn in Ihrer Ausgabe Fehler angezeigt werden, vergewisserrn Sie sich noch einmal, dass die in Ihrer Konfigurationsdatei platzierten Inhalte richtig hinzugefügt wurden.
Sobald Sie sehen, dass in der Ausgabe syntax is ok (Syntax in Ordnung) steht, fahren Sie fort und starten Sie den Nginx-Dienst neu:
Wenn Sie den Leitfaden zur Servereinrichtung befolgt haben, sollte Ihre UFW-Firewall aktiviert sein.
Um Verbindungen zu Nginx zu ermöglichen, können wir die Regeln anpassen, indem wir Folgendes eingeben:
< $> note Hinweis: Wenn Sie dem Nginx-Voraussetzungstutorial gefolgt sind, haben Sie ggf. eine UFW-Regel erstellt, um das Profil Nginx HTTP über die Firewall zuzulassen.
Da das Profil Nginx Full sowohl HTTP- als auch HTTPS-Datenverkehr über die Firewall zulässt, können Sie die von Ihnen im Voraussetzungstutorial erstellte Regel problemlos löschen.
Führen Sie dazu folgenden Befehl aus:
Kibana ist nun über Ihren FQDN oder die öffentliche IP-Adresse des Elastic Stack-Servers zugänglich.
Sie können die Statusseite des Kibana-Servers überprüfen, indem Sie zur folgenden Adresse navigieren und Ihre Anmeldedaten eingeben, wenn Sie dazu aufgefordert werden:
Auf dieser Statusseite werden Informationen zur Ressourcennutzung des Servers angezeigt und die installierten Plugins aufgelistet.
| Kibana-Statusseite
< $> note Hinweis: Wie im Voraussetzungsbereich erwähnt, sollten Sie auf Ihrem Server SSL / TLS aktivieren.
Sie können dem Leitfaden zu Let "s Encrypt folgen, um unter Ubuntu 20.04 ein kostenloses SSL-Zertifikat für Nginx zu erhalten.
Nach der Erlangung Ihrer SSL / TLS-Zertifikate können Sie zurückkehren und dieses Tutorial abschließen.
Nach der Installation des Kibana-Dashboards installieren wir nun die nächste Komponente: Logstash.
Schritt 3 - Installieren und Konfigurieren von Logstash
Zwar kann Beats Daten direkt an die Elasticsearch-Datenbank senden, doch ist es üblich, Logstash zur Verarbeitung der Daten zu verwenden. So können Sie flexibler Daten aus verschiedenen Quellen erfassen, in ein einheitliches Format umwandeln und in eine andere Datenbank exportieren.
Installieren Sie Logstash mit diesem Befehl:
Nach der Installation von Logstash können Sie mit der Konfiguration fortfahren. Die Konfigurationsdateien von Logstash befinden sich im Verzeichnis / etc / logstash / conf.d.
Weitere Informationen zur Konfigurationssyntax finden Sie in der von Elastic bereitgestellten Konfigurationsreferenz.
Beim Konfigurieren der Datei ist es hilfreich, sich Logstash als Pipeline vorzustellen, die Daten an einem Ende annimmt, auf die eine oder andere Weise verarbeitet und dann an das Ziel sendet (in diesem Fall an Elasticsearch).
Eine Logstash-Pipeline verfügt über zwei erforderliche Elemente (input und output) sowie ein optionales Element (filter).
Die Input-Plugins konsumieren Daten aus einer Quelle, die Filter-Plugins verarbeiten die Daten und die Output-Plugins schreiben die Daten an ein Ziel.
Logstash-Pipeline
Erstellen Sie eine Konfigurationsdatei namens 02-beats-input.conf, in der Sie Ihren Filebeat-Input einrichten werden:
Geben Sie die folgende input-Konfiguration ein.
Dadurch wird ein beats-Input angegeben, der an TCP-Port 5044 lauschen wird.
Erstellen Sie als Nächstes eine Konfigurationsdatei namens 30-elasticsearch-output.conf:
Fügen Sie die folgende output-Konfiguration ein.
Im Wesentlichen konfiguriert dieser Output Logstash so, dass die Beats-Daten in Elasticsearch, das bei localhost: 9200 ausgeführt wird, gespeichert werden - und zwar in einem nach dem verwendeten Beat benannten Index.
Der in diesem Tutorial verwendete Beat ist Filebeat:
Testen Sie Ihre Logstash-Konfiguration mit diesem Befehl:
Wenn keine Syntaxfehler vorhanden sind, wird Ihre Ausgabe nach ein paar Sekunden Config Validation Result: OK.
Exiting Logstash anzeigen.
Wenn Sie das nicht in Ihrer Ausgabe sehen, prüfen Sie auf Fehler, die in Ihrer Ausgabe aufgeführt sind, und aktualisieren Sie Ihre Konfiguration, um sie zu korrigieren.
Beachten Sie, dass Sie Warnungen von OpenJDK erhalten werden; diese sollten jedoch keine Probleme verursachen und können ignoriert werden.
Wenn Ihr Konfigurationstest erfolgreich war, starten und aktivieren Sie Logstash, um die Konfigurationsänderungen anzuwenden:
Nachdem Logstash nun korrekt ausgeführt wird und vollständig konfiguriert ist, installieren wir Filebeat.
Schritt 4 - Installieren und Konfigurieren von Filebeat
Der Elastic Stack verwendet verschiedene schlanke Datenversender namens Beats, um Daten aus verschiedenen Quellen zu erfassen und zu Logstash oder Elasticsearch zu transportieren.
Hier sind die Beats, die derzeit bei Elastic verfügbar sind:
Filebeat: erfasst und versendet Protokolldateien.
Metricbeat: erfasst Metriken aus Ihren Systemen und Diensten.
Packetbeat: erfasst und analysiert Netzwerkdaten.
Winlogbeat: erfasst Windows-Ereignisprotokolle.
Auditbeat: erfasst Linux-Audit-Frameworkdaten und überwacht die Dateiintegrität.
Heartbeat: überwacht Dienste durch aktive Sondierung auf ihre Verfügbarkeit.
In diesem Tutorial werden wir Filebeat verwenden, um lokale Protokolle an unseren Elastic Stack weiterzuleiten.
Installieren Sie Filebeat mit apt:
Konfigurieren Sie als Nächstes Filebeat so, dass eine Verbindung zu Logstash hergestellt wird.
Hier werden wir die Beispielkonfigurationsdatei ändern, die mit Filebeat geliefert wird.
Öffnen Sie die Filebeat-Konfigurationsdatei:
< $> note Hinweis: Wie bei Elasticsearch liegt die Konfigurationsdatei von Filebeat im YAML-Format vor.
Das bedeutet, dass eine richtige Einrückung von entscheidender Bedeutung ist; stellen Sie also sicher, dass Sie die gleiche Anzahl von Leerzeichen verwenden, die in diesen Anweisungen angegeben ist.
Filebeat unterstützt verschiedene Ausgaben; Sie werden in der Regel Ereignisse jedoch nur zur weiteren Verarbeitung direkt an Elasticsearch oder Logstash senden.
In diesem Tutorial verwenden wir Logstash, um eine weitere Verarbeitung der von Filebeat erfassten Daten vorzunehmen.
Filebeat muss Daten nicht direkt an Elasticsearch senden; lassen Sie uns diese Ausgabe also deaktivieren.
Suchen Sie dazu nach dem Abschnitt output.elasticsearch und kommentieren Sie die folgenden Zeilen aus, indem Sie ihnen ein # voranstellen:
Konfigurieren Sie dann den Abschnitt output.logstash.
Heben Sie die Kommentierung der Zeilen output.logstash: und hosts: ["localhost: 5044"] auf, indem Sie das # entfernen.
Dadurch wird Filebeat so konfiguriert, dass auf Ihrem Elastic Stack-Server an Port 5044 eine Verbindung hergestellt wird. Das ist der Port, für den wir zuvor ein Logstash-Input angegeben haben:
Die Funktionalität von Filebeat kann mit Filebeat-Modulen erweitert werden.
In diesem Tutorial werden wir das system-Modul verwenden, das Protokolle erfasst und analysiert, die vom Systemprotokollierungsdienst gängiger Linux-Distributionen erstellt werden.
Aktivieren wir es:
Sie können eine Liste aktivierter und deaktivierter Module anzeigen, indem Sie Folgendes ausführen:
Sie sehen eine Liste, die der folgenden ähnelt:
Standardmäßig ist Filebeat so konfiguriert, dass für die syslog- und authorization-Protokolle Standardpfade verwendet werden.
Im Fall dieses Tutorials müssen Sie in der Konfiguration nichts ändern.
Sie können die Parameter des Moduls in der Konfigurationsdatei / etc / filebeat / modules.d / system.yml anzeigen.
Als Nächstes müssen wir die Filebeat-Aufnahme-Pipelines einrichten, die die Protokolldaten analysieren, bevor sie über Logstash an Elasticsearch gesendet werden.
Um die Aufnahme-Pipeline für das system-Modul zu laden, geben Sie folgenden Befehl ein:
Laden Sie als Nächstes die Indexvorlage in Elasticsearch.
Ein Elasticsearch-Index ist eine Sammlung von Dokumenten, die ähnliche Merkmale aufweisen.
Indizes werden mit einem Namen identifiziert, der bei der Ausführung verschiedener Operationen darin zum Verweisen auf den Index verwendet wird. Die Indexvorlage wird bei Erstellung eines neuen Index automatisch angewendet.
Um die Vorlage zu laden, verwenden Sie folgenden Befehl:
Filebeat ist mit beispielhaften Kibana-Dashboards ausgestattet, mit denen Sie Filebeat-Daten in Kibana visualisieren können.
Bevor Sie die Dashboards verwenden können, müssen Sie das Indexmuster erstellen und die Dashboards in Kibana laden.
Während die Dashboards geladen werden, verbindet sich Filebeat mit Elasticsearch, um Versionsdaten zu überprüfen.
Um Dashboards zu laden, wenn Logstash aktiviert ist, müssen Sie die Logstash-Ausgabe deaktivieren und die Elasticsearch-Ausgabe aktivieren:
Jetzt können Sie Filebeat starten und aktivieren:
Wenn Sie Ihren Elastic Stack richtig eingerichtet haben, wird Filebeat mit dem Versand Ihrer syslog- und authorization-Protokolle an Logstash beginnen, was die Daten dann in Elasticsearch laden wird.
Um zu prüfen, ob Elasticsearch diese Daten tatsächlich erhält, fragen Sie den Filebeat-Index mit folgendem Befehl ab:
Wenn Ihre Ausgabe insgesamt 0 Treffer anzeigt, lädt Elasticsearch keine Protokolle unter dem Index, den Sie angefragt haben; in diesem Fall müssen Sie Ihre Einrichtung auf Fehler prüfen.
Wenn Sie die erwartete Ausgabe erhalten haben, fahren Sie mit dem nächsten Schritt fort, in dem Sie erfahren werden, wie Sie durch einige Kibana-Dashboards navigieren können.
Schritt 5 - Erkunden von Kibana-Dashboards
Kehren wir zurück zur Kibana-Weboberfläche, die wir zuvor installiert haben.
Rufen Sie in einem Webbrowser den FQDN oder die öffentliche IP-Adresse Ihres Elastic Stack-Servers auf.
Wenn Ihre Sitzung unterbrochen wurde, müssen Sie die in Schritt 2 festgelegten Anmeldedaten erneut eingeben. Sobald Sie sich angemeldet haben, sollten Sie die Homepage von Kibana angezeigt werden:
Kibana-Homepage
Klicken Sie in der linken Navigationsleiste auf den Link Discover (Erkunden) (Sie müssen möglicherweise links unten auf das Symbol zum Erweitern klicken, um die Elemente des Navigationsmenüs anzuzeigen).
Wählen Sie auf der Seite Discover das vordefinierte filebeat- * -Indexmuster aus, um Filebeat-Daten anzuzeigen. Standardmäßig werden Ihnen die Protokolldaten der letzten 15 Minuten angezeigt.
Im Folgenden sehen Sie ein Histogramm mit Protokollereignissen und einige Protokollnachrichten:
Seite "Discover" (Erkunden)
Hier können Sie Ihre Protokolle durchsuchen sowie Ihr Dashboard anpassen.
An diesem Punkt wird es hier jedoch nicht viel geben, da Sie nur Syslogs von Ihrem Elastic Stack-Server erfassen.
Verwenden Sie den linken Bereich, um zur Dashboard-Seite zu navigieren und nach den Filebeat System-Dashboards zu suchen.
Nun können Sie die Beispiel-Dashboards auswählen, die mit dem system-Modul von Filebeat geliefert werden.
Beispielsweise können Sie anhand Ihrer syslog-Nachrichten genaue Statistiken anzeigen:
Syslog-Dashboard
Außerdem können Sie sehen, welche Benutzer den sudo-Befehl verwendet haben und wann:
Sudo-Dashboard
Kibana verfügt über viele andere Funktionen wie Diagrammerstellung und Filter; sehen Sie sich diese Funktionen bei Interesse genauer an.
In diesem Tutorial haben Sie gelernt, wie Sie den Elastic Stack installieren und so konfigurieren können, dass er Systemprotokolle erfasst und analysiert.
Denken Sie daran, dass Sie mit Beats fast jede Art von Protokoll oder indizierten Daten an Logstash senden können. Die Daten werden jedoch noch nützlicher, wenn sie mit einem Logstash-Filter analysiert und strukturiert werden; dabei werden die Daten in ein konsistentes Format umgewandelt, das von Elasticsearch leicht gelesen werden kann.
Einrichten eines IKEv2-VPN-Servers mit StrongSwan unter Ubuntu 20.04
5742
Eine frühere Version dieses Tutorials wurde von Justin Ellingwood und Namo verfasst
Ein virtuelles privates Netzwerk oder VPN ermöglicht Ihnen die sichere Verschlüsselung des Datenverkehrs auf dem Weg durch nicht vertrauenswürdige Netzwerke, wie z. B. im Café, bei einer Konferenz oder auf einem Flughafen.
Internet Key Exchange v2 oder IKEv2 ist ein Protokoll, das ein direktes IPSec-Tunneling zwischen Server und Client ermöglicht.
In IKEv2-VPN-Implementierungen bietet IPSec eine Verschlüsselung für den Netzwerkverkehr.
IKEv2 wird auf einigen Plattformen (OS X 10.11 +, iOS 9.1 + und Windows 10) nativ unterstützt, ohne dass zusätzliche Anwendungen erforderlich sind und Client-Hickups werden reibungslos verwaltet.
In diesem Tutorial richten Sie einen IKEv2-VPN-Server mit StrongSwan auf einem Ubuntu-20.04-Server ein.
Anschließend lernen Sie, wie mit Windows-, macOS-, Ubuntu-, iOS- und Android-Clients eine Verbindung zu diesem Server herstellen können.
Einen Ubuntu 20.04-Server, der gemäß des Leitfadens zur Ersteinrichtung des Servers für Ubuntu 20.04 konfiguriert wurde, einschließlich eines Nicht-Root-Benutzer mit sudo-Berechtigungen und einer Firewall.
Schritt 1 - Installieren von StrongSwan
Zuerst installieren wir StrongSwan, einen Open-Source-IPSec-Daemon, den wir als unseren VPN-Server konfigurieren werden.
Außerdem installieren wir die Komponente "Public Key Infrastructure" (PKI), sodass wir eine Zertifizierungsstelle (Certificate Authority, CA) erstellen können, die die Anmeldedaten für unsere Infrastruktur bereitstellt.
Beginnen Sie mit der Aktualisierung des lokalen Paket-Caches:
Installieren Sie dann die Software durch folgende Eingabe:
Das zusätzliche Paket libcharon-extauth-plugins wird verwendet, um sicherzustellen, dass verschiedene Clients sich bei Ihrem Server mit einem gemeinsamen Benutzernamen und einer gemeinsamen Passphrase authentifizieren können.
Nachdem nun alles installiert ist, fahren wir mit der Erstellung unserer Zertifikate fort.
Schritt 2 - Erstellen einer Zertifizierungsstelle
Ein IKEv2-Server erfordert ein Zertifikat, um sich gegenüber Clients zu identifizieren.
Um bei der Erstellung des erforderlichen Zertifikats zu helfen, enthält das Paket strongswan-pki ein Dienstprogramm namens pki zur Generierung einer Zertifizierungsstelle und von Serverzertifikaten.
Zu Beginn erstellen wir einige Verzeichnisse zum Speichern aller Assets, an denen wir arbeiten möchten.
Die Verzeichnisstruktur entspricht einigen der Verzeichnisse in / etc / ipsec.d, wohin wir schließlich alle von uns erstellten Objekte verschieben werden:
Dann sperren wir die Berechtigungen, sodass unsere privaten Dateien von anderen Benutzern nicht gesehen werden können:
Nachdem wir nun über eine Verzeichnisstruktur verfügen, um alles zu speichern, können wir einen Stammschlüssel generieren.
Dies wird ein 4096-Bit-RSA-Schlüssel sein, der zum Signieren unserer Stammzertifizierungsstelle verwendet wird.
Führen Sie diese Befehle zur Generierung des Schlüssels aus:
Danach können wir zur Erstellung unserer Stammzertifizierungsstelle übergehen und den gerade generierten Schlüssel zum Signieren des Stammzertifikats verwenden:
Das Flag --lifetime 3650 wird verwendet, um sicherzustellen, dass das Stammzertifikat der Zertifizierungsstelle für 10 Jahre gültig ist.
Das Stammzertifikat einer Stelle ändert sich in der Regel nicht, da es an alle Server und Clients, die darauf angewiesen sind, neu verteilt werden müsste. 10 Jahre ist als ein sicherer Standardwert für die Gültigkeitsdauer.
Sie können den Wert für den unterscheidenden Namen (Distinguished name, DN) in etwas anderes ändern, wenn Sie möchten.
Der gewöhnliche Name (Common Name) (CN-Feld) ist hier nur der Indikator, sodass er mit nichts in Ihrer Infrastruktur übereinstimmen muss.
Nachdem wir nun unsere Stammzertifizierungsstelle eingerichtet haben, können wir ein Zertifikat erstellen, das der VPN-Server verwenden wird.
Schritt 3 - Erstellen eines Zertifikats für den VPN-Server
Wir erstellen nun ein Zertifikat und einen Schlüssel für den VPN-Server.
Dieses Zertifikat ermöglicht es dem Client, die Authentifizierung des Servers mit dem gerade generierten CA-Zertifikat zu überprüfen.
Erstellen Sie zunächst einen privaten Schlüssel für den VPN-Server mit dem folgenden Befehl:
Erstellen und signieren Sie nun das VPN-Serverzertifikat mit dem Schlüssel der Zertifizierungsstelle, den Sie im vorherigen Schritt erstellt haben.
Führen Sie den folgenden Befehl aus, ändern Sie jedoch den Common Name (CN) und das Feld Subject Alternate Name (SAN) in den DNS-Namen oder die IP-Adresse Ihres VPN-Servers:
< $> note Anmerkung: Wenn Sie anstelle eines DNS-Namens eine IP-Adresse verwenden, müssen mehrere --san Einträge angegeben werden.
Die Zeile im vorherigen Befehlsblock, in dem Sie den Distinguished Name (--dn...) angeben, muss mit dem zusätzlichen Eintrag wie die folgende Auszugszeile modifiziert werden:
Der Grund für diesen zusätzlichen Eintrag --san @ < ^ > IP _ address < ^ > ist, dass einige Clients prüfen, ob das TLS-Zertifikat sowohl über einen DNS-Eintrag als auch einen IP-Adressen-Eintrag für einen Server verfügt, wenn sie seine Identität überprüfen.
Die Option ---flag serverAuth wird verwendet, um anzugeben, dass das Zertifikat explizit für die Serverauthentifizierung verwendet wird, bevor der verschlüsselte Tunnel hergestellt wird.
Die Option --flag ikeIntermediate wird zur Unterstützung älterere MacOS-Clients verwendet.
Nachdem wir nun alle TLS / SSL-Dateien erzeugt haben, die StrongSwan benötigt, können wir die Dateien in das Verzeichnis / etc / ipsec.d verschieben, indem wir Folgendes eingeben:
In diesem Schritt haben wir ein Zertifikatpaar erstellt, das zur Sicherung der Kommunikation zwischen dem Client und dem Server verwendet wird.
Wir haben die Zertifikate auch mit dem CA-Schlüssel signiert, sodass der Client die Authentifizierung des VPN-Servers mit dem CA-Zertifikat überprüfen kann.
Nachdem nun alle diese Zertifikate fertig sind, gehen wir zur Konfiguration der Software über.
Schritt 4 - Konfigurieren von StrongSwan
StrongSwan hat eine Standardkonfigurationsdatei mit einigen Beispielen, aber wir werden die Konfiguration größtenteils selbst vornehmen müssen.
Lassen Sie uns die Datei als Referenz sichern, bevor wir von Grund auf starten:
Erstellen und öffnen Sie eine neue leere Konfigurationsdatei mit Ihrem bevorzugten Texteditor.
< $> note Anmerkung: Während Sie diesen Abschnitt zur Konfiguration des Serverteils Ihres VPNs durcharbeiten, werden Sie auf Einstellungen stoßen, die sich auf die linke und die rechte Seite einer Verbindung beziehen.
Bei der Arbeit mit IPSec bezieht sich die linke Seite per Konvention auf das lokale System, das Sie konfigurieren, in diesem Fall den Server.
Die Anweisungen der rechten Seite in diesen Einstellungen beziehen sich auf entfernte Clients wie Telefone und andere Computer.
Wenn Sie später in diesem Tutorial mit der Konfiguration von Clients fortfahren, beziehen sich die Client-Konfigurationsdateien mit verschiedenen linken Anweisungen auf sich selbst, und der Server wird mit der Terminologie der rechten Seite bezeichnet.
Zuerst weisen wir StrongSwan an, den Daemon-Status zur Fehlersuche zu protokollieren und doppelte Verbindungen zulassen.
Fügen Sie der Datei diese Zeilen hinzu:
Dann erstellen wir einen Konfigurationsabschnitt für unser VPN.
Außerdem weisen wir StrongSwan an, IKEv2 VPN-Tunnel zu erstellen und diesen Konfigurationsabschnitt automatisch beim Starten zu laden.
Fügen Sie der Datei folgende Zeilen an:
Wir konfigurieren ebenfalls die Dead-Peer-Erkennung, um alle "unreferenzierten" Verbindungen zu löschen, falls der Client die Verbindung unerwartet trennt.
Fügen Sie diese Zeilen hinzu:
Als Nächstes konfigurieren wir die IPSec-Parameter der "linken" Seite des Servers.
Jede der folgenden Parameter stellt sicher, dass der Server für die Annahme von Verbindungen von Clients und für seine korrekte Identifizierung konfiguriert ist.
Sie fügen jede dieser Einstellungen der Datei / etc / ipsec.conf hinzu, sobald Sie sich damit vertraut gemacht haben, was sie sind und warum sie verwendet werden:
left =% any Der Wert% any stellt sicher, dass der Server die Netzwerkschnittstelle, an der er eingehende Verbindungen empfängt, für die nachfolgende Kommunikation mit Clients verwendet.
Wenn Sie beispielsweise einen Client über ein privates Netzwerk verbinden, verwendet der Server für den Rest der Verbindung die private IP-Adresse, an der er den Datenverkehr empfängt.
leftid = < ^ > @ server _ domain _ or _ IP < ^ > Diese Option steuert den Namen, den der Server den Clients präsentiert.
In Kombination mit der nächsten Option leftcert, stellt die Option leftid sicher, dass der konfigurierte Name des Servers und der im öffentlichen Zertifikat enthaltene Dinstinguished Name (DN) übereinstimmen.
leftcert = server-cert.pem Diese Option ist der Pfad zum öffentlichen Zertifikat für den Server, den Sie in Schritt 3 konfiguriert haben. Ohne ihn kann sich der Server nicht bei Clients authentifizieren und die Aushandlung der IKEv2-Einrichtung nicht beenden.
leftsendcert = always Der Wert always stellt sicher, dass jeder Client, der sich mit dem Server verbindet, als Teil des anfänglichen Verbindungsaufbaus immer eine Kopie des öffentlichen Zertifikats des Servers erhält.
leftsubnet = 0.0.0.0 / 0 Die letzte Option der "linken" Seite, die Sie hinzufügen werden, informiert Clients über die Subnetze, die hinter dem Server erreichbar sind.
In diesem Fall wird 0.0.0.0 / 0 verwendet, um den gesamten Satz der IPv4-Adressen zu repräsentieren, was bedeutet, dass der Server Clients standardmäßig anweist, ihren gesamten Datenverkehr über das VPN zu senden.
Nachdem Sie nun mit jeder der entsprechenden Optionen der "linken" Seite vertraut sind, fügen Sie sie alle wie folgt zur Datei hinzu:
< $> note Anmerkung: Geben Sie bei der Konfiguration der Server-ID (leftid) das @ -Zeichen nur dann an, wenn Ihr VPN-Server durch einen Domänennamen identifiziert werden soll:
Wenn der Server durch seine IP-Adresse identifiziert werden soll, geben Sie einfach die IP-Adresse ein:
Als Nächstes können wir die IPSec-Parameter der "rechten" Seite des Clients konfigurieren.
Jede der folgenden Parameter teilt dem Server mit, wie Verbindungen von Clients akzeptiert werden sollen, wie sich Clients beim Server authentifizieren sollen und welche privaten IP-Adressbereiche und DNS-Server diese Clients verwenden werden.
Fügen Sie jede dieser Einstellungen der Datei / etc / ipsec.conf hinzu, sobald Sie sich damit vertraut gemacht haben, was sie sind und warum sie verwendet werden:
right =% any Die Option% any für die rechte Seite der Verbindung weist den Server an, eingehende Verbindungen von jedem Remote-Client anzunehmen.
rightid =% any Diese Option stellt sicher, dass der Server keine Verbindungen von Clients ablehnt, die eine Identität angeben, bevor der verschlüsselte Tunnel hergestellt ist.
rightauth = eap-mschapv2 Diese Option konfiguriert die Authentifizierungsmethode, die Clients zur Authentifizierung gegenüber dem Server verwenden werden. eap-mschapv2 wird hier für eine breite Kompatibilität verwendet, um Clients wie Windows-, MacOS- und Android-Geräte zu unterstützen.
rightsourceip = 10.10.10.0 / 24 Diese Option weist den Server an, Clients private IP-Adressen aus dem angegebenen IP-Pool 10.10.10.0 / 24 zuzuweisen.
rightdns = 8.8.8.8,8.8.4.4 Diese IP-Adressen sind die öffentlichen DNS-Auflöser von Google.
Sie können geändert werden, um andere öffentliche Auflöser, die Auflösung des VPN-Servers oder einen anderen Auflöser zu verwenden, den die Clients erreichen können.
rightsendcert = never Diese Option weist den Server an, dass Clients kein Zertifikat senden müssen, um sich selbst zu authentifizieren.
Nachdem Sie nun mit den erforderlichen Optionen der "rechten" Seite vertraut sind, fügen Sie die folgenden Zeilen in / etc / ipsec.conf ein:
Nun werden wir StrongSwan anweisen, den Client beim Verbindungsaufbau nach den Anmeldedaten der Benutzer zu fragen:
Zum Schluss fügen Sie die folgenden Zeilen hinzu, um Linux-, Windows-, macOS-, iOS- und Android-Clients zu unterstützen.
Diese Zeilen spezifizieren die verschiedenen Schlüsselaustausch-, Hashing-, Authentifizierungs- und Verschlüsselungsalgorithmen (allgemein als Cipher Suites bezeichnet), die StrongSwan den verschiedenen Clients zur Verfügung stellt:
Jede unterstützte Cipher Suite wird durch ein Komma von den anderen abgegrenzt.
Beispielsweise ist chacha20poly1305-sha512-curve25519-prfsha512 eine Suite, und aes256gcm16-sha384-prfsha384-ecp384 ist eine andere.
Die hier aufgelisteten Cipher Suits sind so ausgewählt, dass sie die größtmögliche Kompatibilität zwischen Windows-, macOS-, iOS-, Android- und Linux-Clients gewährleisten.
Die vollständige Konfigurationsdatei sollte wie folgt aussehen:
Speichern und schließen Sie die Datei, nachdem Sie überprüft haben, dass Sie jede Zeile korrekt hinzugefügt haben.
Wenn Sie nano verwendet haben, drücken Sie STRG + X, Y, dann die EINGABETASTE.
Nachdem wir nun die VPN-Parameter konfiguriert haben, gehen wir zur Erstellung eines Kontos über, damit unsere Benutzer eine Verbindung zum Server herstellen können.
Schritt 5 - Konfigurieren der VPN-Authentifizierung
Unser VPN-Server ist nun so konfiguriert, dass er Client-Verbindungen akzeptiert, aber wir haben noch keine Anmeldedaten konfiguriert.
Wir müssen einige Dinge in einer speziellen Konfigurationsdatei namens ipsec.secrets konfigurieren:
Wir müssen StrongSwan mitteilen, wo der private Schlüssel für unser Serverzertifikat zu finden ist, damit sich der Server gegenüber den Clients authentifizieren kann.
Außerdem müssen wir eine Liste von Benutzern erstellen, die eine Verbindung mit dem VPN herstellen dürfen.
Öffnen wie die Datei "Secrets" zum Bearbeiten:
Zuerst teilen wir StrongSwan mit, wo unser privater Schlüssel zu finden ist:
Dann definieren wir die Anmeldedaten des Benutzers.
Sie können jede beliebige Kombination von Benutzername und Passwort zusammenstellen, die Ihnen gefällt:
Nachdem wir die Arbeit mit den VPN-Parametern beendet haben, starten wir den VPN-Dienst neu, damit unsere Konfiguration übernommen wird:
Nachdem der VPN-Server nun sowohl mit den Serveroptionen als auch mit den Anmeldedaten der Benutzer vollständig konfiguriert ist, ist es an der Zeit, mit der Konfiguration des wichtigsten Teils fortzufahren: der Firewall.
Schritt 6 - Konfigurieren der Firewall & Kernel-IP-Weiterleitung
Nachdem die Konfiguration von StrongSwan abgeschlossen ist, müssen wir die Firewall so konfigurieren, dass sie VPN-Verkehr durchlässt und weiterleitet.
Wenn Sie dem Tutorial zur Ersteinrichtung des Servers gefolgt sind, sollten Sie eine UFW-Firewall aktiviert haben.
Wenn Sie UFW noch nicht konfiguriert haben, sollten Sie zunächst mit dem Hinzufügen einer Regel beginnen, um SSH-Verbindungen durch die Firewall zuzulassen, damit Ihre aktuelle nicht geschlossen wird, wenn Sie UFW aktivieren:
Aktivieren Sie dann die Firewall durch folgende Eingabe:
Fügen Sie dann eine Regel hinzu, um den UDP-Verkehr zu den IPSec-Standardports 500 und 4500 zulassen:
Als Nächstes öffnen wir eine der Konfigurationsdateien von UFW, um einige niedrigstufige Richtlinien für das Routing und die Weiterleitung von IPSec-Paketen hinzuzufügen.
Zuvor müssen wir jedoch herausfinden, welche Netzwerkschnittstelle auf unserem Server für den Internetzugang verwendet wird.
Finden Sie diese Schnittstelle, indem Sie das mit der Standardroute verknüpfte Gerät abfragen:
Ihre öffentliche Schnittstelle sollte dem Wort "dev" folgen.
Dieses Ergebnis zeigt beispielsweise die im folgenden Beispiel hervorgehobene Schnittstelle eth0 an:
Wenn Sie Ihre öffentliche Netzwerkschnittstelle haben, öffnen Sie die Datei / etc / ufw / before.rules in Ihrem Texteditor.
Die Regeln in dieser Datei werden der Firewall vor den übrigen üblichen Eingabe- und Ausgaberegeln hinzugefügt.
Sie werden verwendet, um die Netzwerkadressübersetzung (Network Address Translation, NAT) zu konfigurieren, damit der Server Verbindungen zu und von Clients und dem Internet korrekt weiterleiten kann.
Fügen Sie am Anfang der Datei (vor der Zeile * filter) den folgenden Konfigurationsblock hinzu.
Ändern Sie jede Instanz von eth0 in der obigen Konfiguration so, dass sie mit dem Schnittstellennamen übereinstimmt, den Sie mit ip route gefunden haben.
Die Zeilen * nat erstellen Regeln, damit die Firewall den Datenverkehr zwischen den VPN-Clients und dem Internet korrekt weiterleiten und manipulieren kann.
Die Zeile * mangle passt die maximale Paketsegmentgröße an, um potenzielle Probleme mit bestimmten VPN-Clients zu verhindern:
Fügen Sie als Nächstes nach den Zeilen * filter und Kettendefinitionen einen weiteren Konfigurationsblock hinzu:
Diese Zeilen weisen die Firewall an, den ESP-Datenverkehr (Encapsulating Security Payload-Datenverkehr) weiterzuleiten, sodass die VPN-Clients eine Verbindung herstellen können.
ESP bietet für unsere VPN-Pakete zusätzliche Sicherheit, da sie nicht vertrauenswürdige Netzwerke durchqueren.
Wenn Sie fertig sind, speichern und schließen Sie die Datei, nachdem Sie überprüft haben, dass Sie jede Zeile korrekt hinzugefügt haben.
Vor dem Neustart der Firewall werden wir einige Netzwerk-Kernel-Parameter ändern, um die Weiterleitung von einer Schnittstelle zu einer anderen zu ermöglichen.
Die Datei, die diese Einstellungen steuert, heißt / etc / ufw / sysctl.conf.
In dieser Datei müssen wir einige Dinge konfigurieren.
Zuerst muss die IPv4-Paketweiterleitung eingeschaltet werden, damit der Datenverkehr zwischen dem VPN und den öffentlich zugänglichen Netzwerkschnittstellen auf dem Server ausgetauscht werden kann.
Als Nächstes deaktivieren wir die Path-MTU-Erkennung, um Probleme mit der Paketfragmentierung zu verhindern.
Abschließend werden wir weder ICMP-Weiterleitungen akzeptieren noch ICMP-Weierleitungen senden, um Man-in-the-Middle-Angriffe zu verhindern.
Öffnen Sie die Konfigurationsdatei der UFW-Kernel-Parameter in nano oder Ihrem bevorzugten Texteditor:
Fügen Sie nun am Ende der Datei die folgende Einstellung net / ipv4 / ip _ forward = 1 hinzu, um die Weiterleitung von Paketen zwischen Schnittstellen zu ermöglichen:
Blockieren Sie als Nächstes das Senden und Empfangen von ICMP-Umleitungspaketen, indem Sie die folgenden Zeilen am Ende der Datei hinzufügen:
Deaktivieren Sie abschließend die Path-MTU-Erkennung, indem Sie diese Zeile am Ende der Datei hinzufügen:
Speichern Sie die Datei, wenn Sie fertig sind.
Jetzt können wir alle unsere Änderungen durch Deaktivieren und erneutes Aktivieren der Firewall aktivieren, da UFW diese Einstellungen bei jedem Neustart anwendet:
Sie werden dazu aufgefordert, den Vorgang zu bestätigen.
Geben Sie Y ein, um UFW mit den neuen Einstellungen erneut zu aktivieren.
Schritt 7 - Testen der VPN-Verbindung unter Windows, macOS, Ubuntu, iOS und Android
Nachdem nun alles eingerichtet ist, ist es Zeit, es auszuprobieren. Zuerst müssen Sie das von Ihnen erstellte CA-Zertifikat kopieren und auf Ihrem / Ihren Client-Gerät (en) installieren, das / die sich mit Ihrem Server verbindet / verbinden.
Am einfachsten geht das, indem Sie sich bei Ihrem Server anmelden und den Inhalt der Zertifikatsdatei ausgeben:
Kopieren Sie diese Ausgabe auf Ihren Computer, einschließlich der Zeilen --BEGIN CERTIFICATE---- und --END CERTIFICATE--, und speichern Sie sie in einer Datei mit einem erkennbaren Namen wie ca-cert.pem.
Stellen Sie sicher, dass die von Ihnen erstellte Datei die Erweiterung .pem hat.
Alternativ können Sie SFTP verwenden, um die Datei auf Ihren Computer zu übertragen.
Sobald die Datei ca-cert.pem auf Ihren Computer heruntergeladen haben, können Sie die Verbindung zum VPN einrichten.
Herstellen der Verbindung von Windows
Es gibt mehrere Möglichkeiten, das Stammzertifikat zu importieren und Windows für die Verbindung mit einem VPN zu konfigurieren.
Die erste Methode verwendet grafische Tools für jeden Schritt.
Die zweite Methode verwendet PowerShell-Befehle, die in Skripts geschrieben und an Ihre VPN-Konfiguration angepasst werden können.
< $> note Anmerkung: Diese Anweisungen wurden in Windows 10-Installationen getestet, die die Versionen 1903 und 1909 ausführen.
Konfigurieren von Windows mit grafischen Tools
Importieren Sie zunächst das Stammzertifikat, indem Sie die folgenden Schritte ausführen:
Drücken Sie WINDOWS + R, um das Dialogfeld Ausführen aufzurufen, und geben Sie mmc.exe zum Starten der Windows Managementkonsole ein.
Navigieren Sie im Menü Datei zu Snap-In hinzufügen oder entfernen, wählen Sie aus der Liste der verfügbaren Snap-Ins Zertifikate aus und klicken Sie auf Hinzufügen.
Wir wollen, dass das VPN mit jedem Benutzer funktioniert, daher wählen wir Computerkonto und klicken auf Weiter.
Wir konfigurieren Dinge auf dem lokalen Computer, daher wählen wir Lokaler Computer und klicken auf Fertigstellen.
Erweitern Sie unter dem Knoten Konsolenstamm den Eintrag Zertifikate (Lokaler Computer), erweitern Sie Vertrauenswürdige Stammzertifizierungsstellen und wählen Sie dann den Eintrag Zertifikate: Zertifikatsansicht.
Wählen Sie im Menü Aktion Alle Aufgaben aus und klicken Sie auf Improtieren, um den Assistenten für den Zertifikatimport anzuzeigen.
Klicken Sie auf Weiter, um die Einführung zu überspringen.
Klicken Sie auf dem Bildschirm Zu importierende Datei die Schaltfläche Durchsuchen, stellen Sie sicher, dass Sie den Dateityp von "X.509-Zertifikat (.cer; .crt)" in "Alle Dateien (.)" ändern und wählen Sie die Datei ca-cert.pem, die Sie gespeichert haben.
Klicken Sie dann auf Weiter.
Stellen Sie sicher, dass der Zertifikatspeicher auf Vertrauenswürdige Stammzertifizierungsstellen eingestellt ist und klicken Sie auf Weiter.
Klicken Sie auf Fertigstellen, um das Zertifikat zu importieren.
Konfigurieren Sie das VPN dann mit diesen Schritten:
Starten Sie die Systemsteuerung, navigieren Sie dann zum Netzwerk- und Freigabecenter.
Klicken Sie auf Einrichten einer neuen Verbindung oder eines neuen Netzwerks und wählen Sie Mit einem Arbeitsplatz verbinden.
Wählen Sie Meine Internetverbindung (VPN) verwenden.
Geben Sie die Details des VPN-Servers ein.
Geben Sie den Domänennamen oder die IP-Adresse des Servers in das Feld Internetadresse ein und geben Sie dann den Zielnamen mit einer Beschreibung Ihrer VPN-Verbindung ein.
Klicken Sie dann auf Fertig.
Konfigurieren von Windows mit PowerShell
Um das Stamm-CA-Zertifikat mit PowerShell zu importieren, öffnen Sie zunächst eine PowerShell-Eingabeaufforderung mit Administratorberechtigungen.
Klicken Sie dazu mit der rechten Maustaste auf das Symbol im Startmenü und wählen Sie Windows PowerShell (Admin).
Sie können auch eine Eingabeaufforderung als Administrator öffnen und powershell eingeben.
Als Nächstes importieren wir das Zertifikat mit dem Powershell cmdlet Import-Certificate.
Im folgenden Befehl wird mit dem ersten Argument -CertStoreLocation sichergestellt, dass das Zertifikat in dem Speicher der Vertrauenswürdigen Stammzertifizierungsstelle des Computers importiert wird, damit alle Programme und Benutzer das Zertifikat des VPN-Servers überprüfen können.
Das Argument -FilePath sollte auf den Speicherort verweisen, in den Sie das Zertifikat kopiert haben.
Im folgenden Beispiel lautet der Pfad C:\ Users\ sammy\ Documents\ ca-cert.pem.
Stellen Sie sicher, dass Sie den Befehl so bearbeiten, dass er mit dem von Ihnen verwendeten Speicherort übereinstimmt.
Der Befehl gibt etwas wie das Folgende aus:
Um nun das VPN mit PowerShell zu konfigurieren, führen Sie den folgenden Befehl aus.
Ersetzen Sie den DNS-Namen oder die IP-Adresse Ihres Servers in der Zeile -Serveraddress.
Die verschiedenen Flags stellen sicher, dass Windows korrekt mit den entsprechenden Sicherheitsparametern konfiguriert wird, die den Optionen entsprechen, die Sie in / etc / ipsec.conf festgelegt haben.
Wenn der Befehl erfolgreich ist, gibt es keine Ausgabe.
Um zu bestätigen, dass das VPN richtig konfiguriert ist, verwenden Sie das cmdlet Get-VPNConnection:
Sie erhalten eine Ausgabe wie die folgende:
Standardmäßig wählt Windows ältere und langsamere Algorithmen.
Führen Sie das cmdlet Set-VpnConnectionIPsecConfiguration aus, um die Verschlüsselungsparameter zu aktualisieren, die Windows für den IKEv2-Schlüsselaustausch verwendet und um Pakete zu verschlüsseln:
< $> note Anmerkung: Wenn Sie die VPN-Verbindung löschen und mit verschiedenen Optionen neu konfigurieren möchten, können Sie das cmdlet Remove-VpnConnection ausführen.
Das Flag -Force wird übersprungen und Sie werden aufgefordert, das Entfernen zu bestätigen.
Wenn Sie versuchen, die VPN-Verbindung mit diesem Befehl zu entfernen, müssen Sie vom VPN getrennt sein.
Herstellen einer Verbindung mit dem VPN
Sobald Sie das Zertifikat importiert und das VPN mit einer der beiden Methoden konfiguriert haben, wird Ihre neue VPN-Verbindung unter der Liste der Netzwerke sichtbar.
Wählen Sie das VPN und klicken Sie auf Verbinden.
Sie werden zur Eingabe Ihres Benutzernamens und Ihres Passworts aufgefordert.
Geben Sie sie ein, klicken Sie auf OK, und Sie werden verbunden.
Herstellen einer Verbindung von macOS
Folgen Sie diesen Schritten, um das Zertifikat zu importieren:
Doppelklicken Sie auf die Zertifikatsdatei.
Keychain Access wird mit einem Dialogfeld eingeblendet, in dem es heißt "Keychain Access is trying to modify the system keychain.
Enter your password to allow this. "(Keychain Access versucht die System-Keychain zu ändern. Geben Sie Ihr Passwort ein, um dies zuzulassen).
Geben Sie Ihr Passwort ein, klicken Sie dann auf Modify Keychain (Keychain ändern)
Doppelklicken Sie auf das neu importierte VPN-Zertifikat.
Dadurch wird ein kleines Eigenschaftenfenster eingeblendet, in dem Sie die Vertrauensstufen angeben können.
Setzen Sie IP Security (IPSec) auf Always Trust (Immer vertrauen) und Sie werden erneut zur Eingabe Ihres Passworts aufgefordert.
Diese Einstellung wird nach der Eingabe des Passworts automatisch gespeichert.
Nachdem das Zertifikat importiert und vertrauenswürdig ist, konfigurieren Sie die VPN-Verbindung nun mit diesen Schritten:
Gehen Sie zu Systemeinstellungen und wählen Sie Netzwerk.
Klicken Sie auf die kleine "Plus" -Schaltfläche unten links in der Liste der Netzwerke.
Stellen Sie in dem angezeigten Popup-Fenster Schnittstelle auf VPN, setzen Sie den VPN-Typ auf IKEv2 und geben Sie der Verbindung einen Namen.
Geben Sie im Feld Server und Remote ID den Domänennamen oder die IP-Adresse des Servers ein.
Lassen Sie Lokale ID leer.
Klicken Sie auf Authentifizierungseinstellungen, wählen Sie Bentuzername und geben Sie Ihren Benutzernamen und Ihr Passwort ein, das Sie für Ihren VPN-Benutzer konfiguriert haben.
Klicken Sie dann auf OK.
Klicken Sie abschließend auf Verbinden, um sich mit dem VPN zu verbinden.
Sie sollten nun mit dem VPN verbunden sein.
Herstellen einer Verbindung von Ubuntu
Um eine Verbindung von einem Ubuntu-Rechner herzustellen, können Sie StrongSwan als Dienst einrichten und verwalten oder bei jeder gewünschten Verbindung einen einmaligen Befehl verwenden.
Für beides stehen Anweisungen zur Verfügung.
Verwalten von StrongSwan als Dienst
Um StrongSwan als Dienst zu verwalten, müssen Sie die folgenden Konfigurationsschritte durchführen.
Aktualisieren Sie zunächst Ihren lokalen Paket-Cache mit apt
Installieren Sie als Nächstes StrongSwan und die erforderlichen Plugins für die Authentifizierung:
Jetzt benötigen Sie im Verzeichnis / etc / ipsec.d / cacerts eine Kopie des CA-Zertifikats, damit Ihr Client die Identität des Servers überprüfen kann.
Führen Sie den folgenden Befehl aus, um die Datei ca-cert.pem in den richtigen Speicherort zu kopieren:
Um sicherzustellen, dass das VPN nur bei Bedarf ausgeführt wird, deaktivieren Sie mit systemctl die automatische Ausführung von StrongSwan:
Konfigurieren Sie als Nächstes den Benutzernamen und das Passwort, das Sie zur Authentifizierung auf dem VPN-Server verwenden werden.
Bearbeiten Sie / etc / ipsec.secrets mit nano oder Ihrem bevorzugten Editor:
Fügen Sie die folgende Zeile hinzu und bearbeiten Sie die hervorgehobenen Werte für Benutzername und Passwort, damit sie mit denen übereinstimmen, die Sie auf dem Server konfiguriert haben:
Bearbeiten Sie abschließend die Datei / etc / ipsec.conf, um Ihren Client so zu konfigurieren, dass er mit der Konfiguration des Servers übereinstimmt:
Um sich mit dem VPN zu verbinden, geben Sie Folgendes ein:
Um sich wieder zu trennen, geben Sie Folgendes ein:
Verwenden des Clients charon-cmd für einmalige Verbindungen
Zu diesem Zeitpunkt können Sie sich mit charon-cmd unter Verwendung des CA-Zertifikats des Servers, der IP-Adresse des VPN-Servers und des von Ihnen konfigurierten Benutzernamens mit dem VPN-Server verbinden.
Führen Sie den folgenden Befehl aus, wann immer Sie eine Verbindung mit dem VPN herstellen möchten:
Geben Sie, wenn Sie dazu aufgefordert werden, das Passwort des VPN-Benutzers ein und Sie werden mit dem VPN verbunden.
Um die Verbindung zu trennen, drücken Sie im Terminal Strg + C und warten Sie, bis die Verbindung geschlossen wird.
Herstellen einer Verbindung von iOS
Führen Sie die folgenden Schritte aus, um die VPN-Verbindung auf einem iOS-Gerät zu konfigurieren:
Senden Sie sich selbst eine E-Mail mit dem Stammzertifikat im Anhang.
Öffnen Sie die E-Mail auf Ihrem iOS-Gerät und tippen Sie auf die angehängte Zertifikatdatei, tippen Sie dann auf Installieren und geben Sie Ihren Passcode ein.
Sobald diese installiert ist, tippen Sie auf Fertig.
Gehen Sie zu Einstellungen, Allgemein, VPN und tippen Sie auf VPN-Konfiguration hinzufügen.
Daraufhin wird der Bildschirm zur Konfiguration der VPN-Verbindung angezeigt.
Tippen Sie auf Typ und wählen Sie IKEv2.
Geben Sie im Feld Beschreibung einen kurzen Namen für die VPN-Verbindung ein.
Dieser Name kann beliebig sein.
Das Feld Lokale ID kann leer gelassen werden.
Geben Sie Ihren Benutzernamen und Ihr Passwort in den Abschnitt Authentifizierung ein, und tippen Sie dann auf Fertig.
Wählen Sie die gerade erstellte VPN-Verbindung, tippen Sie auf die Schaltfläche oben auf der Seite und schon sind Sie verbunden.
Herstellen einer Verbindung von Android
Senden Sie sich selbst eine E-Mail mit dem CA-Zertifikat im Anhang.
Speichern Sie das CA-Zertifikat in Ihrem Download-Ordner.
Laden Sie den StrongSwan VPN-Client aus dem Play-Store herunter.
Öffnen Sie die App. Tippen Sie auf das Symbol "Mehr" (...) in der rechten oberen Ecke und wählen Sie CA-Zertifikate.
Tippen Sie erneut auf das Symbol "Mehr" (...) in der rechten oberen Ecke.
Wählen Sie Zertifikat importieren.
Navigieren Sie zu der CA-Zertifikatsdatei in Ihrem Donwload-Ordner und wählen Sie sie aus, um sie in die Anwendung zu importieren.
Nachdem das Zertifikat nun in die StrongSwan-Anwendung importiert wurde, können Sie die VPN-Verbindung mit diesen Schritten konfigurieren:
Tippen Sie oben in der App auf VPN-PROFIL HINZUFÜGEN.
Geben Sie bei Server den Domänennamen oder die öffentliche IP-Adresse Ihres VPN-Servers ein.
Stellen Sie sicher, dass IKEv2 EAP (Benutzername / Passwort) als VPN-Typ ausgewählt ist.
Geben Sie bei Benutzername und Passwort die Anmeldedaten ein, die Sie auf dem Server definiert haben.
Deaktivieren Sie Automatisch auswählen im Abschnitt CA-Zertifikat und klicken Sie auf CA-Zertifikat auswählen.
Tippen Sie auf die Registerkarte IMPORTIERT oben auf dem Bildschirm und wählen Sie die von Ihnen importierte CA (sie heißt "VPN root CA" wenn Sie den "DN" nicht zuvor geändert haben).
Wenn Sie möchten, können Sie den Profilnamen (optional) mit einem aussagekräftigeren Namen ausfüllen.
Wenn Sie eine Verbindung mit dem VPN herstellen möchten, klicken Sie auf das Profil, das Sie gerade in der Anwendung StrongSwan erstellt haben.
Fehlerbehebung bei Verbindungen
Wenn Sie das Zertifikat nicht importieren können, stellen Sie sicher, dass die Datei die Erweiterung .pem und nicht .pem.txt hat.
Wenn Sie keine Verbindung mit dem VPN herstellen können, überprüfen Sie den von Ihnen verwendeten Servernamen oder die IP-Adresse.
Der Domänenname oder die IP-Adresse des Servers muss mit dem übereinstimmen, was Sie bei der Erstellung des Zertifikats als Common Name (CN) konfiguriert haben.
Wenn sie nicht übereinstimmen, funktioniert die VPN-Verbindung nicht.
Wenn Sie beispielsweise ein Zertifikat mit dem CN von vpn.example.com einrichten, müssen Sie vpn.example.com verwenden, wenn Sie die VPN-Serverdetails eingeben.
Überprüfen Sie den Befehl, den Sie zum Generieren des Zertifikats verwendet haben, und die Werte, die Sie bei der Erstellung Ihrer VPN-Verbindung verwendet haben.
Abschließend überprüfen Sie die VPN-Konfiguration, um sicherzustellen, dass der Wert leftid mit dem Symbol @ konfiguriert ist, wenn Sie einen Domänennamen verwenden:
Wenn Sie eine IP-Adresse verwenden, stellen Sie sicher, dass das Symbol @ ausgelassen ist.
Stellen Sie außerdem sicher, dass Sie beim Generieren der Datei server-cert.pem die beiden Flags -san @ < ^ > IP _ address < ^ > und -san < ^ > IP _ address < ^ > eingefügt haben.
In diesem Tutorial haben Sie einen VPN-Server erstellt, der das IKEv2-Protokoll verwendet.
Sie haben sich mit den Anweisungen vertraut gemacht, die die linke und die rechte Seite einer Verbindung sowohl auf dem Server als auch auf den Clients steuern.
Außerdem haben Sie einen Windows-, macOS-, iOS-, Android- oder Linux-Client für die Verbindung mit dem VPN konfiguriert.
Um Benutzer hinzuzufügen oder zu entfernen, führen Sie erneut Schritt 5 aus.
Jede Zeile in / etc / ipsec.secrets ist für einen Benutzer, sodass das Hinzufügen oder Entfernen von Benutzern oder das Ändern von Passwörtern nur die Bearbeitung der Datei erfordert.
Jetzt können Sie sicher sein, dass Ihre Online-Aktivitäten überall und mit jedem Gerät, das Sie für den Zugang zum Internet benutzen, geschützt bleiben.
