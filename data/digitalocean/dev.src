How To Build Custom Pagination with React
A method for handling large datasets on the view is by using the infinite scrolling technique - where more data is loaded in chunks as the user continues scrolling very close to the end of the page.
This is the technique used in displaying search results in Google Images.
In this tutorial, we will see how to build a custom pagination component with React for paginating large datasets.
We will build a paginated view of the countries in the world.
3642
We often get involved in building web applications where we need to fetch large sets of data records from a remote server, API, or a database.
If you are building a payment system, for example, it could be fetching thousands of transactions.
If it is a social media app, it could be fetching many user comments, profiles, or activities.
Whatever the case may be, there are several solutions for presenting the data in a manner that does not overwhelm the end-user interacting with the app.
One method for handling large datasets is using pagination.
Pagination works effectively when you already know the size of the dataset (the total number of records in the dataset) upfront.
Secondly, you only load the required chunk of data from the total dataset based on the end-users interaction with the pagination control.
This is the technique used in displaying search results in Google Search.
In this tutorial, you will learn how to build a custom pagination component with React for paginating large datasets.
You will build a paginated view of the countries in the world - a dataset with a known size.
Here is a demo of what you will be building in this tutorial:
Demo App Screenshot - showing the countries of the world
Node installed on your machine.
Steps can be found at How to Install Node.js and Create a Local Development Environment.
The create-react-app command-line package to create the boilerplate code for your React app. If you are using npm < 5.2, then you may need to install create-react-app as a global dependency.
Finally, this tutorial assumes that you are already familiar with React.
If that is not the case, you can check out the How To Code in React.js series to learn more about React.
This tutorial was verified with Node v14.2.0, npm v6.14.4, react v16.13.1, and react-scripts v3.4.1.
Step 1 - Setting Up the Project
Start a new React application using the create-react-app command.
You can name the application whatever you'd like, but this tutorial will name it react-pagination:
Next, you will install the dependencies needed for your application.
First, use the terminal window to navigate to the project directory:
Run the following command to install the required dependencies:
This will install bootstrap, prop-types, react-flags, countries-api, and node-sass.
You installed the bootstrap package as a dependency for your application since you will need some default styling.
You will also use styles from the Bootstrap pagination component.
To include Bootstrap in the application, edit the src / index.js file:
And add the following line before the other import statements:
Now, Bootstrap styling will be available throughout your application.
You also installed react-flags as a dependency for your application.
In order to get access to the flag icons from your application, you will need to copy the icon images to the public directory of your application.
Create an img directory in your public directory:
Copy the image files in flags to img:
This provides a copy of all the react-flag images to your application.
Now that you've included some dependencies, start the application by running the following command with npm from the react-pagination project directory:
Now that you've started the application, development can begin.
Notice that a browser tab has been opened for you with live reloading functionality to keep in sync with the application as you develop.
At this point, the application view should look like the following screenshot:
Initial View - Welcome to React Screen
You're now ready to start creating components.
Step 2 - Creating the CountryCard Component
In this step, you will create the CountryCard component.
The CountryCard component renders the name, region, and flag of a given country.
First, let's create a components directory in the src directory:
Then, create a new CountryCard.js file in the src / components directory:
And add the following code snippet to it:
The CountryCard component requires a country prop that contains the data about the country to be rendered.
As seen in the propTypes for the CountryCard component, the country prop object must contain the following data:
cca2 - 2-digit country code
region - the country region (e.g., "Africa ")
name.common - the common name of the country (e.g., "Nigeria ")
Here is a sample country object:
Also, notice how you render the country flag using the react-flags package.
You can check the react-flags documentation to learn more about the required props and how to use the package.
You now have a completed an individual CountryCard component.
Ultimately, you will be using CountryCards multiple times to display different flags and country information in your application.
Step 3 - Creating the Pagination Component
In this step, you will create the Pagination component.
The Pagination component contains the logic for building, rendering, and switching pages on the pagination control.
Create a new Pagination.js file in the src / components directory:
The Pagination component can take four special props as specified in the propTypes object.
onPageChanged is a function called with data of the current pagination state only when the current page changes.
totalRecords indicates the total number of records to be paginated.
It is required.
pageLimit indicates the number of records to be shown per page.
If not specified, it defaults to 30 as defined in the constructor ().
pageNeighbours indicates the number of additional page numbers to show on each side of the current page.
The minimum value is 0, and the maximum value is 2. If not specified, it defaults to 0 as defined in the constructor ().
The following image illustrates the effect of different values of the pageNeighbours prop:
Page Neighbours Illustration
In the constructor () function, you compute the total pages as follows:
Notice that you use Math.ceil () here to ensure that you get an integer value for the total number of pages.
This also ensures that the excess records are captured in the last page, especially in cases where the number of excess records is less than the number of records to be shown per page.
Finally, you initialized the state with the currentPage property set to 1. You need this state property to keep track of the currently active page internally.
Next, you will create the method for generating the page numbers.
After the imports but before the Pagination class, add the following constants and range function:
In the Pagination class, after the constructor, add the following fetchPageNumbers method:
Here, you first define two constants: LEFT _ PAGE and RIGHT _ PAGE.
These constants will be used to indicate points where you have page controls for moving left and right, respectively.
You also defined a helper range () function that can help you generate ranges of numbers.
The following code snippet shows the difference between the range () function you just defined and the one from Lodash:
Next, you defined the fetchPageNumbers () method in the Pagination class.
This method handles the core logic for generating the page numbers to be shown on the pagination control.
You want the first page and the last page to always be visible.
First, you defined a couple of variables. totalNumbers represents the total page numbers that will be shown on the control. totalBlocks represents the total page numbers to be shown plus two additional blocks for the left and right page indicators.
If totalPages is not greater than totalBlocks, you return a range of numbers from 1 to totalPages.
Otherwise, you return the array of page numbers, with LEFT _ PAGE and RIGHT _ PAGE at points where you have pages spilling to the left and right, respectively.
However, notice that your pagination control ensures that the first page and last page are always visible.
The left and right page controls appear inwards.
Now, you will add the render () method to enable you to render the pagination control.
In the Pagination class, after the constructor and fetchPageNumbers method, add the following render method:
Here, you generate the page numbers array by calling the fetchPageNumbers () method you created earlier.
You then render each page number using Array.prototype.map ().
Notice that you register click event handlers on each rendered page number to handle clicks.
Also, notice that the pagination control will not be rendered if the totalRecords prop was not correctly passed to the Pagination component or in cases where there is only 1 page.
Finally, you will define the event handler methods.
In the Pagination class, after the constructor and fetchPageNumbers method and render method, add the following:
You define the gotoPage () method that modifies the state and sets the currentPage to the specified page.
It ensures that the page argument has a minimum value of 1 and a maximum value of the total number of pages.
It finally calls the onPageChanged () function that was passed in as a prop, with data indicating the new pagination state.
Notice how you use (this.pageNeighbours * 2) in handleMoveLeft () and handleMoveRight () to slide the page numbers to the left and to the right respectively based on the current page number.
Here is a demo of the interaction of the left to right movement.
Left-Right Movement of the interaction
You now have completed Pagination component.
Users will be able to interact with the navigation controls in this component to display different pages of flags.
Step 4 - Building the App Component
Now that you have a CountryCard and Pagination component, you can use them in your App component.
Modify the App.js file in the src directory:
Replace the contents of App.js with the following lines of code:
Here you initialize the App component's state with the following attributes:
allCountries - This an array of all the countries in your app. Initialized to an empty array ([]).
currentCountries - This an array of all the countries to be shown on the currently active page.
Initialized to an empty array ([]).
currentPage - The page number of the currently active page.
Initialized to null.
totalPages - The total number of pages for all the country records.
Next, in the componentDidMount () lifecycle method, you fetch all the world countries using the countries-api package by invoking Countries.findAll ().
You then update the app state, setting allCountries to contain all the world countries.
You can see the countries-api documentation to learn more about the package.
Finally, you defined the onPageChanged () method, which will be called each time you navigate to a new page from the pagination control.
This method will be passed to the onPageChanged prop of the Pagination component.
There are two lines that are worth paying attention to in this method.
The first is this line:
The offset value indicates the starting index for fetching the records for the current page.
Using (currentPage - 1) ensures that the offset is zero-based.
Let's say, for example, that you are displaying 25 records per page, and you are currently viewing page 5. Then the offset will be ((5 - 1) * 25 = 100).
For example, if you are fetching records on demand from a database, this is a sample SQL query to show you how offset can be used:
Since you are not fetching records from a database or any external source, you need a way to extract the required chunk of records to be shown for the current page.
The second is this line:
Here you use the Array.prototype.slice () method to extract the required chunk of records from allCountries by passing the offset as the starting index for the slice and (offset + pageLimit) as the index before which to end the slice.
Note: In this tutorial, you did not fetch records from any external source.
In a real application, you will probably be fetching records from a database or an API.
The logic for fetching the records can go into the onPageChanged () method of the App component.
Let's say you have a fictitious API endpoint / api / countries? page = {current _ page} & limit = {page _ limit}.
The following snippet shows how you can fetch countries on demand from the API using the axios HTTP package:
Now, you can finish up the App component by adding the render () method.
In the App class, but after componentDidMount and onPageChanged, add the following render method:
In the render () method, you render the total number of countries, the current page, the total number of pages, the < Pagination > control, and then the < CountryCard > for each country in the current page.
Notice that you passed the onPageChanged () method you defined earlier to the onPageChanged prop of the < Pagination > control.
This is very important for capturing page changes from the Pagination component.
You are also displaying 18 countries per page.
At this point, the app will look like the following screenshot:
App Screenshot with 248 countries listed and page numbers to the top to go through each page
You now have an App component that displays multiple CountryCard components and a Pagination component that breaks up the content into separate pages.
Next you will explore styling your application.
Step 5 - Adding Custom Styles
You may have noticed that you have been adding some custom classes to the components you created earlier.
Let's define some style rules for those classes in the src / App.scss file.
The App.scss file will look like the following snippet:
Modify your App.js file to reference App.scss instead of App.css.
Note: For more information about this, see the Create React App documentation.
After adding the styles, the app will now look like the following screenshot:
App Screenshot page 1 of 14, with Styles
You now have a complete application with additional custom styling.
You can use custom styles to modify and enhance any default stylings provided by libraries like Bootstrap.
In this tutorial, you created a custom pagination widget in your React application.
Although you didn't make calls to any API or interact with any database backend in this tutorial, your application may demand such interactions.
You are not in any way limited to the approach used in this tutorial - you can extend it as you wish to suit the requirements of your application.
You can also get a live demo of this tutorial on Code Sandbox.
How To Implement Smooth Scrolling in React
What is smooth scrolling?
Instead of clicking on a button and being instantly taken to a different part of the (same) page, the user is navigated there via a scroll animation.
It's one of those subtle features on a site that makes an aesthetic difference.
3602
Smooth scrolling is when instead of clicking on a button and being instantly taken to a different part of the same page, the user is navigated there via a scroll animation.
It's one of those subtle UI features on a site that makes an aesthetic difference.
In this article, you are going to use the react-scroll package on npm to implement smooth scrolling.
You will need the following to complete this tutorial:
A valid Git installation.
To set this up, review the Getting Started with Git tutorial.
Node.js installed locally, which you can do by following the How to Install Node.js and Create a Local Development Environment tutorial.
This tutorial was verified with Node v13.14.0, npm v6.14.5, react v16.13.1, and react-scroll v.1.7.16.
Quickstart: Using react-scroll
You'll be building a simple app in this tutorial, but if you want a quick rundown of how react-scroll works, feel free to reference these condensed steps:
Install react-scroll:
Import the react-scroll package:
Add the link component.
The < Link / > component will point to a certain area of your app:
Let's take a deeper dive and build a little React app with smooth scrolling.
Step 1 - Install and Run a React App
For convenience, this tutorial will use a starter React project (using Create React App 2.0) that has a navigation bar (or navbar) at the top along with five different sections of content.
The links in the navbar are just anchor tags at this point, but you will update them shortly to enable smooth scrolling.
You can find the project at React With Smooth Scrolling.
Please note that this link is for the start branch.
The master branch includes all of the finished changes.
GitHub Repo Screenshot
To clone the project, you can use the following command:
If you look into the src / Components directory, you'll find a Navbar.js file that contains the < Navbar > with nav-items corresponding to five different < Section > s.
Then, if you open up the App.js file in the src directory, you'll see where the < Navbar > is included along with the five actual < Section > s "
Each < Section > component takes in a title and subtitle.
Since this project is using dummy text in the different sections, to reduce repeating code this text was added to a DummyText.js file, imported, and passed into each < Section > component.
To run the app, you can use the following commands.
This will start the app in development mode and automatically refresh the app when you save on of your files.
You can view it in the browser at localhost: 3000.
Screenshot of app in browser
Step 2 - Installing and Configuring React-Scroll
Now it's time to install the react-scroll package and add that functionality.
You can find information for the package on npm.
react-scroll package on npm
To install the package, run the following command:
Next, open the Navbar.js file back up and add an import for two named imports, Link and animateScroll.
Notice that I've aliased animatedScroll to scroll for ease of use.
With all of your imports defined, you can now update your nav-items to use the < Link > component.
This component takes several properties.
You can read about all of them on the documentation page.
For now, pay special attention to activeClass, to, spy, smooth, offset, and duration.
activeClass - The class applied when element is reached.
to - The target to scroll to.
smooth - To animate the scrolling.
offset - To scroll additional px (like padding).
duration - The time of the scroll animation.
This can be a number or a function.
The to property is the most important part as it tells the component which element to scroll to.
In this case, this will be each of your < Section > s.
With the offset property, you can define an additional amount of scrolling to perform to get to each < Section >.
Here's an example of the properties that you will use for each < Link > component.
The only difference between them will be the to property as they each point to a different < Section >:
You'll need to update each of the nav-items accordingly.
With these added, you should be able to go back to your browser (your app should have automatically restarted already) and see smooth scrolling in action.
Step 3 - Styling Active Links
The activeClass property allows you to define a class to apply to the < Link > component when its to element is active.
A < Link > is considered active if its to element is in view near the top of the page.
This can be triggered by clicking on the < Link > itself or by scrolling down to the < Section > manually.
To prove this, I opened up the Chrome DevTools and inspected the fifth < Link > as shown below.
When I clicked on that < Link > or manually scrolled to the bottom of the page, I noticed that the active class is, in fact, applied.
Browser view of React app
To take advantage of this, you can create an active class and add an underline to the link.
You can add this bit of CSS in the App.css file in the src directory:
Now, if you go back to your browser and scroll around a bit, you should see the appropriate < Link > is underlined.
Updated browser view of React app
Step 4 - Adding Additional Functions
For one last bit of content, this package also provides some functions that can be called directly like scrollToTop, scrollToBottom, etc. As well as various events that you can handle.
In reference to these functions, typically, the application logo in a navbar will bring the user to the home page or the top of the current page.
As a simple example of how to call one of these provided functions, I added a click handler to the nav-logo to scroll the user back to the top of the page, like so:
Back in the browser, you should be able to scroll down on the page, click the logo in the navbar, and be taken back to the top of the page.
Smooth scrolling is one of those features that can add a lot aesthetic value to your application.
The react-scroll package allow you to leverage this feature without significant overhead.
In this tutorial, you added smooth scrolling to an app and experimented with different settings.
If you're curious, spend some time exploring the other functions and events that this package has to offer.
filter () Array Method in JavaScript
Use filter () on arrays to go through an array and return a new array with the elements that pass the filtering rules.
4539
The filter () Array method creates a new array with elements that fall under a given criteria from an existing array:
The example above takes the numbers array and returns a new filtered array with only those values that are greater than seven.
Filter syntax
The < ^ > item < ^ > argument is a reference to the current element in the array as filter () checks it against the < ^ > condition < ^ >.
This is useful for accessing properties, in the case of objects.
If the current < ^ > item < ^ > passes the condition, it gets sent to the new array.
Filtering an array of objects
A common use case of .filter () is with an array of objects through their properties:
Additional Resources
For more details on filter () see MDN Reference.
Filter is only one of several iteration methods on Arrays in JavaScript, read How To Use Array Iteration Methods in JavaScript to learn about the other methods like map () and reduce ().
How To Restart Your Node.js Apps Automatically with nodemon
In Node.js, you need to restart the process to make changes take effect.
This adds an extra step to your workflow to have the changes take place.
You can eliminate this extra step by using nodemon to restart the process automatically.
In this article, you will learn about installing, setting up, and configuring nodemon.
4931
nodemon is a command-line interface (CLI) utility developed by @ rem that wraps your Node app, watches the file system, and automatically restarts the process.
If you would like to follow along with this article, you will need:
Node.js installed locally, which you can do by following How to Install Node.js and Create a Local Development Environment.
Step 1 - Installing nodemon
First, you will need to install nodemon on your machine.
Install the utility either globally or locally on your project using npm or Yarn:
Global Installation
You can install nodemon globally with npm:
Or with Yarn:
Local Installation
You can also install nodemon locally with npm.
When performing a local installation, we can install nodemon as a dev dependency with --save-dev (or --dev):
One thing to be aware of with a local install is you won't be able to use the nodemon command directly from the command line:
However, you can use it as part of some npm scripts or with npx.
This concludes the nodemon installation process.
Next, we will use nodemon with our projects.
Step 2 - Setting Up an Example Express Project with nodemon
We can use nodemon to start a Node script.
For example, if we have an Express server setup in a server.js file, we can start it and watch for changes like this:
You can pass in arguments the same way as if you were running the script with Node:
Every time you make a change to a file with one of the default watched extensions (.js, .mjs, .json, .coffee, or .litcoffee) in the current directory or a subdirectory, the process will restart.
Let's assume we write an example server.js file that outputs the message: Dolphin app listening on port ${port}!.
We can run the example with nodemon:
We see the following terminal output:
While nodemon is still running, let's make a change to the server.js file to output the message: < ^ > Shark < ^ > app listening on port ${port}!.
We see the following additional terminal output:
The terminal output from our Node.js app is displaying as expected.
You can restart the process at any time by typing rs and hitting ENTER.
Alternatively, nodemon will also look for a main file specified in your project's package.json file:
Or, a start script:
Once you make the changes to package.json, you can then call nodemon to start the example app in watch mode without having to pass in server.js.
Step 3 - Using Options
You can modify the configuration settings available to nodemon.
Let's go over a few of the main options:
--exec: Use the --exec switch to specify a binary to execute the file with.
For example, when combined with the ts-node binary, --exec can become useful to watch for changes and run TypeScript files.
--ext: Specify different file extensions to watch.
For this switch, provide a comma-separated list of file extensions (e.g., --ext js, ts).
--delay: By default, nodemon waits for one second to restart the process when a file changes, but with the --delay switch, you can specify a different delay.
For example, nodemon --delay 3.2 for a 3.2-second delay.
--watch: Use the --watch switch to specify multiple directories or files to watch.
Add one --watch switch for each directory you want to watch.
By default, the current directory and its subdirectories are watched, so with --watch you can narrow that to only specific subdirectories or files.
--ignore: Use the --ignore switch to ignore certain files, file patterns, or directories.
--verbose: A more verbose output with information about what file (s) changed to trigger a restart.
You can view all the available options with the following command:
Using these options, let's create the command to satisfy the following scenario:
watching the server directory
specifying files with a .ts extension
ignoring files with a .test.ts suffix
executing the file (server / server.ts) with ts-node
waiting for three seconds to restart after a file changes
This command combines --watch, --ext, --exec, --ignore, and --delay options to satisfy the conditions for our scenario.
Step 4 - Using Configurations
In the previous example, adding configuration switches when running nodemon can get quite tedious.
A better solution for projects that need specific configurations is to specify these configs in a nodemon.json file.
For example, here are the same configurations as the previous command line example, but placed in a nodemon.json file:
Note the use of execMap instead of the --exec switch. execMap allows you to specify binaries that should be used given certain file extensions.
Alternatively, if you'd rather not add a nodemon.json config file to your project, you can add these configurations to the package.json file under a nodemonConfig key:
Once you make the changes to either nodemon.json or package.json, you can then start nodemon with the desired script:
nodemon will pick up the configurations and use them.
This way, your configurations can be saved, shared, and repeated to avoid copy-and-pasting or typing errors in the command line.
In this article, you explored how to use nodemon with your Node.js applications.
This tool helps automate the process of stopping and starting a Node server to view the changes.
For more information about the available features and troubleshooting errors, consult the official documentation.
If you'd like to learn more about Node.js, check out our Node.js topic page for exercises and programming projects.
How To Create a Parallax Scrolling Effect with Pure CSS in Chrome
In this guide, you will set up a few CSS lines to create a scrolling parallax effect on a web page.
4882
Modern CSS is a powerful tool you can use to create many advanced User Interface (UI) features.
In the past, these features relied on JavaScript libraries.
You will use images from placekitten.com as placeholder background images.
You will have a webpage with a pure CSS scrolling parallax effect once you've completed the tutorial.
Warning: This article uses experimental CSS properties that do not work across browsers.
This project has been tested and works on Chrome.
This technique doesn "t work well in Firefox, Safari, and iOS due to some of those browsers' optimizations.
Step 1 - Creating a New Project
In this step, use the command line to set up a new project folder and files.
To start, open your terminal and create a new project folder.
Type the following command to create the project folder:
In this case, you called the folder css-parallax.
Now, change into the css-parallax folder:
Next, create an index.html file in your css-parallax folder with the nano command:
You will put all the HTML for the project in this file.
In the next step, you will start creating the structure of the webpage.
Step 2 - Setting Up the Application Structure
In this step, you will add the HTML needed to create the structure of the project.
Inside your index.html file add the following code:
This is the basic structure of most webpages that use HTML.
Add the following code inside the < body > tag:
This code creates three different sections.
Two will have a background image, and one will be a static, plain background.
In the next few steps, you will add the styles for each section using the classes you added in the HTML.
Step 3 - Creating a CSS File and Adding Initial CSS
In this step, you will create a CSS file.
Then you will add in the initial CSS needed to style the website and create the parallax effect.
First, create a styles.css file in your css-parallax folder with the nano command:
This is where you will put all of the CSS needed to create the parallax scrolling effect.
Next, start with the .wrapper class.
Inside your styles.css file add the following code:
The .wrapper class sets the perspective and scroll properties for the whole page.
The height of the wrapper needs to be set to a fixed value for the effect to work.
You can use the viewport unit vh set to 100 to get the full height of the screen's viewport.
When you scale the images, they will add a horizontal scrollbar to the screen, so you can disable it by adding overflow-x: hidden;.
The perspective property simulates the distance from the viewport to the pseudo-elements you will create and transform further down in the CSS.
In the next step, you will add more CSS to style your webpage.
Step 4 - Adding Styles for the .section Class
In this step, you will add styles to the .section class.
Inside your styles.css file add the following code below the wrapper class:
The .section class defines the size, display, and text properties for the main sections.
Set a position of relative so that the child, .parallax:: after can be absolutely positioned relative to the parent element .section.
Each section has a view-height (vh) of 100 to take up the viewport's full height.
This value can be changed and set to whatever height you prefer for each section.
Finally, the remainder CSS properties are used to format and add styling to the text inside each section.
It positions the text in the center of each section and adds a color of white.
Next, you will add a pseudo-element and style it to create the parallax effect on two of the sections in your HTML.
Step 5 - Adding Styles for the .parallax Class
In this step, you will add the styles to the .parallax class.
First, you will add a pseudo-element on the .parallax class to be styled.
Note: You can visit MDN web docs to learn more about CSS pseudo-elements.
Add the following code below the .section class:
The.parallax class adds an:: after pseudo-element to the background image and provides the transforms needed for the parallax effect.
The pseudo-element is the last child of the element with the class .parallax.
The first half of the code displays and positions the psuedo-element.
The transform property moves the pseudo-element back away from the camera on the z-index, then scales it back up to fill the viewport.
Because the pseudo-element is further away, it appears to move more slowly.
In the next step, you will add in the background images and static background style.
Step 6 - Adding the Images and Background For Each Section
In this step, you will add the final CSS properties to add in the background images and background color of the static section.
First, add a solid background color to the .static section with the following code after the .parallax:: after class:
The .static class adds a background to the static section that does not have an image.
The two sections with the .parallax class also have an extra class that is different for each.
Use the .bg1 and .bg2 classes to add the Kitten background images.
Add the following code to the .static class:
The .bg1, .bg2 classes add the respective background images for each section.
The images are from the placekitten website.
It is a service for getting pictures of kittens for use as placeholders.
Now that all of the code for the parallax scrolling effect is added, you can link to your styles.css file in your index.html.
Step 7 - Linking styles.css and Opening index.html in Your Browser
In this step, you will link your styles.css file and open up the project in your browser to see the parallax scrolling effect.
First, add the following code to the < head > tag in the index.html file:
Now, you can open your index.html file in your browser:
Scrolling parallax effect gif
With that, you have set up a functioning webpage with a scrolling effect.
Check out this GitHub repository to see the full code.
In this article, you set up a project with an index.html and styles.css file and now have a functional webpage.
You added in the structure of your webpage and created styles for the various sections on the site.
It "s possible to put the images you use or the parallax effect further away so that they move more slowly.
You "ll have to change the pixel amount on perspective and the transform properties.
If you don "t want a background image to scroll at all, use background-attachment: fixed; instead of perspective / translate / scale.
How To Use SSH to Connect to a Remote Server
SSH is an important tool used for administering remote Linux servers.
In this guide, we will discuss the basic usage of this utility and how to configure your SSH environment.
579
One essential tool to master as a system administrator is SSH.
SSH, or Secure Shell, is a protocol used to securely log onto remote systems.
It is the most common way to access remote Linux servers.
In this guide, we will discuss how to use SSH to connect to a remote system.
Basic Syntax
To connect to a remote system using SSH, we'll use the ssh command.
The most basic form of the command is:
The < ^ > remote _ host < ^ > in this example is the IP address or domain name that you are trying to connect to.
This command assumes that your username on the remote system is the same as your username on your local system.
If your username is different on the remote system, you can specify it by using this syntax:
Once you have connected to the server, you may be asked to verify your identity by providing a password.
Later, we will cover how to generate keys to use instead of passwords.
To exit the ssh session and return back into your local shell session, type:
How Does SSH Work?
SSH works by connecting a client program to an ssh server, called sshd.
In the previous section, ssh was the client program.
The ssh server is already running on the remote _ host that we specified.
On your server, the sshd server should already be running.
If this is not the case, you may need to access your server through a web-based console, or local serial console.
The process needed to start an ssh server depends on the distribution of Linux that you are using.
On Ubuntu, you can start the ssh server by typing:
That should start the sshd server and you can then log in remotely.
How To Configure SSH
When you change the configuration of SSH, you are changing the settings of the sshd server.
In Ubuntu, the main sshd configuration file is located at / etc / ssh / sshd _ config.
Back up the current version of this file before editing:
Open it with a text editor:
You will want to leave most of the options in this file alone.
However, there are a few you may want to take a look at:
The port declaration specifies which port the sshd server will listen on for connections.
By default, this is 22. You should probably leave this setting alone, unless you have specific reasons to do otherwise.
If you do change your port, we will show you how to connect to the new port later on.
The host keys declarations specify where to look for global host keys.
We will discuss what a host key is later.
These two items indicate the level of logging that should occur.
If you are having difficulties with SSH, increasing the amount of logging may be a good way to discover what the issue is.
These parameters specify some of the login information.
LoginGraceTime specifies how many seconds to keep the connection alive without successfully logging in.
It may be a good idea to set this time just a little bit higher than the amount of time it takes you to log in normally.
PermitRootLogin selects whether the root user is allowed to log in.
In most cases, this should be changed to no when you have created a user account that has access to elevated privileges (through su or sudo) and can log in through ssh.
strictModes is a safety guard that will refuse a login attempt if the authentication files are readable by everyone.
This prevents login attempts when the configuration files are not secure.
These parameters configure an ability called X11 Forwarding.
This allows you to view a remote system's graphical user interface (GUI) on the local system.
This option must be enabled on the server and given with the SSH client during connection with the -X option.
After making your changes, save and close the file by typing CTRL + X and Y, followed by ENTER.
If you changed any settings in / etc / ssh / sshd _ config, make sure you reload your sshd server to implement your modifications:
You should thoroughly test your changes to ensure that they operate in the way you expect.
It may be a good idea to have a few sessions active when you are making changes.
This will allow you to revert the configuration if necessary.
How To Log Into SSH with Keys
While it is helpful to be able to log in to a remote system using passwords, it's a much better idea to set up key-based authentication.
How Does Key-based Authentication Work?
Key-based authentication works by creating a pair of keys: a private key and a public key.
The private key is located on the client machine and is secured and kept secret.
The public key can be given to anyone or placed on any server you wish to access.
When you attempt to connect using a key-pair, the server will use the public key to create a message for the client computer that can only be read with the private key.
The client computer then sends the appropriate response back to the server and the server will know that the client is legitimate.
This entire process is done automatically after you set up keys.
How To Create SSH Keys
SSH keys should be generated on the computer you wish to log in from.
This is usually your local machine.
Enter the following into the command line:
Press enter to accept the defaults.
Your keys will be created at ~ / .ssh / id _ rsa.pub and ~ / .ssh / id _ rsa.
Change into the .ssh directory by typing:
Look at the permissions of the files:
As you can see, the id _ rsa file is readable and writable only to the owner.
This is how it should be to keep it secret.
The id _ rsa.pub file, however, can be shared and has permissions appropriate for this activity.
How To Transfer Your Public Key to the Server
If you currently have password-based access to a server, you can copy your public key to it by issuing this command:
This will start an SSH session.
After you enter your password, it will copy your public key to the server's authorized keys file, which will allow you to log in without the password next time.
Client-Side Options
There are a number of optional flags that you can select when connecting through SSH.
Some of these may be necessary to match the settings in the remote host's sshd configuration.
For instance, if you changed the port number in your sshd configuration, you will need to match that port on the client-side by typing:
If you only wish to execute a single command on a remote system, you can specify it after the host like so:
You will connect to the remote machine, authenticate, and the command will be executed.
As we said before, if X11 forwarding is enabled on both computers, you can access that functionality by typing:
Providing you have the appropriate tools on your computer, GUI programs that you use on the remote system will now open their window on your local system.
Disabling Password Authentication
If you have created SSH keys, you can enhance your server "s security by disabling password-only authentication.
Apart from the console, the only way to log into your server will be through the private key that pairs with the public key you have installed on the server.
Warning: Before you proceed with this step, be sure you have installed a public key to your server.
Otherwise, you will be locked out!
As root or user with sudo privileges, open the sshd configuration file:
Locate the line that reads Password Authentication, and uncomment it by removing the leading #.
You can then change its value to no:
Two more settings that should not need to be modified (provided you have not modified this file before) are PubkeyAuthentication and ChallengeResponseAuthentication.
They are set by default, and should read as follows:
After making your changes, save and close the file.
You can now reload the SSH daemon:
Password authentication should now be disabled, and your server should be accessible only through SSH key authentication.
Learning your way around SSH is a worthwhile pursuit, if only because it is such a common activity.
As you use the various options, you will discover more advanced functionality that can make your life easier.
SSH has remained popular because it is secure, light-weight, and useful in diverse situations.
Understanding Date and Time in JavaScript
JavaScript comes with the built in Date object and related methods.
This tutorial will go over how to format and use date and time in JavaScript.
2469
Date and time are a regular part of our everyday lives and therefore feature prominently in computer programming.
In JavaScript, you might have to create a website with a calendar, a train schedule, or an interface to set up appointments.
These applications need to show relevant times based on the user's current timezone, or perform calculations around arrivals and departures or start and end times.
Additionally, you might need to use JavaScript to generate a report at a certain time every day, or filter through currently open restaurants and establishments.
To achieve all of these objectives and more, JavaScript comes with the built in Date object and related methods.
The Date Object
The Date object is a built-in object in JavaScript that stores the date and time.
It provides a number of built-in methods for formatting and managing that data.
By default, a new Date instance without arguments provided creates an object corresponding to the current date and time.
This will be created according to the current computer's system settings.
To demonstrate JavaScript's Date, let's create a variable and assign the current date to it. This article is being written on Wednesday, October 18th in London (GMT), so that is the current date, time, and timezone that is represented below.
Looking at the output, we have a date string containing the following:
Day of the Week
Month
Day
Year
Hour
Minute
Second
Timezone
Wed
Oct
18
2017
12
41
34
GMT + 0000 (UTC)
The date and time is broken up and printed in a way that we can understand as humans.
JavaScript, however, understands the date based on a timestamp derived from Unix time, which is a value consisting of the number of milliseconds that have passed since midnight on January 1st, 1970.
We can get the timestamp with the getTime () method.
The large number that appears in our output for the current timestamp represents the same value as above, October 18th, 2017.
Epoch time, also referred to as zero time, is represented by the date string 01 January, 1970 00: 00: 00 Universal Time (UTC), and by the 0 timestamp.
We can test this in the browser by creating a new variable and assigning to it a new Date instance based on a timestamp of 0.
Epoch time was chosen as a standard for computers to measure time by in earlier days of programming, and it is the method that JavaScript uses.
It is important to understand the concept of both the timestamp and the date string, as both may be used depending on the settings and purpose of an application.
So far, we learned how to create a new Date instance based on the current time, and how to create one based on a timestamp.
In total, there are four formats by which you can create a new Date in JavaScript.
In addition to the current time default and timestamp, you can also use a date string, or specify particular dates and times.
Date Creation
Output
new Date ()
Current date and time
new Date (timestamp)
Creates date based on milliseconds since Epoch time
new Date (date string)
Creates date based on date string
new Date (year, month, day, hours, minutes, seconds, milliseconds)
Creates date based on specified date and time
To demonstrate the different ways to refer to a specific date, we'll create new Date objects that will represent July 4th, 1776 at 12: 30pm GMT in three different ways.
The three examples above all create a date containing the same information.
You'll notice the timestamp method has a negative number; any date prior to Epoch time will be represented as a negative number.
In the date and time method, our seconds and milliseconds are set to 0. If any number is missing from the Date creation, it will default to 0. However, the order cannot be changed, so keep that in mind if you decide to leave off a number.
You may also notice that the month of July is represented by 6, not the usual 7. This is because the date and time numbers start from 0, as most counting in programming does.
See the next section for a more detailed chart.
Retrieving the Date with get
Once we have a date, we can access all the components of the date with various built-in methods.
The methods will return each part of the date relative to the local timezone.
Each of these methods starts with get, and will return the relative number.
Below is a detailed table of the get methods of the Date object.
Date / Time
Method
Range
Example
getFullYear ()
YYYY
1970
getMonth ()
0-11
0 = January
Day (of the month)
getDate ()
1 = 1st of the month
Day (of the week)
getDay ()
0-6
0 = Sunday
getHours ()
0 = midnight
getMinutes ()
getSeconds ()
Millisecond
getMilliseconds ()
0-999
Timestamp
getTime ()
Milliseconds since Epoch time
Let's make a new date, based on July 31, 1980, and assign it to a variable.
Now we can use all our methods to get each date component, from year to millisecond.
Sometimes it may be necessary to extract only part of a date, and the built-in get methods are the tool you will use to achieve this.
For an example of this, we can test the current date against the day and month of October 3rd to see whether it's October 3rd or not.
Since, at the time of writing, it's not October 3rd, the console reflects that.
The built-in Date methods that begin with get allow us to access date components that return the number associated with what we are retrieving from the instantiated object.
Modifying the Date with set
For all the get methods that we learned about above, there is a corresponding set method.
Where get is used to retrieve a specific component from a date, set is used to modify components of a date.
Below is a detailed chart of the set methods of the Date object.
setFullYear ()
setMonth ()
setDate ()
setDay ()
setHours ()
setMinutes ()
setSeconds ()
setMilliseconds ()
setTime ()
We can use these set methods to modify one, more, or all of the components of a date.
For example, we can change the year of our birthday variable from above to be 1997 instead of 1980.
We see in the example above that when we call the birthday variable we receive the new year as part of the output.
The built-in methods beginning with set let us modify different parts of a Date object.
Date Methods with UTC
The get methods discussed above retrieve the date components based on the user's local timezone settings.
For increased control over the dates and times, you can use the getUTC methods, which are exactly the same as the get methods, except they calculate the time based on the UTC (Coordinated Universal Time) standard.
Below is a table of the UTC methods for the JavaScript Date object.
getUTCFullYear ()
getUTCMonth ()
getUTCDate ()
getUTCDay ()
getUTCHours ()
getUTCMinutes ()
getUTCSeconds ()
getUTCMilliseconds ()
To test the difference between local and UTC get methods, we can run the following code.
Running this code will print out the current hour, and the hour of the UTC timezone.
If you are currently in the UTC timezone the numbers that are output from running the program above will be the same.
UTC is useful in that it provides an international time standard reference and can therefore keep your code consistent across timezones if that is applicable to what you are developing.
In this tutorial, we learned how to create an instance of the Date object, and use its built-in methods to access and modify components of a specific date.
For a more in-depth view of dates and times in JavaScript, you can read the Date reference on the Mozilla Developer Network.
Knowing how to work with dates is essential for many common tasks in JavaScript, as this can enable you to do many things from setting up a repeating report to displaying dates and schedules in the correct time zone.
The Python string data type is a sequence made up of one or more individual characters that could consist of letters, numbers, whitespace characters, or symbols.
Because a string is a sequence, it can be accessed in the same ways that other sequence-based data types are, through indexing and slicing.
S
y
h
!
9
The exclamation point (!)
Accessing Characters by Positive Index Number
Since the letter y is at index number 4 of the string ss = "Sammy Shark!", when we print ss [4] we receive y as the output.
Index numbers allow us to access specific characters within a string.
Accessing Characters by Negative Index Number
the negative index breakdown looks like this:
-10
-7
When constructing a slice, as in [6: 11], the first index number is where the slice starts (inclusive), and the second index number is where the slice ends (exclusive), which is why in our example above the range has to be the index number that would occur just after the string ends.
When we call ss [6: 11], we are calling the substring Shark that exists within the string Sammy Shark!.
If we want to include either end of a string, we can omit one of the numbers in the string [n: n] syntax.
You can also use negative index numbers to slice a string.
When using negative index numbers, we "ll start with the lower number first as it occurs earlier in the string.
So, a stride of 1 will take in every character between two index numbers of a slice.
< ^ > k < ^ > r < ^ > a < ^ > h < ^ > S < ^ > whitespace < ^ > y < ^ > m < ^ > m < ^ > a < ^ > S
Let "s print the length of the string ss:
is 12 characters long, including the whitespace character and the exclamation point symbol.
Let "s work with our string ss =" Sammy Shark! "
Let "s try str.count () with a sequence of characters:
We can do this with the str.find () method, and it will return the position of the character based on index number.
Let "s check to see where the first" likes "character sequence occurs in the string likes:
What if we want to see where the second sequence of "likes" begins?
In this second example that begins at the index number of 9, the first occurrence of the character sequence "likes" begins at index number 34.
Like slicing, we can do so by counting backwards using a negative index number:
Being able to call specific index numbers of strings, or a particular slice of a string gives us greater flexibility when working with this data type.
Because strings, like lists and tuples, are a sequence-based data type, it can be accessed through indexing and slicing.
To the computer, these are considered "processes".
top
PID USER PR NI VIRT RES SHR S% CPU% MEM TIME + COMMAND
3 root 20 0 0 0 0 S 0.0 0.0 0: 00.07 ksoftirqd / 0
6 root RT 0 0 0 0 S 0.0 0.0 0: 00.00 migration / 0
The top chunk of information give system statistics, such as system load and the total number of tasks.
htop
Mem [| | | | | | | | | | | 49 / 995MB] Load average: 0.00 0.03 0.05
root 1 0.0 0.2 24188 2120?
root 3 0.0 0.0 0 0?
root 6 0.0 0.0 0 0?
root 8 0.0 0.0 0 0?
ps axjf
2 6 0 0? -1 S 0 0: 00\ _ [migration / 0]
2 8 0 0? -1 S < 0 0: 00\ _ [cpuset]
A Note About Process IDs
Any communication between the user and the operating system about processes involves translating between process names and PIDs at some point during the operation.
This is why utilities tell you the PID.
If the program is misbehaving and does not exit when given the TERM signal, we can escalate the signal by passing the KILL signal:
They can also be used to perform other actions.
For instance, many daemons will restart when they are given the HUP, or hang-up signal.
You can list all of the signals that are possible to send with kill by typing:
kill -l
The above command is the equivalent of:
Often, you will want to adjust which processes are given priority in a server environment.
Tasks: 56 total, 1 running, 55 sleeping, 0 stopped, 0 zombie
Swap: 0k total, 0k used, 0k free, 264812k cached
3 root 20 0 0 0 0 S 0.0 0.0 0: 00.11 ksoftirqd / 0
Note: While nice operates with a command name by necessity, renice operates by calling the process PID
Process management is a topic that is sometimes difficult for new users to grasp because the tools used are different from their graphical counterparts.
However, the ideas are familiar and intuitive, and with a little practice, will become natural.
It harnesses Pipfile, pip, and virtualenv into one single command.
Open your terminal and run the following command:
A query string resembles the following:
Let's add a query string to the query-example route.
You will need to program the part that handles the query arguments.
By calling request.args.get (' language '), the application will continue to run if the language key doesn't exist in the URL.
The argument from the URL gets assigned to the language variable and then gets returned to the browser.
Create a key of "framework" and a value of "Flask ":
Then, run the app and navigate to the URL:
Remove the language key from the URL:
Let's continue to the next type of incoming data.
The most important thing to know about this form is that it performs a POST request to the same route that generated the form. The keys that will be read in the app all come from the name attributes on our form inputs.
Modify the form-example route in app.py with the following code:
Fill out the language field with value of Python and the framework field with the value of Flask.
Then, press Submit.
That is fairly anti-climatic but is to be expected because the code for handling the JSON data response has yet to be written.
To read the data, you must understand how Flask translates JSON data into Python data structures:
Anything that is an object gets converted to a Python dict. {"key ":" value "} in JSON corresponds to somedict ['key'], which returns a value in Python.
The values inside of quotes in the JSON object become strings in Python.
Note how you access elements that aren't at the top level. ['version'] ['python'] is used because you are entering a nested object.
If you don't want it to fail when a key doesn't exist, you'll have to check if the key exists before trying to access it.
Run the app and submit the example JSON request using Postman.
Now you understand handling JSON objects.
How To Install Anaconda on Ubuntu 18.04 Quickstart
2711
Designed for data science and machine learning workflows, Anaconda is an open-source package manager, environment manager, and distribution of the Python and R programming languages.
This tutorial will guide you through installing Anaconda on an Ubuntu 18.04 server.
For a more detailed version of this tutorial, with better explanations of each step, please refer to How To Install the Anaconda Python Distribution on Ubuntu 18.04.
Step 1 - Retrieve the Latest Version of Anaconda
From a web browser, go to the Anaconda Distribution page, available via the following link:
Find the latest Linux version and copy the installer bash script.
Step 2 - Download the Anaconda Bash Script
Logged into your Ubuntu 18.04 server as a sudo non-root user, move into the / tmp directory and use curl to download the link you copied from the Anaconda website:
Step 3 - Verify the Data Integrity of the Installer
Ensure the integrity of the installer with cryptographic hash verification through SHA-256 checksum:
Step 4 - Run the Anaconda Script
You "ll receive the following output to review the license agreement by pressing ENTER until you reach the end.
When you get to the end of the license, type yes as long as you agree to the license to complete installation.
Step 5 - Complete Installation Process
Once you agree to the license, you will be prompted to choose the location of the installation.
You can press ENTER to accept the default location, or specify a different location.
At this point, the installation will proceed.
Note that the installation process takes some time.
Step 6 - Select Options
Once installation is complete, you "ll receive the following output:
It is recommended that you type yes to use the conda command.
Step 7 - Activate Installation
You can now activate the installation with the following command:
Step 8 - Test Installation
Use the conda command to test the installation and activation:
You "ll receive output of all the packages you have available through the Anaconda installation.
Step 9 - Set Up Anaconda Environments
You can create Anaconda environments with the conda create command.
For example, a Python 3 environment named < ^ > my _ env < ^ > can be created with the following command:
Activate the new environment like so:
Your command prompt prefix will change to reflect that you are in an active Anaconda environment, and you are now ready to begin work on a project.
Here are links to more detailed tutorials that are related to this guide:
How To Install the Anaconda Python Distribution on Ubuntu 18.04
How To Set Up Jupyter Notebook for Python 3
How To Install the pandas Package and Work with Data Structures in Python 3
How to Install and Configure VNC on Ubuntu 18.04
2701
Virtual Network Computing, or VNC, is a connection system that allows you to use your keyboard and mouse to interact with a graphical desktop environment on a remote server.
It makes managing files, software, and settings on a remote server easier for users who are not yet comfortable with the command line.
In this guide, you'll set up a VNC server on an Ubuntu 18.04 server and connect to it securely through an SSH tunnel.
You'll use TightVNC, a fast and lightweight remote control package.
This choice will ensure that our VNC connection will be smooth and stable even on slower internet connections.
A local computer with a VNC client installed that supports VNC connections over SSH tunnels.
On Winows, you can use TightVNC, RealVNC, or UltraVNC.
On macOS, you can use the built-in Screen Sharing program, or can use a cross-platform app like RealVNC.
On Linux, you can choose from many options, including vinagre, krdc, RealVNC, or TightVNC.
Step 1 - Installing the Desktop Environment and VNC Server
By default, an Ubuntu 18.04 server does not come with a graphical desktop environment or a VNC server installed, so we'll begin by installing those.
Specifically, we will install packages for the latest Xfce desktop environment and the TightVNC package available in the official Ubuntu repository.
On your server, update your list of packages:
Now install the Xfce desktop environment on your server:
Once that installation completes, install the TightVNC server:
To complete the VNC server's initial configuration after installation, use the vncserver command to set up a secure password and create the initial configuration files:
You'll be prompted to enter and verify a password to access your machine remotely:
The password must be between six and eight characters long.
Passwords more than 8 characters will be truncated automatically.
Once you verify the password, you'll have the option to create a a view-only password.
Users who log in with the view-only password will not be able to control the VNC instance with their mouse or keyboard.
This is a helpful option if you want to demonstrate something to other people using your VNC server, but this isn't required.
The process then creates the necessary default configuration files and connection information for the server:
Now let's configure the VNC server.
Step 2 - Configuring the VNC Server
The VNC server needs to know which commands to execute when it starts up.
Specifically, VNC needs to know which graphical desktop it should connect to.
These commands are located in a configuration file called xstartup in the .vnc folder under your home directory.
The startup script was created when you ran the vncserver in the previous step, but we'll create our own to launch the Xfce desktop.
When VNC is first set up, it launches a default server instance on port 5901.
This port is called a display port, and is referred to by VNC as: 1.
VNC can launch multiple instances on other display ports, like: 2,: 3, and so on.
Because we are going to be changing how the VNC server is configured, first stop the VNC server instance that is running on port 5901 with the following command:
The output should look like this, although you'll see a different PID:
Before you modify the xstartup file, back up the original:
Now create a new xstartup file and open it in your text editor:
Commands in this file are executed automatically whenever you start or restart the VNC server.
We need VNC to start our desktop environment if it's not already started.
Add these commands to the file:
The first command in the file, xrdb $HOME /.
Xresources, tells VNC's GUI framework to read the server user's.
Xresources file..
Xresources is where a user can make changes to certain settings of the graphical desktop, like terminal colors, cursor themes, and font rendering.
The second command tells the server to launch Xfce, which is where you will find all of the graphical software that you need to comfortably manage your server.
To ensure that the VNC server will be able to use this new startup file properly, we'll need to make it executable.
Now, restart the VNC server.
With the configuration in place, let's connect to the server from our local machine.
Step 3 - Connecting the VNC Desktop Securely
VNC itself doesn't use secure protocols when connecting.
We'll use an SSH tunnel to connect securely to our server, and then tell our VNC client to use that tunnel rather than making a direct connection.
Create an SSH connection on your local computer that securely forwards to the localhost connection for VNC.
You can do this via the terminal on Linux or macOS with the following command:
The -L switch specifies the port bindings.
In this case we're binding port 5901 of the remote connection to port 5901 on your local machine.
The -C switch enables compression, while the -N switch tells ssh that we don't want to execute a remote command.
The -l switch specifies the remote login name.
Remember to replace < ^ > sammy < ^ > and < ^ > your _ server _ ip < ^ > with the sudo non-root username and IP address of your server.
If you are using a graphical SSH client, like PuTTY, use < ^ > your _ server _ ip < ^ > as the connection IP, and set localhost: 5901 as a new forwarded port in the program's SSH tunnel settings.
Once the tunnel is running, use a VNC client to connect to localhost: 5901.
You'll be prompted to authenticate using the password you set in Step 1.
Once you are connected, you'll see the default Xfce desktop.
VNC connection to Ubuntu 18.04 server
You can access files in your home directory with the file manager or from the command line, as seen here:
Files via VNC connection to Ubuntu 18.04
Press CTRL + C in your terminal to stop the SSH tunnel and return to your prompt.
This will disconnect your VNC session as well.
Next let's set up our VNC server as a service.
Step 4 - Running VNC as a System Service
Next, we'll set up the VNC server as a systemd service so we can start, stop, and restart it as needed, like any other service.
This will also ensure that VNC starts up when your server reboots.
First, create a new unit file called / etc / systemd / system / vncserver @ .service using your favorite text editor:
The @ symbol at the end of the name will let us pass in an argument we can use in the service configuration.
We'll use this to specify the VNC display port we want to use when we manage the service.
Add the following lines to the file.
Be sure to change the value of User, Group, WorkingDirectory, and the username in the value of PIDFILE to match your username:
The ExecStartPre command stops VNC if it's already running.
The ExecStart command starts VNC and sets the color depth to 24-bit color with a resolution of 1280x800.
You can modify these startup options as well to meet your needs.
Next, make the system aware of the new unit file.
Enable the unit file.
The 1 following the @ sign signifies which display number the service should appear over, in this case the default: 1 as was discussed in Step 2..
Stop the current instance of the VNC server if it's still running.
Then start it as you would start any other systemd service.
You can verify that it started with this command:
If it started correctly, the output should look like this:
Your VNC server will now be available when you reboot the machine.
Start your SSH tunnel again:
Then make a new connection using your VNC client software to localhost: 5901 to connect to your machine.
You now have a secured VNC server up and running on your Ubuntu 18.04 server.
Now you'll be able to manage your files, software, and settings with an easy-to-use and familiar graphical interface, and you'll be able to run graphical software like web browsers remotely.
How To Install Java with apt on Ubuntu 18.04
2630
The author selected the Open Internet / Free Speech Fund to receive a $100 donation as part of the Write for DOnations program.
You'll install OpenJDK as well as official packages from Oracle.
One Ubuntu 18.04 server set up by following the the Ubuntu 18.04 initial server setup guide tutorial, including a sudo non-root user and a firewall.
By default, Ubuntu 18.04 includes Open JDK, which is an open-source variant of the JRE and JDK.
This package will install either OpenJDK 10 or 11.
Prior to September 2018, this will install OpenJDK 10.
After September 2018, this will install OpenJDK 11.
Execute the following command to install OpenJDK:
This command will install the Java Runtime Environment (JRE).
This will allow you to run almost all Java software.
Next, let's look at specifying which OpenJDK version we want to install.
Installing Specific Versions of OpenJDK
While you can install the default OpenJDK package, you can also install different versions of OpenJDK.
OpenJDK 8
Java 8 is the current Long Term Support version and is still widely supported, though public maintenance ends in January 2019.
To install OpenJDK 8, execute the following command:
Verify that this is installed with
It is also possible to install only the JRE, which you can do by executing sudo apt install openjdk-8-jre.
OpenJDK 10 / 11
Ubuntu's repositories contain a package that will install either Java 10 or 11. Prior to September 2018, this package will install OpenJDK 10. Once Java 11 is released, this package will install Java 11.
To install OpenJDK 10 / 11, execute the following command:
To install the JRE only, use the following command:
Installing the Oracle JDK
If you want to install the Oracle JDK, which is the official version distributed by Oracle, you'll need to add a new package repository for the version you'd like to use.
To install Java 8, which is the latest LTS version, first add its package repository:
When you add the repository, you'll see a message like this:
Press ENTER to continue.
Then update your package list:
Once the package list updates, install Java 8:
Your system will download the JDK from Oracle and ask you to accept the license agreement.
Accept the agreement and the JDK will install.
This is what the output would look like if you've installed all versions of Java in this tutorial:
OpenJDK 8 is located at / usr / lib / jvm / java-8-openjdk-amd64 / jre / bin / java.
Oracle Java 8 is located at / usr / lib / jvm / java-8-oracle / jre / bin / java.
At the end of this file, add the following line, making sure to replace the highlighted path with your own copied path:
How To Install Linux, Apache, MySQL, PHP (LAMP) stack on Ubuntu 18.04
2614
A previous version of this tutorial was written by Brennan Bearnes.
A "LAMP" stack is a group of open-source software that is typically installed together to enable a server to host dynamic websites and web apps.
This term is actually an acronym which represents the Linux operating system, with the Apache web server.
In this guide, we will install a LAMP stack on an Ubuntu 18.04 server.
In order to complete this tutorial, you will need to have an Ubuntu 18.04 server with a non-root sudo-enabled user account and a basic firewall.
This can be configured using our initial server setup guide for Ubuntu 18.04.
It's well-documented and has been in wide use for much of the history of the web, which makes it a great default choice for hosting a website.
Since this is a sudo command, these operations are executed with root privileges.
It will ask you for your regular user's password to verify your intentions.
Once you've entered your password, apt will tell you which packages it plans to install and how much extra disk space they'll take up.
Press Y and hit ENTER to continue, and the installation will proceed.
Adjust the Firewall to Allow Web Traffic
Next, assuming that you have followed the initial server setup instructions and enabled the UFW firewall, make sure that your firewall allows HTTP and HTTPS traffic.
You can check that UFW has an application profile for Apache like so:
If you look at the Apache Full profile, it should show that it enables traffic to ports 80 and 443:
Allow incoming HTTP and HTTPS traffic for this profile:
You will see the default Ubuntu 18.04 Apache web page, which is there for informational and testing purposes.
Ubuntu 18.04 Apache default
Now that you have your web server up and running, it is time to install MySQL.
MySQL is a database management system.
Basically, it will organize and provide access to databases where your site can store information.
< $> note Note: In this case, you do not have to run sudo apt update prior to the command.
This is because you recently ran it in the commands above to install Apache.
The package index on your computer should already be up-to-date.
This command, too, will show you a list of the packages that will be installed, along with the amount of disk space they'll take up.
Enter Y to continue.
When the installation is complete, run a simple security script that comes pre-installed with MySQL which will remove some dangerous defaults and lock down access to your database system.
This will cause issues if you use a weak password in conjunction with software which automatically configures MySQL user credentials, such as the Ubuntu packages for phpMyAdmin.
This is an administrative account in MySQL that has increased privileges.
Think of it as being similar to the root account for the server itself (although the one you are configuring now is a MySQL-specific account).
Make sure this is a strong, unique password, and do not leave it blank.
If you enabled password validation, you'll be shown the password strength for the root password you just entered and your server will ask if you want to change that password.
If you are happy with your current password, enter N for "no" at the prompt:
Note that in Ubuntu systems running MySQL 5.7 (and later versions), the root MySQL user is set to authenticate using the auth _ socket plugin by default rather than with a password.
This allows for some greater security and usability in many cases, but it can also complicate things when you need to allow an external program (e.g., phpMyAdmin) to access the user.
If you prefer to use a password when connecting to MySQL as root, you will need to switch its authentication method from auth _ socket to mysql _ native _ password.
To do this, open up the MySQL prompt from your terminal:
Next, check which authentication method each of your MySQL user accounts use with the following command:
In this example, you can see that the root user does in fact authenticate using the auth _ socket plugin.
To configure the root account to authenticate with a password, run the following ALTER USER command.
Be sure to change < ^ > password < ^ > to a strong password of your choosing:
Then, run FLUSH PRIVILEGES which tells the server to reload the grant tables and put your new changes into effect:
Check the authentication methods employed by each of your users again to confirm that root no longer authenticates using the auth _ socket plugin:
You can see in this example output that the root MySQL user now authenticates using a password.
Once you confirm this on your own server, you can exit the MySQL shell:
At this point, your database system is now set up and you can move on to installing PHP, the final component of the LAMP stack.
PHP is the component of your setup that will process code to display dynamic content.
It can run scripts, connect to your MySQL databases to get information, and hand the processed content over to your web server to display.
Once again, leverage the apt system to install PHP.
In addition, include some helper packages this time so that PHP code can run under the Apache server and talk to your MySQL database:
This should install PHP without any problems.
We'll test this in a moment.
In most cases, you will want to modify the way that Apache serves files when a directory is requested.
Currently, if a user requests a directory from the server, Apache will first look for a file called index.html.
We want to tell the web server to prefer PHP files over others, so make Apache look for an index.php file first.
To do this, type this command to open the dir.conf file in a text editor with root privileges:
It will look like this:
Move the PHP index file (highlighted above) to the first position after the DirectoryIndex specification, like this:
When you are finished, save and close the file by pressing CTRL + X.
Confirm the save by typing Y and then hit ENTER to verify the file save location.
After this, restart the Apache web server in order for your changes to be recognized.
Do this by typing this:
You can also check on the status of the apache2 service using systemctl:
Press Q to exit this status output.
To enhance the functionality of PHP, you have the option to install some additional modules.
To see the available options for PHP modules and libraries, pipe the results of apt search into less, a pager which lets you scroll through the output of other commands:
Use the arrow keys to scroll up and down, and press Q to quit.
The results are all optional components that you can install.
It will give you a short description for each:
To learn more about what each module does, you could search the internet for more information about them.
Alternatively, look at the long description of the package by typing:
There will be a lot of output, with one field called Description which will have a longer explanation of the functionality that the module provides.
For example, to find out what the php-cli module does, you could type this:
Along with a large amount of other information, you'll find something that looks like this:
If, after researching, you decide you would like to install a package, you can do so by using the apt install command like you have been doing for the other software.
If you decided that php-cli is something that you need, you could type:
If you want to install more than one module, you can do that by listing each one, separated by a space, following the apt install command, like this:
At this point, your LAMP stack is installed and configured.
Before you do anything else, we recommend that you set up an Apache virtual host where you can store your server's configuration details.
To learn more about setting up a domain name with DigitalOcean, see our Introduction to DigitalOcean DNS.
Apache on Ubuntu 18.04 has one server block enabled by default that is configured to serve documents from the / var / www / html directory.
Instead of modifying / var / www / html, let's create a directory structure within / var / www for our your _ domain site, leaving / var / www / html in place as the default directory to be served if a client request doesn't match any other sites.
Next, assign ownership of the directory with the $USER environment variable:
Next, create a sample index.html page using nano or your favorite editor:
In order for Apache to serve this content, it's necessary to create a virtual host file with the correct directives.
Instead of modifying the default configuration file located at / etc / apache2 / sites-available / 000-default.conf directly, let's make a new one at / etc / apache2 / sites-available / < ^ > your _ domain < ^ > .conf:
Paste in the following configuration block, which is similar to the default, but updated for our new directory and domain name:
Notice that we've updated the DocumentRoot to our new directory and ServerAdmin to an email that the your _ domain site administrator can access.
We've also added two directives: ServerName, which establishes the base domain that should match for this virtual host definition, and ServerAlias, which defines further names that should match as if they were the base name.
Let's enable the file with the a2ensite tool:
Next, let's test for configuration errors:
You can test this by navigating to http: / / < ^ > your _ domain < ^ >, where you should see something like this:
With that, you virtual host is fully set up.
Before making any more changes or deploying an application, though, it would be helpful to proactively test out your PHP configuration in case there are any issues that should be addressed.
In order to test that your system is configured properly for PHP, create a very basic PHP script called info.php.
In order for Apache to find this file and serve it correctly, it must be saved to your web root directory.
Create the file at the web root you created in the previous step by running:
Now you can test whether your web server is able to correctly display content generated by this PHP script.
To try this out, visit this page in your web browser.
You'll need your server's public IP address again.
The address you will want to visit is:
The page that you come to should look something like this:
Ubuntu 18.04 default PHP info
This page provides some basic information about your server from the perspective of PHP.
If you can see this page in your browser, then your PHP is working as expected.
You probably want to remove this file after this test because it could actually give information about your server to unauthorized users.
To do this, run the following command:
Now that you have a LAMP stack installed, you have many choices for what to do next.
Basically, you've installed a platform that will allow you to install most kinds of websites and web software on your server.
As an immediate next step, you should ensure that connections to your web server are secured, by serving them via HTTPS.
The easiest option here is to use Let's Encrypt to secure your site with a free TLS / SSL certificate.
Some other popular options are:
Install Wordpress the most popular content management system on the internet.
Set Up PHPMyAdmin to help manage your MySQL databases from web browser.
Learn how to use SFTP to transfer files to and from your server.
How To Install WordPress with LAMP on Ubuntu 18.04
2692
WordPress is the most popular CMS (content management system) on the internet.
It allows you to easily set up flexible blogs and websites on top of a MySQL backend with PHP processing.
WordPress has seen incredible adoption and is a great choice for getting a website up and running quickly.
After setup, almost all administration can be done through the web frontend.
In this guide, we'll focus on getting a WordPress instance set up on a LAMP stack (Linux, Apache, MySQL, and PHP) on an Ubuntu 18.04 server.
In order to complete this tutorial, you will need access to an Ubuntu 18.04 server.
You will need to perform the following tasks before you can start this guide:
Create a sudo user on your server: We will be completing the steps in this guide using a non-root user with sudo privileges.
You can create a user with sudo privileges by following our Ubuntu 18.04 initial server setup guide.
Install a LAMP stack: WordPress will need a web server, a database, and PHP in order to correctly function.
Setting up a LAMP stack (Linux, Apache, MySQL, and PHP) fulfills all of these requirements.
Follow this guide to install and configure this software.
Secure your site with SSL: WordPress serves dynamic content and handles user authentication and authorization.
TLS / SSL is the technology that allows you to encrypt the traffic from your site so that your connection is secure.
The way you set up SSL will depend on whether you have a domain name for your site.
If you have a domain name... the easiest way to secure your site is with Let's Encrypt, which provides free, trusted certificates.
Follow our Let's Encrypt guide for Apache to set this up.
If you do not have a domain... and you are just using this configuration for testing or personal use, you can use a self-signed certificate instead.
This provides the same type of encryption, but without the domain validation.
Follow our self-signed SSL guide for Apache to get set up.
When you are finished with the setup steps, log into your server as your sudo user and continue below.
Step 1 - Creating a MySQL Database and User for WordPress
The first step that we will take is a preparatory one.
WordPress uses MySQL to manage and store site and user information.
We have MySQL installed already, but we need to make a database and a user for WordPress to use.
To get started, log into the MySQL root (administrative) account by issuing this command:
You will be prompted for the password you set for the MySQL root account when you installed the software.
First, we can create a separate database that WordPress will control.
You can call this whatever you would like, but we will be using wordpress in this guide to keep it simple.
Create the database for WordPress by typing:
< $> note Note: Every MySQL statement must end in a semi-colon (;).
Check to make sure this is present if you are running into any issues.
Next, we are going to create a separate MySQL user account that we will use exclusively to operate on our new database.
Creating one-function databases and accounts is a good idea from a management and security standpoint.
We will use the name wordpressuser in this guide.
Feel free to change this if you'd like.
We are going to create this account, set a password, and grant access to the database we created.
We can do this by typing the following command.
Remember to choose a strong password here for your database user:
You now have a database and user account, each made specifically for WordPress.
We need to flush the privileges so that the current instance of MySQL knows about the recent changes we've made:
Exit out of MySQL by typing:
Step 2 - Installing Additional PHP Extensions
When setting up our LAMP stack, we only required a very minimal set of extensions in order to get PHP to communicate with MySQL.
WordPress and many of its plugins leverage additional PHP extensions.
We can download and install some of the most popular PHP extensions for use with WordPress by typing:
< $> note Note: Each WordPress plugin has its own set of requirements.
Some may require additional PHP packages to be installed.
Check your plugin documentation to discover its PHP requirements.
If they are available, they can be installed with apt as demonstrated above.
We will restart Apache to load these new extensions in the next section.
If you are returning here to install additional plugins, you can restart Apache now by typing:
Step 3 - Adjusting Apache's Configuration to Allow for .htaccess Overrides and Rewrites
Next, we will be making a few minor adjustments to our Apache configuration.
Based on the prerequisite tutorials, you should have a configuration file for your site in the / etc / apache2 / sites-available / directory.
We'll use / etc / apache2 / sites-available / wordpress.conf as an example here, but you should substitute the path to your configuration file where appropriate.
Additionally, we will use / var / www / wordpress as the root directory of our WordPress install.
You should use the web root specified in your own configuration.
< $> note Note: It's possible you are using the 000-default.conf default configuration (with / var / www / html as your web root).
This is fine to use if you're only going to host one website on this server.
If not, it's best to split the necessary configuration into logical chunks, one file per site.
Enabling .htaccess Overrides
Currently, the use of .htaccess files is disabled.
WordPress and many WordPress plugins use these files extensively for in-directory tweaks to the web server's behavior.
Open the Apache configuration file for your website:
To allow .htaccess files, we need to set the AllowOverride directive within a Directory block pointing to our document root.
Add the following block of text inside the VirtualHost block in your configuration file, being sure to use the correct web root directory:
Enabling the Rewrite Module
Next, we can enable mod _ rewrite so that we can utilize the WordPress permalink feature:
Enabling the Changes
Before we implement the changes we've made, check to make sure we haven't made any syntax errors:
The output might have a message that looks like this:
If you wish to suppress the top line, just add a ServerName directive to your main (global) Apache configuration file at / etc / apache2 / apache2.conf.
The ServerName can be your server's domain or IP address.
This is just a message however and doesn't affect the functionality of our site.
As long as the output contains Syntax OK, you are ready to continue.
Restart Apache to implement the changes:
Next, we will download and set up WordPress itself.
Step 4 - Downloading WordPress
Now that our server software is configured, we can download and set up WordPress.
For security reasons in particular, it is always recommended to get the latest version of WordPress from their site.
Change into a writable directory and then download the compressed release by typing:
Extract the compressed file to create the WordPress directory structure:
We will be moving these files into our document root momentarily.
Before we do, we can add a dummy .htaccess file so that this will be available for WordPress to use later.
Create the file by typing:
We'll also copy over the sample configuration file to the filename that WordPress actually reads:
We can also create the upgrade directory, so that WordPress won't run into permissions issues when trying to do this on its own following an update to its software:
Now, we can copy the entire contents of the directory into our document root.
We are using a dot at the end of our source directory to indicate that everything within the directory should be copied, including hidden files (like the .htaccess file we created):
Step 5 - Configuring the WordPress Directory
Before we do the web-based WordPress setup, we need to adjust some items in our WordPress directory.
Adjusting the Ownership and Permissions
One of the big things we need to accomplish is setting up reasonable file permissions and ownership.
We'll start by giving ownership of all the files to the www-data user and group.
This is the user that the Apache webserver runs as, and Apache will need to be able to read and write WordPress files in order to serve the website and perform automatic updates.
Update the ownership with chown:
Next we will run two find commands to set the correct permissions on the WordPress directories and files:
These should be a reasonable permissions set to start with.
Some plugins and procedures might require additional tweaks.
Setting up the WordPress Configuration File
Now, we need to make some changes to the main WordPress configuration file.
When we open the file, our first order of business will be to adjust some secret keys to provide some security for our installation.
WordPress provides a secure generator for these values so that you do not have to try to come up with good values on your own.
These are only used internally, so it won't hurt usability to have complex, secure values here.
To grab secure values from the WordPress secret key generator, type:
You will get back unique values that look something like this:
< $> warning Warning!
It is important that you request unique values each time.
Do NOT copy the values shown below!
These are configuration lines that we can paste directly in our configuration file to set secure keys.
Copy the output you received now.
Now, open the WordPress configuration file:
Find the section that contains the dummy values for those settings.
Delete those lines and paste in the values you copied from the command line:
Next, we need to modify some of the database connection settings at the beginning of the file.
You need to adjust the database name, the database user, and the associated password that we configured within MySQL.
The other change we need to make is to set the method that WordPress should use to write to the filesystem.
Since we've given the web server permission to write where it needs to, we can explicitly set the filesystem method to "direct".
Failure to set this with our current settings would result in WordPress prompting for FTP credentials when we perform some actions.
This setting can be added below the database connection settings, or anywhere else in the file:
Step 6 - Completing the Installation Through the Web Interface
Now that the server configuration is complete, we can complete the installation through the web interface.
In your web browser, navigate to your server's domain name or public IP address:
Select the language you would like to use:
WordPress language selection
Next, you will come to the main setup page.
Select a name for your WordPress site and choose a username (it is recommended not to choose something like "admin" for security purposes).
A strong password is generated automatically.
Save this password or select an alternative strong password.
Enter your email address and select whether you want to discourage search engines from indexing your site:
WordPress setup installation
When you click ahead, you will be taken to a page that prompts you to log in:
WordPress login prompt
Once you log in, you will be taken to the WordPress administration dashboard:
WordPress should be installed and ready to use!
Some common next steps are to choose the permalinks setting for your posts (can be found in Settings > Permalinks) or to select a new theme (in Appearance > Themes).
If this is your first time using WordPress, explore the interface a bit to get acquainted with your new CMS.
How to Set Up SSH Keys on Ubuntu 18.04
2620
In this guide, we'll focus on setting up SSH keys for a vanilla Ubuntu 18.04 installation.
Step 1 - Create the RSA Key Pair
By default ssh-keygen will create a 2048-bit RSA key pair, which is secure enough for most use cases (you may optionally pass in the -b 4096 flag to create a larger 4096-bit key).
Step 2 - Copy the Public Key to Ubuntu Server
Copying Public Key Using ssh-copy-id
To use the utility, you simply need to specify the remote host that you would like to connect to and the user account that you have password SSH access to.
Step 3 - Authenticate to Ubuntu Server Using SSH Keys
If you have successfully completed one of the procedures above, you should be able to log into the remote host without the remote account's password.
Step 4 - Disable Password Authentication on your Server
This may be commented out. Uncomment the line and set the value to "no".
This will disable your ability to log in via SSH using account passwords:
Save and close the file when you are finished by pressing CTRL + X, then Y to confirm saving the file, and finally ENTER to exit nano.
As a precaution, open up a new terminal window and test that the SSH service is functioning correctly before closing this session:
Once you have verified your SSH service, you can safely close all current server sessions.
The SSH daemon on your Ubuntu server now only responds to SSH keys.
Understanding Classes in JavaScript
2626
JavaScript is a prototype-based language, and every object in JavaScript has a hidden internal property called [[Prototype]] that can be used to extend object properties and methods.
You can read more about prototypes in our Understanding Prototypes and Inheritance in JavaScript tutorial.
Until recently, industrious developers used constructor functions to mimic an object-oriented design pattern in JavaScript.
The language specification ECMAScript 2015, often referred to as ES6, introduced classes to the JavaScript language.
Classes in JavaScript do not actually offer additional functionality, and are often described as providing "syntactical sugar" over prototypes and inheritance in that they offer a cleaner and more elegant syntax.
Because other programming languages use classes, the class syntax in JavaScript makes it more straightforward for developers to move between languages.
Classes Are Functions
A JavaScript class is a type of function.
Classes are declared with the class keyword.
We will use function expression syntax to initialize a function and class expression syntax to initialize a class.
We can access the [[Prototype]] of an object using the Object.getPrototypeOf () method.
Let's use that to test the empty function we created.
We can also use that method on the class we just created.
The code declared with function and class both return a function [[Prototype]].
With prototypes, any function can become a constructor instance using the new keyword.
This applies to classes as well.
These prototype constructor examples are otherwise empty, but we can see how underneath the syntax, both methods are achieving the same end result.
Defining a Class
In the prototypes and inheritance tutorial, we created an example based around character creation in a text-based role-playing game.
Let's continue with that example here to update the syntax from functions to classes.
A constructor function is initialized with a number of parameters, which would be assigned as properties of this, referring to the function itself.
The first letter of the identifier would be capitalized by convention.
When we translate this to the class syntax, shown below, we see that it is structured very similarly.
We know a constructor function is meant to be an object blueprint by the capitalization of the first letter of the initializer (which is optional) and through familiarity with the syntax.
The class keyword communicates in a more straightforward fashion the objective of our function.
The only difference in the syntax of the initialization is using the class keyword instead of function, and assigning the properties inside a constructor () method.
Defining Methods
The common practice with constructor functions is to assign methods directly to the prototype instead of in the initialization, as seen in the greet () method below.
With classes this syntax is simplified, and the method can be added directly to the class.
Using the method definition shorthand introduced in ES6, defining a method is an even more concise process.
Let's take a look at these properties and methods in action.
We will create a new instance of Hero using the new keyword, and assign some values.
If we print out more information about our new object with console.log (hero1), we can see more details about what is happening with the class initialization.
We can see in the output that the constructor () and greet () functions were applied to the _ _ proto _ _, or [[Prototype]] of hero1, and not directly as a method on the hero1 object.
While this is clear when making constructor functions, it is not obvious while creating classes.
Classes allow for a more simple and succinct syntax, but sacrifice some clarity in the process.
Extending a Class
An advantageous feature of constructor functions and classes is that they can be extended into new object blueprints based off of the parent.
This prevents repetition of code for objects that are similar but need some additional or more specific features.
New constructor functions can be created from the parent using the call () method.
In the example below, we will create a more specific character class called Mage, and assign the properties of Hero to it using call (), as well as adding an additional property.
At this point, we can create a new instance of Mage using the same properties as Hero as well as a new one we added.
Sending hero2 to the console, we can see we have created a new Mage based off the constructor.
With ES6 classes, the super keyword is used in place of call to access the parent functions.
We will use extends to refer to the parent class.
Now we can create a new Mage instance in the same manner.
We will print hero2 to the console and view the output.
The output is nearly exactly the same, except that in the class construction the [[Prototype]] is linked to the parent, in this case Hero.
Below is a side-by-side comparison of the entire process of initialization, adding methods, and inheritance of a constructor function and a class.
Although the syntax is quite different, the underlying result is nearly the same between both methods.
Classes give us a more concise way of creating object blueprints, and constructor functions describe more accurately what is happening under the hood.
In this tutorial, we learned about the similarities and differences between JavaScript constructor functions and ES6 classes.
Both classes and constructors imitate an object-oriented inheritance model to JavaScript, which is a prototype-based inheritance language.
Understanding prototypical inheritance is paramount to being an effective JavaScript developer.
Being familiar with classes is extremely helpful, as popular JavaScript libraries such as React make frequent use of the class syntax.
How To Automate Your Node.js Production Deployments with Shipit on CentOS 7
3858
Shipit is a universal automation and deployment tool for Node.js developers.
It features a task flow based on the popular Orchestrator package, login and interactive SSH commands through OpenSSH, and an extensible API.
Developers can use Shipit to automate build and deployment workflows for a wide range of Node.js applications.
The Shipit workflow allows developers to not only configure tasks, but also to specify the order in which they are executed; whether they should be run synchronously or asynchronously and on which environment.
In this tutorial you will install and configure Shipit to deploy a Node.js application from your local development environment to your production environment.
You'll use Shipit to deploy your application and configure the remote server by:
transferring your Node.js application's files from your local environment to the production environment (using rsync, git, and ssh).
installing your application's dependencies (node modules).
configuring and managing the Node.js processes running on the remote server with PM2.
Before you begin this tutorial you'll need the following:
Two CentOS 7 servers (in this tutorial they will be named app and web) configured with private networking by following the How To Set Up a Node.js Application for Production on CentOS 7 tutorial.
Nginx (on your web server) secured with TLS / SSL as shown in the How To Secure Nginx with Let's Encrypt on CentOS 7 tutorial.
Note, if you are following the prerequisites chronologically, then you only need to complete steps 1, 4, and 6 on your web server.
Node.js and npm installed on your development environment.
A local development computer with rsync and git installed.
On macOS you can install these with Homebrew.
To install git on Linux distributions, follow the How To Install Git tutorial.
An account with GitHub or another hosted git service provider.
This tutorial will use GitHub.
< $> note Note: Windows users will need to install the Windows Subsystem for Linux to execute the commands in this guide.
Step 1 - Setting Up the Remote Repository
Shipit requires a Git repository to synchronize between the local development machine and the remote server.
In this step you'll create a remote repository on Github.com.
While each provider is slightly different the commands are somewhat transferrable.
To create a repository, open Github.com in your web browser and log in.
You will notice that in the upper-right corner of any page there is a + symbol.
Click +, and then click New repository.
Github-new-repository
Type a short, memorable name for your repository, for example, hello-world.
Note that whatever name you choose here will be replicated as the project folder that you'll work from on your local machine.
Github-repository-name
Optionally, add a description of your repository.
Github-repository-description
Set your repository's visibility to your preference, either public or private.
Make sure the repository is initialized with a .gitignore, select Node from the Add .gitignore dropdown list.
This step is important to avoid having unnecessary files (like the node _ modules folder) being added to your repository.
Github-gitignore-node
Click the Create repository button.
The repository now needs to be cloned from Github.com to your local machine.
Open your terminal and navigate to the location where you want to store all your Node.js project files.
Note that this process will create a sub-folder within the current directory.
To clone the repository to your local machine, run the following command:
You will need to replace < ^ > your-github-username < ^ > and < ^ > your-github-repository-name < ^ > to reflect your Github username and the previously supplied repository name.
< $> note Note: If you have enabled two-factor authentication (2FA) on Github.com, you must use a personal access token or SSH key instead of your password when accessing Github on the command line.
The Github Help page related to 2FA provides further information.
Move to the repository by running the following command:
Inside the repository is a single file and folder, both of which are files used by Git to manage the repository.
You can verify this with:
Now that you have configured a working git repository, you'll create the shipit.js file that manages your deployment process.
Step 2 - Integrating Shipit into a Node.js Project
In this step, you'll create an example Node.js project and then add the Shipit packages.
This tutorial provides an example app - the Node.js web server that accepts HTTP requests and responds with Hello World in plain text.
To create the application, run the following command:
Add the following example application code to hello.js (updating the APP _ PRIVATE _ IP _ ADDRESS variable to your app server's private network IP address):
Now create your package.json file for your application:
This command creates a package.json file, which you'll use to configure your Node.js application.
In the next step, you'll add dependencies to this file with the npm command line interface.
Next, install the necessary npm packages with the following command:
You use the --save-dev flag here as the Shipit packages are only required on your local machine.
This also added the three packages to your package.json file as development dependencies:
With your local environment configured, you can now move on to preparing the remote app server for Shipit-based deployments.
Step 3 - Preparing the Remote App Server
In this step, you'll use ssh to connect to your app server and install your remote dependency rsync.
Rsync is a utility for efficiently transferring and synchronizing files between local computer drives and across networked computers by comparing the modification times and sizes of files.
Shipit uses rsync to transfer and synchronize files between your local computer and the remote app server.
You won't be issuing any commands to rsync directly; Shipit will handle it for you.
< $> note Note: How To Set Up a Node.js Application for Production on CentOS 7 left you with two servers app and web.
These commands should be executed on app only.
Connect to your remote app server via ssh:
Install rsync on your server by running the following command:
Confirm the installation with:
You'll see a similar line within the output of this command:
You can end your ssh session by typing exit.
With rsync installed and available on the command line, you can move on to deployment tasks and their relationship with events.
Step 4 - Configuring and Executing Deployment Tasks
Both events and tasks are key components of Shipit deployments and it is important to understand how they complement the deployment of your application.
The events triggered by Shipit represent specific points in the deployment lifecycle.
Your tasks will execute in response to these events, based on the sequence of the Shipit lifecycle.
A common example of where this task / event system is useful in a Node.js application, is the installation of the app's dependencies (node _ modules) on the remote server.
Later in this step you'll have Shipit listen for the updated event (which is issued after the application's files are transferred) and run a task to install the application's dependencies (npm install) on the remote server.
To listen to events and execute tasks, Shipit needs a configuration file that holds information about your remote server (the app server) and registers event listeners and the commands to be executed by these tasks.
This file lives on your local development computer, inside your Node.js application's directory.
To get started, create this file, including information about your remote server, the event listeners you want to subscribe to, and some definitions of your tasks.
Create shipitfile.js within your application root directory on your local machine by running the following command:
Now that you've created a file, it needs to be populated with the initial environment information that Shipit needs.
This is primarily the location of your remote Git repository and importantly, your app server's public IP address and SSH user account.
Add this initial configuration and update the highlighted lines to match your environment:
Updating the < ^ > variables < ^ > in your shipit.initConfig method provides Shipit with configuration specific to your deployment.
These represent the following to Shipit:
deployTo: is the directory where Shipit will deploy your application's code to on the remote server.
Here you use the / home / folder for a non-root user with sudo privileges (/ home / < ^ > sammy < ^ >) as it is secure, and will avoid permission issues.
The / < ^ > your-domain < ^ > component is a naming convention to distinguish the folder from others in the user's home folder.
repositoryUrl: is the URL to the full Git repository, Shipit will use this URL to ensure the project files are in sync prior to deployment.
keepReleases: is the number of releases to keep on the remote server.
A release is a date-stamped folder containing your application's files at the time of release.
These can be useful for rollback of a deployment.
shared: is configuration that corresponds with keepReleases that allows directories to be shared between releases.
In this instance, we have a single node _ modules folder that is shared between all releases.
production: represents a remote server to deploy your application to.
In this instance, you have a single server (app server) that you name production, with the servers: configuration matching your SSH user and public ip address.
The name production, corresponds with the Shipit deploy command used toward the end of this tutorial (npx shipit < ^ > server name < ^ > deploy or in your case npx shipit production deploy).
Further information on the Shipit Deploy Configuration object can be found in the Shipit Github repository.
Before continuing to update your shipitfile.js, let's review the following example code snippet to understand Shipit tasks:
This is an example task that uses the shipit.on method to subscribe to the deploy event.
This task will wait for the deploy event to be emitted by the Shipit lifecycle, then when the event is received, the task executes the shipit.start method that tells Shipit to start the say-hello task.
The shipit.on method takes two parameters, the name of the event to listen for and the callback function to execute when the event is received.
Under the shipit.on method declaration, the task is defined with the shipit.blTask method.
This creates a new Shipit task that will block other tasks during its execution (it is a synchronous task).
The shipit.blTask method also takes two parameters, the name of the task it is defining and a callback function to execute when the task is triggered by shipit.start.
Within the callback function of this example task (say-hello), the shipit.local method executes a command on the local machine.
The local command echos "hello from your local computer" into the terminal output.
If you wanted to execute a command on the remote server, you would use the shipit.remote method.
The two methods, shipit.local and shipit.remote, provide an API to issue commands either locally, or remotely as part of a deployment.
Now update the shipitfile.js to include event listeners to subscribe to the Shipit lifecycle with shipit.on.
Add the event listeners to your shipitfile.js, inserting them following the comment placeholder from the initial configuration / / Our tasks will go here:
These two methods are listening for the updated and the published events that are emitted as part of the Shipit deployment lifecycle.
When the event is received, they will each initiate tasks using the shipit.start method, similarly to the example task.
Now that you've scheduled the listeners, you'll add the corresponding task.
Add the following task to your shipitfile.js, inserting them after your event listeners:
You first declare a task called copy-config.
This task creates a local file called ecosystem.config.js and then copies that file to your remote app server.
PM2 uses this file to manage your Node.js application.
It provides the necessary file path information to PM2 to ensure that it is running your latest deployed files.
Later in the build process, you'll create a task that runs PM2 with ecosystem.config.js as configuration.
If your application needs environment variables (like a database connection string) you can declare them either locally in env: or on the remote server in env _ production: in the same manner that you set the NODE _ ENV variable in these objects.
Add the next task to your shipitfile.js following the copy-config task:
Next, you declare a task called npm-install.
This task uses a remote bash terminal (via shipit.remote) to install the app's dependencies (npm packages).
Add the last task to your shipitfile.js following the npm-install task:
Finally you declare a task called pm2-server.
This task also uses a remote bash terminal to first stop PM2 from managing your previous deployment through the delete command and then start a new instance of your Node.js server providing the ecosystem.config.js file as a variable.
You also let PM2 know that it should be using environment variables from the production block in your initial configuration and you ask PM2 to watch the application, restarting it if it crashes.
The complete shipitfile.js file:
Save and exit the file when you're ready.
With your shipitfile.js configured, event listeners, and associated tasks finalized you can move on to deploying to the app server.
Step 5 - Deploying Your Application
In this step, you will deploy your application remotely and test that the deployment made your application available to the internet.
Because Shipit clones the project files from the remote Git repository, you need to push your local Node.js application files from your local machine to Github.
Navigate to your Node.js project's application directory (where your hello.js and shiptitfile.js are located) and run the following command:
The git status command displays the state of the working directory and the staging area.
It lets you see which changes have been staged, which haven't, and which files aren't being tracked by Git.
Your files are untracked and appear red in the output:
You can add these files to your repository with the following command:
This command does not produce any output, although if you were to run git status again, the files would appear green with a note that there are changes to be committed.
You can create a commit running the following command:
The output of this command provides some Git-specific information about the files.
All that is left now is to push your commit to the remote repository for Shipit to clone to your app server during deployment.
The output includes information about the synchronization with the remote repository:
To deploy your application, run the following command:
The output of this command (which is too large to include in its entirety) provides detail on the tasks being executed and the result of the specific function.
The output following for the pm2-server task shows the Node.js app has been launched:
To view your application as a user would, you can enter your website URL < ^ > your-domain < ^ > in your browser to access your web server.
This will serve the Node.js Application, via reverse proxy, on the app server where your files were deployed.
You'll see a Hello World greeting.
< $> note Note: After the first deployment, your Git repository will be tracking a newly created file named ecosystem.config.js.
As this file will be rebuilt on each deploy, and may contain compiled application secrets it should be added to the .gitignore file in the application root directory on your local machine prior to your next git commit.
You've deployed your Node.js application to your app server, that refers to your new deployment.
With everything up and running, you can move on to monitoring your application processes.
Step 6 - Monitoring Your Application
PM2 is a great tool for managing your remote processes, but it also provides features to monitor the performance of these application processes.
Connect to your remote app server via SSH with this command:
To obtain specific information related to your PM2 managed processes, run the following:
You'll see a summary of the information PM2 has collected.
To see detailed information, you can run:
The output expands on the summary information provided by the pm2 list command.
It also provides information on a number of ancillary commands and provides log file locations:
PM2 also provides an in-terminal monitoring tool, accessible with:
The output of this command is an interactive dashboard, where pm2 provides realtime process information, logs, metrics, and metadata.
This dashboard may assist in monitoring resources and error logs:
With an understanding of how you can monitor your processes with PM2, you can move on to how Shipit can assist in rolling back to a previous working deployment.
End your ssh session on your app server by running exit.
Step 7 - Rolling Back a Bugged Deployment
Deployments occasionally expose unforeseen bugs, or issues that cause your site to fail.
The developers and maintainers of Shipit have anticipated this and have provided the ability for you to roll back to the previous (working) deployment of your application.
To ensure your PM2 configuration persists, add another event listener to shipitfile.js on the rollback event:
You add a listener to the rollback event to run your npm-install and copy-config tasks.
This is needed because unlike the published event, the updated event is not run by the Shipit lifecycle when rolling back a deployment.
Adding this event listener ensures your PM2 process manager points to the most recent deployment, even in the event of a rollback.
This process is similar to deploying, with a minor change in command.
To try rolling back to a previous deployment you can execute the following:
Like the deploy command, rollback provides details on the roll back process and the tasks being executed:
You have configured Shipit to keep 5 releases through the keepReleases: 5 configuration in shipitfile.js.
Shipit keeps track of these releases internally to ensure it is able to roll back when required.
Shipit also provides a handy way to identify the releases by creating a directory named as a timestamp (YYYYMMDDHHmmss - Example: / home / deployer / < ^ > your-domain < ^ > / releases / 20190420210548).
If you wanted to further customize the roll back process, you can listen for events specific to the roll back operation.
You can then use these events to execute tasks that will complement your roll back.
You can refer to the event list provided in the breakdown of the Shipit lifecycle and configure the tasks / listeners within your shipitfile.js.
The ability to roll back means that you can always serve a functioning version of your application to your users even if a deployment introduces unexpected bugs / issues.
In this tutorial, you configured a workflow that allows you to create a highly customizable alternative to Platform as a Service, all from a couple of servers.
This workflow allows for customized deployment and configuration, process monitoring with PM2, the potential to scale and add services, or additional servers or environments to the deployment when required.
If you are interested in continuing to develop your Node.js skills, check out the DigtalOcean Node.js content as well as the How To Code in Node.js Series.
How To Build a Node.js Application with Docker Quickstart
3698
This tutorial will walk you through creating an application image for a static website that uses the Express framework and Bootstrap.
You will then build a container using that image, push it to Docker Hub, and use it to build another container, demonstrating how you can recreate and scale your application.
For a more detailed version of this tutorial, with more detailed explanations of each step, please refer to How To Build a Node.js Application with Docker.
A sudo user on your server or in your local environment.
Docker.
Node.js and npm.
A Docker Hub account.
Step 1 - Installing Your Application Dependencies
First, create a directory for your project in your non-root user's home directory:
Navigate to this directory:
This will be the root directory of the project.
Next, create a package.json with your project's dependencies:
Add the following information about the project to the file; be sure to replace the author information with your own name and contact details:
Install your project's dependencies:
Step 2 - Creating the Application Files
We will create a website that offers users information about sharks.
Open app.js in the main project directory to define the project's routes:
Add the following content to the file to create the Express application and Router objects, define the base directory, port, and host as variables, set the routes, and mount the router middleware along with the application's static assets:
Next, let's add some static content to the application.
Create the views directory:
Open index.html:
Add the following code to the file, which will import Boostrap and create a jumbotron component with a link to the more detailed sharks.html info page:
Next, open a file called sharks.html:
Add the following code, which imports Bootstrap and the custom style sheet and offers users detailed information about certain sharks:
Finally, create the custom CSS style sheet that you've linked to in index.html and sharks.html by first creating a css folder in the views directory:
Open the style sheet and add the following code, which will set the desired color and font for our pages:
Start the application:
Navigate your browser to http: / / < ^ > your _ server _ ip < ^ >: 8080 or localhost: 8080 if you are working locally.
You will see the following landing page:
Application Landing Page
Click on the Get Shark Info button.
You will see the following information page:
Shark Info Page
You now have an application up and running.
When you are ready, quit the server by typing CTRL + C.
Step 3 - Writing the Dockerfile
In your project's root directory, create the Dockerfile:
This Dockerfile uses an alpine base image and ensures that application files are owned by the non-root node user that is provided by default by the Docker Node image.
Next, add your local node modules, npm logs, Dockerfile, and .dockerignore to your .dockerignore file:
Build the application image using the docker build command:
The. specifies that the build context is the current directory.
Check your images:
Run the following command to build a container using this image:
Inspect the list of your running containers with docker ps:
With your container running, you can now visit your application by navigating your browser to http: / / < ^ > your _ server _ ip < ^ > or localhost.
You will see your application landing page once again:
Now that you have created an image for your application, you can push it to Docker Hub for future use.
Step 4 - Using a Repository to Work with Images
The first step to pushing the image is to log in to the your Docker Hub account:
Logging in this way will create a ~ / .docker / config.json file in your user's home directory with your Docker Hub credentials.
Push your image up using your own username in place of < ^ > your _ dockerhub _ username < ^ >:
If you would like, you can test the utility of the image registry by destroying your current application container and image and rebuilding them.
First, list your running containers:
Using the CONTAINER ID listed in your output, stop the running application container.
Be sure to replace the highlighted ID below with your own CONTAINER ID:
List your all of your images with the -a flag:
You will see the following output with the name of your image, < ^ > your _ dockerhub _ username < ^ > / < ^ > nodejs-image-demo < ^ >, along with the node image and the other images from your build:
Remove the stopped container and all of the images, including unused or dangling images, with the following command:
With all of your images and containers deleted, you can now pull the application image from Docker Hub:
List your images once again:
You will see your application image:
You can now rebuild your container using the command from Step 3:
List your running containers:
Visit http: / / < ^ > your _ server _ ip < ^ > or localhost once again to view your running application.
How To Install Docker Compose on Ubuntu 18.04.
How To Provision and Manage Remote Docker Hosts with Docker Machine on Ubuntu 18.04.
How To Share Data between Docker Containers.
How To Share Data Between the Docker Container and the Host.
You can also look at the longer series on From Containers to Kubernetes with Node.js, from which this tutorial is adapated.
Additionally, see our full library of Docker resources for more on Docker.
How To Deploy and Manage Your DNS Using DNSControl on Debian 10
3343
The author selected the Electronic Frontier Foundation Inc to receive a donation as part of the Write for DOnations program.
DNSControl is an infrastructure-as-code tool that allows you to deploy and manage your DNS zones using standard software development principles, including version control, testing, and automated deployment.
DNSControl was created by Stack Exchange and is written in Go.
Using DNSControl eliminates many of the pitfalls of manual DNS management, as zone files are stored in a programmable format.
This allows you to deploy zones to multiple DNS providers simultaneously, identify syntax errors, and push out your DNS configuration automatically, reducing the risk of human error.
Another common usage of DNSControl is to quickly migrate your DNS to a different provider; for example, in the event of a DDoS attack or system outage.
In this tutorial, you'll install and configure DNSControl, create a basic DNS configuration, and begin deploying DNS records to a live provider.
As part of this tutorial, we will use DigitalOcean as the example DNS provider.
If you wish to use a different provider, the setup is very similar.
When you're finished, you'll be able to manage and test your DNS configuration in a safe, offline environment, and then automatically deploy it to production.
One Debian 10 server set up by following the Initial Server Setup with Debian 10, including a sudo non-root user and enabled firewall to block non-essential ports. < ^ > your-server-ipv4-address < ^ > refers to the IP address of the server where you're hosting your website or domain. < ^ > your-server-ipv6-address < ^ > refers to the IPv6 address of the server where you're hosting your website or domain.
A fully registered domain name with DNS hosted by a supported provider.
This tutorial will use < ^ > your _ domain < ^ > throughout and DigitalOcean as the service provider.
A DigitalOcean API key (Personal Access Token) with read and write permissions.
To create one, visit How to Create a Personal Access Token.
Step 1 - Installing DNSControl
DNSControl is written in Go, so you'll start this step by installing Go to your server and setting your GOPATH.
Go is available within Debian's default software repositories, making it possible to install using conventional package management tools.
You'll also need to install Git, as this is required to allow Go to download and install the DNSControl software from it's repository on GitHub.
Begin by updating the local package index to reflect any new upstream changes:
Then, install the golang-go and git packages:
After confirming the installation, apt will download and install Go and Git, as well as all of their required dependencies.
Next, you'll configure the required path environment variables for Go.
If you would like to know more about this, you can read this tutorial on Understanding the GOPATH.
Start by editing the ~ / .profile file:
Add the following lines to the very end of your file:
Once you have added these lines to the bottom of the file, save and close it. Then reload your profile by either logging out and back in, or sourcing the file again:
Now you've installed and configured Go, you can install DNSControl.
The go get command can be used to fetch a copy of the code, automatically compile it, and install it into your Go directory:
Once this is complete, you can check the installed version to make sure that everything is working:
Your output will look similar to the following:
If you see a dnscontrol: command not found error, double-check your Go path setup.
Now that you've installed DNSControl, you can create a configuration directory and connect DNSControl to your DNS provider in order to allow it to make changes to your DNS records.
Step 2 - Configuring DNSControl
In this step, you'll create the required configuration directories for DNSControl, and connect it to your DNS provider so that it can begin to make live changes to your DNS records.
Firstly, create a new directory in which you can store your DNSControl configuration, and then move into it:
< $> note Note: This tutorial will focus on the initial set up of DNSControl; however for production use it is recommended to store your DNSControl configuration in a version control system (VCS) such as Git.
The advantages of this include full version control, integration with CI / CD for testing, seamlessly rolling-back deployments, and so on.
If you plan to use DNSControl to write BIND zone files, you should also create the zones directory:
BIND zone files are a raw, standardized method for storing DNS zones / records in plain text format.
They were originally used for the BIND DNS server software, but are now widely adopted as the standard method for storing DNS zones.
BIND zone files produced by DNSControl are useful if you want to import them to a custom or self-hosted DNS server, or for auditing purposes.
However, if you just want to use DNSControl to push DNS changes to a managed provider, the zones directory will not be needed.
Next, you need to configure the creds.json file, which is what will allow DNSControl to authenticate to your DNS provider and make changes.
The format of creds.json differs slightly depending on the DNS provider that you are using.
Please see the Service Providers list in the official DNSControl documentation to find the configuration for your own provider.
Create the file creds.json in the ~ / dnscontrol directory:
Add the sample creds.json configuration for your DNS provider to the file.
If you're using DigitalOcean as your DNS provider, you can use the following:
This file tells DNSControl to which DNS providers you want it to connect.
You'll need to provide some form of authentication for your DNS provider.
This is usually an API key or OAuth token, but some providers require extra information, as documented in the Service Providers list in the official DNSControl documentation.
< $> warning Warning: This token will grant access to your DNS provider account, so you should protect it as you would a password.
Also, ensure that if you're using a version control system, either the file containing the token is excluded (e.g. using .gitignore), or is securely encrypted in some way.
If you're using DigitalOcean as your DNS provider, you can use the required OAuth token in your DigitalOcean account settings that you generated as part of the prerequisites.
If you have multiple different DNS providers - for example, for multiple domain names, or delegated DNS zones - you can define these all in the same creds.json file.
You've set up the initial DNSControl configuration directories, and configured creds.json to allow DNSControl to authenticate to your DNS provider and make changes.
Next you'll create the configuration for your DNS zones.
Step 3 - Creating a DNS Configuration File
In this step, you'll create an initial DNS configuration file, which will contain the DNS records for your domain name or delegated DNS zone.
dnsconfig.js is the main DNS configuration file for DNSControl.
In this file, DNS zones and their corresponding records are defined using JavaScript syntax.
This is known as a DSL, or Domain Specific Language.
The JavaScript DSL page in the official DNSControl documentation provides further details.
To begin, create the DNS configuration file in the ~ / dnscontrol directory:
Then, add the following sample configuration to the file:
This sample file defines a domain name or DNS zone at a particular provider, which in this case is < ^ > your _ domain < ^ > hosted by DigitalOcean.
An example A record is also defined for the zone root (@), pointing to the IPv4 address of the server that you're hosting your domain / website on.
There are three main functions that make up a basic DNSControl configuration file:
NewRegistrar (name, type, metadata): defines the domain registrar for your domain name.
DNSControl can use this to make required changes, such as modifying the authoritative nameservers.
If you only want to use DNSControl to manage your DNS zones, this can generally be left as NONE.
NewDnsProvider (name, type, metadata): defines a DNS service provider for your domain name or delegated zone.
This is where DNSControl will push the DNS changes that you make.
D (name, registrar, modifiers): defines a domain name or delegated DNS zone for DNSControl to manage, as well as the DNS records present in the zone.
You should configure NewRegistrar (), NewDnsProvider (), and D () accordingly using the Service Providers list in the official DNSControl documentation.
If you're using DigitalOcean as your DNS provider, and only need to be able to make DNS changes (rather than authoritative nameservers as well), the sample in the preceding code block is already correct.
Once complete, save and close the file.
In this step, you set up a DNS configuration file for DNSControl, with the relevant providers defined.
Next, you'll populate the file with some useful DNS records.
Step 4 - Populating Your DNS Configuration File
Next, you can populate the DNS configuration file with useful DNS records for your website or service, using the DNSControl syntax.
Unlike traditional BIND zone files, where DNS records are written in a raw, line-by-line format, DNS records within DNSControl are defined as a function parameter (domain modifier) to the D () function, as shown briefly in Step 3.
A domain modifier exists for each of the standard DNS record types, including A, AAAA, MX, TXT, NS, CAA, and so on.
A full list of available record types is available in the Domain Modifiers section of the DNSControl documentation.
Modifiers for individual records are also available (record modifiers).
Currently these are primarily used for setting the TTL (time to live) of individual records.
A full list of available record modifiers is available in the Record Modifiers section of the DNSControl documentation.
Record modifiers are optional, and in most basic use cases can be left out.
The syntax for setting DNS records varies slightly for each record type.
Following are some examples for the most common record types:
A records:
Purpose: To point to an IPv4 address.
Syntax: A (' < ^ > name < ^ > ',' < ^ > address < ^ > ', optional record modifiers)
Example: A (' < ^ > @ < ^ > ',' < ^ > your-server-ipv4-address < ^ > ', TTL (< ^ > 30 < ^ >))
AAAA records:
Purpose: To point to an IPv6 address.
Syntax: AAAA (' < ^ > name < ^ > ',' < ^ > address < ^ > ', optional record modifiers)
Example: AAAA (' < ^ > @ < ^ > ',' < ^ > your-server-ipv6-address < ^ > ') (record modifier left out, so default TTL will be used)
CNAME records:
Purpose: To make your domain / subdomain an alias of another.
Syntax: CNAME (' < ^ > name < ^ > ',' < ^ > target < ^ > ', optional record modifiers)
Example: CNAME (' < ^ > subdomain1 < ^ > ',' < ^ > example.org. < ^ > ') (note that a trailing. must be included if there are any dots in the value)
MX records:
Purpose: To direct email to specific servers / addresses.
Syntax: MX (' < ^ > name < ^ > ',' < ^ > priority < ^ > ',' < ^ > target < ^ > ', optional record modifiers)
Example: MX (' < ^ > @ < ^ > ', < ^ > 10 < ^ >,' < ^ > mail.example.net < ^ > ') (note that a trailing. must be included if there are any dots in the value)
TXT records:
Purpose: To add arbitrary plain text, often used for configurations without their own dedicated record type.
Syntax: TXT (' < ^ > name < ^ > ',' < ^ > content < ^ > ', optional record modifiers)
Example: TXT (' < ^ > @ < ^ > ',' < ^ > This is a TXT record.
< ^ > ')
CAA records:
Purpose: To restrict and report on Certificate Authorities (CAs) who can issue TLS certificates for your domain / subdomains.
Syntax: CAA (' < ^ > name < ^ > ',' < ^ > tag < ^ > ',' < ^ > value < ^ > ', optional record modifiers)
Example: CAA (' < ^ > @ < ^ > ',' < ^ > issue < ^ > ',' < ^ > letsencrypt.org < ^ > ')
In order to begin adding DNS records for your domain or delegated DNS zone, edit your DNS configuration file:
Next, you can begin populating the parameters for the existing D () function using the syntax described in the previous list, as well as the Domain Modifiers section of the official DNSControl documentation.
A comma (,) must be used in-between each record.
For reference, the code block here contains a full sample configuration for a basic, initial DNS setup:
Once you have completed your initial DNS configuration, save and close the file.
In this step, you set up the initial DNS configuration file, containing your DNS records.
Next, you will test the configuration and deploy it.
Step 5 - Testing and Deploying Your DNS Configuration
In this step, you will run a local syntax check on your DNS configuration, and then deploy the changes to the live DNS server / provider.
Firstly, move into your dnscontrol directory:
Next, use the preview function of DNSControl to check the syntax of your file, and output what changes it will make (without actually making them):
If the syntax of your DNS configuration file is correct, DNSControl will output an overview of the changes that it will make.
This should look similar to the following:
If you see an error warning in your output, DNSControl will provide details on what and where the error is located within your file.
< $> warning Warning: The next command will make live changes to your DNS records and possibly other settings.
Please ensure that you are prepared for this, including taking a backup of your existing DNS configuration, as well as ensuring that you have the means to roll back if needed.
Finally, you can push out the changes to your live DNS provider:
You'll see an output similar to the following:
Now, if you check the DNS settings for your domain in the DigitalOcean control panel, you'll see the changes.
A screenshot of the DigitalOcean control panel, showing some of the DNS changes that DNSControl has made.
You can also check the record creation by running a DNS query for your domain / delegated zone using dig.
If you don't have dig installed, you'll need to install the dnsutils package:
Once you've installed dig, you can use it to make a DNS lookup for your domain.
You'll see that the records have been updated accordingly:
You'll see output showing the IP address and relevant DNS record from your zone that was deployed using DNSControl.
DNS records can take some time to propagate, so you may need to wait and run this command again.
In this final step, you ran a local syntax check of the DNS configuration file, then deployed it to your live DNS provider, and tested that the changes were made successfully.
In this article you set up DNSControl and deployed a DNS configuration to a live provider.
Now you can manage and test your DNS configuration changes in a safe, offline environment before deploying them to production.
If you wish to explore this subject further, DNSControl is designed to be integrated into your CI / CD pipeline, allowing you to run in-depth tests and have more control over your deployment to production.
You could also look into integrating DNSControl into your infrastructure build / deployment processes, allowing you to deploy servers and add them to DNS completely automatically.
If you wish to go further with DNSControl, the following DigitalOcean articles provide some interesting next steps to help integrate DNSControl into your change management and infrastructure deployment workflows:
An Introduction to Continuous Integration, Delivery, and Deployment
CI / CD Tools Comparison: Jenkins, GitLab CI, Buildbot, Drone, and Concourse
Getting Started with Configuration Management
How To Import Existing DigitalOcean Assets into Terraform
3759
Terraform is an infrastructure as code tool created by HashiCorp that helps developers with deploying, updating, and removing different assets of their infrastructure in an efficient and more scalable way.
Developers can use Terraform to organize different environments, track changes through version control, and automate repetitive work to limit human error.
It also provides a way for teams to collaborate on improving their infrastructure through shared configurations.
In this tutorial you'll import existing DigitalOcean infrastructure into Terraform.
By the end of this tutorial you'll be able to use Terraform for all of your existing infrastructure in addition to creating new assets.
A DigitalOcean Personal Access Token.
To create this, you can follow the How To Create a Personal Access Token guide.
A DigitalOcean Droplet with a tag.
You can use the following guide on How To Create a Droplet from the DigitalOcean Control Panel.
A DigitalOcean Cloud Firewall applied to your Droplet.
You can use the guide How To Create Firewalls.
The DigitalOcean Command Line Client installed on your local machine by following the install instructions on the doctl GitHub page.
You can read the following tutorial for guidance on How To Use Doctl, the official DigitalOcean Command-Line Client.
Step 1 - Installing Terraform Locally
In this first step you'll install Terraform on your local machine.
This step details the installation of the Linux binary.
If you use Windows or Mac, you can check the Download Terraform page on the Terraform website.
Move to the folder you want to download Terraform to on your local machine, then use the wget tool to download the Terraform < ^ > 0.12.12 < ^ > binary:
To check if the sha256 checksum is the same value provided on the Terraform website, you'll download the checksum file with the following command:
Then run the following command to verify the checksums:
The SHA256SUMS file you downloaded lists the filenames and their hashes.
This command will look for the same file terraform _ < ^ > 0.12.12 < ^ > _ SHA256SUMS locally and then check that the hashes match by using the -c flag.
Since this file has more than one filename and its platform listed, you use the --ignore-missing flag to avoid errors in your output because you don't have a copy of the other files.
You will see output like the following:
Use unzip to extract the binary:
Now check if Terraform is installed properly by checking the version:
You've installed Terraform to your local machine, you'll now prepare the configuration files.
Step 2 - Preparing Terraform Configuration Files
In this step you'll import your existing assets into Terraform by creating a project directory and writing configuration files.
Since Terraform doesn't support generating configs from the import command at this time, you need to create those configurations manually.
Run the following command to create your project directory:
Then move into that directory with:
Within this step you'll create three additional files that will contain the required configurations.
Your directory structure for this project will look like the following:
To begin you'll create the file provider.tf to define your DigitalOcean Access Token as an environment variable instead of hardcoding it into your configuration.
< $> warning Warning: Your access token gives access to your complete infrastructure with unrestricted access, so treat it as such.
Be sure that you're the only one who has access to the machine where that token is stored.
Besides your access token, you'll also specify which provider you want to use.
In this tutorial that's digitalocean.
For a full list of available Data Sources and Resources for DigitalOcean with Terraform, visit the Providers page on their website.
Create and edit provider.tf with the following command:
Add the following content into the provider.tf file:
In this file you add your DigitalOcean Access Token as a variable, which Terraform will use as identification for the DigitalOcean API.
You also specify the version of the DigitalOcean provider plugin.
Terraform recommends that you specify which version of the provider you're using so that future updates don't potentially break your current setup.
Now you'll create the digitalocean _ droplet.tf file.
Here you'll specify the resource that you're going to use, in this case: droplet.
Create the file with the following command:
Add the following configuration:
Here you specify four parameters:
name: The Droplet name.
region: The region that the Droplet is located in.
tags: A list of the tags that are applied to this Droplet.
count: The number of resources needed for this configuration.
Next you'll create a configuration file for your firewall.
Create the file digitalocean _ firewall.tf with the following command:
Add the following content to the file:
Here you specify the name of the firewall you wish to import and the tags of the Droplets to which the firewall rules apply.
Finally the count value of 1 defines the required number of the particular resource.
< $> note Note: You can include firewall resources in the digitalocean _ droplet.tf file as well, however if you have multiple environments where multiple Droplets share the same firewall, it's a good idea to separate it in case you only want to remove a single Droplet.
This will then leave the firewall unaffected.
Now it's time to initialize those changes so Terraform can download the required dependencies.
You will use the terraform init command for this, which will allow you to initialize a working directory containing Terraform configuration files.
Run this command from your project directory:
Terraform has successfully prepared the working directory by downloading plugins, searching for modules, and so on.
Next you'll begin importing your assets to Terraform.
Step 3 - Importing Your Assets to Terraform
In this step, you'll import your DigitalOcean assets to Terraform.
You'll use doctl to find the ID numbers of your Droplets before importing your assets.
You'll then check the import configuration with the terraform show and terraform plan commands.
To begin, you'll export your DigitalOcean Access Token as an environment variable, which you'll then inject into Terraform during runtime.
Export it as an environment variable into your current shell session with the following command:
In order to import your existing Droplet and firewall you'll need their ID numbers.
You can use doctl, the command line interface for the DigitalOcean API.
Run the following command to list your Droplets and access their IDs:
Now you'll import your existing Droplet and firewall into Terraform:
You use the -var flag to specify your DigitalOcean Access Token value that you previously exported to your shell session.
This is needed so the DigitalOcean API can verify who you are and apply changes to your infrastructure.
Now run the same command for your firewall:
You'll check that the import was successful by using the terraform show command.
This command provides human-readable output of your infrastructure state.
It can be used to inspect a plan to ensure that wanted changes are going to be executed, or to inspect the current state as Terraform sees it.
In this context state refers to the mapping of your DigitalOcean assets to the Terraform configuration that you've written and the tracking of metadata.
This allows you to confirm that there's no difference between existing DigitalOcean assets that you want to import and assets that Terraform is keeping track of:
You'll see two resources in the output along with their attributes.
After you import your Droplet and firewall into Terraform state, you need to make sure that configurations represent the current state of the imported assets.
To do this, you'll specify your Droplet's image and its size.
You can find these two values in the output of terraform show for digitalocean _ droplet.do _ droplet resource.
Open the digitalocean _ droplet.tf file:
In this tutorial:
The operating system image used for our existing Droplet is ubuntu-16-04-x64.
The region your Droplet is located in is fra1.
The Droplet tag for your existing Droplet is terraform-testing.
The Droplet you imported using the configuration in digitalocean _ droplet.tf will look like this:
Next you'll add in the firewall rules.
In our example, open ports for inbound traffic are 22, 80, and 443. All ports are opened for outbound traffic.
You can adjust this configuration accordingly to your open ports.
Open digitalocean _ firewall.tf:
These rules replicate the state of the existing example firewall.
If you'd like to limit traffic to different IP addresses, different ports, or different protocol, you can adjust the file to replicate your existing firewall.
After you've updated your Terraform files, you'll use the plan command to see if changes you made replicate state of existing assets on DigitalOcean.
The terraform plan command is used as a dry run.
With this command you can check if changes Terraform is going to make are the changes you want to make.
It is a good idea to always run this command for confirmation before applying changes.
Run terraform plan with the following:
You'll see output similar to the following output:
You've successfully imported existing DigitalOcean assets in Terraform, and now you can make changes to your infrastructure through Terraform without the risk of accidentally deleting or modifying existing assets.
Step 4 - Creating New Assets via Terraform
In this step you'll add two additional Droplets to your existing infrastructure.
Adding assets in this way to your existing infrastructure can be useful, for example, if you have a live website and don't want to make any potentially breaking changes to that website while working on it. Instead you can add one more Droplet to use as a development environment and work on your project in the same environment as the production Droplet, without any of the potential risk.
Now open digitalocean _ droplet.tf to add the rules for your new Droplets:
Add the following lines to your file:
You use the count meta-argument to tell Terraform how many Droplets with the same specifications you want.
These new Droplets will also be added to your existing firewall as you specify the same tag as per your firewall.
Apply these rules to check the changes you're specifying in digitalocean _ droplet.tf:
Verify that the changes you want to make are replicated in the output of this command.
Once you're satisfied with the output, use the terraform apply command to apply the changes you've specified to the state of the configuration:
Confirm the changes by entering yes on the command line.
After successful execution, you'll see output similar to the following:
You'll see two new Droplets in your DigitalOcean web panel: New Droplets
You'll also see them attached to your existing firewall: Existing Firewall
You've created new assets with Terraform using your existing assets.
To learn how to destroy these assets you can optionally complete the next step.
Step 5 - Destroying Imported and Created Assets (Optional)
In this step, you'll destroy assets that you've imported and created by adjusting the configuration.
Begin by opening digitalocean _ droplet.tf:
In the file, set the count to 0 as per the following:
Open your firewall configuration file to alter the count as well:
Set the count to 0 like the following highlighted line:
Now apply those changes with the following command:
Terraform will ask you to confirm if you wish to destroy the Droplets and firewall.
This will destroy all assets you imported and created via Terraform, so ensure you verify that you wish to proceed before typing yes.
You've deleted all assets managed by Terraform.
This is a useful workflow if you no longer need an asset or are scaling down.
In this tutorial you installed Terraform, imported existing assets, created new assets, and optionally destroyed those assets.
You can scale this workflow to a larger project, such as deploying a production-ready Kubernetes cluster.
Using Terraform you could manage all of the nodes, DNS entries, firewalls, storage, and other assets, as well as use version control to track changes and collaborate with a team.
To explore further features of Terraform read their documentation.
You can also read DigitalOcean's Terraform content for further tutorials and Q & A.
How To Install and Use TimescaleDB on CentOS 7
3694
The author selected the Computer History Museum to receive a donation as part of the Write for DOnations program.
Many applications, such as monitoring systems and data collection systems, accumulate data for further analysis.
These analyses often look at the way a piece of data or a system changes over time.
In these instances, data is represented as a time series, with every data point accompanied by a timestamp.
An example would look like this:
The relevance of time series data has recently grown thanks to the new deployments of the Internet of Things (IoT) and Industrial Internet of Things.
There are more and more devices that collect various time series information: fitness trackers, smart watches, home weather stations, and various sensors, to name a few.
These devices collect a lot of information, and all this data must be stored somewhere.
Classic relational databases are most often used to store data, but they don't always fit when it comes to the huge data volumes of time series.
When you need to process a large amount of time series data, relational databases can be too slow.
Because of this, specially optimized databases, called NoSQL databases, have been created to avoid the problems of relational databases.
TimescaleDB is an open-source database optimized for storing time series data. It is implemented as an extension of PostgreSQL and combines the ease-of-use of relational databases and the speed of NoSQL databases.
As a result, it allows you to use PostgreSQL for both storing business data and time series data in one place.
By following this tutorial, you'll set up TimescaleDB on CentOS 7, configure it, and learn how to work with it. You'll run through creating time series databases and making simple queries.
Finally, you'll see how to remove unnecessary data.
One CentOS 7 server set up by following our Initial Server Setup with CentOS 7 guide, including a non-root user with sudo privileges and a firewall set up with firewalld.
To set up firewalld, follow the "Configuring a Basic Firewall" section of the Additional Recommended Steps for New CentOS 7 Servers tutorial.
PostgreSQL installed on your server.
Follow our How To Install and Use PostgreSQL on CentOS 7 guide to install and configure it.
Step 1 - Installing TimescaleDB
TimescaleDB is not available in CentOS default package repositories, so in this step you will install it from the TimescaleDB's third-party repository.
First, create a new repository file:
Enter insert mode by pressing i and paste the following configuration into the file:
When you're finished, press ESC to leave insert mode, then: wq and ENTER to save and exit the file.
To learn more about the text editor vi and its successor vim, check out our Installing and Using the Vim Text Editor on a Cloud Server tutorial.
You can now proceed with the installation.
This tutorial uses PostgreSQL version 11; if you are using a different version of PostgreSQL (9.6 or 11, for example), replace the value in the following command and run it:
TimescaleDB is now installed and ready to be used.
Next, you will turn it on and adjust some of the settings associated with it in the PostgreSQL configuration file to optimize the database.
Step 2 - Configuring TimescaleDB
The TimescaleDB module works fine with the default PostgreSQL configuration settings, but to improve performance and make better use of processor, memory, and disk resources, developers of TimescaleDB suggest configuring some individual parameters.
This can be done automatically with the timescaledb-tune tool or by manually editing your server's postgresql.conf file.
In this tutorial, you will use the timescaledb-tune tool.
It reads the postgresql.conf file and interactively suggests making changes.
Run the following command to start the configuration wizard:
First, you will be asked to confirm the path to the PostgreSQL configuration file:
The utility automatically detects the path to the configuration file, so confirm this by entering y:
Next, enable the TimescaleDB module by typing y at the next prompt and pressing ENTER:
Based on the characteristics of your server and the PostgreSQL version, you will then be offered to tune your settings.
Press y to start the tuning process:
timescaledb-tune will automatically detect the server's available memory and calculate recommended values for the shared _ buffers, effective _ cache _ size, maintenance _ work _ mem, and work _ mem settings.
If you'd like to learn more about how this is done, check out the GitHub page for timescaledb-tune.
If these settings look OK, enter y:
At this point, if your server has multiple CPUs, you will find the recommendations for parallelism settings.
However if you have one CPU, timescaledb-tune will send you directly to the WAL settings.
Those with multiple CPUs will encounter recommendations like this:
These settings regulate the number of workers that process requests and background tasks.
You can learn more about these settings from the TimescaleDB and PostgreSQL documentation.
Type y then ENTER to accept these settings:
Next, you will find recommendations for Write Ahead Log (WAL):
WAL preserves data integrity, but the default settings can cause inefficient I / O that slows down write performance.
Type and enter y to optimize these settings:
You'll now find some miscellaneous recommendations:
All of these different parameters are aimed at increasing performance.
For example, SSDs can process many concurrent requests, so the best value for the effective _ io _ concurrency might be in the hundreds.
You can find more info about these options in the PostgreSQL documentation.
Press y then ENTER to continue.
As a result, you will get a ready-made configuration file at / var / lib / pgsql / < ^ > 11 < ^ > / data / postgresql.conf.
< $> note Note: If you are doing the installation from scratch, you could also run the initial command with the --quiet and --yes flags, which will automatically apply all the recommendations and will make changes to the postgresql.conf configuration file:
In order for the configuration changes to take effect, you must restart the PostgreSQL service:
Now the database is running with optimal parameters and is ready to work with the time series data. In the next steps, you'll try out working with this data: creating new databases and hypertables and performing operations.
Step 3 - Creating a New Database and Hypertable
With your TimescaleDB setup optimized, you are ready to work with time series data. TimescaleDB is implemented as an extension of PostgreSQL, so operations with time series data are not much different from relational data operations.
At the same time, the database allows you to freely combine data from time series and relational tables in the future.
First, you will create a new database and turn on the TimescaleDB extension for it. Log in to your PostgreSQL database:
Now create a new database and connect to it. This tutorial will name the database < ^ > timeseries < ^ >:
You can find additional information about working with the PostgreSQL database in our How To Create, Remove & Manage Tables in PostgreSQL on a Cloud Server tutorial.
Finally, enable the TimescaleDB extension:
The primary point of interaction with your timeseries data are hypertables, an abstraction of many individual tables holding the data, called chunks.
To create a hypertable, start with a regular SQL table and then convert it into a hypertable via the function create _ hypertable.
Make a table that will store data for tracking temperature and humidity across a collection of devices over time:
This command will create a table called conditions with four columns.
The first column will store the timestamp, which includes the time zone and cannot be empty.
Next, you will use the time column to transform your table into a hypertable that is partitioned by time:
This command calls the create _ hypertable () function, which creates a TimescaleDB hypertable from a PostgreSQL table, replacing the latter.
You will receive the following output:
In this step, you created a new hypertable to store timeseries data. Now you can populate it with data by writing to the hypertable, then run through the process of deleting it.
Step 4 - Writing and Deleting Data
In this step, you will insert data using standard SQL commands and import large sets of data from external sources.
This will show you the relational database aspects of TimescaleDB.
First, try out the basic commands.
Data can be inserted into the hypertable using the standard INSERT SQL command.
Insert some sample temperature and humidity data for the theoretical device weather-pro-000000 using the following command:
You can also insert multiple rows of data at once.
Try the following:
You will receive the following:
You can also specify that the INSERT command will return some or all of the inserted data using the RETURNING statement:
If you want to delete data from the hypertable, use the standard DELETE SQL command.
Run the following to delete whatever data has a temperature higher than 80 or a humidity higher than 50:
After the delete operation, it is recommended to use the VACUUM command, which will reclaim space still used by data that had been deleted.
You can find more info about VACUUM command in the PostgreSQL documentation.
These commands are fine for small-scale data entry, but since time series data often generates huge datasets from multiple devices simultaneously, it's essential also to know how to insert hundreds or thousands of rows at a time.
If you have prepared data from external sources in a structured form, for example in csv format, this task can be accomplished quickly.
To test this out, you will use a sample dataset that represents temperature and humidity data from a variety of locations.
It was created by TimescaleDB developers to allow you to test out their database.
You can check out more info about sample datasets in the TimescaleDB documentation.
Let's see how you can import data from the weather _ small sample dataset into your database.
First, quit Postgresql:
Then download the dataset and extract it:
Next, import the temperature and humidity data into your database:
This connects to the < ^ > timeseries < ^ > database and executes the\ COPY command that copies the data from the chosen file into the conditions hypertable.
It will run for a few seconds.
When the data has been entered into your table, you will receive the following output:
In this step, you added data to the hypertable manually and in batches.
Next, continue on to performing queries.
Step 5 - Querying Data
Now that your table contains data, you can perform various queries to analyze it.
To get started, log in to the database:
As mentioned before, to work with hypertables you can use standard SQL commands.
For example, to show the last 10 entries from the conditions hypertable, run the following command:
This command lets you see what data is in the database.
Since the database contains a million records, you used LIMIT 10 to limit the output to 10 entries.
To see the most recent entries, sort the data array by time in descending order:
This will output the top 20 most recent entries.
You can also add a filter.
For example, to see entries from the weather-pro-000000 device, run the following:
In this case, you will see the 10 most recent temperature and humidity datapoints recorded by the weather-pro-000000 device.
In addition to standard SQL commands, TimescaleDB also provides a number of special functions that are useful for timeseries data analysis.
For example, to find the median of the temperature values, you can use the following query with the percentile _ cont function:
In this way, you'll see the median temperature for the entire observation period where the weather-pro-00000 sensor is located.
To show the latest values from each of the sensors, you can use the last function:
In the output you will see a list of all the sensors and relevant latest values.
To get the first values use the first function.
The following example is more complex.
It will show the hourly average, minimum, and maximum temperatures for the chosen sensor within the last 24 hours:
Here you used the time _ bucket function, which acts as a more powerful version of the PostgreSQL date _ trunc function.
As a result, you will see which periods of the day the temperature rises or decreases:
You can find more useful functions in the TimescaleDB documentation.
Now you know how to handle your data. Next, you will go through how to delete unnecessary data and how to compress data.
Step 6 - Configuring Data Compression and Deletion
As data accumulates, it will take up more and more space on your hard drive.
To save space, the latest version of TimescaleDB provides a data compression feature.
This feature doesn't require tweaking any file system settings, and can be used to quickly make your database more efficient.
For more information on how this compression works, take a look at this Compression article from TimescaleDB.
First, enable the compression of your hypertable:
You will receive the following data:
< $> note Note: You can also set up TimescaleDB to compress data over the specified time period.
For example, you could run:
In this example, the data will be automatically compressed after a week.
You can see the statistics on the compressed data with the command:
You will then see a list of chunks with their statuses: compression status and how much space is taken up by uncompressed and compressed data in bytes.
If you don't have the need to store data for a long period of time, you can delete out-of-date data to free up even more space.
There is a special drop _ chunks function for this.
It allows you to delete chunks with data older than the specified time:
This query will drop all chunks from the hypertable conditions that only include data older than a day ago.
To automatically delete old data, you can configure a cron task.
See our tutorial to learn more about how to use cron to automate various system tasks.
Exit from the database:
Next, edit your crontab with the following command, which should be run from the shell:
Now add the following line to the end of the file:
This job will delete obsolete data that is older than one day at 1: 00 AM every day.
You've now set up TimescaleDB on your CentOS server.
You also tried out creating hypertables, inserting data into it, querying the data, compressing, and deleting unnecessary records.
With these examples, you'll be able to take advantage of TimescaleDB's key benefits over traditional relational database management systems for storing time-series data, including:
Higher data ingest rates
Quicker query performance
Time-oriented features
How To Set Up Apache Virtual Hosts on Ubuntu 18.04 Quickstart
3824
This tutorial will guide you through setting up multiple domains and websites using Apache virtual hosts on an Ubuntu 18.04 server.
During this process, you "ll learn how to serve different content to different visitors depending on which domains they are requesting.
For a more detailed version of this tutorial, with more explanations of each step, please refer to How To Set Up Apache Virtual Hosts on Ubuntu 18.04.
An Apache2 web server, which you can install with sudo apt install apache2
Step 1 - Create the Directory Structure
We "ll first make a directory structure that will hold the site data that we will be serving to visitors in our top-level Apache directory.
We "ll be using example domain names, highlighted below.
You should replace these with your actual domain names.
Step 2 - Grant Permissions
We should now change the permissions to our current non-root user to be able to modify the files.
Additionally, we "ll ensure that read access is permitted to the general web directory and all of the files and folders it contains so that pages can be served correctly.
Step 3 - Create Demo Pages for Each Virtual Host
Let's create some content to serve, we "ll make a demonstration index.html page for each site.
We can open up an index.html file in a text editor for our first site, using nano for example.
Within this file, create a domain-specific HTML document, like the following:
Save and close the file, then copy this file to use as the basis for our second site:
Open the file and modify the relevant pieces of information:
Save and close this file as well.
Step 4 - Create New Virtual Host Files
Apache comes with a default virtual host file called 000-default.conf that we "ll use as a template.
We "ll copy it over to create a virtual host file for each of our domains.
Create the First Virtual Host File
Start by copying the file for the first domain:
Open the new file in your editor (we "re using nano below) with root privileges:
We will customize this file for our own domain.
Modify the highlighted text below for your own circumstances.
At this point, save and close the file.
Copy First Virtual Host and Customize for Second Domain
Now that we have our first virtual host file established, we can create our second one by copying that file and adjusting it as needed.
Start by copying it:
Open the new file with root privileges in your editor:
You now need to modify all of the pieces of information to reference your second domain.
The final file should look something like this, with highlighted text corresponding to your own relevant domain information.
Step 5 - Enable the New Virtual Host Files
With our virtual host files created, we must enable them.
We "ll be using the a2ensite tool to achieve this goal.
Next, disable the default site defined in 000-default.conf:
When you are finished, you need to restart Apache to make these changes take effect and use systemctl status to verify the success of the restart.
Your server should now be set up to serve two websites.
Step 6 - Set Up Local Hosts File (Optional)
If you haven't been using actual domain names that you own to test this procedure and have been using some example domains instead, you can test your work by temporarily modifying the hosts file on your local computer.
On a local Mac or Linux machine, type the following:
For a local Windows machine, find instructions on altering your hosts file here.
Using the domains used in this guide, and replacing your server IP for the < ^ > your _ server _ IP < ^ > text, your file should look like this:
This will direct any requests for example.com and test.com on our computer and send them to our server.
Step 7 - Test your Results
Now that you have your virtual hosts configured, you can test your setup by going to the domains that you configured in your web browser:
You should see a page that looks like this:
You can also visit your second page and see the file you created for your second site.
If both of these sites work as expected, you "ve configured two virtual hosts on the same server.
If you adjusted your home computer "s hosts file, delete the lines you added.
Here are links to more additional guides related to this tutorial:
How To Set Up Apache Virtual Hosts on Ubuntu 18.04
Domains and DNS on DigitalOcean
How to Rewrite URLs with mod _ rewrite for Apache on Ubuntu 18.04
How To Set Up ReadWriteMany (RWX) Persistent Volumes with NFS on DigitalOcean Kubernetes
3800
With the distributed and dynamic nature of containers, managing and configuring storage statically has become a difficult problem on Kubernetes, with workloads now being able to move from one Virtual Machine (VM) to another in a matter of seconds.
To address this, Kubernetes manages volumes with a system of Persistent Volumes (PV), API objects that represent a storage configuration / volume, and PersistentVolumeClaims (PVC), a request for storage to be satisfied by a Persistent Volume.
Additionally, Container Storage Interface (CSI) drivers can help automate and manage the handling and provisioning of storage for containerized workloads.
These drivers are responsible for provisioning, mounting, unmounting, removing, and snapshotting volumes.
The digitalocean-csi integrates a Kubernetes cluster with the DigitalOcean Block Storage product.
A developer can use this to dynamically provision Block Storage volumes for containerized applications in Kubernetes.
However, applications can sometimes require data to be persisted and shared across multiple Droplets.
DigitalOcean's default Block Storage CSI solution is unable to support mounting one block storage volume to many Droplets simultaneously.
This means that this is a ReadWriteOnce (RWO) solution, since the volume is confined to one node.
The Network File System (NFS) protocol, on the other hand, does support exporting the same share to many consumers.
This is called ReadWriteMany (RWX), because many nodes can mount the volume as read-write.
We can therefore use an NFS server within our cluster to provide storage that can leverage the reliable backing of DigitalOcean Block Storage with the flexibility of NFS shares.
In this tutorial, you will configure dynamic provisioning for NFS volumes within a DigitalOcean Kubernetes (DOKS) cluster in which the exports are stored on DigitalOcean Block storage volumes.
You will then deploy multiple instances of a demo Nginx application and test the data sharing between each instance.
A DigitalOcean Kubernetes cluster with your connection configured as the kubectl default.
To create a Kubernetes cluster on DigitalOcean, see our Kubernetes Quickstart.
Instructions on how to configure kubectl are shown under the Connect to your Cluster step when you create your cluster.
The Helm package manager installed on your local machine, and Tiller installed on your cluster.
To do this, complete Steps 1 and 2 of the How To Install Software on Kubernetes Clusters with the Helm Package Manager tutorial.
< $> note Note: Starting with Helm version 3.0, Tiller no longer needs to be installed for Helm to work.
If you are using the latest version of Helm, see the Helm installation documentation for instructions.
Step 1 - Deploying the NFS Server with Helm
To deploy the NFS server, you will use a Helm chart.
Deploying a Helm chart is an automated solution that is faster and less error-prone than creating the NFS server deployment by hand.
First, make sure that the default chart repository stable is available to you by adding the repo:
Next, pull the metadata for the repository you just added.
This will ensure that the Helm client is updated:
To verify access to the stable repo, perform a search on the charts:
This will give you list of available charts, similar to the following:
This result means that your Helm client is running and up-to-date.
Now that you have Helm set up, install the nfs-server-provisioner Helm chart to set up the NFS server.
If you would like to examine the contents of the chart, take a look at its documentation on GitHub.
When you deploy the Helm chart, you are going to set a few variables for your NFS server to further specify the configuration for your application.
You can also investigate other configuration options and tweak them to fit the application's needs.
To install the Helm chart, use the following command:
This command provisions an NFS server with the following configuration options:
Adds a persistent volume for the NFS server with the --set flag.
This ensures that all NFS shared data persists across pod restarts.
For the persistent storage, uses the do-block-storage storage class.
Provisions a total of 200Gi for the NFS server to be able to split into exports.
< $> note Note: The persistence.size option will determine the total capacity of all the NFS volumes you can provision.
At the time of this publication, only DOKS version 1.16.2-do.3 and later support volume expanding, so resizing this volume will be a manual task if you are on an earlier version.
If this is the case, make sure to set this size with your future needs in mind.
After this command completes, you will get output similar to the following:
To see the NFS server you provisioned, run the following command:
This will show the following:
Next, check for the storageclass you created:
You now have an NFS server running, as well as a storageclass that you can use for dynamic provisioning of volumes.
Next, you can create a deployment that will use this storage and share it across multiple instances.
Step 2 - Deploying an Application Using a Shared PersistentVolumeClaim
In this step, you will create an example deployment on your DOKS cluster in order to test your storage setup.
This will be an Nginx web server app named web.
To deploy this application, first write the YAML file to specify the deployment.
Open up an nginx-test.yaml file with your text editor; this tutorial will use nano:
In this file, add the following lines to define the deployment with a PersistentVolumeClaim named nfs-data:
Save the file and exit the text editor.
This deployment is configured to use the accompanying PersistentVolumeClaim nfs-data and mount it at / data.
In the PVC definition, you will find that the storageClassName is set to nfs.
This tells the cluster to satisfy this storage using the rules of the nfs storageClass you created in the previous step.
The new PersistentVolumeClaim will be processed, and then an NFS share will be provisioned to satisfy the claim in the form of a Persistent Volume.
The pod will attempt to mount that PVC once it has been provisioned.
Once it has finished mounting, you will verify the ReadWriteMany (RWX) functionality.
Run the deployment with the following command:
Next, check to see the web pod spinning up:
This will output the following:
Now that the example deployment is up and running, you can scale it out to three instances using the kubectl scale command:
This will give the output:
Now run the kubectl get command again:
You will find the scaled-up instances of the deployment:
You now have three instances of your Nginx deployment that are connected into the same Persistent Volume.
In the next step, you will make sure that they can share data between each other.
Step 3 - Validating NFS Data Sharing
For the final step, you will validate that the data is shared across all the instances that are mounted to the NFS share.
To do this, you will create a file under the / data directory in one of the pods, then verify that the file exists in another pod's / data directory.
To validate this, you will use the kubectl exec command.
This command lets you specify a pod and perform a command inside that pod.
To learn more about inspecting resources using kubectl, take a look at our kubectl Cheat Sheet.
To create a file named hello _ world within one of your web pods, use the kubectl exec to pass along the touch command.
Note that the number after web in the pod name will be different for you, so make sure to replace the highlighted pod name with one of your own pods that you found as the output of kubectl get pods in the last step.
Next, change the name of the pod and use the ls command to list the files in the / data directory of a different pod:
Your output will show the file you created within the first pod:
This shows that all the pods share data using NFS and that your setup is working properly.
In this tutorial, you created an NFS server that was backed by DigitalOcean Block Storage.
The NFS server then used that block storage to provision and export NFS shares to workloads in a RWX-compatible protocol.
In doing this, you were able to get around a technical limitation of DigitalOcean block storage and share the same PVC data across many pods.
In following this tutorial, your DOKS cluster is now set up to accommodate a much wider set of deployment use cases.
If you'd like to learn more about Kubernetes, check out our Kubernetes for Full-Stack Developers curriculum, or look through the product documentation for DigitalOcean Kubernetes.
How To Set Up the code-server Cloud IDE Platform on Debian 10
3331
With developer tools moving to the cloud, creation and adoption of cloud IDE (Integrated Development Environment) platforms is growing.
Cloud IDEs allow for real-time collaboration between developer teams to work in a unified development environment that minimizes incompatibilities and enhances productivity.
Accessible through web browsers, cloud IDEs are available from every type of modern device.
Visual Studio Code is a modern code editor with integrated Git support, a code debugger, smart autocompletion, and customizable and extensible features.
This means that you can use various devices running different operating systems, and always have a consistent development environment on hand.
In this tutorial, you will set up the code-server cloud IDE platform on your Debian 10 machine and expose it at your domain, secured with free Let's Encrypt TLS certificates.
In the end, you'll have Microsoft Visual Studio Code running on your Debian 10 server, available at your domain and protected with a password.
A server running Debian 10 with at least 2GB RAM, root access, and a sudo, non-root account.
You can set this up by following this initial server setup guide.
For a guide on how to do this, complete Steps 1 to 4 of How To Install Nginx on Debian 10.
This tutorial will use code-server. < ^ > your-domain < ^ > throughout.
In this section, you will set up code-server on your server.
This entails downloading the latest version and creating a systemd service that will keep code-server always running in the background.
You'll also specify a restart policy for the service, so that code-server stays available after possible crashes or reboots.
You'll store all data pertaining to code-server in a folder named ~ / code-server.
Create it by running the following command:
You'll need to head over to the Github releases page of code-server and pick the latest Linux build (the file will contain 'linux' in its name).
At the time of writing, the latest version was < ^ > 2.1692 < ^ >.
Download it using wget by running the following command:
Then, unpack the archive by running:
You'll get a folder named exactly as the original file you downloaded, which contains the code-server executable.
Copy the code-server executable to / usr / local / bin so you'll be able to access it system wide by running the following command:
Next, create a folder for code-server, where it will store user data:
Now that you've downloaded code-server and made it available system-wide, you will create a systemd service to keep code-server running in the background at all times.
You'll store the service configuration in a file named code-server.service, in the / lib / systemd / system directory, where systemd stores its services.
Create it using your text editor:
Here you first specify the description of the service.
Then, you state that the nginx service must be started before this one.
After the [Unit] section, you define the type of the service (simple means that the process should be simply run) and provide the command that will be executed.
You also specify that the global code-server executable should be started with a few arguments specific to code-server. --host 127.0.0.1 binds it to localhost, so it's only directly accessible from inside of your server. --user-data-dir / var / lib / code-server sets its user data directory, and --auth password specifies that it should authenticate visitors with a password, specified in the PASSWORD environment variable declared on the line above it.
Remember to replace < ^ > your _ password < ^ > with your desired password, then save and close the file.
The next line tells systemd to restart code-server in all malfunction events (for example, when it crashes or the process is killed).
The [Install] section orders systemd to start this service when it becomes possible to log in to your server.
Start the code-server service by running the following command:
Check that it's started correctly by observing its status:
To make code-server start automatically after a server reboot, enable its service by running the following command:
In this step, you've downloaded code-server and made it available globally.
Then, you've created a systemd service for it and enabled it, so code-server will start at every server boot.
Next, you'll expose it at your domain by configuring Nginx to serve as a reverse proxy between the visitor and code-server.
Step 2 - Exposing code-server at Your Domain
In this section, you will configure Nginx as a reverse proxy for code-server.
As you have learned in the Nginx prerequisite step, its site configuration files are stored under / etc / nginx / sites-available and must later be symlinked to / etc / nginx / sites-enabled to become active.
You'll store the configuration for exposing code-server at your domain in a file named code-server.conf, under / etc / nginx / sites-available.
Start off by creating it using your editor:
Replace code-server. < ^ > your-domain < ^ > with your desired domain, then save and close the file.
In this file, you define that Nginx should listen to HTTP port 80. Then, you specify a server _ name that tells Nginx for which domain to accept requests and apply this particular configuration.
In the next block, for the root location (/), you specify that requests should be passed back and forth to code-server running at localhost: 8080.
The next three lines (starting with proxy _ set _ header) order Nginx to carry over some HTTP request headers that are needed for correct functioning of WebSockets, which code-server extensively uses.
To make this site configuration active, you will need to create a symlink of it in the / etc / nginx / sites-enabled folder by running:
To test the validity of the configuration, run the following command:
For the configuration to take effect, you'll need to restart Nginx:
You now have your code-server installation accessible at your domain.
In the next step, you'll secure it by applying a free Let's Encrypt TLS certificate.
In this section, you will secure your domain using a Let's Encrypt TLS certificate, which you'll provision using Certbot.
To install the latest version of Certbot and its Nginx plugin, run the following command:
As part of the prerequisites, you have enabled ufw (Uncomplicated Firewall) and configured it to allow unencrypted HTTP traffic.
To be able to access the secured site, you'll need to configure it to accept encrypted traffic by running the following command:
Similarly to Nginx, you'll need to reload it for the configuration to take effect:
Then, in your browser, navigate to the domain you used for code-server.
You will see the code-server login prompt.
code-server is asking you for your password.
Enter the one you set in the previous step and press Enter IDE.
You'll now enter code-server and immediately see its editor GUI.
Now that you've checked that code-server is correctly exposed at your domain, you'll install Let's Encrypt TLS certificates to secure it, using Certbot.
To request certificates for your domain, run the following command:
In this command, you run certbot to request certificates for your domain - you pass the domain name with the -d parameter.
The --nginx flag tells it to automatically change Nginx site configuration to support HTTPS.
Remember to replace code-server. < ^ > your-domain < ^ > with your domain name.
If this is your first time running Certbot, you'll be asked to provide an email address for urgent notices and to accept the EFF's Terms of Service.
Certbot will then request certificates for your domain from Let's Encrypt.
It will then ask you if you'd like to redirect all HTTP traffic to HTTPS:
It is recommended to select the second option in order to maximize security.
After you input your selection, press ENTER.
This means that Certbot has successfully generated TLS certificates and applied them to the Nginx configuration for your domain.
You can now reload your code-server domain in your browser and observe a padlock to the left of the site address, which means that your connection is properly secured.
Now that you have code-server accessible at your domain through a secured Nginx reverse proxy, you're ready to review the user interface of code-server.
Step 4 - Using the code-server Interface
In this section, you'll use some of the features of the code-server interface.
Since code-server is Visual Studio Code running in the cloud, it has the same interface as the standalone desktop edition.
On the left-hand side of the IDE, there is a vertical row of six buttons opening the most commonly used features in a side panel known as the Activity Bar.
code-server GUI - Sidepanel
This bar is customizable so you can move these views to a different order or remove them from the bar.
By default, the first button opens the general menu in a dropdown, while the second view opens the Explorer panel that provides tree-like navigation of the project's structure.
You can manage your folders and files here - creating, deleting, moving, and renaming them as necessary.
The next view provides access to a search and replace functionality.
Following this, in the default order, is your view of the source control systems, like Git.
Visual Studio code also supports other source control providers and you can find further instructions for source control work flows with the editor in this documentation.
Git pane with context-menu open
The debugger option on the Activity Bar provides all the common actions for debugging in the panel.
Visual Studio Code comes with built-in support for the Node.js runtime debugger and any language that transpiles to Javascript.
For other languages you can install extensions for the required debugger.
You can save debugging configurations in the launch.json file.
Debugger View with launch.json open
The final view in the Activity Bar provides a menu to access available extensions on the Marketplace.
code-server GUI - Tabs
The central part of the GUI is your editor, which you can separate by tabs for your code editing.
You can change your editing view to a grid system or to side-by-side files.
Editor Grid View
After creating a new file through the File menu, an empty file will open in a new tab, and once saved, the file's name will be viewable in the Explorer side panel.
Creating folders can be done by right clicking on the Explorer sidebar and clicking on New Folder.
You can expand a folder by clicking on its name as well as dragging and dropping files and folders to upper parts of the hierarchy to move them to a new location.
code-server GUI - New Folder
You can gain access to a terminal by entering CTRL + SHIFT + ', or by clicking on Terminal in the upper menu dropdown, and selecting New Terminal.
The terminal will open in a lower panel and its working directory will be set to the project's workspace, which contains the files and folders shown in the Explorer side panel.
You've explored a high-level overview of the code-server interface and reviewed some of the most commonly used features.
You now have code-server, a versatile cloud IDE, installed on your Debian 10 server, exposed at your domain and secured using Let's Encrypt certificates.
You can now work on projects individually, as well as in a team-collaboration setting.
Running a cloud IDE frees resources on your local machine and allows you to scale the resources when needed.
If you would like to run code-server on your DigitalOcean Kubernetes cluster check out our tutorial on How To Set Up the code-server Cloud IDE Platform on DigitalOcean Kubernetes.
How To Use Ansible to Install and Set Up WordPress with LAMP on Ubuntu 18.04
3340
Server automation now plays an essential role in systems administration, due to the disposable nature of modern application environments.
Configuration management tools such as Ansible are typically used to streamline the process of automating server setup by establishing standard procedures for new servers while also reducing human error associated with manual setups.
Ansible offers a simple architecture that doesn't require special software to be installed on nodes.
It also provides a robust set of features and built-in modules which facilitate writing automation scripts.
This guide explains how to use Ansible to automate the steps contained in our guide on How To Install WordPress with LAMP on Ubuntu 18.04.
WordPress is the most popular CMS (content management system) on the internet, allowing users to set up flexible blogs and websites on top of a MySQL backend with PHP processing.
In order to execute the automated setup provided by the playbook we're discussing in this guide, you'll need:
One Ansible control node: an Ubuntu 18.04 machine with Ansible installed and configured to connect to your Ansible hosts using SSH keys.
Make sure the control node has a regular user with sudo permissions and a firewall enabled, as explained in our Initial Server Setup guide.
To set up Ansible, please follow our guide on How to Install and Configure Ansible on Ubuntu 18.04.
One or more Ansible Hosts: one or more remote Ubuntu 18.04 servers previously set up following the guide on How to Use Ansible to Automate Initial Server Setup on Ubuntu 18.04.
< $> note Before proceeding, you first need to make sure your Ansible control node is able to connect and execute commands on your Ansible host (s).
For a connection test, please check step 3 of How to Install and Configure Ansible on Ubuntu 18.04.
What Does this Playbook Do?
This Ansible playbook provides an alternative to manually running through the procedure outlined in our guide on How To Install WordPress with LAMP on Ubuntu 18.04.
Running this playbook will perform the following actions on your Ansible hosts:
Install aptitude, which is preferred by Ansible as an alternative to the apt package manager.
Install the required LAMP packages and PHP extensions.
Create and enable a new Apache VirtualHost for the WordPress website.
Enable the Apache rewrite (mod _ rewrite) module.
Disable the default Apache website.
Set the password for the MySQL root user.
Remove anonymous MySQL accounts and the test database.
Create a new MySQL database and user for the WordPress website.
Set up UFW to allow HTTP traffic on the configured port (80 by default).
Download and unpack WordPress.
Set up correct directory ownership and permissions.
Set up the wp-config.php file using the provided template.
Once the playbook has finished running, you will have a WordPress installation running on top of a LAMP environment, based on the options you defined within your configuration variables.
How to Use this Playbook
The first thing we need to do is obtain the WordPress on LAMP playbook and its dependencies from the do-community / ansible-playbooks repository.
We need to clone this repository to a local folder inside the Ansible Control Node.
In case you have cloned this repository before while following a different guide, access your existing ansible-playbooks copy and run a git pull command to make sure you have updated contents:
If this is your first time using the do-community / ansible-playbooks repository, you should start by cloning the repository to your home folder with:
The files we're interested in are located inside the wordpress-lamp _ ubuntu1804 folder, which has the following structure:
Here is what each of these files are:
files / apache.conf.j2: Template file for setting up the Apache VirtualHost.
files / wp-config.php.j2: Template file for setting up WordPress's configuration file.
vars / default.yml: Variable file for customizing playbook settings.
playbook.yml: The playbook file, containing the tasks to be executed on the remote server (s).
readme.md: A text file containing information about this playbook.
We'll edit the playbook's variable file to customize its options.
Access the wordpress-lamp _ ubuntu1804 directory and open the vars / default.yml file using your command line editor of choice:
This file contains a few variables that require your attention:
The following list contains a brief explanation of each of these variables and how you might want to change them:
php _ modules: An array containing PHP extensions that should be installed to support your WordPress setup.
You don't need to change this variable, but you might want to include new extensions to the list if your specific setup requires it.
mysql _ root _ password: The desired password for the root MySQL account.
mysql _ db: The name of the MySQL database that should be created for WordPress.
mysql _ user: The name of the MySQL user that should be created for WordPress.
mysql _ password: The password for the new MySQL user.
http _ host: Your domain name.
http _ conf: The name of the configuration file that will be created within Apache.
http _ port: HTTP port for this virtual host, where 80 is the default.
Once you're done updating the variables inside vars / default.yml, save and close this file.
You're now ready to run this playbook on one or more servers.
Most playbooks are configured to be executed on every server in your inventory, by default.
We can use the -l flag to make sure that only a subset of servers, or a single server, is affected by the playbook.
We can also use the -u flag to specify which user on the remote server we're using to connect and execute the playbook commands on the remote hosts.
To execute the playbook only on < ^ > server1 < ^ >, connecting as < ^ > sammy < ^ >, you can use the following command:
You will get output similar to this:
< $> note Note: For more information on how to run Ansible playbooks, check our Ansible Cheat Sheet Guide.
When the playbook is finished running, you can go to your web browser to finish WordPress's installation from there.
Navigate to your server "s domain name or public IP address:
You will see a page like this:
WordPress language selection page
After selecting the language you'd like to use for your WordPress installation, you'll be presented with a final step to set up your WordPress user and password so you can log into your control panel:
WordPress Setup
WP login prompt
WP Admin Panel
Some common next steps for customizing your WordPress installation include choosing the permalinks setting for your posts (can be found in Settings > Permalinks) and selecting a new theme (in Appearance > Themes).
The Playbook Contents
You can find the WordPress on LAMP server setup featured in this tutorial in the wordpress-lamp _ ubuntu1804 folder inside the DigitalOcean Community Playbooks repository.
To copy or download the script contents directly, click the Raw button towards the top of each script.
The full contents of the playbook as well as its associated files are also included here for your convenience.
vars / default.yml
The default.yml variable file contains values that will be used within the playbook tasks, such as the database settings and the domain name to configure within Apache.
files / apache.conf.j2
The apache.conf.j2 file is a Jinja 2 template file that configures a new Apache VirtualHost.
The variables used within this template are defined in the vars / default.yml variable file.
files / wp-config.php.j2
The wp-config.php.j2 file is another Jinja template, used to set up the main configuration file used by WordPress.
Unique authentication keys and salts are generated using a hash function.
playbook.yml
The playbook.yml file is where all tasks from this setup are defined.
It starts by defining the group of servers that should be the target of this setup (all), after which it uses become: true to define that tasks should be executed with privilege escalation (sudo) by default.
Then, it includes the vars / default.yml variable file to load configuration options.
Feel free to modify these files to best suit your individual needs within your own workflow.
In this guide, we used Ansible to automate the process of installing and setting up a WordPress website with LAMP on an Ubuntu 18.04 server.
If you'd like to include other tasks in this playbook to further customize your server setup, please refer to our introductory Ansible guide Configuration Management 101: Writing Ansible Playbooks.
How To Write Asynchronous Code in Node.js
3691
For many programs in JavaScript, code is executed as the developer writes it - line by line.
This is called synchronous execution, because the lines are executed one after the other, in the order they were written.
However, not every instruction you give to the computer needs to be attended to immediately.
For example, if you send a network request, the process executing your code will have to wait for the data to return before it can work on it. In this case, time would be wasted if it did not execute other code while waiting for the network request to be completed.
To solve this problem, developers use asynchronous programming, in which lines of code are executed in a different order than the one in which they were written.
With asynchronous programming, we can execute other code while we wait for long activities like network requests to finish.
JavaScript code is executed on a single thread within a computer process.
Its code is processed synchronously on this thread, with only one instruction run at a time.
Therefore, if we were to do a long-running task on this thread, all of the remaining code is blocked until the task is complete.
By leveraging JavaScript's asynchronous programming features, we can offload long-running tasks to a background thread to avoid this problem.
When the task is complete, the code we need to process the task's data is put back on the main single thread.
In this tutorial, you will learn how JavaScript manages asynchronous tasks with help from the Event Loop, which is a JavaScript construct that completes a new task while waiting for another.
You will then create a program that uses asynchronous programming to request a list of movies from a Studio Ghibli API and save the data to a CSV file.
The asynchronous code will be written in three ways: callbacks, promises, and with the async / await keywords.
< $> note Note: As of this writing, asynchronous programming is no longer done using only callbacks, but learning this obsolete method can provide great context as to why the JavaScript community now uses promises.
The async / await keywords enable us to use promises in a less verbose way, and are thus the standard way to do asynchronous programming in JavaScript at the time of writing this article.
Node.js installed on your development machine.
You will also need to be familiar with installing packages in your project.
Get up to speed by reading our guide on How To Use Node.js Modules with npm and package.json.
It is important that you're comfortable creating and executing functions in JavaScript before learning how to use them asynchronously.
If you need an introduction or refresher, you can read our guide on How To Define Functions in JavaScript
The Event Loop
Let's begin by studying the internal workings of JavaScript function execution.
Understanding how this behaves will allow you to write asynchronous code more deliberately, and will help you with troubleshooting code in the future.
As the JavaScript interpreter executes the code, every function that is called is added to JavaScript's call stack.
The call stack is a stack - a list-like data structure where items can only be added to the top, and removed from the top.
Stacks follow the "Last in, first out" or LIFO principle.
If you add two items on the stack, the most recently added item is removed first.
Let's illustrate with an example using the call stack.
If JavaScript encounters a function functionA () being called, it is added to the call stack.
If that function functionA () calls another function functionB (), then functionB () is added to the top of the call stack.
As JavaScript completes the execution of a function, it is removed from the call stack.
Therefore, JavaScript will execute functionB () first, remove it from the stack when complete, and then finish the execution of functionA () and remove it from the call stack.
This is why inner functions are always executed before their outer functions.
When JavaScript encounters an asynchronous operation, like writing to a file, it adds it to a table in its memory.
This table stores the operation, the condition for it to be completed, and the function to be called when it's completed.
As the operation completes, JavaScript adds the associated function to the message queue.
A queue is another list-like data structure where items can only be added to the bottom but removed from the top.
In the message queue, if two or more asynchronous operations are ready for their functions to be executed, the asynchronous operation that was completed first will have its function marked for execution first.
Functions in the message queue are waiting to be added to the call stack.
The event loop is a perpetual process that checks if the call stack is empty.
If it is, then the first item in the message queue is moved to the call stack.
JavaScript prioritizes functions in the message queue over function calls it interprets in the code.
The combined effect of the call stack, message queue, and event loop allows JavaScript code to be processed while managing asynchronous activities.
Now that you have a high-level understanding of the event loop, you know how the asynchronous code you write will be executed.
With this knowledge, you can now create asynchronous code with three different approaches: callbacks, promises, and async / await.
Asynchronous Programming with Callbacks
A callback function is one that is passed as an argument to another function, and then executed when the other function is finished.
We use callbacks to ensure that code is executed only after an asynchronous operation is completed.
For a long time, callbacks were the most common mechanism for writing asynchronous code, but now they have largely become obsolete because they can make code confusing to read.
In this step, you'll write an example of asynchronous code using callbacks so that you can use it as a baseline to see the increased efficiency of other strategies.
There are many ways to use callback functions in another function.
Generally, they take this structure:
While it is not syntactically required by JavaScript or Node.js to have the callback function as the last argument of the outer function, it is a common practice that makes callbacks easier to identify.
It's also common for JavaScript developers to use an anonymous function as a callback.
Anonymous functions are those created without a name.
It's usually much more readable when a function is defined at the end of the argument list.
To demonstrate callbacks, let's create a Node.js module that writes a list of Studio Ghibli movies to a file.
First, create a folder that will store our JavaScript file and its output:
Then enter that folder:
We will start by making an HTTP request to the Studio Ghibli API, which our callback function will log the results of.
To do this, we will install a library that allows us to access the data of an HTTP response in a callback.
In your terminal, initialize npm so we can have a reference for our packages later:
Then, install the request library:
Now open a new file called callbackMovies.js in a text editor like nano:
In your text editor, enter the following code.
Let's begin by sending an HTTP request with the request module:
In the first line, we load the request module that was installed via npm.
The module returns a function that can make HTTP requests; we then save that function in the request constant.
We then make the HTTP request using the request () function.
Let's now print the data from the HTTP request to the console by adding the highlighted changes:
When we use the request () function, we give it two parameters:
The URL of the website we are trying to request
A callback function that handles any errors or successful responses after the request is complete
Our callback function has three arguments: error, response, and body.
When the HTTP request is complete, the arguments are automatically given values depending on the outcome.
If the request failed to send, then error would contain an object, but response and body would be null.
If it made the request successfully, then the HTTP response is stored in response.
If our HTTP response returns data (in this example we get JSON) then the data is set in body.
Our callback function first checks to see if we received an error.
It's best practice to check for errors in a callback first so the execution of the callback won't continue with missing data. In this case, we log the error and the function's execution.
We then check the status code of the response.
Our server may not always be available, and APIs can change causing once sensible requests to become incorrect.
By checking that the status code is 200, which means the request was "OK", we can have confidence that our response is what we expect it to be.
Finally, we parse the response body to an Array and loop through each movie to log its name and release year.
After saving and quitting the file, run this script with:
We successfully received a list of Studio Ghibli movies with the year they were released.
Now let's complete this program by writing the movie list we are currently logging into a file.
Update the callbackMovies.js file in your text editor to include the following highlighted code, which creates a CSV file with our movie data:
Noting the highlighted changes, we see that we import the fs module.
This module is standard in all Node.js installations, and it contains a writeFile () method that can asynchronously write to a file.
Instead of logging the data to the console, we now add it to a string variable movieList.
We then use writeFile () to save the contents of movieList to a new file - callbackMovies.csv.
Finally, we provide a callback to the writeFile () function, which has one argument: error.
This allows us to handle cases where we are not able to write to a file, for example when the user we are running the node process on does not have those permissions.
Save the file and run this Node.js program once again with:
In your ghibliMovies folder, you will see callbackMovies.csv, which has the following content:
It's important to note that we write to our CSV file in the callback of the HTTP request.
Once the code is in the callback function, it will only write to the file after the HTTP request was completed.
If we wanted to communicate to a database after we wrote our CSV file, we would make another asynchronous function that would be called in the callback of writeFile ().
The more asynchronous code we have, the more callback functions have to be nested.
Let's imagine that we want to execute five asynchronous operations, each one only able to run when another is complete.
If we were to code this, we would have something like this:
When nested callbacks have many lines of code to execute, they become substantially more complex and unreadable.
As your JavaScript project grows in size and complexity, this effect will become more pronounced, until it is eventually unmanageable.
Because of this, developers no longer use callbacks to handle asynchronous operations.
To improve the syntax of our asynchronous code, we can use promises instead.
Using Promises for Concise Asynchronous Programming
A promise is a JavaScript object that will return a value at some point in the future.
Asynchronous functions can return promise objects instead of concrete values.
If we get a value in the future, we say that the promise was fulfilled.
If we get an error in the future, we say that the promise was rejected.
Otherwise, the promise is still being worked on in a pending state.
Promises generally take the following form:
As shown in this template, promises also use callback functions.
We have a callback function for the then () method, which is executed when a promise is fulfilled.
We also have a callback function for the catch () method to handle any errors that come up while the promise is being executed.
Let's get firsthand experience with promises by rewriting our Studio Ghibli program to use promises instead.
Axios is a promise-based HTTP client for JavaScript, so let's go ahead and install it:
Now, with your text editor of choice, create a new file promiseMovies.js:
Our program will make an HTTP request with axios and then use a special promised-based version of fs to save to a new CSV file.
Type this code in promiseMovies.js so we can load Axios and send an HTTP request to the movie API:
In the first line we load the axios module, storing the returned function in a constant called axios.
We then use the axios.get () method to send an HTTP request to the API.
The axios.get () method returns a promise.
Let's chain that promise so we can print the list of Ghibli movies to the console:
Let's break down what's happening.
After making an HTTP GET request with axios.get (), we use the then () function, which is only executed when the promise is fulfilled.
In this case, we print the movies to the screen like we did in the callbacks example.
To improve this program, add the highlighted code to write the HTTP data to a file:
We additionally import the fs module once again.
Note how after the fs import we have .promises.
Node.js includes a promised-based version of the callback-based fs library, so backward compatibility is not broken in legacy projects.
The first then () function that processes the HTTP request now calls fs.writeFile () instead of printing to the console.
Since we imported the promise-based version of fs, our writeFile () function returns another promise.
As such, we append another then () function for when the writeFile () promise is fulfilled.
A promise can return a new promise, allowing us to execute promises one after the other.
This paves the way for us to perform multiple asynchronous operations.
This is called promise chaining, and it is analogous to nesting callbacks.
The second then () is only called after we successfully write to the file.
< $> note Note: In this example, we did not check for the HTTP status code like we did in the callback example.
By default, axios does not fulfil its promise if it gets a status code indicating an error.
As such, we no longer need to validate it. < $>
To complete this program, chain the promise with a catch () function as it is highlighted in the following:
If any promise is not fulfilled in the chain of promises, JavaScript automatically goes to the catch () function if it was defined.
That's why we only have one catch () clause even though we have two asynchronous operations.
Let's confirm that our program produces the same output by running:
In your ghibliMovies folder, you will see the promiseMovies.csv file containing:
With promises, we can write much more concise code than using only callbacks.
The promise chain of callbacks is a cleaner option than nesting callbacks.
However, as we make more asynchronous calls, our promise chain becomes longer and harder to maintain.
The verbosity of callbacks and promises come from the need to create functions when we have the result of an asynchronous task.
A better experience would be to wait for an asynchronous result and put it in a variable outside the function.
That way, we can use the results in the variables without having to make a function.
We can achieve this with the async and await keywords.
Writing JavaScript with async / await
The async / await keywords provide an alternative syntax when working with promises.
Instead of having the result of a promise available in the then () method, the result is returned as a value like in any other function.
We define a function with the async keyword to tell JavaScript that it's an asynchronous function that returns a promise.
We use the await keyword to tell JavaScript to return the results of the promise instead of returning the promise itself when it's fulfilled.
In general, async / await usage looks like this:
Let's see how using async / await can improve our Studio Ghibli program.
Use your text editor to create and open a new file asyncAwaitMovies.js:
In your newly opened JavaScript file, let's start by importing the same modules we used in our promise example:
The imports are the same as promiseMovies.js because async / await uses promises.
Now we use the async keyword to create a function with our asynchronous code:
We create a new function called saveMovies () but we include async at the beginning of its definition.
This is important as we can only use the await keyword in an asynchronous function.
Use the await keyword to make an HTTP request that gets the list of movies from the Ghibli API:
In our saveMovies () function, we make an HTTP request with axios.get () like before.
This time, we don't chain it with a then () function.
Instead, we add await before it is called.
When JavaScript sees await, it will only execute the remaining code of the function after axios.get () finishes execution and sets the response variable.
The other code saves the movie data so we can write to a file.
Let's write the movie data to a file:
We also use the await keyword when we write to the file with fs.writeFile ().
To complete this function, we need to catch errors our promises can throw.
Let's do this by encapsulating our code in a try / catch block:
Since promises can fail, we encase our asynchronous code with a try / catch clause.
This will capture any errors that are thrown when either the HTTP request or file writing operations fail.
Finally, let's call our asynchronous function saveMovies () so it will be executed when we run the program with node
At a glance, this looks like a typical synchronous JavaScript code block.
It has fewer functions being passed around, which looks a bit neater.
These small tweaks make asynchronous code with async / await easier to maintain.
Test this iteration of our program by entering this in your terminal:
In your ghibliMovies folder, a new asyncAwaitMovies.csv file will be created with the following contents:
You have now used the JavaScript features async / await to manage asynchronous code.
In this tutorial, you learned how JavaScript handles executing functions and managing asynchronous operations with the event loop.
You then wrote programs that created a CSV file after making an HTTP request for movie data using various asynchronous programming techniques.
First, you used the obsolete callback-based approach.
You then used promises, and finally async / await to make the promise syntax more succinct.
With your understanding of asynchronous code with Node.js, you can now develop programs that benefit from asynchronous programming, like those that rely on API calls.
Have a look at this list of public APIs.
To use them, you will have to make asynchronous HTTP requests like we did in this tutorial.
For further study, try building an app that uses these APIs to practice the techniques you learned here.
Initial Server Setup with CentOS 8
3696
When you first create a new CentOS 8 server, there are a few configuration steps that you should take early on as part of the basic setup.
This will increase the security and usability of your server and will give you a solid foundation for subsequent actions.
Step 1 - Logging in as Root
To log into your server, you will need to know your server's public IP address.
You will also need the password or, if you installed an SSH key for authentication, the private key for the root user's account.
If you have not already logged into your server, you may want to follow our documentation on how to connect to your Droplet with SSH, which covers this process in detail.
If you are not already connected to your server, log in as the root user now using the following command (substitute the highlighted portion of the command with your server's public IP address):
Accept the warning about host authenticity if it appears.
If you are using password authentication, provide your root password to log in.
If you are using an SSH key that is passphrase protected, you may be prompted to enter the passphrase the first time you use the key each session.
If this is your first time logging into the server with a password, you may also be prompted to change the root password.
About Root
The root user is the administrative user in a Linux environment, and it has very broad privileges.
Because of the heightened privileges of the root account, you are discouraged from using it on a regular basis.
This is because part of the power inherent with the root account is the ability to make very destructive changes, even by accident.
As such, the next step is to set up an alternative user account with a reduced scope of influence for day-to-day work.
This account will still be able to gain increased privileges when necessary.
Step 2 - Creating a New User
Once you are logged in as root, you can create the new user account that we will use to log in from now on.
This example creates a new user called sammy, but you should replace it with any username that you prefer:
Next, set a strong password for the < ^ > sammy < ^ > user:
You will be prompted to enter the password twice.
After doing so, your user will be ready to use, but first we'll give this user additional privileges to use the sudo command.
This will allow us to run commands as root when necessary.
Step 3 - Granting Administrative Privileges
Now, we have a new user account with regular account privileges.
However, we may sometimes need to do administrative tasks.
To avoid having to log out of our normal user and log back in as the root account, we can set up what is known as "superuser" or root privileges for our normal account.
This will allow our normal user to run commands with administrative privileges by putting the word sudo before each command.
To add these privileges to our new user, we need to add the new user to the wheel group.
By default, on CentOS 8, users who belong to the wheel group are allowed to use the sudo command.
As root, run this command to add your new user to the wheel group (substitute the highlighted word with your new username):
Now, when logged in as your regular user, you can type sudo before commands to perform actions with superuser privileges.
Step 4 - Setting Up a Basic Firewall
Firewalls provide a basic level of security for your server.
These applications are responsible for denying traffic to every port on your server, except for those ports / services you have explicitly approved.
CentOS has a service called firewalld to perform this function.
A tool called firewall-cmd is used to configure firewalld firewall policies.
< $> note Note: If your servers are running on DigitalOcean, you can optionally use DigitalOcean Cloud Firewalls instead of the UFW firewall.
We recommend using only one firewall at a time to avoid conflicting rules that may be difficult to debug.
First install firewalld:
The default firewalld configuration allows ssh connections, so we can turn the firewall on immediately:
Check the status of the service to make sure it started:
Note that it is both active and enabled, meaning it will start by default if the server is rebooted.
Now that the service is up and running, we can use the firewall-cmd utility to get and set policy information for the firewall.
First let's list which services are already allowed:
To see the additional services that you can enable by name, type:
To add a service that should be allowed, use the --add-service flag:
This would add the http service and allow incoming TCP traffic to port 80. The configuration will update after you reload the firewall:
Remember that you will have to explicitly open the firewall (with services or ports) for any additional services that you may configure later.
Step 5 - Enabling External Access for Your Regular User
Now that we have a regular non-root user for daily use, we need to make sure we can use it to SSH into our server.
< $> note Note: Until verifying that you can log in and use sudo with your new user, we recommend staying logged in as root.
This way, if you have problems, you can troubleshoot and make any necessary changes as root.
If you are using a DigitalOcean Droplet and experience problems with your root SSH connection, you can log into the Droplet using the DigitalOcean Console.
The process for configuring SSH access for your new user depends on whether your server's root account uses a password or SSH keys for authentication.
If the Root Account Uses Password Authentication
If you logged in to your root account using a password, then password authentication is enabled for SSH.
You can SSH to your new user account by opening up a new terminal session and using SSH with your new username:
After entering your regular user's password, you will be logged in.
Remember, if you need to run a command with administrative privileges, type sudo before it like this:
You will be prompted for your regular user password when using sudo for the first time each session (and periodically afterwards).
To enhance your server's security, we strongly recommend setting up SSH keys instead of using password authentication.
Follow our guide on setting up SSH keys on CentOS 8 to learn how to configure key-based authentication.
If the Root Account Uses SSH Key Authentication
If you logged in to your root account using SSH keys, then password authentication is disabled for SSH.
You will need to add a copy of your public key to the new user's ~ / .ssh / authorized _ keys file to log in successfully.
Since your public key is already in the root account's ~ / .ssh / authorized _ keys file on the server, we can copy that file and directory structure to our new user account.
The simplest way to copy the files with the correct ownership and permissions is with the rsync command.
This will copy the root user's .ssh directory, preserve the permissions, and modify the file owners, all in a single command.
Make sure to change the highlighted portions of the command below to match your regular user's name:
< $> note Note: The rsync command treats sources and destinations that end with a trailing slash differently than those without a trailing slash.
When using rsync below, be sure that the source directory (~ / .ssh) does not include a trailing slash (check to make sure you are not using ~ / .ssh /).
If you accidentally add a trailing slash to the command, rsync will copy the contents of the root account's ~ / .ssh directory to the sudo user's home directory instead of copying the entire ~ / .ssh directory structure.
The files will be in the wrong location and SSH will not be able to find and use them.
Now, back in new terminal on your local machine, open up a new SSH session with your non-root user:
You should be logged in to the new user account without using a password.
At this point, you have a solid foundation for your server.
You can install any of the software you need on your server now.
How To Build a Hashicorp Vault Server Using Packer and Terraform on DigitalOcean
3871
Vault, by Hashicorp, is an open-source tool for securely storing secrets and sensitive data in dynamic cloud environments.
It provides strong data encryption, identity-based access using custom policies, and secret leasing and revocation, as well as a detailed audit log that is recorded at all times.
Vault also features a HTTP API, making it the ideal choice for storing credentials in scattered service-oriented deployments, such as Kubernetes.
Packer and Terraform, also developed by Hashicorp, can be used together to create and deploy images of Vault.
Within this workflow, developers can use Packer to write immutable images for different platforms from a single configuration file, which specifies what the image should contain.
Terraform will then deploy as many customized instances of the created images as needed.
In this tutorial, you'll use Packer to create an immutable snapshot of the system with Vault installed, and orchestrate its deployment using Terraform.
In the end, you'll have an automated system for deploying Vault in place, allowing you to focus on working with Vault itself, and not on the underlying installation and provisioning process.
Packer installed on your local machine.
For instructions, visit the official documentation.
Terraform installed on your local machine.
Visit the official documentation for a guide.
A personal access token (API key) with read and write permissions for your DigitalOcean account.
To learn how to create one, visit How to Create a Personal Access Token from the docs.
An SSH key you'll use to authenticate with the deployed Vault Droplets, available on your local machine and added to your DigitalOcean account.
You'll also need its fingerprint, which you can copy from the Security page of your account once you've added it. See the DigitalOcean documentation for detailed instructions or the How To Set Up SSH Keys tutorial.
Step 1 - Creating a Packer Template
In this step, you will write a Packer configuration file, called a template, that will instruct Packer on how to build an image that contains Vault pre-installed.
You'll be writing the configuration in JSON format, a commonly used human-readable configuration file format.
For the purposes of this tutorial, you'll store all files under ~ / vault-orchestration.
Create the directory by running the following command:
You'll store config files for Packer and Terraform separately, in different subdirectories.
Create them using the following command:
Because you'll first be working with Packer, navigate to its directory:
Using Template Variables
Storing private data and application secrets in a separate variables file is the ideal way of keeping them out of your template.
When building the image, Packer will substitute the referenced variables with their values.
Hard coding secret values into your template is a security risk, especially if it's going to be shared with team members or put up on public sites, such as GitHub.
You'll store them in the packer subdirectory, in a file called variables.json.
Create it using your favorite text editor:
The variables file consists of a JSON dictionary, which maps variable names to their values.
You'll use these variables in the template you are about to create.
If you wish, you can edit the base image, region, and Droplet size values according to the developer docs.
Remember to replace < ^ > your _ do _ api _ key < ^ > with your API key you created as part of the prerequisites, then save and close the file.
Creating Builders and Provisioners
With the variables file ready, you'll now create the Packer template itself.
You "ll store the Packer template for Vault in a file named template.json.
In the template, you define arrays of builders and provisioners.
Builders tell Packer how to build the system image (according to their type) and where to store it, while provisioners contain sets of actions Packer should perform on the system before turning it into an immutable image, such as installing or configuring software.
Without any provisioners, you would end up with an untouched base system image.
Both builders and provisioners expose parameters for further work flow customization.
You first define a single builder of the type digitalocean, which means that when ordered to build an image, Packer will use the provided parameters to create a temporary Droplet of the defined size using the provided API key, with the specified base system image and in the specified region.
The format for fetching a variable is {{user '< ^ > variable _ name < ^ >'}}, where the highlighted part is its name.
When the temporary Droplet is provisioned, the provisioner will connect to it using SSH with the specified username, and will sequentially execute all defined provisioners before creating a DigitalOcean Snapshot from the Droplet and deleting it.
It's of type shell, which will execute given commands on the target.
Commands can be specified either inline, as an array of strings, or defined in separate script files if inserting them into the template becomes unwieldy due to size.
The commands in the template will wait 30 seconds for the system to boot up, and will then download and unpack Vault < ^ > 1.3.2 < ^ >.
Check the official Vault download page and replace the link in the commands with a newer version for Linux, if available.
When you're done, save and close the file.
To verify the validity of your template, run the following command:
Packer accepts a path to the variables file via the -var-file argument.
If you get an error, Packer will specify exactly where it occurred, so you'll be able to correct it.
You now have a working template that produces an image with Vault installed, with your API key and other parameters defined in a separate file.
You're now ready to invoke Packer and build the snapshot.
Step 2 - Building the Snapshot
In this step, you'll build a DigitalOcean Snapshot from your template using the Packer build command.
To build your snapshot, run the following command:
This command will take some time to finish.
You'll see a lot of output, which will look like this:
Packer logs all the steps it took while building your template.
The last line contains the name of the snapshot (such as packer-1581537927) and its ID in parentheses, marked in red. Note your ID of the snapshot, because you'll need it in the next step.
If the build process fails due to API errors, wait a few minutes and then retry.
You've built a DigitalOcean Snapshot according to your template.
The snapshot has Vault pre-installed, and you can now deploy Droplets with it as their system image.
In the next step, you'll write Terraform configuration for automating such deployments.
Step 3 - Writing Terraform Configuration
In this step, you'll write Terraform configuration for automating Droplet deployments of the snapshot containing the Vault you just built using Packer.
Before writing actual Terraform configuration for deploying Vault from the previously built snapshot, you'll first need to configure the DigitalOcean provider for it. Navigate to the terraform subdirectory by running:
Then, create a file named do-provider.tf, where you'll store the provider:
This file declares parameter variables and provides the digitalocean provider with an API key.
You'll later use these variables in your Terraform template, but you'll first need to specify their values.
For that purpose, Terraform supports specifying variable values in a variable definitions file similarly to Packer.
The filename must end in either .tfvars or .tfvars.json.
You'll later pass that file to Terraform using the -var-file argument.
Create a variable definitions file called definitions.tfvars using your text editor:
Remember to replace < ^ > your _ do _ api _ key < ^ >, < ^ > your _ ssh _ key _ fingerprint < ^ >, and < ^ > your _ do _ snapshot _ id < ^ > with your account API key, the fingerprint of your SSH key, and the snapshot ID you noted from the previous step, respectively.
The do _ region and do _ size parameters must have the same values as in the Packer variables file.
If you want to deploy multiple instances at once, adjust instance _ count to your desired value.
When finished, save and close the file.
For more information on the DigitalOcean Terraform provider, visit the official docs.
You'll store the Vault snapshot deployment configuration in a file named deployment.tf, under the terraform directory.
Here you define a single resource of the type digitalocean _ droplet named vault.
Then, you set its parameters according to the variable values and add a SSH key (using its fingerprint) from your DigitalOcean account to the Droplet resource.
Finally, you output the IP addresses of all newly deployed instances to the console.
Before doing anything else with your deployment configuration, you'll need to initialize the directory as a Terraform project:
When initializing a directory as a project, Terraform reads the available configuration files and downloads plugins deemed necessary, as logged in the output.
You now have Terraform configuration for deploying your Vault snapshot ready.
You can now move on to validating it and deploying it on a Droplet.
Step 4 - Deploying Vault Using Terraform
In this section, you'll verify your Terraform configuration using the validate command.
Once it verifies successfully, you'll apply it and deploy a Droplet as a result.
Run the following command to test the validity of your configuration:
Next, run the plan command to see what Terraform will attempt when it comes to provision the infrastructure according to your configuration:
Terraform accepts a variable definitions file via the -var-file parameter.
The output will look similar to:
The green + on the beginning of the resource "digitalocean _ droplet" "vault" line means that Terraform will create a new Droplet called vault, using the parameters that follow.
This is correct, so you can now execute the plan by running terraform apply:
Enter yes when prompted.
After a few minutes, the Droplet will finish provisioning and you'll see output similar to this:
In the output, Terraform logs what actions it has performed (in this case, to create a Droplet) and displays its public IP address at the end.
You'll use it to connect to your new Droplet in the next step.
You have created a new Droplet from the snapshot containing Vault and are now ready to verify it.
Step 5 - Verifying Your Deployed Droplet
In this step, you'll access your new Droplet using SSH and verify that Vault was installed correctly.
If you are on Windows, you can use software such as Kitty or Putty to connect to the Droplet with an SSH key.
On Linux and macOS machines, you can use the already available ssh command to connect:
Answer yes when prompted.
Once you are logged in, run Vault by executing:
You'll see its "help" output, which looks like this:
You can quit the connection by typing exit.
You have now verified that your newly deployed Droplet was created from the snapshot you made, and that Vault is installed correctly.
You now have an automated system for deploying Hashicorp Vault on DigitalOcean Droplets using Terraform and Packer.
You can now deploy as many Vault servers as you need.
To start using Vault, you'll need to initialize it and further configure it. For instructions on how to do that, visit the official docs.
For more tutorials using Terraform, check out our Terraform content page.
How To Install Linux, Nginx, MySQL, PHP (LEMP) Stack on CentOS 8
3801
The LEMP software stack is a group of software that can be used to serve dynamic web pages and web applications written in PHP.
The name "LEMP" is an acronym that describes a Linux operating system with an Nginx (pronounced like "Engine-X ") web server.
The database layer in a LEMP stack is typically a MySQL database server, but prior to the release of CentOS 8, MySQL wasn't available from the default CentOS repositories.
Because of this, MariaDB, a community fork of MySQL, became a widely accepted alternative to MySQL as the default database system for LEMP stacks on CentOS machines.
In this guide, you'll install a LEMP stack on a CentOS 8 server.
Although MySQL is available from the default repositories in CentOS 8, this guide will walk through the process of setting up a LEMP stack with MariaDB as the database management system.
Step 1 - Installing the Nginx Web Server
In order to display web pages to our site visitors, we are going to employ Nginx, a high-performance web server.
Install the nginx package with:
When prompted, enter y to confirm that you want to install nginx.
In case you have enabled the firewalld firewall as per our initial server setup guide, you will need to allow connections to Nginx.
Type the address that you receive in your web browser and it will take you to Nginx "s default landing page:
Default Nginx Page CentOS 8
At this point, your database system is set up and you can move on to installing PHP, the final component of the LEMP stack.
Step 3 - Installing PHP-FPM
You have Nginx installed to serve your content and MariaDB installed to store and manage your data. Now you can install PHP to process code and generate dynamic content for the web server.
While Apache embeds the PHP interpreter in each request, Nginx requires an external program to handle PHP processing and act as bridge between the PHP interpreter itself and the web server.
This allows for a better overall performance in most PHP-based websites, but it requires additional configuration.
You "ll need to install php-fpm, which stands for" PHP fastCGI process manager ", and tell Nginx to pass PHP requests to this software for processing.
Additionally, you "ll need php-mysqlnd, a PHP module that allows PHP to communicate with MySQL-based databases.
To install the php-fpm and php-mysql packages, run:
When the installation is finished, you'll need to edit the / etc / php-fpm.d / www.conf file in order to adjust a couple settings.
The default text editor that comes with CentOS 8 is vi. vi is an extremely powerful text editor, but it can be somewhat obtuse for users who lack experience with it. You might want to install a more user-friendly editor such as nano to facilitate editing configuration files on your CentOS 8 server:
Now open the / etc / php-fpm.d / www.conf configuration file using nano or your editor of choice:
Now look for the user and group directives.
If you are using nano, you can hit CTRL + W to search for these terms inside the open file.
You'll notice that both the user and group variables are set to apache.
We need to change these to nginx:
If you are using nano, do so by pressing CTRL + X, then Y and ENTER.
To enable and start the php-fpm service, run:
Finally, restart the Nginx web server so that it loads the configuration files created by the php-fpm installation:
Step 4 - Testing PHP with Nginx
On CentOS 8, the default php-fpm installation automatically creates configuration files that will allow your Nginx web server to handle .php files in the default document root located at / usr / share / nginx / html.
You won't need to make any changes to Nginx's configuration in order for PHP to work correctly within your web server.
The only adjustment we'll make is to change the default permission settings on your Nginx document root folder.
The following command will change the ownership of the default Nginx document root to a user and group called < ^ > sammy < ^ >, so be sure to replace the highlighted username and group in this command to reflect your system's username and group.
Create a new PHP file called info.php at the / usr / share / nginx / html directory:
CentOS 8 default PHP info
In this guide, you've built a flexible foundation for serving PHP websites and applications to your visitors, using Nginx as web server.
You've set up Nginx to handle PHP requests through php-fpm, and you also set up a MariaDB database to store your website's data.
How To Install and Use ClickHouse on Debian 10
3268
ClickHouse is an open-source, column-oriented analytics database created by Yandex for OLAP and big data use cases.
ClickHouse's support for real-time query processing makes it suitable for applications that require sub-second analytical results.
ClickHouse's query language is a dialect of SQL that enables powerful declarative querying capabilities while offering familiarity and a smaller learning curve for the end user.
Column-oriented databases store records in blocks grouped by columns instead of rows.
By not loading data for columns absent in the query, column-oriented databases spend less time reading data while completing queries.
As a result, these databases can compute and return results much faster than traditional row-based systems for certain workloads, such as OLAP.
Online Analytics Processing (OLAP) systems allow for organizing large amounts of data and performing complex queries.
They are capable of managing petabytes of data and returning query results quickly.
In this way, OLAP is useful for work in areas like data science and business analytics.
In this tutorial, you'll install the ClickHouse database server and client on your machine.
You'll use the DBMS for typical tasks and optionally enable remote access from another server so that you'll be able to connect to the database from another machine.
Then you'll test ClickHouse by modeling and querying example website-visit data.
One Debian 10 with a sudo enabled non-root user and firewall setup.
You can follow the initial server setup tutorial to create the user and set up the firewall.
(Optional) A secondary Debian 10 with a sudo enabled non-root user and firewall setup.
You can follow the initial server setup tutorial.
Step 1 - Installing ClickHouse
In this section, you will install the ClickHouse server and client programs using apt.
First, SSH into your server by running:
dirmngr is a server for managing certificates and keys.
It is required for adding and verifying remote repository keys, install it by running:
Yandex maintains an APT repository that has the latest version of ClickHouse.
Add the repository's GPG key so that you'll be able to securely download validated ClickHouse packages:
The output confirms it has successfully verified and added the key.
Add the repository to your APT repositories list by executing:
Here you've piped the output of echo to sudo tee so that this output can print to a root-owned file.
Now, run apt update to update your packages:
The clickhouse-server and clickhouse-client packages will now be available for installation.
As of ClickHouse version 19.13.3, certain OpenSSL 1.1.1 configurations such as MinProtocol and CipherVersion are not read correctly.
In order to workaround this incompatibility, modify the OpenSSL config file and comment out the ssl _ conf = ssl _ sect line in / etc / ssl / openssl.cnf.
Edit the configuration file by executing:
Then comment out the line containing ssl _ conf = ssl _ sect, so it looks like the following file:
Now that the OpenSSL config has been patched, you're ready to install the ClickHouse server and client packages.
Install them with:
During the installation, you will be asked to set a password for the default ClickHouse user.
You've installed the ClickHouse server and client successfully.
You're now ready to start the database service and ensure that it's running correctly.
Step 2 - Starting the Service
The clickhouse-server package that you installed in the previous section creates a systemd service, which performs actions such as starting, stopping, and restarting the database server. systemd is an init system for Linux to initialize and manage services.
In this section you'll start the service and verify that it is running successfully.
Start the clickhouse-server service by running:
The previous command will not display any output.
To verify that the service is running successfully, execute:
The output notes that the server is running.
You have successfully started the ClickHouse server and will now be able to use the clickhouse-client CLI program to connect to the server.
Step 3 - Creating Databases and Tables
In ClickHouse, you can create and delete databases by executing SQL statements directly in the interactive database prompt.
Statements consist of commands following a particular syntax that tell the database server to perform a requested operation along with any data required.
You create databases by using the CREATE DATABASE < ^ > table _ name < ^ > syntax.
To create a database, first start a client session by running the following command:
You will be asked to enter the password you had set during the installation - enter it to successfully start the client session.
The previous command will log you in to the client prompt where you can run ClickHouse SQL statements to perform actions such as:
Creating, updating, and deleting databases, tables, indexes, partitions, and views.
Executing queries to retrieve data that is optionally filtered and grouped using various conditions.
In this step, with the ClickHouse client ready for inserting data, you're going to create a database and table.
For the purposes of this tutorial, you'll create a database named < ^ > test < ^ >, and inside that you'll create a table named < ^ > visits < ^ > that tracks website-visit durations.
Now that you're inside the ClickHouse command prompt, create your < ^ > test < ^ > database by executing:
You'll see the following output that shows that you have created the database:
A ClickHouse table is similar to tables in other relational databases; it holds a collection of related data in a structured format.
You can specify columns along with their types, add rows of data, and execute different kinds of queries on tables.
The syntax for creating tables in ClickHouse follows this example structure:
The table _ name and column _ name values can be any valid ASCII identifiers.
ClickHouse supports a wide range of column types; some of the most popular are:
UInt64: used for storing integer values in the range 0 to 18446744073709551615.
Float64: used for storing floating point numbers such as 2039.23, 10.5, etc.
String: used for storing variable length characters.
It does not require a max-length attribute since it can store arbitrary lengths.
Date: used for storing dates that follow the YYYY-MM-DD format.
DateTime: used for storing dates coupled with time and follows the YYYY-MM-DD HH: MM: SS format.
After the column definitions, you specify the engine used for the table.
In ClickHouse, Engines determine the physical structure of the underlying data, the table's querying capabilities, its concurrent access modes, and support for indexes.
Different engine types are suitable for different application requirements.
The most commonly used and widely applicable engine type is MergeTree.
Now that you have an overview of table creation, you'll create a table.
Start by confirming the database you'll be modifying:
You will see the following output showing that you have switched to the < ^ > test < ^ > database from the default database:
The remainder of this guide will assume that you are executing statements within this database's context.
Create your < ^ > visits < ^ > table by running this command:
Here's a breakdown of what the command does.
You create a table named < ^ > visits < ^ > that has four columns:
id: The primary key column.
Similarly to other RDBMS systems, a primary key column in ClickHouse uniquely identifies a row; each row should have a unique value for this column.
duration: A float column used to store the duration of each visit in seconds. float columns can store decimal values such as 12.50.
url: A string column that stores the URL visited, such as http: / / example.com.
created: A date and time column that tracks when the visit occurred.
After the column definitions, you specify MergeTree as the storage engine for the table.
The MergeTree family of engines is recommended for production databases due to its optimized support for large real-time inserts, overall robustness, and query support.
Additionally, MergeTree engines support sorting of rows by primary key, partitioning of rows, and replicating and sampling data.
If you intend to use ClickHouse for archiving data that is not queried often or for storing temporary data, you can use the Log family of engines to optimize for that use-case.
After the column definitions, you'll define other table-level options.
The PRIMARY KEY clause sets id as the primary key column and the ORDER BY clause will store values sorted by the id column.
A primary key uniquely identifies a row and is used for efficiently accessing a single row and efficient colocation of rows.
On executing the create statement, you will see the following output:
In this section, you've created a database and a table to track website-visit data. In the next step, you'll insert data into the table, update existing data, and delete that data.
Step 4 - Inserting, Updating, and Deleting Data and Columns
In this step, you'll use your < ^ > visits < ^ > table to insert, update, and delete data. The following command is an example of the syntax for inserting rows into a ClickHouse table:
Now, insert a few rows of example website-visit data into your < ^ > visits < ^ > table by running each of the following statements:
You'll see the following output repeated for each insert statement.
The output for each row shows that you've inserted it successfully into the < ^ > visits < ^ > table.
Now you'll add an additional column to the < ^ > visits < ^ > table.
When adding or deleting columns from existing tables, ClickHouse supports the ALTER syntax.
For example, the basic syntax for adding a column to a table is as follows:
Add a column named < ^ > location < ^ > that will store the location of the visits to a website by running the following statement:
The output shows that you have added the < ^ > location < ^ > column successfully.
As of version 19.13.3, ClickHouse doesn't support updating and deleting individual rows of data due to implementation constraints.
ClickHouse has support for bulk updates and deletes, however, and has a distinct SQL syntax for these operations to highlight their non-standard usage.
The following syntax is an example for bulk updating rows:
You'll run the following statement to update the url column of all rows that have a duration of less than 15. Enter it into the database prompt to execute:
The output of the bulk update statement will be as follows:
The output shows that your update query completed successfully.
The 0 rows in set in the output denotes that the query did not return any rows; this will be the case for any update and delete queries.
The example syntax for bulk deleting rows is similar to updating rows and has the following structure:
To test deleting data, run the following statement to remove all rows that have a duration of less than 5:
The output of the bulk delete statement will be similar to:
The output confirms that you have deleted the rows with a duration of less than five seconds.
To delete columns from your table, the syntax would follow this example structure:
Delete the location column you added previously by running the following:
The DROP COLUMN output confirming that you have deleted the column will be as follows:
Now that you've successfully inserted, updated, and deleted rows and columns in your < ^ > visits < ^ > table, you'll move on to query data in the next step.
ClickHouse's query language is a custom dialect of SQL with extensions and functions suited for analytics workloads.
In this step, you'll run selection and aggregation queries to retrieve data and results from your < ^ > visits < ^ > table.
Selection queries allow you to retrieve rows and columns of data filtered by conditions that you specify, along with options such as the number of rows to return.
You can select rows and columns of data using the SELECT syntax.
The basic syntax for SELECT queries is:
Execute the following statement to retrieve url and duration values for rows where the url is http: / / example.com.
The output has returned two rows that match the conditions you specified.
Now that you've selected values, you can move on to executing aggregation queries.
Aggregation queries are queries that operate on a set of values and return single output values.
In analytics databases, these queries are run frequently and are well optimized by the database.
Some aggregate functions supported by ClickHouse are:
count: returns the count of rows matching the conditions specified.
sum: returns the sum of selected column values.
avg: returns the average of selected column values.
Some ClickHouse-specific aggregate functions include:
uniq: returns an approximate number of distinct rows matched.
topK: returns an array of the most frequent values of a specific column using an approximation algorithm.
To demonstrate the execution of aggregation queries, you'll calculate the total duration of visits by running the sum query:
Now, calculate the top two URLs by executing:
Now that you have successfully queried your < ^ > visits < ^ > table, you'll delete tables and databases in the next step.
Step 6 - Deleting Tables and Databases
In this section, you'll delete your < ^ > visits < ^ > table and < ^ > test < ^ > database.
The syntax for deleting tables follows this example:
To delete the < ^ > visits < ^ > table, run the following statement:
You will see the following output declaring that you've deleted the table successfully:
You can delete databases using the DROP database < ^ > table _ name < ^ > syntax.
To delete the < ^ > test < ^ > database, execute the following statement:
The resulting output shows that you've deleted the database successfully.
You've deleted tables and databases in this step.
Now that you've created, updated, and deleted databases, tables, and data in your ClickHouse instance, you'll enable remote access to your database server in the next section.
Step 7 - Setting Up Firewall Rules (Optional)
If you intend to only use ClickHouse locally with applications running on the same server, or do not have a firewall enabled on your server, you don't need to complete this section.
If instead, you'll be connecting to the ClickHouse database server remotely, you should follow this step.
Currently your server has a firewall enabled that disables your public IP address accessing all ports.
You'll complete the following two steps to allow remote access:
Modify ClickHouse's configuration and allow it to listen on all interfaces.
Add a firewall rule allowing incoming connections to port 8123, which is the HTTP port that the ClickHouse server runs.
If you are inside the database prompt, exit it by typing CTRL + D.
Then uncomment the line containing <! -- < listen _ host > 0.0.0.0 < / listen _ host > -- >, like the following file:
Save the file and exit.
For the new configuration to apply restart the service by running:
You won't see any output from this command.
ClickHouse's server listens on port 8123 for HTTP connections and port 9000 for connections from clickhouse-client.
Allow access to both ports for your second server's IP address with the following command:
You will see the following output for both commands that shows that you've enabled access to both ports:
ClickHouse will now be accessible from the IP that you added.
Feel free to add additional IPs such as your local machine's address if required.
To verify that you can connect to the ClickHouse server from the remote machine, first follow the steps in Step 1 of this tutorial on the second server and ensure that you have the clickhouse-client installed on it.
Now that you have logged in to the second server, start a client session by executing:
You will see the following output that shows that you have connected successfully to the server:
In this step, you've enabled remote access to your ClickHouse database server by adjusting your firewall rules.
You have successfully set up a ClickHouse database instance on your server and created a database and table, added data, performed queries, and deleted the database.
Within ClickHouse's documentation you can read about their benchmarks against other open-source and commercial analytics databases and general reference documents.
Further features ClickHouse offers include distributed query processing across multiple servers to improve performance and protect against data loss by storing data over different shards.
How To Migrate Redis Data with Replication on Ubuntu 18.04
3213
Redis is an in-memory, key-value data store known for its flexibility, performance, wide language support, and built-in features like replication.
Replication is the practice of regularly copying data from one database to another in order to have a replica that always remains an exact duplicate of the primary instance.
One common use of Redis replication is to migrate an existing Redis data store to a new server, as one might do when scaling up their infrastructure for better performance.
This tutorial outlines the process of using Redis's built-in replication features to migrate data from one Ubuntu 18.04 server (the "source ") to another (the" target ").
This involves making a few configuration changes to each server, setting the target server to function as a replica of the source, and then promoting the replica back to a primary after the migration is completed.
Two servers running Ubuntu 18.04.
Each server should have a user configured with administrative privileges and a firewall set up with ufw.
To set up this environment, follow our initial server setup guide for Ubuntu 18.04 for both servers.
The latest version of Redis installed on each server.
To set this up, follow our guide on How To Install Redis from Source on Ubuntu 18.04.
Step 1 - (Optional) Loading Your Source Redis Instance with Sample Data
This optional step involves loading your source Redis instance with some sample data so you can experiment with migrating data to your target instance.
If you already have data that you want to migrate over to your target, you can move ahead to Step 2 which will go over how to back it up.
To begin, connect to the Ubuntu server you'll use as your source Redis instance as your non-root user:
Then run the following command to access your Redis server:
If you've configured your Redis server to require password authentication, run the auth command followed by your Redis password:
Next, run the following commands.
These will create a number of keys holding a few strings, a hash, a list, and a set:
Additionally, run the following expire commands to provide a few of these keys with a timeout.
This will make them volatile, meaning that Redis will delete them after a specified amount of time (7500 seconds, in this case):
With that, you have some example data you can export to your target Redis instance.
Keep the redis-cli prompt open for now, since we will run a few more commands from it in the next step to back this data up.
Step 2 - Backing Up Your Source Redis Instance
Any time you plan to move data from one server to another, there's a risk that something could go wrong and you could lose data as a result.
Even though this risk is small, we will use Redis's bgsave command to create a backup of your source Redis database in case you encounter an error during the replication process.
If you don't already have it open, start by opening up the Redis command line interface:
Also, if you've configured your Redis server to require password authentication, run the auth command followed by your Redis password:
Next, run the bgsave command.
This will create a snapshot of your current data set and export it to a dump file held in Redis's working directory:
< $> note Note: You can take a snapshot of your Redis database with either the save or bgsave commands.
The reason we use the bgsave command here, though, is that the save command runs synchronously, meaning it will block any other clients connected to the database.
Because of this, the save command documentation recommends that you should almost never run it in a production environment.
Instead, it suggests using the bgsave command which runs asynchronously.
This will cause Redis to fork the database into two processes: the parent process will continue to serve clients while the child saves the database before exiting:
Note that if clients add or modify data while the bgsave operation is running, these changes won't be captured in the snapshot.
Following that, you can close the connection to your Redis instance by running the exit command:
If you need it in the future, you can find the data dump file in your Redis instance's working directory.
Recall how in the prerequisite Redis installation tutorial you set your Redis instance to use / var / lib / redis as its working directory.
List the contents of your Redis working directory to confirm that it's holding the data dump file:
If the dump file was exported correctly, you will see it in this command's output.
By default, this file is named dump.rdb:
After confirming that your data was backed up correctly, you're all set to configure your source Redis server to accept external connections and allow for replication.
Step 3 - Configuring Your Source Redis Instance
By default, Redis isn't configured to listen for external connections, meaning that any replicas you configure won't be able to sync with your source instance unless you update its configuration.
Here, we will update the source instance's configuration file to allow for external connections and also set a password which the target instance will use to authenticate once replication begins.
After that, we'll add a firewall rule to allow connections to the port on which Redis is running.
Open up your source Redis instance's configuration file with your preferred text editor.
Navigate to the line that begins with the bind directive.
It will look like this by default:
This directive binds Redis to 127.0.0.1, an IPv4 loopback address that represents localhost.
This means that this Redis instance is configured to only listen for connections that originate from the same server as the one where it's installed.
To allow your source instance to accept any connection made to its public IP address, such as those made from your target instance, add your source Redis server's IP address after the 127.0.0.1.
Note that you shouldn't include any commas after 127.0.0.1:
Next, if you haven't already done so, use the requirepass directive to configure a password which users must enter before they can interact with the data on the source instance.
Do so by uncommenting the directive and setting it to a complex password or passphrase:
Be sure to take note of the password you set here, as you will need it when you configure the target server.
Following that change, you can save and close the Redis configuration file.
If you edited it with nano, do so by pressing CTRL + X, Y, then ENTER.
Then, restart the Redis service to put these changes into effect:
That's all you need to do in terms of configuring Redis, but if you configured a firewall on your server it will continue to block any attempts by your target server to connect with the source.
Assuming you configured your firewall with ufw, you could update it to allow connections to the port on which Redis is running with the following command.
Note that Redis is configured to use port 6379 by default:
After making that final change you're all done configuring your source Redis server.
Continue on to configure your target Redis instance to function as a replica of the source.
Step 4 - Configuring your Target Redis Instance
By this point you've configured your source Redis instance to accept external connections.
However, because you've locked down access to the source by uncommenting the requirepass directive, your target instance won't be able to replicate the data held on the source.
Here, you will configure your target Redis instance to be able to authenticate its connection to the source, thereby allowing replication.
Begin by connecting to your target Redis server as your non-root user:
Next, open up your target server's Redis configuration file:
If you haven't done so already, you should configure a password for your target Redis instance with the requirepass directive:
Next, uncomment the masterauth directive and set it to your source Redis instance's authentication password.
By doing this, your target server will be able to authenticate to the source instance after you enable replication:
Lastly, if you have clients writing information to your source instance, you will want to configure them to write data to your target instance as well.
This way, if a client writes any data after you promote the target back to being a primary instance, it won't get lost.
To do this, though, you will need to adjust the replica-read-only directive.
This is set to yes by default, which means that it's configured to become a "read-only" replica which clients won't be able to write to.
Set this directive to no to allow clients to write to it:
Those are all the changes you need to make to the target's configuration file, so you can save and close it.
After restarting the Redis service your target server will be ready to become a replica of the source.
All you'll need to do to turn it into one is to run a single command, which we'll do shortly.
< $> note Note: If you have any clients writing data to your source Redis instance, now would be a good time to configure them to also write data to your target.
Step 5 - Starting and Verifying Replication
By this point, you have configured your source Redis instance to accept connections from your target server and you've configured your target Redis instance to be able to authenticate to the source as a replica.
With these pieces in place, you're ready to turn your target instance into a replica of the source.
Begin by opening up the Redis command line interface on your target Redis server:
Run the auth command to authenticate the connection:
Next, turn the target instance into a replica of the source with the replicaof command.
Be sure to replace < ^ > source _ server _ ip < ^ > with your source instance's public IP address and < ^ > source _ port < ^ > with the port used by Redis on your source instance:
From the prompt, run the following scan command.
This will return all the keys currently held by the replica:
If replication is working as expected, you will see all the keys from your source instance held in the replica.
If you loaded your source with the sample data in Step 1, the scan command's output will look like this:
< $> note Note: Be aware that this command may return the keys in a different order than what's shown in this example.
However, if this command doesn't return the same keys held on your source Redis instance, it may be that there is an error in one of your servers' configuration files preventing the target database from connecting to the source.
In this case, close the connection to your target Redis instance, and double check that you've edited the configuration files on both your source and target Redis servers correctly.
While you have the connection open, you can also confirm that the keys you set to expire are still volatile.
Do so by running the ttl command with one of these keys as an argument:
This will return the number of seconds before this key will be deleted:
Once you've confirmed that the data on your source instance was correctly synced to your target, you can promote the target back to being a primary instance by running the replicaof command once again.
This time, however, instead of following replicaof with an IP address and port, follow it with no one.
This will cause the target instance to stop syncing with the source immediately:
To confirm that the data replicated from the source persist on the target, rerun the scan command you entered previously:
You should see the same keys in this command's output as when you ran the scan command when the target was still replicating the source:
With that, you've successfully migrated all the data from your source Redis instance to your target.
If you have any clients that are still writing data to the source instance, now would be a good time to configure them to only write to the target.
There are several methods besides replication you can use to migrate data from one Redis instance to another, but replication has the advantages of requiring relatively few configuration changes to work and only a single command to initiate or stop.
If you'd like to learn more about working with Redis, we encourage you to check out our tutorial series on How To Manage a Redis Database.
Also, if you want to move your Redis data to a Redis instance managed by DigitalOcean, follow our guide on how to do so.
How To Set Up the Eclipse Theia Cloud IDE Platform on DigitalOcean Kubernetes
3323
Cloud IDEs are accessible from every type of modern device through web browsers, and they offer numerous advantages for real-time collaboration scenarios.
Working in a cloud IDE provides a unified development and testing environment for you and your team, while minimizing platform incompatibilities.
Because they are natively based on cloud technologies, they are able to make use of the cluster to achieve tasks, which can greatly exceed the power and reliability of a single development computer.
Eclipse Theia is an extensible cloud IDE running on a remote server and accessible from a web browser.
Visually, it's designed to look and behave similarly to Microsoft Visual Studio Code, which means that it supports many programming languages, has a flexible layout, and has an integrated terminal.
What separates Eclipse Theia from other cloud IDE software is its extensibility; it can be modified using custom extensions, which allow you to craft a cloud IDE suited to your needs.
In this tutorial, you will set up the default version of the Eclipse Theia cloud IDE platform on your DigitalOcean Kubernetes cluster and expose it at your domain, secured with Let "s Encrypt certificates and requiring the visitor to authenticate.
In the end, you "ll have Eclipse Theia running on your Kubernetes cluster available via HTTPS and requiring the visitor to log in.
Instructions on how to configure kubectl are shown in the Connect to your Cluster step when you create your cluster.
To create a Kubernetes cluster on DigitalOcean, see Kubernetes Quickstart.
The Nginx Ingress Controller and Cert Manager installed on your cluster using Helm in order to expose Eclipse Theia using Ingress Resources.
To do this, follow How to Set Up an Nginx Ingress on DigitalOcean Kubernetes Using Helm.
A fully registered domain name to host Eclipse Theia.
This tutorial will use < ^ > theia.your _ domain < ^ > throughout.
Step 1 - Installing and Exposing Eclipse Theia
To begin you'll install Eclipse Theia to your DigitalOcean Kubernetes cluster.
Then, you will expose it at your desired domain using an Nginx Ingress.
Since you created two example deployments and a resource as part of the prerequisites, you can freely delete them by running the following commands:
For this tutorial, you'll store the deployment configuration on your local machine, in a file named eclipse-theia.yaml.
Create it using the following command:
Add the following lines to the file:
This configuration defines a Namespace, a Deployment, a Service, and an Ingress.
The Namespace is called theia and will contain all Kubernetes objects related to Eclipse Theia, separated from the rest of the cluster.
The Deployment consists of one instance of the theiaide / theia: next Docker image with the port 3000 exposed on the container.
The Service looks for the Deployment and remaps the container port to the usual HTTP port, 80, allowing in-cluster access to Eclipse Theia.
The Ingress contains a rule to serve the Service at port 80 externally at your desired domain.
In its annotations, you specify that the Nginx Ingress Controller should be used for request processing.
Remember to replace < ^ > theia.your _ domain < ^ > with your desired domain that you've pointed to your cluster's Load Balancer, then save and close the file.
Then, create the configuration in Kubernetes by running the following command:
You can watch the Eclipse Theia pod creation by running:
After some time, the status will turn to RUNNING, which means you've successfully installed Eclipse Theia to your cluster.
Navigate to your domain in your browser.
You'll see the default Eclipse Theia editor GUI.
The default Eclipse Theia editor GUI
You've deployed Eclipse Theia to your DigitalOcean Kubernetes cluster and exposed it at your desired domain with an Ingress.
Next, you'll secure access to your Eclipse Theia deployment by enabling login authentication.
Step 2 - Enabling Login Authentication For Your Domain
In this step, you'll enable username and password authentication for your Eclipse Theia deployment.
You'll achieve this by first curating a list of valid login combinations using the htpasswd utility.
Then, you'll create a Kubernetes secret containing that list and configure the Ingress to authenticate visitors according to it. In the end, your domain will only be accessible when the visitor inputs a valid username and password combination.
This will prevent guests and other unwanted visitors from accessing Eclipse Theia.
The htpasswd utility comes from the Apache web server and is used for creating files that store lists of login combinations.
The format of htpasswd files is one username: hashed _ password combination per line, which is the format the Nginx Ingress Controller expects the list to conform to.
Start by installing htpasswd on your system by running the following command:
You'll store the list in a file called auth.
Create it by running:
This file needs to be named auth because the Nginx Ingress Controller expects the secret to contain a key called data.auth.
If it's missing, the controller will return HTTP 503 Service Unavailable status.
Add a username and password combination to auth by running the following command:
Remember to replace < ^ > username < ^ > with your desired username.
You'll be asked for an accompanying password and the combination will be added into the auth file.
You can repeat this command for as many users as you wish to add.
< $> note Note: If the system you are working on does not have htpasswd installed, you can use a Dockerized version instead.
You'll need to have Docker installed on your machine.
For instructions on how to do so, visit the official docs.
Run the following command to run a dockerized version:
Remember to replace < ^ > < username > < ^ > with the username you want to use.
You'll be asked for a password.
The hashed login combination will be written out on the console, and you'll need to manually add it to the end of the auth file.
Repeat this process for as many logins as you wish to add.
When you are done, create a new secret in Kubernetes with the contents of the file by running the following command:
You can see the secret with:
Next, you'll need to edit the Ingress to make it use the secret.
Open the deployment configuration for editing:
Add the highlighted lines to your file:
First, in the auth-type annotation, you specify that the authentication type is basic.
This means that Nginx will require the user to type in a username and password.
Then, in auth-secret, you specify that the secret that contains the list of valid combinations is theia-basic-auth, which you've just created.
The remaining auth-realm annotation specifies a message that will be shown to the user as an explanation of why authentication is required.
You can change the message contained in this field to your liking.
To propagate the changes to your cluster, run the following command:
You'll see the output:
Navigate to your domain in your browser, where you'll now be asked to log in.
You've enabled basic login authentication on your Ingress by configuring it to use the secret containing the hashed username and password combinations.
In the next step, you'll secure access further by adding TLS certificates, so that the traffic between you and your Eclipse Theia deployment stays encrypted.
Step 3 - Applying Let's Encrypt HTTPS Certificates
Next you will secure your Eclipse Theia installation by applying Let "s Encrypt certificates to your Ingress, which Cert-Manager will automatically provision.
After completing this step, your Eclipse Theia installation will be accessible via HTTPS.
Open eclipse-theia.yaml for editing:
Add the highlighted lines to your file, making sure to replace the placeholder domain with your own:
First, you specify the letsencrypt-prod ClusterIssuer you created as part of the prerequisites as the issuer that will be used to provision certificates for this Ingress.
Then, in the tls section, you specify the exact domain that should be secured, as well as a name for a secret that will be holding those certificates.
Apply the changes to your cluster by running the following command:
It will take a few minutes for the certificates to be provisioned and fully applied.
You can track the progress by observing the output of the following command:
When it finishes, the end of the output will look similar to this:
Refresh your domain in your browser.
You'll see a green padlock shown on the leftmost side of the address bar signifying that the connection is secure.
You've configured the Ingress to use Let's Encrypt certificates thus making your Eclipse Theia deployment more secure.
Now you can review the default Eclipse Theia user interface.
Step 4 - Using the Eclipse Theia Interface
In this section, you "ll explore some of the features of the Eclipse Theia interface.
On the left-hand side of the IDE, there is a vertical row of four buttons opening the most commonly used features in a side panel.
Eclipse Theia GUI - Sidepanel
By default, the first view opens the Explorer panel that provides tree-like navigation of the project's structure.
After creating a new file through the File menu, you'll see an empty file open in a new tab. Once saved, you can view the file's name in Explorer side panel.
To create folders right click on the Explorer sidebar and click on New Folder.
Eclipse Theia GUI - New Folder
The next two options provide access to search and replace functionality.
Following it, the next one provides a view of source control systems that you may be using, such as Git.
The final view is the debugger option, which provides all the common actions for debugging in the panel.
Like all modern IDEs, Eclipse Theia supports syntax highlighting for your code.
You can gain access to a terminal by typing CTRL + SHIFT + ', or by clicking on Terminal in the upper menu, and selecting New Terminal.
Terminal open
You "ve explored a high-level overview of the Eclipse Theia interface and reviewed some of the most commonly used features.
You now have Eclipse Theia, a versatile cloud IDE, installed on your DigitalOcean Kubernetes cluster.
You've secured it with a free Let's Encrypt TLS certificate and set up the instance to require a login from the visitor.
You can work on your source code and documents with it individually or collaborate with your team.
You can also try building your own version of Eclipse Theia if you need additional functionality.
For further information on how to do that, visit the Theia docs.
How To Use Visual Studio Code for Remote Development via the Remote-SSH Plugin
3927
Visual Studio Code is a popular Integrated Developer Environment (IDE) for developers.
Its large selection of plugins, minimal design, and cross-platform support make it a great choice for developers of all levels.
This tutorial focuses on using the Remote-SSH plugin to enable remote software development.
With this plugin you can edit files on your local workstation, but run development tasks such as program execution, unit tests, or static analysis on a remote server.
There are many reasons why this may be beneficial to you.
For example, you may have a Windows workstation and want to develop on Windows, but your code will eventually run on Linux.
You may need more RAM or processing power than your current machine has available, or you want to keep code off of your personal machine due to a company policy, or the desire to keep your workstation prestine.
In this tutorial, you'll enable the Remote-SSH plugin, configure Visual Studio Code to execute code on the remote server, and execute code from your local Visual Studio Code installation on the remote server.
In order to follow along with this guide, you'll need:
A local development machine running Windows, MacOSX, or Linux.
This tutorial will not work on ChromeOS devices.
Visual Studio Code, which you can download and install from the official web site.
An SSH key pair generated:
If you're using macOS or Linux, you can follow Step 1 from How to Set Up SSH Keys on Ubuntu 18.04.
The commands are the same, so don't worry that the tutorial says it is for Ubuntu 18.04.
If you're using Windows, follow the tutorial How to Create SSH Keys with PuTTY on Windows to create your SSH Key.
If you're using DigitalOcean, you can follow the How to Upload SSH Public Keys to a DigitalOcean Account guide.
One Ubuntu 18.04 server set up by following the Ubuntu 18.04 initial server setup guide, including a non-root sudo-enabled user and a firewall.
Step 1 - Installing the Remote-SSH Plugin
The Extensions Marketplace is where you can download supported and third-party extensions for a variety of different tools and programming languages.
This is where you will search for the Remote-SSH plugin and install it.
On the left-hand side of the IDE there is a vertical row of five icons.
The bottom icon, which looks like four squares in a box with the top right square exploding out, is the icon for the Extensions Marketplace:
Extensions Marketplace Icon Location
You can also access this section by pressing Ctrl + Shift + X.
When you open this page you will see suggested plugins to download and install.
Once you have the Extensions Marketplace open, type Remote-SSH in the Search Extensions in Marketplace search bar.
When you find the plugin, select it and then click the green Install button to install the extension.
Search for the Remote SSH Plugin
The extension is now installed.
Next, you'll configure the extension so you can connect to your server.
Step 2 - Configuring the Remote-SSH Plugin and Connecting To Your Server
Now that you have the plugin installed you can configure it to connect to a server.
To do so, you'll need the following pieces of information:
The server's IP or hostname.
The username you'll connect with.
The private key you'll use to authenticate your user.
You'll use this information to create an SSH configuration file that Visual Studio Code can use to SSH to the server to sync files and execute code on your behalf.
You will create this configuration using Visual Studio Code.
Now that you have the Remote-SSH plugin installed, you'll see a small green box in the bottom left-hand corner of the Visual Studio Code interface.
If you hover over the box with your mouse pointer, the popup will say Open a remote window.
The button looks like a greater than sign slightly under a less than sign > <, like the one in the following image:
Open a remote window green UI button
Click the button, and a dialog box appears in the top center.
Select Remote-SSH: Open Configuration File... from the list:
Selecting Configure SSH in the UI
The next prompt will ask you which configuration file you want to open.
If you're on Windows, you'll see two locations: one in your personal user directory, and one in the installation location for SSH.
You should use the file in your user directory when configuring the server.
Select the file and your editor will open the config file.
Add the following code to the file to define the connection to your server, replacing the highlighted sections with the information for your server:
Here's how this configuration file works:
Host: This specifies a name for your host.
This lets you use a short name or abbreviation instead of the full IP address or host name when connecting to the server.
HostName: The actual hostname of the server, which is either an IP address or a fully qualified domain name.
User: The user you want to use to connect with.
IdentityFile: The path to your SSH private key.
On Mac and Linux systems, you'll find this in your home directory in a hidden .ssh directory, typically called id _ rsa.
If you are on Windows you will have specified a location to save this file when you created it using putty-gen.
Specify the appropriate values in your file and save the file.
Visual Studio Code is now configured and ready to connect to your server.
Click on the green Open a remote window button in the bottom left-hand corner and select Remote-SSH: Connect to Host...
Connecting to the Server from Visual Studio Code
Once you've done this all the availble and configured servers will appear in the dropdown menu.
Select the server that you want to connect to from this list.
If this is the first time you have connected to this server from your machine, you'll likely be prompted with the SSH Fingerprint verification dialog, like the one in the following image:
Confirming your SSH Fingerprint
This is to ensure that you are really connecting to the server you think you are.
You can verify this by logging in to your server manually and running ssh-keygen -l -f / etc / ssh / ssh _ host _ key.pub to view the fingerprint of the server.
If this fingerprint is the same as the one being presented to you in Visual Studio Code, then you are indeed connecting to the server you think you are so you can click Continue.
Visual Studio Code defaults to opening a new window when a new connection is made.
A new window will appear with the welcome screen.
You'll know that your connection was successful if you see SSH: < ^ > your _ ip _ address _ or _ hostname < ^ > in the green box in the bottom left-hand corner.
This means that Visual Studio Code is connected and communicating with your remote server.
Successful SSH connection
Now that you're connected, you can run commands and code from your editor.
Step 3 - Executing Code on the Remote Server
The Remote-SSH plugin is configured, and it's time to run some code on your remote machine.
Open a terminal window by selecting Terminal from the navigation bar at the top of the Visual Studio window and clicking New Terminal.
You can also open a terminal by pressing CTRL + Shift + '.
The terminal that is opened is a terminal on your remote server, not one on your local machine.
When the terminal opens, issue the following command to view the IP address of your server to verify that you are connected to your remote server:
You'll see the following output in your terminal:
To test out the ability to run remote code, create a new Python file called hello.py in your editor.
When you are connected to your remote server, all files created through Visual Studio Code will be saved to that server, not on your local machine.
To run this program on your server, open a terminal in Visual Studio Code from the navigation menu or by pressing the key sequence CTRL + Shift + '.
Since this terminal session is connected to your remote server, run the following command in the terminal to execute your hello.py program:
Your program's output will be displayed.
Executing your Python Script
You can also execute the file from the Debug context menu by selecting Run without Debugging.
< $> note Note: If you have any development extensions installed in Visual Studio Code, like the Python extension, you will have to reinstall these extensions on your server through the Extension Marketplace.
If you have previously installed these plugins in Visual Studio Code, when you search for them again, the Marketplace will say Install on SSH: hostname.
Always pay attention to what devlopment context you are in, because this is where Visual Studio Code will install your plugins and create your files.
If you try to run your code without these plugins installed, error dialog boxes will appear in the bottom right-hand corner of the screen prompting you to install them on your remote server.
After you have installed these they will likely require you to reload Visual Studio Code.
When you relaunch it, it will continue working on the remote server without you having to manually reconnect.
You now have Visual Studio Code configured for development on a remote server using SSH.
Remote execution with an IDE provides many benefits, including the ability to quickly test how your code runs on different operating systems and different hardware specifications.
As long as you have an internet connection you could connect to your server and work on your code from any computer, and you'll be able to develop using a Linux environment even if you run Windows as your primary operating system.
How To Build an Inspirational Quote Application Using AdonisJs and MySQL
3206
AdonisJs is a Node.js web framework written in plain JavaScript that runs on all major operating systems.
It uses the popular MVC (Model - View - Controller) design pattern and offers a stable ecosystem for writing server-side web applications.
The framework features seamless authentication, SQL ORM (object-relational mapping), migrations, and database seeding.
AdonisJs has a similar architecture to the PHP web application framework Laravel, including the same folder structure and several shared setup concepts.
By default, AdonisJs uses the Edge template engine that is designed for intuitive use.
Just like Laravel, AdonisJs ships with an ORM called Lucid that serves as an interface for communication between an application "s models and the database.
With AdonisJs, developers can build a full-stack application where the back-end server will be responsible for applying the business logic, routing, and rendering all the pages for the application.
It is also possible to create a web service API to return JSON responses from a controller; these web services can then be consumed using front-end frameworks such as Vue.js, React, and Angular.
In this tutorial, you'll build an application with AdonisJs using its CLI.
You'll create routes, controllers, models, and views within your application and you'll carry out form validations.
The example in this tutorial will be an inspirational quote application in which a user can sign up and log in to create an inspirational quote.
This demo application will give you the opportunity to carry out CRUD (Create, Read, Update, and Delete) operations.
Before you begin this guide, you will need the following:
A local installation of Node.js (at least v8) and npm (at least v3.0).
Node.js is a JavaScript run-time environment that allows you to run your code outside of the browser.
It comes with a pre-installed package manager called npm, which lets you install and update packages.
To install these on macOS or Ubuntu 18.04, follow the steps in How to Install Node.js and Create a Local Development Environment on macOS or the Installing Using a PPA section of How To Install Node.js on Ubuntu 18.04.
MySQL installed on your machine.
Follow the instructions here to download and install it for your choice of the operating system.
To successfully install MySQL, you can either install it by using Homebrew on Mac or for Ubuntu 18.04 follow our tutorial How To Install MySQL on Ubuntu 18.04.
A basic understanding of JavaScript; see our How To Code in JavaScript series.
A text editor installed, such as Visual Studio Code, Atom, or Sublime Text.
< $> note Note: This tutorial uses a macOS machine for development.
If you're using another operating system, you may need to use sudo for npm commands in the first steps.
Step 1 - Installing the Adonis CLI
In this section, you will install Adonis CLI and all its required packages on your local machine.
The CLI will allow you to scaffold a new AdonisJs project as well as create and generate boilerplate for controllers, middlewares, and models in your application.
You'll also create your database for the project.
Run the following command to install the AdonisJs CLI globally on your machine via npm:
Once the installation process is complete, type the following command in the terminal to confirm the installation of AdonisJs and view the current version:
You will see output showing the current version of AdonisJs:
With the successful installation of the AdonisJs CLI, you now have access to and can use the adonis command to create fresh installations of an AdonisJs project, manage your project, and generate relevant files such as the controllers, models, etc.
Now, you can proceed to create a new AdonisJs project by using the adonis command as shown here:
The preceding command will create an application named < ^ > adonis-quotes-app < ^ > in a new directory with the same name in your local project directory with the relevant AdonisJs MVC structure.
Move into the new application folder:
Then start your application by running:
This will start the development server on the default port 3333 as specified inside the root .env file for your application.
Navigate to http: / / localhost: 3333 to view the welcome page of AdonisJs.
Welcome page of AdonisJs
Now you'll complete the setup of your database.
Here, you'll install the mysql driver to connect to your MySQL server from your Node.js application via npm.
To begin, go back to your terminal where the application is currently running, stop the process with CTRL + C and run the following command:
Now that you have successfully installed the MySQL Node.js driver for this application, you need to create the application database and set up the appropriate connection to it.
The latest version of MySQL that you have installed from the prerequisite tutorial uses a default authentication plugin named caching _ sha2 _ password.
This is currently not supported by Node.js drivers for MySQL.
To avoid any database connection issue from your application, you will need to create a new MySQL user and use the currently supported authentication plugin - mysql _ native _ password.
To begin, access the MySQL client using the root account:
You will be prompted to enter your root account password set up during the MySQL installation.
Next, create the user and password using the mysql _ native _ password plugin:
Next, create a database for the application with:
You "ve now successfully created the database for this application.
Now, enable access to the created database for the new MySQL user.
Run the following command to grant all privileges to the user in the database:
Reload the grant tables by running the following command to apply the changes that you just made:
Exit the MySQL client with:
You've successfully installed the AdonisJs CLI, created a new AdonisJs project, and installed mysql via npm.
You also created the database for this application and set up a MySQL user with the appropriate privileges to it. This is the basic setup for your application and in the next section you will begin to create the necessary views for your application.
Step 2 - Using the Edge Templating Engine
AdonisJs is shipped with its own template engine called Edge.
It allows you to create a reusable HTML template and enables the introduction of front-end logic into your application with minimal code.
Edge provides JavaScript developers with the tools while developing an application to build a component-based layout, write conditionals, use iterations, and create view layers to hold logic.
All template files end with the .edge extension and are stored in the resources / views directory.
The following are the views that your application will need to function properly:
Master Layout: With Edge, you can create a page that will contain the CSS, common JavaScript files, jQuery, and common parts of the user interface that will stay the same throughout the application - for example, the navigation bar, logo, header, etc. Once you've established the Master Layout page, other views (pages) in your application will inherit it.
Index view: This page will use the master layout to inherit common files and will also render contents for the homepage of the application.
Login page: This page will also use the master layout and render the form with the input fields for both username and password for users to log in.
Register page: Here, users will see a form to register and have their details persisted into the database.
Create quote page: Users will use this page to create an inspirational quote.
Edit quote page: Users will use this page to edit a quote.
View quote page: Users will use this page to view a particular quote.
To begin, use the adonis command to create the master layout page by running the following command:
This command will automatically create a master.edge file in your resources / views / layouts folder.
Open the new file:
Add the following code in it:
In this file, you include the CDN files for Bootstrap CSS, Bootstrap JavaScript, and jQuery.
You add a global CSS file name of style.css and within the div you include a partial file named navbar.
To reuse fragments of HTML code that you require across multiple pages in your application, like nav or footer, you can incorporate partials.
These are smaller files containing the repeated code making it quicker to update code for these elements in one place rather than at every instance it occurs.
The navbar contains markup for a Login and Register buttons, a logo, and a home link.
With this in place, all the subsequent pages that will be created for this application can extend the master layout and have the navbar rendered without the need to write the content all over again.
You'll create this navbar file later in the tutorial.
Finally, you define a section tag @!
section () to include content from other pages and have them rendered by the master layout.
For this to work as expected, all the new pages that will extend the master layout must also define a section tag with the same name (i.e., @ section ("'" content "'")).
Save and exit the file once you're finished editing it.
Next, you will use the adonis command to create the navigation bar:
Open the newly created file:
Then add the following code to it:
In addition to defining the links to the homepage and a button to register and login, you add a @ loggedIn tag.
With this in place you can write a conditional statement around the authenticated user and display appropriate contents where necessary.
For an authenticated user, the application will display their username and a button to create a new quote.
If a user is not logged in, your application will display a button to either log in or register.
This page will be included as a partial on every page as it was earlier in the master layout for this application.
Now, you'll create the index page that you'll use for the application's homepage.
It will render and display the list of all inspirational quotes that users write:
You will see an output similar to the following:
The file created here will be located in resources / views / index.edge.
Open the file:
Then add the following code:
Here, you indicate that this view will use the master layout by extending it. This page can now have access to all the libraries, stylesheets, and the navbar included in the master layout.
Next, you iterate over an array of quotes using the built-in @ each tag.
The quotes array will be passed to this view from the QuoteController that you'll create later in this tutorial.
If there are no quotes, an appropriate message will be displayed.
Now, to create the login page, run the following command from the terminal:
You will see an output similar to:
This will automatically create an auth folder within resources / views and also create a login.edge file within it. Open the login.edge file:
This file holds a form that contains input elements that you'll use to collect the username and password of a registered user before they can successfully get authenticated and start creating quotes.
Another important element to note on this page is the {{csrfField ()}}.
It is a global variable that AdonisJs will use to pass the CSRF access token when sending a POST, PUT, and DELETE request from your application.
This was put in place to protect your application from Cross-Site Request Forgery (CSRF) attacks.
It works by generating a unique CSRF secret for each user visiting your website and once your users send an HTTP request from the frontend, a corresponding token is generated for this secret and passed along with the request.
This will allow the middleware created for this request within AdonisJs to verify that both the token and CSRF secret is valid and belong to the currently authenticated user.
Save and exit the file once you're finished.
Next, you will create the register page with this command:
Locate and open the newly created file in resources / views / auth / register.edge:
Similarly to what you have on the login page, this file contains an HTML form with input fields to collect the name, email, and password of a user during the registration process.
Also included is the {{csrfField ()}} as it is required for each post request for an AdonisJs application.
Now, you will generate a new file to create an inspirational quote by running the following command from the terminal:
You will see output like:
Open resources / views / quotes / create-quote.edge:
And add the following content to it:
This page extends the master layout and contains an HTML form with a text area element that allows a user to input text over multiple rows before being posted and handled by the appropriate route.
Next, you will create a page for editing a particular quote.
Run the following command from the terminal:
Open the file with:
Add the following content to resources / views / quotes / edit-quote:
This page holds similar content as the create-quote.edge file - the difference is that it contains the details of a particular quote that needs to be edited, < form method = "POST" action = "/ update-quote / {{quote.id}}" >.
Finally, generate a page to view a single inspirational quote:
You will see an output similar to this:
This page renders the details of a particular quote, that includes the body of the quote, quote.body, and the author who created it, quote.username.
Once you're finished with the file, save and exit.
You have created all the required pages for your application by using the Edge templating engine.
Next, you'll configure and create a connection to your application's database.
Step 3 - Creating a Database Schema
If you serve your application now it will throw an error as you are yet to connect the application to a database.
In this section, you will set up a connection to the database and then use the adonis command to generate a migration file that will be used to create the tables for it.
AdonisJs ships with an ORM named Lucid ORM, which provides active record implementation for working with your database.
It takes away the hassle of writing SQL queries that retrieve data from the database in realtime.
This is especially helpful when working on a complex application that requires a lot of queries.
As an example, retrieving all the quotes from your application can be achieved by writing this:
To proceed with the appropriate configuration for your application database, ensure that you are still within the root directory of your application and create a .env file:
Open the newly created file and add the following content:
By default the database connection for an AdonisJs application is SQLite, which you will update to MySQL here.
You also specify the PORT for the application, the application environment, and your database credentials.
Ensure that you replace the DB _ USER, DB _ PASSWORD, and DB _ DATABASE placeholder with your credentials.
Next, you will create the model and a migration file for Quote using the Adonis CLI.
To accomplish that, run the following command:
You "ll see output similar to the following:
This command will create a model for Quote in the app / Models folder and a schema file in the database / migrations folder.
The newly created schema file will be prefixed with the current timestamp.
Open the schema file with:
Update its content with the following code:
A schema file in AdonisJs requires two different methods, which are:
up: Used to create a new table or alter an existing one.
down: Used to revert the changes applied in the up method.
In addition to the timestamps () and increments () fields, you update the content of the schema file with the field attributes user _ id, username, and the body of a quote that will be created.
The user _ id and username fields reference the details of the user who create a particular quote.
This defines a one to many relationship and means a user can own an infinite number of quotes while a single quote can only belong to a user.
AdonisJs comes installed with a User model and its migration file by default, which requires only a small modification to establish the relationship between the User and Quote model.
Open the User model in app / Models / User.js:
Add this method immediately after the tokens () method:
This will establish a one to many relationship with the Quote table using user _ id as the foreign key.
To wrap up this section, use the following command to run migrations, which will execute the up () method of all migration files:
You've configured and secured a connection with your database.
You also created a Quote model and its corresponding schema file and created a one to many relationship between a User and Quote.
Next, you'll generate the routes and create controllers to handle HTTP requests and the business logic to create, edit, and delete an inspirational quote.
Step 4 - Creating Controllers and Setting Up Routes
In this section, you will start by creating controllers to handle all the logic for the application and later attach these controllers to a specific route for it to be accessed by users via a URL.
To start, you'll use the Adonis CLI to create a new HTTP request controller to handle all authentication processes for your application by running the following command:
This command will create an AuthController.js file and save it within app / Controllers / Http folder.
You use the flag --type to indicate that you want this controller to be an HTTP controller.
Next, open the newly created controller file:
Update it with the following content:
In this file, you import the User model and then create two methods named loginView () and registerView () to render the login and register pages respectively.
Finally, you create the following asynchronous methods:
postLogin (): This method will obtain the value of the email and password posted through the help of the inbuilt request method in AdonisJs and then validate this user against the details in the database.
If such a user exists in the database and has inputted the correct credential, they will be redirected back to the homepage and authenticated before they can create a new quote.
Otherwise, a message indicating the wrong credentials will be displayed.
postRegister (): This will receive the value of the username, email, and password for a user to create an account for such user in the database.
A message indicating that such user has been created successfully will be passed to the session and the user will be redirected to the login page to get authenticated and start creating a quote.
logout (): This method will handle the logout functionality and redirect the user back to the homepage.
Now that you have set up the controller to register and authenticate users, you will proceed by creating an HTTP request controller to manage all operations regarding quotes.
Back in the terminal, run the following command to create the QuoteController:
Using the --resource flag will create a controller with predefined resourceful methods to do CRUD (Create, Read, Update, and Delete) operations.
You will see:
Locate this file within app / Controllers / Http / QuoteController.js:
In this controller, you imported the Quote model and updated the following methods that were automatically created by using AdonisJs CLI:
index (): to fetch all quotes from the database and render it on the homepage of the application.
create (): to render a page for creating quotes.
store (): to persist a newly created quote into the database and return an appropriate response.
show (): to obtain the id of a particular quote, retrieve it from the database, and render it on the edit quote page.
edit (): to obtain the detail of a particular quote from the database and render it for editing.
update (): to process any update to a quote and redirect the user back to the homepage.
destroy (): to delete a particular quote and remove it entirely from the database.
After creating all the necessary controllers for this application, you can now set up the routes so that users can easily interact with your application.
To begin, navigate to start / routes.js file
Replace its content with the following:
Here, you define the path for each route in your application, specify the HTTP verbs for each action, and bound the route to a particular method in each controller.
You also name each of these routes as they have been referenced within the controllers and views.
To ensure that only authenticated users can access all the quotes routes, you assign a group middleware named to it. And lastly, you attach a validator method to the register.store route to validate user input.
You've created your controllers and set up the routes for your application.
Next you'll create the validator method defined in this step.
Step 5 - Validating User Input
AdonisJs does not have validators built-in by default.
As a result you'll install and register the validator for your application manually.
Run the following command to install it:
Open the following file to register the validator provider:
Then register the validator provider by appending it to the list of providers as shown following:
Now that you have installed and registered the validator provider within your application, create a custom validator to validate user input during registration with the following command:
This will create a Register.js file in the App / validators directory.
You define rules for specific fields in your application.
If validations fail at any time, the validator automatically sets the error as a flash message and the user will be redirected back to the form.
Save and exit the file once you're finished editing.
Finally, to add styling for your application, open the following file:
Replace its contents with the following:
In this file you update the CSS styling of your application in the style.css file.
You've installed and registered a validator provider for the purpose of checking users' input during the registration process.
You also updated the content of your stylesheet to add more styling to the application.
In the final step you'll test your application.
Step 6 - Serving the Application
In this step, you'll serve your application and create a user and password to test the authentication.
You'll also add a quote to your app and view this on the homepage.
To test your application, start the development server with the following command from the root directory of your application:
This will start the application on the port defined inside the root .env file, which is 3333.
Navigate to http: / / localhost: 3333 from your browser.
Quote app homepage
The homepage is empty at the moment as you have not created any quotes.
Click on the Register button.
Registration page
Enter your details and click on the Submit button to complete the registration process.
You will be redirected to the login page.
Enter your email address and password for authentication.
Login page
Once you're authenticated, click on the Create Quote button.
Create quote page
Enter a quote and navigate to the View all page to see your quote.
View all quotes page
You've tested your application by creating and authenticating a user and then writing a quote.
In this tutorial you've built a web application with AdonisJs.
You set up the application using the AdonisJs CLI and leveraged the CLI for creating other relevant files such as controllers, models, and views.
You can build web applications with this framework irrespective of their size and complexity.
Feel free to download the source code for this project here on GitHub.
To explore further features, you can also visit the official documentation.
If you would like to explore some of our other JavaScript framework tutorials, check out the following:
How To Build a Customer List Management App with React and TypeScript
How To Build a Blog with Nest.js, MongoDB, and Vue.js
How To Build a Weather App with Angular, Bootstrap, and the APIXU API
How To Create a New Sudo-enabled User on CentOS 8 Quickstart
4002
This guide will show you how to create a new user with sudo access on CentOS 8, without having to modify your server's / etc / sudoers file.
If you want to configure sudo for an existing CentOS user, skip to step 3.
Use your server's IP address or hostname in place of < ^ > your _ server _ ip _ address < ^ > above.
Be sure to replace < ^ > sammy < ^ > with the username you'd like to create.
Use the passwd command to update the new user's password:
Remember to replace < ^ > sammy < ^ > with the user that you just created.
You will be prompted twice for a new password:
Step 3 - Adding the User to the wheel Group
Use the usermod command to add the user to the wheel group:
Once again, be sure to replace < ^ > sammy < ^ > with the username you'd like to give sudo priveleges to.
By default, on CentOS, all members of the wheel group have full sudo access.
To test that the new sudo permissions are working, first use the su command to switch from the root user to the new user account:
The first time you use sudo in a session, you will be prompted for the password of that user's account.
Enter the password of the sudo-enabled user, not the root password.
In this quickstart tutorial we created a new user account and added it to the wheel group to enable sudo access.
For more detailed information on setting up a CentOS 8 server, please read our Initial Server Setup with CentOS 8 tutorial.
How To Install Discourse on Ubuntu 18.04
3984
An Article from Discourse
Discourse is an open-source discussion platform.
It can be used as a mailing list, a discussion forum, or a long-form chat room.
In this tutorial, we'll install Discourse in an isolated environment using Docker, a containerization application.
Before we get started, there are a few things we need to set up first:
One Ubuntu 18.04 server with at least 2GB of RAM, set up by following this Initial Server Setup on Ubuntu 18.04 tutorial, including a sudo non-root user and a firewall.
Docker installed on your server, which you can do by following Step 1 of the Docker installation tutorial for Ubuntu 18.04.
A domain name that resolves to your server, which you can set up by following this hostname tutorial.
An SMTP mail server.
If you don't want to run your own mail server, you can use another service, like a free account on Mailgun.
Note: Discourse requires a swap file if you are using 1 GB of RAM.
Although swap is generally recommended for systems utilizing traditional spinning hard drives, using swap with SSDs can cause issues with hardware degradation over time.
Due to this consideration, we do not recommend enabling swap on DigitalOcean or any other provider that utilizes SSD storage.
Doing so can impact the reliability of the underlying hardware for you and your neighbors.
Hence, we recommend a minimum of 2 GB of RAM to run Discourse on a DigitalOcean Droplet.
Refer to How To Add Swap Space on Ubuntu 18.04 for details on using swap.
Step 1 - Downloading Discourse
With all the prerequisites out of the way, you can go straight to installing Discourse.
You will need to be root through the rest of the setup and bootstrap process, so first, switch to a root shell.
Next, create the / var / discourse directory, where all the Discourse-related files will reside.
Finally, clone the official Discourse Docker Image into / var / discourse.
With the files we need in place, we can move on to configuration and bootstrapping.
Step 2 - Configuring and Bootstrapping Discourse
Move to the / var / discourse directory, where the Discourse files are.
From here, you can launch the included setup script.
You will be asked the following questions:
Hostname for your Discourse?
Enter the hostname you'd like to use for Discourse, e.g. discourse. < ^ > your _ domain.com < ^ >, replacing < ^ > your _ domain.com < ^ > with your domain name.
You do need to use a domain name because an IP address won't work when sending email.
Email address for admin account?
Choose the email address that you want to use for the Discourse admin account.
It can be totally unrelated to your Discourse domain and can be any email address you find convenient.
Note that this email address will be made the Discourse admin by default when the first user registers with that email.
You'll also need this email address later when you set up Discourse from its web control panel.
SMTP server address?
SMTP user name?
SMTP port?
SMTP password?
Enter your SMTP server details for these questions.
If you're using Mailgun, the SMTP server address will be smtp.mailgun.org, the user name and password are SMTP credentials for your domain under domains tab.
Finally, you will be asked to confirm all the settings you just entered.
After you confirm your settings, the script will generate a configuration file called app.yml and then the bootstrap process will start.
Note: If you need to change or fix these settings after bootstrapping, edit your / containers / app.yml file and run. / launcher rebuild app. Otherwise, your changes will not take effect.
Bootstrapping takes between 2-8 minutes, after which your instance will be running!
Let's move on to creating an administrator account.
Step 3 - Registering an Admin Account
Visit your Discourse domain in your favorite web browser to view the Discourse web page.
congratulations
If you receive a 502 Bad Gateway error, try waiting a minute or two and then refreshing; Discourse may not have finished starting yet.
When the page loads, click the blue Register button.
You'll see a form entitled Register Admin Account with the following fields:
Email: Choose the email address you provided earlier from the pull-down menu.
Username: Choose a username.
Password: Choose a strong password.
Then click the blue Register button on the form to submit it. You'll see a dialog that says Confirm your Email.
Check your inbox for the confirmation email.
If you didn't receive it, try clicking the Resend Activation Email button.
If you're still unable to register a new admin account, please see the Discourse email troubleshooting checklist.
After registering your admin account, the setup wizard will launch and guide you through Discourse's basic configuration.
You can walk through it now or click Maybe Later to skip.
wizard
After completing or skipping the setup wizard, you'll see some topics and the Admin Quick Start Guide (labeled READ ME FIRST), which contains tips for further customizing your Discourse installation.
homepage
You're all set!
If you need to upgrade Discourse in the future, you can do it from the command line by pulling the latest version of the code from the Git repo and rebuliding the app, like this:
You can also update it in your browser by visiting http: / / discourse. < ^ > your _ domain.com < ^ > / admin / upgrade, clicking Upgrade to the Latest Version, and following the instructions.
upgrade
You can now start managing your Discourse forum and let users sign up.
Learn more about Discourse's features on the Discourse About page.
How To Set Up a React Project with Create React App
3994
React is a popular JavaScript framework for creating front-end applications.
Originally created by Facebook, it has gained popularity by allowing developers to create fast applications using an intuitive programming paradigm that ties JavaScript with an HTML-like syntax known as JSX.
Starting a new React project used to be a complicated multi-step process that involved setting up a build system, a code transpiler to convert modern syntax to code that is readable by all browsers, and a base directory structure.
But now, Create React App includes all the JavaScript packages you need to run a React project, including code transpiling, basic linting, testing, and build systems.
It also includes a server with hot reloading that will refresh your page as you make code changes.
Finally, it will create a structure for your directories and components so you can jump in and start coding in just a few minutes.
In other words, you don "t have to worry about configuring a build system like Webpack.
You don "t need to set up Babel to transpile you code to be cross-browser usable.
You don "t have to worry about most of the complicated systems of modern front-end development.
You can start writing React code with minimal preparation.
By the end of this tutorial, you'll have a running React application that you can use as a foundation for any future applications.
You'll make your first changes to React code, update styles, and run a build to create a fully minified version of your application.
You'll also use a server with hot reloading to give you instant feedback and will explore the parts of a React project in depth.
Finally, you will begin writing custom components and creating a structure that can grow and adapt with your project.
To follow this tutorial, you "ll need the following:
Node.js version 10.16.0 installed on your computer.
It will also help to have a basic understanding of JavaScript, which you can find in the How To Code in JavaScript series, along with a basic knowledge of HTML and CSS.
Step 1 - Creating a New Project with Create React App
In this step, you'll create a new application using the npm package manager to run a remote script.
The script will copy the necessary files into a new directory and install all dependencies.
When you installed Node, you also installed a package managing application called npm. npm will install JavaScript packages in your project and also keep track of details about the project.
If you'd like to learn more about npm, take a look at our How To Use Node.js Modules with npm and package.json tutorial.
npm also includes a tool called npx, which will run executable packages.
What that means is you will run the Create React App code without first downloading the project.
The executable package will run the installation of create-react-app into the directory that you specify.
It will start by making a new project in a directory, which in this tutorial will be called < ^ > digital-ocean-tutorial < ^ >.
Again, this directory does not need to exist beforehand; the executable package will create it for you.
The script will also run npm install inside the project directory, which will download any additional dependencies.
To install the base project, run the following command:
This command will kick off a build process that will download the base code along with a number of dependencies.
When the script finishes you will see a success message that says:
< ^ > your _ file _ path < ^ > will be your current path.
If you are a macOS user, it will be something like / Users / < ^ > your _ username < ^ >; if you are on an Ubuntu server, it will say something like / home / < ^ > your _ username < ^ >.
You will also see a list of npm commands that will allow you to run, build, start, and test your application.
You'll explore these more in the next section.
< $> note Note: There is another package manager for JavaScript called yarn.
It's supported by Facebook and does many of the same things as npm.
Originally, yarn provided new functionality such as lock files, but now these are implemented in npm as well. yarn also includes a few other features such as offline caching.
Further differences can be found on the yarn documentation.
If you have previously installed yarn on your system, you will see a list of yarn commands such as yarn start that work the same as npm commands.
You can run npm commands even if you have yarn installed.
If you prefer yarn, just replace npm with yarn in any future commands.
The results will be the same.
Now your project is set up in a new directory.
Change into the new directory:
You are now inside the root of your project.
At this point, you've created a new project and added all of the dependencies.
But you haven't take any actions to run the project.
In the next section, you'll run custom scripts to build and test the project.
Step 2 - Using react-scripts
In this step, you will learn about the different react-scripts that are installed with the repo.
You will first run the test script to execute the test code.
Then you will run the build script to create a minified version.
Finally, you'll look at how the eject script can give you complete control over customization.
Now that you are inside the project directory, take a look around.
You can either open the whole directory in your text editor, or if you are on the terminal you can list the files out with the following command:
The -a flag ensures that the output also includes hidden files.
Either way, you will see a structure like this:
Let's explain these one by one:
node _ modules / contains all of the external JavaScript libraries used by the application.
You will rarely need to open it.
The public / directory contains some base HTML, JSON, and image files.
These are the roots of your project.
You'll have an opportunity to explore them more in Step 4.
The src / directory contains the React JavaScript code for your project.
Most of the work you do will be in that directory.
You'll explore this directory in detail in Step 5.
The .gitignore file contains some default directories and files that git - your source control - will ignore, such as the node _ modules directory.
The ignored items tend to be larger directories or log files that you would not need in source control.
It also will include some directories that you'll create with some of the React scripts.
README.md is a markdown file that contains a lot of useful information about Create React App, such as a summary of commands and links to advanced configuration.
For now, it's best to leave the README.md file as you see it. As your project progresses, you will replace the default information with more detailed information about your project.
The last two files are used by your package manager.
When you ran the initial npx command, you created the base project, but you also installed the additional dependencies.
When you installed the dependencies, you created a package-lock.json file.
This file is used by npm to ensure that the packages match exact versions.
This way if someone else installs your project, you can ensure they have identical dependencies.
Since this file is created automatically, you will rarely edit this file directly.
The last file is a package.json.
This contains metadata about your project, such as the title, version number, and dependencies.
It also contains scripts that you can use to run your project.
Open the package.json file in your favorite text editor:
When you open the file, you will see a JSON object containing all the metadata.
If you look at the scripts object, you'll find four different scripts: start, build, test, and eject.
These scripts are listed in order of importance.
The first script starts the local development environment; you'll get to that in the next step.
The second script will build your project.
You'll explore this in detail in Step 4, but it's worth running now to see what happens.
The build Script
To run any npm script, you just need to type npm run < ^ > script _ name < ^ > in your terminal.
There are a few special scripts where you can omit the run part of the command, but it's always fine to run the full command.
To run the build script, type the following in your terminal:
You will immediately see the following message:
This tells you that Create React App is compiling your code into a usable bundle.
When it's finished, you'll see the following output:
List out the project contents and you will see some new directories:
You now have a build directory.
If you opened the .gitignore file, you may have noticed that the build directory is ignored by git.
That's because the build directory is just a minified and optimized version of the other files.
There's no need to use version control since you can always run the build command.
You'll explore the output more later; for now, it's time to move on to the test script.
The test Script
The test script is one of those special scripts that doesn't require the run keyword, but works even if you include it. This script will start up a test runner called Jest.
The test runner looks through your project for any files with a .spec.js or .test.js extension, then runs those files.
To run the test script, type the following command:
After running this script your terminal will have the output of the test suite and the terminal prompt will disappear.
There are a few things to notice here.
First, as noted before, it automatically detects any files with test extensions including .test.js and .spec.js.
In this case, there is only one test suite - that is, only one file with a .test.js extension - and that test suite contains only one test.
Jest can detect tests in your code hierarchy, so you can nest tests in a directory and Jest will find them.
Second, Jest doesn't run your test suite once and then exit.
Rather, it continues running in the terminal.
If you make any changes in the source code, it will rerun the tests again.
You can also limit which tests you run by using one of the keyboard options.
If you type o, for example, you will only run the tests on files that have changed.
This can save you lots of time as your test suites grow.
Finally, you can exit the test runner by typing q. Do this now to regain your command prompt.
The eject Script
The final script is npm eject.
This script copies your dependencies and configuration files into your project, giving you full control over your code but ejecting the project from the Create React App integrated toolchain.
You will not run this now because, once you run this script, you can't undo this action and you will lose any future Create React App updates.
The value in Create React App is that you don't have to worry about a significant amount of configuration.
Building modern JavaScript applications requires a lot of tooling from build systems, such as Webpack, to compilation tools, such as Babel.
Create React App handles all the configuration for you, so ejecting means dealing with this complexity yourself.
The downside of Create React App is that you won't be able to fully customize the project.
For most projects that's not a problem, but if you ever want to take control of all aspects of the build process, you'll need to eject the code.
However, as mentioned before, once you eject the code you will not be able to update to new versions of Create React App, and you'll have to manually add any enhancements on your own.
At this point, you've executed scripts to build and test your code.
In the next step, you'll start the project on a live server.
Step 3 - Starting the Server
In this step, you will initialize a local server and run the project in your browser.
You start your project with another npm script.
Like npm test, this script does not need the run command.
When you run the script you will start a local server, execute the project code, start a watcher that listens for code changes, and open the project in a web browser.
Start the project by typing the following command in the root of your project.
For this tutorial, the root of your project is the < ^ > digital-ocean-tutorial < ^ > directory.
Be sure to open this in a separate terminal or tab, because this script will continue running as long as you allow it:
You'll see some placeholder text for a brief moment before the server starts up, giving this output:
If you are running the script locally, it will open the project in your browser window and shift the focus from the terminal to the browser.
If that doesn't happen, you can visit http: / / localhost: 3000 / to see the site in action.
If you already happen to have another server running on port 3000, that's fine.
Create React App will detect the next available port and run the server with that.
In other words, if you already have one project running on port 3000, this new project will start on port 3001.
If you are running this from a remote server you can still see your site without any additional configuration.
The address will be http: / / < ^ > your _ server _ ip < ^ >: 3000.
If you have a firewall configured, you'll need to open up the port on your remote server.
In the browser, you will see the following React template project:
React template project
As long as the script is running, you will have an active local server.
To stop the script, either close the terminal window or tab or type CTRL + C or  - + c in the terminal window or tab that is running your script.
At this point, you have started the server and are running your first React code.
But before you make any changes to the React JavaScript code, you will see how React renders to the page in the first place.
Step 4 - Modifying the Homepage
In this step, you will modify code in the public / directory.
The public directory contains your base HTML page.
This is the page that will serve as the root to your project.
You will rarely edit this directory in the future, but it is the base from which the project starts and a crucial part of a React project.
If you cancelled your server, go ahead and restart it with npm start, then open public / in your favorite text editor in a new terminal window:
Alternatively, you can list the files with the ls command:
You will see a list of files such as this:
favicon.ico, logo192.png, and logo512.png are icons that a user would see either in the tab of their browser or on their phone.
The browser will select the proper-sized icons.
Eventually, you'll want to replace these with icons that are more suited to your project.
For now, you can leave them alone.
The manifest.json is a structured set of metadata that describes your project.
Among other things, it lists which icon will be used for different size options.
The robots.txt file is information for web crawlers.
It tells crawlers which pages they are or are not allowed to index.
You will not need to change either file unless there is a compelling reason to do so.
For instance, if you wanted to give some users a URL to special content that you do not want easily accessible, you can add it to robots.txt and it will still be publicly available, but not indexed by search engines.
The index.html file is the root of your application.
This is the file the server reads, and it is the file that your browser will display.
Open it up in your text editor and take a look.
If you are working from the command line, you can open it with the following command:
Here's what you will see:
The file is pretty short.
There are no images or words in the < body >.
That's because React builds the entire HTML structure itself and injects it with JavaScript.
But React needs to know where to inject the code, and that's the role of index.html.
In your text editor, change the < title > tag from React App to Sandbox:
Save and exit your text editor.
Check your browser.
The title is the name located on the browser tab. It will update automatically.
If not, refresh the page and notice the change.
Now go back to your text editor.
Every React project starts from a root element.
There can be multiple root elements on a page, but there needs to be at least one.
This is how React knows where to put the generated HTML code.
Find the element < div id = "root" >.
This is the div that React will use for all future updates.
Change the id from root to base:
Save the changes.
You will see an error in your browser:
Error message saying "Target container is not a DOM element"
React was looking for an element with an id of root.
Now that it is gone, React can't start the project.
Change the name back from base to root:
Save and exit index.html.
At this point, you've started the server and made a small change to the root HTML page.
You haven't yet changed any JavaScript code.
In the next section, you will update the React JavaScript code.
Step 5 - Modifying the Heading Tag and Styling
In this step, you will make your first change to a React component in the src / directory.
You "ll make a small change to the CSS and the JavaScript code that will automatically update in your browser using the built-in hot reloading.
If you stopped the server, be sure to restart it with npm start.
Now, take some time to see the parts of the src / directory.
You can either open the full directory in your favorite text editor, or you can list out the project in a terminal with the following command:
You will see the following files in your terminal or text editor.
Let's go through these files one at a time.
You will not spend much time with the serviceWorker.js file at first, but it can be important as you start to make progressive web applications.
The service worker can do many things including push notifications and offline caching, but for now it's best to leave it alone.
The next files to look at are setupTests.js and App.test.js.
These are used for test files.
In fact, when you ran npm test in Step 2, the script ran these files.
The setupTests.js file is short; all it includes is a few custom expect methods.
You'll learn more about these in future tutorials in this series.
Open App.test.js:
When you open it, you'll see a basic test:
The test is looking for the phrase learn react to be in the document.
If you go back to the browser running your project, you'll see the phrase on the page.
React testing is different from most unit tests.
Since components can include visual information, such as markup, along with logic for manipulating data, traditional unit tests do not work as easily.
React testing is closer to a form of functional or integration testing.
Next, you'll see some styling files: App.css, index.css, and logo.svg.
There are multiple ways of working with styling in React, but the easiest is to write plain CSS since that requires no additional configuration.
There are multiple CSS files because you can import the styles into a component just like they were another JavaScript file.
Since you have the power to import CSS directly into a component, you might as well split the CSS to only apply to an individual component.
What you are doing is separating concerns.
You are not keeping all the CSS separate from the JavaScript.
Instead you are keeping all the related CSS, JavaScript, markup, and images grouped together.
Open App.css in your text editor.
This is the code you'll see:
This is a standard CSS file with no special CSS preprocessors.
You can add them later if you want, but at first, you only have plain CSS.
Create React App tries to be unopinionated while still giving an out-of-the-box environment.
Back to App.css, one of the benefits of using Create React App is that it watches all files, so if you make a change, you'll see it in your browser without reloading.
To see this in action make a small change to the background-color in App.css.
Change it from # 282c34 to blue then save the file.
The final style will look like this:
Check out your browser.
Here's how it looked before:
React app with dark background
Here's how it will look after the change:
React app with blue background
Go ahead and change background-color back to # 282c34.
You've made a small CSS change.
Now it's time to make changes to the React JavaScript code.
Start by opening index.js.
Here's what you'll see:
At the top, you are importing React, ReactDOM, index.css, App, and serviceWorker.
By importing React, you are actually pulling in code to convert JSX to JavaScript.
JSX are the HTML-like elements.
For example, notice how when you use App, you treat it like an HTML element < App / >.
You'll explore this more in future tutorials in this series.
ReactDOM is the code that connects your React code to the base elements, like the index.html page you saw in public /.
Look at the following highlighted line:
This code instructs React to find an element with an id of root and inject the React code there.
< App / > is your root element, and everything will branch from there.
This is the beginning point for all future React code.
At the top of the file, you'll see a few imports.
You import index.css, but don't actually do anything with it. By importing it, you are telling Webpack via the React scripts to include that CSS code in the final compiled bundle.
If you don't import it, it won't show up.
Exit from src / index.js.
At this point, you still haven't seen anything that you are viewing in your browser.
To see this, open up App.js:
The code in this file will look like a series of regular HTML elements.
Change the contents of the < p > tag from Edit < code > src / App.js < / code > and save to reload. to Hello, world and save your changes.
Head over to your browser and you'll see the change:
React app with "Hello, world" in paragraph tag
You've now made your first update to a React component.
Before you go, notice a few more things.
In this component, you import the logo.svg file and assign it to a variable.
Then in the < img > element, you add that code as the src.
There are a few things going on here.
Look at the img element:
Notice how you pass the logo into curly braces.
Anytime you are passing attributes that are not strings or numbers, you need to use the curly braces.
React will treat those as JavaScript instead of strings.
In this case, you are not actually importing the image; instead you are referencing the image.
When Webpack builds the project it will handle the image and set the source to the appropriate place.
Exit the text editor.
If you look at the DOM elements in your browser, you'll see it adds a path.
If you are using Chrome, you can inspect the element by right-clicking the element and selecting Inspect.
Here's how it would look in the browser:
Inspecting element with chrome dev tools
The DOM has this line:
Your code will be slightly different since the logo will have a different name.
Webpack wants to make sure the image path is unique.
So even if you import images with the same name, they will be saved with different paths.
At this point, you've made a small change to the React JavaScript code.
In the next step, you'll use the build command to minify the code into a small file that can be deployed to a server.
Step 6 - Building the Project
In this step, you will build the code into a bundle that can be deployed to external servers.
Head back to your terminal and build the project.
You ran this command before, but as a reminder, this command will execute the build script.
It will create a new directory with the combined and minified files.
To execute the build, run the following command from the root of your project:
There will be a delay as the code compiles and when it's finished, you'll have a new directory called build /.
Open up build / index.html in a text editor.
You will see something like this:
The build directory takes all of your code and compiles and minifies it into the smallest usable state.
It doesn't matter if a human can read it, since this is not a public-facing piece of code.
Minifying like this will make the code take up less space while still allowing it to work.
Unlike some languages like Python, the whitespace doesn't change how the computer interprets the code.
In this tutorial, you have created your first React application, configuring your project using JavaScript build tools without needing to go into the technical details.
That's the value in Create React App: you don't need to know everything to get started.
It allows you to ignore the complicated build steps so you can focus exclusively on the React code.
You've learned the commands to start, test, and build a project.
You'll use these commands regularly, so take note for future tutorials.
Most importantly, you updated your first React component.
If you would like to see React in action, try our How To Display Data from the DigitalOcean API with React tutorial.
How To Set Up and Configure a Certificate Authority (CA) On CentOS 8
4001
A Certificate Authority (CA) is an entity responsible for issuing digital certificates to verify identities on the internet.
Although public CAs are a popular choice for verifying the identity of websites and other services that are provided to the general public, private CAs are typically used for closed groups and private services.
Building a private Certificate Authority will enable you to configure, test, and run programs that require encrypted connections between a client and a server.
With a private CA, you can issue certificates for users, servers, or individual programs and services within your infrastructure.
Some examples of programs on Linux that use their own private CA are OpenVPN and Puppet.
You can also configure your web server to use certificates issued by a private CA in order to make development and staging environments match production servers that use TLS to encrypt connections.
In this guide, we'll learn how to set up a private Certificate Authority on a CentOS 8 server, and how to generate and sign a testing certificate using your new CA.
You will also learn how to import the CA server "s public certificate into your operating system" s certificate store so that you can verify the chain of trust between the CA and remote servers or users.
Finally you will learn how to revoke certificates and distribute a Certificate Revocation List to make sure only authorized users and systems can use services that rely on your CA.
To follow this tutorial, you will need a CentOS 8 server with a sudo enabled, non-root user, and a firewall set up with firewalld.
You can follow our Initial Server Setup with CentOS 8 guide to complete that set up.
This server will be referred to as the CA Server in this tutorial.
Ensure that the CA Server is a standalone system.
It will only be used to import, sign, and revoke certificate requests.
It should not run any other services, and ideally it will be offline or completely shut down when you are not actively working with your CA.
< $> note Note: The last section of this tutorial is optional if you would like to learn about signing and revoking certificates.
If you choose to complete those practice steps, you will need a second CentOS 8 server or you can also use your own local Linux computer running CentOS 8, Fedora or a RedHat derivative.
Step 1 - Installing Easy-RSA
The first task in this tutorial is to install the easy-rsa set of scripts on your CA Server. easy-rsa is a Certificate Authority management tool that you will use to generate a private key, and public root certificate, which you will then use to sign requests from clients and servers that will rely on your CA.
The easy-rsa package is not available by default in CentOS 8, so you will need to enable the Extra Packages for Enterprise Linux (EPEL) repository.
EPEL is managed by the Fedora Project and contains non-standard but popular packages for Fedora, CentOS, and other Linux distributions that use the RPM package format.
Login to your CA Server as the non-root sudo user that you created during the initial setup steps and run the following:
You will be prompted to download the package and install it. Press y to confirm you want to install the package.
Now install the easy-rsa package, again entering y at the prompt:
At this point you have everything you need set up and ready to use Easy-RSA.
In the next step you will create a Public Key Infrastructure, and then start building your Certificate Authority.
Step 2 - Preparing a Public Key Infrastructure Directory
Now that you have installed easy-rsa, it is time to create a skeleton Public Key Infrastructure (PKI) on the CA Server.
Ensure that you are still logged in as your non-root user and create an easy-rsa directory.
Make sure that you do not use sudo to run any of the following commands, since your normal user should manage and interact with the CA without elevated privileges.
This will create a new directory called easy-rsa in your home folder.
We'll use this directory to create symbolic links pointing to the easy-rsa package files that we've installed in the previous step.
These files are located in the / usr / share / easy-rsa / 3 folder on the CA Server.
Create the symlinks with the ln command:
To restrict access to your new PKI directory, ensure that only the owner can access it using the chmod command:
Finally, initialize the PKI inside the easy-rsa directory:
After completing this section you have a directory that contains all the files that are needed to create a Certificate Authority.
In the next section you will create the private key and public certificate for your CA.
Step 3 - Creating a Certificate Authority
Before you can create your CA "s private key and certificate, you need to create and populate a file called vars with some default values.
First you will cd into the easy-rsa directory, then you will create and edit the vars file with nano or your preferred text editor.
When you are prompted to install nano enter y to continue with the installation steps.
Now you are ready to edit the vars file:
Once the file is opened, paste in the following lines and edit each highlighted value to reflect your own organization info.
The important part here is to ensure that you do not leave any of the values blank:
If you are using nano, you can do so by pressing CTRL + X, then Y and ENTER to confirm.
You are now ready to build your CA.
To create the root public and private key pair for your Certificate Authority, run the. / easy-rsa command again, this time with the build-ca option:
In the output, you "ll see some lines about the OpenSSL version and you will be prompted to enter a passphrase for your key pair.
Be sure to choose a strong passphrase, and note it down somewhere safe.
You will need to input the passphrase any time that you need to interact with your CA, for example to sign or revoke a certificate.
You will also be asked to confirm the Common Name (CN) for your CA.
The CN is the name used to refer to this machine in the context of the Certificate Authority.
You can enter any string of characters for the CA's Common Name but for simplicity's sake, press ENTER to accept the default name.
< $> note Note: If you don "t want to be prompted for a password every time you interact with your CA, you can run the build-ca command with the nopass option, like this:
You now have two important files - ~ / easy-rsa / pki / ca.crt and ~ / easy-rsa / pki / private / ca.key - which make up the public and private components of a Certificate Authority.
ca.crt is the CA's public certificate file.
Users, servers, and clients will use this certificate to verify that they are part of the same web of trust.
Every user and server that uses your CA will need to have a copy of this file.
All parties will rely on the public certificate to ensure that someone is not impersonating a system and performing a Man-in-the-middle attack.
ca.key is the private key that the CA uses to sign certificates for servers and clients.
If an attacker gains access to your CA and, in turn, your ca.key file, you will need to destroy your CA.
With that, your CA is in place and it is ready to be used to sign certificate requests, and to revoke certificates.
Step 4 - Distributing your Certificate Authority "s Public Certificate
Now your CA is configured and ready to act as a root of trust for any systems that you want to configure to use it. You can add the CA "s certificate to your OpenVPN servers, web servers, mail servers, and so on.
Any user or server that needs to verify the identity of another user or server in your network should have a copy of the ca.crt file imported into their operating system's certificate store.
To import the CA's public certificate into a second Linux system like another server or a local computer, first obtain a copy of the ca.crt file from your CA server.
You can use the cat command to output it in a terminal, and then copy and paste it into a file on the second computer that is importing the certificate.
You can also use tools like scp, rsync to transfer the file between systems.
However we "ll use copy and paste with nano in this step since it will work on all systems.
As your non-root user on the CA Server, run the following command:
There will be output in your terminal that is similar to the following:
Copy everything, including the -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- lines and the dashes.
On your second Linux system use nano or your preferred text editor to open a file called / tmp / ca.crt:
Paste the contents that you just copied from the CA Server into the editor.
Now that you have a copy of the ca.crt file on your second Linux system, it is time to import the certificate into its operating system certificate store.
On CentOS, Fedora, or other RedHat derived Linux systems run the following commands to import the certificate:
To import the CA Server "s certificate on a Debian or Ubuntu based system, copy and paste the file contents onto the system just like in the previous example in a file called / tmp / ca.crt.
Next, copy the certificate into / usr / local / share / ca-certificates /, then run the update-ca-certificates command.
Now your second Linux system will trust any certificate that has been signed by the CA server.
< $> note Note: If you are using your CA with web servers and use Firefox as a browser you will need to import the public ca.crt certificate into Firefox directly.
Firefox does not use the local operating system "s certificate store.
For details on how to add your CA "s certificate to Firefox please see this support article from Mozilla on Setting Up Certificate Authorities (CAs) in Firefox.
If you are using your CA to integrate with a Windows environment or desktop computers, please see the documentation on how to use certutil.exe to install a CA certificate.
If you are using this tutorial as a prerequisite for another tutorial, or are familiar with how to sign and revoke certificates you can stop here.
If you would like to learn more about how to sign and revoke certificates, then the following optional section will explain each process in detail.
(Optional) - Creating Certificate Signing Requests and Revoking Certificates
The following sections of the tutorial are optional.
If you have completed all the previous steps then you have a fully configured and working Certificate Authority that you can use as a prerequisite for other tutorials.
You can import your CA "s ca.crt file and verify certificates in your network that have been signed by your CA.
If you would like to practice and learn more about how to sign certificate requests, and how to revoke certificates, then these optional sections will explain how both processes work.
(Optional) - Creating and Signing a Practice Certificate Request
Now that you have a CA ready to use, you can practice generating a private key and certificate request to get familiar with the signing and distribution process.
A Certificate Signing Request (CSR) consists of three parts: a public key, identifying information about the requesting system, and a signature of the request itself, which is created using the requesting party "s private key.
The private key will be kept secret, and will be used to encrypt information that anyone with the signed public certificate can then decrypt.
The following steps will be run on your second Linux system running CentOS, Fedora, or another RedHat derived Linux distribution.
It can be another remote server, or a local Linux machine like a laptop or a desktop computer.
Since easy-rsa is not available by default on all systems, we'll use the openssl tool to create a practice private key and certificate.
openssl is usually installed by default on most Linux distributions, but just to be certain, run the following on your system:
When you are prompted to install openssl enter y to continue with the installation steps.
Now you are ready to create a practice CSR with openssl.
The first step that you need to complete to create a CSR is generating a private key.
To create a private key using openssl, create a practice-csr directory and then generate a key inside it. We will make this request for a fictional server called sammy-server, as opposed to creating a certificate that is used to identify a user or another CA.
Now that you have a private key you can create a corresponding CSR, again using the openssl utility.
You will be prompted to fill out a number of fields like Country, State, and City.
You can enter a. if you'd like to leave a field blank, but be aware that if this were a real CSR, it is best to use the correct values for your location and organization:
If you would like to automatically add those values as part of the openssl invocation instead of via the interactive prompt, you can pass the -subj argument to OpenSSL.
Be sure to edit the highlighted values to match your practice location, organization, and server name:
To verify the contents of a CSR, you can read in a request file with openssl and examine the fields inside:
Once you're happy with the subject of your practice certificate request, copy the sammy-server.req file to your CA server using scp:
In this step you generated a Certificate Signing Request for a fictional server called sammy-server.
In a real-world scenario, the request could be from something like a staging or development web server that needs a TLS certificate for testing; or it could come from an OpenVPN server that is requesting a certificate so that users can connect to a VPN.
In the next step, we'll proceed to signing the certificate signing request using the CA Server "s private key.
(Optional) - Signing a CSR
In the previous step, you created a practice certificate request and key for a fictional server.
You copied it to the / tmp directory on your CA server, emulating the process that you would use if you had real clients or servers sending you CSR requests that need to be signed.
Continuing with the fictional scenario, now the CA Server needs to import the practice certificate and sign it. Once a certificate request is validated by the CA and relayed back to a server, clients that trust the Certificate Authority will also be able to trust the newly issued certificate.
Since we will be operating inside the CA's PKI where the easy-rsa utility is available, the signing steps will use the easy-rsa utility to make things easier, as opposed to using the openssl directly like we did in the previous example.
The first step to sign the fictional CSR is to import the certificate request using the easy-rsa script:
Now you can sign the request by running the easyrsa script with the sign-req option, followed by the request type and the Common Name that is included in the CSR.
The request type can either be one of client, server, or ca. Since we're practicing with a certificate for a fictional server, be sure to use the server request type:
In the output, you'll be asked to verify that the request comes from a trusted source.
If you encrypted your CA key, you'll be prompted for your password at this point.
With those steps complete, you have signed the sammy-server.req CSR using the CA Server's private key in / home / sammy / easy-rsa / pki / private / ca.key.
The resulting sammy-server.crt file contains the practice server's public encryption key, as well as a new signature from the CA Server.
The point of the signature is to tell anyone who trusts the CA that they can also trust the sammy-server certificate.
If this request was for a real server like a web server or VPN server, the last step on the CA Server would be to distribute the new sammy-server.crt and ca.crt files from the CA Server to the remote server that made the CSR request:
At this point, you would be able to use the issued certificate with something like a web server, a VPN, configuration management tool, database system, or for client authentication purposes.
(Optional) - Revoking a Certificate
Occasionally, you may need to revoke a certificate to prevent a user or server from using it. Perhaps someone's laptop was stolen, a web server was compromised, or an employee or contractor has left your organization.
To revoke a certificate, the general process follows these steps:
Revoke the certificate with the. / easyrsa revoke < ^ > client _ name < ^ > command.
Generate a new CRL with the. / easyrsa gen-crl command.
Transfer the updated crl.pem file to the server or servers that rely on your CA, and on those systems copy it to the required directory or directories for programs that refer to it.
Restart any services that use your CA and the CRL file.
You can use this process to revoke any certificates that you've previously issued at any time.
We "ll go over each step in detail in the following sections, starting with the revoke command.
Revoking a Certificate
To revoke a certificate, navigate to the easy-rsa directory on your CA server:
Next, run the easyrsa script with the revoke option, followed by the client name you wish to revoke.
Following the practice example above, the Common Name of the certificate is sammy-server:
Note the highlighted value on the Revoking Certificate line.
This value is the unique serial number of the certificate that is being revoked.
If you want to examine the revocation list in the last step of this section to verify that the certificate is in it, you "ll need this value.
After confirming the action, the CA will revoke the certificate.
However, remote systems that rely on the CA have no way to check whether any certificates have been revoked.
Users and servers will still be able to use the certificate until the CA's Certificate Revocation List (CRL) is distributed to all systems that rely on the CA.
In the next step you "ll generate a CRL or update an existing crl.pem file.
Generating a Certificate Revocation List
Now that you have revoked a certificate, it is important to update the list of revoked certificates on your CA server.
Once you have an updated revocation list you will be able to tell which users and systems have valid certificates in your CA.
To generate a CRL, run the easy-rsa command with the gen-crl option while still inside the ~ / easy-rsa directory:
If you have used a passphrase when creating your ca.key file, you will be prompted to enter it. The gen-crl command will generate a file called crl.pem, containing the updated list of revoked certificates for that CA.
Next you'll need to transfer the updated crl.pem file to all servers and clients that rely on this CA each time you run the gen-crl command.
Otherwise, clients and systems will still be able to access services and systems that use your CA, since those services need to know about the revoked status of the certificate.
Transferring a Certificate Revocation List
Now that you have generated a CRL on your CA server, you need to transfer it to remote systems that rely on your CA.
To transfer this file to your servers, you can use the scp command.
< $> note Note: This tutorial explains how to generate and distribute a CRL manually.
While there are more robust and automated methods to distribute and check revocation lists like OCSP-Stapling, configuring those methods is beyond the scope of this article.
Ensure you are logged into your CA server as your non-root user and run the following, substituting in your own server IP or DNS name in place of your _ server _ ip:
Now that the file is on the remote system, the last step is to update any services with the new copy of the revocation list.
Updating Services that Support a CRL
Listing the steps that you need to use to update services that use the crl.pem file is beyond the scope of this tutorial.
In general you will need to copy the crl.pem file into the location that the service expects and then restart it using systemctl.
Once you have updated your services with the new crl.pem file, your services will be able to reject connections from clients or servers that are using a revoked certificate.
Examining and Verifying the Contents of a CRL
If you would like to examine a CRL file, for example to confirm a list of revoked certificates, use the following openssl command from within your easy-rsa directory on your CA server:
You can also run this command on any server or system that has the openssl tool installed with a copy of the crl.pem file.
For example, if you transferred the crl.pem file to your second system and want to verify that the sammy-server certificate is revoked, you can use an openssl command like the following, substituting the serial number that you noted earlier when you revoked the certificate in place of the highlighted one here:
Notice how the grep command is used to check for the unique serial number that you noted in the revocation step.
Now you can verify the contents of your Certificate Revocation List on any system that relies on it to restrict access to users and services.
In this tutorial you created a private Certificate Authority using the Easy-RSA package on a standalone CentOS 8 server.
You learned how the trust model works between parties that rely on the CA.
You also created and signed a Certificate Signing Request (CSR) for a practice server and then learned how to revoke a certificate.
Finally, you learned how to generate and distribute a Certificate Revocation List (CRL) for any system that relies on your CA to ensure that users or servers that should not access services are prevented from doing so.
Now you can issue certificates for users and use them with services like OpenVPN.
You can also use your CA to configure development and staging web servers with certificates to secure your non-production environments.
Using a CA with TLS certificates during development can help ensure that your code and environments match your production environment as closely as possible.
If you would like to learn more about how to use OpenSSL, our OpenSSL Essentials: Working with SSL Certificates, Private Keys and CSRs tutorial has lots of additional information to help you become more familiar with OpenSSL fundamentals.
How To Set Up and Configure a Certificate Authority (CA) On Debian 10
4017
In this guide, we'll learn how to set up a private Certificate Authority on a Debian 10 server, and how to generate and sign a testing certificate using your new CA.
To complete this tutorial, you will need access to a Debian 10 server to host your OpenVPN service.
You can follow our Debian 10 initial server setup guide to set up a user with appropriate permissions.
If you choose to complete those practice steps, you will need a second Debian 10 server or you can also use your own local Linux computer running Debian or Ubuntu, or distributions derived from either of those.
These files are located in the / usr / share / easy-rsa folder on the CA Server.
First you will cd into the easy-rsa directory, then you will create and edit the vars file with nano or your preferred text editor:
Now your CA is configured and ready to act as a root of trust for any systems that you want to configure to use it. You can add the CA's certificate to your OpenVPN servers, web servers, mail servers, and so on.
On Debian and Ubuntu based systems, run the following commands to import the certificate:
To import the CA Server "s certificate on CentOS, Fedora, or RedHat based system, copy and paste the file contents onto the system just like in the previous example in a file called / tmp / ca.crt.
Next, you'll copy the certificate into / etc / pki / ca-trust / source / anchors /, then run the update-ca-trust command.
The following steps will be run on your second Linux system Debian, Ubuntu, or distribution that is derived from either of those.
In this tutorial you created a private Certificate Authority using the Easy-RSA package on a standalone Debian 10 server.
Understanding Default Parameters in JavaScript
4004
In ECMAScript 2015, default function parameters were introduced to the JavaScript language.
These allow developers to initialize a function with default values if the arguments are not supplied to the function call.
Initializing function parameters in this way will make your functions easier to read and less error-prone, and will provide default behavior for your functions.
This will help you avoid errors that stem from passing in undefined arguments and destructuring objects that don't exist.
In this article, you will review the difference between parameters and arguments, learn how to use default parameters in functions, see alternate ways to support default parameters, and learn what types of values and expressions can be used as default parameters.
You will also run through examples that demonstrate how default parameters work in JavaScript.
Arguments and Parameters
Before explaining default function parameters, it is important to know what it is that parameters can default to.
Because of this, we will first review the difference between arguments and parameters in a function.
If you would like to learn more about this distinction, check out our earlier article in the JavaScript series, How to Define Functions in JavaScript.
In the following code block, you will create a function that returns the cube of a given number, defined as x:
The x variable in this example is a parameter - a named variable passed into a function.
A parameter must always be contained in a variable and must never have a direct value.
Now take a look at this next code block, which calls the cube function you just created:
In this case, 10 is an argument - a value passed to a function when it is invoked.
Often the value will be contained in a variable as well, such as in this next example:
This will yield the same result:
If you do not pass an argument to a function that expects one, the function will implicitly use undefined as the value:
This will return:
In this case, cube () is trying to calculate the value of undefined * undefined * undefined, which results in NaN, or "not a number".
For more on this, take a look at the number section of Understanding Data Types in JavaScript.
This automatic behavior can sometimes be a problem.
In some cases, you might want the parameter to have a value even if no argument was passed to the function.
That's where the default parameters feature comes in handy, a topic that you will cover in the next section.
Default Parameter Syntax
With the addition of default parameters in ES2015, you can now assign a default value to any parameter, which the function will use instead of undefined when called without an argument.
This section will first show you how to do this manually, and then will guide you through setting default parameters.
Without default parameters, you would have to explicitly check for undefined values in order to set defaults, as is shown in this example:
This uses a conditional statement to check if the value has been automatically provided as undefined, then sets the value of x as 5. This will result in the following output:
In contrast, using default parameters accomplishes the same goal in much less code.
You can set a default value to the parameter in cube by assigning it with the equality assignment operator (=), as highlighted here:
Now when the cube function is invoked without an argument, it will assign 5 to x and return the calculation instead of NaN:
It will still function as intended when an argument is passed, ignoring the default value:
However, one important caveat to note is that the default parameter value will also override an explicit undefined passed as an argument to a function, as demonstrated here:
This will give the calculation with x equal to 5:
In this case, the default parameter values were calculated, and an explicit undefined value did not override them.
Now that you have an idea of the basic syntax of default parameters, the next section will show how default parameters work with different data types.
Default Parameter Data Types
Any primitive value or object can be used as a default parameter value.
In this section, you will see how this flexibility increases the ways in which default parameters can be used.
First, set parameters using a number, string, boolean, object, array, and null value as a default value.
This example will use arrow function syntax:
When these functions are invoked without parameters, they will all use the default values:
Note that any object created in a default parameter will be created every time the function is called.
One of the common use cases for default parameters is to use this behavior to obtain values out of an object.
If you try to destructure or access a value from an object that doesn't exist, it will throw an error.
However, if the default parameter is an empty object, it will simply give you undefined values instead of throwing an error:
This will avoid the error caused by destructuring objects that don't exist.
Now that you've seen how default parameters operate with different data types, the next section will explain how multiple default parameters can work together.
Using Multiple Default Parameters
You can use as many default parameters as you want in a function.
This section will show you how to do this, and how to use it to manipulate the DOM in a real-world example.
First, declare a sum () function with multiple default parameters:
This will result in the following default calculation:
Additionally, the value used in a parameter can be used in any subsequent default parameter, from left to right.
For example, this createUser function creates a user object userObj as the third parameter, and all the function itself does is return userObj with the first two parameters:
If you call user here, you will get the following:
It is usually recommended to put all default parameters at the end of a list of parameters, so that you can easily leave off optional values.
If you use a default parameter first, you will have to explicitly pass undefined to use the default value.
Here is an example with the default parameter at the beginning of the list:
When calling this function, you would have to call defaultFirst () with two arguments:
This would give the following:
Here is an example with the default parameter at the end of the list:
This would yield the same value:
Both functions have the same result, but the one with the default value last allows a much cleaner function call.
For a real-world example, here is a function that will create a DOM element, and add a text label and classes, if they exist.
You can call the function with some classes in an array:
Calling greeting will give the following value:
However, if you leave the classNames array out of the function call, it will still work.
greeting2 now has the following value:
In this example, forEach () can be used on an empty array without an issue.
If that empty array were not set in the default parameter, you would get the following error:
Now that you have seen how multiple default parameters can interact, you can move on to the next section to see how function calls work as default parameters.
Function Calls as Default Parameters
In addition to primitives and objects, the result of calling a function can be used as a default parameter.
In this code block, you will create a function to return a random number, and then use the result as the default parameter value in a cube function:
Now invoking the cube function without a parameter will have potentially different results every time you call it:
The output from these function calls will vary:
You can even use built-in methods, like those on the Math object, and use the value returned in one function call as a parameter in another function.
In the following example, a random number is assigned to x, which is used as the parameter in the cube function you created.
The y parameter will then calculate the cube root of the number and check to see if x and y are equal:
This will give the following:
A default parameter can even be a function definition, as seen in this example, which defines a parameter as the inner function and returns the function call of parameter:
This inner function will be created from scratch every time the outer function is invoked.
In this article, you learned what default function parameters are and how to use them.
Now you can use default parameters to help keep your functions clean and easy to read.
You can also assign empty objects and arrays to parameters upfront to reduce both complexity and lines of code when dealing with situations such as retrieving values from an object or looping through an array.
fcgid is a high performance alternative to mod _ cgi that starts a sufficient number of instances of the CGI program to handle concurrent requests.
In this file you updated the DocumentRoot to your new directory and ServerAdmin to an email that the < ^ > your _ domain < ^ > site administrator can access.
How To Install PostgreSQL on Ubuntu 20.04 Quickstart
5246
To install PostgreSQL, first refresh your server's local package index:
These are, in some ways, similar to regular Unix-style users and groups.
One way is to switch over to the postgres account on your server by typing:
Then you can access the Postgres prompt by typing:
This will log you into the PostgreSQL prompt, and from here you are free to interact with the database management system right away.
To exit out of the PostgreSQL prompt, run the following:
You can also run the command you'd like to run with the postgres account directly with sudo:
If you are logged in as the postgres account, you can create a new role by typing:
Either way, the script will prompt you with some choices and, based on your responses, execute the correct Postgres commands to create a user to your specifications.
How To Install Python 3 and Set Up a Programming Environment on Ubuntu 20.04 Quickstart
5301
Python is a flexible and versatile programming language, with strengths in scripting, automation, data analysis, machine learning, and back-end development.
This tutorial will walk you through installing Python and setting up a programming environment on an Ubuntu 20.04 server.
For a more detailed version of this tutorial, with more thorough explanations of each step, please refer to How To Install Python 3 and Set Up a Programming Environment on an Ubuntu 20.04 Server.
Step 1 - Update and Upgrade
Logged into your Ubuntu 20.04 server as a sudo non-root user, first update and upgrade your system to ensure that your shipped version of Python 3 is up-to-date.
Confirm installation if prompted to do so.
Step 2 - Check Version of Python
Check which version of Python 3 is installed by typing:
You "ll receive output similar to the following, depending on when you have updated your system.
Step 3 - Install pip
To manage software packages for Python, install pip, a tool that will help you manage libraries or modules to use in your projects.
Step 4 - Install Additional Tools
There are a few more packages and development tools to install to ensure that we have a robust set-up for our programming environment:
Step 5 - Install venv
Virtual environments enable you to have an isolated space on your server for Python projects.
We "ll use venv, part of the standard Python 3 library, which we can install by typing:
Step 6 - Create a Virtual Environment
You can create a new environment with the pyvenv command.
Here, we "ll call our new environment < ^ > my _ env < ^ >, but you should call yours something meaningful to your project.
Step 7 - Activate Virtual Environment
Activate the environment using the command below, where < ^ > my _ env < ^ > is the name of your programming environment.
Your command prompt will now be prefixed with the name of your environment:
Step 8 - Test Virtual Environment
Open the Python interpreter:
Note that within the Python 3 virtual environment, you can use the command python instead of python3, and pip instead of pip3.
You "ll know you" re in the interpreter when you receive the following output:
Now, use the print () function to create the traditional Hello, World program:
Step 9 - Deactivate Virtual Environment
Quit the Python interpreter:
Then exit the virtual environment:
Further Reading
From here, there is a lot you can learn about Python, here are some links related to this guide:
Free How To Code in Python 3 eBook
Python Tutorials
Initial Server Setup with Ubuntu 20.04
5248
When you first create a new Ubuntu 20.04 server, you should perform some important configuration steps as part of the basic setup.
These steps will increase the security and usability of your server, and will give you a solid foundation for subsequent actions.
Step 1 - Logging in as root
You will also need the password or - if you installed an SSH key for authentication - the private key for the root user's account.
If you have not already logged into your server, you may want to follow our guide on how to connect to Droplets with SSH, which covers this process in detail.
If you are not already connected to your server, log in now as the root user using the following command (substitute the highlighted portion of the command with your server's public IP address):
About root
The root user is the administrative user in a Linux environment that has very broad privileges.
The next step is setting up a new user account with reduced privileges for day-to-day use.
Later, we'll teach you how to gain increased privileges during only the times when you need them.
Once you are logged in as root, we're prepared to add the new user account.
In the future, we'll log in with this new account instead of root.
This example creates a new user called sammy, but you should replace that with a username that you like:
You will be asked a few questions, starting with the account password.
Enter a strong password and, optionally, fill in any of the additional information if you would like.
This is not required and you can just hit ENTER in any field you wish to skip.
To avoid having to log out of our normal user and log back in as the root account, we can set up what is known as superuser or root privileges for our normal account.
To add these privileges to our new user, we need to add the user to the sudo group.
By default, on Ubuntu 20.04, users who are members of the sudo group are allowed to use the sudo command.
As root, run this command to add your new user to the sudo group (substitute the highlighted username with your new user):
Ubuntu 20.04 servers can use the UFW firewall to make sure only connections to certain services are allowed.
We can set up a basic firewall very easily using this application.
Applications can register their profiles with UFW upon installation.
These profiles allow UFW to manage these applications by name.
OpenSSH, the service allowing us to connect to our server now, has a profile registered with UFW.
You can see this by typing:
We need to make sure that the firewall allows SSH connections so that we can log back in next time.
We can allow these connections by typing:
Afterwards, we can enable the firewall by typing:
Type y and press ENTER to proceed.
You can see that SSH connections are still allowed by typing:
As the firewall is currently blocking all connections except for SSH, if you install and configure additional services, you will need to adjust the firewall settings to allow traffic in.
You can learn some common UFW operations in our UFW Essentials guide.
Now that we have a regular user for daily use, we need to make sure we can SSH into the account directly.
If the root Account Uses Password Authentication
Follow our guide on setting up SSH keys on Ubuntu 20.04 to learn how to configure key-based authentication.
You will need to add a copy of your local public key to the new user's ~ / .ssh / authorized _ keys file to log in successfully.
Since your public key is already in the root account's ~ / .ssh / authorized _ keys file on the server, we can copy that file and directory structure to our new user account in our existing session.
Now, open up a new terminal session on you local machine, and use SSH with your new username:
Where To Go From Here?
How To Install and Secure Redis on Ubuntu 20.04 Quickstart
5420
Redis is an in-memory key-value store known for its flexibility, performance, and wide language support.
This quickstart tutorial demonstrates how to install, configure, and secure Redis on an Ubuntu 20.04 server.
To complete this guide, you will need access to an Ubuntu 20.04 server that has a non-root user with sudo privileges and a firewall configured with ufw.
You can set this up by following our Initial Server Setup guide for Ubuntu 20.04.
Step 1 - Installing and Configuring Redis
Begin by updating your local apt package cache:
Then install Redis by typing:
Next, open up the Redis configuration file with your preferred text editor:
Inside the file, find the supervised directive which allows you to declare an init system to manage Redis as a service.
Since you are running Ubuntu, which uses the systemd init system, change its value from no to systemd:
If you used nano to edit the file, do so by pressing CTRL + X, Y, then ENTER.
Then, restart the Redis service to reflect the changes you made to the configuration file:
To test that Redis is functioning correctly, connect to the server using redis-cli, Redis's command-line client:
In the prompt that follows, test connectivity with the ping command:
This output confirms that the server connection is active.
Next, check that you "re able to set keys by running:
Retrieve the value by typing:
Assuming everything is working, you will be able to retrieve the value you stored:
After confirming that you can fetch the value, exit the Redis prompt to get back to the shell:
Step 2 - Configuring a Redis Password
You can configure a Redis password directly in Redis's configuration file, / etc / redis / redis.conf.
Open that file again with your preferred editor:
Scroll to the SECURITY section and look for a commented directive that reads:
Uncomment it by removing the #, and change foobared to a secure password:
After setting the password, save and close the file, then restart Redis:
To test that the password works, open up the Redis client:
The following shows a sequence of commands used to test whether the Redis password works.
The first command tries to set a key to a value before authentication:
That won't work because you didn "t authenticate, so Redis returns an error:
The next command authenticates with the password specified in the Redis configuration file:
Redis acknowledges:
After that, running the previous command again will succeed:
get key1 queries Redis for the value of the new key.
After confirming that you "re able to run commands in the Redis client after authenticating, you can exit redis-cli:
Step 3 - Renaming Dangerous Commands
The other security feature built into Redis involves renaming or completely disabling certain commands that are considered dangerous.
Some of the commands that are considered dangerous include: FLUSHDB, FLUSHALL, KEYS, PEXPIRE, DEL, CONFIG, SHUTDOWN, BGREWRITEAOF, BGSAVE, SAVE, SPOP, SREM, RENAME, and DEBUG.
By disabling or renaming these and other commands, you make it more difficult for unauthorized users to reconfigure, destroy, or otherwise wipe your data.
To rename or disable Redis commands, open the configuration file once more:
< $> warning Warning: The following steps showing how to disable and rename commands are examples.
You should only choose to disable or rename the commands that make sense for you.
You can review the full list of commands for yourself and determine how they might be misused at redis.io / commands.
To disable a command, simply rename it to an empty string (signified by a pair of quotation marks with no characters between them), as shown below:
To rename a command, give it another name as shown in the examples below.
Renamed commands should be difficult for others to guess, but easy for you to remember:
Save your changes and close the file.
After renaming a command, apply the change by restarting Redis:
To test the new command, enter the Redis command line:
Then authenticate:
Assuming that you renamed the CONFIG command to ASC12 _ CONFIG as in the preceding example, try using the original CONFIG command.
It should fail, because you "ve renamed it:
Calling the renamed command, however, will be successful.
It is not case-sensitive:
In this quickstart tutorial, you installed and configured Redis, validated that your Redis installation is functioning correctly, and used its built-in security features to make it less vulnerable to attacks from malicious actors.
How To Install and Secure Redis on Ubuntu 20.04
5418
This tutorial demonstrates how to install, configure, and secure Redis on an Ubuntu 20.04 server.
We'll use the APT package manager to install redis from the official Ubuntu repositories.
As of this writing, the version available in the default repositories is < ^ > 5.0.7 < ^ >.
This will download and install Redis and its dependencies.
Following this, there is one important configuration change to make in the Redis configuration file, which was generated automatically during the installation.
Open this file with your preferred text editor:
Inside the file, find the supervised directive.
This directive allows you to declare an init system to manage Redis as a service, providing you with more control over its operation.
The supervised directive is set to no by default.
Since you are running Ubuntu, which uses the systemd init system, change this to systemd:
That "s the only change you need to make to the Redis configuration file at this point, so save and close it when you are finished.
With that, you "ve installed and configured Redis and it" s running on your machine.
Before you begin using it, though, it "s prudent to first check whether Redis is functioning correctly.
Step 2 - Testing Redis
As with any newly-installed software, it "s a good idea to ensure that Redis is functioning as expected before making any further changes to its configuration.
We will go over a handful of ways to check that Redis is working correctly in this step.
Start by checking that the Redis service is running:
If it is running without any errors, this command will produce output similar to the following:
Here, you can see that Redis is running and is already enabled, meaning that it is set to start up every time the server boots.
< $> note Note: This setting is desirable for many common use cases of Redis.
If, however, you prefer to start up Redis manually every time your server boots, you can configure this with the following command:
This output confirms that the server connection is still alive.
As a final test, we will check whether Redis is able to persist data even after it "s been stopped or restarted.
To do this, first restart the Redis instance:
Then connect with the command-line client again:
And confirm that your test value is still available
The value of your key should still be accessible:
Exit out into the shell again when you are finished:
With that, your Redis installation is fully operational and ready for you to use.
However, some of its default configuration settings are insecure and provide malicious actors with opportunities to attack and gain access to your server and its data. The remaining steps in this tutorial cover methods for mitigating these vulnerabilities, as prescribed by the official Redis website.
Although these steps are optional and Redis will still function if you choose not to follow them, it is strongly recommended that you complete them in order to harden your system "s security.
Step 3 - Binding to localhost
By default, Redis is only accessible from localhost.
However, if you installed and configured Redis by following a different tutorial than this one, you might have updated the configuration file to allow connections from anywhere.
This is not as secure as binding to localhost.
To correct this, open the Redis configuration file for editing:
Locate this line and make sure it is uncommented (remove the # if it exists):
Save and close the file when finished (press CTRL + X, Y, then ENTER).
Then, restart the service to ensure that systemd reads your changes:
To check that this change has gone into effect, run the following netstat command:
< $> note Note: The netstat command may not be available on your system by default.
If this is the case, you can install it (along with a number of other handy networking tools) with the following command:
This output shows that the redis-server program is bound to localhost (127.0.0.1), reflecting the change you just made to the configuration file.
If you see another IP address in that column (0.0.0.0, for example), then you should double check that you uncommented the correct line and restart the Redis service again.
Now that your Redis installation is only listening in on localhost, it will be more difficult for malicious actors to make requests or gain access to your server.
However, Redis isn "t currently set to require users to authenticate themselves before making changes to its configuration or the data it holds.
To remedy this, Redis allows you to require users to authenticate with a password before making changes via the Redis client (redis-cli).
Step 4 - Configuring a Redis Password
Configuring a Redis password enables one of its two built-in security features - the auth command, which requires clients to authenticate to access the database.
The password is configured directly in Redis's configuration file, / etc / redis / redis.conf, so open that file again with your preferred editor:
Uncomment it by removing the #, and change foobared to a secure password.
< $> note Note: Above the requirepass directive in the redis.conf file, there is a commented warning:
Thus, it "s important that you specify a very strong and very long value as your password.
Rather than make up a password yourself, you can use the openssl command to generate a random one, as in the following example.
By piping the output of the first command to the second openssl command, as shown here, it will remove any line breaks produced by that the first command:
Your output should look something like:
After copying and pasting the output of that command as the new value for requirepass, it should read:
Next, we'll look at renaming Redis commands which, if entered by mistake or by a malicious actor, could cause serious damage to your machine.
Step 5 - Renaming Dangerous Commands
When run by unauthorized users, such commands can be used to reconfigure, destroy, or otherwise wipe your data. Like the authentication password, renaming or disabling commands is configured in the same SECURITY section of the / etc / redis / redis.conf file.
This is not a comprehensive list, but renaming or disabling all of the commands in that list is a good starting point for enhancing your Redis server "s security.
Whether you should disable or rename a command depends on your specific needs or those of your site.
If you know you will never use a command that could be abused, then you may disable it. Otherwise, it might be in your best interest to rename it.
Then, authenticate:
Let "s assume that you renamed the CONFIG command to ASC12 _ CONFIG, as in the preceding example.
First, try using the original CONFIG command.
Finally, you can exit from redis-cli:
Note that if you're already using the Redis command line and then restart Redis, you'll need to re-authenticate.
Otherwise, you'll get this error if you type a command:
< $> warning Regarding the practice of renaming commands, there's a cautionary statement at the end of the SECURITY section in / etc / redis / redis.conf which reads:
Note: The Redis project chooses to use the terms "master" and "slave," while DigitalOcean generally prefers the alternatives "primary" and "secondary."
In order to avoid confusion we "ve chosen to use the terms used in the Redis documentation here.
That means if the renamed command is not in the AOF file, or if it is but the AOF file has not been transmitted to slaves, then there should be no problem.
So, keep that in mind when you're trying to rename commands.
The best time to rename a command is when you're not using AOF persistence, or right after installation, that is, before your Redis-using application has been deployed.
When you're using AOF and dealing with a master-slave installation, consider this answer from the project's GitHub issue page.
The following is a reply to the author's question:
The commands are logged to the AOF and replicated to the slave the same way they are sent, so if you try to replay the AOF on an instance that doesn't have the same renaming, you may face inconsistencies as the command cannot be executed (same for slaves).
Thus, the best way to handle renaming in cases like that is to make sure that renamed commands are applied to all instances in master-slave installations.
In this tutorial, you installed and configured Redis, validated that your Redis installation is functioning correctly, and used its built-in security features to make it less vulnerable to attacks from malicious actors.
Keep in mind that once someone is logged in to your server, it's very easy to circumvent the Redis-specific security features we've put in place.
Therefore, the most important security feature on your Redis server is your firewall (which you configured if you followed the prerequisite Initial Server Setup tutorial), as this makes it extremely difficult for malicious actors to jump that fence.
How To Secure Apache with Let's Encrypt on Ubuntu 20.04
5386
Let's Encrypt is a Certificate Authority (CA) that facilitates obtaining and installing free TLS / SSL certificates, thereby enabling encrypted HTTPS on web servers.
It simplifies the process by providing a software client, Certbot, that attempts to automate most (if not all) of the required steps.
Currently, the entire process of obtaining and installing a certificate is fully automated on both Apache and Nginx.
In this guide, we'll use Certbot to obtain a free SSL certificate for Apache on Ubuntu 20.04, and make sure this certificate is set up to renew automatically.
This tutorial uses a separate virtual host file instead of Apache's default configuration file for setting up the website that will be secured by Let's Encrypt.
We recommend creating new Apache virtual host files for each domain hosted in a server, because it helps to avoid common mistakes and maintains the default configuration files as a fallback setup.
One Ubuntu 20.04 server set up by following this initial server setup for Ubuntu 20.04 tutorial, including a sudo non-root user and a firewall.
This tutorial will use your _ domain as an example throughout.
Apache installed by following How To Install Apache on Ubuntu 20.04.
Be sure that you have a virtual host file for your domain.
This tutorial will use / etc / apache2 / sites-available / < ^ > your _ domain < ^ > .conf as an example.
Step 1 - Installing Certbot
In order to obtain an SSL certificate with Let's Encrypt, we'll first need to install the Certbot software on your server.
We'll use the default Ubuntu package repositories for that.
We need two packages: certbot, and python3-certbot-apache.
The latter is a plugin that integrates Certbot with Apache, making it possible to automate obtaining a certificate and configuring HTTPS within your web server with a single command.
You will be prompted to confirm the installation by pressing Y, then ENTER.
Certbot is now installed on your server.
In the next step, we'll verify Apache's configuration to make sure your virtual host is set appropriately.
This will ensure that the certbot client script will be able to detect your domains and reconfigure your web server to use your newly generated SSL certificate automatically.
Step 2 - Checking your Apache Virtual Host Configuration
In order to be able to automatically obtain and configure SSL for your web server, Certbot needs to find the correct virtual host within your Apache configuration files.
Your server domain name (s) will be retrieved from the ServerName and ServerAlias directives defined within your VirtualHost configuration block.
If you followed the virtual host setup step in the Apache installation tutorial, you should have a VirtualHost block set up for your domain at / etc / apache2 / sites-available / < ^ > your _ domain < ^ > .conf with the ServerName and also the ServerAlias directives already set appropriately.
To check this up, open the virtual host file for your domain using nano or your preferred text editor:
Find the existing ServerName and ServerAlias lines.
They should look like this:
If you already have your ServerName and ServerAlias set up like this, you can exit your text editor and move on to the next step.
If you're using nano, you can exit by typing CTRL + X, then Y and ENTER to confirm.
If your current virtual host configuration doesn't match the example, update it accordingly.
When you're done, save the file and quit the editor.
Then, run the following command to validate your changes:
You should get a Syntax OK as a response.
If you get an error, reopen the virtual host file and check for any typos or missing characters.
Once your configuration file's syntax is correct, reload Apache so that the changes take effect:
With these changes, Certbot will be able to find the correct VirtualHost block and update it.
Next, we'll update the firewall to allow HTTPS traffic.
Step 3 - Allowing HTTPS Through the Firewall
If you have the UFW firewall enabled, as recommended by the prerequisite guides, you'll need to adjust the settings to allow HTTPS traffic.
Upon installation, Apache registers a few different UFW application profiles.
We can leverage the Apache Full profile to allow both HTTP and HTTPS traffic on your server.
To verify what kind of traffic is currently allowed on your server, you can use:
If you have followed one of our Apache installation guides, your output should look something like this, meaning that only HTTP traffic on port 80 is currently allowed:
To additionally let in HTTPS traffic, allow the "Apache Full" profile and delete the redundant "Apache" profile:
Your status will now look like this:
You are now ready to run Certbot and obtain your certificates.
Step 4 - Obtaining an SSL Certificate
Certbot provides a variety of ways to obtain SSL certificates through plugins.
The Apache plugin will take care of reconfiguring Apache and reloading the configuration whenever necessary.
To use this plugin, type the following:
This script will prompt you to answer a series of questions in order to configure your SSL certificate.
First, it will ask you for a valid e-mail address.
This email will be used for renewal notifications and security notices:
After providing a valid e-mail address, hit ENTER to proceed to the next step.
You will then be prompted to confirm if you agree to Let's Encrypt terms of service.
You can confirm by pressing A and then ENTER:
Next, you'll be asked if you would like to share your email with the Electronic Frontier Foundation to receive news and other information.
If you do not want to subscribe to their content, type N. Otherwise, type Y. Then, hit ENTER to proceed to the next step.
The next step will prompt you to inform Certbot of which domains you'd like to activate HTTPS for.
The listed domain names are automatically obtained from your Apache virtual host configuration, that's why it's important to make sure you have the correct ServerName and ServerAlias settings configured in your virtual host.
If you'd like to enable HTTPS for all listed domain names (recommended), you can leave the prompt blank and hit ENTER to proceed.
Otherwise, select the domains you want to enable HTTPS for by listing each appropriate number, separated by commas and / or spaces, then hit ENTER.
Next, you'll be prompted to select whether or not you want HTTP traffic redirected to HTTPS.
In practice, that means when someone visits your website through unencrypted channels (HTTP), they will be automatically redirected to the HTTPS address of your website.
Choose 2 to enable the redirection, or 1 if you want to keep both HTTP and HTTPS as separate methods of accessing your website.
After this step, Certbot's configuration is finished, and you will be presented with the final remarks about your new certificate, where to locate the generated files, and how to test your configuration using an external tool that analyzes your certificate's authenticity:
Your certificate is now installed and loaded into Apache's configuration.
Try reloading your website using https: / / and notice your browser's security indicator.
It should point out that your site is properly secured, typically by including a lock icon in the address bar.
You can use the SSL Labs Server Test to verify your certificate's grade and obtain detailed information about it, from the perspective of an external service.
In the next and final step, we'll test the auto-renewal feature of Certbot, which guarantees that your certificate will be renewed automatically before the expiration date.
Step 5 - Verifying Certbot Auto-Renewal
Let's Encrypt's certificates are only valid for ninety days.
This is to encourage users to automate their certificate renewal process, as well as to ensure that misused certificates or stolen keys will expire sooner rather than later.
The certbot package we installed takes care of renewals by including a renew script to / etc / cron.d, which is managed by a systemctl service called certbot.timer.
This script runs twice a day and will automatically renew any certificate that's within thirty days of expiration.
To check the status of this service and make sure it's active and running, you can use:
You'll get output similar to this:
To test the renewal process, you can do a dry run with certbot:
If you see no errors, you're all set. When necessary, Certbot will renew your certificates and reload Apache to pick up the changes.
If the automated renewal process ever fails, Let "s Encrypt will send a message to the email you specified, warning you when your certificate is about to expire.
In this tutorial, you've installed the Let's Encrypt client certbot, configured and installed an SSL certificate for your domain, and confirmed that Certbot's automatic renewal service is active within systemctl.
If you have further questions about using Certbot, their documentation is a good place to start.
How To Remove Docker Images, Containers, and Volumes
2109
A Docker Cheat Sheet
Docker makes it easy to wrap your applications and services in containers so you can run them anywhere.
As you work with Docker, however, it's also easy to accumulate an excessive number of unused images, containers, and data volumes that clutter the output and consume disk space.
Docker gives you all the tools you need to clean up your system from the command line.
This cheat sheet-style guide provides a quick reference to commands that are useful for freeing disk space and keeping your system organized by removing unused Docker images, containers, and volumes.
How to Use This Guide:
This guide is in cheat sheet format with self-contained command-line snippets
Jump to any section that is relevant to the task you are trying to complete.
The command substitution syntax, < ^ > command < ^ > $(< ^ > command < ^ >), used in the commands is available in many popular shells such as bash, zsh, and Windows Powershell.
Purging All Unused or Dangling Images, Containers, Volumes, and Networks
Docker provides a single command that will clean up any resources - images, containers, volumes, and networks - that are dangling (not associated with a container):
To additionally remove any stopped containers and all unused images (not just dangling images), add the -a flag to the command:
Removing Docker Images
Remove one or more specific images
Use the docker images command with the -a flag to locate the ID of the images you want to remove.
This will show you every image, including intermediate image layers.
When you've located the images you want to delete, you can pass their ID or tag to docker rmi:
List:
Remove:
Remove dangling images
Docker images consist of multiple layers.
Dangling images are layers that have no relationship to any tagged images.
They no longer serve a purpose and consume disk space.
They can be located by adding the filter flag, -f with a value of dangling = true to the docker images command.
When you're sure you want to delete them, you can use the docker images purge command:
< $> note Note: If you build an image without tagging it, the image will appear on the list of dangling images because it has no association with a tagged image.
You can avoid this situation by providing a tag when you build, and you can retroactively tag an images with the docker tag command.
Removing images according to a pattern
You can find all the images that match a pattern using a combination of docker images and grep.
Once you're satisfied, you can delete them by using awk to pass the IDs to docker rmi.
Note that these utilities are not supplied by Docker and are not necessarily available on all systems:
Remove all images
All the Docker images on a system can be listed by adding -a to the docker images command.
Once you're sure you want to delete them all, you can add the -q flag to pass the Image ID to docker rmi:
Removing Containers
Remove one or more specific containers
Use the docker ps command with the -a flag to locate the name or ID of the containers you want to remove:
Remove a container upon exit
If you know when you "re creating a container that you won" t want to keep it around once you "re done, you can run docker run --rm to automatically delete it when it exits.
Run and Remove:
Remove all exited containers
You can locate containers using docker ps -a and filter them by their status: created, restarting, running, paused, or exited.
To review the list of exited containers, use the -f flag to filter based on status.
When you've verified you want to remove those containers, using -q to pass the IDs to the docker rm command.
Remove containers using more than one filter
Docker filters can be combined by repeating the filter flag with an additional value.
This results in a list of containers that meet either condition.
For example, if you want to delete all containers marked as either Created (a state which can result when you run a container with an invalid command) or Exited, you can use two filters:
Remove containers according to a pattern
You can find all the containers that match a pattern using a combination of docker ps and grep.
When you're satisfied that you have the list you want to delete, you can use awk and xargs to supply the ID to docker rmi.
Note that these utilities are not supplied by Docker and not necessarily available on all systems:
Stop and remove all containers
You can review the containers on your system with docker ps.
Adding the -a flag will show all containers.
When you're sure you want to delete them, you can add the -q flag to supply the IDs to the docker stop and docker rm commands:
Removing Volumes
Remove one or more specific volumes - Docker 1.9 and later
Use the docker volume ls command to locate the volume name or names you wish to delete.
Then you can remove one or more volumes with the docker volume rm command:
Remove dangling volumes - Docker 1.9 and later
Since the point of volumes is to exist independent from containers, when a container is removed, a volume is not automatically removed at the same time.
When a volume exists and is no longer connected to any containers, it's called a dangling volume.
To locate them to confirm you want to remove them, you can use the docker volume ls command with a filter to limit the results to dangling volumes.
When you're satisfied with the list, you can remove them all with docker volume prune:
Remove a container and its volume
If you created an unnamed volume, it can be deleted at the same time as the container with the -v flag.
Note that this only works with unnamed volumes.
When the container is successfully removed, its ID is displayed.
Note that no reference is made to the removal of the volume.
If it is unnamed, it is silently removed from the system.
If it is named, it silently stays present.
This guide covers some of the common commands used to remove images, containers, and volumes with Docker.
There are many other combinations and flags that can be used with each.
For a comprehensive guide to what's available, see the Docker documentation for docker system prune, docker rmi, docker rm and docker volume rm.
If there are common cleanup tasks you'd like to see in the guide, please ask or make suggestions in the comments.
How To Install Linux, Apache, MySQL, PHP (LAMP) stack on Ubuntu 20.04 Quickstart
5476
In this quickstart guide, we'll install a LAMP stack on an Ubuntu 20.04 server.
For a more detailed version of this tutorial, with more explanations of each step, please refer to How To Install Linux, Apache, MySQL, PHP (LAMP) Stack on Ubuntu 20.04
To follow this guide, you'll need access to an Ubuntu 20.04 server as a sudo user.
Step 1 - Install Apache
Update your package manager cache and then install Apache with:
Once the installation is finished, you'll need to adjust your firewall settings to allow HTTP traffic on your server.
Run the following command to allow external access on port 80 (HTTP):
Step 2 - Install MySQL
We'll now install MySQL, a popular database management system used within PHP environments.
Your server will next ask you to select and confirm a password for the MySQL root user.
Please refer to step 6 of our detailed LAMP on Ubuntu 20.04 guide to learn how to do that.
Step 3 - Install PHP
To install PHP and its dependencies, run:
Step 4 - Create a Virtual Host for your Website
Then, open a new configuration file in Apache's sites-available directory using your preferred command-line editor:
If you're using nano, you can do that by pressing CTRL + X, then Y and ENTER.
Step 5 - Test PHP with Apache
We'll now create a PHP test script to confirm that Apache is able to handle and process requests for PHP files.
Add the following content inside the file:
Go to your web browser and access your server's domain name or IP address, followed by the script name, which in this case is info.php:
Initial Server Setup on Ubuntu 20.04
Managing DNS records on DigitalOcean
How to Secure Apache with Let's Encrypt on Ubuntu 20.04
How To Build a Neural Network to Translate Sign Language into English
5505
The author selected Code Org to receive a donation as part of the Write for DOnations program.
Computer vision is a subfield of computer science that aims to extract a higher-order understanding from images and videos.
This powers technologies such as fun video chat filters, your mobile device "s face authenticator, and self-driving cars.
In this tutorial, you'll use computer vision to build an American Sign Language translator for your webcam.
As you work through the tutorial, you "ll use OpenCV, a computer-vision library, PyTorch to build a deep neural network, and onnx to export your neural network.
You "ll also apply the following concepts as you build a computer-vision application:
You'll use the same three-step method as used in How To Apply Computer Vision to Build an Emotion-Based Dog Filter tutorial: preprocess a dataset, train a model, and evaluate the model.
You'll also expand each of these steps: employ data augmentation to address rotated or non-centered hands, change learning rate schedules to improve model accuracy, and export models for faster inference speed.
Along the way, you'll also explore related concepts in machine learning.
By the end of this tutorial, you'll have both an American Sign Language translator and foundational deep learning know-how.
You can also access the complete source code for this project.
A local development environment for Python 3 with at least 1GB of RAM.
You can follow How to Install and Set Up a Local Programming Environment for Python 3 to configure everything you need.
A working webcam to do real-time image detection.
(Recommended) Build an Emotion-Based Dog Filter; this tutorial is not explicitly used but the same ideas are reinforced and built upon.
Step 1 - Creating the Project and Installing Dependencies
Let's create a workspace for this project and install the dependencies we'll need.
On Linux distributions, start by preparing your system package manager and install the Python3 virtualenv package.
Use:
We'll call our workspace SignLanguage:
Navigate to the SignLanguage directory:
Then create a new virtual environment for the project:
Activate your environment:
Then install PyTorch, a deep-learning framework for Python that we'll use in this tutorial.
On macOS, install Pytorch with the following command:
On Linux and Windows, use the following commands for a CPU-only build:
Now install prepackaged binaries for OpenCV, numpy, and onnx, which are libraries for computer vision, linear algebra, AI model exporting, and AI model execution, respectively.
OpenCV offers utilities such as image rotations, and numpy offers linear algebra utilities such as a matrix inversion:
On Linux distributions, you will need to install libSM.so:
With the dependencies installed, let's build the first version of our sign language translator: a sign language classifier.
Step 2 - Preparing the Sign Language Classification Dataset
In these next three sections, you'll build a sign language classifier using a neural network.
Your goal is to produce a model that accepts a picture of a hand as input and outputs a letter.
The following three steps are required to build a machine learning classification model:
Preprocess the data: Apply one-hot encoding to your labels and wrap your data in PyTorch Tensors.
Train your model on augmented data to prepare it for "unusual" input, like an off-center or rotated hand.
Specify and train the model: Set up a neural network using PyTorch.
Define training hyper-parameters - such as how long to train for - and run stochastic gradient descent.
You'll also vary a specific training hyper-parameter, which is learning rate schedule.
These will boost model accuracy.
Run a prediction using the model: Evaluate the neural network on your validation data to understand its accuracy.
Then, export the model to a format called ONNX for faster inference speeds.
In this section of the tutorial, you will accomplish step 1 of 3. You will download the data, create a Dataset object to iterate over your data, and finally apply data augmentation.
At the end of this step, you will have a programmatic way of accessing images and labels in your dataset to feed to your model.
First, download the dataset to your current working directory:
< $> note Note: On macOS, wget is not available by default.
To do so, install Homebrew by following this DigitalOcean tutorial.
Then, run brew install wget.
Unzip the zip file, which contains a data / directory:
Create a new file, named step _ 2 _ dataset.py:
As before, import the necessary utilities and create the class that will hold your data. For data processing here, you will create the train and test datasets.
You'll implement PyTorch's Dataset interface, allowing you to load and use PyTorch's built-in data pipeline for your sign language classification dataset:
Delete the pass placeholder in the SignLanguageMNIST class.
In its place, add a method to generate a label mapping:
Labels range from 0 to 25. However, letters J (9) and Z (25) are excluded.
This means there are only 24 valid label values.
So that the set of all label values starting from 0 is contiguous, we map all labels to 0, 23. This mapping from dataset labels 0, 23 to letter indices 0, 25 is provided by this get _ label _ mapping method.
Next, add a method to extract labels and samples from a CSV file.
The following assumes that each line starts with the label and is then followed by 784 pixel values.
These 784 pixel values represent a 28x28 image:
For an explanation of how these 784 values represent an image, see Build an Emotion-Based Dog Filter, Step 4.
Note that each line in the csv.reader iterable is a list of strings; the int and map (int,...) invocations cast all strings to integers.
Directly beneath our static method, add a function that will initialize our data holder:
This function starts by loading the samples and labels.
Then it wraps the data in NumPy arrays.
The mean and standard deviation information will be explained shortly, in the _ _ getitem _ _ section following.
Directly after the _ _ init _ _ function, add a _ _ len _ _ function.
The Dataset requires this method to determine when to stop iterating over data:
Finally, add a _ _ getitem _ _ method, which returns a dictionary containing the sample and the label:
You use a technique called data augmentation, where samples are perturbed during training, to increase the model's robustness to these perturbations.
In particular, randomly zoom in on the image by varying amounts and on different locations, via RandomResizedCrop.
Note that zooming in should not affect the final sign language class; thus, the label is not transformed.
You additionally normalize the inputs so that image values are rescaled to the 0, 1 range in expectation, instead of 0, 255; to accomplish this, use the dataset _ mean and _ std when normalizing.
Your completed SignLanguageMNIST class will look like the following:
As before, you will now verify our dataset utility functions by loading the SignLanguageMNIST dataset.
Add the following code to the end of your file after the SignLanguageMNIST class:
This code initializes the dataset using the SignLanguageMNIST class.
Then for the train and validation sets, it wraps the dataset in a DataLoader.
This will translate the dataset into an iterable to use later.
Now you'll verify that the dataset utilities are functioning.
Create a sample dataset loader using DataLoader and print the first element of that loader.
Add the following to the end of your file:
You can check that your file matches the step _ 2 _ dataset file in this (repository).
Exit your editor and run the script with the following:
This outputs the following pair of tensors.
Our data pipeline outputs two samples and two labels.
This indicates that our data pipeline is up and ready to go:
You've now verified that your data pipeline works.
This concludes the first step - preprocessing your data - which now includes data augmentation for increased model robustness.
Next you will define the neural network and optimizer.
Step 3 - Building and Training the Sign Language Classifier Using Deep Learning
With a functioning data pipeline, you will now define a model and train it on the data. In particular, you will build a neural network with six layers, define a loss, an optimizer, and finally, optimize the loss function for your neural network predictions.
At the end of this step, you will have a working sign language classifier.
Create a new file called step _ 3 _ train.py:
Import the necessary utilities:
Define a PyTorch neural network that includes three convolutional layers, followed by three fully connected layers.
Add this to the end of your existing script:
Now initialize the neural network, define a loss function, and define optimization hyperparameters by adding the following code to the end of the script:
Finally, you'll train for two epochs:
You define an epoch to be an iteration of training where every training sample has been used exactly once.
At the end of the main function, the model parameters will be saved to a file called "checkpoint.pth".
Add the following code to the end of your script to extract image and label from the dataset loader and then wrap each in a PyTorch Variable:
This code will also run the forward pass and then backpropagate through the loss and neural network.
At the end of your file, add the following to invoke the main function:
Double-check that your file matches the following:
Save and exit.
Then, launch our proof-of-concept training by running:
You'll see output akin to the following as the neural network trains:
To obtain lower loss, you could increase the number of epochs to 5, 10, or even 20. However, after a certain period of training time, the network loss will cease to decrease with increased training time.
To sidestep this issue, as training time increases, you will introduce a learning rate schedule, which decreases learning rate over time.
To understand why this works, see Distill's visualization at "Why Momentum Really Works".
Amend your main function with the following two lines, defining a scheduler and invoking scheduler.step.
Furthermore, change the number of epochs to 12:
Check that your file matches the step 3 file in this repository.
Training will run for around 5 minutes.
Your output will resemble the following:
The final loss obtained is 0.007608, which is 3 orders of magnitude smaller than the starting loss 3.20.
This concludes the second step of our workflow, where we set up and train the neural network.
With that said, as small as this loss value is, it has little meaning.
To put the model's performance in perspective, we will compute its accuracy - the percentage of images the model correctly classified.
Step 4 - Evaluating the Sign Language Classifier
You will now evaluate your sign language classifier by computing its accuracy on the validation set, a set of images the model did not see during training.
This will provide a better sense of model performance than the final loss value did.
Furthermore, you will add utilities to save our trained model at the end of training and load our pre-trained model when performing inference.
Create a new file, called step _ 4 _ evaluate.py.
Next, define a utility to evaluate the neural network's performance.
The following function compares the neural network's predicted letter to the true letter, for a single image:
outputs is a list of class probabilities for each sample.
For example, outputs for a single sample may be [0.1, 0.3, 0.4, 0.2]. labels is a list of label classes.
For example, the label class may be 3.
Y =... converts the labels into a NumPy array.
Next, Yhat = np.argmax (...) converts the outputs class probabilities into predicted classes.
For example, the list of class probabilities [0.1, 0.3, 0.4, 0.2] would yield the predicted class 2, because the index 2 value of 0.4 is the largest value.
Since both Y and Yhat are now classes, you can compare them.
Yhat = = Y checks if the predicted class matches the label class, and np.sum (...) is a trick that computes the number of truth-y values.
In other words, np.sum will output the number of samples that were classified correctly.
Add the second function batch _ evaluate, which applies the first function evaluate to all images:
batch is a group of images stored as a single tensor.
First, you increment the total number of images you're evaluating (n) by the number of images in this batch.
Next, you run inference on the neural network with this batch of images, outputs = net (...).
The type check if isinstance (...) converts the outputs in a NumPy array if needed.
Finally, you use evaluate to compute the number of correctly-classified samples.
At the conclusion of the function, you compute the percent of samples you correctly classified, score / n.
Finally, add the following script to leverage the preceding utilities:
This loads a pretrained neural network and evaluates its performance on the provided sign language dataset.
Specifically, the script here outputs accuracy on the images you used for training and a separate set of images you put aside for testing purposes, called the validation set.
You will next export the PyTorch model to an ONNX binary.
This binary file can then be used in production to run inference with your model.
Most importantly, the code running this binary does not need a copy of the original network definition.
At the end of the validate function, add the following:
This exports the ONNX model, checks the exported model, and then runs inference with the exported model.
Double-check that your file matches the step 4 file in this repository:
To use and evaluate the checkpoint from the last step, run the following:
This will yield output similar to the following, affirming that your exported model not only works, but also agrees with your original PyTorch model:
Your neural network attains a train accuracy of 99.9% and a 97.4% validation accuracy.
This gap between train and validation accuracy indicates your model is overfitting.
This means that instead of learning generalizable patterns, your model has memorized the training data. To understand the implications and causes of overfitting, see Understanding Bias-Variance Tradeoffs.
At this point, we have completed a sign language classifier.
In essence, our model can correctly disambiguate between signs correctly almost all the time.
This is a reasonably good model, so we move on to the final stage of our application.
We will use this sign language classifier in a real-time webcam application.
Step 5 - Linking the Camera Feed
Your next objective is to link the computer's camera to your sign language classifier.
You will collect camera input, classify the displayed sign language, and then report the classified sign back to the user.
Now create a Python script for the face detector.
Create the file step _ 6 _ camera.py using nano or your favorite text editor:
Add the following code into the file:
This code imports OpenCV, which contains your image utilities, and the ONNX runtime, which is all you need to run inference with your model.
The rest of the code is typical Python program boilerplate.
Now replace pass in the main function with the following code, which initializes a sign language classifier using the parameters you trained previously.
Additionally add a mapping from indices to letters and image statistics:
You will use elements of this test script from the official OpenCV documentation.
Specifically, you will update the body of the main function.
Start by initializing a VideoCapture object that is set to capture live feed from your computer's camera.
Place this at the end of the main function:
Then add a while loop, which reads from the camera at every timestep:
Write a utility function that takes the center crop for the camera frame.
Place this function before main:
Next, take the center crop for the camera frame, convert to grayscale, normalize, and resize to 28x28.
Place this inside the while loop within the main function:
Still within the while loop, run inference with the ONNX runtime.
Convert the outputs to a class index, then to a letter:
Display the predicted letter inside the frame, and display the frame back to the user:
At the end of the while loop, add this code to check if the user hits the q character and, if so, quit the application.
This line halts the program for 1 millisecond.
Add the following:
Finally, release the capture and close all windows.
Place this outside of the while loop to end the main function.
Double-check your file matches the following or this repository:
Exit your file and run the script.
Once the script is run, a window will pop up with your live webcam feed.
The predicted sign language letter will be shown in the top left.
Hold up your hand and make your favorite sign to see your classifier in action.
Here are some sample results showing the letter L and D.
Screenshot of your sample OpenCV program, for sign language 'L'.
Sreenshot of your sample OpenCV program, for sign language 'D'
While testing, note that the background needs to be fairly clear for this translator to work.
This is an unfortunate consequence of the dataset's cleanliness.
Had the dataset included images of hand signs with miscellaneous backgrounds, the network would be robust to noisy backgrounds.
However, the dataset features blank backgrounds and nicely centered hands.
As a result, this webcam translator works best when your hand is likewise centered and placed against a blank background.
This concludes the sign language translator application.
In this tutorial, you built an American Sign Language translator using computer vision and a machine learning model.
In particular, you saw new aspects of training a machine learning model - specifically, data augmentation for model robustness, learning rate schedules for lower loss, and exporting AI models using ONNX for production use.
This then culminated in a real-time computer vision application, which translates sign language into letters using a pipeline you built.
It's worth noting that combatting the brittleness of the final classifier can be tackled with any or all of the following methods.
For further exploration try the following topics to in improve your application:
Generalization: This isn't a sub-topic within computer vision, rather, it's a constant problem throughout all of machine learning.
See Understanding Bias-Variance Tradeoffs.
Domain Adaptation: Say your model is trained in domain A (for example, sunny environments).
Can you adapt the model to domain B (for example, cloudy environments) quickly?
Adversarial Examples: Say an adversary is designing images intentionally to fool your model.
How can you design such images?
How can you combat such images?
How To Configure Apache HTTP with MPM Event and PHP-FPM on Ubuntu 18.04
5607
The Apache HTTP web server has evolved through the years to work in different environments and solve different needs.
One important problem Apache HTTP has to solve, like any web server, is how to handle different processes to serve an http protocol request.
This involves opening a socket, processing the request, keeping the connection open for a certain period, handling new events occurring through that connection, and returning the content produced by a program made in a particular language (such as PHP, Perl, or Python).
These tasks are performed and controlled by a Multi-Processing Module (MPM).
Apache HTTP comes with three different MPM:
Pre-fork: A new process is created for each incoming connection reaching the server.
Each process is isolated from the others, so no memory is shared between them, even if they are performing identical calls at some point in their execution.
This is a safe way to run applications linked to libraries that do not support threading - typically older applications or libraries.
Worker: A parent process is responsible for launching a pool of child processes, some of which are listening for new incoming connections, and others are serving the requested content.
Each process is threaded (a single thread can handle one connection) so one process can handle several requests concurrently.
This method of treating connections encourages better resource utilization, while still maintaining stability.
This is a result of the pool of available processes, which often has free available threads ready to immediately serve new connections.
Event: Based on worker, this MPM goes one step further by optimizing how the parent process schedules tasks to the child processes and the threads associated to those.
A connection stays open for 5 seconds by default and closes if no new event happens; this is the keep-alive directive default value, which retains the thread associated to it. The Event MPM enables the process to manage threads so that some threads are free to handle new incoming connections while others are kept bound to the live connections.
Allowing re-distribution of assigned tasks to threads will make for better resource utilization and performance.
The MPM Event module is a fast multi-processing module available on the Apache HTTP web server.
PHP-FPM is the FastCGI Process Manager for PHP.
The FastCGI protocol is based on the Common Gateway Interface (CGI), a protocol that sits between applications and web servers like Apache HTTP.
This allows developers to write applications separately from the behavior of web servers.
Programs run their processes independently and pass their product to the web server through this protocol.
Each new connection in need of processing by an application will create a new process.
By combining the MPM Event in Apache HTTP with the PHP FastCGI Process Manager (PHP-FPM) a website can load faster and handle more concurrent connections while using fewer resources.
In this tutorial you will improve the performance of the LAMP stack by changing the default multi-processing module from pre-fork to event and by using the PHP-FPM process manager to handle PHP code instead of the classic mod _ php in Apache HTTP.
The LAMP stack installed on your server following How To Install Linux, Apache, MySQL, PHP (LAMP stack) on Ubuntu 18.04.
Step 1 - Changing the Multi-Processing Module
Ubuntu inherits scripts to enable or disable Apache HTTP modules from its parent distribution, Debian.
You'll use this toolset in this step to disable the Pre-fork module and enable the Event module.
In this step you will stop Apache HTTP, disable the < ^ > PHP 7.2 < ^ > module linked to the Pre-fork module, and then disable Pre-fork to immediately enable the Event module.
First you'll stop the Apache HTTP service:
Now you can disable the < ^ > PHP 7.2 < ^ > module, which is related to the Pre-fork module:
Then disable the Pre-fork MPM module:
Now enable the Event MPM module:
You've switched the MPM from pre-fork to event and removed the < ^ > PHP 7.2 < ^ > module connection between PHP and Apache HTTP.
In the next step you'll install the php-fpm module, as well as the related libraries and proxy modules.
You'll configure Apache HTTP so that it can communicate with PHP too.
Step 2 - Configuring Apache HTTP to Use the FastCGI Process Manager
At this stage you've switched the way Apache HTTP processes connections by moving from the Pre-fork MPM to Event.
However along the way you've disabled the PHP module that connected Apache HTTP with any program running on PHP.
In this step you'll install the PHP-FPM processor so Apache HTTP is again able to process PHP programs.
And you'll also install the dependency libraries and enable the modules so both can cooperate smoothly and quicker than before.
First install php-fpm.
The following command will install the PHP-FPM package and it will automatically enable the < ^ > php7.2-fpm < ^ > service integrated with systemd, so the service is started at boot time:
In order to communicate, Apache HTTP and PHP need a library enabling that capacity.
You'll now install libapache2-mod-fcgid, which is able to serve as an interface between programs with web servers, and it's specific to Apache HTTP.
This communication will happen through a UNIX socket.
Install this library:
You've installed php-fpm and the libapache2-mod-fcgid, but neither are enabled yet.
First enable the php-fpm module with the following command:
Second enable Apache HTTP proxy module:
Third enable the FastCGI proxy module in Apache HTTP:
< $> note Note: You can read the configuration of this interaction between PHP programs and Apache HTTP through a UNIX socket with the following:
Everything is now in place so you can start Apache HTTP.
You'll make a configuration check first:
After that you can proceed to restart Apache HTTP, since it was automatically started when installing the FastCGI library libapache2-mod-fcgid:
You've installed the php-fpm module, configured Apache HTTP to work with it, enabled the necessary modules for the FastCGI protocol to work, and started the corresponding services.
Now that Apache has the Event MPM module enabled and PHP-FPM is present and running, it is time to check everything is working as intended.
Step 3 - Checking Your Configuration
In order to check that the configuration changes have been applied you'll run some tests.
The first one will check what multi-processing module Apache HTTP is using.
The second will verify that PHP is using the FPM manager.
Check the Apache HTTP server by running the following command:
Your output will be as follows:
You can repeat the same for the proxy module and FastCGI:
If you would like to see the entire list of the modules, you can remove the the second part of the command after -M.
It is now time to check if PHP is using the FastCGI Process Manager.
To do so you'll write a small PHP script that will show you all the information related to PHP.
Run the following command to write a file named as follows:
Add the following content into the info.php file:
Now visit your server's URL and append info.php at the end like so: http: / / < ^ > your _ domain < ^ > / info.php.
The server API entry will be FPM / FastCGI.
PHP Screen the Server API entry FPM / FastCGI
Delete the info.php file after this check so no information about the server is publicly disclosed:
You've checked the working status of the MPM module, the modules handling the FastCGI and the handling of PHP code.
You've optimized your original LAMP stack, so the number of connections to create new Apache HTTP processes has increased, PHP-FPM will handle PHP code more efficiently, and overall resource utilization has improved.
See the Apache HTTP server project documentation for more information on the different modules and related projects.
How To Install the Apache Web Server on CentOS 8 Quickstart
5509
For a more detailed version of this tutorial, please refer to How To Install the Apache Web Server on CentOS 8.
Step 2 -- Adjusting the Firewall
Create the html directory for
example.com < ^ > as follows, using the -p flag to create any necessary parent directories:
How To Install and Configure Postfix on Ubuntu 20.04
5685
Postfix is a popular open-source Mail Transfer Agent (MTA) that can be used to route and deliver email on a Linux system.
It is estimated that around 25% of public mail servers on the internet run Postfix.
In this guide, you'll learn how to install and configure Postfix on an Ubuntu 20.04 server.
Then, you'll test that Postfix is able to correctly route mail by installing s-nail, a Mail User Agent (MUA), also known as an email client.
Note that the goal of this tutorial is to help you get Postfix up and running quickly with only some bare-bones email functionality.
You won't have a full featured email server by the end of this guide, but you will have some of the foundational components of such a setup to help you get started.
In order to follow this guide, you'll need the following:
A server running Ubuntu 20.04 to function as your Postfix mail server.
This server should have a non-root user with sudo privileges and a firewall configured with UFW.
You can follow our Ubuntu 20.04 initial server setup guide to set this up.
A Fully Qualified Domain Name pointed at your Ubuntu 20.04 server.
You can find help on setting up your domain name with DigitalOcean by following our Domains and DNS Networking documentation.
Be aware that if you plan on accessing mail from an external location, you will need to make sure you have an MX record pointing to your mail server as well.
Note that this tutorial assumes that you are configuring a host that has the FQDN of mail.example.com.
Wherever necessary, be sure to change example.com or mail.example.com to reflect your own FQDN.
Postfix is included in Ubuntu's default repositories, so you can install it with APT.
To begin, update your local apt package cache:
Then install the postfix package with the following command.
Note that here we pass the DEBIAN _ PRIORITY = low environmental variable into this installation command.
This will cause the installation process to prompt you to configure some additional options:
This installation process will open a series of interactive prompts.
For the purposes of this tutorial, use the following information to fill in your prompts:
General type of mail configuration?: For this, choose Internet Site since this matches our infrastructure needs.
System mail name: This is the base domain used to construct a valid email address when only the account portion of the address is given.
For instance, let's say the hostname of your server is mail. < ^ > example.com < ^ >.
You will likely want to set the system mail name to < ^ > example.com < ^ > so that, given the username user1, Postfix will use the address user1 @ < ^ > example.com < ^ >.
Root and postmaster mail recipient: This is the Linux account that will be forwarded mail addressed to root @ and postmaster @.
Use your primary account for this.
In this example case, sammy.
Other destinations to accept mail for: This defines the mail destinations that this Postfix instance will accept.
If you need to add any other domains that this server will be responsible for receiving, add those here.
Otherwise, the default will be sufficient.
Force synchronous updates on mail queue?: Since you are likely using a journaled filesystem, accept No here.
Local networks: This is a list of the networks for which your mail server is configured to relay messages.
The default will work for most scenarios.
If you choose to modify it, though, make sure to be very restrictive in regards to the network range.
Mailbox size limit: This can be used to limit the size of messages.
Setting it to 0 disables any size restriction.
Local address extension character: This is the character that can be used to separate the regular portion of the address from an extension (used to create dynamic aliases).
The default, + will work for this tutorial.
Internet protocols to use: Choose whether to restrict the IP version that Postfix supports.
For the purposes of this tutorial, pick all.
To be explicit, these are the settings used in this guide:
General type of mail configuration?: Internet Site
System mail name: < ^ > example.com < ^ > (not < ^ > mail.example.com < ^ >)
Root and postmaster mail recipient: The username of your primary Linux account (sammy in our examples)
Other destinations to accept mail for: $myhostname, < ^ > example.com < ^ >, < ^ > mail.example.com < ^ >, < ^ > localhost.example.com < ^ >, localhost
Force synchronous updates on mail queue?: No
Local networks: 127.0.0.0 / 8 [:: ffff: 127.0.0.0] / 104 [:: 1] / 128
Mailbox size limit: 0
Local address extension character: +
Internet protocols to use: all
< $> note Note: If you need to ever return to change these settings, you can do so by typing:
The prompts will be pre-populated with your previous responses.
When the installation process finishes, you're ready to make a few updates to your Postfix configuration.
Step 2 - Changing the Postfix Configuration
Now you can adjust some settings that the package installation process didn't prompt you for.
Many of Postfix's configuration settings are defined in the / etc / postfix / main.cf file.
Rather than editing this file directly, you can use Postfix's postconf command to query or set configuration settings.
To begin, set the location for your non-root Ubuntu user's mailbox.
In this guide, we'll use the Maildir format, which separates messages into individual files that are then moved between directories based on user action.
The alternative option that isn't covered in this guide is the mbox format, which stores all messages within a single file.
Set the home _ mailbox variable to Maildir /.
Later, you will create a directory structure under that name within your user's home directory.
Configure home _ mailbox by typing:
Next, set the location of the virtual _ alias _ maps table, which maps arbitrary email accounts to Linux system accounts.
Run the following command, which maps the table location to a hash database file named / etc / postfix / virtual:
Now that you've defined the location of the virtual maps file in your main.cf file, you can create the file itself and begin mapping email accounts to user accounts on your Linux system.
Create the file with your preferred text editor; in this example, we'll use nano:
List any addresses that you wish to accept email for, followed by a whitespace and the Linux user you'd like that mail delivered to.
For example, if you would like to accept email at contact @ < ^ > example.com < ^ > and admin @ < ^ > example.com < ^ > and would like to have those emails delivered to the sammy Linux user, you could set up your file like this:
After you've mapped all of the addresses to the appropriate server accounts, save and close the file.
If you used nano, do this by pressing CTRL + X, Y, then ENTER.
Apply the mapping by typing:
Restart the Postfix process to be sure that all of your changes have been applied:
Assuming you followed the prerequisite Initial Server Setup guide, you will have configured a firewall with UFW.
This firewall will block external connections to services on your server by default unless those connections are explicitly allowed, so you'll have to add a firewall rule to allow an exception for Postfix.
You can allow connections to the service by typing:
With that, Postfix is configured and ready to accept external connections.
However, you aren't yet ready to test it out with a mail client.
Before you can install a client and use it to interact with the mail being delivered to your server, you'll need to make a few changes to your Ubuntu server's setup.
Step 3 - Installing the Mail Client and Initializing the Maildir Structure
In order to interact with the mail being delivered, this step will walk you through the process of installing the s-nail package.
This is a feature-rich variant of the BSD xmail client which can handle the Maildir format correctly.
Before installing the client, though, it would be prudent to make sure your MAIL environment variable is set correctly. s-nail will look for this variable to figure out where to find mail for your user.
To ensure that the MAIL variable is set regardless of how you access your account - whether through ssh, su, su -, or sudo, for example - you'll need to set the variable in the / etc / bash.bashrc file and add it to a file within / etc / profile.d to make sure it is set for all users by default.
To add the variable to these files, type:
To read the variable into your current session, source the / etc / profile.d / mail.sh file:
With that complete, install the s-nail email client with APT:
Before running the client, there are a few settings you need to adjust.
Open the / etc / s-nail.rc file in your editor:
At the bottom of the file, add the following options:
Here's what these lines do:
set emptystart: allows the client to open even with an empty inbox
set folder = Maildir: sets the Maildir directory to the internal folder variable
set record = + sent creates a sent mbox file for storing sent mail within whichever directory is set as the folder variable, in this case Maildir
You're now ready to initialize your system's Maildir structure.
A quick way to create the Maildir structure within your home directory is to send yourself an email with the s-nail command.
Because the sent file will only be available once the Maildir is created, you should disable writing to it for this initial email.
Do this by passing the -Snorecord option.
Send the email by piping a string to the s-nail command.
Adjust the command to mark your Linux user as the recipient:
< $> note Note: You may get the following response:
This is normal and may only appear when sending this first message.
You can can check to make sure the directory was created by looking for your ~ / Maildir directory:
You will see the directory structure has been created and that a new message file is in the ~ / Maildir / new directory:
Now that the directory structure has been created, you're ready to test out the s-nail client by viewing the init message you sent and sending a message to an external email address.
Step 5 - Testing the Client
To open the client, run the s-nail command:
In your console, you'll see a rudimentary inbox with the init message waiting:
Press ENTER to display the message:
You can get back to the message list by typing h, and then ENTER:
Notice that the message now has a state of R, indicating that it's been read.
Since this message isn't very useful, you can delete it by pressing d, and then ENTER:
To get back to the terminal, type q and then ENTER:
As a final test, check whether s-nail is able to correctly send email messages.
To do this, you can pipe the contents of a text file into the s-nail process, like you did with the init message you sent in the previous step.
Begin by writing a test message in a text editor:
Inside, enter some text you'd like to send:
Save and close the file after writing your message.
Then, use the cat command to pipe the message to the s-nail process.
You can do so with the following example, which uses these options:
-s: This defines the subject line of the email message
-r: An optional change to the "From:" field of the email.
By default, the Linux user you are logged in as will be used to populate this field.
The -r option allows you to override this with a valid address, such as one of those you defined in the / etc / postfix / virtual file.
To illustrate, the following command uses contact @ example.com
Also, be sure to change < ^ > user < ^ > @ < ^ > email.com < ^ > to a valid email address which you have access to:
Then, navigate to the inbox for the email address to which you sent the message.
You will see your message waiting there almost immediately.
< $> note Note: If the message isn't in your inbox, it may have been delivered to your Spam folder.
You can view your sent messages within your s-nail client.
Start the interactive client again:
From the email client, view your sent messages by typing:
You can manage sent mail using the same commands you use for incoming mail.
You now have Postfix configured on your Ubuntu 20.04 server.
Managing email servers can be a tough task for new system administrators, but with this configuration, you should have enough MTA email functionality to get yourself started.
How To Install and Use Docker Compose on Ubuntu 20.04
5810
Docker simplifies the process of managing application processes in containers.
While containers are similar to virtual machines in certain ways, they are more lightweight and resource-friendly.
This allows developers to break down an application environment into multiple isolated services.
For applications depending on several services, orchestrating all the containers to start up, communicate, and shut down together can quickly become unwieldy.
Docker Compose is a tool that allows you to run multi-container application environments based on definitions set in a YAML file.
In this guide, we'll demonstrate how to install Docker Compose on an Ubuntu 20.04 server and how to get started using this tool.
To follow this article, you will need:
Access to an Ubuntu 20.04 local machine or development server as a non-root user with sudo privileges.
To set these up, please refer to our Initial Server Setup Guide for Ubuntu 20.04.
Docker installed on your server or local machine, following Steps 1 and 2 of How To Install and Use Docker on Ubuntu 20.04.
Step 1 - Installing Docker Compose
To make sure we obtain the most updated stable version of Docker Compose, we'll download this software from its official Github repository.
First, confirm the latest version available in their releases page.
At the time of this writing, the most current stable version is 1.26.0.
The following command will download the 1.26.0 release and save the executable file at / usr / local / bin / docker-compose, which will make this software globally accessible as docker-compose:
Next, set the correct permissions so that the docker-compose command is executable:
To verify that the installation was successful, you can run:
Docker Compose is now successfully installed on your system.
In the next section, we'll see how to set up a docker-compose.yml file and get a containerized environment up and running with this tool.
Step 2 - Setting Up a docker-compose.yml File
To demonstrate how to set up a docker-compose.yml file and work with Docker Compose, we'll create a web server environment using the official Nginx image from Docker Hub, the public Docker registry.
This containerized environment will serve a single static HTML file.
Start off by creating a new directory in your home folder, and then moving into it:
In this directory, set up an application folder to serve as the document root for your Nginx environment:
Using your preferred text editor, create a new index.html file within the app folder:
Place the following content into this file:
Next, create the docker-compose.yml file:
Insert the following content on your docker-compose.yml file:
The docker-compose.yml file typically starts off with the version definition.
This will tell Docker Compose which configuration version we're using.
We then have the services block, where we set up the services that are part of this environment.
In our case, we have a single service called web.
This service uses the nginx: alpine image and sets up a port redirection with the ports directive.
All requests on port 8000 of the host machine (the system from where you're running Docker Compose) will be redirected to the web container on port 80, where Nginx will be running.
The volumes directive will create a shared volume between the host machine and the container.
This will share the local app folder with the container, and the volume will be located at / usr / share / nginx / html inside the container, which will then overwrite the default document root for Nginx.
We have set up a demo page and a docker-compose.yml file to create a containerized web server environment that will serve it. In the next step, we'll bring this environment up with Docker Compose.
Step 3 - Running Docker Compose
With the docker-compose.yml file in place, we can now execute Docker Compose to bring our environment up.
The following command will download the necessary Docker images, create a container for the web service, and run the containerized environment in background mode:
Docker Compose will first look for the defined image on your local system, and if it can't locate the image it will download the image from Docker Hub.
Your environment is now up and running in the background.
To verify that the container is active, you can run:
This command will show you information about the running containers and their state, as well as any port redirections currently in place:
You can now access the demo application by pointing your browser to either localhost: 8000 if you are running this demo on your local machine, or < ^ > your _ server _ domain _ or _ IP < ^ >: 8000 if you are running this demo on a remote server.
Docker Compose Demo Page
Because the shared volume you've set up within the docker-compose.yml file keeps your app folder files in sync with the container's document root.
If you make any changes to the index.html file, they will be automatically picked up by the container and thus reflected on your browser when you reload the page.
In the next step, you'll see how to manage your containerized environment with Docker Compose commands.
Step 4 - Getting Familiar with Docker Compose Commands
You've seen how to set up a docker-compose.yml file and bring your environment up with docker-compose up.
You'll now see how to use Docker Compose commands to manage and interact with your containerized environment.
To check the logs produced by your Nginx container, you can use the logs command:
If you want to pause the environment execution without changing the current state of your containers, you can use:
To resume execution after issuing a pause:
The stop command will terminate the container execution, but it won't destroy any data associated with your containers:
If you want to remove the containers, networks, and volumes associated with this containerized environment, use the down command:
Notice that this won't remove the base image used by Docker Compose to spin up your environment (in our case, nginx: alpine).
This way, whenever you bring your environment up again with a docker-compose up, the process will be much faster since the image is already on your system.
In case you want to also remove the base image from your system, you can use:
< $> note Note: Please refer to our guide on How to Install and Use Docker for a more detailed reference on Docker commands.
In this guide, we've seen how to install Docker Compose and set up a containerized environment based on an Nginx web server image.
We've also seen how to manage this environment using Compose commands.
For a complete reference of all available docker-compose commands, check the official documentation.
How To Install and Configure Zabbix to Securely Monitor Remote Servers on Ubuntu 20.04
6049
Zabbix is open-source monitoring software for networks and applications.
It offers real-time monitoring of thousands of metrics collected from servers, virtual machines, network devices, and web applications.
These metrics can help you determine the current health of your IT infrastructure and detect problems with hardware or software components before customers complain.
Useful information is stored in a database so you can analyze data over time and improve the quality of provided services or plan upgrades of your equipment.
Zabbix uses several options for collecting metrics, including agentless monitoring of user services and client-server architecture.
To collect server metrics, it uses a small agent on the monitored client to gather data and send it to the Zabbix server.
Zabbix supports encrypted communication between the server and connected clients, so your data is protected while it travels over insecure networks.
The Zabbix server stores its data in a relational database powered by MySQL or PostgreSQL.
You can also store historical data in NoSQL databases like Elasticsearch and TimescaleDB.
Zabbix provides a web interface so you can view data and configure system settings.
In this tutorial, you will configure Zabbix on two Ubuntu 20.04 machines.
One will be configured as the Zabbix server, and the other as a client that you'll monitor.
The Zabbix server will use a MySQL database to record monitoring data and use Nginx to serve the web interface.
Two Ubuntu 20.04 servers set up by following the Initial Server Setup Guide for Ubuntu 20.04, including a non-root user with sudo privileges and a firewall configured with ufw.
On one server, you will install Zabbix; this tutorial will refer to this as the Zabbix server.
It will monitor your second server; this second server will be referred to as the second Ubuntu server.
The server that will run the Zabbix server needs Nginx, MySQL, and PHP installed.
Follow Steps 1-3 of our Ubuntu 20.04 LEMP Stack guide to configure those on your Zabbix server.
A registered domain name.
This tutorial will use your _ domain throughout.
You can purchase a domain name from Namecheap, get one for free with Freenom, or use the domain registrar of your choice.
Both of the following DNS records set up for your Zabbix server.
If you are using DigitalOcean, please see our DNS documentation for details on how to add them.
An A record with < ^ > your _ domain < ^ > pointing to your Zabbix server's public IP address.
An A record with www. < ^ > your _ domain < ^ > pointing to your Zabbix server's public IP address.
Additionally, because the Zabbix Server is used to access valuable information about your infrastructure that you would not want unauthorized users to access, it's important that you keep your server secure by installing a TLS / SSL certificate.
If you would like to secure your server, follow the Let's Encrypt on Ubuntu 20.04 guide after Step 3 of this tutorial.
Step 1 - Installing the Zabbix Server
First, you need to install Zabbix on the server where you installed MySQL, Nginx, and PHP.
Log in to this machine as your non-root user:
Zabbix is available in Ubuntu's package manager, but it's outdated, so use the official Zabbix repository to install the latest stable version.
Download and install the repository configuration package:
Update the package index so the new repository is included:
Then install the Zabbix server and web frontend with MySQL database support:
Also, install the Zabbix agent, which will let you collect data about the Zabbix server status itself.
Before you can use Zabbix, you have to set up a database to hold the data that the Zabbix server will collect from its agents.
You can do this in the next step.
Step 2 - Configuring the MySQL Database for Zabbix
You need to create a new MySQL database and populate it with some basic information in order to make it suitable for Zabbix.
You'll also create a specific user for this database so Zabbix isn't logging in to MySQL with the root account.
Log in to MySQL as the root user:
Create the Zabbix database with UTF-8 character support:
Then create a user that the Zabbix server will use, give it access to the new database, and set the password for the user:
That takes care of the user and the database.
Exit out of the database console.
Next you have to import the initial schema and data. The Zabbix installation provided you with a file that sets this up.
Run the following command to set up the schema and import the data into the zabbix database.
Use zcat since the data in the file is compressed:
Enter the password for the zabbix MySQL user that you configured when prompted.
This command may take a minute or two to execute.
In order for the Zabbix server to use this database, you need to set the database password in the Zabbix server configuration file.
Open the configuration file in your preferred text editor.
This tutorial will use nano:
Look for the following section of the file:
These comments in the file explain how to connect to the database.
You need to set the DBPassword value in the file to the password for your database user.
Add this line after those comments to configure the database:
Save and close zabbix _ server.conf by pressing CTRL + X, followed by Y and then ENTER if you're using nano.
You've now configured the Zabbix server to connect to the database.
Next, you will configure the Nginx web server to serve the Zabbix frontend.
Step 3 - Configuring Nginx for Zabbix
To configure Nginx automatically, install the automatic configuration package:
As a result, you will get the configuration file / etc / zabbix / nginx.conf, as well as a link to it in the Nginx configuration directory / etc / nginx / conf.d / zabbix.conf.
Next, you need to make changes to this file.
Open the configuration file:
The file contains an automatically generated Nginx server block configuration.
It contains two lines that determine the server name and what port it is listening on:
Uncomment the two lines, and replace example.com with your domain name.
Your settings will look like this:
Next, test to make sure that there are no syntax errors in any of your Nginx files and reload the configuration:
Now that Nginx is set up to serve the Zabbix frontend, you will make some modifications to your PHP setup in order for the Zabbix web interface to work properly.
If you would like to do this, follow our Ubuntu 20.04 Let's Encrypt tutorial before you move on to Step 4 to obtain a free SSL certificate for Nginx.
This process will automatically detect your Zabbix server block and configure it for HTTPS.
Step 4 - Configuring PHP for Zabbix
The Zabbix web interface is written in PHP and requires some special PHP server settings.
The Zabbix installation process created a PHP-FPM configuration file that contains these settings.
It is located in the directory / etc / zabbix and is loaded automatically by PHP-FPM.
You need to make a small change to this file, so open it up with the following:
The file contains PHP settings that meet the necessary requirements for the Zabbix web interface.
However, the timezone setting is commented out by default.
To make sure that Zabbix uses the correct time, you need to set the appropriate timezone:
Uncomment the timezone line highlighted in the preceding code block and change it to your timezone.
You can use this list of supported time zones to find the right one for you.
Then save and close the file.
Now restart PHP-FPM to apply these new settings:
You can now start the Zabbix server:
Then check whether the Zabbix server is running properly:
You will see the following status:
Finally, enable the server to start at boot time:
The server is set up and connected to the database.
Next, set up the web frontend.
Step 5 - Configuring Settings for the Zabbix Web Interface
The web interface lets you see reports and add hosts that you want to monitor, but it needs some initial setup before you can use it. Launch your browser and go to the address http: / / < ^ > zabbix _ server _ name < ^ > or https: / / < ^ > zabbix _ server _ name < ^ > if you set up Let's Encrypt.
On the first screen, you will see a welcome message.
Click Next step to continue.
On the next screen, you will see the table that lists all of the prerequisites to run Zabbix.
All of the values in this table must be OK, so verify that they are.
Be sure to scroll down and look at all of the prerequisites.
Once you've verified that everything is ready to go, click Next step to proceed.
The next screen asks for database connection information.
DB Connection
You told the Zabbix server about your database, but the Zabbix web interface also needs access to the database to manage hosts and read data. Therefore enter the MySQL credentials you configured in Step 2. Click Next step to proceed.
On the next screen, you can leave the options at their default values.
Zabbix Server Details
The Name is optional; it is used in the web interface to distinguish one server from another in case you have several monitoring servers.
Click Next step to proceed.
The next screen will show the pre-installation summary so you can confirm everything is correct.
Summary
Click Next step to proceed to the final screen.
The web interface setup is now complete.
This process creates the configuration file / usr / share / zabbix / conf / zabbix.conf.php, which you could back up and use in the future.
Click Finish to proceed to the login screen.
The default user is Admin and the password is zabbix.
Before you log in, set up the Zabbix agent on your second Ubuntu server.
Step 6 - Installing and Configuring the Zabbix Agent
Now you need to configure the agent software that will send monitoring data to the Zabbix server.
Log in to the second Ubuntu server:
Just like on the Zabbix server, run the following commands to install the repository configuration package:
Next, update the package index:
Then install the Zabbix agent:
While Zabbix supports certificate-based encryption, setting up a certificate authority is beyond the scope of this tutorial.
But you can use pre-shared keys (PSK) to secure the connection between the server and agent.
First, generate a PSK:
Show the key by using cat so you can copy it somewhere:
The key will look something like this:
Save this for later; you will need it to configure the host.
Now edit the Zabbix agent settings to set up its secure connection to the Zabbix server.
Open the agent configuration file in your text editor:
Each setting within this file is documented via informative comments throughout the file, but you only need to edit some of them.
First you have to edit the IP address of the Zabbix server.
Change the default value to the IP of your Zabbix server:
By default, Zabbix server connects to the agent.
But for some checks (for example, monitoring the logs), a reverse connection is required.
For correct operation, you need to specify the Zabbix server address and a unique host name.
Find the section that configures the active checks and change the default values:
Next, find the section that configures the secure connection to the Zabbix server and enable pre-shared key support.
Find the TLSConnect section, which looks like this:
Then add this line to configure pre-shared key support:
Next, locate the TLSAccept section, which looks like this:
Configure incoming connections to support pre-shared keys by adding this line:
Next, find the TLSPSKIdentity section, which looks like this:
Choose a unique name to identify your pre-shared key by adding this line:
You'll use this as the PSK ID when you add your host through the Zabbix web interface.
Then set the option that points to your previously created pre-shared key.
Locate the TLSPSKFile option:
Add this line to point the Zabbix agent to your PSK file you created:
Now you can restart the Zabbix agent and set it to start at boot time:
For good measure, check that the Zabbix agent is running properly:
You will see the following status, indicating the agent is running:
The agent will listen on port 10050 for connections from the server.
Configure UFW to allow connections to this port:
You can learn more about UFW in How To Set Up a Firewall with UFW on Ubuntu 20.04.
Your agent is now ready to send data to the Zabbix server.
But in order to use it, you have to link to it from the server's web console.
In the next step, you will complete the configuration.
Step 7 - Adding the New Host to the Zabbix Server
Installing an agent on a server you want to monitor is only half of the process.
Each host you want to monitor needs to be registered on the Zabbix server, which you can do through the web interface.
Log in to the Zabbix Server web interface by navigating to the address http: / / < ^ > zabbix _ server _ name < ^ > or https: / / < ^ > zabbix _ server _ name < ^ >:
The Zabbix login screen
When you have logged in, click on Configuration and then Hosts in the left navigation bar.
Then click the Create host button in the top right corner of the screen.
This will open the host configuration page.
Creating a host
Adjust the Host name and IP address to reflect the host name and IP address of your second Ubuntu server, then add the host to a group.
You can select an existing group, for example Linux servers, or create your own group.
The host can be in multiple groups.
To do this, enter the name of an existing or new group in the Groups field and select the desired value from the proposed list.
Before adding the group, click the Templates tab.
Adding a template to the host
Type Template OS Linux by Zabbix agent in the Search field and then select it from the list to add this template to the host.
Next, navigate to the Encryption tab. Select PSK for both Connections to host and Connections from host.
Then set PSK identity to PSK 001, which is the value of the TLSPSKIdentity setting of the Zabbix agent you configured previously.
Then set PSK value to the key you generated for the Zabbix agent.
It's the one stored in the file / etc / zabbix / zabbix _ agentd.psk on the agent machine.
Setting up the encryption
Finally, click the Add button at the bottom of the form to create the host.
You will see your new host in the list.
Wait for a minute and reload the page to see green labels indicating that everything is working fine and the connection is encrypted.
Zabbix shows your new host
If you have additional servers you need to monitor, log in to each host, install the Zabbix agent, generate a PSK, configure the agent, and add the host to the web interface following the same steps you followed to add your first host.
The Zabbix server is now monitoring your second Ubuntu server.
Now, set up email notifications to be notified about problems.
Step 8 - Configuring Email Notifications
Zabbix automatically supports many types of notifications: email, OTRS, Slack, Telegram, SMS, etc. You can see the full list of integrations at the Zabbix website.
As an example, this tutorial will configure notifications for the Email media type.
You will see the list of all media types.
There are two preconfigured options for emails: for the plain text notification and for the HTML notifications.
In this tutorial you will use plain text notification.
Click on Email.
Adjust the SMTP options according to the settings provided by your email service.
This tutorial uses Gmail's SMTP capabilities to set up email notifications; if you would like more information about setting this up, see How To Use Google's SMTP Server.
< $> note Note: If you use 2-Step Verification with Gmail, you need to generate an App Password for Zabbix.
You "ll only have to enter an App password once during setup.
You will find instructions on how to generate this password in the Google Help Center.
If you are using Gmail, type in smtp.gmail.com for the SMTP server field, 465 for the SMTP server port field, gmail.com for SMTP helo, and your email for SMTP email.
Then choose SSL / TLS for Connection security and Username and password for Authentication.
Enter your Gmail address as the Username, and the App Password you generated from your Google account as the Password.
Setting up email media type
On the Message templates tab you can see the list of predefined messages for various types of notifications.
Finally, click the Update button at the bottom of the form to update the email parameters.
Now you can test sending notifications.
To do this, click the Test underlined link in the corresponding line.
You will see a pop-up window.
Enter your email address in the Send to field and click the Test button.
You will see a message about the successful sending and you will receive a test message.
Testing email
Close the pop-up by clicking the Cancel button.
Now, create a new user.
Click on Administration, and then Users in the left navigation bar.
You will see the list of users.
Then click the Create user button in the top right corner of the screen.
This will open the user configuration page:
Creating a user
Enter the new username in the Alias field and set up a new password.
Next, add the user to the administrator's group.
Type Zabbix administrators in the Groups field and select it from the proposed list.
Once you've added the group, click the Media tab and click on the Add underlined link (not the Add button below it).
Adding an email
Select the Email option from the Type drop down.
Enter your email address in the Send to field.
You can leave the rest of the options at the default values.
Click the Add button at the bottom to submit.
Now navigate to the Permissions tab. Select Zabbix Super Admin from the User type drop-down menu.
Finally, click the Add button at the bottom of the form to create the user.
< $> note Note: Using the default password is not safe.
In order to change the password of the built-in user Admin click on the alias in the list of users.
Then click Change password, enter a new password, and confirm the changes by clicking Update button.
Now you need to enable notifications.
Click on the Configuration tab and then Actions in the left navigation bar.
You will see a pre-configured action, which is responsible for sending notifications to all Zabbix administrators.
You can review and change the settings by clicking on its name.
For the purposes of this tutorial, use the default parameters.
To enable the action, click on the red Disabled link in the Status column.
Now you are ready to receive alerts.
In the next step, you will generate one to test your notification setup.
Step 9 - Generating a Test Alert
In this step, you will generate a test alert to ensure everything is connected.
By default, Zabbix keeps track of the amount of free disk space on your server.
It automatically detects all disk mounts and adds the corresponding checks.
This discovery is executed every hour, so you need to wait a while for the notification to be triggered.
Create a temporary file that's large enough to trigger Zabbix's file system usage alert.
To do this, log in to your second Ubuntu server if you're not already connected:
Next, determine how much free space you have on the server.
You can use the df command to find out:
The command df will report the disk space usage of your file system, and the -h will make the output human-readable.
You'll see output like the following:
In this case, the free space is 77G.
Your free space may differ.
Use the fallocate command, which allows you to pre-allocate or de-allocate space to a file, to create a file that takes up more than 80% of the available disk space.
This will be enough to trigger the alert:
After around an hour, Zabbix will trigger an alert about the amount of free disk space and will run the action you configured, sending the notification message.
You can check your inbox for the message from the Zabbix server.
You will see a message like:
You can also navigate to the Monitoring tab and then Dashboard to see the notification and its details.
Main dashboard
Now that you know the alerts are working, delete the temporary file you created so you can reclaim your disk space:
After a minute Zabbix will send the recovery message and the alert will disappear from the main dashboard.
In this tutorial, you learned how to set up a simple and secure monitoring solution that will help you monitor the state of your servers.
It can now warn you of problems, and you have the opportunity to analyze the processes occurring in your IT infrastructure.
To learn more about setting up monitoring infrastructure, check out our Monitoring topic page.
How To Centralize Logs With Journald on Debian 10
6077
Two Debian 10 servers.
Follow the Initial Server Setup with Debian 10 guide for instructions on how to do this.
< $> note Note: Throughout the tutorial, command blocks are labeled with the server name (client or server) that the command should be run on.
First, install certbot and the curl utility on both hosts:
Finally, you need to download a copy of the Let's Encrypt CA and intermediate certificates and put them into the same file. journald will use this file to verify the authenticity of the certificates on the client and server when they communicate with each other.
The following command will download the two certificates from the Let's Encrypt website and put them into a single file called letsencrypt-combined-certs.pem in your user's home directory.
SplitMode = host: The logs from the remote clients will be split by host in / var / log / journal / remote.
Next, change the group ownership of the private key to systemd-journal-remote's group:
How To Install Discourse on Ubuntu 20.04
6065
You can use Discourse as a mailing list, a discussion forum, or a long-form chat room.
In this tutorial, you will install Discourse in an isolated environment using Docker, a containerization application.
Before you get started, there are a few things you will need:
One Ubuntu 20.04 server with at least 2GB of RAM, a sudo non-root user, and a firewall.
For guidance, you can reference our initial server setup tutorial for Ubuntu 20.04.
Docker installed on your server.
To accomplish this, you can follow step 1 of our Docker installation tutorial for Ubuntu 20.04.
A domain (or subdomain) with an available A record pointed at your server's IP.
If you are managing your DNS on DigitalOcean, then you can follow this guide to associate your IP with your domain.
Step 1 & mdash; Downloading Discourse
Before downloading and installing Discourse, create the / var / discourse directory.
This is where all your Discourse-related files will reside:
Finally, clone the official Discourse Docker Image into / var / discourse:
With the Discourse Docker image in place, you can now install and configure your platform.
Step 2 & mdash; Installing and Configuring Discourse
Move to the / var / discourse directory:
Now launch the included setup script:
The Discourse installation script will ask the following questions:
Enter < ^ > discourse.your _ domain < ^ >, or whatever hostname you've chosen for your platform.
It can be unrelated to your Discourse domain and can be any email address you find convenient.
Note that this email address will become the Discourse administrator default.
Later, you will need to reuse this email address when you set up Discourse from its control panel.
If you're using Mailgun, the SMTP server address will be smtp.mailgun.org, and the username and password are the SMTP credentials for your domain under Mailgun's domains tab.
Finally, the Discourse installation script will ask you to confirm all these settings.
Confirm your settings, and the script will generate a configuration file called app.yml.
The installation process will begin automatically.
Note: If you need to change or fix these settings after installation, edit your / containers / app.yml file and run. / launcher rebuild app. Otherwise, your changes will not take effect.
The Discourse installation will take approximately 2-8 minutes, after which your instance will be running.
Now you can open a web browser and create an administrator account.
Step 3 & mdash; Registering an Administrator Account
Visit < ^ > discourse.your _ domain < ^ > in your favorite web browser, and you will see the Discourse 'Congrats' splash screen.
Discourse congratulations screen
If you receive a 502 Bad Gateway error, try waiting a minute or two and then refreshing your browser; your Discourse installation might not have completed.
Discourse configuration wizard
After completing or skipping the setup wizard, you'll see some topics and Discourse's Admin Quick Start Guide.
The quick start guide is labeled READ ME FIRST, and contains tips for further customizing your Discourse installation.
Discourse homepage and link to Admin Quick Start Guide
Your Discourse platform is now ready for use.
If you need to upgrade Discourse in the future, you can do so from the command line by pulling the latest version of the code from the Git repo and rebuilding the app:
You can also update Discourse in your browser.
Visit http: / / < ^ > discourse.your _ domain < ^ > / admin / upgrade, click Upgrade to the Latest Version, and following the instructions.
Discourse upgrade admin upgrade page
You can learn more about Discourse's features on the official Discourse About page.
How To Use the pathlib Module to Manipulate Filesystem Paths in Python 3
6079
Python 3 includes the pathlib module for manipulating filesystem paths agnostically whatever the operating system. pathlib is similar to the os.path module, but pathlib offers a higher level - and often times more convenient - interface than os.path.
We can identify files on a computer with hierarchical paths.
For example, we might identify the file wave.txt on a computer with this path: / Users / < ^ > sammy < ^ > / ocean / wave.txt.
Operating systems represent paths slightly differently.
Windows might represent the path to the wave.txt file like C:\ Users\ < ^ > sammy < ^ >\ ocean\ wave.txt.
You might find the pathlib module useful if in your Python program you are creating or moving files on the filesystem, listing files on the filesystem that all match a given extension or pattern, or creating operating system appropriate file paths based on collections of raw strings.
While you might be able to use other tools (like the os.path module) to accomplish many of these tasks, the pathlib module allows you to perform these operations with a high degree of readability and minimal amount of code.
In this tutorial, we'll go over some of the ways to use the pathlib module to represent and manipulate filesystem paths.
To get the most out of this tutorial, it is recommended to have some familiarity with programming in Python 3. You can review these tutorials for the necessary background information:
How To Code in Python 3
Constructing Path Instances
The pathlib module provides several classes, but one of the most important is the Path class.
Instances of the Path class represent a path to a file or directory on our computer's filesystem.
For example, the following code instantiates a Path instance that represents part of the path to a wave.txt file:
If we run this code, we'll receive output like the following:
from pathlib import Path makes the Path class available to our program.
Then Path (" ocean "," wave.txt ") instantiates a new Path instance.
Printing the output shows that Python has added the appropriate operating system separator of / between the two path components we gave it: "ocean" and "wave.txt".
< $> note Note: Depending on your operating system, your output may vary slightly from the example outputs shown in this tutorial.
If you are running Windows, for example, your output for this first example might look like ocean\ wave.txt.
Currently, the Path object assigned to the wave variable contains a relative path.
In other words, ocean / wave.txt might exist in several places on our filesystem.
As an example, it may exist in / Users / < ^ > user _ 1 < ^ > / ocean / wave.txt or / Users / < ^ > user _ 2 < ^ > / research / ocean / wave.txt, but we haven't specified exactly which one we are referring to.
An absolute path, by contrast, unambiguously refers to one location on the filesystem.
You can use Path.home () to get the absolute path to the home directory of the current user:
If we run this code, we'll receive output roughly like the following:
< $> note Note: As mentioned earlier, your output will vary depending on your operating system.
Your home directory, of course, will also be different than / Users / < ^ > sammy < ^ >.
Path.home () returns a Path instance with an absolute path to the current user's home directory.
We then pass in this Path instance and the strings "ocean" and "wave.txt" into another Path constructor to create an absolute path to the wave.txt file.
The output shows the first line is the home directory, and the second line is the home directory plus ocean / wave.txt.
This example also illustrates an important feature of the Path class: the Path constructor accepts both strings and preexisting Path objects.
Let's look at the support of both strings and Path objects in the Path constructor a little more closely:
If we run this Python code, we'll receive output similar to the following:
shark is a Path to a file that we constructed using both Path objects (Path.home () and Path (" fish "," shark.txt ")) and strings (" ocean "and" animals ").
The Path constructor intelligently handles both types of objects and cleanly joins them using the appropriate operating system separator, in this case /.
Accessing File Attributes
Now that we've learned how to construct Path instances, let's review how you can use those instances to access information about a file.
We can use the name and suffix attributes to access file names and file suffixes:
Running this code, we'll receive output similar to the following:
This output shows that the name of the file at the end of our path is wave.txt and the suffix of that file is .txt.
Path instances also offer the with _ name function that allow you to seamlessly create a new Path object with a different name:
If we run this, we'll receive output like the following:
The code first constructs a Path instance that points to a file named wave.txt.
Then, we call the with _ name method on wave to return a second Path instance that points to a new file named tides.txt.
The ocean / directory portion of the path remains unchanged, leaving the final path as ocean / tides.txt
Accessing Ancestors
Sometimes it is useful to access directories that contain a given path.
Let's consider an example:
If we run this code, we'll receive output that looks like the following:
The parent attribute on a Path instance returns the most immediate ancestor of a given file path.
In this case, it returns the directory that contains the shark.txt file: ocean / animals / fish.
We can access the parent attribute multiple times in a row to traverse up the ancestry tree of a given file:
If we run this code, we'll receive the following output:
The output is similar to the earlier output, but now we've traversed yet another level higher by accessing .parent a second time.
Two directories up from shark.txt is the ocean / animals directory.
Using Glob to List Files
It's also possible to use the Path class to list files using the glob method.
Let's say we had a directory structure that looked like this:
An ocean directory contains the files tides.txt and wave.txt.
We have a file named shark.txt nested under the ocean directory, an animals directory, and a fish directory: ocean / animals / fish.
To list all the .txt files in the ocean directory, we could say:
This code would yield output like:
The "* .txt" glob pattern finds all files ending in .txt.
Since the code sample executes that glob in the ocean directory, it returns the two .txt files in the ocean directory: wave.txt and tides.txt.
< $> note Note: If you would like to duplicate the outputs shown in this example, you'll need to mimic the directory structure illustrated here on your computer.
We can also use the glob method recursively.
To list all the .txt files in the ocean directory and all its subdirectories, we could say:
If we run this code, we'd receive output like the following:
The * * part of the glob pattern will match this directory and all directories beneath it, recursively.
So, not only do we have the wave.txt and tides.txt files in the output, but we also receive the shark.txt file that was nested under ocean / animals / fish.
Computing Relative Paths
We can use the Path.relative _ to method to compute paths relative to one another.
The relative _ to method is useful when, for example, you want to retrieve a portion of a long file path.
Consider the following code:
The relative _ to method returns a new Path object relative to the given argument.
In our example, we compute the Path to shark.txt relative to the ocean directory, and then relative to both the ocean and animals directories.
If relative _ to can't compute an answer because we give it an unrelated path, it raises a ValueError:
We'll receive a ValueError exception raised from this code that will be something like this:
unrelated / path is not a part of ocean / animals / fish / shark.txt, so there's no way for Python to compute a relative path for us.
The pathlib module is a powerful part of the Python Standard Library that lets us manipulate filesystem paths quickly on any operating system.
In this tutorial, we have learned to use some of pathlib's key utilities for accessing file attributes, listing files with glob patterns, and traversing parent files and directories.
The pathlib module exposes additional classes and utilities that we did not cover in this tutorial.
Now that you have a baseline, you can use the pathlib module's documentation to learn more about other available classes and utilities.
If you're interested in using other Python libraries, check out the following tutorials:
How To Use the collections Module in Python 3
How To Use the sqlite3 Module in Python 3
How To Use ThreadPoolExecutor in Python 3
How To Scale and Secure a Django Application with Docker, Nginx, and Let's Encrypt
6039
In cloud-based environments, there are multiple ways to scale and secure a Django application.
By scaling horizontally, and running several copies of your app, you can build a more fault-tolerant and highly-available system, while also increasing its throughput so that requests can be processed simultaneously.
One way to horizontally scale a Django app is to provision additional app servers that run your Django application and its WSGI HTTP server (like Gunicorn or uWSGI).
To route and distribute incoming requests across this set of app servers, you can use a load balancer and reverse proxy like Nginx.
Nginx can also cache static content and terminate Transport Layer Security (TLS) connections, used to provide HTTPS and secure connections to your app.
In addition, containers provide many features that facilitate packaging and configuring your application.
In this tutorial, you'll horizontally scale a containerized Django and Gunicorn Polls application by provisioning two application servers that will each run a copy of a Django and Gunicorn app container.
You'll also enable HTTPS by provisioning and configuring a third proxy server that will run an Nginx reverse proxy container and a Certbot client container.
Certbot will provision TLS certificates for Nginx from the Let's Encrypt certificate authority.
This will ensure that your site receives a high security rating from SSL Labs.
This proxy server will receive all of your app's external requests and sit in front of the two upstream Django application servers.
Finally, you'll harden this distributed system by restricting external access to only the proxy server.
Three Ubuntu 18.04 servers:
Two servers will be application servers, used to run your Django and Gunicorn app.
One server will be a proxy server, used to run Nginx and Certbot.
All should have a non-root user with sudo privileges, and an active firewall.
For guidance on how to set these up, please see this Initial Server Setup guide.
Docker installed on all three servers.
For guidance on installing Docker, follow Steps 1 and 2 of How To Install and Use Docker on Ubuntu 18.04.
This tutorial will use < ^ > your _ domain.com < ^ > throughout.
You can get one for free at Freenom, or use the domain registrar of your choice.
An A DNS record with < ^ > your _ domain.com < ^ > pointing to your proxy server "s public IP address.
You can follow this introduction to DigitalOcean DNS for details on how to add it to a DigitalOcean account, if that "s what you" re using.
An S3 object storage bucket such as a DigitalOcean Space to store your Django project "s static files and a set of Access Keys for this Space.
To learn how to create a Space, consult the How to Create Spaces product documentation.
To learn how to create Access Keys for Spaces, consult Sharing Access to Spaces with Access Keys.
With minor changes, you can use any object storage service that the django-storages plugin supports.
The PostgreSQL database should be called polls (or another memorable name to input in your config files below) and in this tutorial the database user will be named sammy.
For guidance on creating these, follow Step 1 of How to Build a Django and Gunicorn Application with Docker.
You can perform these steps from any of the three servers.
A DigitalOcean Managed PostgreSQL cluster is used in this tutorial.
To learn how to create a cluster, consult the DigitalOcean Managed Databases product documentation.
You can also install and run your own PostgreSQL instance.
For guidance on installing and administering PostgreSQL on an Ubuntu server, please see How To Install and Use PostgreSQL on Ubuntu 18.04.
Step 1 - Configuring the First Django Application Server
To begin, we'll clone the Django application repository onto the first app server.
Then, we'll configure and build the application Docker image, and test the application by running the Django container.
< $> note Note: If you're continuing from How to Build a Django and Gunicorn Application with Docker, you will have already completed Step 1 and can skip ahead to Step 2 to configure the second app server.
Start by logging in to the first of the two Django application servers and using git to clone the polls-docker branch of the Django Tutorial Polls App GitHub repository.
This repo contains code for the Django documentation's sample Polls application.
The polls-docker branch contains a Dockerized version of the Polls app. To learn how the Polls app was modified to work effectively in a containerized environment, please see How to Build a Django and Gunicorn Application with Docker.
Navigate into the django-polls directory:
This directory contains the Django application Python code, a Dockerfile that Docker will use to build the container image, as well as an env file that contains a list of environment variables to be passed into the container's running environment.
Inspect the Dockerfile using cat:
This Dockerfile uses the official Python 3.7.4 Docker image as a base, and installs Django and Gunicorn's Python package requirements, as defined in the django-polls / requirements.txt file.
It then removes some unnecessary build files, copies the application code into the image, and sets the execution PATH.
Finally, it declares that port 8000 will be used to accept incoming container connections, and runs gunicorn with 3 workers, listening on port 8000.
To learn more about each of the steps in this Dockerfile, please see Step 6 of How to Build a Django and Gunicorn Application with Docker.
Now, build the image using docker build:
We name the image polls using the -t flag and pass in the current directory as a build context, the set of files to reference when constructing the image.
After Docker builds and tags the image, list available images using docker images:
You should see the polls image listed:
Before we run the Django container, we need to configure its running environment using the env file present in the current directory.
This file will be passed into the docker run command used to run the container, and Docker will inject the configured environment variables into the container's running environment.
Open the env file with nano or your favorite editor:
We "ll be configuring the file like so, and you" ll need to add some additional values as outlined below.
Fill in the missing values for the following keys:
DJANGO _ SECRET _ KEY: Set this to a unique, unpredictable value, as detailed in the Django docs.
One method of generating this key is provided in Adjusting the App Settings of the Scalable Django App tutorial.
DJANGO _ ALLOWED _ HOSTS: This variable secures the app and prevents HTTP Host header attacks.
For testing purposes, set this to *, a wildcard that will match all hosts.
In production you should set this to < ^ > your _ domain.com < ^ >.
To learn more about this Django setting, consult Core Settings from the Django docs.
DATABASE _ USERNAME: Set this to the PostgreSQL database user created in the prerequisite steps.
DATABASE _ NAME: Set this to polls or the name of the PostgreSQL database created in the prerequisite steps.
DATABASE _ PASSWORD: Set this to the PostgreSQL user password created in the prerequisite steps.
DATABASE _ HOST: Set this to your database "s hostname.
DATABASE _ PORT: Set this to your database's port.
STATIC _ ACCESS _ KEY _ ID: Set this to your S3 bucket or Space "s access key.
STATIC _ SECRET _ KEY: Set this to your S3 bucket or Space "s access key Secret.
STATIC _ BUCKET _ NAME: Set this to your S3 bucket or Space name.
STATIC _ ENDPOINT _ URL: Set this to the appropriate S3 bucket or Space endpoint URL, for example https: / / < ^ > space-name < ^ > .nyc3.digitaloceanspaces.com if your Space is located in the nyc3 region.
Once you've finished editing, save and close the file.
We "ll now use docker run to override the CMD set in the Dockerfile and create the database schema using the manage.py makemigrations and manage.py migrate commands:
We run the polls: latest container image, pass in the environment variable file we just modified, and override the Dockerfile command with sh -c "python manage.py makemigrations & & python manage.py migrate", which will create the database schema defined by the app code.
If you're running this for the first time you should see:
This indicates that the database schema has successfully been created.
If you're running migrate a subsequent time, Django will perform a no-op unless the database schema has changed.
Next, we "ll run another instance of the app container and use an interactive shell inside of it to create an administrative user for the Django project.
This will provide you with a shell prompt inside of the running container which you can use to create the Django user:
Enter a username, email address, and password for your user, and after creating the user, hit CTRL + D to quit the container and kill it.
Finally, we "ll generate the static files for the app and upload them to the DigitalOcean Space using collectstatic.
Note that this may take a bit of time to complete.
After these files are generated and uploaded, you "ll receive the following output.
We can now run the app:
Here, we run the default command defined in the Dockerfile, gunicorn --bind: 8000 --workers 3 mysite.wsgi: application, and expose container port 8000 so that port 80 on the Ubuntu server gets mapped to port 8000 of the polls container.
You should now be able to navigate to the polls app using your web browser by typing http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ > in the URL bar.
Since there is no route defined for the / path, you "ll likely receive a 404 Page Not Found error, which is expected.
< $> warning Warning: When using the UFW firewall with Docker, Docker bypasses any configured UFW firewall rules, as documented in this GitHub issue.
This explains why you have access to port 80 of your server, even though you haven't explicitly created a UFW access rule in any prerequisite step.
In Step 5 we will address this security hole by patching the UFW configuration.
If you are not using UFW and are using DigitalOcean's Cloud Firewalls, you can safely ignore this warning.
Navigate to http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ > / polls to see the Polls app interface:
Polls Apps Interface
To view the administrative interface, visit http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ > / admin.
You should see the Polls app admin authentication window:
Polls Admin Auth Page
Enter the administrative username and password you created with the createsuperuser command.
After authenticating, you can access the Polls app "s administrative interface:
Polls Admin Main Interface
Note that static assets for the admin and polls apps are being delivered directly from object storage.
To confirm this, consult Testing Spaces Static File Delivery.
When you are finished exploring, hit CTRL + C in the terminal window running the Docker container to kill the container.
Now that you've confirmed that the app container runs as expected, you can run it in detached mode, which will run it in the background and allow you to log out of your SSH session:
The -d flag instructs Docker to run the container in detached mode, the -rm flag cleans up the container's filesystem after the container exits, and we name the container polls.
Log out of the first Django app server, and navigate to http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ > / polls to confirm that the container is running as expected.
Now that your first Django app server is up and running, you can set up your second Django app server.
Step 2 - Configuring the Second Django Application Server
Since many of the commands to set up this server will be the same as those in the previous step, they will be presented here in abbreviated form. Please review Step 1 for more information on any particular command in this step.
Begin by logging in to the second Django application server.
Clone the polls-docker branch of the django-polls GitHub repository:
Build the image using docker build:
Fill in the missing values as in Step 1. When you've finished editing, save and close the file.
Finally, run the app container in detached mode:
Navigate to http: / / < ^ > APP _ SERVER _ 2 _ IP < ^ > / polls to confirm that the container is running as expected.
You can safely log out of the second app server without terminating your running container.
With both Django app containers up and running, you can move on to configuring the Nginx reverse proxy container.
Step 3 - Configuring the Nginx Docker Container
Nginx is a versatile web server that offers a number of features including reverse proxying, load balancing, and caching.
In this tutorial we've offloaded Django's static assets to object storage, so we won't use Nginx's caching capabilities.
However, we will use Nginx as a reverse proxy to our two backend Django app servers, and distribute incoming requests between them.
In addition, Nginx will perform TLS termination and redirection using a TLS certificate provisioned by Certbot.
This means that it will force clients to use HTTPS, redirecting incoming HTTP requests to port 443. It will then decrypt HTTPS requests and proxy them to the upstream Django servers.
In this tutorial we've made the design decision to decouple the Nginx containers from the backend servers.
Depending on your use case, you may choose to run the Nginx container on one of the Django app servers, proxying requests locally, as well as to the other Django server.
Another possible architecture would be running two Nginx containers, one on each backend server, with a cloud load balancer in front.
Each architecture presents different security and performance advantages, and you should load test your system to discover bottlenecks.
The flexible architecture described in this tutorial allows you to scale both the backend Django app layer, as well as the Nginx proxying layer.
Once the single Nginx container becomes a bottleneck, you can scale out to multiple Nginx proxies, and add a cloud load balancer or fast L4 load balancer like HAProxy.
With both Django app servers up and running, we can begin setting up the Nginx proxy server.
Log in to your proxy server and create a directory called conf:
Create a configuration file called nginx.conf using nano or your favorite editor:
Paste in the following Nginx configuration:
These upstream, server, and location blocks configure Nginx to redirect HTTP requests to HTTPS, and load balance them across the two Django app servers configured in Steps 1 and 2. To learn more about Nginx configuration file structure, please refer to this article on Understanding the Nginx Configuration File Structure and Configuration Contexts.
Additionally, this article on Understanding Nginx Server and Location Block Selection Algorithms may be helpful.
This configuration was assembled from sample configuration files provided by Gunicorn, Cerbot, and Nginx and is meant as a minimal Nginx configuration to get this architecture up and running.
Tuning this Nginx configuration goes beyond the scope of this article, but you can use a tool like NGINXConfig to generate performant and secure Nginx configuration files for your architecture.
The upstream block defines the group of servers used to proxy requests to using the proxy _ pass directive:
In this block we name the upstream django and include the IP addresses of both Django app servers.
If the app servers are running on DigitalOcean and have VPC Networking enabled, you should use their private IP addresses here.
To learn how to enable VPC Networking on DigitalOcean, please see How to Enable VPC Networking on Existing Droplets.
The first server block captures requests that do not match your domain and terminates the connection.
For example, a direct HTTP request to your server's IP address would be handled by this block:
The next server block redirects HTTP requests to your domain to HTTPS using an HTTP 301 redirect.
These requests are then handled by the final server block:
These two directives define the paths to the TLS certificate and secret key.
These will be provisioned using Certbot and mounted into the Nginx container in the next step.
These parameters are SSL security defaults recommended by Certbot.
To learn more about them, please see Module ngx\ _ http\ _ ssl\ _ module from the Nginx docs.
Mozilla's Security / Server Side TLS is another helpful guide that you can use to tune your SSL configuration.
These two directives from Gunicorn's sample Nginx configuration set the maximum allowed size of the client request body and assign the timeout for keep-alive connections with the client.
Nginx will close connections with the client after keepalive _ timeout seconds.
The first location block instructs Nginx to proxy requests to the upstream django servers over HTTP.
It additionally preserves client HTTP headers that capture the originating IP address, protocol used to connect, and target host:
To learn more about these directives, please see Deploying Gunicorn and Module ngx\ _ http\ _ proxy\ _ module from the Nginx docs.
The final location block captures requests to the / well-known / acme-challenge / path, used by Certbot for HTTP-01 challenges to verify your domain with Let's Encrypt and provision or renew TLS certificates.
For more information on the HTTP-01 challenge used by Certbot, please see Challenge Types from the Let's Encrypt docs.
You can now use this configuration file to run an Nginx Docker container.
In this tutorial we'll use the nginx: 1.19.0 image, version 1.19.0 of the official Docker image maintained by Nginx.
When we run the container for the first time, Nginx will throw an error and fail as we haven't yet provisioned the certificates defined in the configuration file.
However, we'll still run the command to download the Nginx image locally and test that everything else is functioning correctly:
Here we name the container nginx and map the host ports 80 and 443 to the respective container ports.
The -v flag mounts the config file into the Nginx container at / etc / nginx / conf.d / nginx.conf, which the Nginx image is preconfigured to load.
It is mounted in ro or "read only" mode, so the container cannot modify the file.
The web root directory / var / www / html is also mounted into the container.
Finally nginx: 1.19.0 instructs Docker to pull and run the nginx: 1.19.0 image from Dockerhub.
Docker will pull and run the image, then Nginx will throw an error when it doesn't find the configured TLS certificate and secret key.
In the next step we'll provision these using a Dockerized Certbot client and the Let's Encrypt certificate authority.
Step 4 - Configuring Certbot and Let's Encrypt Certificate Renewal
Certbot is a Let's Encrypt client developed by the Electronic Frontier Foundation.
It provisions free TLS certificates from the Let's Encrypt certificate authority which allow browsers to verify the identity of your web servers.
Given that we have Docker installed on our Nginx proxy server, we'll use the Certbot Docker image to provision and renew the TLS certificates.
Begin by ensuring that you have a DNS A record mapped to the proxy server's public IP address.
Then, on your proxy server, provision a staging version of the certificates using the certbot Docker image:
This command runs the certbot Docker image in interactive mode, and forwards port 80 on the host to container port 80. It creates and mounts two host directories into the container: / etc / letsencrypt / and / var / lib / letsencrypt /. certbot is run in standalone mode, without Nginx, and will use the Let's Encrypt staging servers to perform domain validation.
When prompted, enter your email address and agree to the Terms of Service.
If domain validation was successful, you should see the following output:
You can inspect the certificate using cat:
With the TLS certificate provisioned, we can test the Nginx configuration assembled in the previous step:
This is the same command run in Step 3, with the addition of both recently created Let's Encrypt directories.
Once Nginx is up and running, navigate to http: / / < ^ > your _ domain.com < ^ >.
You may receive a warning in your browser that the certificate authority is invalid.
This is expected as we've provisioned staging certificates and not production Let's Encrypt certificates.
Check the URL bar of your browser to confirm that your HTTP request was redirected to HTTPS.
Hit CTRL + C in your terminal to quit Nginx, and run the certbot client again, this time omitting the --staging flag:
When prompted to either keep the existing certificate or renew and replace it, hit 2 to renew it and then ENTER to confirm your choice.
With the production TLS certificate provisioned, run the Nginx server once again:
In your browser, navigate to http: / / < ^ > your _ domain.com < ^ >.
In the URL bar, confirm that the HTTP request has been redirected to HTTPS.
Given that the Polls app has no default route configured, you should see a Django Page not found error.
Navigate to https: / / < ^ > your _ domain.com < ^ > / polls and you'll see the standard Polls app interface:
At this point you've provisioned a production TLS certificate using the Certbot Docker client, and are reverse proxying and load balancing external requests to the two Django app servers.
Let's Encrypt certificates expire every 90 days.
To ensure that your certificate remains valid, you should renew it regularly before its scheduled expiry.
With Nginx running, you should use the Certbot client in webroot mode instead of standalone mode.
This means that Certbot will perform validation by creating a file in the / var / www / html / .well-known / acme-challenge / directory, and the Let's Encrypt validation requests to this path will be captured by the location rule defined in the Nginx config in Step 3. Certbot will then rotate certificates, and you can reload Nginx so that it uses this newly provisioned certificate.
There are multiple ways to automate this procedure and the automatic renewal of TLS certificates goes beyond the scope of this tutorial.
For a similar process using the cron scheduling utility, please see Step 6 of How To Secure a Containerized Node.js Application with Nginx, Let's Encrypt, and Docker Compose.
In your terminal, hit CTRL + C to kill the Nginx container.
Run it again in detached mode by appending the -d flag:
With Nginx running in the background, use the following command to perform a dry run of the certificate renewal procedure:
We use the --webroot plugin, specify the web root path, and use the --dry-run flag to verify that everything is working correctly without actually performing the certificate renewal.
If the renewal simulation succeeds, you should see the following output:
In a production setting, after renewing certificates, you should reload Nginx so that the changes take effect.
To reload Nginx, run the following command:
This command will send a HUP Unix signal to the Nginx process running inside of the nginx Docker container.
Upon receiving this signal, Nginx will reload its configuration and renewed certificates.
With HTTPS enabled and all the components of this architecture up and running, the final step is to lock down the setup by preventing external access to the two backend app servers; all HTTP requests should flow through the Nginx proxy.
Step 5 - Preventing External Access to Django App Servers
In the architecture described in this tutorial, SSL termination occurs at the Nginx proxy.
This means that Nginx decrypts the SSL connection, and packets are proxied to the Django app servers unencrypted.
For many use cases, this level of security is sufficient.
For applications involving financial or health data, you may want to implement end-to-end encryption.
You can do this by forwarding encrypted packets through the load balancer and decrypting on the app servers, or re encrypting at the proxy and once again decrypting on the Django app servers.
These techniques go beyond the scope of this article, but to learn more please consult End-to-end encryption.
The Nginx proxy acts as a gateway between external traffic and the internal network.
Theoretically no external clients should have direct access to the internal app servers, and all requests should flow through the Nginx server.
The note in Step 1 briefly describes an open issue with Docker where Docker bypasses ufw firewall settings by default and opens ports externally, which may be insecure.
To address this security concern, it's recommended to use cloud firewalls when working with Docker-enabled servers.
To get more information on creating Cloud Firewalls with DigitalOcean, consult How to Create Firewalls.
You can also manipulate iptables directly instead of using ufw.
To learn more about using iptables with Docker, please see Docker and iptables.
In this step we'll modify UFW's configuration to block external access to host ports opened by Docker.
When running Django on the app servers, we passed the -p 80: 8000 flag to docker, which forwards port 80 on the host to container port 8000.
This also opened up port 80 to external clients, which you can verify by visiting http: / / < ^ > your _ app _ server _ 1 _ IP < ^ >.
To prevent direct access, we'll modify UFW's configuration using the method described in the ufw-docker GitHub repository.
Begin by logging in to the first Django app server.
Then, open the / etc / ufw / after.rules file with superuser privileges, using nano or your favorite editor:
Enter your password when prompted, and hit ENTER to confirm.
You should see the following ufw rules:
Scroll to the bottom, and paste in the following block of UFW configuration rules:
These rules restrict public access to ports opened by Docker, and enable access from the 10.0.0.0 / 8, 172.16.0.0 / 12, and 192.168.0.0 / 16 private IP ranges.
If you are using VPC with DigitalOcean, then Droplets in your VPC network will have access to the open port over the private network interface, but external clients will not.
For more information about VPC, please see the VPC official documentation.
To learn more about the rules implemented in this snippet, please see How it works?
from the ufw-docker README.
If you are not using VPC with DigitalOcean, and have entered the public IP addresses of the app servers in the upstream block of your Nginx config, you will have to explicitly modify the UFW firewall to allow traffic from the Nginx server through port 80 on the Django app servers.
For guidance on creating allow rules with the UFW firewall, please see UFW Essentials: Common Firewall Rules and Commands.
When you've finished editing, save and close the file.
Restart ufw so that it picks up the new configuration:
Navigate to http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ > in your web browser to confirm that you can no longer access the app server over port 80.
Repeat this process on the second Django app server.
Log out of the first app server or open another terminal window, and log in to the second Django app server.
Navigate to http: / / < ^ > APP _ SERVER _ 2 _ IP < ^ > in your web browser to confirm that you can no longer access the app server over port 80.
Finally, navigate to https: / / < ^ > your _ domain _ here < ^ > / polls to confirm that the Nginx proxy still has access to the upstream Django servers.
You should see the default Polls app interface.
In this tutorial, you've set up a scalable Django Polls application using Docker containers.
As your traffic grows and load on the system increases, you can scale each layer separately: the Nginx proxying layer, the Django backend app layer, and the PostgreSQL database layer.
When building a distributed system, there are often multiple design decisions you must face, and several architectures may satisfy your use case.
The architecture described in this tutorial is meant as a flexible blueprint for designing scalable apps with Django and Docker.
You may wish to control the behavior of your containers when they encounter errors, or run containers automatically when your system boots.
To do this, you can use a process manager like Systemd or implement restart policies.
For more information about these, please see Start containers automatically from the Docker documentation.
When working at scale with multiple hosts running the same Docker image, it can be more efficient to automate steps using a configuration management tool like Ansible or Chef.
To learn more about configuration management, please consult An Introduction to Configuration Management and Automating Server Setup with Ansible: A DigitalOcean Workshop Kit.
Instead of building the same image on every host, you can also streamline deployment using an image registry like Docker Hub, which centrally builds, stores, and distributes Docker images to multiple servers.
Along with an image registry, a continuous integration and deployment pipeline can help you build, test, and deploy images to your app servers.
For more information on CI / CD, please consult An Introduction to CI / CD Best Practices.
How To Install and Configure Postfix as a Send-Only SMTP Server on Ubuntu 20.04
6158
This is useful in situations when you need to regularly send email notifications from your apps or have a lot of outbound traffic that a third-party email service provider won't allow.
One Ubuntu 20.04 server set up with the Initial Server Setup with Ubuntu 20.04, including creating a sudo non-root user.
In this step, you'll configure Postfix to send and receive emails only from the server on which it is running - that is, from localhost.
For that to happen, you need to configure Postfix to listen only on the loopback interface, the virtual network interface that the server uses to communicate internally.
Another directive you'll need to modify is mydestination, which specifies the list of domains that are delivered via the local _ transport mail delivery transport.
If your domain is actually a subdomain and you want the email messages to look as if they were sent from the main domain, you can add the following line to the end of main.cf:
The optional masquerade _ domains setting specifies the domains for which the subdomain will be stripped off in the email address.
If you receive an error from the mail command, or you haven't received a message after a prolonged period of time, check that the Postfix configuration you edited is valid and that your server's name and hostname are set to your domain.
The only directive present specifies that system-generated emails are sent to root.
In this step, you set up forwarding system-generated messages to your email address.
Ubuntu includes Certbot in their default package repositories, so you can install it by running the following command:
When asked for confirmation, type Y and press ENTER.
Modify it to look like this, replacing < ^ > your _ domain < ^ > with your domain where necessary.
This will update your TLS settings for Postfix:
Encrypting all outgoing messages is an effective first step to email providers not marking your messages as spam outright.
However, if your use case is to send emails to potential site users (such as confirmation emails for a message board sign-up), look into setting up SPF records, so that your server's emails are even more likely to be seen as legitimate.
How To Scrape a Website Using Node.js and Puppeteer
6187
The author selected the Free and Open Source Fund to receive a donation as part of the Write for DOnations program.
Web scraping is the process of automating data collection from the web.
The process typically deploys a "crawler" that automatically surfs the web and scrapes data from selected pages.
There are many reasons why you might want to scrape data. Primarily, it makes data collection much faster by eliminating the manual data-gathering process.
Scraping is also a solution when data collection is desired or needed but the website does not provide an API.
In this tutorial, you will build a web scraping application using Node.js and Puppeteer.
Your app will grow in complexity as you progress.
First, you will code your app to open Chromium and load a special website designed as a web-scraping sandbox: books.toscrape.com.
In the next two steps, you will scrape all the books on a single page of books.toscrape and then all the books across multiple pages.
In the remaining steps, you will filter your scraping by book category and then save your data as a JSON file.
Warning: The ethics and legality of web scraping are very complex and constantly evolving.
They also differ based on your location, the data's location, and the website in question.
This tutorial scrapes a special website, books.toscrape.com, which was specifically designed to test scraper applications.
Scraping any other domain falls outside the scope of this tutorial.
This tutorial was tested on Node.js version 12.18.3 and npm version 6.14.6.
You can follow this guide to install Node.js on macOS or Ubuntu 18.04, or you can follow this guide to install Node.js on Ubuntu 18.04 using a PPA.
Step 1 & mdash; Setting Up the Web Scraper
With Node.js installed, you can begin setting up your web scraper.
First, you will create a project root directory and then install the required dependencies.
This tutorial requires just one dependency, and you will install it using Node.js's default package manager npm. npm comes preinstalled with Node.js, so you don "t need to install it.
Create a folder for this project and then move inside:
You will run all subsequent commands from this directory.
We need to install one package using npm, or the node package manager.
First initialize npm in order to create a packages.json file, which will manage your project's dependencies and metadata.
Initialize npm for your project:
npm will present a sequence of prompts.
You can press ENTER to every prompt, or you can add personalized descriptions.
Make sure to press ENTER and leave the default values in place when prompted for entry point: and test command:.
Alternately, you can pass the y flag to npm & mdash; npm init -y & mdash; and it will submit all the default values for you.
Your output will look something like this:
Type yes and press ENTER. npm will save this output as your package.json file.
Now use npm to install Puppeteer:
This command installs both Puppeteer and a version of Chromium that the Puppeteer team knows will work with their API.
On Linux machines, Puppeteer might require some additional dependencies.
If you are using Ubuntu 18.04, check the 'Debian Dependencies' dropdown inside the 'Chrome headless doesn't launch on UNIX' section of Puppeteer's troubleshooting docs.
You can use the following command to help find any missing dependencies:
With npm, Puppeteer, and any additional dependencies installed, your package.json file requires one last configuration before you start coding.
In this tutorial, you will launch your app from the command line with npm run start.
You must add some information about this start script to package.json.
Specifically, you must add one line under the scripts directive regarding your start command.
Open the file in your preferred text editor:
Find the scripts: section and add the following configurations.
Remember to place a comma at the end of the test script line, or your file will not parse correctly.
You will also notice that puppeteer now appears under dependencies near the end of the file.
Your package.json file will not require any more revisions.
Save your changes and close your editor.
You are now ready to start coding your scraper.
In the next step, you will set up a browser instance and test your scraper's basic functionality.
Step 2 & mdash; Setting Up the Browser Instance
When you open a traditional browser, you can do things like click buttons, navigate with your mouse, type, open the dev tools, and more.
A headless browser like Chromium allows you to do these same things, but programmatically and without a user interface.
In this step, you will set up your scraper's browser instance.
When you launch your application, it will automatically open Chromium and navigate to books.toscrape.com.
These initial actions will form the basis of your program.
In this step, you will create all four files and then continually update them as your program grows in sophistication.
Start with browser.js; this file will contain the script that starts your browser.
From your project's root directory, create and open browser.js in a text editor:
First, you will require Puppeteer and then create an async function called startBrowser ().
This function will start the browser and return an instance of it. Add the following code:
This method returns a Promise, so you have to make sure the Promise resolves by using a .then or await block.
You are using await to make sure the Promise resolves, wrapping this instance around a try-catch code block, and then returning an instance of the browser.
Notice that the .launch () method takes a JSON parameter with several values:
headless - false means the browser will run with an Interface so you can watch your script execute, while true means the browser will run in headless mode.
Note well, however, that if you want to deploy your scraper to the cloud, set headless back to true.
Most virtual machines are headless and do not include a user interface, and hence can only run the browser in headless mode.
Puppeteer also includes a headful mode, but that should be used solely for testing purposes.
ignoreHTTPSErrors - true allows you to visit websites that aren't hosted over a secure HTTPS protocol and ignore any HTTPS-related errors.
Now create your second .js file, index.js:
Here you will require browser.js and pageController.js.
You will then call the startBrowser () function and pass the created browser instance to our page controller, which will direct its actions.
Create your third .js file, pageController.js:
pageController.js controls your scraping process.
It uses the browser instance to control the pageScraper.js file, which is where all the scraping scripts execute.
Eventually, you will use it to specify what book category you want to scrape.
For now, however, you just want to make sure that you can open Chromium and navigate to a web page:
This code exports a function that takes in the browser instance and passes it to a function called scrapeAll ().
This function, in turn, passes this instance to pageScraper.scraper () as an argument which uses it to scrape pages.
Finally, create your last .js file, pageScraper.js:
Here you will create an object literal with a url property and a scraper () method.
The url is the web URL of the web page you want to scrape, while the scraper () method contains the code that will perform your actual scraping, although at this stage it merely navigates to a URL.
Puppeteer has a newPage () method that creates a new page instance in the browser, and these page instances can do quite a few things.
In our scraper () method, you created a page instance and then used the page.goto () method to navigate to the books.toscrape.com homepage.
Your program's file-structure is now complete.
The first level of your project's directory tree will look like this:
Now run the command npm run start and watch your scraper application execute:
It will automatically open a Chromium browser instance, open a new page in the browser, and navigate to books.toscrape.com.
In this step, you created a Puppeteer application that opened Chromium and loaded the homepage for a dummy online bookstore & mdash; books.toscrape.com.
In the next step, you will scrape the data for every book on that homepage.
Step 3 & mdash; Scraping Data from a Single Page
Before adding more functionality to your scraper application, open your preferred web browser and manually navigate to the books to scrape homepage.
Browse the site and get a sense of how data is structured.
Books to scrape websites image
You will find a category section on the left and books displayed on the right.
When you click on a book, the browser navigates to a new URL that displays relevant information regarding that particular book.
In this step, you will replicate this behavior, but with code; you will automate the business of navigating the website and consuming its data.
First, if you inspect the source code for the homepage using the Dev Tools inside your browser, you will notice that the page lists each book's data under a section tag.
Inside the section tag every book is under a list (li) tag, and it is here that you find the link to the book's dedicated page, the price, and the in-stock availability.
books.toscrape source code viewed in dev tools
You'll be scraping these book URLs, filtering for books that are in-stock, navigating to each individual book page, and scraping that book's data.
Reopen your pageScraper.js file:
Add the following highlighted content.
You will nest another await block inside await page.goto (this.url);:
In this code block, you called the page.waitForSelector () method.
This waited for the div that contains all the book-related information to be rendered in the DOM, and then you called the page. $$eval () method.
This method gets the URL element with the selector section ol li (be sure that you always return only a string or a number from the page. $eval () and page. $$eval () methods).
Every book has two statuses; a book is either In Stock or Out of stock.
You only want to scrape books that are In Stock.
Because page. $$eval () returns an array of all matching elements, you have filtered this array to ensure that you are only working with in-stock books.
You did this by searching for and evaluating the class .instock.availability.
You then mapped out the href property of the book links and returned it from the method.
Re-run your application:
The browser will open, navigate to the web page, and then close once the task completes.
Now check your console; it will contain all the scraped URLs:
This is a great start, but you want to scrape all the relevant data for a particular book and not only its URL.
You will now use these URLs to open each page and scrape the book's title, author, price, availability, UPC, description, and image URL.
Reopen pageScraper.js:
Add the following code, which will loop through each scraped link, open a new page instance, and then retrieve the relevant data:
You have an array of all URLs.
You want to loop through this array, open up the URL in a new page, scrape data on that page, close that page, and open a new page for the next URL in the array.
Notice that you wrapped this code in a Promise.
This is because you want to be able to wait for each action in your loop to complete.
Therefore each Promise opens a new URL and won't resolve until the program has scraped all the data on the URL, and then that page instance has closed.
Warning: note well that you waited for the Promise using a for-in loop.
Any other loop will be sufficient but avoid iterating over your URL arrays using an array-iteration method like forEach, or any other method that uses a callback function.
This is because the callback function will have to go through the callback queue and event loop first, hence, multiple page instances will open all at once.
This will place a much larger strain on your memory.
Take a closer look at your pagePromise function.
Your scraper first created a new page for each URL, and then you used the page. $eval () function to target selectors for relevant details that you wanted to scrape on the new page.
Some of the texts contain whitespaces, tabs, newlines, and other non-alphanumeric characters, which you stripped off using a regular expression.
You then appended the value for every piece of data scraped in this page to an Object and resolved that object.
Run the script again:
The browser opens the homepage and then opens each book page and logs the scraped data from each of those pages.
This output will print to your console:
In this step, you scraped relevant data for every book on the homepage of books.toscrape.com, but you could add much more functionality.
Each page of books, for instance, is paginated; how do you get books from these other pages?
Also, on the left side of the website you found book categories; what if you don't want all the books, but you just want books from a particular genre?
You will now add these features.
Step 4 & mdash; Scraping Data From Multiple Pages
Pages on books.toscrape.com that are paginated have a next button beneath their content, while pages that are not paginated do not.
You will use the presence of this button to determine if the page is paginated or not.
Since the data on each page is of the same structure and has the same markup, you won't be writing a scraper for every possible page.
Rather, you will use the practice of recursion.
First, you need to change the structure of your code a bit to accommodate recursively navigating to several pages.
Reopen pagescraper.js:
You will add a new function called scrapeCurrentPage () to your scraper () method.
This function will contain all the code that scrapes data from a particular page and then click the next button if it exists.
Add the following highlighted code:
You set the nextButtonExist variable to false initially, and then check if the button exists.
If the next button exists, you set nextButtonExists to true and proceed to click the next button, and then call this function recursively.
If nextButtonExists is false, it returns the scrapedData array as usual.
Run your script again:
This might take a while to complete; your application, after all, is now scraping the data from over 800 books.
Feel free to either close the browser or press CTRL + C to cancel the process.
You have now maximized your scraper's capabilities, but you've created a new problem in the process.
Now the issue is not too little data but too much data. In the next step, you will fine-tune your application to filter your scraping by book category.
Step 5 & mdash; Scraping Data by Category
To scrape data by category, you will need to modify both your pageScraper.js file and your pageController.js file.
Open pageController.js in a text editor:
Call the scraper so that it only scrapes travel books.
You are now passing two parameters into your pageScraper.scraper () method, with the second parameter being the category of books you want to scrape, which in this example is Travel.
But your pageScraper.js file does not recognize this parameter yet.
You will need to adjust this file, too.
Open pageScraper.js:
Add the following code, which will add your category parameter, navigate to that category page, and then begin scraping through the paginated results:
This code block uses the category that you passed in to get the URL where the books of that category reside.
The page. $$eval () can take in arguments by passing the argument as a third parameter to the $$eval () method, and defining it as the third parameter in the callback as such:
This was what you did in your code; you passed the category of books you wanted to scrape, mapped through all the categories to check which one matches, and then returned the URL of this category.
This URL is then used to navigate to the page that displays the category of books you want to scrape using the page.goto (selectedCategory) method.
Run your application again.
You will notice that it navigates to the Travel category, recursively opens books in that category page by page, and logs the results:
In this step, you scraped data across multiple pages and then scraped data across multiple pages from one particular category.
In the final step, you will modify your script to scrape data across multiple categories and then save this scraped data to a stringified JSON file.
Step 6 & mdash; Scraping Data from Multiple Categories and Saving the Data as JSON
In this final step, you will make your script scrape data off of as many categories as you want and then change the manner of your output.
Rather than logging the results, you will save them in a structured file called data.json.
You can quickly add more categories to scrape; doing so requires only one additional line per genre.
Open pageController.js:
Adjust your code to include additional categories.
The example below adds HistoricalFiction and Mystery to our existing Travel category:
Run the script again and watch it scrape data for all three categories:
With the scraper fully-functional, your final step involves saving your data in a more useful format.
You will now store it in a JSON file using the fs module in Node.js.
First, reopen pageController.js:
First, you are requiring Node, js's fs module in pageController.js.
This ensures that you can save your data as a JSON file.
Then you are adding code so that when the scraping completes and the browser closes, the program will create a new file called data.json.
Note that the contents of data.json are stringified JSON.
Therefore, when reading the content of data.json, always parse it as JSON before reusing the data.
You have now built a web-scraping application that scrapes books across multiple categories and then stores your scraped data in a JSON file.
As your application grows in complexity, you might want to store this scraped data in a database or serve it over an API.
How this data is consumed is really up to you.
In this tutorial, you built a web crawler that scraped data across multiple pages recursively and then saved it in a JSON file.
In short, you learned a new way to automate data-gathering from websites.
Puppeteer has quite a lot of features that were not within the scope of this tutorial.
To learn more, check out Using Puppeteer for Easy Control Over Headless Chrome.
You can also visit Puppeteer's official documentation.
How To Use Font Awesome 5 with React
Font Awesome is a toolkit for websites providing icons and social logos.
React is a coding library using JavaScript for creating user interfaces.
While the Font Awesome team have made a React component to promote integration, there are some fundamentals to understand about Font Awesome 5 and how it's structured.
In this tutorial you'll learn the ways to use the React Font Awesome component.
3618
Font Awesome is a toolkit for websites that provides icons and social logos.
React is a coding library that is used for creating user interfaces.
While the Font Awesome team has made a React component to promote integration, there are some fundamentals to understand about Font Awesome 5 and how it is structured.
In this tutorial you'll explore how to use the React Font Awesome component.
Font Awesome website with its icons
No coding is required for this tutorial, but if you are interested in experimenting with some of the examples you will need the following:
Create React App, which you can do by following How To Set Up A React Project.
Step 1 - Using Font Awesome
The Font Awesome team created a React component so you can use the two together.
With this library, you can follow the tutorial after you select your icon.
In this example, we'll use the home icon and do everything in the App.js file:
Your app now has a small home icon.
You'll notice that this code only selects the home icon so that only one icon is added to our bundle size.
code sandbox with home icon showing
Now, Font Awesome will make sure that this component will replace itself with the SVG version of that icon once this component is mounted.
Step 2 - Choosing Icons
Before installing and using the icons, it's important to know how the Font Awesome libraries are structured.
Since there are many icons, the team decided to split them up into multiple packages.
When picking and choosing which icons you want, it's recommend to visit the Font Awesome icons page to explore your options.
You will see different filters to choose from along the left side of the page.
These filters are very important because they will indicate what package to import your icon from.
In the example above, we pulled the home icon out of the @ fortawesome / free-solid-svg-icons package.
Determining which Package an Icon Belongs To
You can figure out which package an icon belongs to by reviewing the filters on the left.
You can also click into an icon and see the package it belongs to.
Once you know which package a font belongs to, it's important to remember the three-letter shorthand for that package:
Solid Style - fas
Regular Style - far
Light Style - fal
Duotone Style - fad
You can search for a specific type from the icons page:
icon page with some of the package names on the left
Using Icons from Specific Packages
If you browse the Font Awesome icons page, you'll notice that there are usually multiple versions of the same icon.
For example, let's take a look at the boxing-glove icon:
boxing glove icon three different versions
In order to use a specific icon, you will need to adjust < FontAwesomeIcon >.
Following are multiple types of the same icon from different packages.
These include three-letter shorthands discussed earlier.
Note: The following examples won't work until we build an icon library in a few sections.
Here is an example of the solid version:
This defaults to solid version if a type is not specified:
And the light version using fal:
We had to switch our icon prop to be an array instead of a simple string.
Step 3 - Installing Font Awesome
Since there are multiple versions of an icon, multiple packages, and free / pro packages, installing them all involves more than one npm package.
You may need to install multiples and then choose the icons you want.
For this article, we'll install everything so we can demonstrate how to install multiple packages.
Run the following command to install the base packages:
Run the following commands to install the regular icons:
These will install the solid icons:
Use this command for light icons:
This will install duotone icons:
Finally, this will install brand icons:
Or, if you prefer to get them all installed in one go, you can use this command to install the free icon sets:
If you have a pro account with Font Awesome, you can use the following to install all of the icons:
You've installed the packages but haven't yet used them in your application or added them to our app bundles yet.
Let's look at how you can do that in the next step.
Step 4 - Creating an Icon Library
It can be tedious to import the icon you want into multiple files.
Let's say you use the Twitter logo in several places, you don't want to have to write that multiple times.
To import everything in one place, instead of importing each icon into each separate file, we'll create a Font Awesome library.
Let's create fontawesome.js in the src folder and then import that into index.js.
Feel free to add this file wherever as long as the components you want to use the icons in have access (are child components).
You could even do this right in your index.js or App.js.
However, it can be better to move this out to a separate file since it can get large:
If you did this in its own file, then you'll need to import into index.js:
Importing an Entire Icon Package
It isn't recommended to import an entire package because you're importing every single icon into your app which could cause a large bundle size.
If you do need to important an entire package, you certainly can.
In this example, let's say you wanted all the brand icons in @ fortawesome / free-brands-svg-icons.
You would use the following to import the entire package:
fab represents the entire brand icon package.
Importing Icons Individually
The recommended way to use Font Awesome icons is to import them one by one so that your final bundle sizes are as small as possible, as you will only import what you need.
You can create a library from multiple icons from the different packages like so:
Importing the Same Icon from Multiple Styles
If you want all the types of boxing-glove for the fal, far, and fas packages, you can import them all as a different name and then add them.
You can then use them by implementing the different prefixes:
Step 5 - Using Icons
Now that you have installed what you need and have added your icons to your Font Awesome library, you are ready to use them and assign sizes.
In this tutorial, we'll use the light (fal) package.
This first example will use the normal size:
The second example can use named sizing, which uses abbreviations for small (sm), medium (md), large (lg), and extra-large (xl):
The third option is to use numbered sizing which can go up to 6:
When using numbered sizing, you can also use decimals to find the perfect size:
Font Awesome styles the SVGs it uses by taking the text-color of the CSS.
If you were to place a < p > tag where this icon were to go, the color of the paragraph would be the color of the icon:
Font Awesome also has a power transforms feature where you can string together different transforms:
You can use any of the transforms found on the Font Awesome site.
You can use this to move icons up, down, left, or right to get the positioning perfect next to text or inside of buttons.
Fixed Width Icons
When using icons in a spot where they all need to be the same width and uniform, Font Awesome lets us use the fixedWidth prop.
For instance, let's say that you need fixed widths for your navigation dropdown:
Scotch website with menu dropdown and "Courses" highlighted
Spinning Icons
Spinning is useful to implement on form buttons when a form is processing.
You can use the spinner icon to make a nice loading effect:
You can use the spin prop on anything!
Advanced: Masking Icons
Font Awesome allows you to combine two icons to make effects with masking.
You define your normal icon and then use the mask prop to define a second icon to lay on top.
The first icon will be constrained within the masking icon.
In this example, we created tag filters using masking:
Tag filters with Font Awesome
Notice how you can chain together multiple transform props to move the inner icon to fit inside the masking icon.
We even colorize and change out the background logo with Font Awesome:
The Tag filters again, but now with a blue background
Step 6 - Using react-fontawesome and Icons Outside of React
If your entire site isn't a single-page application (SPA), and instead you have a traditional site and have added React on top.
To avoid importing the main SVG / JS library and also the react-fontawesome library, Font Awesome has created a way to use the React libraries to watch for icons outside of React components.
If you have any < i class = "fas fa-stroopwafel" > < / i >, we can tell Font Awesome to watch and update those using the following:
MutationObserver's are a web technology that allow us to watch the DOM for changes performantly.
Find out more about this technique on the React Font Awesome docs.
Using Font Awesome and React together is a great pairing, but creates the need to use multiple packages and consider different combinations.
In this tutorial you explored some of the ways you can use Font Awesome and React together.
