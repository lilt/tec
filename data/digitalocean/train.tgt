Hinzufügen von Authentifizierung zu Ihrer App mit Flask-Login
Die Möglichkeit für Benutzer, sich bei Ihrer Anwendung anzumelden, ist eine der häufigsten Funktionen, die Sie Ihrer Webanwendung hinzufügen werden.
Dieser Artikel behandelt, wie Sie Ihrer Flask-Anwendung mit dem Paket Flask-Login Authentifizierung hinzufügen können.
3576
Animiertes gif der Flask-Anwendung und des Anmeldefelds
Wir werden einige Registrierungs- und Anmeldeseiten erstellen, die Benutzern die Anmeldung und den Zugriff auf geschützte Seiten ermöglichen, die nicht angemeldete Benutzer nicht sehen können.
Wir werden Informationen aus dem Benutzermodell nehmen und sie auf unseren geschützten Seiten anzeigen, wenn sich der Benutzer anmeldet, um zu simulieren, wie ein Profil aussehen würde.
In diesem Artikel behandeln wir die folgenden Punkte:
Verwenden der Bibliothek Flask-Login für die Sitzungsverwaltung.
Verwenden des integrierten Dienstprogramms Flask für das Hashing von Passwörtern.
Hinzufügen von geschützten Seiten zu unserer Anwendung nur für angemeldete Benutzer.
Verwenden von Flask-SQLAlchemy zum Erstellen eines Benutzermodells.
Erstellen von Registrierungs- und Anmeldeformularen für unsere Benutzer zur Erstellung von Konten und zum Anmelden.
Zurückgeben von Fehlermeldungen an unsere Benutzer, wenn etwas schiefgeht.
Verwenden von Informationen aus dem Konto des Benutzers zur Anzeige auf der Profilseite.
Der Quellcode für dieses Projekt ist auf GitHub verfügbar.
In einer lokalen Umgebung installiertes Python.
Kenntnisse über grundlegende Linux-Navigation und -Dateiverwaltung sind hilfreich, aber nicht erforderlich.
Vertrautheit mit einem Editor wie Visual Studio Code ist hilfreich, aber nicht erforderlich.
Unsere Anwendung wird das Flask Application Factory-Muster mit Blueprints verwenden.
Wir werden eine Blueprint haben, die alles im Zusammenhang mit auth (Authentifizierung) behandelt, und eine weitere für unsere regulären Routen, die den Index und die geschützte Profilseite enthalten.
In einer echten Anwendung können Sie die Funktionalität nach Belieben aufschlüsseln, aber die hier behandelte Lösung wird für dieses Tutorial gut funktionieren.
Hier ist ein Diagramm, das einen Eindruck davon vermittelt, wie die Dateistruktur Ihres Projekts nach Abschluss des Tutorials aussehen wird:
Im weiteren Verlauf des Tutorials werden wir diese Verzeichnisse und Dateien erstellen.
Schritt 1 - Installieren der Pakete
Es gibt drei Hauptpakete, die wir für unser Projekt benötigen:
Flask
Flask-Login: zur Handhabung der Benutzersitzungen nach der Authentifizierung
Flask-SQLAlchemy: zur Darstellung des Benutzermodells und als Schnittstelle zu unserer Datenbank
Wir verwenden SQLite, um keine zusätzlichen Abhängigkeiten für die Datenbank installieren zu müssen.
Zuerst beginnen wir mit der Erstellung des Projektverzeichnisses:
Als Nächstes müssen wir zum Projektverzeichnis navigieren:
Falls Sie keine Python-Umgebung haben, werden Sie eine erstellen wollen.
Je nachdem, wie Python auf Ihrem Rechner installiert wurde, werden Ihre Befehle in etwa so aussehen:
Anmerkung: Sie können das für Ihre lokale Umgebung relevante Tutorial zur Einrichtung von venv konsultieren.
Führen Sie die folgenden Befehle aus Ihrer virtuellen Umgebung aus, um die erforderlichen Pakete zu installieren:
Nachdem Sie die Pakete installiert haben, können Sie nun die Hauptanwendungsdatei erstellen.
Schritt 2 - Erstellen der Hauptanwendungsdatei
Beginnen wir mit der Erstellung eines Verzeichnisses project:
Die erste Datei, an der wir arbeiten, wird die Datei _ _ init _ _ .py für unser Projekt sein:
Diese Datei wird die Funktion zum Erstellen unserer Anwendung haben, die die Datenbank initialisiert und unsere Blueprints registriert.
Im Moment bringt das nicht viel, aber es wird für den Rest unserer Anwendung benötigt. Wir müssen SQLAlchemy initialisieren, einige Konfigurationswerte festlegen und unsere Blueprints registrieren.
Nachdem wir nun die Hauptanwendungsdatei haben, können wir mit dem Hinzufügen unserer Routen beginnen.
Schritt 3 - Hinzufügen von Routen
Für unsere Routen verwenden wir zwei Blueprints.
Für unsere Haupt-Blueprint haben wir eine Startseite (/) und eine Profilseite (/ profile) für nach der Anmeldung.
Versucht der Benutzer, ohne angemeldet zu sein, auf die Profilseite zuzugreifen, wird er auf die Anmeldungsroute umgeleitet.
Für unsere Auth-Blueprint haben wir Routen, um sowohl die Anmeldeseite (/ login) als auch die Registrierungsseite (/ sign-up) abzurufen.
Außerdem haben wir Routen zur Bearbeitung der POST-Anfragen von beiden Routen.
Schließlich haben wir eine Abmeldungsroute (/ logout), um einen aktiven Benutzer abzumelden.
Bis auf Weiteres definieren wir login, signup und logout mit einfachen Rückgaben.
Wir werden sie in einem späteren Schritt erneut besuchen und mit der gewünschten Funktionalität aktualisieren.
Erstellen Sie zunächst main.py für Ihre main _ blueprint:
Erstellen Sie als Nächstes auth.py für Ihre auth _ blueprint:
In einem Terminal können Sie die Werte FLASK _ APP und FLASK _ DEBUG festlegen:
Die Umgebungsvariable FLASK _ APP weist Flask an, wie die Anwendung geladen werden soll. Sie sollte darauf zeigen, wo sich create _ app befindet.
Für unsere Bedürfnisse verweisen wir auf das Projektverzeichnis.
Die Umgebungsvariable FLASK _ DEBUG wird durch Einstellung auf 1 aktiviert. Dadurch wird ein Debugger aktiviert, der Anwendungsfehler im Browser anzeigt.
Stellen Sie sicher, dass Sie sich im Verzeichnis < ^ > flask _ auth _ app < ^ > befinden, und führen Sie dann das Projekt aus:
Nun sollten Sie in einem Webbrowser in der Lage sein, zu den fünf möglichen URLs zu navigieren und den zurückgegebenen Text zu sehen, der in auth.py und main.py definiert wurde.
Wenn Sie beispielsweise localhost: 5000 / profile besuchen, wird angezeigt: Profile:
Screenshot des Projekts auf localhost Port 5000 im Browser
Nachdem wir nun verifiziert haben, dass sich unsere Routen wie erwartet verhalten, können wir mit der Erstellung von Vorlagen fortfahren.
Schritt 4 - Erstellen von Vorlagen
Fahren wir damit fort, die Vorlagen zu erstellen, die in unserer Anwendung verwendet werden. Dies ist der erste Schritt, bevor wir die tatsächliche Anmeldefunktionalität implementieren können.
Unsere Anwendung verwendet vier Vorlagen:
index.html
profile.html
login.html
signup.html
Außerdem haben wir eine Basisvorlage, die einen gemeinsamen Code für jede der Seiten hat.
In diesem Fall wird die Basisvorlage Navigationslinks und das allgemeine Layout der Seite enthalten.
Lassen Sie uns diese jetzt erstellen.
Erstellen Sie zunächst ein Verzeichnis templates in dem Verzeichnis project:
Erstellen Sie dann base.html:
Fügen Sie als Nächstes den folgenden Code in die Datei base.html ein:
Dieser Code erstellt eine Reihe von Menülinks zu jeder Seite der Anwendung und einen Bereich, in dem der Inhalt angezeigt wird.
Anmerkung: Hinter den Kulissen verwenden wir Bulma, für das Styling und Layout.
Für einen tieferen Einblick in Bulma sollten Sie die offizielle Bulma-Dokumentation lesen.
Erstellen Sie als Nächstes templates / index.html:
Fügen Sie der neu erstellten Datei den folgenden Code hinzu, um Inhalte zu der Seite hinzuzufügen:
Dieser Code erstellt eine grundlegende Indexseite mit einem Titel und einem Untertitel.
Erstellen Sie als Nächstes templates / login.html:
Dieser Code generiert eine Anmeldeseite mit Feldern für E-Mail und Passwort.
Es gibt auch ein Kontrollkästchen zur "Erinnerung" an eine angemeldete Sitzung.
Erstellen Sie als Nächstes templates / signup.html:
Fügen Sie den folgenden Code hinzu, um eine Registrierungsseite mit Feldern für E-Mail, Namen und Passwort zu erstellen:
Erstellen Sie als Nächstes templates / profile.html:
Fügen Sie diesen Code hinzu, um eine einfache Seite mit einem Titel zu erstellen, der fest programmiert ist, um Anthony willkommen zu heißen:
Später fügen wir Code hinzu, um jeden Benutzer dynamisch zu begrüßen.
Sobald Sie die Vorlagen hinzugefügt haben, können wir die Return-Anweisungen in jeder der uns zur Verfügung stehenden Routen aktualisieren, um die Vorlage anstelle des Textes zurückzugeben.
Aktualisieren Sie als Nächstes main.py durch Ändern der Importzeile und der Routen für index und profile:
Jetzt aktualisieren Sie auth.py durch Ändern der Importzeile und Routen für login und signup:
Sobald Sie diese Änderungen vorgenommen haben, sieht die Registrierungsseite wie folgt aus, wenn Sie zu / sign-up navigieren:
Registrierungsseite unter / signup
Sie sollten auch die Seiten für /, / login und / profile sehen können.
Wir lassen / logout vorerst in Ruhe, da es keine Vorlage anzeigt, wenn wir fertig sind.
Schritt 5 - Erstellen von Benutzermodellen
Unser Benutzermodell stellt dar, was es für unsere Anwendung bedeutet, einen Benutzer zu haben.
Wir haben Felder für eine E-Mail-Adresse, ein Passwort und einen Namen.
In Ihrer Anwendung können Sie entscheiden, ob Sie mehr Informationen pro Benutzer speichern möchten.
Sie können Dinge wie Geburtstag, Profilbild, Ort oder beliebige Benutzereinstellungen hinzufügen.
In Flask-SQLAlchemy erstellte Modelle werden durch Klassen dargestellt, die dann in Tabellen in einer Datenbank übersetzt werden.
Die Attribute dieser Klassen werden dann zu Spalten für diese Tabellen.
Lassen Sie uns fortfahren und dieses Benutzermodell erstellen:
Dieser Code erstellt ein Benutzermodell mit Spalten für id, email, password und name:
Nachdem Sie ein Benutzermodell erstellt haben, können Sie nun mit der Konfiguration Ihrer Datenbank fortfahren.
Schritt 6 - Konfigurieren der Datenbank
Wie in den Voraussetzungen angegeben, verwenden wir eine SQLite-Datenbank.
Wir könnten selbst eine SQLite-Datenbank erstellen, aber lassen wir das Flask SQLAlchemy für uns erledigen:
Wir haben den Pfad der Datenbank bereits in der Datei _ _ init _ _ .py angegeben. Daher müssen wir Flask-SQLAlchemy nur anweisen, die Datenbank in der Python REPL zu erstellen.
Wenn Sie Ihre Anwendung stoppen und eine Python REPL öffnen, können wir die Datenbank mit der Methode create _ all auf dem Objekt db erstellen.
Stellen Sie sicher, dass Sie sich immer noch in der virtuellen Umgebung und im Verzeichnis < ^ > flask _ auth _ app < ^ > befinden.
Anmerkung: Wenn die Verwendung des Python-Interpreters für Sie neu ist, können Sie die offizielle Dokumentation konsultieren.
In Ihrem Projektverzeichnis sehen Sie nun eine Datei db.sqlite.
Diese Datenbank wird unsere Benutzertabelle enthalten.
Schritt 7 - Einrichten der Autorisierungsfunktion
Für unsere Registrierungsfunktion nehmen wir die Daten, die der Benutzer in das Formular eingibt, und und fügen sie unserer Datenbank hinzu.
Bevor wir sie hinzufügen, müssen wir sicherstellen, dass der Benutzer nicht bereits in der Datenbank vorhanden ist.
Wenn dies nicht der Fall ist, müssen wir sicherstellen, dass wir das Passwort vor dem Hinzufügen in die Datenbank hashen, da wir unsere Passwörter nicht in Klartext speichern möchten.
Beginnen wir mit dem Hinzufügen einer zweiten Funktion zur Verarbeitung der POST-Formulardaten. In dieser Funktion werden wir zuerst die vom Benutzer übergebenen Daten sammeln.
Erstellen Sie die Funktion und fügen Sie ein redirect am Ende hinzu.
Dies bietet dem Benutzer die Erfahrung einer erfolgreichen Registrierung und die Weiterleitung zu der Anmeldeseite.
Aktualisieren Sie auth.py durch Ändern der Importzeile und Implementieren von signup _ post:
Fügen wir nun den Rest des Codes hinzu, der für die Registrierung eines Benutzers erforderlich ist.
Zuerst müssen wir das request-Objekt verwenden, um die Formulardaten zu erhalten.
Fahren Sie mit der Aktualisierung von auth.py fort, indem Sie Importe hinzufügen und signup _ post implementieren:
Anmerkung: Das Speichern von Passwörtern in Klartext wird als schlechte Sicherheitspraxis angesehen.
In der Regel möchten Sie einen komplexen Hash-Algorithmus und ein Passwort-Salt verwenden, um Passwörter sicher zu halten.
Schritt 8 - Testen der Registrierungsmethode
Nachdem wir nun die Registrierungsmethode abgeschlossen haben, sollten wir in der Lage sein, einen neuen Benutzer zu erstellen.
Verwenden Sie das Formular, um einen Benutzer zu erstellen.
Es gibt zwei Möglichkeiten, wie Sie überprüfen können, ob die Registrierung funktioniert: Sie können einen Datenbankbetrachter verwenden, um die Zeile anzuzeigen, die Ihrer Tabelle hinzugefügt wurde, oder Sie können versuchen, sich mit der gleichen E-Mail-Adresse erneut zu registrieren. Wenn Sie einen Fehler erhalten, wissen Sie, dass die erste E-Mail korrekt gespeichert wurde.
Lassen Sie uns also diesen Ansatz wählen.
Wir können Code hinzufügen, um dem Benutzer mitzuteilen, dass die E-Mail bereits existiert und ihm sagen, dass er zur Anmeldeseite gehen soll.
Durch Aufruf der Funktion flash senden wir eine Nachricht an die nächste Anfrage, die in diesem Fall die Weiterleitung "redirect" ist.
Die Seite, auf der wir landen, wird dann Zugriff auf diese Nachricht in der Vorlage haben.
Zuerst fügen wir die Funktion flash hinzu, bevor wir zurück zu unserer Registrierungsseite umleiten.
Um die geflashte Nachricht in der Vorlage zu erhalten, können wir diesen Code oberhalb des Formulars hinzufügen. Dadurch wird die Nachricht direkt über dem Formular angezeigt.
Registrierungsfeld, das eine Nachricht anzeigt "E-Mail-Adresse bereits vorhanden.
Gehen Sie zur Anmeldeseite "in einem dunkel-rosa Feld
Schritt 9 - Hinzufügen der Anmeldemethode
Die Anmeldemethode ähnelt der Registrierungsfunktion insofern, als dass wir die Benutzerinformationen nehmen und etwas damit tun. In diesem Fall vergleichen wir die eingegebene E-Mail-Adresse, um zu sehen, ob sie in der Datenbank enthalten ist.
Wenn dies der Fall ist, testen wir das vom Benutzer bereitgestellte Passwort, indem wir das vom Benutzer eingegebene Passwort hashen und es mit dem gehashten Passwort in der Datenbank vergleichen.
Wenn beide gehaschten Passwörter übereinstimmen, wissen wir, dass der Benutzer das korrekte Passwort eingegeben hat.
Sobald der Benutzer die Passwort-Überprüfung bestanden hat, wissen wir, dass er die richtigen Anmeldedaten hat und wir können in mit Flask-Login anmelden.
Durch den Aufruf von login _ user erstellt Flask-Login eine Sitzung für diesen Benutzer, die bestehen bleibt, während der Benutzer angemeldet bleibt, wodurch der Benutzer geschützte Seiten einsehen kann.
Wir können mit einer neuen Route für den Umgang mit den gePOSTeten Dateien beginnen. Wenn sich der Benutzer erfolgreich anmeldet, leiten wir zur Profilseite weiter.
Nun müssen wir überprüfen, ob der Benutzer die richtigen Anmeldedaten hat:
Fügen wir den Block in der Vorlage hinzu, damit der Benutzer die geflashte Nachricht sehen kann.
Wie bei dem Registrierungsformular fügen wir die potenzielle Fehlermeldung direkt über dem Formular hinzu:
Wir haben nun die Möglichkeit zu sagen, dass ein Benutzer erfolgreich angemeldet wurde, aber es ist nichts vorhanden, wo der Benutzer protokolliert werden kann.
Hier bringen wir Flask-Login zur Verwaltung von Benutzersitzungen ein.
Bevor wir beginnen, benötigen wir einige Dinge, damit Flask-Login funktioniert.
Beginnen Sie mit dem Hinzufügen des UserMixin zu Ihrem Benutzermodell.
Das UserMixin fügt dem Modell Flask-Login-Attribute hinzu, sodass Flask-Login damit arbeiten kann.
Dann müssen wir unseren User-Loader angeben.
Ein User-Loader teilt Flask-Login mit, wie ein bestimmter Benutzer anhand der in seinem Sitzungs-Cookie gespeicherten ID gefunden werden kann.
Wir können dies in unserer Funktion create _ app zusammen mit dem Code init für Flask-Login hinzufügen:
Schließlich können wir die Funktion login _ user hinzufügen, kurz bevor wir auf die Profilseite zur Erstellung der Sitzung weiterleiten:
Mit der Einrichtung von Flask-Login können wir die Route / login verwenden.
Wenn alles eingerichtet ist, sehen Sie die Profilseite.
Profilseite mit "Willkommen, Anthony!"
Schritt 10 - Schützen von Seiten
Wenn Ihr Name nicht auch Anthony ist, werden Sie feststellen, dass Ihr Name falsch ist.
Wir möchten, dass das Profil den Namen in der Datenbank anzeigt.
Zuerst müssen wir also die Seite schützen und dann auf die Daten des Benutzers zugreifen, um den Namen zu erhalten.
Um eine Seite bei der Verwendung von Flask-Login zu schützen, fügen wir den Dekorator @ login _ requried zwischen der Route und der Funktion hinzu.
Dadurch wird verhindert, dass ein nicht angemeldeter Benutzer die Route sehen kann.
Wenn der Benutzer nicht angemeldet ist, wird der Benutzer gemäß der Flask-Login-Konfiguration auf die Anmeldeseite weitergeleitet.
Bei Routen, die mit dem Dekorator @ login _ required versehen sind, haben wir dann die Möglichkeit, das Objekt current _ user innerhalb der Funktion zu verwenden.
Dieser current _ user stellt den Benutzer aus der Datenbank dar und wir können mit der Punktnotation auf alle Attribute dieses Benutzers zugreifen.
Beispielsweise geben current _ user.email, current _ user.password und current _ user.name sowie current _ user.id die tatsächlichen Werte zurück, die in der Datenbank für den angemeldeten Benutzer gespeichert sind.
Verwenden wir den Namen des aktuellen Benutzers und senden ihn an die Vorlage.
Dann verwenden wir diesen Namen und zeigen seinen Wert an.
Aktualisieren Sie dann in der Datei profile.html die Seite, um den Wert name anzuzeigen:
Sobald wir zu unserer Profilseite gehen, sehen wir, dass der Name des Benutzers angezeigt wird.
Begrüßungsseite für den Benutzer mit dem Namen des aktuell angemeldeten Benutzers
Als Letztes können wir die Abmelde-Ansicht aktualisieren.
Wir können die Funktion logout _ user in einer Route zur Abmeldung aufrufen. Wir haben den Dekorator @ login _ required, weil es keinen Sinn macht, einen Benutzer abzumelden, der nicht zuerst angemeldet ist.
Nachdem wir uns abgemeldet und versucht haben, die Profilseite erneut anzuzeigen, wird eine Fehlermeldung angezeigt.
Dies liegt daran, dass Flask-Login eine Nachricht für uns anzeigt, wenn der Benutzer nicht auf eine Seite zugreifen darf.
Anmeldeseite mit einer Nachricht, die zeigt, dass sich der Benutzer anmelden muss, um auf die Seite zuzugreifen.
Eine letzte Sache, die wir tun können, ist, if-Anweisungen in die Vorlage aufzunehmen, um nur die für den Benutzer relevanten Links anzuzeigen.
Bevor sich der Benutzer also anmeldet, hat er die Möglichkeit, sich anzumelden oder zu registrieren.
Nach der Anmeldung können Benutzer zu ihrem Profil gehen oder sich abmelden:
Startseite mit der Navigation Start, Anmeldung und Registrierung oben auf dem Bildschirm
Damit haben Sie Ihre Anwendung mit Authentifizierung erfolgreich erstellt.
Wir haben Flask-Login und Flask-SQLAlchemy verwendet, um ein Anmeldesystem für unsere Anwendung zu erstellen. Wir haben behandelt, wie ein Benutzer authentifiziert werden kann, indem wir zuerst ein Benutzermodell erstellen und die Benutzerinformationen speichern.
Dann mussten wir verifizieren, dass das Passwort des Benutzers korrekt war, indem wir das Passwort aus dem Formular gehasht und mit dem in der Datenbank gespeicherten Benutzer verglichen haben.
Schließlich haben wir unserer Anwendung Autorisierung hinzugefügt, indem wir den Dekorator @ login _ required auf einer Profilseite verwenden, damit nur angemeldete Benutzer diese Seite sehen können.
Was wir in diesem Tutorial erstellt haben, wird für kleinere Anwendungen ausreichen. Wenn Sie jedoch von Anfang an mehr Funktionalität haben möchten, sollten Sie möglicherweise die Bibliotheken Flask-User oder Flask-Security verwenden, die beide auf der Bibliothek Flask-Login aufbauen.
Formatieren von Code mit Prettier in Visual Studio Code
Die konsistente Formatierung von Code ist mühsam, doch ermöglichen moderne Entwicklungswerkzeuge die Konsistenz in der gesamten Codebasis Ihres Teams automatisch zu gewährleisten.
3544
Die konsistente Formatierung von Code ist eine Herausforderung, doch ermöglichen moderne Entwicklungswerkzeuge die Konsistenz in der gesamten Codebasis Ihres Teams automatisch zu gewährleisten.
In diesem Artikel richten Sie Prettier ein, um Ihren Code automatisch in Visual Studio Code, auch als VS-Code bekannt, zu formatieren.
Zu Demonstrationszwecken finden Sie hier den Beispielcode, den Sie formatieren werden:
Wenn Sie mit der Codeformatierung vertraut sind, werden Ihnen vielleicht einige Fehler auffallen:
Eine Mischung aus einzelnen und doppelten Anführungszeichen.
Die erste Eigenschaft des Objekts person sollte in einer eigenen Zeile stehen.
Die Konsolenanweisung innerhalb der Funktion sollte eingerückt sein.
Die optionalen Klammern, die den Parameter der Pfeilfunktion umgeben, können Ihnen gefallen oder auch nicht.
Um diesem Tutorial zu folgen, müssen Sie Visual Studio Code herunterladen und installieren.
Um mit Prettier in Visual Studio Code arbeiten zu können, müssen Sie die Erweiterung installieren.
Suchen Sie dazu im Erweiterungsfeld von VS-Code nach Prettier - Code Formatter.
Prettier-Erweiterung "readme"
Schritt 1 - Verwenden des Befehls "Format Document"
Nachdem die Prettier-Erweiterung installiert ist, können Sie sie nun zur Formatierung Ihres Codes nutzen.
Beginnen wir damit, den Befehl Format Document zu verwenden.
Dieser Befehl macht Ihren Code konsistenter in Bezug auf formatierte Abstände, Zeilenumbrüche und Anführungszeichen.
Suchen Sie in der Befehlspalette format und wählen Sie dann Format Document.
Geöffnete Befehlspalette mit Ergebnissen für "format".
Möglicherweise werden Sie aufgefordert, das zu verwendende Format zu wählen.
Klicken Sie dazu auf die Schaltfläche Configure:
Eingabeaufforderung zur Auswahl eines Standard-Formatierers
Wählen Sie dann Prettier - Code Formatter aus.
Auswahl von Prettier
Anmerkung: Wenn Sie keine Eingabeaufforderung zur Auswahl eines Standard-Formats sehen, können Sie dies manuell in Ihren Einstellungen ändern.
Setzten Sie Editor: Default Formatter auf ebsenp.prettier-vscode.
Ihr Code ist nun mit Abständen, Zeilenumbrüchen und konsistenten Anführungszeichen formatiert:
Dies funktioniert auch bei CSS-Dateien.
Sie können Code mit inkonsistenten Einrückungen, geschweiften Klammern, Zeilenumbrüchen und Semikolons in gut formatierten Code umwandeln.
Wird neu formatiert als:
Nachdem wir diesen Befehl nun erkundet haben, sehen wir uns an, wie dies zur automatischen Ausführung implementiert werden kann.
Schritt 2 - Formatieren von Code beim Speichern
Bislang mussten Sie einen Befehl zur Formatierung Ihres Codes manuell ausführen.
Um diesen Vorgang zu automatisieren, können Sie eine Einstellung in VS-Code wählen, damit Ihre Dateien beim Speichern automatisch formatiert werden.
Dadurch wird auch sichergestellt, dass nicht formatierter Code nicht an die Versionskontrolle übergeben wird.
Sobald das Menü geöffnet ist, suchen Sie nach Editor: Format On Save und stellen Sie sicher, dass diese Option aktiviert ist.
Editor: Format On Save aktiviert
Sobald dies eingestellt ist, können Sie Ihren Code wie gewohnt schreiben, und er wird automatisch formatiert, wenn Sie die Datei speichern.
Schritt 3 - Ändern der Konfigurationseinstellungen von Prettier
Prettier erledigt standardmäßig viele Dinge für Sie, aber Sie können die Einstellungen auch anpassen.
Öffnen Sie das Menü Settings.
Suchen Sie dann nach Prettier.
Daraufhin werden alle Einstellungen angezeigt, die Sie ändern können:
Konfigurationseinstellungen für Prettier
Hier sehen Sie einige der gebräuchlichsten Einstellungen:
Single Quote - Wählen Sie zwischen einfachen und doppelten Anführungszeichen.
Semi - Wählen Sie, ob Semikolons am Ende von Zeilen eingefügt werden sollen oder nicht.
Tab Width - Geben Sie an, wie viele Leerzeichen ein Tabulator einfügen soll.
Der Nachteil der Verwendung des integrierten Einstellungsmenüs in VS-Code ist, dass es keine Konsistenz zwischen den Entwicklern in Ihrem Team gewährleistet.
Schritt 4 - Erstellen einer Prettier-Konfigurationsdatei
Wenn Sie die Einstellungen in Ihrem VS-Code ändern, könnte eine andere Person auf ihrem Rechner eine völlig andere Konfiguration haben.
Sie können eine konsistente Formatierung in Ihrem Team etablieren, indem Sie eine Konfigurationsdatei für Ihr Projekt erstellen.
Erstellen Sie eine neue Datei namens .prettierrc. < ^ > extension < ^ > mit einer der folgenden Erweiterungen:
yml
yaml
json
js
toml
Hier ist ein Beispiel einer einfachen Konfigurationsdatei mit JSON:
Weitere Einzelheiten zu den Konfigurationsdateien finden Sie in den Dokumentationen zu Prettier.
Nachdem Sie eine dieser Dateien erstellt und Ihrem Projekt hinzugefügt haben, können Sie sicherstellen, dass jedes Teammitglied die gleichen Formatierungsregeln befolgt.
Ein einheitlicher konsistenter Code ist eine gute Praxis.
Sie ist besonders dann von Vorteil, wenn mit mehreren Mitarbeitern an einem Projekt gearbeitet wird.
Die Einigung auf eine Reihe von Konfigurationen hilft bei der Lesbarkeit und dem Verständnis des Codes.
Der Lösung anspruchsvoller technischer Probleme kann mehr Zeit gewidmet werden, anstatt mit gelösten Problemen wie dem Einrücken des Codes zu ringen.
Prettier sorgt für Konsistenz bei der Formatierung Ihres Codes und automatisiert den Vorgang.
Zugriff auf Front- und Rück-Kameras mit getUserMedia () von JavaScript
Mit HTML5 kam die Einführung von APIs mit Zugriff auf die Geräte-Hardware, einschließlich der API MediaDevices.
Diese API ermöglicht den Zugriff auf Medieneingabegeräte wie Audio und Video.
In diesem Tutorial erfahren Sie, wie Sie Zugriff auf die Video-Feeds einer Kamera eines Benutzergeräts erhalten.
3579
Mit Hilfe dieser API können Entwickler auf Audio- und Videogeräte zugreifen, um Live-Video-Feeds zu streamen und im Browser anzuzeigen.
In diesem Tutorial greifen Sie über das Gerät des Benutzers auf den Video-Feed zu und zeigen es mit der Methode getUserMedia im Browser an.
Die API getUserMedia verwendet die Medieneingabegeräte, um einen Medien-Stream zu erstellen.
Dieser Medien-Stream enthält die angeforderten Medientypen, egal ob Audio oder Video.
Mit dem von der API zurückgegebenen Stream können Video-Feeds im Browser angezeigt werden, wobei dies für die Echtzeit-Kommunikation im Browser nützlich ist.
Bei gemeinsamer Verwendung mit der API MediaStream Recording können Sie Mediendaten aufzeichnen und speichern, die im Browser erfasst wurden.
Diese API funktioniert wie die übrigen neu eingeführten APIs nur mit sicherer Herkunft, aber sie funktioniert auch für localhost- und Datei-URLs.
Grundkenntnisse in JavaScript.
Wenn JavaScript neu für Sie ist, probiere Sie die Reihe Codieren in JavaScript aus.
In diesem Tutorial werden zunächst Konzepte erklärt und Beispiele mit Codepen demonstriert.
Im letzten Schritt erstellen Sie einen funktionierenden Video-Feed für den Browser.
Schritt 1 - Überprüfen der Geräteunterstützung
Zuerst erfahren Sie, wie Sie überprüfen können, ob der Browser des Benutzers die API mediaDevices unterstützt.
Diese API ist innerhalb der Navigator-Schnittstelle vorhanden und enthält den aktuellen Status und die Identität des Benutzeragenten.
Die Überprüfung wird mit dem folgenden Code durchgeführt, der in Codepen eingefügt werden kann:
Zuerst wird geprüft, ob die API mediaDevices innerhalb des navigator vorhanden ist und dann wird geprüft, ob die API getUserMedia innerhalb der mediaDevices verfügbar ist.
Wenn dies true zurückgibt, können Sie beginnen.
Schritt 2 - Anfordern der Benutzerberechtigung
Nachdem Sie die Unterstützung des Browsers für getUserMedia bestätigt haben, müssen Sie die Berechtigung zur Verwendung der Medieneingabegeräte auf dem Benutzeragenten anfordern.
Normalerweise wird, nachdem ein Benutzer die Berechtigung erteilt hat, ein Promise zurückgegeben, das zu einem Medien-Stream aufgelöst wird.
Dieses Promise wird nicht zurückgegeben, wenn die Berechtigung vom Benutzer verweigert wird, wodurch der Zugriff auf diese Geräte blockiert wird.
Fügen Sie die folgende Zeile in Codepen ein, um die Berechtigung anzufordern:
Das Objekt, das als Argument für die Methode getUserMedia bereitgestellt wurde, wird als constraints (Beschränkungen) bezeichnet.
Dadurch wird festgelegt, für welche der Medieneingabegeräte Sie Zugriffsberechtigungen anfordern.
Wenn das Objekt beispielsweise audio: true enthält, wird der Benutzer aufgefordert, Zugriff auf das Audio-Eingabegerät zu gewähren.
Schritt 3 - Verstehen von Medienbeschränkungen
Dieser Abschnitt behandelt das allgemeine Konzept von constraints.
Das Objekt constraints ist ein MediaStreamConstraints-Objekt, das die anzufordernden Medientypen und die Anforderungen jedes Medientyps angibt.
Mit dem Objekt constraints können Sie Anforderungen für den angeforderten Stream angeben, wie beispielsweise die Auflösung des zu verwendenden Streams (front, back).
Sie müssen entweder audio oder video angeben, wenn Sie die Anfrage stellen.
Ein NotFoundError wird zurückgegeben, wenn die angeforderten Medientypen im Browser des Benutzers nicht gefunden werden können.
Wenn Sie beabsichtigen, einen Video-Stream mit einer Auflösung von 1280 × 720 anzufordern, können Sie das Objekt constraints so aktualisieren, dass es wie folgt aussieht:
Mit dieser Aktualisierung versucht der Browser, die angegebenen Qualitätseinstellungen für den Stream zu übernehmen.
Wenn das Videogerät diese Auflösung nicht liefern kann, gibt der Browser andere verfügbare Auflösungen zurück.
Um sicherzustellen, dass der Browser eine Auflösung zurückgibt, die nicht niedriger als die vorgegebene Auflösung ist, müssen Sie die Eigenschaft min verwenden.
Nachfolgend wird gezeigt, wie Sie das Objekt constraints aktualisieren können, um die Eigenschaft min einzuschließen:
Dadurch wird sichergestellt, dass die zurückgegebene Stream-Auflösung mindestens 1280 × 720 beträgt. Wenn diese Mindestanforderungen nicht erfüllt werden kann, wird das Promise mit einem OverconstrainedError abgelehnt.
In einigen Fällen haben Sie eventuell Bedenken hinsichtlich der Datenspeicherung und möchten, dass der Stream eine festgelegte Auflösung nicht überschreitet.
Dies kann sich als nützlich erweisen, wenn der Benutzer einen begrenzten Datenplan verwendet.
Um diese Funktionalität zu aktivieren, aktualisieren Sie das Beschränkungsobjekt, damit es ein Feld max enthält:
Mit diesen Einstellungen stellt der Browser sicher, dass der zurückgegebene Stream nicht unter 1280 × 720 liegt und 1920 × 1080 nicht überschreitet.
Andere Begriffe, die verwendet werden können, sind exact und ideal.
Die Einstellung ideal wird typischerweise zusammen mit den Eigenschaften min und max verwendet, um die bestmögliche Einstellung zu finden, die den idealen Werten am nächsten kommt.
Sie können die Beschränkungen zur Verwendung des Schlüsselworts ideal aktualisieren:
Um den Browser anzuweisen, die vordere und hintere (bei mobilen Geräten) Kamera von Geräten verwenden, können Sie eine Eigenschaft facingMode im Objekt video angeben:
Diese Einstellung verwendet die nach vorn gerichtete Kamera jederzeit bei allen Geräten.
Um die Rückseitenkamera von mobilen Geräten zu verwenden, können Sie die Eigenschaft facingMode zu environment ändern.
Schritt 4 - Verwenden der Methode enumerateDevices
Wird die Methode enumerateDevices aufgerufen, gibt sie alle verfügbaren Eingabemediengeräte zurück, die auf dem PC des Benutzers verfügbar sind.
Mit der Methode können Sie den Benutzern Optionen zur Verfügung stellen, welches Eingabemediengerät für das Streaming von Audio- oder Videoinhalten verwendet werden soll.
Diese Methode gibt ein Promise aufgelöst an ein Array MediaDeviceInfo zurück, das Informationen zu jedem Gerät enthält.
Ein Beispiel für die Verwendung dieser Methode wird im nachstehenden Ausschnitt gezeigt:
Eine Beispielantwort für jedes der Geräte würde wie folgt aussehen:
Anmerkung: Ein Label wird nur dann zurückgegeben, wenn ein verfügbarer Stream verfügbar ist oder wenn der Benutzer Zugriffsberechtigungen für Geräte erteilt hat.
Schritt 5 - Anzeigen des Video-Streams im Browser
Sie haben den Prozess der Anfrage und des Zugriffs auf die Mediengeräte durchlaufen, Einschränkungen für die erforderlichen Auflösungen konfiguriert und die Kamera ausgewählt, die Sie für die Videoaufzeichnung benötigen.
Nach all diesen Schritten wollen Sie zumindest sehen, ob der Stream anhand der konfigurierten Einstellungen bereitgestellt wird.
Um dies sicherzustellen, verwenden Sie das Element < video > zur Anzeige des Video-Streams im Browser.
Wie bereits erwähnt, gibt die Methode getUserMedia ein Promise zurück, das in einen Stream aufgelöst werden kann.
Der zurückgegebene Stream kann mit der Methode createObjectURL in eine Objekt-URL konvertiert werden.
Diese URL wird als Videoquelle festgelegt.
Sie werden eine kurze Demo erstellen, bei der wir den Benutzer mit der Methode enumerateDevices aus der Liste der verfügbaren Videogeräte auswählen lassen.
Dies ist eine Methode navigator.mediaDevices.
Sie listet die verfügbaren Mediengeräte wie Mikrofone und Kameras auf.
Sie gibt ein auflösbares Promise an ein Array von Objekten zurück, das die verfügbaren Mediengeräte detailliert auflistet.
Erstellen Sie eine Datei index.html und aktualisieren Sie den Inhalt mit dem nachstehenden Code:
Im obigen Ausschnitt haben Sie die von Ihnen benötigten Elemente und einige Steuerelemente für das Video eingerichtet.
Außerdem ist eine Schaltfläche für die Aufnahme von Screenshots des aktuellen Video-Feeds enthalten.
Lassen Sie uns diese Komponenten ein wenig stylen.
Erstellen Sie eine Datei style.css und kopieren Sie die folgenden Stile in sie. Bootstrap wurde hinzugefügt, um die Menge an CSS zu reduzieren, die Sie schreiben müssen, um die Komponenten in Gang zu bringen.
Der nächste Schritt besteht darin, der Demo Funktionalität hinzuzufügen.
Mit der Methode enumerateDevices erhalten Sie die verfügbaren Videogeräte und legen sie als Optionen innerhalb des Elements "select" fest.
Erstellen Sie eine Datei namens script.js und aktualisieren Sie sie mit dem folgenden Ausschnitt:
Im obigen Ausschnitt sind einige Dinge zu erkennen.
Schlüsseln wir diese auf:
feather.replace (): dieser Methodenaufruf instanziiert feather, wobei es sich um einen Symbolsatz für die Webentwicklung handelt.
Die Variable constraints enthält die anfängliche Konfiguration für den Stream.
Diese wird erweitert, um das vom Benutzer gewählte Mediengerät einzubeziehen.
getCameraSelection: diese Funktion ruft die Methode enumerateDevices auf.
Dann filtern Sie durch das Array aus dem aufgelösten Promise und wählen Video-Eingabegeräte aus.
Aus den gefilterten Ergebnissen erstellen Sie < option > für das Element < select >.
Der Aufruf der Methode getUserMedia erfolgt innerhalb des Listeners onclick der Schaltfläche play.
Hier überprüfen Sie vor dem Start des Streams, ob diese Methode vom Browser des Benutzers unterstützt wird.
Als Nächstes rufen Sie die Funktion startStream auf, die ein Argument constraints benötigt.
Sie ruft die Methode getUserMedia mit den angegebenen constraints auf. handleStream wird mit dem Stream aus dem aufgelösten Promise aufgerufen.
Diese Methode setzt den zurückgegebenen Stream auf das srcObject des Videoelements.
Als Nächstes fügen Sie den Schaltflächen-Steuerelementen auf der Seite Klick-Listener für pause, stop und zur Aufnahme von screenshots hinzu.
Außerdem fügen Sie dem Element < select > einen Listener hinzu, um die Stream-Beschränkungen mit dem ausgewählten Videogerät zu aktualisieren.
Aktualisieren Sie die Datei script.js mit dem nachstehenden Code:
Wenn Sie jetzt die Datei index.html im Browser öffnen, wird das Anklicken der Schaltfläche Play den Stream starten.
Hier ist eine vollständige Demo:
https: / / codepen.io / chrisbeast / pen / ebYwpX
In diesem Tutorial wurde die API getUserMedia vorgestellt.
Sie ist eine interessante Ergänzung zu HTML5, die den Prozess der Erfassung von Medien im Web erleichtert.
Die API verwendet einen Parameter (constraints), mit dem der Zugriff auf Audio- und Video-Eingabegeräte konfiguriert werden kann.
Sie kann auch verwendet werden, um die für Ihre Anwendung erforderliche Videoauflösung anzugeben.
Sie können die Demo weiter erweitern, um dem Benutzer eine Option zu geben, die aufgenommenen Screenshots zu speichern, sowie Video- und Audiodaten mit Hilfe der API MediaStream Recording aufzuzeichnen und zu speichern.
Verwenden der Git-Integration in Visual Studio Code
Visual Studio Code hat sich zu einem der beliebtesten Editoren für die Webentwicklung entwickelt.
Dank seiner vielen integrierten Funktionen wie der Integration der Quellenverwaltung, insbesondere mit Git, hat er eine solche Popularität erlangt.
Da Git einer der beliebtesten und leistungsstärksten Anbieter der Quellcodeverwaltung ist, ist die Nutzung seiner Leistungsfähigkeit nur das Sahnehäubchen auf dem Kuchen.
3652
Visual Studio Code (VS-Code) hat sich zu einem der beliebtesten Editoren für die Webentwicklung entwickelt.
Dank seiner vielen integrierten Funktionen wie der Integration der Quellcodeverwaltung, insbesondere mit Git, hat er eine solche Popularität erlangt.
Die Nutzung der Leistungsfähigkeit von Git innerhalb von VS-Code kann Ihren Workflow effizienter und robuster machen.
In diesem Tutorial erkunden Sie die Verwendung der Quellcodeverwaltungs-Integration in VS-Code mit Git.
Auf Ihrem Rechner installiertes Git.
Weitere Einzelheiten dazu finden Sie in dem Tutorial Erste Schritte mit Git.
Auf Ihrem Rechner installierte neueste Version von Visual Studio Code.
Schritt 1 - Vertrautmachen mit der Registerkarte Quellcodeverwaltung
Was Sie zuerst tun müssen, um die Vorteile der Quellcodeverwaltung-Integration zu nutzen, ist die Initialisierung eines Projekts als Git-Repository.
Öffnen Sie Visual Studio Code und greifen Sie auf das integrierte Terminal zu.
Sie können dies durch Verwendung der Tastenkombination STRG + 'unter Linux, macOS oder Windows öffnen.
Erstellen Sie in Ihrem Terminal ein Verzeichnis für ein neues Projekt und wechseln Sie in dieses Verzeichnis:
Erstellen Sie dann Git-Repository:
Eine weitere Möglichkeit, dies mit Visual Studio Code zu bewerkstelligen, ist das Öffnen der Registerkarte Quellcodeverwaltung (das Symbol sieht aus wie eine Straßenverzweigung) im linken Bereich:
Symbol für Quellcodeverwaltung
Wählen Sie als Nächstes Open Folder (Ordner öffnen):
Screenshot mit der Schaltfläche "Ordner öffnen"
Dadurch wird Ihr Datei-Explorer für das aktuelle Verzeichnis geöffnet.
Wählen Sie das bevorzugte Projektverzeichnis und klicken Sie auf Open (Öffnen).
Wählen Sie dann Initialize Repository (Repository initialisieren):
Screenshot, der die Schaltfläche "Repository initialisieren" zeigt
Wenn Sie nun Ihr Dateisystem überprüfen, sehen Sie, dass es ein Verzeichnis .git enthält.
Verwenden Sie dazu das Terminal, um zu Ihrem Projektverzeichnis zu navigieren und alle Inhalte aufzulisten:
Sie sehen das erstellte Verzeichnis .git:
Nachdem das Repository nun initialisiert wurde, fügen Sie eine Datei namens index.html hinzu.
Nachdem Sie dies getan haben, sehen Sie im Fenster Source Control (Quellcodeverwaltung), dass Ihre neue Datei mit dem Buchstaben U daneben angezeigt wird. U steht für unverfolgte Datei, d. h. eine Datei, die neu ist oder geändert wurde, aber dem Repository noch nicht hinzugefügt wurde:
Screenshot einer unverfolgten Datei mit dem Buchstaben U-Indikator
Sie können jetzt auf das Plus-Symbol (+) neben der Auflistung der Datei index.html klicken, um die Datei nach dem Repository zu verfolgen.
Sobald die Datei hinzugefügt wurde, ändert sich der Buchstabe neben der Datei an eine A. A steht für eine neue Datei, die dem Repository hinzugefügt wurde.
Um Ihre Änderungen zu übergeben, geben Sie eine Übergabe-Nachricht in das Eingabefeld oben im Fenster Source Control (Quellcodeverwaltung) ein.
Klicken Sie dann auf das Prüfsymbol, um die Übergabe auszuführen.
Screenshot einer hinzugefügten Datei mit dem Buchstabe-A-Indikator und der Übergabenachricht
Danach werden Sie feststellen, dass keine Änderungen ausstehen.
Fügen Sie als Nächstes Ihrer Datei index.html etwas Inhalt hinzu.
Sie können eine Emmet-Verknüpfung verwenden, um ein HTML5-Skelett in VS-Code zu generieren, indem Sie die Taste! drücken,
gefolgt von der Taste Tab.
Fahren Sie fort und fügen Sie dem < body > etwas wie eine Überschrift < h1 > hinzu, und speichern Sie anschließend.
Im Fenster Quellcodeverwaltung sehen Sie, dass Ihre Datei geändert wurde.
Daneben wird der Buchstabe M angezeigt, der für eine modifizierte Datei steht:
Screenshot der geänderten Datei mit dem Buchstabe-M-Indikator
Zur Übung sollten Sie auch diese Änderung übergeben.
Nachdem Sie nun mit der Bedienung des Quellcodeverwaltungsfensters vertraut sind, fahren Sie mit der Interpretation der "Gutter" -Indikatoren fort.
Schritt 2 - Interpretation der Gutter-Indikatoren
In diesem Schritt werfen Sie einen Blick auf das sogenannte "Gutter" in VS-Code.
Das Gutter ist der schmale Bereich rechts neben der Zeilennummer.
Wenn Sie zuvor Code-Folding verwendet haben, befinden sich die Symbole für Maximieren und Minimieren im Gutter.
Beginnen wir damit, eine kleine an Ihrer Datei index.html vorzunehmen, beispielsweise eine Änderung des Inhalts innerhalb des Tags < h1 >.
Danach werden Sie eine blaue vertikale Markierung im Gutter der Zeile, die Sie geändert haben, bemerken.
Die vertikale blaue Markierung bedeutet, dass die entsprechende Codezeile geändert wurde.
Versuchen Sie nun, eine Codezeile zu löschen.
Sie können eine der Zeilen im Abschnitt < body > Ihrer Datei index.html löschen.
Beachten Sie, dass im Gutter nun ein rotes Dreieck vorhanden ist.
Das rote Dreieck kennzeichnet eine Zeile oder eine Gruppe von Zeilen, die gelöscht.
Fügen Sie schließlich am Ende Ihres Abschnitts < body > eine neue Codezeile hinzu und beachten Sie den grünen Balken.
Der vertikale grüne Balken steht für eine hinzugefügte Codezeile.
Dieses Beispiel zeigt Gutter-Indikatoren für eine geänderte Zeile, eine entfernte Zeile und eine neue Zeile:
Screenshot mit Beispielen der drei Arten von Gutter-Indikatoren
Schritt 3 - Vergleichen von Dateien
VS-Code bietet auch die Möglichkeit, einen Vergleich mit einer Datei auszuführen.
Normalerweise müssten Sie dafür ein separates Diff-Tool herunterladen, sodass diese integrierte Funktion Ihnen dabei helfen kann, effizienter zu arbeiten.
Um einen Vergleich anzuzeigen, öffnen Sie das Quellcodeverwaltungsfenster und doppelklicken Sie auf eine geänderte Datei.
Doppelklicken Sie in diesem Fall auf die Datei index.html.
Sie werden zu einer typischen Diff-Ansicht mit der aktuellen Version der Datei auf der linken und der zuvor übergebenen Version der Datei auf der rechten Seite gebracht.
Dieses Beispiel zeigt, dass in der aktuellen Version eine Zeile hinzugefügt wurde:
Screenshot mit einem Beispiel einer geteilten Bildschirmanzeige eines Vergleichs
Schritt 4 - Arbeiten mit Zweigen
In der unteren Leiste haben Sie die Möglichkeit, Zweige zu erstellen und zu wechseln.
Wenn Sie einen Blick auf die unterste linke Ecke des Editors werfen, sollten Sie das Quellcodeverwaltungs-Symbol sehen (dasjenige, das wie eine Weggabelung aussieht), gefolgt höchstwahrscheinlich von master oder dem Namen des aktuellen Arbeitszweigs.
Zweigindikator in der unteren Leiste von VS-Code mit der Anzeige "master"
Um einen Zweig zu erstellen, klicken Sie auf diesen Zweignamen.
Es sollte sich ein Menü öffnen, das Ihnen die Möglichkeit gibt, einen neuen Zweig zu erstellen:
Eingabeaufforderung zur Erstellung eines neuen Zweigs
Fahren Sie fort und erstellen Sie einen neuen Zweig namens test.
Nehmen Sie nun eine Änderung an Ihrer Datei index.html vor, die anzeigt, dass Sie sich in dem neuen Zweig test befinden, indem Sie beispielsweise den Text "this is the new test branch" (Dies ist der neue Testzweig) hinzufügen.
Übergeben Sie diese Änderungen an den Zweig test.
Klicken Sie dann erneut auf den Zweignamen unten links, um wieder zu dem Zweig master zu wechseln.
Nach dem Wechsel zum Zweig master werden Sie feststellen, dass der Text this is the new test branch (Dies ist der neue Testzweig), der dem Zeig test übergeben wurde, nicht mehr vorhanden ist.
Schritt 5 - Arbeiten mit Remote-Repositorys
In diesem Tutorial wird nicht im Detail darauf eingegangen, aber über das Quellcodeverwaltungsfenster haben Sie Zugriff auf die Arbeit mit Remote-Repositorys.
Wenn Sie schon mit einem Remote-Repository gearbeitet haben, werden Ihnen vertraute Befehle wie pull, sync, publish, stash usw. auffallen.
Schritt 6 - Installieren nützlicher Erweiterungen
VS-Code enthält nicht nur viele integrierte Funktionen für Git, es gibt auch einige sehr beliebte Erweiterungen, um zusätzliche Funktionalität hinzuzufügen.
Git Blame
Diese Erweiterung bietet die Möglichkeit, Git Blame-Informationen in der Statusleiste für die aktuell ausgewählte Zeile anzuzeigen.
Dies kann einschüchternd klingen, aber keine Sorge, bei der Erweiterung "Git Blame" geht es viel mehr um Praktikabilität als darum, jemandem ein schlechtes Gewissen einzureden.
Bei der Idee, jemandem "ein schlechtes Gewissen" für eine Code-Änderung einzureden geht es weniger darum, diese Person zu beschämen, als vielmehr darum, die richtige Person zu finden, der man Fragen zu bestimmten Teilen des Codes stellen kann.
Wie Sie auf dem Screenshot sehen können, bietet diese Erweiterung in der unteren Symbolleiste eine subtile, auf die aktuelle Codezeile, die Sie gerade bearbeiten, bezogene Nachricht, die erklärt, wer die Änderung vorgenommen hat und wann sie vorgenommen wurde.
Git Blame in der unteren Symbolleiste
Git History
Obwohl Sie mit den integrierten Funktionen in VS-Code aktuelle Änderungen anzeigen, Vergleiche durchführen und Zweige verwalten können, bietet es keinen detaillierten Einblick in Ihren Git-Verlauf.
Die Erweiterung Git History löst dieses Problem.
Wie Sie in der nachstehenden Abbildung sehen können, ermöglicht diese Erweiterung Ihnen, den Verlauf einer Datei, eines bestimmten Autors, eines Zweigs usw. sorgfältig zu untersuchen. Um das nachstehende Fenster "Git History" zu aktivieren, klicken Sie mit der rechten Maustaste auf eine Datei und wählen Sie Git: View File History:
Ergebnisse der Erweiterung "Git History"
Zusätzlich können Sie Zweige und Übergaben vergleichen, Zweige aus Übergaben erstellen und vieles mehr.
Git Lens
Git Lens erweitert die in Visual Studio Code integrierten Git-Funktionen.
Es hilft Ihnen, die Code-Autorenschaft auf einen Blick mittels Git Blame-Annotationen und Code Lens zu visualisieren, nahtlos durch Git-Repositorys zu navigieren und diese zu erkunden, wertvolle Erkenntnisse über leistungsstarke Vergleichsbefehle zu gewinnen und vieles mehr.
Die Erweiterung "Git Lens" ist eine der beliebtesten in der Gemeinschaft und ist auch die leistungsstärkste.
In den meisten Fällen kann sie mit ihrer Funktionalität jede der beiden vorherigen Erweiterungen ersetzen.
Für "Blame" -Informationen erscheint rechts neben der Zeile, an der Sie gerade arbeiten, eine subtile Nachricht, die Sie darüber informiert, werde die Änderung vorgenommen hat, wann sie vorgenommen wurde und die zugehörige Übergabemeldung.
Es gibt einige zusätzliche Informationen, die angezeigt werden, wenn Sie mit der Maus über diese Nachricht fahren, wie die Code-Änderung selbst, der Zeitstempel und mehr.
Git Blame-Funktionalität in Git Lens
Für Git-Verlaufsinformationen bietet diese Erweiterung eine Vielzahl von Funktionen.
Sie haben einfachen Zugriff auf zahlreiche Optionen, darunter die Anzeige des Dateiverlaufs, die Durchführung von Vergleichen mit vorherigen Versionen, das Öffnen einer bestimmten Revision und vieles mehr.
Um diese Optionen zu öffnen, können Sie auf den Text in der unteren Statusleiste klicken, der den Autor enthält, der die Codezeile bearbeitet hat und wie lange es her ist, dass sie bearbeitet wurde.
Dadurch wird das folgende Fenster geöffnet:
Git History-Funktionalität in Git Lens
Diese Erweiterung ist vollgepackt mit Funktionalität und es wird eine Weile dauern, bis Sie alles, was sie zu bieten hat, aufgenommen haben.
In diesem Tutorial haben Sie erkundet, wie die Integration der Quellcodeverwaltung mit VS-Code verwendet wird.
VS-Code kann mit vielen Funktionen umgehen, für die zuvor ein separates Tool heruntergeladen werden musste.
Verwenden von Axios mit React
Axios ist ein leichter, Zusage-basierter HTTP-Client.
In diesem Artikel erkunden Sie Beispiele für die Verwendung von Axios für den Zugriff auf die beliebte JSON Placeholder-API innerhalb einer React-Anwendung.
5112
Viele Projekte im Web müssen in einer bestimmten Phase ihrer Entwicklung mit einer REST-API verbunden werden.
Axios ist ein leichter HTTP-Client, der auf dem Dienst $http innerhalb von Angular.js v1.x basiert und dem nativen JavaScript Fetch API ähnlich ist.
Axios ist Zusage-basiert, was Ihnen die Möglichkeit bietet, die Vorteile von async und await von JavaScript zu nutzen, um einen lesbareren asynchronen Code zu erhalten.
Sie können Anfragen auch abfangen und abbrechen, und es gibt einen integrierten Schutz auf Client-Seite gegen die Fälschung von Cross-Site-Anfragen.
In diesem Artikel sehen Sie Beispiele für die Verwendung von Axios für den Zugriff auf die beliebte JSON Placeholder-API innerhalb einer React-Anwendung.
Um diesem Artikel folgen zu können, benötigen Sie Folgendes:
Ein neues React-Projekt, das mit der Create React App eingerichtet wird, indem Sie dem Tutorial Einrichten eines React-Projekts mit Create React App folgen.
Schritt 1 - Hinzufügen von Axios zum Projekt
In diesem Abschnitt fügen Sie Axios dem React-Projekt < ^ > digital-ocean-tutorial < ^ > hinzu, das Sie entsprechend dem Tutorial Einrichten eines React-Projekts mit Create React App erstellt haben.
Um Axios zu dem Projekt hinzuzufügen, öffnen Sie Ihr Terminal und wechseln Sie die Verzeichnisse in Ihrem Projekt:
Führen Sie zur Installation von Axios dann diesen Befehl aus:
Als Nächstes müssen Sie Axios in die Datei importieren, in der Sie es verwenden möchten.
Schritt 2 - Erstellen einer GET-Anfrage
In diesem Beispiel erstellen Sie eine neue Komponente und importieren Axios in diese, um eine GET-Anfrage zu senden.
Erstellen Sie im Ordner src Ihres React-Projekts eine neue Komponente namens PersonList.js:
Fügen Sie den folgenden Code zu der Komponente hinzu:
Zuerst importieren Sie React und Axios, sodass beide in der Komponente verwendet werden können.
Dann haken Sie sich in den Lifecyle-Hook componentDidMount und führen eine GET-Anfrage aus.
Sie verwenden axios.get (url) mit einer URL aus einem API-Endpunkt, um eine Zusage zu erhalten, die ein Antwortobjekt zurückgibt.
Innerhalb des Antwortobjekts gibt es Daten, die dann dem Wert von person zugewiesen werden.
Sie können auch andere Informationen über die Anfrage erhalten, wie den Statuscode unter res.status oder mehr Informationen innerhalb von res.request.
Schritt 3 - Erstellen einer POST-Anfrage
In diesem Schritt verwenden Sie Axios mit einem anderen HTTP-Anfrageverfahren namens POST.
Entfernen Sie den vorherigen Code in PersonList und fügen Sie Folgendes hinzu, um ein Formular zu erstellen, das die Benutzereingaben ermöglicht und anschließend den Inhalt mittels POST an eine API sendet:
Innerhalb der Funktion handleSubmit verhindern Sie die Standardaktion des Formulars. Aktualisieren Sie dann den state auf die user-Eingabe.
Mit der Verwendung von POST erhalten Sie das gleiche Antwortobjekt mit Informationen, die Sie innerhalb eines then-Abrufs verwenden können.
Um die POST-Anfrage abzuschließen, erfassen Sie zunächst die user-Eingabe.
Dann fügen Sie die Eingabe zusammen mit der POST-Anfrage hinzu, wodurch Sie eine Antwort erhalten.
Anschließen können Sie die Antwort in console.log protokollieren, wobei die user-Eingabe im Formular angezeigt werden sollte.
Schritt 4 - Erstellen einer DELETE-Anfrage
In diesem Beispiel sehen Sie, wie Elemente aus einer API unter Verwendung von axios.delete gelöscht werden und eine URL als ein Parameter übergeben wird.
Ändern Sie den Code für das Formular aus dem POST-Beispiel, um einen Benutzer zu löschen, anstatt einen neuen hinzuzufügen:
Auch hier gibt Ihnen das res-Objekt Informationen über die Anfrage.
Nach der Übermittlung des Formulars können Sie dann diese Informationen erneut in console.log protokollieren.
Schritt 5 - Verwenden einer Basisinstanz in Axios
In diesem Beispiel sehen Sie, wie Sie eine Basisinstanz einrichten können, in der Sie eine URL und alle anderen Konfigurationselemente definieren können.
Erstellen Sie eine separate Datei namens api.js:
Exportieren Sie eine neue axios-Instanz mit diesen Standardeinstellungen:
Sobald die Standardinstanz eingerichtet ist, kann sie dann innerhalb der Komponenten PersonList verwendet werden.
Sie importieren die neue Instanz wie folgt:
Da http: / / jsonplaceholder.typicode.com / nun die Basis-URL ist, müssen Sie nicht länger die gesamte URL eingeben, wenn Sie einen anderen Endpunkt auf der API treffen möchten.
Schritt 6 - Verwenden von async und await
In diesem Beispiel sehen Sie, wie Sie async und await verwenden können, um mit Zusagen zu arbeiten.
Das Schlüsselwort await löst die Zusage promise aus und gibt den Wert value zurück.
Der Wert value kann dann einer Variablen zugewiesen werden.
In diesem Code-Beispiel wird das .then () ersetzt.
Die Zusage promise wird aufgelöst und der Wert wird in der Variable response gespeichert.
In diesem Tutorial haben Sie mehrere Beispiele für die Verwendung von Axios innerhalb einer React-Anwendung zur Erstellung von HTTP-Anfragen und zur Handhabung von Antworten untersucht.
Wenn Sie mehr über React erfahren möchten, lesen Sie die Reihe Codieren in React.js oder sehen Sie sich die React-Themenseite für weitere Übungen und Programmierprojekte an.
Verwenden von Rsync zum Synchronisieren von lokalen und entfernten Verzeichnissen
Rsync ist ein Tool zur intelligenten Synchronisierung von lokalen und entfernten Verzeichnissen.
In diesem Artikel werden wir die grundlegende Verwendung dieses Dienstprogramms untersuchen, um Dateien aus dem Verzeichnis in das Verzeichnis und aus dem System in das System zu kopieren.
582
Rsync, was für "remote sync" (entfernte Synchronisierung) steht, ist ein Tool zur Synchronisierung von lokalen und entfernten Dateien.
Es verwendet einen Algorithmus, der die Menge der kopierten Daten minimiert, indem es nur die Abschnitte von Daten verschiebt, die Änderungen aufweisen.
In diesem Leitfaden werden wir die grundlegende Verwendung dieses leistungsfähigen Dienstprogramms abdecken.
Was ist Rsync?
Rsync ist ein sehr flexibles, netzwerkfähiges Synchronisierungstool.
Aufgrund seiner Allgegenwärtigkeit unter Linux- und Unix-ähnlichen Systemen und seiner Beliebtheit als Tool für Systemskripte, ist es in den meisten Linux-Distributionen standardmäßig enthalten.
Die grundlegende Syntax von rsync ist sehr einfach und funktioniert ähnlich wie ssh, scp und cp.
Wir erstellen zwei Testverzeichnisse und einige Testdateien mit den folgenden Befehlen:
Wir haben nun ein Verzeichnis namens dir1 mit 100 leeren Dateien.
Wir haben auch ein leeres Verzeichnis namens dir2.
Um den Inhalt von dir1 mit dir2 auf demselben System zu synchronisieren, geben Sie Folgendes ein:
Die Option -r bedeutet rekursiv, das für die Verzeichnissynchronisation erforderlich ist.
Wir könnten stattdessen auch das Flag -a verwenden:
Die Option -a ist ein Kombinations-Flag.
Sie steht für "Archiv" und synchronisiert rekursiv und erhält symbolische Links, spezielle und Gerätedateien, Modifizierungszeiten, Gruppe, Eigentümer und Berechtigungen.
Sie wird häufiger verwendet als -r und ist normalerweise das, was Sie verwenden möchten.
Ein wichtiger Hinweis
Vielleicht haben Sie bemerkt, dass es am Ende des ersten Arguments in den obigen Befehlen einen nachstehenden Schrägstrich (/) gibt:
Dies ist notwendig, um "den Inhalt von dir1" zu bezeichnen.
Die Alternative ohne den nachstehenden Schrägstrich würde dir1, einschließlich des Verzeichnisses, innerhalb von dir2 platzieren.
Dies würde eine Hierarchie erstellen, die wie folgt aussieht:
Überprüfen Sie immer Ihre Argumente, bevor Sie einen Befehl rsync ausführen.
Rsync bietet eine Methode dafür, indem Sie die Optionen -n oder --dry-run übergeben.
Das Flag -v (für "verbose", ausführlich) ist ebenfalls notwendig, um die entsprechende Ausgabe zu erhalten:
Vergleichen Sie diese Ausgabe mit der Ausgabe, die wir erhalten, wenn wir den nachgestellten Schrägstrich entfernen:
Hier sehen Sie, dass das Verzeichnis selbst übertragen wird.
Verwenden von Rsync zum Synchronisieren mit einem Remote-System
Die Synchronisierung mit einem Remote-System ist trivial, wenn Sie SSH-Zugriff auf den Remote-Rechner haben und rsync auf beiden Seiten installiert ist.
Sobald Sie SSH-Zugriff zwischen den beiden Rechnern verifiziert haben, können Sie den Ordner dir1 von früher auf einen Remote-Computer synchronisieren, indem Sie diese Syntax verwenden (beachten Sie, dass wir in diesem Fall das eigentliche Verzeichnis übertragen möchten, daher lassen wir den nachgestellten Schrägstrich weg):
Dies wird als "push" -Operation bezeichnet, da es ein Verzeichnis vom lokalen System auf ein Remote-System schiebt.
Die entgegengesetzte Operation ist "pull".
Sie wird verwendet, um ein Remote-Verzeichnis mit dem lokalen System zu synchronisieren.
Wenn das dir1 auf dem Remote-System anstelle unseres lokalen Systems wäre, würde die Syntax lauten:
Wie cp und ähnliche Tools ist die Quelle immer das erste Argument, und das Ziel immer das zweite.
Nützliche Optionen für Rsync
Rsync bietet viele Optionen zur Änderung des Standardverhaltens des Dienstprogramms.
Wir haben bereits einige der notwendigen Flags besprochen.
Wenn Sie Dateien übertragen, die nicht bereits komprimiert wurden, wie Textdateien, können Sie die Netzwerkübertragung reduzieren, indem Sie mit der Option -z eine Komprimierung hinzufügen:
Das Flag -P ist sehr hilfreich.
Es kombiniert die Flags --progress und --partial.
Mit dem ersten erhalten Sie eine Fortschrittsleiste für die Übertragungen und mit dem zweiten können Sie unterbrochene Übertragungen wiederaufnehmen:
Wenn wir den Befehl erneut ausführen, erhalten wir eine kürzere Ausgabe, da keine Änderungen vorgenommen wurden.
Dies illustriert die Fähigkeit von rsync anhand von Änderungszeiten, um zu ermitteln, ob Änderungen vorgenommen wurden.
Wir können die Änderungszeit einiger Dateien aktualisieren und sehen, dass rsync auf intelligente Weise nur die geänderten Dateien neu kopiert:
Um zwei Verzeichnisse wirklich synchron zu halten, ist es notwendig, Dateien aus dem Zielverzeichnis zu löschen, wenn sie aus der Quelle entfernt werden.
Standardmäßig löscht rsync nichts aus dem Zielverzeichnis.
Wir können dieses Verhalten mit der Option --delete ändern.
Bevor Sie diese Option verwenden, sollten Sie die Option --dry-run verwenden und Tests durchführen, um Datenverlust zu vermeiden:
Wenn Sie bestimmte Dateien oder Verzeichnisse ausschließen möchten, die sich innerhalb eines synchronisierten Verzeichnisses befinden, können Sie dies tun, indem Sie sie in einer kommagetrennten Liste nach der Option --exclude = angeben:
Wenn wir ein Muster zum Ausschließen angegeben haben, können wir diesen Ausschluss für Dateien, die einem anderen Muster entsprechen, mit der Option --include = überschreiben.
Schließlich kann die Option --backup von rsync verwendet werden, um Backups von wichtigen Dateien zu speichern.
Sie wird in Verbindung mit der Option --backup-dir verwendet, die das Verzeichnis angibt, in dem die Backup-Dateien gespeichert werden sollen.
Rsync kann Dateiübertragungen über Netzwerkverbindungen vereinfachen und die lokale Verzeichnissynchronisierung robuster machen.
Die Flexibilität von rsync macht es zu einer guten Option für viele verschiedene Operationen auf Dateiebene.
Wenn Sie rsync beherrschen, können Sie komplexe Backup-Operationen entwerfen und eine feinkörnige Kontrolle darüber erhalten, was und wie übertragen wird.
6864
Diese containerisierte Einrichtung wurde mit einem Nginx Reverse-Proxy und Let "s Encrypt-signierten TLS-Zertifikaten in Skalieren und Sichern einer Django-Anwendung mit Docker, Nginx und Let" s Encrypt skaliert und gesichert.
Die TLS-Verschlüsselung wird mit einem Ingress-Objekt und dem Open-Source Ingress-Controller ingress-nginx aktiviert.
Das Kubernetes Add-on cert-manager erneuert und stellt Zertifikate mit der kostenlosen Zertifizierungsstelle Let "s Encrypt aus.
Einen Kubernetes 1.15 + -Cluster mit aktivierter rollenbasierter Zugriffskontrolle (RBAC).
Das Befehlszeilen-Tool kubectl, das auf Ihrem lokalen Rechner installiert und für die Verbindung mit Ihrem Cluster konfiguriert ist.
Einen A-DNS-Datensatz mit your _ domain.com, der auf die öffentliche IP-Adresse des Ingress Load Balancers verweist.
Eine PostgreSQL-Server-Instanz, Datenbank und Benutzer für Ihre Django-Anwendung. Mit geringfügigen Änderungen können Sie jede Datenbank verwenden, die Django unterstützt.
Weitere Informationen zu deren Erstellung finden Sie in Repositories in der Docker-Dokumentation.
Wir laden ebenfalls statische Assets wie Stylesheets und Images in den Objektspeicher hoch.
Verschieben Sie das Image in das Repository:
Nachdem Ihr Image nun für Kubernetes unter Docker Hub verfügbar ist, können Sie es in Ihrem Cluster bereitstellen.
Als wir den Django-Container lokal ausgeführt haben, haben wir die Datei env an docker run übergeben, um Konfigurationsvariablen in die Laufzeitumgebung zu injizieren.
Erstellen Sie zunächst ein Verzeichnis namens yaml, in dem wir unsere Kubernetes-Manifeste speichern werden.
Kopieren Sie die gleichen Werte hinein, die Sie im vorherigen Schritt in die Datei env eingegeben haben.
Lassen Sie für Testzwecke DJANGO _ ALLOWED _ HOSTS auf * stehen, um die Host-Header-basierte Filterung zu deaktivieren.
Erstellen Sie die ConfigMap in Ihrem Cluster mit kubectl apply:
Nachdem die ConfigMap erstellt wurde, erstellen wir im nächsten Schritt das von unserer Anwendung verwendete Secret.
Sie können den Vorgang aus dem vorherigen Schritt wiederholen, indem Sie Secret-Werte manuell base64-kodieren und in eine Manifestdatei einfügen.
Sie können das Secret mit kubectl describe inspizieren:
Zu diesem Zeitpunkt haben Sie die Konfiguration Ihrer Anwendung in Ihrem Kubernetes-Cluster mithilfe der Objekttypen Secret und ConfigMap gespeichert.
Hier definieren wir eine Kubernetes Deployment namens polls-app und kennzeichnen es mit dem Schlüsselwertpaar app: polls.
Abschließend geben wir containerPort 8000 frei und nennen ihn gunicorn.
Um mehr über die Konfiguration von Kubernetes Deployments zu erfahren, konsultieren Sie bitte Deployments aus der Kubernetes-Dokumentation.
Wenn Sie mit der Bearbeitung der Datei fertig sind, speichern und schließen Sie sie.
Zwei Replikate Ihrer Django-Anwendung sind nun im Cluster in Betrieb.
Um auf die Anwendung zuzugreifen, müssen Sie einen Kubernetes-Dienst erstellen, was wir als Nächstes tun.
Stellen Sie den Dienst mit kubectl apply bereit:
Diese Ausgabe zeigt die clusterinterne IP des Dienstes und den NodePort (32654).
Zu diesem Zeitpunkt haben Sie zwei Replikate des Django Umfrageanwendungs-Containers mithilfe eines Deployments bereitgestellt.
Schritt 8 - Konfigurieren von HTTPS unter Verwendung von Nginx Ingress und cert-manager
Dies geschieht mit der Verwendung von Ingress-Objekten, die Regeln für das Routing von HTTP- und HTTPS-Verkehr zu Kubernetes-Diensten definieren, und Ingress-Controllern, die die Regeln umsetzen, indem sie den Verkehr durch Lastverteilung an die entsprechenden Backend-Dienste weiterleiten.
Bevor Sie mit diesem Schritt fortfahren, sollten Sie den in dem Voraussetzungs-Tutorial erstellten Ingress echo-ingress löschen:
Wenn Sie DigitalOcean auch für die Verwaltung der DNS-Datensätze Ihrer Domäne verwenden, konsultieren Sie bitte Verwalten von DNS-Datensätzen, um zu erfahren, wie Sie A-Datensätze erstellen.
Um eine Testanfrage zu senden, verwenden wir wget von der Befehlszeile aus:
Wir verwenden das vorgeschlagene Flag --no-check-certificate, um die Zertifikatsprüfung zu umgehen:
Navigieren Sie in Ihrem Webbrowser zu your _ domain.com / polls, um zu bestätigen, dass die HTTPS-Verschlüsselung aktiviert ist und alles wie erwartet funktioniert.
Überprüfen Sie, ob die HTTPS-Verschlüsselung in Ihrem Webbrowser aktiv ist.
Wenn Sie Google Chrome verwenden, bestätigt die Ankunft auf der obigen Seite ohne Fehler, dass alles korrekt funktioniert.
Der einzige Weg darauf zuzugreifen, ist über Ihre Domäne und den in diesem Schritt erstellten Ingress.
Konfigurieren der SSH-Schlüssel-basierten Authentifizierung auf einem Linux-Server
SSH, oder Secure Shell, ist die häufigste Art der Verwaltung von Remote-Linux-Servern.
Obwohl Passwörter auf sichere Weise an den Server gesendet werden, sind sie im Allgemeinen nicht komplex oder lang genug, um wiederholten, hartnäckigen Angreifern zu widerstehen.
Moderne Verarbeitungsleistung kombiniert mit automatisierten Skripten machen das Brute-Forcing eines passwortgeschützten Kontos sehr gut möglich.
Obwohl es andere Methoden gibt, um zusätzliche Sicherheit (fail2ban usw.) hinzuzufügen, erweisen sich SSH-Schlüssel als eine zuverlässige und sichere Alternative.
SSH-Schlüsselpaare sind zwei kryptografisch sichere Schlüssel, die zur Authentifizierung eines Clients gegenüber einem SSH-Server verwendet werden können.
Jedes Schlüsselpaar besteht aus einem öffentlichen Schlüssel und einem privaten Schlüssel.
Der öffentliche Schlüssel kann zur Verschlüsselung von Nachrichten verwendet werden, die nur der private Schlüssel entschlüsseln kann.
Der Schlüssel wird zu einer speziellen Datei innerhalb des Benutzerkontos hinzugefügt, bei dem Sie sich anmelden werden, namens ~ / .ssh / authorized _ keys.
In der Regel ist es am besten, an dieser Stelle den Standardspeicherort beizubehalten.
Wenn Sie einen nicht standardmäßigen Pfad auswählen möchten, geben Sie diesen jetzt ein, andernfalls drücken Sie ENTER, um die Standardeinstellung zu akzeptieren.
Die Passphrase dient als eine zusätzliche Schutzschicht, falls diese Bedingungen kompromittiert werden.
Wenn Sie einen neuen DigitalOcean-Server einrichten, können Sie Ihren öffentlichen SSH-Schlüssel automatisch in das Root-Konto Ihres neuen Servers einbinden.
SSH-Schlüsselauswahl
Fügen Sie im Feld "SSH Key content" (SSH-Schlüsselinhalt) den Inhalt Ihres öffentlichen SSH-Schlüssels ein.
Fügen Sie diesen Wert in seiner Gesamtheit in das größere Feld ein.
Wenn Sie Ihr Droplet erstellen, werden die von Ihnen ausgewählten öffentlichen SSH-Schlüssel in der Datei ~ / .ssh / authorized _ keys des Root-Benutzerkontos abgelegt.
Welche Methode Sie verwenden, hängt weitgehend von den verfügbaren Tools und den Details Ihrer aktuellen Konfiguration ab.
Aufgrund ihrer Einfachheit wird diese Methode empfohlen, wenn sie verfügbar ist.
Sie werden eine Ausgabe sehen, die wie folgt aussieht:
Wenn Ihr Server beispielsweise ein DigitalOcean-Droplet ist, können Sie sich über die Web-Konsole im Bedienfeld anmelden:
Anschließend sollte eine neue Shell-Sitzung mit dem Konto auf dem Remote-System für Sie erzeugt werden.
Stellen Sie vor dem Ausführen der in diesem Abschnitt beschriebenen Schritte sicher, dass entweder die SSH-Schlüssel-basierte Authentifizierung für das Root-Konto auf diesem Server konfiguriert ist oder vorzugsweise, dass Sie die SSH-Schlüssel-basierte Authentifizierung für ein Konto auf diesem Server mit sudo-Zugriff konfiguriert haben.
Von hier aus gibt es viele Richtungen, in die Sie gehen können.
Die Verwendung von for-Schleifen und while-Schleifen in Python ermöglicht Ihnen die Automatisierung und Wiederholung von Aufgaben in effizienter Weise.
Sie setzen die break-Anweisung innerhalb des Codeblocks unter Ihrer Schleifenanweisung ein, normalerweise nach einer bedingten if-Anweisung.
Dies zeigt, dass, sobald die Ganzzahl number als gleichwertig zu 5 ausgewertet wird, die Schleife abbricht, da das Programm mit der Anweisung break dazu aufgefordert wird.
Die Anweisung continue befindet sich innerhalb des Codeblocks unter der Schleifenanweisung, normalerweise nach einer bedingten if-Anweisung.
Pass-Anweisung
Die Anweisung pass, die nach der bedingten if-Anweisung steht, teilt dem Programm mit, dass es die Schleife weiter ausführen und die Tatsache ignorieren soll, dass die Variable number während einer ihrer Iterationen als gleichwertig zu 5 ausgewertet wird.
Wir führen das Programm aus und betrachten die Ausgabe:
Durch die Verwendung der Anweisung pass in diesem Programm stellen wir fest, dass das Programm genau so abläuft, wie es ablaufen würde, wenn es keine bedingte Anweisung in dem Programm gäbe.
Verwenden von Journalctl zum Anzeigen und Manipulieren von Systemd-Protokollen
Einige der überzeugendsten Vorteile von systemd sind diejenigen, die mit der Prozess- und Systemprotokollierung verbunden sind.
Das Journal wird mit dem Daemon journald implementiert, der alle vom Kernel, initrd, services etc. erzeugten Nachrichten verarbeitet. In diesem Leitfaden besprechen wir die Verwendung des Dienstprogramms journalctl, mit dem Sie auf die in dem Journal gespeicherten Daten zugreifen und diese manipulieren können.
Durch die Interaktion mit den Daten über ein einziges Dienstprogramm sind Administratoren in der Lage, Protokolldaten dynamisch nach ihren Bedürfnissen anzuzeigen.
Das Speichern der Protokolldaten in einem Binärformat bedeutet auch, dass die Daten in beliebigen Ausgabeformaten angezeigt werden können, je nachdem, was Sie gerade benötigen.
Für die tägliche Protokollverwaltung sind Sie eventuell daran gewöhnt, die Protokolle im Standard-Format syslog anzuzeigen, aber wenn Sie sich später entscheiden, Dienstunterbrechungen grafisch darzustellen, können Sie jeden Eintrag als JSON-Objekt ausgeben, um ihn für Ihren grafischen Dienst konsumierbar zu machen.
Sie haben vielleicht einen zentralisierten syslog-Server, den Sie verwenden, um Daten von mehreren Servern zu kompilieren, aber Sie möchten die Protokolle auch von mehreren Diensten auf einem einzigen System mit dem Journal systemd zusammenführen.
Wenn Sie diejenige gefunden haben, die dem Standort Ihres Servers entspricht, können Sie sie mit der Option set-timezone einstellen:
Grundlegendes Anzeigen von Protokollen
Allerdings werden hier tatsächlich Daten aus mehr Quellen gesammelt als traditionelle syslog-Implementierungen in der Lage sind.
Wenn Sie die Zeitstempel in UTC anzeigen möchten, können Sie das Flag --utc verwenden:
Dies hilft Ihnen, Informationen zu identifizieren und zu verwalten, die für Ihre aktuelle Umgebung relevant sind.
Während Sie in der Regel die Informationen aus dem aktuellen Bootvorgang anzeigen möchten, gibt es sicherlich Zeiten, in denen auch vergangene Bootvorgänge hilfreich sind.
Bei einigen Distributionen ist das Speichern von Informationen zu früheren Bootvorgängen standardmäßig aktiviert, bei anderen ist diese Funktion deaktiviert.
Um persistente Boot-Informationen zu aktivieren, können Sie entweder das Verzeichnis zum Speichern des Journals erstellen, indem Sie Folgendes eingeben:
Wenn Sie eine absolute Referenz benötigen, steht die Boot-ID in der zweiten Spalte.
Die Zeit, auf die sich die Boot-Sitzung bezieht, erkennen Sie an den beiden Zeitangaben, die am Ende aufgeführt sind.
Um beispielsweise das Journal von dem vorherigen Boot zu sehen, verwenden Sie den relativen Zeiger -1 mit dem Flag -b:
Für absolute Zeitwerte sollten Sie das folgende Format verwenden:
Das Sekundenfeld kann auch weggelassen werden, sodass standardmäßig "00" verwendet wir:
Beispielsweise können Sie die Wörter "gestern", "heute", "morgen" oder "jetzt" verwenden.
Um die Daten von gestern zu erhalten, könnten Sie eingeben:
Wie Sie sehen können, ist es relativ einfach, flexible Zeitfenster zu definieren, um die Einträge zu filtern, die Sie sehen möchten.
Das Journal systemd bietet eine Vielzahl von Möglichkeiten, dies zu tun.
Wir können die Option -u verwenden, um auf diese Weise zu filtern.
Normalerweise würden Sie wahrscheinlich auch nach der Zeit filtern wollen, um die Zeilen anzuzeigen, an denen Sie interessiert sind.
Nach Prozess, Benutzer oder Gruppen-ID
Dazu können wir durch Angabe des Feldes _ PID filtern.
Die Option -F kann verwendet werden, um alle verfügbaren Werte für ein bestimmtes Journalfeld anzuzeigen.
Dies zeigt Ihnen alle Werte, die das Journal für das Feld Gruppen-ID gespeichert hat.
Wir können auch filtern, indem wir eine Pfadposition bereitstellen.
Standardmäßig werden damit die Kernel-Meldungen des aktuellen Bootvorgangs angezeigt.
Um beispielsweise die Meldungen von vor fünf Bootvorgängen zu erhalten, könnten Sie eingeben:
Sie können entweder den Namen der Priorität oder den entsprechenden numerischen Wert verwenden.
In der Reihenfolge von höchster bis niedrigster Priorität sind dies:
Wenn Sie die Priorität auswählen, werden Meldungen angezeigt, die auf der angegebenen Ebene markiert sind, und solche, die darüber liegen.
Es gibt jedoch noch andere Möglichkeiten, die Ausgabe zu ändern.
Kürzen oder Erweitern der Ausgabe
Wenn Sie die Ausgabe lieber gekürzt haben möchten, wobei an den Stellen, an denen Informationen entfernt wurden, eine Ellipse eingefügt wird, können Sie die Option --no-full verwenden:
Mit diesen Optionen können Sie die Journaleinträge in dem Format anzeigen, das Ihren aktuellen Bedürfnissen am besten entspricht.
Um die Protokolle aktiv zu verfolgen, während sie geschrieben werden, können Sie das Flag -f verwenden.
Journalwartung
Wenn Sie Ihr Journal verkleinern möchten, können Sie dies auf zwei verschiedene Arten tun (verfügbar mit systemd Version 218 und höher).
Durch die Einstellung dieser Werte können Sie kontrollieren, wie journald den Speicherplatz auf Ihrem Server verbraucht und bewahrt.
Dies ist wichtig zu bedenken, wenn Sie Dateizählungen nach einer Vakuum-Operation interpretieren.
Verwenden von Grep und regulären Ausdrücken zur Suche nach Textmustern in Linux
Grep ist ein Tool, das zur Suche nach bestimmten Mustern innerhalb von Texteingaben unter Verwendung regulärer Ausdrücke verwendet wird.
Reguläre Ausdrücke sind ein System zur Beschreibung komplexer Textmuster.
Reguläre Ausdrücke sind ein leistungsfähiges Tool, das in vielen verschiedenen Textprogrammen verwendet werden kann.
In diesem Tutorial verwenden Sie grep und reguläre Ausdrücke zur Suche in Text.
396
Der Befehl grep ist einer der nützlichsten Befehle in einer Linux-Terminalumgebung.
Der Name grep steht für "global regular expression print" oder in Langform "Global search for a Regular Expression and Print out matched lines", was in etwa "globale Suche nach einem regulären Ausdruck und Ausgabe der übereinstimmenden Zeilen" bedeutet.
Dies bedeutet, dass Sie grep verwenden können, um zu sehen, ob die Eingabe, die es erhält, mit einem bestimmten Muster übereinstimmt.
Dieses scheinbar triviale Programm ist extrem leistungsfähig; seine Fähigkeit, Eingaben nach komplexen Regeln zu sortieren, macht es zu einem beliebten Glied in vielen Befehlsketten.
In diesem Tutorial werden Sie die Optionen des Befehls grep kennenlernen und dann in die Verwendung regulärer Ausdrücke eintauchen, um eine fortgeschrittenere Suche durchzuführen.
interactive _ terminal bash
Grundlegende Verwendung
In diesem Tutorial verwenden Sie grep, um die GNU General Public License Version 3 nach verschiedenen Wörtern und Ausdrücken zu durchsuchen.
Wenn Sie auf einem Ubuntu-System arbeiten, finden Sie die Datei im Ordner / usr / share / common-licenses.
Kopieren Sie sie in Ihr Home-Verzeichnis:
Wenn Sie ein anderes System verwenden, nutzen Sie den Befehl curl, um eine Kopie herunterzuladen:
In diesem Tutorial verwenden Sie auch die BSD-Lizenzdatei.
Unter Linux können Sie mit dem folgenden Befehl in Ihr Home-Verzeichnis kopieren:
Wenn Sie ein anderes System verwenden, erstellen Sie die Datei mit dem folgenden Befehl:
Nachdem Sie nun die Dateien haben, können Sie mit grep arbeiten.
In der grundlegendsten Form verwenden Sie grep, um wörtliche Muster innerhalb einer Textdatei abzugleichen.
Das heißt, wenn Sie grep ein Wort übergeben, nach dem es suchen soll, wird es jede Zeile in der Datei ausgeben, die dieses Wort enthält.
Führen Sie den folgenden Befehl aus, um mit grep nach jeder Zeile zu suchen, die das Wort GNU enthält:
Das erste Argument, GNU, ist das Muster, nach dem Sie suchen, während das zweite Argument, GPL-3, die Eingabedatei ist, die Sie durchsuchen möchten.
Die resultierende Ausgabe wird jede Zeile sein, die den Mustertext enthält:
Auf einigen Systemen wird das gesuchte Muster in der Ausgabe hervorgehoben.
Gebräuchliche Optionen
Standardmäßig sucht grep in der Eingabedatei nach dem genau angegebenen Muster und gibt die gefundenen Zeilen zurück.
Sie können dieses Verhalten jedoch durch Hinzufügen einiger optionaler Flags zu grep nützlicher gestalten.
Wenn Sie möchten, dass grep die "Groß- / Kleinschreibung" Ihres Suchparameters ignoriert und sowohl nach Varianten mit Groß- als auch Kleinbuchstaben sucht, können Sie die Option -i oder ---ignore-case angeben.
Suchen Sie nach jeder Instanz des Wortes license (mit Groß-, Klein- oder gemischter Schreibung) in derselben Datei wie zuvor mit dem folgenden Befehl:
Die Ergebnisse enthalten: LICENSE, license und License:
Wenn es eine Instanz mit LiCeNsE gegeben hätte, wäre diese auch zurückgegeben worden.
Wenn Sie alle Zeilen finden möchten, die kein bestimmtes Muster enthalten, können Sie die Option -v oder --invert-match verwenden.
Suchen Sie mit dem folgenden Befehl in der BSD-Lizenz nach jeder Zeilen, die das Wort the nicht enthält:
Da Sie die Option "ignore case" nicht angegeben haben, wurden die letzten beiden Elemente als das Wort the nicht enthaltend zurückgegeben.
Es ist oft nützlich, die Zeilennummer zu kennen, an der die Übereinstimmungen auftreten.
Sie können dies mit der Option -n oder --line-number tun.
Führen Sie das vorherige Beispiel mit diesem hinzugefügten Flag erneut aus:
Sie sehen den folgenden Text:
Jetzt können die Zeilennummer referenzieren, wenn Sie Änderungen an jeder Zeile vornehmen möchten, die the nicht enthält.
Dies ist besonders bei der Arbeit mit Quellcode praktisch.
Reguläre Ausdrücke
In der Einführung haben Sie gelernt, dass grep für "global regular expression print" steht.
Ein "regulärer Ausdruck" (regular expression) ist eine Textzeichenfolge, die ein bestimmtes Suchmuster beschreibt.
Verschiedene Anwendungen und Programmiersprachen implementieren reguläre Ausdrücke leicht unterschiedlich.
In diesem Tutorial werden Sie nur eine kleine Teilmenge der Art und Weise kennenlernen, wie grep seine Muster beschreibt.
Wörtliche Übereinstimmungen
In den vorherigen Beispielen in diesem Tutorial als Sie die Wörter GNU und the gesucht haben, haben Sie tatsächlich nach grundlegenden regulären Ausdrücken gesucht, die mit der genauen Zeichenfolge GNU und the übereinstimmen.
Muster, die die abzugleichenden Zeichen genau spezifizieren, werden "Literale" genannt, weil sie mit dem Muster wörtlich, Zeichen für Zeichen, übereinstimmen.
Es ist hilfreich, sich vorzustellen, dass diese eher mit einer Zeichenkette als mit einem Wort übereinstimmen.
Diese Unterscheidung wird immer wichtiger, wenn Sie komplexere Muster lernen.
Alle alphabetischen und numerischen Zeichen (sowie bestimmte andere
Zeichen) werden wörtlich abgestimmt, sofern sie nicht von anderen
Ausdrucksmechanismen geändert werden.
Anker-Übereinstimmungen
Anker sind Sonderzeichen, die angeben, wo in der Zeile eine Übereinstimmung auftreten muss, um gültig zu sein.
Mit der Verwendung von Anken können Sie beispielsweise angeben, dass Sie nur über Zeilen Kenntnis erhalten möchten, die mit GNU ganz am Anfang der Zeile übereinstimmen.
Dazu könnten Sie den ^ -Anker vor der wörtlichen Zeichenfolge verwenden.
Führen Sie den folgenden Befehl aus, um die Datei GPL-3 zu durchsuchen und Zeilen zu finden, in denen GNU am Anfang einer Zeile auftritt:
Sie sehen diese beiden Zeilen:
Verwenden Sie in ähnlicher Weise den Anker $am Ende eines Musters, um anzugeben, dass die Übereinstimmung nur gültig ist, wenn sie ganz am Ende einer Zeile auftritt.
Dieser Befehl wird jede Zeile, die mit dem Wort and in der Datei GPL-3 endet, abstimmen:
Übereinstimmung mit einem beliebigen Zeichen
Das Punktzeichen (.) wird in regulären Ausdrücken verwendet, um zu bezeichnen, dass jedes einzelne Zeichen an der angegebenen Stelle vorkommen kann.
Um beispielsweise etwas in der Datei GPL-3 abzugleichen, das zwei Zeichen und dann die Zeichenfolge cept aufweist, würden Sie das folgende Muster verwenden:
Wie Sie sehen, enthält die Ausgabe Instanzen sowohl von accept als auch except sowie von Variationen der beiden Wörter.
Das Muster hätte auch mit z2cept übereingestimmt, wenn das ebenfalls gefunden worden wäre.
Klammerausdrücke
Durch das Platzieren einer Gruppe von Zeichen innerhalb von Klammern (\ [und\]) können Sie angeben, dass das Zeichen an dieser Position ein beliebiges sein kann, das innerhalb der Klammergruppe gefunden wird.
Um beispielsweise die Zeilen zu finden, die too oder two enthalten, würden Sie diese Variationen kurz und bündig mit dem folgenden Muster angeben:
Die Ausgabe zeigt, dass beide Variationen in der Datei vorhanden sind:
Die Klammerschreibweise bietet Ihnen einige interessante Optionen.
Sie können das Muster mit allem abstimmen, außer den Zeichen innerhalb einer Klammer, indem Sie die Liste der Zeichen innerhalb der Klammern mit einem ^ -Zeichen beginnen.
Dieses Beispiel ist wie das Muster .ode, aber wird nicht mit dem Muster code übereinstimmen:
Hier ist die Ausgabe, die Sie sehen:
Beachten Sie, dass in der zweiten Zeile, die zurückgegeben wird, tatsächlich das Wort code steht.
Dies ist kein Fehler des regulären Ausdrucks oder grep.
Vielmehr wurde diese Zeile zurückgegeben, weil zuvor in der Zeile das Muster mode innerhalb des Wortes model gefunden wurde.
Die Zeile wurde zurückgegeben, weil es eine Instanz gab, die mit dem Muster übereinstimmt.
Eine weitere hilfreiche Funktion von Klammern ist, dass Sie eine Reihe von Zeichen angeben können, anstatt jedes verfügbare Zeichen individuell einzugeben.
Das heißt, wenn Sie jede Zeile finden möchten, die mit einem Großbuchstaben beginnt, können Sie das folgende Muster verwenden:
Aufgrund einiger veralteter Sortierprobleme ist es oft genauer, die POSIX-Zeichenklasse anstelle von Zeichenbereichen zu verwenden, wie Sie es gerade verwendet haben.
Es gibt viele Zeichenklassen, die außerhalb des Rahmens dieses Leitfadens liegen, doch ein Beispiel, das den gleichen Prozess wie das vorherige Beispiel bewerkstelligen würde, verwendet die Zeichenklasse\ [: upper:\] innerhalb eines Klammer-Selektors:
Die Ausgabe ist die gleiche wie zuvor.
Wiederholen von Mustern null oder mehrere Male
Schließlich ist eines der am häufigsten verwendeten Metazeichen das Sternchen oder *, das bedeutet "Wiederhole das vorherige Zeichen oder den vorherigen Ausdruck null oder mehrere Male".
Um jede Zeile in der Datei GPL-3 zu finden, die eine öffnende und schließende Klammer enthält, mit nur Buchstaben und einzelnen Leerzeichen dazwischen, verwenden Sie den folgenden Ausdruck:
Bisher haben Sie Punkte, Sternchen und andere Zeichen in Ihren Ausdrücken verwendet, aber manchmal müssen Sie speziell nach diesen Zeichen suchen.
Ausklammern von Metazeichen
Es gibt Fälle, in denen Sie nach einem buchstäblichen Punkt oder einer buchstäblichen öffnenden Klammer suchen müssen, insbesondere bei der Arbeit mit Quellcode oder Konfigurationsdateien.
Da diese Zeichen in regulären Ausdrücken spezielle Bedeutung haben, müssen Sie diese Zeichen "ausklammern" und grep anweisen, dass Sie diese spezielle Bedeutung in diesem Fall nicht verwenden möchten.
Sie klammern Zeichen durch Verwendung des Backslash-Zeichens (\) vor dem Zeichen, das normalerweise eine spezielle Bedeutung hätte, aus.
Um beispielsweise jede Zeile zu finden, die mit einem Großbuchstaben beginnt und mit einem Punkt endet, verwenden Sie den folgenden Ausdruck, der den abschließenden Punkt anstellte der üblichen Bedeutung "beliebiges Zeichen" darstellt:
Dies ist die Ausgabe, die Sie sehen:
Schauen wir uns nun andere Optionen für reguläre Ausdrücke an.
Erweiterte reguläre Ausdrücke
Der Befehl grep unterstützt eine umfangreichere Sprache für reguläre Ausdrücke durch Verwendung des Flag -E oder durch den Aufruf des Befehls egrep anstelle von grep.
Diese Optionen öffnen die Funktionen von "erweiterten regulären Ausdrücken".
Erweiterte reguläre Ausdrücke umfassen alle grundlegenden Metazeichen sowie zusätzliche Metazeichen, um komplexere Übereinstimmungen auszudrücken.
Gruppierung
Eine der nützlichsten Fähigkeiten, die erweiterte reguläre Ausdrücke eröffnen, ist die Fähigkeit, Ausdrücke zu gruppieren, um sie als eine Einheit zu manipulieren oder zu referenzieren.
Gruppieren von Ausdrücken mithilfe von Klammern.
Wenn Sie
Klammern ohne Verwendung von erweiterten regulären Ausdrücken verwenden möchten, können Sie
sie mit dem Backslash ausklammern, um diese Funktionalität zu aktivieren.
Die folgenden drei Ausdrücke sind funktional äquivalent:
Alternation
Ähnlich wie Klammerausdrücke verschiedene Optionen für einzelne Zeichenübereinstimmungen angeben können, können Sie mit Alternation alternative Übereinstimmungen für Zeichenfolgen oder Ausdruckssätze angeben.
Verwenden Sie zur Angabe der Alternation das Pipe-Zeichen |.
Diese werden häufig innerhalb von Klammerausdrücken verwendet, um anzugeben, dass eine von zwei oder mehreren Möglichkeiten als Übereinstimmung betrachtet werden soll.
Der folgende findet entweder GPL oder General Public License in dem
Text:
Mit Alternation können Sie zwischen mehr als zwei Auswahlmöglichkeiten auswählen, indem Sie zusätzliche Auswahlmöglichkeiten innerhalb der Auswahlgruppe hinzufügen, die durch zusätzliche Pipe-Zeichen (|) getrennt sind.
Quantifizierer
Wie das Metazeichen *, das mit den vorherigen Zeichen oder dem Zeichensatz null oder mehrere Male übereinstimmt, gibt es andere Metazeichen in erweiterten regulären Ausdrücken, die die Anzahl der Vorkommen angeben.
Um ein Zeichen null oder ein Mal abzugleichen, können Sie das Zeichen?
verwenden.
Dadurch werden die vorhergehenden Zeichen oder Zeichensätze im Wesentlichen optional.
Der folgende gleicht copyright und right ab, indem er copy in eine optionale Gruppe setzt:
Das Zeichen + entspricht einem Ausdruck ein oder mehrere Male.
Dies entspricht fast dem Metazeichen *, aber mit dem Zeichen + muss der Ausdruck mindestens einmal übereinstimmen.
Der folgende Ausdruck entspricht der Zeichenfolge free plus ein oder mehrere Zeichen, die keine Leerzeichen sind:
Angeben der Wiederholung der Übereinstimmung
Um die Anzahl der Male, die eine Übereinstimmung wiederholt wird, anzugeben, verwenden Sie die Klammerzeichen ({und}).
Mit diesen Zeichen können Sie eine genaue Zahl, einen Bereich oder eine obere oder untere Grenze für die Anzahl der Übereinstimmungen eines Ausdrucks angeben.
Verwenden Sie den folgenden Ausdruck, um alle Zeilen in der Datei GPL-3 zu finden, die dreifache Vokale enthalten:
Jede zurückgegebene Zeile hat ein Wort mit drei Vokalen:
Um alle Wörter abzugleichen, die zwischen 16 und 20 Zeichen haben, verwenden Sie den folgenden Ausdruck:
Es werden nur Zeilen angezeigt, die Wörter innerhalb dieser Länge enthalten.
grep ist nützlich, um Muster innerhalb von Dateien oder innerhalb der Hierarchie des Dateisystems zu finden, daher lohnt es sich, sich mit seinen Optionen und seiner Syntax vertraut zu machen.
Reguläre Ausdrücke sind noch vielseitiger und können mit vielen gängigen Programmen verwendet werden.
Viele Texteditoren implementieren beispielsweise reguläre Ausdrücke für das Suchen und Ersetzen von Text.
Außerdem verwenden die meisten modernen Programmiersprachen reguläre Ausdrücke, um Verfahren auf bestimmte Datenstücke durchzuführen. Sobald Sie reguläre Ausdrücke verstehen, können Sie dieses Wissen auf viele gängige computerbezogene Aufgaben übertragen, von der Durchführung erweiterter Suchen in Ihrem Texteditor bis hin zur Validierung von Benutzereingaben.
UFW-Grundlagen: Allgemeine Firewall-Regeln und -Befehle
UFW ist ein Firewall-Konfigurationswerkzeug für iptables, das standardmäßig in Ubuntu enthalten ist.
Dieser Leitfaden im Stil eines Spickzettels bietet eine schnelle Referenz zu UFW-Befehlen, mit denen iptables-Firewall-Regeln erstellt werden können, die in gängigen, alltäglichen Szenarien nützlich sind.
Dies beinhaltet UFW-Beispiele für das Zulassen und Blockieren verschiedener Dienste nach Port, Netzwerkschnittstelle und Quell-IP-Adresse.
1737
So verwenden Sie diesen Leitfaden
Wenn Sie gerade erst mit der Verwendung von UFW zur Konfiguration Ihrer Firewall beginnen, lesen Sie unsere Einführung in UFW.
Bei den meisten der hierin beschriebenen Regeln wird davon ausgegangen, dass Sie den Standard-UFW-Regelsatz verwenden.
Das heißt, es ist eingestellt, dass ausgehender Datenverkehr zugelassen und eingehender Datenverkehr über die Standardrichtlinie blockiert wird, sodass Sie den Verkehr selektiv zulassen müssen.
Verwenden Sie die nachfolgenden Abschnitte für das, was Sie erreichen möchten.
Die meisten Abschnitte hängen nicht von den anderen ab, sodass Sie die nachstehenden Beispiele unabhängig voneinander verwenden können.
Verwenden Sie das Menü "Inhalt" auf der rechten Seite dieser Seite (bei breiter Seitenbreite) oder die Suchfunktion Ihres Browsers, um die gewünschten Abschnitte zu finden.
Kopieren Sie die angegebenen Beispiele der Befehlszeilen und fügen Sie sie ein, indem Sie die Werte in Rot mit Ihren eigenen Werten ersetzen.
Denken Sie daran, dass Sie Ihren aktuellen UFW-Regelsatz mit sudo ufw status oder sudo ufw status verbose überprüfen können.
Blockieren einer IP-Adresse
Um alle Netzwerkverbindungen zu blockieren, die von einer bestimmten IP-Adresse stammen, beispielsweise 15.15.15.51, führen Sie diesen Befehl aus:
In diesem Beispiel gibt von 15.15.15.51 eine Quell-IP-Adresse von "15.15.15.51" an.
Wenn Sie möchten, können Sie hier stattdessen ein Subnetz wie 15.15.15.0 / 24 angeben.
Die Quell-IP-Adresse kann in jeder Firewall-Regel angegeben werden, auch in einer allow (Zulassen) -Regel.
Blockieren von Verbindungen zu einer Netzwerkschnittstelle
Um Verbindungen von einer bestimmten IP-Adresse wie 15.15.15.51 an eine bestimmte Netzwerkschnittstelle, z. B. eth0 zu blockieren, verwenden Sie diesen Befehl:
Dies ist das gleiche wie das vorherige Beispiel dem Zusatz von in on eth0.
Die Netzwerkschnittstelle kann in jeder Firewall-Regel angegeben werden, und ist eine großartige Möglichkeit, die Regel auf ein bestimmtes Netzwerk zu beschränken.
Dienst: SSH
Wenn Sie einen Cloud-Server verwenden, werden Sie wahrscheinlich eingehende SSH-Verbindungen (Port 22) zulassen wollen, damit Sie sich mit Ihrem Server verbinden und den Server verwalten können.
Dieser Abschnitt behandelt, wie Sie Ihre Firewall mit verschiedenen SSH-bezogenen Regeln konfigurieren.
Zulassen von SSH-Verbindungen
Um alle eingehenden SSH-Verbindungen zuzulassen, führen Sie diesen Befehl aus:
Eine alternative Syntax ist die Angabe der Portnummer des SSH-Dienstes:
Zulassen von eingehenden SSH-Verbindungen von bestimmten IP-Adressen oder Subnetzen
Um eingehende SSH-Verbindungen von einer bestimmten IP-Adresse oder einem Subnetz zuzulassen, geben Sie die Quelle an.
Wenn Sie beispielsweise das gesamte Subnetz 15.15.15.0 / 24 zulassen möchten, führen Sie diesen Befehl aus:
Zulassen von eingehenden Rsync-Verbindungen von bestimmten IP-Adressen oder Subnetzen
Rsync, das auf Port 873 läuft, kann verwendet werden, um Dateien von einem Computer auf einen anderen Computer zu übertragen.
Um eingehende rsync-Verbindungen von einer bestimmten IP-Adresse oder einem Subnetz zuzulassen, geben Sie die Quell-IP-Adresse und den Ziel-Port an. Wenn Sie beispielsweise dem gesamten Subnetz 15.15.15.0 / 24 das rsync auf Ihren Server erlauben möchten, führen Sie diesen Befehl aus:
Dienst: Webserver
Webserver, wie Apache und Nginx, hören typischerweise auf Anfragen an Port 80 und 443 für HTTP-Verbindungen bzw. HTTPS-Verbindungen.
Wenn Ihre Standardrichtlinie für eingehenden Datenverkehr auf "Verwerfen" (Drop) oder "Ablehnen" (Deny) eingestellt ist, müssen Sie Regeln erstellen, die es Ihrem Server erlauben, auf diese Anfragen zu antworten.
Zulassen aller eingehenden HTTP-Verbindungen
Um alle eingehenden HTTP-Verbindungen (Port 80) zuzulassen, führen Sie diesen Befehl aus:
Eine alternative Syntax ist die Angabe der Portnummer des HTTP-Dienstes:
Zulassen aller eingehenden HTTPS-Verbindungen
Um alle eingehenden HTTPS-Verbindungen (Port 443) zuzulassen, führen Sie diesen Befehl aus:
Eine alternative Syntax ist die Angabe der Portnummer des HTTPS-Dienstes:
Zulassen aller eingehenden HTTP-Verbindungen und HTTPS-Verbindungen
Wenn Sie sowohl HTTP-Datenverkehr als auch HTTPS-Datenverkehr zulassen möchten, können Sie eine einzige Regel erstellen, die beide Ports zulässt.
Um alle eingehenden HTTP- und HTTPS-Verbindungen (Port 443) zuzulassen, führen Sie diesen Befehl aus:
Beachten Sie, dass Sie das Protokoll mit proto tcp angeben müssen, wenn Sie mehrere Ports angeben.
Dienst: MySQL
MySQL hört für Client-Verbindungen auf Port 3306.
Wenn Ihr MySQL-Datenbankserver von einem Client auf einem Remote-Server verwendet wird, müssen Sie sicher sein, dass er diesen Datenverkehr zulässt.
Zulassen von MySQL-Verbindungen von bestimmten IP-Adressen oder Subnetzen
Um eingehende MySQL-Verbindungen von einer bestimmten IP-Adresse oder einem Subnetz zuzulassen, geben Sie die Quelle an.
Zulassen von MySQL-Verbindungen zu einer bestimmten Netzwerkschnittstelle
Um MySQL-Verbindungen zu einer bestimmten Netzwerkschnittstelle zuzulassen, beispielsweise zu Ihrer privaten Netzwerkschnittstelle eth1, verwenden Sie diesen Befehl:
Dienst: PostgreSQL
PostgreSQL hört für Client-Verbindungen auf Port 5432.
Wenn Ihr PostgreSQL-Datenbankserver von einem Client auf einem Remote-Server verwendet wird, müssen Sie sicher sein, dass er diesen Datenverkehr zulässt.
PostgreSQL von einer bestimmten IP-Adresse oder einem Subnetz
Um eingehende PostgreSQL-Verbindungen von einer bestimmten IP-Adresse oder einem Subnetz zuzulassen, geben Sie die Quelle an.
Der zweite Befehl, der den ausgehenden Datenverkehr von etablierten PostgreSQL-Verbindungen ermöglicht, ist nur notwendig, wenn die Richtlinie OUTPUT (Ausgabe) nicht auf ACCEPT (akzeptieren) eingestellt ist.
Zulassen von PostgreSQL-Verbindungen zu einer bestimmten Netzwerkschnittstelle
Um PostgreSQL-Verbindungen zu einer bestimmten Netzwerkschnittstelle zuzulassen, beispielsweise zu Ihrer privaten Netzwerkschnittstelle eth1, verwenden Sie diesen Befehl:
Dienst: Mail
Mail-Server wie Sendmail und Postfix hören auf verschiedenen Ports, je nachdem, welche Protokolle für die Mailzustellung verwendet werden.
Wenn Sie einen Mailserver ausführen, ermitteln Sie, welche Protokolle Sie verwenden, und lassen die entsprechenden Arten von Datenverkehr zu.
Wir zeigen Ihnen auch, wie Sie eine Regel zum Blockieren ausgehender SMTP-Mails erstellen.
Blockieren von ausgehender SMTP-Mail
Wenn Ihr Server keine ausgehenden Mails senden soll, möchten Sie diese Art von Datenverkehr vielleicht blockieren.
Um ausgehende SMTP-Mails zu blockieren, die Port 25 verwenden, führen Sie diesen Befehl aus:
Dies konfiguriert Ihre Firewall, um den gesamten ausgehenden Datenverkehr auf Port 25 zu verwerfen. Wenn Sie einen anderen Dienst durch seine Portnummer ablehnen müssen, anstelle von Port 25, ersetzen Sie den Port einfach.
Zulassen aller eingehenden SMTP-Verbindungen
Um Ihrem Server zu erlauben, auf SMTP-Verbindungen an Port 25 zu antworten, führen Sie diesen Befehl aus:
Anmerkung: Es ist üblich, dass SMTP-Server Port 587 für ausgehende Mails verwenden.
Zulassen aller eingehenden IMAP-Verbindungen
Um Ihrem Server zu erlauben, auf IMAP-Verbindungen an Port 143 zu antworten, führen Sie diesen Befehl aus:
Zulassen aller eingehenden IMAPS-Verbindungen
Um Ihrem Server zu erlauben, auf IMAPS-Verbindungen an Port 993 zu antworten, führen Sie diesen Befehl aus:
Zulassen aller eingehenden POP3-Verbindungen
Um Ihrem Server zu erlauben, auf POP3-Verbindungen an Port 110 zu antworten, führen Sie diesen Befehl aus:
Zulassen aller eingehenden POP3S-Verbindungen
Um Ihrem Server zu erlauben, auf POP3S-Verbindungen an Port 995 zu antworten, führen Sie diesen Befehl aus:
Dies sollte viele der Befehle abdecken, die üblicherweise bei der Verwendung von UFW zur Konfiguration einer Firewall verwendet werden.
Natürlich ist UFW ein sehr flexibles Tool, sodass Sie die Befehle mit verschiedenen Optionen mischen und anpassen können, um Ihren speziellen Anforderungen gerecht zu werden, falls diese hier nicht abgedeckt sind.
Viel Erfolg!
Konfigurieren eines produktionsbereiten Mesosphere-Clusters unter Ubuntu 14.04
In diesem Leitfaden gehen wir auf die Konfiguration eines hochverfügbaren Clusters in Mesosphere ein.
Diese Konfiguration sieht eine Ausfallsicherung für den Fall vor, dass einer unserer Master-Knoten ausfällt, sowie einen Pool von Slave-Servern, die die anstehenden Aufgaben verarbeiten.
1321
Mesosphere ist ein System, das eine Reihe von Komponenten zur effektiven Verwaltung von Server-Clustering und hochverfügbaren Bereitstellungen auf einer vorhandenen Betriebssystemschicht kombiniert.
Im Gegensatz zu Systemen wie CoreOS ist Mesosphere kein spezialisiertes Betriebssystem, sondern ein Satz von Paketen.
Für diesen Leitfaden verwenden wir Ubuntu 14.04-Server.
Bevor Sie diesen Leitfaden durcharbeiten, wird dringend empfohlen, unsere Einführung in Mesosphere zu lesen.
Dies ist eine gute Möglichkeit, sich mit den Komponenten, aus denen das System besteht, vertraut zu machen und Ihnen dabei zu helfen, zu identifizieren, wofür jede Einheit zuständig ist.
Während dieses Tutorials werden wir sechs Ubuntu-Server verwenden.
Damit wird die Mesos-Empfehlung von Apache erfüllt, zumindest drei Master für eine Produktionsumgebung zu haben.
Es stellt außerdem einen Pool von drei Worker- oder Slave-Servern bereit, denen Arbeit zugewiesen wird, wenn Aufgaben an den Cluster gesendet werden.
Die sechs von uns verwendeten Server verwenden zookeeper, um den aktuellen Führer der Master-Servers zu verfolgen.
Die darauf aufbauende Mesos-Schicht wird für die verteilte Synchronisierung und Ressourcenverwaltung sorgen.
Sie ist für die Verwaltung des Clusters verantwortlich.
Marathon, das verteilte Init-System des Clusters, wird verwendet, um Aufgaben zu planen und die Arbeit an die Slave-Server zu verteilen.
Für diesen Leitfaden gehen wir davon aus, dass unsere Rechner die folgende Konfiguration haben:
Hostname
Funktion
IP-Adresse
Master1
Mesos-Master
192.0.2.1
Master2
192.0.2.2
Master3
192.0.2.3
Slave1
Mesos-Slave
192.0.2.51
Slave2
192.0.2.52
Slave3
192.0.2.53
Auf jedem dieser Rechner sollte Ubuntu 14.04 installiert sein.
Führen Sie die grundlegenden Konfigurationsschritte aus, die in unserem Leitfaden zur Ersteinrichtung des Servers Ubuntu 14.04 aufgeführt sind.
Wenn Sie mit den obigen Schritten fertig sind, fahren Sie mit diesem Leitfaden fort.
Installieren von Mesosphere auf den Servern
Der erste Schritt, um Ihren Cluster zum Laufen zu bringen, ist die Installation der Software.
Zum Glück unterhält das Mesosphere-Projekt ein Ubuntu-Repository mit aktuellen Paketen, die einfach zu installieren sind.
Hinzufügen der Mesosphere-Repositorys zu Ihren Hosts
Führen Sie auf allen Hosts (Master und Slaves) die folgenden Schritte durch.
Fügen Sie zunächst das Mesosphere-Repository zu Ihrer Quellenliste hinzu.
Dazu müssen Sie den Schlüssel des Mesosphere-Projekts vom Ubuntu-Schlüsselserver herunterladen und dann die richtige URL für unsere Ubuntu-Version erstellen.
Das Projekt bietet eine bequeme Möglichkeit, dies zu tun:
Installieren der notwendigen Komponenten
Nachdem Sie das Mesosphere-Repository zu Ihrem System hinzugefügt haben, müssen Sie Ihren lokalen Paket-Cache aktualisieren, um Zugriff auf die neue Komponente zu erhalten:
Als Nächstes müssen Sie die notwendigen Pakete installieren.
Welche Komponenten Sie benötigen, hängt von der Rolle des Hosts ab.
Für Ihre Master-Hosts benötigen Sie das mesosphere Meta-Paket.
Diese enthält die Anwendungen zookeeper, mesos, marathon und chronos:
Für Ihre Slave-Hosts benötigen Sie nur das Paket mesos, das auch zookeeper als Abhängigkeit mit einbezieht:
Einrichten der Zookeeper-Verbindungsinformationen für Mesos
Als Erstes konfigurieren wir unsere zookeeper-Verbindungsinformationen.
Dies ist die zugrunde liegende Schicht, die es allen unseren Hosts ermöglicht, sich mit den richtigen Master-Servern zu verbinden, daher ist es sinnvoll, hier zu beginnen.
Unsere Master-Server werden die einzigen Mitglieder unseres zookeeper-Clusters sein, aber alle unsere Server müssen konfiguriert werden, damit sie über das Protokoll kommunizieren können.
Die Datei, die dies definiert, ist / etc / mesos / zk.
Führen Sie auf allen Ihren Hosts den folgenden Schritt durch.
Öffnen Sie die Datei mit root-Berechtigungen:
Innerhalb der Datei finden Sie die Verbindungs-URL, die standardmäßig für den Zugriff auf eine lokale Instanz eingestellt ist.
Wir müssen dies ändern, um auf unsere drei Master-Server zu verweisen.
Dies geschieht durch das Ersetzen von localhost durch die IP-Adresse unseres ersten Meso-Master-Servers.
Wir können dann ein Komma nach der Port-Angabe hinzufügen und das Format wiederholen, um unseren zweiten und dritten Master in die Liste aufzunehmen.
Für unseren Leitfaden haben unsere Master IP-Adressen von 192.0.2.1, 192.168.2.2 und 192.168.2.3.
Mit diesen Werten sieht unsere Datei wie folgt aus:
Die Zeile muss mit zk: / / beginnen und mit / mesos enden.
Dazwischen werden die IP-Adressen Ihrer Master-Server und die zookeeper-Ports (standardmäßig 2181) angegeben.
Verwenden Sie diesen identischen Eintrag in jedem Ihrer Master und Slaves.
Dies hilft jedem einzelnen Server, sich mit den richtigen Master-Servern zu verbinden, um mit dem Cluster zu kommunizieren.
Konfigurieren der Zookeeper-Konfiguration der Master-Server
Auf Ihren Master-Servern müssen wir einige zusätzliche zookeeper-Konfigurationen vornehmen.
Der erste Schritt ist die Definition einer eindeutigen ID-Nummer von 1 bis 255 für jeden Ihrer Master-Server.
Diese wird in der Datei / etc / zookeeper / conf / myid gespeichert.
Öffnen Sie sie jetzt.
Löschen Sie alle Angaben in dieser Datei und ersetzen Sie sie durch eine einzelne Nummer, von 1 bis 255. Jeder Ihrer Master-Server muss eine eindeutige Nummer haben.
Der Einfachheit halber ist es am einfachsten, bei 1 zu beginnen und sich hochzuarbeiten.
Für unseren Leitfaden verwenden wir 1, 2 und 3.
Unser erster Server wird nur dies in der Datei haben:
Führen Sie dies auf jedem Ihrer Master-Server aus.
Als Nächstes müssen wir unsere zookeeper-Konfigurationsdatei ändern, um unsere zookeeper-IDs aktuellen Hosts zuzuordnen.
Dadurch wird sichergestellt, dass der Dienst jeden Host korrekt aus dem von ihm verwendeten ID-System auflösen kann.
Öffnen Sie nun die zookeeper-Konfigurationsdatei:
Innerhalb dieser Datei müssen Sie jede ID einem Host zuordnen.
Die Host-Spezifikation wird zwei Ports beinhalten, den ersten für die Kommunikation mit dem Führer und den zweiten für die Handhabung von Wahlen, wenn ein neuer Führer erforderlich ist.
Die zookeeper-Server werden durch "server", gefolgt von einem Punkt und ihrer ID-Nummer identifiziert.
Für unseren Leitfaden verwenden wir die Standard-Ports für jede Funktion und unsere IDs sind 1-3.
Unsere Datei wird so aussehen:
Fügen Sie dieselben Zuordnungen in die Konfigurationsdateien jedes Ihrer Master-Server ein.
Damit ist unsere zookeeper-Konfiguration abgeschlossen.
Wir können uns nun auf Mesos und Marathon konzentrieren.
Konfigurieren von Mesos auf den Master-Servern
Als Nächstes konfigurieren wir Mesos auf den drei Master-Servern.
Diese Schritte sollten auf jedem Ihrer Master-Server durchgeführt werden.
Anpassen des Quorums an Ihre Cluster-Größe
Zuerst müssen wir das für die Entscheidungsfindung notwendige Quorum anpassen.
Damit wird die Anzahl der Hosts festgelegt, die für einen funktionierenden Cluster erforderlich sind.
Das Quorum sollte so eingestellt werden, dass über 50 Prozent der Master-Mitglieder anwesend sein müssen, um Entscheidungen zu treffen.
Wir wollen aber auch eine gewisse Fehlertoleranz einbauen, damit der Cluster auch dann noch funktioniert, wenn nicht alle Master anwesend sind.
Wir haben drei Master, also ist die einzige Einstellung, die beide Anforderungen erfüllt, ein Quorum von zwei.
Da die anfängliche Konfiguration von einer Einrichtung mit einem einzelnen Server ausgeht, ist das Quorum derzeit auf eins eingestellt.
Öffnen Sie die Quorum-Konfigurationsdatei:
Ändern Sie den Wert auf "2 ":
Wiederholen Sie dies auf jedem Ihrer Master-Server.
Konfigurieren des Hostnamens und der IP-Adresse
Als Nächstes geben wir den Hostnamen und die IP-Adresse für jeden unserer Master-Server an.
Wir verwenden die IP-Adresse für den Hostnamen, damit unsere Instanzen keine Probleme mit der korrekten Auflösung haben werden.
Für unsere Master-Server muss die IP-Adresse in diese Dateien eingetragen werden:
/ etc / mesos-master / ip
/ etc / mesos-master / hostname
Fügen Sie zunächst die individuelle IP-Adresse jedes Master-Knotens in die Datei / etc / mesos-master / ip ein.
Denken Sie daran, diese für jeden Server zu ändern, damit sie dem entsprechenden Wert entspricht:
Jetzt können wir diesen Wert in die Datei hostname kopieren:
Konfigurieren von Marathon auf den Master-Servern
Nachdem Mesos nun konfiguriert ist, können wir Marathon konfigurieren, die Implementierung des Cluster-Init-Systems von Mesosphere.
Marathon wird auf jedem unserer Master-Hosts ausgeführt, aber nur der führende Master-Server wird in der Lage sein, tatsächlich Aufgaben zu planen.
Die anderen Marathon-Instanzen werden Anfragen transparent an den Master-Server weiterleiten.
Zuerst müssen wir wieder den Hostnamen für die Marathon-Instanz jedes Servers festlegen.
Auch hier verwenden wir die IP-Adresse, die wir bereits in einer Datei haben.
Diese können wir an den gewünschten Speicherort der Datei kopieren.
Jedoch wird die von uns benötigte Verzeichnisstruktur der Marathon-Konfiguration nicht automatisch erstellt.
Wir müssen das Verzeichnis erstellen und können dann die Datei kopieren:
Als Nächstes müssen wir die Liste der zookeeper-Master definieren, mit denen sich Marathon für Informationen und Planung verbinden wird.
Dies ist die gleiche zookeeper-Verbindungszeichenfolge, die wir schon für Mesos verwendet haben, also können wir die Datei einfach kopieren.
Wir müssen sie in einer Datei namens master ablegen:
Dies ermöglicht unserem Marathon-Dienst, sich mit dem Mesos-Cluster zu verbinden.
Wir wollen jedoch auch, dass Marathon seine eigenen Statusinformationen in zookeeper speichert.
Dafür verwenden wir die andere zookeeper-Verbindungsdatei als Grundlage und ändern nur den Endpunkt.
Kopieren Sie zunächst die Datei an den Marathon-zookeeper-Speicherort:
Öffnen Sie als Nächstes die Datei in Ihrem Editor:
Der einzige Abschnitt, den wir in dieser Datei ändern müssen, ist der Endpunkt.
Wir ändern ihn von / mesos zu / marathon:
Dies ist alles, was wir für unsere Marathon-Konfiguration tun müssen.
Konfigurieren von Dienst-Init-Regeln und Neustarten von Diensten
Als Nächstes starten wir die Dienste unserer Master-Server neu, um die Einstellungen zu verwenden, die wir konfiguriert haben.
Zuerst müssen wir sicherstellen, dass unsere Master-Server nur den Mesos-Master-Prozess und nicht den Slave-Prozess ausführen.
Wir können alle derzeit laufenden Slave-Prozesse stoppen (dies kann fehlschlagen, aber das ist in Ordnung, da dies nur dazu dient, sicherzustellen, dass der Prozess gestoppt wird).
Wir können auch sicherstellen, dass der Server den Slave-Prozess beim Booten nicht startet, indem wir eine Override-Datei erstellen:
Jetzt müssen wir nur noch zookeeper neu starten, wodurch unsere Master-Wahlen eingerichtet werden.
Wir können dann unsere Mesos-Master- und Marathon-Prozesse starten:
Um einen Blick darauf zu werfen, was Sie gerade eingerichtet haben, besuchen Sie einen Ihrer Master-Server in Ihrem Webbrowser an Port 5050:
Sie sollte die Hauptoberfläche von Mesos sehen.
Möglicherweise wird Ihnen mitgeteilt, dass Sie zu dem aktiven Master umgeleitet werden, je nachdem, ob Sie sich mit dem gewählten Führer verbunden haben oder nicht.
In jedem Fall sieht der Bildschirm ähnlich wie folgt aus:
Mesos Hauptoberfläche
Dies ist eine Ansicht Ihres aktuellen Clusters
Es gibt nicht viel zu sehen, da keine Slave-Knoten vorhanden sind und keine Aufgaben gestartet sind.
Wir haben auch Marathon konfiguriert, den langjährigen Task-Controller von Mesosphere.
Dieser wird an Port 8080 auf jedem Ihrer Master verfügbar sein:
Marathon Hauptoberfläche
Wir werden kurz darauf eingehen, wie diese Oberflächen verwendet werden, sobald wir unsere Slaves eingerichtet haben.
Konfigurieren der Slave-Server
Nachdem wir nun unsere Master-Server konfiguriert haben, können wir mit der Konfiguration unserer Slave-Server beginnen.
Wir haben unsere Slaves bereits mit den zookeeper-Verbindungsinformationen unserer Master-Server konfiguriert.
Die Slaves selbst führen keine eigenen zookeeper-Instanzen aus.
Wir können alle zookeeper-Prozesse, die derzeit auf unseren Slave-Knoten laufen, stoppen und eine Override-Datei erstellen, damit sie nicht automatisch gestartet werden, wenn der Server neu startet:
Als Nächstes möchten wir eine andere Override-Datei erstellen, um sicherzustellen, dass der Mesos-Master-Prozess nicht auf unseren Slave-Servern startet.
Wir stellen auch sicher, dass er derzeit gestoppt ist (dieser Befehl kann fehlschlagen, wenn der Prozess bereits gestoppt ist.
Dies ist kein Problem):
Als Nächstes müssen wir die IP-Adresse und den Hostnamen festlegen, genau wie wir es für unsere Master-Server getan haben.
Dies beinhaltet die Bereitstellung der IP-Adresse jedes Knotens, diesmal unter dem Verzeichnis / etc / mesos-slave.
Wir verwenden dies auch als Hostnamen, um den Zugriff auf die Dienste über die Weboberfläche zu erleichtern:
Verwenden Sie erneut die individuelle IP-Adresse jedes Slave-Servers für den ersten Befehl.
Dadurch wird sichergestellt, dass sie an die richtige Oberfläche gebunden ist.
Nun haben wir alle Voraussetzungen geschaffen, um unsere Mesos-Slaves zu starten.
Wir müssen nur den Dienst aktivieren:
Führen Sie dies auf jedem Ihrer Slave-Rechner aus.
Um zu sehen, ob sich Ihre Slaves erfolgreich in Ihrem Cluster registrieren, kehren Sie zu Ihrem Master-Server an Port 5050 zurück:
Sie sollten auf der Oberfläche die Anzahl aktiver Slaves als "3" sehen:
Drei Slaves auf Mesos
Sie können auch sehen, dass die verfügbaren Ressourcen in der Oberfläche aktualisiert wurden, um die gepoolten Ressourcen Ihrer Slave-Rechner widerzuspiegeln:
Mesos-Ressourcen
Um zusätzliche Informationen über jeden Ihrer Slave-Rechner zu erhalten, können Sie auf den Link "Slaves" oben in der Oberfläche klicken.
Dadurch erhalten Sie einen Überblick über den Ressourcenbeitrag jedes Rechners als auch Links zu einer Seite für jeden Slave:
Mesos-Slavesseite
Starten von Diensten auf Mesos und Marathon
Marathon ist das Dienstprogramm von Mesosphere für die Planung lang laufender Aufgaben.
Es ist einfach, sich Marathon als das Init-System für einen Mesosphere-Cluster vorzustellen, da es das Starten und Stoppen von Diensten, das Planen von Aufgaben und das Sicherstellen, dass Anwendungen wieder ausgeführt werden, wenn sie ausfallen, übernimmt.
Sie können Dienste und Aufgaben auf verschiedene Weise zu Marathon hinzufügen.
Wir werden hier nur grundlegende Dienste behandeln.
Docker-Container werden in einem zukünftigen Leitfaden behandelt.
Starten eines Dienstes über die Weboberfläche
Der einfachste Weg, um einen Dienst schnell auf dem Cluster zu starten, ist das Hinzufügen einer Anwendung über die Marathon-Oberfläche.
Rufen Sie zunächst die Marathon-Weboberfläche auf einem Ihrer Master-Server auf.
Denken Sie daran: Die Marathon-Oberfläche ist auf Port 8080:
Von hier aus können Sie auf die Schaltfläche "New App" (Neue Anwendung) in der oberen rechten Ecke klicken.
Dadurch wird ein Overlay eingeblendet, in dem Sie Informationen über Ihre neue Anwendung hinzufügen können:
Marathon - Neue Anwendung
Füllen Sie die Felder mit den Anforderungen für Ihre Anwendung aus. Die einzigen obligatorischen Felder sind:
ID: Eine von dem Benutzer gewählte eindeutige ID zur Identifizierung eines Prozesses.
Dies kann sein, was immer Sie möchten, muss aber eindeutig sein.
Command: Dies ist der eigentliche Befehl, der von Marathon ausgeführt werden soll.
Dies ist der Prozess, der überwacht und neu gestartet werden soll, wenn er fehlschlägt.
Mit diesen Informationen können Sie einen einfachen Dienst einrichten, der nur "hello" ausgibt und 10 Sekunden lang schläft.
Wir nennen diese "hello ":
Marathon - Einfache Anwendung
Wenn Sie zu der Oberfläche zurückkehren, wechselt der Dienst von "Deploying" (Bereitstellen) zu "Running" (Ausführen):
Marathon-Anwendung wird ausgeführt
Etwa alle 10 Sekunden wechselt die Anzeige "Tasks / Instances" (Aufgaben / Instanzen) von "1 / 1" auf "0 / 1", wenn die Ruhezeit abgelaufen ist und der Dienst stoppt.
Marathon startet den Dienst dann automatisch neu.
Wir können diesen Prozess deutlicher in der Weboberfläche von Mesos an Port 5050 sehen:
Hier können Sie sehen, wie der Prozess beendet und neu gestartet wird:
Mesos startet Aufgabe neu
Wenn Sie bei einer der Aufgaben auf "Sandbox" und dann "stdout" klicken, können Sie sehen, wie die Ausgabe "hello" produziert wird:
Mesos-Ausgabe
Starten eines Dienstes über die API
Wir können Dienste auch über die API von Marathon eingeben.
Dies beinhaltet die Übergabe eines JSON-Objekts, das alle Felder enthält, die das Overlay enthalten hat.
Dies ist ein relativ einfacher Prozess.
Auch hier sind die einzigen erforderlichen Felder id für den Prozessidentifikator und cmd, das den eigentlichen Befehl enthält, der ausgeführt werden soll.
Mit diesen Informationen könnten wir also eine JSON-Datei namens hello.json erstellen:
In dieser würde die minimale Spezifikation so aussehen:
Dieser Dienst wird gut funktionieren.
Wenn wir jedoch den Dienst, den wir in der Web-Benutzeroberfläche erstellt haben, wirklich emulieren möchten, müssen wir einige zusätzliche Felder hinzufügen.
Diese waren in der Web-Benutzeroberfläche voreingestellt und wir können sie hier replizieren:
Wenn Sie fertig sind, speichern und schließen Sie die JSON-Datei.
Als Nächstes können wir sie mit der Marathon-API übermitteln.
Das Ziel ist ein Marathon-Dienst unserer Master an Port 8080 und der Endpunkt ist / v2 / apps.
Die Daten-Nutzlast ist unsere JSON-Datei, die wir in curl einlesen können, indem wir das Flag -d mit dem Flag @ verwenden, um eine Datei anzugeben.
Der zu übermittelnde Befehl sieht wie folgt aus:
Wenn wir die Marathon-Oberfläche ansehen, können wir sehen, dass sie erfolgreich hinzugefügt wurde.
Er scheint die gleichen Eigenschaften wie unser erster Dienst zu haben:
Marathon mit zwei Diensten
Der neue Dienst kann auf die gleiche Weise überwacht und angesprochen werden wie der erste Dienst.
An diesem Punkt sollten Sie einen produktionsbereiten Mesosphere-Cluster in Betrieb haben.
Wir haben an dieser Stelle nur die Grundkonfiguration behandelt, jedoch sollten Sie in der Lage sein, die Möglichkeiten der Nutzung des Mesosphere-Systems zu erkennen.
In zukünftigen Leitfäden werden wir den Einsatz von Docker-Containern in Ihrem Cluster und die Verwendung einiger der Tools ausführlicher behandeln.
Im zweiten Beispiel haben wir eine neue Zeichenfolge () verwendet, um ein Zeichenfolgenobjekt zu erstellen und einer Variable zuzuweisen.
Meistens erstellen Sie Zeichenfolgenprimitive.
Im Wesentlichen stehen allen Zeichenfolgen Methoden und Eigenschaften zur Verfügung. Im Hintergrund führt JavaScript bei jedem Aufruf einer Methode oder Eigenschaft eine Konvertierung in ein Objekt und zurück in eine Primitive durch.
Wie Zeichenfolgen indiziert sind
Jedes der Zeichen in einer Zeichenfolge entspricht einer Indexnummer, beginnend mit 0.
w
e
5
10
11
Zugriff auf Zeichen
Mit der quadratischen Klammernotation können wir auf jedes Zeichen in der Zeichenfolge zugreifen.
Alternativ können wir indexOf () verwenden, um die Indexnummer durch die erste Instanz eines Zeichens zurückzugeben.
Konvertieren in Groß- oder Kleinschreibung
JavaScript bietet eine sehr nützliche Methode, um eine Zeichenfolge durch ein Zeichen zu teilen und aus den Abschnitten eine neue Anordnung zu erstellen.
Wir können nicht nur einen Wert durch einen anderen Zeichenfolgenwert ersetzen, sondern auch reguläre Ausdrücke verwenden, um replace () leistungsfähiger zu machen.
Dies ist eine sehr häufige Aufgabe, die reguläre Ausdrücke verwendet.
Besuchen Sie Regexr, um weitere Beispiele für RegEx zu üben.
Eine allgemeinere Übersicht über Zeichenfolgen finden Sie im Lernprogramm "So arbeiten Sie mit Zeichenfolgen in JavaScript".
In diesem Artikel zeigen wir Ihnen, wie Sie Ihre MongoDB-Datenbanken sichern, wiederherstellen und migrieren.
7055
Die einfachste Lösung für dieses Problem besteht darin, die Exporte und Sicherungen während der Nacht oder außerhalb der Stoßzeiten auszuführen.
Beachten Sie, dass wir im obigen Verzeichnispfad das Datum + "% m-% d-% y" verwendet haben, das automatisch das aktuelle Datum abruft.
Auf diese Weise können wir Backups im Verzeichnis wie / var / backups / < ^ > 10-29-20 < ^ > / erstellen.
Fügen Sie in die Crontab-Eingabeaufforderung den folgenden mongodump-Befehl ein:
Aus diesem Grund wird auch empfohlen, die alten Backups regelmäßig zu bereinigen oder zu komprimieren.
Um beispielsweise alle Backups zu löschen, die älter als sieben Tage sind, können Sie den folgenden Bash-Befehl verwenden:
Er sollte kurz vor dem Start des nächsten Backups ausgeführt werden, z. B. um 03: 01 Uhr.
Wenn Sie alle Aufgaben in diesem Schritt ausführen, wird eine ordnungsgemäße Backup-Lösung für Ihre MongoDB-Datenbanken sichergestellt.
Wir verwenden newdb. * zum Wiederherstellen aller Sammlungen.
Wenn wir dann --drop verwenden, stellen wir sicher, dass die Zieldatenbank zuerst weggelassen wird, sodass das Backup in einer sauberen Datenbank wiederhergestellt wird.
Sobald Sie ein zeitgestempeltes Backup haben, können Sie dieses mit folgendem Befehl wiederherstellen:
So installieren Sie MongoDB aus den Standard-APT-Repositorys unter Ubuntu 20.04
6903
MongoDB ist eine kostenlose und Open-Source-NoSQL-Dokumentendatenbank, die häufig in modernen Webanwendungen verwendet wird.
Schritt 1 - Installieren von MongoDB
Installieren Sie nun die MongoDB selbst:
Drücken Sie dazu Y und dann ENTER.
Schritt 3 - Verwalten des MongoDB-Dienstes
Um den Server zu starten, wenn er angehalten wurde, geben Sie Folgendes ein:
Schritt 4 - Anpassen der Firewall (optional)
Wenn Sie MongoDB nur lokal mit Anwendungen verwenden möchten, die auf dem gleichen Server ausgeführt werden, ist dies die empfohlene und sichere Einstellung.
In den meisten Fällen sollte nur von bestimmten vertrauenswürdigen Orten auf MongoDB zugegriffen werden, wie z. B. von einem anderen Server, der eine Anwendung hostet.
Sie können die Änderung der Firewall-Einstellungen mit ufw überprüfen:
Sie sollten in der Ausgabe den Datenverkehr zu Port 27017 erlauben.
Beachten Sie, dass, wenn Sie beschlossen haben, nur eine bestimmte IP-Adresse für die Verbindung zum MongoDB-Server zuzulassen, die IP-Adresse des zulässigen Speicherorts anstelle von Anywhere in der Ausgabe dieses Befehls aufgeführt wird:
Weitere erweiterte Firewall-Einstellungen zum Einschränken des Zugriffs auf Dienste finden Sie in UFW-Grundlagen: gebräuchliche Firewall-Regeln und -Befehle.
Öffnen Sie die MongoDB-Konfigurationsdatei in Ihrem bevorzugten Editor.
Dieser Beispielbefehl verwendet nano:
Stellen Sie sicher, dass Sie ein Komma zwischen der vorhandenen IP-Adresse und der hinzugefügten Adresse einrichten:
Installation und Nutzung von PostgreSQL auf Ubuntu 18.04
2628
Es ist eine beliebte Wahl für viele kleine und große Projekte und hat den Vorteil, dass sie standardkonform ist und zahlreiche fortgeschrittene Funktionen, wie zuverlässige Transaktionen und Parallelität, ohne Leseschutz aufweist.
Dieser Leitfaden beschreibt die Installation von Postgres auf einer Ubuntu 18.04 VPS-Instanz und enthält auch Anweisungen für die grundlegende Datenbankverwaltung.
Um diesem Tutorial zu folgen, benötigen Sie einen Ubuntu 18.04 Server, der im Sinne unseres Leitfadens Ersteinrichtung eines Servers mit Ubuntu 18.04 konfiguriert wurde.
Und weil Sie apt in dieser Sitzung das erste Mal verwenden, sollten Sie Ihren lokalen Paketindex aktualisieren.
Nachdem jetzt die Software installiert ist, können wir die Funktionsweise besprechen und auch wie sie sich möglicherweise von anderen Datenbank-Managementsystemen unterscheidet, die Sie verwendet haben.
Postgres verwendet zum Umgang mit Authentifizierung und Autorisierung standardmäßig ein Konzept namens "Rollen".
Sie können jetzt sofort auf eine Postgres-Eingabeaufforderung zugreifen, indem Sie Folgendes eingeben:
Wenn der von Ihnen im letzten Abschnitt erstellte Benutzer Sammy heißt, wird diese Rolle demnach versuchen, standardmäßig eine Verbindung mit einer Datenbank namens "Sammy" herzustellen.
Erstellen Sie zunächst eine Tabelle zum Speichern von Daten, wie zum Beispiel eine Tabelle, die Spielplatzgeräte beschreibt.
Die grundlegende Syntax für diesen Befehl lautet wie folgt:
Erstellen Sie zur Veranschaulichung eine einfache Tabelle wie folgt:
Mit diesen Befehlen erstellen Sie eine Tabelle, mit der Spielplatzgeräte erfasst werden.
Das beginnt mit einer Geräte-ID vom Typ Serie.
Der Datentyp ist eine automatisch steigende Ganzzahl.
Sie haben dieser Spalte auch die Primärschlüssel-Einschränkung gegeben, was bedeutet, dass die Werte einzigartig sein müssen und nicht Null sein dürfen.
Für zwei Spalten (equip _ id und install _ date) geben die Befehle keine Feldlänge vor.
Das liegt daran, dass für einige Spalten keine bestimmte Länge vorgegeben wird, weil die Länge durch den Typ bestimmt wird.
Die nächsten beiden Befehle erstellen Spalten für den Typ bzw. die Farbe des Geräts, die beide nicht leer sein können.
Der nächste Befehl erstellt eine location-Spalte für den Standort und eine Einschränkung, die vorschreibt, dass der Wert einer von acht möglichen Werten anzeigt.
Der letzte Befehl erstellt eine Datumsspalte, in der das Datum aufgezeichnet wird, an dem Sie die Geräte installiert haben.
Jetzt können Sie Daten in die Tabelle einfügen.
Sie können eine Rutschbahn und eine Schaukel hinzufügen, indem Sie die Tabelle aufrufen, der Sie diese Geräte hinzufügen möchten, die Spalten bezeichnen und dann Daten in jede Spalte eingeben, wie zum Beispiel:
Dieser Wert wird automatisch beim Erstellen einer neuen Zeile in der Tabelle erstellt.
Sie werden dann sehen, dass Ihre Rutschbahn nicht mehr länger Teil der Tabelle ist.
Nach Erstellen der Tabelle können Sie diese relativ einfach ändern, um Spalten hinzuzufügen oder zu entfernen.
Wenn sie sich die Tabelleninformationen erneut ansehen, werden Sie sehen, dass eine neue Spalte hinzugefügt wurde (aber keine Daten eingegeben wurden).
Das Löschen einer Spalte ist ebenso einfach.
Sie können eine Abfrage nach dem "Schaukel" -Datensatz vornehmen (damit erhalten Sie alle Einträge mit dem Wort Schaukel in der Tabelle) und Sie können die Farbe auf "rot" ändern.
Das könnte nützlich sein, wenn das Schaukelgerüst zum Beispiel neu lackiert werden muss:
Wie Sie sehen, wird die Farbe der Schaukel jetzt als "rot" angegeben.
Sie haben jetzt PostgreSQL auf Ihrem Server Ubuntu 18.04 eingerichtet.
Sie können allerdings noch mehr über Postgres lernen.
Hier finden Sie einige Anleitungen zur Nutzung von Postgres:
Lernen Sie das Erstellen und die Verwaltung von Tabellen mit Postgres
Lernen Sie besseres Management der Rollen und Berechtigungen
Erstellen Sie Abfragen mit Postgres mit "Select"
Lernen Sie, wie Sie PostgreSQL sichern können
Lernen Sie, wie Sie eine Postgres-Datenbank sichern können
Installation von Git auf Ubuntu 18.04 Quickstart
2712
Versionskontrollsysteme unterstützen Sie bei der Freigabe von und Zusammenarbeit an Softwareentwicklungsprojekten.
Git ist eines der beliebtesten Versionskontrollsysteme, die derzeit erhältlich sind.
Diese Anleitung erläutert die Installation und Konfiguration von Git auf einem Ubuntu 18.04 Server.
Eine ausführlichere Version dieses Tutorials mit besseren Erklärungen zu den einzelnen Schritten finden Sie unter Installation von Git auf Ubuntu 18.04.
Schritt 1 - Aktualisierung der Standardpakete
Melden Sie sich als sudo-Benutzer ohne Rootberechtigung bei Ihrem Ubuntu 18.04 Server an und aktualisieren Sie zunächst Ihre Standardpakete.
Schritt 2 - Installation von Git
Schritt 3 - Bestätigung der erfolgreichen Installation
Sie können prüfen, ob Sie Git korrekt installiert haben, indem Sie diesen Befehl ausführen und eine Ausgabe erhalten, die der folgenden ähnelt:
Schritt 4 - Einrichten von Git
Nach der Git-Installation sollten Sie zur Vermeidung von Warnungen es auf Ihre Informationen konfigurieren.
Falls Sie diese Datei bearbeiten müssen, verwenden Sie einen Texteditor wie etwa nano:
Installation von Git auf 18.04
Effektive Nutzung von Git
Nutzung von Git-Verzweigungen
Eine Einführung in Open Source
Installation von Node.js auf Ubuntu 18.04
2605
Node.js ist eine JavaScript-Plattform für die allgemeine Programmierung, die es Benutzern ermöglicht, schnell Netzwerkanwendungen zu erstellen.
Durch die Nutzung von JavaScript am Front- und Backend ermöglicht Node.js eine gleichbleibende, integrierte Entwicklung.
In diesem Leitfaden zeigen wir Ihnen, wie Sie mit Node.js auf einem Ubuntu 18.04 Server starten können.
Dieser Leitfaden geht davon aus, dass Sie Ubuntu 18.04 verwenden.
Dazu können Sie sich den Leitfaden zur Ersteinrichtung für Ubuntu 18.04 ansehen.
Installation der Distro-Stable Version für Ubuntu
Ubuntu 18.04 enthält eine Version von Node.js in seinen Standard-Repositorys, die verwendet werden kann, um ein gleichbleibendes, systemübergreifendes Erlebnis zu erhalten.
Zum Veröffentlichungszeitpunkt war die Version 8.10.0 in den Repositorys gültig.
Installieren Sie Node.js aus den Repositorys:
Aufgrund eines Konflikts mit einem anderen Paket wird das ausführbare Programm aus den Ubuntu Repositorys anstelle von node nodejs genannt.
Denken Sie daran, wenn Sie die Software ausführen.
Um zu prüfen, welche Version von Node.js Sie nach diesen ersten Schritten installiert haben, geben Sie Folgendes ein:
Sobald Sie wissen, welche Version von Node.js Sie aus den Ubuntu Repositorys installiert haben, können Sie entscheiden, ob Sie mit verschiedenen Versionen, Paketarchiven oder Versionsmanagern arbeiten möchten.
Als Nächstes werden wir diese Elemente sowie flexible und robuste Installationsmethoden besprechen.
Installation unter Verwendung eines PPA
Um eine neuere Version von Node.js zu erhalten, können Sie das PPA (persönliches Paketarchiv) hinzufügen, das von NodeSource gepflegt wird.
Diese enthalten aktuellere Versionen von Node.js als die offiziellen Ubuntu Repositorys, und Sie haben die Wahl zwischen Node.js v6.x (Support bis April 2019), Node.js v8.x (die aktuelle LTS-Version, Support bis Dezember 2019), Node.js v10.x (die zweite aktuelle LTS-Version, Support bis April 2021) und Node.js v11.x (die aktuelle Version, Support bis Juni 2019).
Installieren Sie zunächst das PPA, um Zugriff auf dessen Inhalt zu erhalten.
Verwenden Sie curl aus dem Stammverzeichnis, um das Installationsskript für Ihre bevorzugte Version abzurufen, und achten Sie darauf, < ^ > 10.x < ^ > durch Ihre bevorzugte Versionszeichenfolge zu ersetzen (sofern sich diese unterscheiden):
Sie können den Inhalt dieses Skripts mit nano (oder Ihrem bevorzugten Texteditor) prüfen:
Führen Sie das Skript unter sudo aus:
Nach der Durchführung des Setup-Skripts aus Nodesource können Sie das Node.js Paket auf die gleiche Art installieren, wie Sie es oben getan haben:
Das nodejs Paket enthält das binäre nodejs sowie npm, daher müssen Sie npm nicht separat installieren.
npm verwendet eine Konfigurationsdatei in Ihrem Home Verzeichnis, um Aktualisierungen zu verfolgen.
Sie wird erstellt, wenn Sie npm das erste Mal ausführen.
Führen Sie diesen Befehl aus, um zu verifizieren, dass npm installiert ist, und um die Konfigurationsdatei zu erstellen:
Damit bestimmte npm Pakete funktionieren (z. B. jene, deren Code aus Source erstellt werden muss), müssen Sie das build-essential Paket installieren:
Jetzt haben Sie die notwendigen Tools, um mit npm Paketen zu arbeiten, deren Code aus Source erstellt werden muss.
Installation mit NVM
Eine Alternative zur Installation von Node.js mit apt ist ein Tool namens nvm, das für "Node.js Version Manager" steht.
Nvm läuft auf der Ebene eines unabhängigen Verzeichnisses in Ihrem Stammverzeichnis anstatt auf Betriebssystemebene.
Das bedeutet, dass Sie ohne Auswirkungen auf das gesamte System mehrere in sich geschlossene Versionen von Node.js installieren können.
Durch die Kontrolle Ihrer Umgebung mit nvm können Sie auf die neuesten Versionen von Node.js zugreifen und frühere Versionen beibehalten und verwalten.
Es ist jedoch ein anderes Utility als apt und die Versionen von Node.js, die Sie damit verwalten, unterscheiden sich von den Versionen, die Sie mit apt verwalten.
Um das nvm Installationsskript von der GitHub-Seite des Projekts herunterzuladen, können Sie curl verwenden.
Beachten Sie, dass sich die Versionsnummer von der hier hervorgehobenen eventuell unterscheidet:
Untersuchen Sie das Installationsskript mit nano:
Führen Sie das Skript mit bash aus:
Damit wird die Software in ein Unterverzeichnis Ihres Stammverzeichnisses unter ~ / .nvm installiert.
Es werden auch die zur Verwendung erforderlichen Zeilen in Ihre Datei ~ / .profile eingefügt.
Um auf die nvm Funktionalität Zugriff zu erhalten, müssen Sie sich entweder ab- und wieder anmelden oder sich auf die Datei ~ / .profile beziehen, damit Ihre aktuelle Sitzung über die Änderungen informiert wird:
Wenn nvm installiert ist, können Sie isolierte Node.js Versionen installieren.
Für Informationen zu den verfügbaren Versionen von Node.js geben Sie Folgendes ein:
Wie Sie sehen können, ist die aktuelle LTS-Version zum Zeitpunkt der Abfassung v8.11.1.
Sie können die Installation vornehmen, indem Sie Folgendes eingeben:
Nvm wechselt normalerweise auf die neueste installierte Version.
Sie können nvm mitteilen, die Version zu verwenden, die sie gerade heruntergeladen haben, indem Sie Folgendes eingeben:
Wenn Sie Node.js mit nvm installieren, wird die ausführbare Datei node genannt.
Sie können die von der Shell derzeit verwendete Version sehen, indem Sie Folgendes eingeben:
Wenn Sie mehrere Node.js Versionen haben, können Sie prüfen, welche installiert ist, indem Sie Folgendes eingeben:
Wenn Sie eine der Versionen als Standard festlegen möchten, geben Sie Folgendes ein:
Diese Version wird automatisch für eine neue Sitzung ausgewählt.
Sie können sich auch, wie folgt, über den Alias darauf verweisen:
Jede Version von Node.js verfolgt ihre eigenen Pakete und kann diese über npm verwalten.
Sie können auch npm anweisen, Pakete im Verzeichnis. / node _ modules des Node.js Projekts zu installieren.
Verwenden Sie die folgende Syntax, um das express-Modul zu installieren:
Wenn Sie das Modul global installieren und es anderen Projekten mit der gleichen Version von Node.js bereitstellen möchten, können Sie die Flag -g hinzufügen.
Damit wird das Paket installiert in:
Wenn Sie das Modul global installieren, können Sie Befehle aus der Befehlszeile ausführen, aber Sie müssen das Paket mit Ihrem lokalen Umfeld verbinden, damit ein Programm es aufrufen kann:
Erfahren Sie mehr über die mit nvm zur Verfügung stehenden Optionen, indem Sie Folgendes eingeben:
Node.js entfernen
Je nach anvisierter Version können Sie Node.js mit apt oder nvm deinstallieren.
Um die distro-stabile Version zu entfernen, müssen Sie mit dem apt-Utility auf Systemebene arbeiten.
Um die distro-stabile Version zu entfernen, geben Sie Folgendes ein:
Mit diesem Befehl entfernen Sie das Paket und behalten die Konfigurationsdateien bei.
Das kann nützlich sein, wenn Sie das Paket später erneut installieren wollen.
Wenn Sie die Konfigurationsdateien nicht zur späteren Verwendung speichern möchten, führen Sie Folgendes aus:
Damit wird das Paket deinstalliert und die damit verbundenen Konfigurationsdateien werden entfernt.
Als letzten Schritt können Sie alle ungenutzten Pakete entfernen, die automatisch mit dem entfernten Paket installiert wurden:
Um eine Version von Node.js, die Sie mit nvm aktiviert haben, zu deinstallieren, bestimmen Sie zunächst, ob die Version, die Sie entfernen möchten, die aktuelle, aktive Version ist:
Wenn die von Ihnen gewünschte Version nicht die aktuelle, aktive Version ist, können Sie Folgendes ausführen:
Mit diesem Befehl wird die ausgewählte Version von Node.js deinstalliert.
Wenn die Version, die Sie entfernen möchten, die aktuelle, aktive Version ist, müssen Sie nvm zunächst deaktivieren, um Ihre Änderungen zu aktivieren:
Sie können jetzt die aktuelle Version mit dem uninstall Befehl wie oben deinstallieren, wodurch alle Dateien, die mit der gewünschten Version von Node.js assoziiert sind, entfernt werden, mit Ausnahme der Cache-Dateien, die zur Neuinstallierung verwendet werden können.
Sie haben verschiedene Möglichkeiten, mit Node.js die Arbeit auf Ihrem Ubuntu 18.04 Server aufzunehmen.
Obwohl die Paketversion in Ubuntus Repositorys die einfachste Methode ist, bietet nvm zusätzliche Flexibilität.
So containerisieren Sie eine Ruby-on-Rails-Anwendung zur Entwicklung mit Docker Compose
3332
Wenn Sie eine Anwendung aktiv entwickeln, kann Docker den Workflow und den Prozess der Bereitstellung der Anwendung in der Produktion vereinfachen.
Die Arbeit mit Containern im Entwicklungsbereich bietet folgende Vorteile:
Die Umgebungen sind konsistent, was bedeutet, dass Sie die Sprachen und Abhängigkeiten für Ihr Projekt wählen können, ohne dass es zu Systemkonflikten kommt.
Die Umgebungen werden isoliert, wodurch Probleme leichter behoben und die neuen Teammitglieder schneller eingearbeitet werden können.
Die Umgebungen sind tragbar, sodass Sie den Code verpacken und mit anderen teilen können.
Dieses Tutorial zeigt Ihnen, wie Sie eine Entwicklungsumgebung für eine Ruby on Rails-Anwendung mit Docker einrichten.
Sie erstellen mehrere Container - für die Anwendung selbst, die PostgreSQL Datenbank, Redis und einen Sidekiq Dienst - mit Docker Compose.
Das Setup bewirkt Folgendes:
Synchronisieren des Anwendungscodes im Host mit dem Code im Container, um Änderungen während der Entwicklung zu ermöglichen.
Beibehaltung der Anwendungsdaten zwischen Container-Neustarts.
Konfiguration von Sidekiq-Workern, um die Vorgänge wie erwartet zu verarbeiten.
Am Ende dieses Tutorials haben Sie eine funktionierende Anwendung mit Informationen zu Haien, die auf Docker Containern ausgeführt wird:
Sidekiq App Home
Einen lokalen Entwicklungsrechner oder einen Server, auf dem Ubuntu 18.04 ausgeführt wird, zusammen mit einem Benutzer ohne Rootberechtigung mit sudo-Privilegien und einer aktiven Firewall.
Docker muss auf Ihrem lokalen Rechner oder Server installiert sein. Folgen Sie dazu Schritt 1 und 2 in So installieren und verwenden Sie Docker unter Ubuntu 18.04.
Docker Compose muss auf Ihrem lokalen Rechner oder Server installiert sein. Folgen Sie dazu Schritt 1 in So installieren Sie Docker Compose unter Ubuntu 18.04.
Schritt 1 - Klonen des Projekts und Hinzufügen von Abhängigkeiten
Unser erster Schritt besteht darin, das Repository rails-sidekiq aus dem DigitalOcean Community GitHub-Konto zu klonen.
Dieses Repository enthält den Code aus dem Setup, das in So fügen Sie Sidekiq und Redis zu einer Ruby-on-Rails-Anwendung hinzu beschrieben wird. Dort wird erklärt, wie Sie einem bestehenden Rails 5-Projekt Sidekiq hinzufügen.
Klonen Sie das Repository in ein Verzeichnis namens < ^ > rails-docker < ^ >:
Navigieren Sie zum Verzeichnis < ^ > rails-docker < ^ >:
In diesem Tutorial verwenden wir PostgreSQL als Datenbank.
Um mit PostgreSQL anstelle von SQLite 3 zu arbeiten, müssen Sie das pg gem zu den Abhängigkeiten des Projekts hinzufügen, die in seiner Gemfile aufgelistet sind.
Öffnen Sie diese Datei, um sie in nano oder Ihrem bevorzugten Texteditor zu bearbeiten:
Fügen Sie das Gem an beliebiger Stelle in die Abhängigkeiten des Hauptprojekts ein (oberhalb der Entwicklungsabhängigkeiten):
Auch können wir den sqlite Gem auskommentieren, da wir ihn nicht mehr brauchen:
Kommentieren Sie schließlich das spring-watcher-listen Gem unter development aus:
Wird dieses Gem nicht deaktiviert, erhalten wir beim Zugriff auf die Rails-Konsole ständig Fehlermeldungen.
Diese Fehlermeldungen ergeben sich aus der Tatsache, dass dieses Gem Rails veranlasst, mit listen auf Änderungen in "development" zu achten, statt das Dateisystem nach Änderungen abzufragen.
Da dieses Gem die Root des Projekts überwacht, einschließlich des Verzeichnisses node _ modules, wirft es Fehlermeldungen über beobachtete Verzeichnisse aus, die die Konsole überladen.
Wenn Sie allerdings Bedenken bezüglich der Einsparung von CPU-Ressourcen haben, ist die Deaktivierung dieses Gem für Sie wahrscheinlich ungeeignet.
In dem Fall kann es sinnvoll sein, Ihre Rails-Anwendung auf Rails 6 zu aktualisieren.
Wenn die Bearbeitung abgeschlossen wurde, speichern und schließen Sie die Datei.
Sobald Ihr Projekt-Repository vorhanden ist, das Gem pg der Gemfile hinzugefügt und das Gem spring-watcher-listen auskommentiert wurde, können Sie Ihre Anwendung so konfigurieren, dass sie mit PostgreSQL funktioniert.
Schritt 2 - Konfiguration der Anwendung zur Verwendung mit PostgreSQL und Redis
Um mit PostgreSQL und Redis in der Entwicklung zu arbeiten, gehen Sie wie folgt vor:
Konfigurieren Sie die Anwendung so, dass sie mit PostgreSQL als Standardadapter arbeitet.
Fügen sie dem Projekt eine .env-Datei mit unserem Datenbankbenutzernamen und Passwort und Redis-Host hinzu.
Erstellen Sie ein init.sql-Skript, um den Benutzer sammy für die Datenbank zu erstellen.
Fügen Sie einen initializer für Sidekiq hinzu, damit er mit unserem containerisierten redis-Dienst zusammenarbeiten kann.
Fügen Sie die .env-Datei und andere relevante Dateien den gitignore- und dockerignore-Dateien des Projekts hinzu.
Erstellen Sie Datenbank-Einträge (Seeds), damit unsere Anwendung einige Datensätze zur Bearbeitung hat, wenn wir sie in Betrieb nehmen.
Öffnen Sie zunächst Ihre Datenbank-Konfigurationsdatei, die sich in config / database.yml befindet:
Derzeit enthält die Datei die folgenden Standardeinstellungen, die anstelle anderer, noch nicht festgelegter Einstellungen verwendet werden:
Diese müssen geändert werden, um anzuzeigen, dass wir den postgresql-Adapter verwenden, da wir einen PostgreSQL-Dienst mit Docker Compose einrichten, um die Anwendungsdaten beizubehalten.
Löschen Sie den Code, der SQLite als Adapter festlegt, und ersetzen Sie ihn durch die folgenden Einstellungen, mit denen der Adapter und die anderen, zur Verbindung benötigten Variablen passend festgelegt werden:
Als Nächstes ändern wir die Einstellung für die Entwicklungsumgebung, da dies die Umgebung ist, die wir bei diesem Setup verwenden.
Löschen Sie die bestehende SQLite-Datenbankkonfiguration, damit der Abschnitt so aussieht:
Löschen Sie schließlich auch die Datenbankeinstellungen für die Produktions- und Testumgebungen:
Durch diese Änderungen an den standardmäßigen Datenbankeinstellungen können wir unsere Datenbankinformationen dynamisch anhand von Umgebungsvariablen festlegen, die in .env-Dateien definiert sind, die nicht der Versionskontrolle unterstehen.
Beachten Sie, dass Sie bei Neuerstellung eines Rails-Projekts ohne Vorlage den Adapter mit dem Befehl rails new festlegen können, wie in Schritt 3 unter So verwenden Sie PostgreSQL mit der Ruby-on-Rails-Anwendung unter Ubuntu 18.04 beschrieben.
Damit wird Ihr Adapter in config / database.yml festgelegt und das Gem pg dem Projekt automatisch hinzugefügt.
Da wir nun auf unsere Umgebungsvariablen verwiesen haben, können wir mit den bevorzugten Einstellungen eine Datei für sie einrichten.
Die Konfigurationseinstellungen auf diese Weise zu extrahieren ist Teil des 12-Faktoren-Ansatzes der Anwendungsentwicklung, mit dem bewährte Praktiken für die Anwendungsbeständigkeit in verteilten Umgebungen definiert werden.
Wenn wir dann in Zukunft Produktiv- und Testumgebungen einrichten, beinhaltet die Konfiguration unserer Datenbankeinstellungen die Erstellung zusätzlicher .env-Dateien und den Verweis auf die entsprechende Datei in den Docker Compose-Dateien.
Öffnen Sie eine .env-Datei:
Fügen Sie der Datei folgende Werte hinzu:
Neben der Einstellung des Datenbanknamens, des Benutzers und des Passworts haben wir auch für den DATABASE _ HOST einen Wert festgelegt.
Der Wert database verweist auf den PostgreSQL-Datenbank-Dienst, den wir mit Docker Compose erstellen.
Auch haben wir einen REDIS _ HOST festgelegt, um unseren redis-Dienst anzugeben.
Um den Datenbankbenutzer sammy zu erstellen, können wir ein init.sql-Skript schreiben, das wir dann beim Start an den Datenbank-Container anhängen können.
Öffnen Sie die Skript-Datei:
Fügen Sie folgenden Code hinzu, um einen Benutzer sammy mit administrativen Zugriffsberechtigungen einzurichten:
Dieses Skript erstellt den entsprechenden Benutzer in der Datenbank und erteilt ihm administrative Berechtigungen.
Legen Sie entsprechende Berechtigungen im Skript fest:
Als Nächstes konfigurieren wir Sidekiq so, dass er mit unserem containerisierten redis-Dienst arbeitet.
Wir können dem Verzeichnis config / initializers einen Initialisator hinzufügen. In diesem Verzeichnis sucht Rails nach Konfigurationseinstellungen, sobald Frameworks und Plugins geladen sind, womit ein Wert für einen Redis-Host festgelegt wird.
Öffnen Sie die Datei sidekiq.rb, um diese Einstellungen vorzunehmen:
Fügen Sie der Datei folgenden Code hinzu, um Werte für REDIS _ HOST und REDIS _ PORT anzugeben:
Ähnlich wie die Datenbank-Konfigurationseinstellungen geben diese Einstellungen uns die Möglichkeit, unsere Host- und Portparameter dynamisch einzustellen, damit wir die entsprechenden Werte zur Laufzeit ersetzen können, ohne den Anwendungscode selbst zu ändern.
Neben REDIS _ HOST gibt es einen Standardwert für REDIS _ PORT, falls er nicht anderweitig festgelegt wurde.
Um zu gewährleisten, dass die sensiblen Anwendungsdaten nicht in die Versionskontrolle kopiert werden, fügen wir der Datei .gitignore unseres Projekts .env hinzu, was Git mitteilt, welche Dateien im Projekt ignoriert werden können.
Öffnen Sie die Datei zur Bearbeitung:
Fügen Sie unten in der Datei einen Eintrag für .env hinzu:
Als Nächstes erstellen wir eine .dockerignore-Datei, um festzulegen, was nicht in die Container kopiert werden soll.
Fügen Sie der Datei folgenden Code hinzu, der Docker anweist, bestimmte Dinge zu ignorieren, die nicht in die Container kopiert werden brauchen:
Fügen Sie auch in dieser Datei unten .env hinzu:
Der letzte Schritt ist die Erstellung von Seed-Daten, damit die Anwendung ein paar Einträge hat, wenn wir sie starten.
Öffnen Sie eine Datei für die Seed-Daten im Verzeichnis db:
Fügen Sie der Datei folgenden Code hinzu, um vier Demo-Haie und ein Beispiel-Posting zu erstellen:
Diese Seed-Daten erstellen vier Haie und ein Posting, das mit dem ersten Hai verbunden ist.
Da Sie Ihre Anwendung so konfiguriert haben, dass sie mit PostgreSQL und Ihren Umgebungsvariablen funktioniert, können Sie nun die Dockerfile-Anwendung schreiben.
Schritt 3 - Dockerfile und Entrypoint-Skripts schreiben
Ihr Dockerfile gibt an, was im Anwendungs-Container nach der Erstellung enthalten sein soll.
Durch den Einsatz eines Dockerfiles können Sie Ihre Container-Umgebung definieren und Diskrepanzen mit Abhängigkeiten oder Laufzeitversionen vermeiden.
Anhand dieser Richtlinien zum Aufbau von optimierten Containern werden wir unser Image so effizient wie möglich gestalten, indem wir eine Alpine-Basis verwenden und versuchen, die Image-Ebenen allgemein zu minimieren.
Öffnen Sie ein Dockerfile in Ihrem aktuellen Verzeichnis:
Die Docker-Images werden aus einer Folge von Image-Ebenen erstellt, die aufeinander aufbauen.
Unser erster Schritt besteht darin, das Basisimage für die Anwendung hinzuzufügen, das den Ausgangspunkt der Anwendung bildet.
Fügen Sie der Datei folgenden Code hinzu, um das Ruby alpine-Image als Basis aufzunehmen:
Das alpine-Image stammt aus dem Alpine Linux-Projekt und unterstützt eine kleinere Image-Größe.
Weitere Informationen, ob das alpine-Image die richtige Wahl für Ihr Projekt ist, finden Sie in der Diskussion im Abschnitt Image-Varianten auf der Docker Hub Ruby-Image-Seite.
Bedenken Sie bei Gebrauch von alpine in der Entwicklung Folgendes:
Das Minimieren der Image-Größe verringert die Ladezeit von Seiten und Ressourcen, insbesondere wenn Sie auch die Volumes auf ein Mindestmaß beschränken.
Damit wird während der Entwicklung das Benutzererlebnis zügig und annähernd so gehalten, wie es wäre, wenn Sie lokal in einer nicht-containerisierten Umgebung arbeiten.
Parität zwischen Entwicklungs- und Produktions-Images erleichtert eine erfolgreiche Bereitstellung.
Da Teams sich in der Produktion häufig aus Geschwindigkeitsgründen für Alpine-Images entscheiden, trägt die Entwicklung unter Einsatz einer Alpine-Basis dazu bei, Probleme beim Wechsel in die Produktion zu verringern.
Geben Sie als Nächstes eine Umgebungsvariable ein, um die Bundler-Version anzugeben:
Das ist einer der Schritte, die wir durchführen, um Versionskonflikte zwischen der standardmäßigen bundler-Version, die in unserer Umgebung verfügbar ist, und dem Anwendungscode zu vermeiden, der Bundler 2.0.2 erfordert.
Fügen Sie als Nächstes dem Dockerfile die Pakete hinzu, die mit der Anwendung zusammenarbeiten sollen:
Diese Pakete umfassen u. a. nodejs und yarn.
Da die Anwendung Ressourcen mit webpack bedient, müssen wir Node.js und Yarn einbeziehen, damit die Anwendung wie erwartet läuft.
Denken Sie daran, dass das alpine-Image äußerst minimal ist: die Liste der hier aufgeführten Pakete enthält nicht alles, was Sie bei der Entwicklung eventuell brauchen, wenn Sie die eigene Anwendung containerisieren.
Installieren Sie als Nächstes die entsprechende bundler-Version:
Durch diesen Schritt wird die Parität zwischen der containerisierten Umgebung und den Spezifikationen der Gemfile.lock-Datei dieses Projekts gewährleistet.
Richten Sie jetzt das Arbeitsverzeichnis für die Anwendung im Container ein:
Kopieren Sie Gemfile und Gemfile.lock:
Der separate Schritt des Kopierens dieser Dateien, gefolgt vom bundle install, bewirkt, dass die Projekt-Gems nicht nach jeder Änderung des Anwendungscodes neu gebaut werden müssen.
Dies funktioniert in Verbindung mit dem gem-Volume, das wir in unsere Compose-Datei aufnehmen werden. Dadurch werden Gems in den Fällen in Ihren Anwendungs-Container eingebunden, in denen der Dienst neu erstellt wurde, aber die Projekt-Gems gleich bleiben.
Legen Sie als Nächstes die Konfigurationsoptionen für den Gem-Build nokogiri fest:
Dieser Schritt baut nokigiri mit den Bibliotheksversionen libxml2 und libxslt, die wir im oben stehenden Schritt RUN apk add... dem Anwendungs-Container hinzugefügt haben.
Installieren Sie als Nächstes die Projekt-Gems:
Diese Anweisung überprüft vor der Installation, ob die Gems bereits installiert sind.
Als Nächstes wiederholen wir das gleiche Verfahren, das wir mit Gems mit den JavaScript-Paketen und Abhängigkeiten verwendet haben.
Zuerst kopieren wir die Paket-Metadaten, dann installieren wir Abhängigkeiten und kopieren schließlich den Anwendungscode in das Container-Image.
Starten Sie mit dem Javascript-Abschnitt unseres Dockerfiles, indem Sie package.json und yarn.lock aus dem aktuellen Projektverzeichnis auf dem Host in den Container kopieren:
Installieren Sie dann die erforderlichen Pakete mit yarn install:
Diese Anweisung enthält das Flag --check-files mit dem Befehl yarn, eine Funktion, die sicherstellt, dass zuvor installierte Dateien nicht entfernt wurden.
Wie bei den Gems verwalten wir die Persistenz der Pakete im Verzeichnis node _ modules mit dem Volume, wenn wir die Compose-Datei schreiben.
Kopieren Sie schließlich den Rest des Anwendungscodes und starten Sie die Anwendung mit einem Entrypoint-Skript (Einstiegspunkt-Skript):
Mit dem Entrypoint-Skript können wir den Container als exe-Datei ausführen.
Das endgültige Dockerfile sieht ungefähr so aus:
Erstellen Sie als Nächstes ein Verzeichnis entrypoints für die Entrypoint-Skripts:
Dieses Verzeichnis enthält unser Haupt-entrypoint-Skript und ein Skript für den Sidekiq-Dienst.
Öffnen Sie die Datei für das Entrypoint-Skript der Anwendung:
Die erste wichtige Zeile ist set -e, die das skriptausführende Shell / bin / sh anweist, schnell ein "fail" auszugeben, wenn weiter unten im Skript Probleme auftauchen.
Als Nächstes prüft das Skript, dass kein tmp / pids / server.pid vorhanden ist, um sicherzugehen, dass es beim Start der Anwendung keine Serverkonflikte geben wird.
Schließlich startet das Skript den Rails-Server mit dem Befehl bundle exec rails s.
Mit diesem Befehl verwenden wir die Option -b, um den Server an alle IP-Adressen anstatt an den Standard localhost zu binden.
Diese Invokation bewirkt, dass der Rails-Server eingehende Anfragen an die Container-IP anstatt an den standardmäßigen localhost leitet.
Machen Sie das Skript ausführbar:
Als Nächstes erstellen wir ein Skript, das unseren sidekiq-Dienst startet, der die Sidekiq-Aufträge verarbeitet.
Weitere Informationen darüber, wie die Anwendung Sidekiq einsetzt, finden Sie unter So fügen Sie Sidekiq und Redis einer Ruby-on-Rails-Anwendung hinzu.
Öffnen Sie eine Datei für das Sidekiq-Entrypoint-Skript:
Fügen Sie der Datei folgenden Code hinzu, um Sidekiq zu starten:
Dieses Skript startet Sidekiq im Kontext unseres Anwendungspakets.
Machen Sie sie ausführbar:
Mit den vorhandenen Entrypoint-Skripts und dem Dockerfile können Sie jetzt Ihre Dienste in der Compose-Datei definieren.
Schritt 4 - Dienste mit Docker Compose definieren
Mit Docker Compose können wir die verschiedenen, für das Setup erforderlichen Container ausführen.
Wir definieren unsere Compose-Dienste in der Hauptdatei docker-compose.yml.
Ein Dienst in Compose ist ein laufender Container und die Dienstdefinitionen, die Sie der Datei docker-compose.yml hinzufügen, enthalten Informationen darüber, wie das jeweilige Container-Image ausgeführt wird.
Das Compose-Tool ermöglicht es, mehrere Dienste für den Aufbau von Multi-Container-Anwendungen zu definieren.
Unser Anwendungs-Setup enthält folgende Dienste:
Die Anwendung selbst
Die PostgreSQL-Datenbank
Redis
Sidekiq
Wir nehmen auch eine bind-Bereitstellung in unser Setup auf, damit alle Codeänderungen, die wir bei der Entwicklung vornehmen, sofort mit den Containern synchronisiert werden, die Zugriff auf diesen Code benötigen.
Beachten Sie, dass wir keinen test-Dienst definieren, da die Tests außerhalb des Bereichs dieses Tutorials und der Serie liegen, aber Sie können dies mithilfe des Präzedenzfalls tun, den wir hier für den sidekiq-Dienst verwenden.
Öffnen Sie die Datei docker-compose.yml:
Fügen Sie zunächst die Definition des Anwendungsdienstes hinzu:
Die app-Dienstdefinition enthält die folgenden Optionen:
build: Dies definiert die Konfigurationsoptionen, einschließlich Kontext und dockerfile, die verwendet werden, wenn Compose das Anwendungsimage baut.
Wenn Sie ein bestehendes Image aus einer Registry wie Docker Hub verwenden möchten, können Sie stattdessen die Anweisung image verwenden, die Informationen über Ihren Benutzernamen, das Repository und das Image-Tag enthält.
context: Dieser definiert den build-Kontext für den Imageaufbau - in diesem Fall das aktuelle Projektverzeichnis.
dockerfile: Dies gibt das Dockerfile aus dem aktuellen Projektverzeichnis als die von Compose verwendete Datei zum Bau des Anwendungsimages an.
depends _ on: Dies erstellt zunächst die Datenbank- und redis-Container, damit sie vor app ausgeführt werden.
ports: Damit wird Port 3000 auf dem Host dem Port 3000 auf dem Container zugeordnet.
volumes: Wir nehmen hier zwei Arten von Bereitstellungen auf:
Die erste ist eine bind-Bereitstellung, die unseren Anwendungscode auf dem Host in das Verzeichnis / app auf dem Container einbindet.
Dies erleichtert die schnelle Entwicklung, da Änderungen, die Sie am Host-Code vornehmen, sofort im Container gefüllt werden.
Die zweite ist ein benanntes Volume gem _ cache.
Wenn die Anweisung bundle install im Container ausgeführt wird, werden die Projekt-Gems installiert.
Das Hinzufügen dieses Volumes bewirkt, dass, wenn Sie den Container neu erstellen, die Gems in den neuen Container eingebunden werden.
Bei dieser Bereitstellung wird davon ausgegangen, dass es keine Änderungen am Projekt gegeben hat. Wenn Sie also Änderungen an den Projekt-Gems in der Entwicklung vornehmen, müssen Sie daran denken, dieses Volume zu löschen, bevor Sie den Anwendungsdienst neu erstellen.
Das dritte Volume ist ein benanntes Volume für das Verzeichnis node _ modules.
Anstatt node _ modules auf dem Host bereitzustellen, was zu Diskrepanzen und Berechtigungskonflikten in der Entwicklung führen kann, sorgt dieses Volume dafür, dass die Pakete in diesem Verzeichnis persistent gespeichert werden und den aktuellen Zustand des Projekts widerspiegeln.
Auch hier gilt: Wenn Sie die Node-Abhängigkeiten des Projekts ändern, müssen Sie dieses Volume entfernen und neu erstellen.
env _ file: Dies teilt Compose mit, dass wir Umgebungsvariablen aus einer Datei namens .env hinzufügen möchten, die sich im build-Kontext befindet.
environment: Mit dieser Option können wir eine nicht empfindliche Umgebungsvariable einrichten, die Informationen über die Rails-Umgebung an den Container überträgt.
Fügen Sie als Nächstes unterhalb der app-Dienstdefinition den folgenden Code hinzu, um Ihren Datenbank-Dienst festzulegen:
Anders als der app-Dienst ruft der Datenbank-Dienst ein postgres-Image direkt aus dem Docker-Hub ab.
Beachten Sie, dass wir die Version hier auch anheften, anstatt sie auf die aktuellste zu setzen oder nicht anzugeben (wodurch standardmäßig die aktuellste festgelegt wird).
Auf diese Weise können wir sicherstellen, dass dieses Setup mit den hier angegebenen Versionen funktioniert und unerwartete Überraschungen durch fehlschlagende Codeänderungen am Image vermieden werden.
Auch beziehen wir das Volume db _ data ein, das die Anwendungsdaten zwischen Container-Starts persistent speichert.
Zusätzlich haben wir unser Startskript init.sql auf dem entsprechenden Verzeichnis docker-entrypoint-initdb.d / im Container bereitgestellt, um den Datenbankbenutzer sammy zu erstellen.
Nachdem der Image-Entrypoint den standardmäßigen postgres-Benutzer und die Datenbank erstellt, werden alle im Verzeichnis docker-entrypoint-initdb.d / gefundenen Skripts ausgeführt, die Sie für notwendige Initialisierungsaufgaben benötigen.
Weitere Informationen finden Sie im Abschnitt Initialisierungskripts der PostgreSQL Image-Dokumentation.
Fügen Sie als Nächstes die redis-Dienstdefinition hinzu:
Wie der Datenbank-Dienst, verwendet der redis-Dienst ein Image von Docker Hub.
In diesem Fall bleibt der Sidekiq Auftrags-Cache nicht persistent gespeichert.
Fügen Sie schließlich die sidekiq-Dienstdefinition hinzu:
Unser sidekiq-Dienst gleicht unserem app-Dienst in wenigen Punkten: er verwendet denselben Build-Kontext, dasselbe Image, dieselben Umgebungsvariablen und Volumes.
Allerdings ist er von den app-, redis- und den Datenbank-Diensten abhängig und startet somit zuletzt.
Außerdem verwendet er einen entrypoint, der den im Dockerfile festgelegten Entrypoint außer Kraft setzt.
Diese entrypoint-Einstellung verweist auf entrypoints / sidekiq-entrypoint.sh, die den entsprechenden Befehl enthält, um den sidekiq-Dienst zu starten.
Fügen Sie als letzten Schritt die Volume-Definitionen unterhalb der sidekiq-Dienstdefinition hinzu:
Unsere Volumes auf oberster Ebene definieren die Volumes gem _ cache, db _ data und node _ modules.
Wenn Docker Volumes erstellt, werden die Inhalte des Volumes in einem Teil des Host-Dateisystems, nämlich / var / lib / docker / volumes, gespeichert, das von Docker verwaltet wird.
Die Inhalte jedes Volumes werden in einem Verzeichnis unter / var / lib / docker / volumes / gespeichert und in jedem Container bereitgestellt, der das Volume verwendet.
Auf diese Weise werden die Info-Daten zu Haien, die unsere Benutzer erstellen, im Volumen db _ data persistent gespeichert, selbst wenn wir den Datenbank-Dienst entfernen und neu erstellen.
Die fertige Datei sieht ungefähr so aus:
Mit den fertiggestellten Dienstdefinitionen können Sie die Anwendung starten.
Schritt 5 - Testen der Anwendung
Mit der vorhandenen docker-compose.yml-Datei können Sie Ihre Dienste mit dem Befehl docker-compose up erstellen und für die Datenbank ein Seeding durchführen.
Sie können auch testen, ob die Daten persistent gespeichert werden, indem Sie die Container mit docker-compose down entfernen und neu erstellen.
Erstellen Sie zunächst die Container-Images und dann die Dienste, indem Sie docker-compose up mit dem Flag -d ausführen, das die Container im Hintergrund ausführt:
Sie sehen die von Ihren Dienste erstellte Ausgabe:
Sie können auch ausführlichere Informationen über die Startprozesse erhalten, indem Sie die Protokolle der Dienste anzeigen:
Wenn alles richtig gestartet wurde, sieht dies in etwa wie folgt aus:
Sie können auch den Status Ihrer Container mit docker-compose ps überprüfen:
Die Ausgabe zeigt an, dass die Container ausgeführt werden:
Erstellen Sie als Nächstes die Datenbank und führen Sie ein Seeding durch. Führen Sie dann mit dem folgenden docker-compose exec-Befehl darin Migrationen aus:
Der Befehl docker-compose exec ermöglicht es Ihnen, Befehle in Ihren Diensten auszuführen; wir verwenden ihn hier, um rake db: setup und db: migrate im Kontext unseres Anwendungspakets auszuführen, um die Datenbank zu erstellen, ein Seeding und Migrationen durchzuführen.
In der Entwicklung wird sich docker-compose exec für Sie als nützlich erweisen, wenn Sie Migrationen gegen die Entwicklungsdatenbank ausführen möchten.
Nach Durchführung des Befehls sehen Sie die folgende Ausgabe:
Bei laufenden Diensten können Sie localhost: 3000 oder http: / / your _ server _ ip: 3000 im Browser aufrufen.
Die Startseite wird in etwa so aussehen:
Jetzt können Sie die Datenpersistenz testen.
Erstellen Sie einen neuen Hai, indem Sie auf die Schaltfläche Get Shark Info klicken, welche die Route shark / index aufruft:
Hai-Indexseite mit per Seeding hinzugefügten Daten
Um zu verifizieren, dass die Anwendung funktioniert, können wir ihr einige Demo-Informationen hinzufügen. Klicken Sie auf New Shark.
Dank der Authentifizierungseinstellungen des Projekts werden Sie zur Eingabe des Benutzernamens (sammy) und Passworts (shark) aufgefordert.
Geben Sie auf der Seite New Shark unter Name "Mako" und unter Facts "Fast" ein.
Klicken Sie auf die Schaltfläche Create Shark, um den Hai zu erstellen.
Wenn Sie den Hai erstellt haben, klicken Sie in der Navigationsleiste der Site auf Home, um zur Startseite der Anwendung zurückzukehren.
Jetzt können wir testen, ob Sidekiq funktioniert.
Klicken Sie auf die Schaltfläche Which Sharks Are in Danger?
.
Da Sie keine gefährdeten Haie hochgeladen haben, wird Ihnen die Ansicht des gefährdeten Index gezeigt:
Ansicht des gefährdeten Index
Klicken Sie auf Import Endangered Sharks, um die gefährdeten Haie zu importieren.
Eine Statusmeldung teilt Ihnen mit, dass die Haie importiert wurden:
Import beginnen
Sie sehen auch den Beginn des Imports.
Aktualisieren Sie die Seite, um die gesamte Tabelle zu sehen:
Tabelle aktualisieren
Dank Sidekiq ist unser umfangreiches Batch-Upload der gefährdeten Haie gelungen, ohne den Browser zu blockieren oder die Funktionsweise anderer Anwendungen zu beeinträchtigen.
Klicken Sie auf die Schaltfläche Home unten auf der Seite, die Sie zur Hauptseite der Anwendung zurückbringen wird:
Klicken Sie hier erneut auf Which Sharks Are in Danger?
Sie sehen dann erneut die hochgeladenen Haie.
Da wir jetzt wissen, dass die Anwendung richtig funktioniert, können wir die Datenpersistenz testen.
Geben Sie am Terminal den folgenden Befehl ein, um die Container zu stoppen und zu entfernen:
Beachten Sie, dass die Option --volumes darin nicht enthalten ist; daher wird unser db _ data-Volume nicht entfernt.
Die folgende Ausgabe bestätigt, dass die Container und das Netzwerk entfernt wurden:
Erstellen Sie die Container neu:
Öffnen Sie die Rails-Konsole im Container app mit docker-compose exec und bundle exec rails console:
Untersuchen Sie an der Eingabeaufforderung den letzten Hai-Datensatz in der Datenbank:
Sie sehen den Datensatz, den Sie gerade erstellt haben:
Hier können Sie prüfen, ob die gefährdeten Haie (Endangered) mit dem folgenden Befehl persistent gespeichert wurden:
Ihr db _ data-Volume wurde im neu eingerichteten Datenbank-Dienst erfolgreich bereitgestellt, damit Sie über den app-Dienst auf die gespeicherten Daten zugreifen können. Wenn Sie direkt zur index shark-Seite navigieren, indem Sie localhost: 3000 / sharks oder http: / / your _ server _ ip: 3000 / sharks aufrufen, wird Ihnen auch so der Datensatz angezeigt:
Hai-Indexseite mit Mako
Die gefährdeten Haie finden Sie auch in der Ansicht auf localhost: 3000 / endangered / data oder http: / / your _ server _ ip: 3000 / endangered / data:
Die Anwendung wird jetzt in Docker Containern mit aktivierter Datenpersistenz und Codesynchronisierung ausgeführt.
Sie können die lokalen Codeänderungen auf dem Host ausprobieren, die dank der bind-Bereitstellung, die wir im Rahmen des app-Dienstes festgelegt haben, mit dem Container synchronisiert werden.
In diesem Tutorial haben Sie ein Entwicklungs-Setup für die Rails-Anwendung mit Docker Containern erstellt.
Sie haben Ihr Projekt modularer und portabler gemacht, indem Sie sensible Informationen extrahiert und den Zustand der Anwendung vom Code entkoppelt haben.
Sie haben auch eine docker-compose.yml-Standarddatei konfiguriert, die Sie je nach Ihren Entwicklungsbedürfnissen und -anforderungen ändern können.
Bei der Entwicklungsarbeit können Sie sich bei Interesse näher über das Konzipieren von Anwendungen für containerisierte und Cloud Native Workflows informieren.
Weitere Informationen zu diesen Themen finden Sie in Anwendungen aufbauen für Kubernetes und Modernisierung von Anwendungen für Kubernetes.
Wenn Sie in eine Fortbildung zu Kubernetes investieren möchten, sehen Sie sich den Leitfaden Kubernetes for Full-Stack Developers an.
Weitere Informationen über den Anwendungscode selbst entnehmen Sie den Tutorials der Reihe:
So erstellen Sie eine Ruby-on-Rails-Anwendung
So erstellen Sie verschachtelte Ressourcen für eine Ruby-on-Rails-Anwendung
So fügen Sie einer Ruby-on-Rails-Anwendung Impulse hinzu
So fügen Sie einer Ruby-on-Rails-Anwendung Bootstrap hinzu
So fügen Sie einer Ruby-on-Rails-Anwendung Sidekiq und Redis hinzu
Kubernetes for Full-Stack Developers
3688
Kubernetes for Full-Stack Developers eBook im EPUB-Format
Kubernetes for Full-Stack Developers eBook im PDF-Format < $>
Dieses Buch soll Newcomern und erfahrenen Benutzern dabei helfen, sich über Kubernetes zu informieren.
Die Kapitel im Buch stellen Ihnen zuerst die wichtigsten Kubernetes-Konzepte vor und bauen dann schrittweise darauf auf, bis schließlich das Ausführen einer Anwendung auf einem Produktionscluster leicht, wiederholbar und automatisch wird.
Daran anschließend werden fortgeschrittenere Themen vorgestellt, wie das Management eines Kubernetes-Clusters.
Es gibt zahlreiche Tools, Netzwerkkonfigurationen und Prozesse, die dazu beitragen können, Kubernetes verständlicher zu machen.
Dieses Buch erörtert nacheinander die einzelnen Themen, damit jeder, der es liest, in der Lage ist, selbst ein Kubernetes-Cluster zu erstellen, zu verwalten und zu überwachen.
Dieses Buch basiert auf dem Leitfaden Kubernetes for Full-Stack Developers, der in der DigitalOcean Community zu finden ist.
Es ist in einige Hauptthemen untergliedert:
Erlernen der Kubernetes-Grundkonzepte
Modernisieren von Anwendungen, sodass sie mit Containern arbeiten
Containerisieren von Awendungen
Der Einsatz von Anwendungen auf Kubernetes
Das Management des Cluster-Betriebs
Sie müssen die Themen nicht unbedingt in einer bestimmten Reihenfolge bearbeiten.
Wenn ein Abschnitt für Sie interessanter oder relevanter ist, können Sie Teile überspringen und später wieder darauf zurückkommen.
Und wenn Sie die Konzepte und Tools eines Abschnitts bereits beherrschen, können Sie den Abschnitt überspringen und sich anderen Themen zuwenden.
Für weitere Kubernetes-Ressourcen und zur Teilnahme in der DigitalOcean Community anderer Developer, die Kubernetes verwenden, können Sie sich unsere immer größer werdende Bibliothek mit Tutorials, Fragen und Projekten mit der Markierung Kubernetes ansehen.
Erste Schritte mit der Requests-Bibliothek in Python
3540
In vielen Webanwendungen ist es normal, sich über APIs mit verschiedenen Diensten von Drittanbietern zu verbinden.
Wenn Sie diese APIs verwenden, können Sie auf Daten wie Wetterinformationen, Sportergebnisse, Filmlisten, Tweets, Suchmaschinenergebnisse und Bilder zugreifen.
Sie können APIs auch verwenden, um Ihrer Anwendung Funktionen hinzuzufügen. Beispiele hierfür sind Zahlungen, Terminplanung, E-Mails, Übersetzungen, Karten und Dateiübertragungen.
Wenn Sie eine dieser Funktionen selbst erstellen würden, würde das eine Menge Zeit in Anspruch nehmen, aber mit APIs kann es nur Minuten dauern, eine Verbindung zu einer dieser Anwendungen herzustellen und auf ihre Funktionen und Daten zuzugreifen.
In diesem Artikel lernen wir die Python-Requests-Bibliothek kennen, die es Ihnen ermöglicht, HTTP-Anfragen in Python zu senden.
Und da die Verwendung einer API im Senden von HTTP-Anfragen und Empfangen von Antworten besteht, ermöglicht Ihnen Requests die Verwendung von APIs in Python.
Wir demonstrieren hier die Verwendung einer Sprachübersetzungs-API, damit Sie ein Beispiel für die Funktionsweise sehen können.
Schneller Überblick über HTTP-Anfragen
HTTP-Anfragen sind die Art und Weise, wie das Web funktioniert.
Jedes Mal, wenn Sie zu einer Webseite navigieren, stellt Ihr Browser mehrere Anfragen an den Server der Webseite.
Der Server antwortet dann mit allen Daten, die für die Darstellung der Seite erforderlich sind, und Ihr Browser rendert die Seite dann tatsächlich so, dass Sie sie sehen können.
Der generische Prozess ist folgender: Ein Client (wie ein Browser oder ein Python-Skript, das Anfragen verwendet) sendet einige Daten an eine URL. Dann liest der Server, der sich unter der URL befindet, die Daten, entscheidet, was damit zu tun ist, und gibt eine Antwort an den Client zurück.
Schließlich kann der Client entscheiden, was mit den Daten in der Antwort geschehen soll.
Ein Teil der Daten, die der Client in einer Anfrage sendet, ist die Anfragemethode.
Einige übliche Anfragemethoden sind GET, POST und PUT.
GET-Anfragen dienen normalerweise nur zum Lesen von Daten, ohne dass eine Änderung an etwas vorgenommen wird, während POST- und PUT-Anfragen im Allgemeinen dazu dienen, Daten auf dem Server zu ändern.
Die Stripe-API ermöglicht es Ihnen beispielsweise, POST-Anfragen zu verwenden, um eine neue Gebühr zu erstellen, damit ein Benutzer etwas von Ihrer Anwendung kaufen kann.
< $> note Anmerkung: Dieser Artikel behandelt GET-Anfragen, da wir keine Daten auf einem Server ändern werden.
Wenn Sie eine Anfrage aus einem Python-Skript oder innerhalb einer Webanwendung senden, können Sie als Entwickler entscheiden, was in jeder Anfrage gesendet wird und was mit der Antwort geschehen soll.
Untersuchen wir das also, indem wir zuerst eine Anfrage an Scotch.io senden und dann eine Sprachübersetzungs-API verwenden.
Installieren von Phython Requests
Als Erstes müssen wir die Bibliothek installieren.
Dazu installieren wir Requests unter Verwendung von pip.
Es ist eine gute Idee, zuerst eine virtuelle Umgebung zu erstellen, wenn Sie noch keine haben.
Unsere erste Anfrage
Wir beginnen damit, Requests zum Aufrufen der Site Scotch.io zu verwenden.
Erstellen Sie eine Datei namens script.py und fügen Sie den folgenden Code hinzu. In diesem Artikel haben wir nicht viel Code, mit dem wir arbeiten können. Wenn sich also etwas ändert, können Sie einfach den bestehenden Code aktualisieren, anstatt neue Zeilen hinzuzufügen.
Dieser Code sendet also lediglich eine GET-Anfrage an Scotch.io.
Dies ist die gleiche Art von Anfrage, die Ihr Browser zum Anzeigen dieser Seite gesendet hat. Der einzige Unterschied besteht darin, dass Requests HTML nicht wirklich rendern kann, sodass Sie stattdessen nur das rohe HTML und die anderen Antwortinformationen erhalten.
Wir verwenden hier die Funktion .get (), aber Requests ermöglicht Ihnen die Verwendung anderer Funktionen wie .post () und .put (), um diese Anfragen ebenfalls zu senden.
Sie können sie ausführen, indem Sie die Datei script.py ausführen.
Folgendes erhalten Sie als Ergebnis: Skriptausführung mit Ausgabe von Response 200
Statuscodes
Als Erstes können wir den Statuscode überprüfen.
Die HTTP-Codes reichen von 1XX bis 5XX.
Häufige Statuscodes, die Sie wahrscheinlich gesehen haben, sind 200, 404 und 500.
Hier ist ein kurzer Überblick über die Bedeutung der einzelnen Statuscodes:
1XX - Informationen
2XX - Erfolg
3XX - Umleitung
4XX - Client-Fehler (Sie haben einen Fehler gemacht)
5XX - Server-Fehler (sie haben einen Fehler gemacht)
Wenn Sie Ihre eigenen Anfragen durchführen, suchen Sie im Allgemeinen nach Statuscodes in dem Bereich 200.
Requests erkennt, dass 4XX- und 5XX-Statuscodes Fehler sind. Wenn diese Statuscodes also zurückgegeben werden, wird das Antwortobjekt aus der Anfrage zu False ausgewertet.
Sie können testen, ob eine Anfrage erfolgreich geantwortet hat, indem Sie die Antwort auf Wahrheit prüfen.
Die Ausgabe Response 200 mit nachfolgendem Ergebnis von Response OK
Die Meldung "Response Failed" wird nur dann angezeigt, wenn ein Statuscode 400 oder 500 zurückgegeben wird.
Versuchen Sie, die URL in irgendeinen Unsinn zu ändern, um zu sehen, ob die Antwort mit einer 404 fehlschlägt.
Sie können sich den Statuscode direkt ansehen, indem Sie Folgendes hinzufügen:
Dadurch wird Ihnen der Statuscode direkt angezeigt, sodass Sie die Zahl selbst überprüfen können.
Fehlerausgabe mit 404
Headers
Was man außerdem aus der Antwort entnehmen kann, sind die Header.
Sie können sie mithilfe des Header-Verzeichnisses auf dem Antwortobjekt ansehen.
Ausgabe mit Headern in der Standardausgabe
Die Header werden zusammen mit der Anfrage gesendet und in der Antwort zurückgegeben.
Header werden verwendet, damit sowohl der Client als auch der Server wissen, wie die Daten, die in der Antwort / Antwort gesendet und empfangen werden, zu interpretieren sind.
Wir sehen die verschiedenen Header, die zurückgegeben werden.
In vielen Fällen werden Sie die Header-Informationen nicht direkt verwenden müssen, aber sie sind vorhanden, wenn Sie sie benötigen.
Der Inhaltstyp ist in der Regel derjenige, den Sie benötigen, da er das Format der Daten offenbart, z. B. HTML, JSON, PDF, Text usw. Aber der Inhaltstyp wird normalerweise von Requests gehandhabt, sodass Sie auf die zurückgegebenen Daten zugreifen können.
Antworttext
Und schließlich können wir bei Betrachtung von res.text (das funktioniert bei Textdaten, z. B. bei einer HTML-Seite, wie wir sie gerade sehen) das gesamte HTML sehen, das für die Erstellung der Homepage von Scotch benötigt wird.
Es wird nicht gerendert, aber stellen fest, dass es so aussieht, als ob es zu Scotch gehört.
Wenn Sie dies in einer Datei gespeichert hätten und diese öffnen würden, würden Sie etwas sehen, das der Scotch-Site ähnelt.
In einer realen Situation werden mehrere Anfragen für eine einzige Webseite gestellt, um Elemente wie Bilder, Skripte und Formatvorlagen zu laden. Wenn Sie also nur den HTML-Code in einer Datei speichern, sieht diese nicht so aus, wie die Seite Scotch.io in Ihrem Browser aussieht, weil nur eine einzige Anfrage ausgeführt wurde, um die HTML-Daten zu erhalten.
Gedruckte HTML-Daten auf der Befehlszeile
Verwenden der Translate API
Lassen Sie uns nun zu etwas Interessanterem übergehen.
Wir werden die Yandex Translate API verwenden, um eine Anfrage zur Übersetzung eines Textes in eine andere Sprache durchzuführen.
Um die API zu verwenden, müssen Sie sich zuerst anmelden.
Nachdem Sie sich angemeldet haben, gehen Sie zu der Translate API und erstellen einen API-Schlüssel.
Sobald Sie den API-Schlüssel haben, fügen Sie ihn als Konstante zu Ihrer Datei hinzu.
Hier ist der Link, unter dem Sie all diese Dinge tun können: https: / / tech.yandex.com / translate /
Wir benötigen einen API-Schlüssel, damit Yandex uns jedes Mal authentifizieren kann, wenn wir ihre API verwenden wollen.
Der API-Schlüssel ist eine leichtgewichtige Form der Authentifizierung, da er beim Senden am Ende der Anfrage-URL hinzugefügt wird.
Um zu wissen, welche URL wir senden müssen, um die API zu verwenden, können wir uns die Dokumentation von Yandex ansehen.
Wenn wir dort nachschauen, sehen wir alle Informationen, die wir für die Verwendung ihrer Translate API zur Übersetzung von Text benötigen.
Anfragesyntax zur Verwendung der API
Wenn wir eine URL mit Und-Zeichen (&), Fragezeichen (?) und Gleichheitszeichen (=) sehen, können Sie sicher sein, dass die URL für GET-Anfragen bestimmt ist.
Diese Symbole geben die Parameter an, die mit der URL einhergehen.
Normalerweise sind Angaben in eckigen Klammern () optional.
In diesem Fall sind format, options und callback optional, während key, text und lang für die Anfrage erforderlich sind.
Fügen wir also etwas Code hinzu, der an diese URL gesendet werden soll.
Sie können die erste von uns erstellte Anfrage wie folgt ersetzen:
Es gibt zwei Möglichkeiten, wie wir die Parameter hinzufügen können.
Wir können sie entweder direkt an das Ende der URL anhängen, oder wir können dies von Requests erledigen lassen.
Um Letzteres zu tun, können wir ein Wörterbuch für unsere Parameter erstellen.
Die drei Elemente, die wir benötigen, sind der Schlüssel, der Text und die Sprache.
Lassen Sie uns das Wörterbuch unter Verwendung des API-Schlüssels, "Hello" für den Text und "en-es" als Sprache erstellen, was bedeutet, dass wir von Englisch nach Spanisch übersetzen wollen.
Wenn Sie weitere Sprachcodes benötigen, können Sie hier nachschauen.
Sie suchen nach der Spalte 639-1.
Wir erstellen ein params-Wörterbuch, indem wir die Funktion dict () verwenden und die Schlüssel und Werte, die wir in unserem Wörterbuch haben wollen, eingeben.
Jetzt nehmen wir das Parameter-Wörterbuch und übergeben es an die Funktion .get ().
Wenn wir die Parameter auf diese Weise übergeben, fährt Requests fort und fügt die Parameter für uns in die URL ein.
Nun fügen wir eine Druckanweisung für den Antworttext hinzu und sehen uns an, was in der Antwort zurückgegeben wird.
Ausgabewörterbuch mit den eingegebenen Werten
Wir sehen drei Dinge.
Wir sehen den Statuscode, der genau derselbe Statuscode der Antwort selbst ist, wir sehen die von uns angegebene Sprache und wir sehen den übersetzten Text innerhalb der Liste.
Sie sollten also "Hola" für den übersetzten Text sehen.
Versuchen Sie noch einmal mit en-fr als Sprachcode und Sie sollten in der Antwort nun "Bonjour" sehen.
Französischer übersetzter Text
Schauen wir uns die Header für diese spezielle Antwort an.
In der Ausgabe gedruckte Header
Offensichtlich sollten die Header unterschiedlich sein, weil wir mit einem anderen Server kommunizieren, aber in diesem Fall ist der Inhaltstyp application / json statt text / html.
Das bedeutet, dass die Daten als JSON interpretiert werden können.
Wenn application / json der Inhaltstyp der Antwort ist, können wir die Antwort von Requests in ein Wörterbuch und eine Liste konvertieren lassen, damit wir leichter auf die Daten zugreifen können.
Damit die Daten als JSON geparst werden, verwenden wir die .json () -Methode auf dem Antwortobjekt.
Wenn Sie es ausdrucken, werden Sie sehen, dass die Daten gleich aussehen, aber das Format etwas anders ist.
Der Grund für den Unterschied liegt darin, dass es sich nicht mehr um reinen Text handelt, den Sie von res.text erhalten.
Diesmal ist es eine gedruckte Version eines Wörterbuchs.
Nehmen wir an, wir wollen auf den Text zugreifen.
Da dies nun ein Wörterbuch ist, können wir den Schlüssel text verwenden.
Und jetzt sehen wir nur noch die Daten für diesen einen Schlüssel.
In diesem Fall handelt es sich um eine Liste mit einem Element. Wenn wir also diesen Text direkt in die Liste aufnehmen wollten, können wir über den Index darauf zugreifen.
"Bonjour" ohne die eckigen Klammern
Und jetzt sehen wir nur noch das übersetzte Wort.
Wenn wir also unsere Parameter ändern, erhalten wir natürlich andere Ergebnisse.
Ändern wir den zu übersetzenden Text von "Hello" in "Goodbye", ändern wir die Zielsprache wieder auf Spanisch und senden die Anfrage erneut.
"Adios" zur Ausgabe gedruckt. Versuchen Sie, längere Texte in verschiedene Sprachen zu übersetzen und sehen Sie, welche Antworten die API Ihnen gibt.
Translate API Fehlerfälle
Zum Schluss sehen wir uns einen Fehlerfall an.
Es funktioniert nicht immer alles, deshalb müssen wir wissen, wenn das passiert.
Versuchen Sie, Ihren API-Schlüssel zu ändern, indem Sie ein Zeichen entfernen.
Wenn Sie dies tun, ist Ihr API-Schlüssel nicht mehr gültig.
Versuchen Sie dann, eine Anfrage zu senden.
Wenn Sie einen Blick auf den Statuscode werfen, erhalten Sie Folgendes:
Fehler 403 Wenn Sie also die API verwenden, sollten Sie prüfen, ob alles erfolgreich ist oder nicht, damit Sie die Fehlerfälle entsprechend den Anforderungen Ihrer Anwendung behandeln können.
Wir haben Folgendes gelernt:
Wie HTTP-Anfragen funktionieren
Die verschiedenen möglichen Statuscodes in einer Antwort
Wie man mithilfe der Python-Requests-Bibliothek Anfragen sendet und Antworten erhält
Wie man eine Sprachübersetzungs-API zum Übersetzen von Text verwendet
Wie man die Antworten von Anwendungen / JSON-Inhalten in Wörterbücher konvertiert
Wenn Sie mehr tun möchten, sehen Sie sich diese Liste an, um die verschiedenen verfügbaren APIs zu sehen, und versuchen Sie, sie mit Python-Requests zu verwenden.
So installieren und verwenden Sie Radamsa zum Fuzz-Testen von Programmen und Netzwerkdiensten unter Ubuntu 18.04
3537
Sicherheitsbedrohungen werden immer ausgefeilter, sodass Entwickler und Systemadministratoren bei der Verteidigung und dem Testen der Sicherheit ihrer Anwendungen einen proaktiven Ansatz verfolgen müssen.
Eine gängige Methode zum Testen der Sicherheit von Client-Anwendungen oder Netzwerkdiensten ist das Fuzzing, bei dem wiederholt ungültige oder fehlerhafte Daten an die Anwendung gesendet und deren Antwort analysiert wird.
Dies ist nützlich zum Testen, wie widerstandsfähig und robust die Anwendung gegenüber unerwarteten Eingaben ist, zu denen beschädigte Daten oder tatsächliche Angriffe gehören können.
Radamsa ist ein Open-Source-Fuzzing-Werkzeug, das Testfälle auf der Grundlage von benutzerdefinierten Eingabedaten generieren kann. Radamsa ist vollständig skriptfähig und hat bisher erfolgreich Schwachstellen in realen Anwendungen, wie z. B. Gzip, gefunden.
In diesem Tutorial werden Sie Radamsa installieren und verwenden, um Fuzz-Tests von Befehlszeilen- und netzwerkbasierten Anwendungen mit Ihren eigenen Testfällen durchzuführen.
< $> warning Warnung: Radamsa ist ein Penetrationstesttool, mit dem Sie möglicherweise Schwachstellen oder Schwächen in bestimmten Systemen oder Anwendungen identifizieren können.
Sie dürfen die mit Radamsa gefundenen Schwachstellen nicht für fahrlässiges Verhalten, Schädigung oder böswillige Ausnutzung verwenden.
Schwachstellen sollten dem Betreuer der betroffenen Anwendung auf ethische Weise gemeldet und nicht ohne ausdrückliche Genehmigung öffentlich bekannt gegeben werden.
Einen Ubuntu-18.04-Server, der gemäß Ersteinrichtung des Servers mit Ubuntu 18.04 eingerichtet wurde, einschließlich eines sudo-Benutzers ohne Rootberechtigung und einer aktivierten Firewall, die weniger wichtige Ports blockiert.
Eine befehlszeilen- oder netzwerkbasierte Anwendung, die Sie testen möchten, z. B. Gzip, Tcpdump, Bind, Apache, jq oder eine andere Anwendung Ihrer Wahl.
Als Beispiel für die Zwecke dieses Tutorials verwenden wir jq.
< $> warning Warnung: Radamsa kann dazu führen, dass Anwendungen oder Systeme instabil laufen oder abstürzen. Führen Sie Radamsa daher nur in einer Umgebung aus, in der Sie darauf vorbereitet sind, z. B. auf einem dedizierten Server.
Bitte stellen Sie auch sicher, dass Sie die ausdrückliche schriftliche Genehmigung des Eigentümers eines Systems haben, bevor Sie Fuzz-Tests gegen dieses System durchführen. < $>
Schritt 1 - Installation von Radamsa
Zuerst werden Sie Radamsa herunterladen und kompilieren, um es auf Ihrem System zu verwenden.
Der Radamsa-Quellcode ist im offiziellen Repository von GitLab verfügbar.
Installieren Sie dann die Pakete gcc, git, make und wget, die zum Kompilieren des Quellcodes in eine ausführbare Binärdatei benötigt werden:
Nach der Bestätigung der Installation lädt apt die angegebenen Pakete und alle erforderlichen Abhängigkeiten herunter und installiert sie.
Als Nächstes laden Sie eine Kopie des Quellcodes für Radamsa herunter, indem Sie ihn aus dem Repository, das auf GitLab gehostet wird, klonen:
Dadurch wird ein Verzeichnis namens radamsa erstellt, das den Quellcode für die Anwendung enthält.
Wechseln Sie in das Verzeichnis, um mit der Kompilierung des Codes zu beginnen:
Als Nächstes können Sie den Kompilierungsprozess mit make starten:
Schließlich können Sie die kompilierte Radamsa-Binärdatei in Ihren $PATH installieren:
Wenn Sie einen Fehler radamsa: command not found sehen, überprüfen Sie noch einmal, ob alle erforderlichen Abhängigkeiten installiert wurden und ob es beim Kompilieren keine Fehler gab.
Nachdem Sie nun Radamsa installiert haben, können Sie damit beginnen, einige Beispieltestfälle zu generieren, um zu verstehen, wie Radamsa funktioniert und wofür es verwendet werden kann.
Schritt 2 - Generieren von Fuzzing-Testfällen
Nachdem Radamsa jetzt installiert wurde, können Sie damit einige Fuzzing-Testfälle generieren.
Ein Testfall ist ein Datenteil, der als Eingabe für das zu testende Programm verwendet wird.
Wenn Sie beispielsweise einen Fuzz-Test für ein Archivierungsprogramm wie Gzip durchführen, kann ein Testfall ein Dateiarchiv sein, das Sie zu dekomprimieren versuchen.
< $> note Anmerkung: Radamsa manipuliert Eingabedaten auf eine Vielzahl unerwarteter Arten, einschließlich extremer Wiederholungen, Bit-Flips, Einfügen von Steuerzeichen und so weiter.
Dies kann dazu führen, dass Ihre Terminal-Sitzung abbricht oder instabil wird. Seien Sie sich dessen bewusst, bevor Sie fortfahren.
Geben Sie zunächst ein einfaches Textstück an Radamsa weiter, um zu sehen, was passiert:
Dadurch werden die eingegebenen Daten manipuliert (oder gefuzzt) und ein Testfall ausgegeben, wie beispielsweise:
In diesem Fall hat Radamsa ein zusätzliches Komma zwischen Hello und world eingefügt.
Dies mag nicht wie eine wesentliche Änderung erscheinen, aber in einigen Anwendungen kann dies dazu führen, dass die Daten falsch interpretiert werden.
Versuchen wir es noch einmal, indem wir den gleichen Befehl ausführen.
Sie werden eine andere Ausgabe sehen:
Dieses Mal wurden mehrere einfache Anführungszeichen (') in die Zeichenfolge eingefügt, darunter eines, das das l in world überschrieb.
Dieser spezielle Testfall führt mit größerer Wahrscheinlichkeit zu Problemen für eine Anwendung, da einfache / doppelte Anführungszeichen oft verwendet werden, um verschiedene Datenteile in einer Liste zu trennen.
Versuchen wir es noch einmal:
In diesem Fall fügte Radamsa eine Shell-Injektionszeichenfolge ein, die zum Testen auf Befehlsinjektions-Schwachstellen in der zu testenden Anwendung nützlich sein wird.
Sie haben Radamsa dazu verwendet, eine Eingabezeichenfolge zu fuzzen und eine Reihe von Testfällen zu erstellen.
Als Nächstes werden Sie Radamsa zum Fuzzen einer Befehlszeilenanwendung verwenden.
Schritt 3 - Fuzzing einer Befehlszeilenanwendung
In diesem Schritt werden Sie Radamsa verwenden, um eine Befehlszeilenanwendung zu fuzzen und über alle auftretenden Abstürze zu berichten.
Die genaue Technik für das Fuzzing jedes Programms variiert erheblich und verschiedene Methoden werden für verschiedene Programme am effektivsten sein.
In diesem Tutorial werden wir jedoch das Beispiel von jq verwenden, das ein Befehlszeilenprogramm zur Verarbeitung von JSON-Daten ist.
Sie können jedes andere ähnliche Programm verwenden, solange es dem allgemeinen Prinzip folgt, eine Form von strukturierten oder unstrukturierten Daten zu nehmen, etwas damit zu tun und dann ein Ergebnis auszugeben.
Dieses Beispiel würde zum Beispiel auch mit Gzip, Grep, bc, tr usw. funktionieren.
Wenn Sie jq nicht bereits installiert haben, können Sie es mit apt installieren:
jq wird nun installiert.
Um mit dem Fuzzing zu beginnen, erstellen Sie eine JSON-Beispieldatei, die Sie als Eingabe für Radamsa verwenden:
Fügen Sie dann die folgenden JSON-Beispieldaten in die Datei ein:
Sie können diese Datei mit jq parsen, wenn Sie überprüfen wollen, ob die JSON-Syntax gültig ist:
Wenn die JSON gültig ist, wird jq die Datei ausgeben.
Andernfalls wird ein Fehler ausgegeben, mit dem Sie die Syntax bei Bedarf korrigieren können.
Als nächstes fuzzen Sie die JSON-Testdatei mit Radamsa und übergeben diese dann an jq.
Dadurch liest jq den gefuzzten / manipulierten Testfall und nicht die ursprünglich gültigen JSON-Daten:
Wenn Radamsa die JSON-Datei auf eine Art und Weise fuzzt, dass sie syntaktisch noch gültig sind, gibt jq die Daten aus, jedoch mit den Änderungen, die Radamsa daran vorgenommen hat.
Wenn Radamsa bewirkt, dass die JSON-Daten ungültig werden, zeigt jq alternativ einen entsprechenden Fehler an.
Das alternative Ergebnis wäre, dass jq nicht in der Lage ist, die gefuzzten Daten korrekt zu verarbeiten, was zu einem Absturz oder Fehlverhalten führt.
Genau das ist es, was Sie beim Fuzzing wirklich suchen, da dies auf eine Sicherheitsschwachstelle wie einen Pufferüberlauf oder eine Befehlsinjektion hindeuten könnte.
Um effizienter auf solche Schwachstellen zu testen, kann ein Bash-Skript verwendet werden, um den Fuzzing-Prozess zu automatisieren, einschließlich der Generierung von Testfällen, der Übergabe an das Zielprogramm und der Erfassung aller relevanten Ausgaben.
Erstellen Sie eine Datei namens jq-fuzz.sh:
Der genaue Inhalt des Skripts hängt von der Art des Programms, das Sie fuzzen, und den Eingabedaten ab, aber im Fall von jq und anderen ähnlichen Programmen reicht das folgende Skript aus.
Kopieren Sie das Skript in Ihre jq-fuzz.sh-Datei:
Dieses Skript enthält ein while, um den Inhalt in einer Schleife zu wiederholen.
Bei jedem Durchlaufen der Skript-Schleife generiert Radamsa einen Testfall auf der Basis von test.json und speichert ihn in input.txt.
Der input.txt-Testfall wird dann durch jq ausgeführt, wobei alle Standard- und Fehlerausgaben zu / dev / null umgeleitet werden, um zu vermeiden, dass der Terminal-Bildschirm überfüllt wird.
Schließlich wird der Exit-Wert von jq überprüft.
Wenn der Exit-Wert größer als 127 ist, was auf einen fatalen Abbruch (einen Absturz) hindeutet, werden die Eingabedaten zur späteren Überprüfung in einer Datei mit dem Namen crash- gespeichert - gefolgt vom aktuellen Datum in Unix-Sekunden und -Nanosekunden.
Markieren Sie das Skript als ausführbar und führen Sie es aus, um mit dem automatischen Fuzz-Test jq zu beginnen:
Um das Skript zu beenden können Sie jederzeit STRG + C drücken.
Sie können dann überprüfen, ob Abstürze gefunden wurden, indem Sie mit ls eine Verzeichnisliste mit den erzeugten Absturzdateien anzeigen.
Vielleicht möchten Sie Ihre JSON-Eingabedaten verbessern, da die Verwendung einer komplexeren Eingabedatei wahrscheinlich die Qualität Ihrer Fuzzing-Ergebnisse verbessert.
Vermeiden Sie die Verwendung einer großen Datei oder einer Datei, die viele sich wiederholende Daten enthält - eine ideale Eingabedatei ist eine Datei, die klein ist, aber dennoch so viele "komplexe" Elemente wie möglich enthält.
Eine gute Eingabedatei enthält beispielsweise Datenproben, die in allen Formaten gespeichert sind, einschließlich Zeichenfolgen, Ganzzahlen, Boolesche, Listen und Objekte, sowie, wenn möglich, verschachtelte Daten.
Sie haben Radamsa zum Fuzzen einer Befehlszeilenanwendung verwendet.
Als Nächstes werden Sie Radamsa zum Fuzzen von Anfragen an Netzwerkdienste verwenden.
Schritt 4 - Fuzzing von Anfragen an Netzwerkdienste
Radamsa kann auch zum Fuzzen von Netzwerkdiensten verwendet werden, wobei es entweder als Netzwerk-Client oder als Server fungiert.
In diesem Schritt verwenden Sie Radamsa zum Fuzzen eines Netzwerkdienstes, wobei Radamsa als Client fungiert.
Der Zweck des Fuzzings von Netzwerkdiensten besteht darin, zu testen, wie belastbar ein bestimmter Netzwerkdienst gegenüber Clients ist, die ihm fehlerhafte und / oder bösartige Daten senden. Viele Netzwerkdienste wie Web- oder DNS-Server sind in der Regel dem Internet ausgesetzt, was bedeutet, dass sie ein häufiges Ziel für Angreifer sind.
Ein Netzwerkdienst, der nicht ausreichend widerstandsfähig gegen den Empfang fehlerhafter Daten ist, kann abstürzen oder - schlimmer noch - in einem offenen Zustand ausfallen, sodass Angreifer sensible Daten wie Verschlüsselungsschlüssel oder Benutzerdaten lesen können.
Die spezifische Technik zum Fuzzing von Netzwerkdiensten variiert je nach dem betreffenden Netzwerkdienst enorm. In diesem Beispiel werden wir jedoch Radamsa zum Fuzzing eines einfachen Webservers verwenden, der statische HTML-Inhalte anbietet.
Zuerst müssen Sie den Webserver für die Tests einrichten.
Sie können dies mit dem integrierten Entwicklungsserver tun, der mit dem php-cli-Paket geliefert wird.
Zum Testen Ihres Webservers benötigen Sie auch curl.
Wenn Sie php-cli und / oder curl nicht installiert haben, können Sie sie mit apt installieren:
Als Nächstes erstellen Sie ein Verzeichnis, in dem Sie Ihre Webserver-Dateien speichern, und wechseln zu diesem:
Erstellen Sie dann eine HTML-Datei, die einen Beispieltext enthält:
Fügen Sie der Datei Folgendes hinzu:
Nun können Sie Ihren PHP-Webserver ausführen.
Sie müssen in der Lage sein, das Webserver-Protokoll einzusehen, während Sie noch eine andere Terminal-Sitzung verwenden. Öffnen Sie dazu eine weitere Terminal-Sitzung und SSH zu Ihrem Server:
Sie können jetzt wieder zu Ihrer ursprünglichen Terminalsitzung zurückwechseln und mit curl testen, ob der Webserver funktioniert:
Dies wird die Beispieldatei index.html ausgeben, die Sie zuvor erstellt haben:
Ihr Webserver muss nur lokal erreichbar sein, Sie sollten also keine Ports auf Ihrer Firewall dafür öffnen.
Nachdem Sie Ihren Testwebserver eingerichtet haben, können Sie nun damit beginnen, einen Fuzz-Test mit Radamsa durchzuführen.
Zuerst müssen Sie eine HTTP-Beispielanfrage erstellen, die als Eingabedaten für Radamsa verwendet werden soll.
Erstellen Sie eine neue Datei, in der diese gespeichert wird:
Kopieren Sie dann die folgende HTTP-Beispielanfrage in die Datei:
Als Nächstes können Sie Radamsa verwenden, um diese HTTP-Anfrage an Ihren lokalen Webserver zu senden.
Dazu müssen Sie Radamsa als TCP-Client verwenden, was durch die Angabe einer IP-Adresse und eines Ports für die Verbindung möglich ist:
< $> note Anmerkung: Beachten Sie, dass die Verwendung von Radamsa als TCP-Client möglicherweise die Übertragung von fehlerhaften / bösartigen Daten über das Netzwerk verursacht.
Dies kann zu Problemen führen. Achten Sie daher sehr darauf, nur auf Netzwerke zuzugreifen, die Sie testen dürfen, oder halten Sie sich vorzugsweise an die Adresse localhost (127.0.0.1).
Wenn Sie sich die ausgegebenen Protokolle für Ihren lokalen Webserver ansehen, werden Sie feststellen, dass er die Anfragen erhalten, sie aber höchstwahrscheinlich nicht verarbeitet hat, da sie ungültig / fehlerhaft waren.
Die ausgegebenen Protokolle werden in Ihrem zweiten Terminalfenster sichtbar sein:
Für optimale Ergebnisse und um sicherzustellen, dass Abstürze aufgezeichnet werden, sollten Sie ein Automatisierungsskript ähnlich dem in Schritt 3 verwendeten schreiben. Sie sollten außerdem in Betracht ziehen, eine komplexere Eingabedatei zu verwenden, die möglicherweise Zusätze wie zusätzliche HTTP-Header enthält.
Sie haben einen Netzwerkdienst mit Radamsa als TCP-Client gefuzzt.
Als Nächstes werden Sie einen Netzwerk-Client mit Radamsa als Server fuzzen.
Schritt 5 - Fuzzing von Netzwerk-Client-Anwendungen
In diesem Schritt werden Sie Radamsa zum Fuzz-Test einer Netzwerk-Client-Anwendung verwenden.
Dies wird erreicht, indem die Antworten eines Netzwerkdienstes abgefangen und gefuzzt werden, bevor sie vom Client empfangen werden.
Der Zweck dieser Art des Fuzzings besteht darin, zu testen, wie belastbar Netzwerk-Client-Anwendungen gegenüber dem Empfang von fehlerhaften oder bösartigen Daten von Netzwerkdiensten sind.
Zum Beispiel das Testen eines Webbrowsers (Client), der fehlerhaftes HTML von einem Webserver (Netzwerkdienst) empfängt, oder das Testen eines DNS-Clients, der fehlerhafte DNS-Antworten von einem DNS-Server empfängt.
Wie beim Fuzzing von Befehlszeilenanwendungen oder Netzwerkdiensten variiert die genaue Technik für das Fuzzing jeder Netzwerk-Client-Anwendung erheblich. In diesem Beispiel wird jedoch whois verwendet, eine einfache TCP-basierte Sende- / Empfangsanwendung.
Die whois-Anwendung wird verwendet, um Anfragen an WHOIS-Server zu stellen und WHOIS-Datensätze als Antworten zu erhalten.
WHOIS arbeitet im Klartext über den TCP-Port 43 und ist somit ein guter Kandidat für netzwerkbasierte Fuzz-Tests.
Wenn Sie noch nicht über whois verfügen, können Sie es mit apt installieren:
Zunächst müssen Sie eine Probe der whois-Antwort erwerben, die Sie als Eingabedaten verwenden können. Sie können dies tun, indem Sie eine whois-Abfrage stellen und die Ausgabe in einer Datei speichern.
Sie können hier jede beliebige Domäne verwenden, da Sie das whois-Programm lokal mit Beispieldaten testen:
Als Nächstes müssen Sie Radamsa als Server einrichten, der gefuzzte Versionen dieser whois-Antwort liefert.
Sie müssen in der Lage sein, Ihr Terminal weiter zu benutzen, sobald Radamsa im Servermodus läuft. Es wird daher empfohlen, eine weitere Terminalsitzung und eine SSH-Verbindung zu Ihrem Server zu öffnen:
Radamsa läuft nun im TCP-Server-Modus und stellt bei jedem Verbindungsaufbau zum Server eine gefuzzte Version von whois.txt zur Verfügung, unabhängig davon, welche Anfragedaten empfangen werden.
Sie können nun mit dem Testen der whois-Client-Anwendung fortfahren.
Sie müssen eine normale whois-Abfrage für eine beliebige Domäne Ihrer Wahl stellen (es muss nicht dieselbe sein, für die die Beispieldaten bestimmt sind), aber mit whois, das auf Ihren lokalen Radamsa-Server verweist:
Die Antwort wird aus Ihren Beispieldaten bestehen, die jedoch von Radamsa gefuzzt wurden.
Solange Radamsa ausgeführt wird, können Sie weiterhin Anfragen an den lokalen Server stellen, und es wird jedes Mal eine andere gefuzzte Antwort ausgegeben.
Wie beim Fuzzing von Netzwerkdiensten können Sie, um die Effizienz dieses Netzwerk-Client-Fuzz-Tests zu verbessern und sicherzustellen, dass alle Abstürze erfasst werden, ein Automatisierungsskript schreiben, das dem in Schritt 3 verwendeten ähnelt.
In diesem letzten Schritt haben Sie Radamsa zur Durchführung von Fuzz-Tests einer Netzwerk-Client-Anwendung verwendet.
In diesem Artikel richteten Sie Radamsa ein und verwendeten es zum Fuzzen einer Befehlszeilenanwendung, eines Netzwerkdienstes und eines Netzwerk-Clients.
Sie verfügen nun über die grundlegenden Kenntnisse, die für das Fuzz-Testen Ihrer eigenen Anwendungen erforderlich sind, hoffentlich mit dem Ergebnis, ihre Robustheit und Widerstandsfähigkeit gegen Angriffe zu verbessern.
Wenn Sie Radamsa weiter erkunden möchten, sollten Sie sich die Radamsa README-Datei im Detail ansehen, da sie weitere technische Informationen und Beispiele für die Verwendung des Tools enthält:
Radamsa README-Datei
Vielleicht möchten Sie sich auch einige andere Fuzzing-Tools ansehen, wie z. B. American Fuzzy Lop (AFL), ein fortschrittliches Fuzzing-Tool, das für das Testen von binären Anwendungen mit extrem hoher Geschwindigkeit und Genauigkeit entwickelt wurde:
American Fuzzy Lop
So installieren Sie Docker Compose unter Debian 10
3541
Docker ist ein hervorragendes Tool zur Automatisierung der Bereitstellung von Linux-Anwendungen in Software-Containern. Um jedoch das volle Potenzial dieser Anwendung auszuschöpfen, sollte jede Komponente einer Anwendung in ihrem ihren eigenen Container ausgeführt werden.
Bei komplexen Anwendungen mit vielen Komponenten kann es sich sehr schnell als schwierig erweisen, den gemeinsamen Start, die Kommunikation und das gemeinsame Abschalten aller Container zu koordinieren.
Die Docker-Community hat eine beliebte Lösung namens Fig gefunden, mit der Sie eine einzige YAML-Datei verwenden können, um alle Docker-Container und -Konfigurationen zu koordinieren.
Sie ist so populär geworden, dass das Docker-Team beschlossen hat, Docker Compose basierend auf der Fig-Quelle zu erstellen, die inzwischen veraltet ist.
Docker Compose ermöglicht die Koordination der Prozesse von Docker-Containern, einschließlich des Startens, Herunterfahrens und Einrichtens von Verknüpfungen zwischen Containern und Volumes.
In diesem Tutorial installieren Sie die neueste Version von Docker Compose, die Sie bei der Verwaltung von Multi-Container-Anwendungen auf einem Debian 10 Server unterstützt.
Einen Debian-10-Server und einen Benutzer ohne Rootberechtigung mit Sudo-Privilegien.
Dieses Tutorial zur Ersteinrichtung des Servers mit Debian 10 erklärt das Einrichten.
Laut den Anweisungen aus Schritt 1 und Schritt 2 unter So installieren und verwenden Sie Docker unter Debian 10 installierter Docker
< $> note Anmerkung: Obwohl die Voraussetzungen Anleitungen zur Installation von Docker unter Debian 10 enthalten, sollten die Docker-Befehle in diesem Artikel mit anderen Betriebssystemen funktionieren, solange Docker installiert ist.
Sie können Docker Compose zwar aus den offiziellen Debian-Repositorys installieren, es handelt sich jedoch nicht die aktuellste Version, daher werden Sie es in diesem Tutorial vom Docker GitHub-Repository installieren.
Der nachfolgende Befehl unterscheidet sich geringfügig von dem, den Sie auf der Seite Releases finden.
Indem Sie das Flag -o verwenden, um die Ausgabedatei zuerst anzugeben, anstatt die Ausgabe umzuleiten, verhindert diese Syntax, dass der Fehler "Permission denied" auftritt, wenn Sie sudo verwenden.
Überprüfen Sie die aktuelle Version und aktualisieren Sie sie in dem nachfolgenden Befehl:
Als Nächstes legen wir die Berechtigungen fest:
Dann verifizieren wir, ob die Installation erfolgreich war, indem wir die Version überprüfen:
Damit wird die von uns installierte Version ausgedruckt:
Jetzt haben wir Docker Compose installiert und können ein "Hello World" -Beispiel ausführen.
Schritt 2 - Ausführen eines Containers mit Docker Compose
Die öffentliche Docker-Registrierung, Docker Hub, enthält ein Hello World-Image zu Demonstrations- und Testzwecken.
Sie zeigt, dass nur eine minimale Konfiguration notwendig ist, um einen Container mit Docker Compose ausführen: eine YAML-Datei, die ein einzelnes Image aufruft.
Wir erstellen diese minimale Konfiguration, um unseren hello-world-Container auszuführen.
Erstellen Sie zunächst ein Verzeichnis für die YAML-Datei und wechseln Sie zu ihm:
Erstellen Sie dann die YAML-Datei:
Legen Sie den folgenden Inhalt in der Datei ab, speichern Sie die Datei und beenden Sie das Textbearbeitungsprogramm:
Die erste Zeile in der YAML-Datei wird als Teil des Container-Namens verwendet.
Die zweite Zeile gibt an, welches Image zum Erstellen des Containers verwendet wird.
Wenn wir den Befehl docker-compose up ausführen, sucht er nach einem lokalen Image mit dem Namen, den wir angegeben haben, nämlich hello-world.
Dann speichern wir die Datei und schließen sie.
Mit dem Befehl docker images können Sie Images manuell auf dem System ansehen:
Wenn es keine lokalen Images gibt, werden nur die Spalten-Überschriften angezeigt:
Solange Sie noch im Verzeichnis ~ / hello-world sind, sollten Sie den folgenden Befehl ausführen:
Wenn Sie den Befehl zum ersten Mal ausführen und es kein lokales Image namens hello-world gibt, holt sich Docker Compose dieses Image aus dem öffentlichen Repository von Docker Hub:
Nachdem das Image abgerufen wurde, erstellt docker-compose einen Container, fügt das Programm hello an und führt es aus, was wiederum bestätigt, dass die Installation anscheinend funktioniert:
Dann wird eine Erklärung der Vorgehensweise gedruckt:
Docker-Container laufen nur solange, wie der Befehl aktiv ist. Sobald also sobald hello nicht mehr ausgeführt wird, wird der Container angehalten.
Wenn wir uns also aktive Prozesse ansehen, werden die Spalten-Überschriften angezeigt, aber der hello-world-Container wird nicht aufgelistet, weil er nicht ausgeführt wird:
Sie können die Container-Informationen sehen, die Sie im nächsten Schritt benötigen werden, indem Sie das Flag -a verwenden.
Damit werden alle Container angezeigt, nicht nur die aktiven:
Damit werden die Informationen angezeigt, die Sie benötigen, um den Container zu entfernen, wenn Sie damit fertig sind.
Schritt 3 - Entfernen eines Images (optional)
Um die Verwendung von unnötigem Speicherplatz zu vermeiden, werden wir das lokale Image entfernen.
Dazu müssen wir mit dem Befehl docker rm, gefolgt von der CONTAINER ID oder dem NAMEN, alle Container löschen, die auf das Image verweisen.
Im folgenden Beispiel verwenden wir die CONTAINER ID vom Befehl docker ps -a, den wir gerade ausgeführt haben.
Vergewissern Sie sich, dass Sie die ID Ihres Containers ersetzen:
Sobald alle Container, die auf das Image verweisen, entfernt wurden, können wir das Image entfernen:
Sie haben Docker Compose unter Debian 10 installiert, Ihre Installation getestet, indem Sie ein Hello World-Beispiel ausgeführt und das Testimage und den Container entfernt haben.
Während das Hallo World-Beispiel Ihre Installation bestätigt hat, zeigt diese grundlegende Konfiguration nicht einen der Hauptvorteile von Docker Compose - nämlich, dass damit eine Gruppe von Docker-Containern alle gleichzeitig hoch- und heruntergefahren werden können.
Nähere Details zur Verwendung von Docker Compose finden Sie unter So installieren Sie WordPress mit Docker Compose.
So installieren Sie Linux, Nginx, MySQL, PHP (LEMP) Stack unter CentOS 8 Schnellstart
3856
In diesem Tutorial installieren Sie einen LEMP-Stack auf einem CentOS 8-Server.
Eine detailliertere Version dieses Tutorials mit weiteren Erklärungen zu jedem Schritt finden Sie unter So installieren Sie Linux, Nginx, MySQL, PHP (LEMP) Stack unter CentOS 8.
Um diesem Leitfaden zu folgen, müssen Sie auf einen CentOS-8-Server als sudo-Benutzer zugreifen.
Schritt 1 - Installation von Nginx
Wenn firewalld aktiv ist, müssen Sie den folgenden Befehl ausführen, um den externen Zugriff auf Port 80 (HTTP) zu gestatten:
Schritt 2 - Installation von MariaDB
Wir installieren jetzt MariaDB, eine gemeinschaftlich entwickelte Kopie des Original-MySQL-Servers von Oracle.
Starten Sie das interaktive Skript mit:
Schritt 3 - Installation von PHP-FPM
Wir installieren nano, um das Bearbeiten dieser Dateien zu erleichtern:
Suchen Sie nach den Anweisungen user und group.
Vergewissern Sie sich, dass Sie beide Werte von apache auf nginx ändern:
Schritt 4 - Testen Sie PHP mit Nginx
Wir müssen lediglich den Standard-Eigentümer und die Gruppe im Nginx-Dokumentstamm ändern, damit Sie Dateien an diesem Speicherort mit Ihrem normalen Systembenutzer ohne Rootberechtigung erstellen und ändern können.
Kopieren Sie diesen Inhalt in Ihre info.php-Datei und vergessen Sie nicht, sie zu speichern, wenn Sie fertig sind.
So installieren Sie Tinc und richten ein einfaches VPN unter Ubuntu 18.04 ein
3338
Tinc ist ein Open-Source Virtual Private Network (VPN) -Daemon mit nützlichen Funktionen wie Verschlüsselung, optionaler Komprimierung und automatischem Mesh-Routing, der VPN-Verkehr opportunistisch direkt zwischen Servern routen kann.
Diese Funktionen unterscheiden tinc von anderen VPN-Lösungen und machen es zu einer guten Wahl für die Erstellung eines VPN aus vielen kleinen, geografisch verteilten Netzwerken.
In diesem Tutorial gehen wir darauf ein, wie Sie mit tinc ein sicheres VPN erstellen, in dem Ihre Server kommunizieren können, als ob sie sich in einem lokalen Netzwerk befinden würden.
Wir werden auch demonstrieren, wie man mit tinc einen sicheren Tunnel in einem privaten Netzwerk einrichtet.
Wir verwenden Ubuntu-18.04-Server, aber die Konfigurationen können für die Verwendung mit jedem anderen Betriebssystem angepasst werden.
Ziele
Um mehrere Anwendungsfälle abzudecken, beschreibt dieses Tutorial die Verbindung eines Client-Knotens mit dem VPN über eine private Netzwerkschnittstelle und eines anderen über eine öffentliche.
Sie können diese Einrichtung jedoch an Ihre eigenen Bedürfnisse anpassen.
Sie müssen nur planen, wie Ihre Server aufeinander zugreifen sollen, und die in diesem Tutorial vorgestellten Beispiele an Ihre eigenen Bedürfnisse anpassen.
Achten Sie darauf, die in den Beispielen hervorgehobenen Werte durch Ihre eigenen Werte zu ersetzen, wenn Sie eine Anpassung an Ihre eigene Einrichtung vornehmen.
Es könnte jedoch in Ihrem Interesse sein, zunächst dem Tutorial in der vorliegenden Form zu folgen, um sicherzustellen, dass Sie die beteiligten Komponenten und Prozesse verstehen, bevor Sie diese Anleitung modifizieren.
Um die Übersichtlichkeit zu wahren, wird in diesem Tutorial wie folgt auf die Server verwiesen:
server-01: Alle VPN-Knoten stellen eine Verbindung mit diesem Rechner her und die Verbindung muss für die einwandfreie VPN-Funktionalität aufrechterhalten werden.
Weitere Server können auf die gleiche Weise wie dieser konfiguriert werden, um, falls gewünscht, Redundanz bereitzustellen.
client-01: Verbindet sich mit dem VPN-Knoten von server-01 über dessen private Netzwerkschnittstelle
client-02: Verbindet sich mit dem VPN-Knoten von server-01 über dessen öffentliche Netzwerkschnittstelle
< $> note Anmerkung: Tinc selbst unterscheidet nicht zwischen Servern (Rechnern, die VPN-Dienste hosten und bereitstellen) und Clients (Rechnern, die sich mit dem sicheren privaten Netzwerk verbinden und es verwenden). Es kann jedoch hilfreich sein, die Funktionsweise von tinc zu verstehen und zu visualisieren, indem Sie sich Ihre Server so vorstellen.
Hier ist eine Darstellung des VPN, das wir einrichten möchten:
Tinc VPN-Einrichtung
Das blaue Feld steht für unser VPN und das rosa Feld für das zugrunde liegende private Netzwerk.
Alle drei Server können über das VPN kommunizieren, obwohl das private Netzwerk ansonsten für client-02 nicht zugänglich ist.
Wenn Sie diesem Tutorial genau folgen möchten, stellen Sie zwei Ubuntu-18.04-Server (server-01 und client-01) in demselben Datencenter bereit und aktivieren Sie auf jedem das private Netzwerk.
Erstellen Sie dann einen weiteren Ubuntu-18.04-Server (client-02) in einem separaten Datencenter.
Jeder Server sollte über einen administrativen Benutzer und eine mit ufw konfigurierte Firewall verfügen.
Um dies einzurichten, folgen Sie unserem Leitfaden für die Ersteinrichtung des Servers für Ubuntu 18.04.
Zusätzlich müssen wir später in diesem Tutorial einige Dateien zwischen den einzelnen Rechnern mit scp übertragen.
Aus diesem Grund müssen Sie auf jedem Ihrer Server SSH-Schlüssel generieren, die SSH-Schlüssel von client-01 und client-02 zu der Datei authorized _ keys von server-01 und dann den SSH-Schlüssel von server-01 zu den Dateien authorized _ keys von client-01 und client-02 hinzufügen.
Hilfe für das Einrichten finden Sie in unserem Leitfaden So richten Sie SSH-Schlüssel unter Ubuntu 18.04 ein.
Schritt 1 - Installieren von Tinc
Tinc ist aus den Standard-APT-Repositorys von Ubuntu verfügbar, d. h., wir können es mit nur wenigen Befehlen installieren.
Sofern Sie dies nicht kürzlich getan haben, führen Sie auf jedem Server den folgenden Befehl aus, um die jeweiligen Paketindizes zu aktualisieren:
Installieren Sie dann tinc auf jedem Server, indem Sie den folgenden Befehl ausführen:
Damit haben Sie tinc auf jedem Ihrer Server installiert.
Sie müssen jedoch einige Änderungen an der Konfiguration von tinc auf jedem Rechner vornehmen, damit Ihr VPN ausgeführt werden kann.
Beginnen wir mit der Aktualisierung von server-01.
Schritt 2 - Konfigurieren des Tinc-Servers
Tinc erfordert, dass jeder Rechner, der Teil des VPN sein wird, die folgenden drei Konfigurationskomponenten aufweist:
Tinc-Konfigurationsdateien: Es gibt drei verschiedene Dateien, die den tinc-Daemon konfigurieren:
tinc.conf, die den Netznamen, das Netzwerkgerät, über das VPN ausgeführt wird, und andere VPN-Optionen definiert;
tinc-up, ein Skript, das das in tinc.conf definierte Netzwerkgerät nach dem Start von tinc aktiviert;
tinc-down, die das Netzwerkgerät deaktiviert, wenn tinc stoppt.
Öffentliche / private Schlüsselpaare: Tinc verwendet öffentliche / private Schlüsselpaare, damit nur Benutzer mit gültigen Schlüsseln auf das VPN zugreifen können.
Host-Konfigurationsdateien: Jeder Rechner (oder Host) im VPN hat seine eigene Konfigurationsdatei, die die tatsächliche IP-Adresse des Hosts und das Subnetz, in dem tinc ihn bedient, enthält.
Tinc verwendet einen Netznamen, um ein tinc-VPN von einem anderen zu unterscheiden.
Dies ist hilfreich, wenn Sie mehrere VPNs einrichten möchten. Es wird jedoch empfohlen, einen Netznamen selbst dann zu verwenden, wenn Sie nur ein VPN konfigurieren wollen.
Sie können Ihrem VPN einen beliebigen Netznamen geben, aber der Einfachheit halber nennen wir unser VPN < ^ > netname < ^ >.
Erstellen Sie auf server-01 die Konfigurationsverzeichnisstruktur für das VPN:
Verwenden Sie Ihren bevorzugten Texteditor, um eine Datei tinc.conf zu erstellen.
Fügen Sie der leeren Datei die folgenden Zeilen hinzu.
Diese konfigurieren einen tinc-Knoten namens < ^ > server _ 01 < ^ > ​ ​ ​ mit einer Netzwerkschnittstelle namens tun0, die IPv4 verwenden wird:
< $> warning Warnung: Beachten Sie, dass der Wert nach der Anweisung Name einen Unterstrich (_) anstatt eines Bindestrichs (-) enthält.
Dies ist wichtig, da tinc verlangt, dass der Wert Name nur alphanumerische oder Unterstrich-Zeichen enthält.
Wenn Sie hier einen Bindestrich verwenden, werden Sie bei dem Versuch, das VPN später in diesem Leitfaden zu starten, auf einen Fehler stoßen.
Speichern und schließen Sie die Datei nach dem Hinzufügen dieser Zeilen.
Erstellen Sie als Nächstes eine Host-Konfigurationsdatei namens server _ 01 im Unterverzeichnis hosts.
Letztendlich werden die Client-Knoten diese Datei zur Kommunikation mit server-01 verwenden:
Auch hier ist zu beachten, dass der Name dieser Datei einen Unterstrich und keinen Bindestrich enthält.
Auf diese Weise entspricht er der Anweisung Name in der Datei tinc.conf, was es tinc ermöglicht, den öffentlichen RSA-Schlüssel des Servers automatisch an diese Datei anzuhängen, wenn wir die Datei später generieren.
Fügen Sie die folgenden Zeilen in die Datei ein und stellen Sie sicher, dass die öffentliche IP-Adresse von server-01 enthalten ist:
Das Feld Address gibt an, wie sich andere Knoten mit diesem Server verbinden werden, und Subnet gibt an, welches Subnetz dieser Daemon bedienen wird.
Erzeugen Sie als Nächstes mit dem folgenden Befehl ein Paar öffentlicher und privater RSA-Schlüssel für diesen Host:
Nach Ausführung dieses Befehls werden Sie dazu aufgefordert, die Dateinamen einzugeben, unter denen tinc die öffentlichen und privaten RSA-Schlüssel speichern wird:
Drücken Sie die EINGABETASTE, um bei jeder Eingabeaufforderung die Standardspeicherorte zu akzeptieren; dadurch wird tinc angewiesen, den privaten Schlüssel in einer Datei namens rsa _ key.priv zu speichern und den öffentlichen Schlüssel an die Host-Konfigurationsdatei server _ 01 anzuhängen.
Als Nächstes erstellen Sie tinc-up, das Skript, das bei jedem Start des VPNs < ^ > netname < ^ > ausgeführt wird:
Im Folgenden wird erläutert, was jede dieser Zeilen bewirkt:
ip link...: setzt den Status der virtuellen Netzwerkschnittstelle von tinc auf up
ip addr...: fügt der virtuellen Netzwerkschnittstelle von tinc die IP-Adresse 10.0.0.1 mit einer Netzmaske von 32 hinzu, wodurch die anderen Rechner im VPN die IP-Adressse von server-01 als 10.0.0.1 sehen.
ip route...: fügt eine Route (10.0.0.0 / 24) hinzu, die über die virtuelle Netzwerkschnittstelle von tinc erreicht werden kann.
Als Nächstes erstellen Sie ein Skript, um die virtuelle Netzwerkschnittstelle zu entfernen, wenn Ihr VPN gestoppt wird:
Diese Zeilen haben die entgegengesetzte Wirkung wie die des Skripts tinc-up:
ip route...: Löscht die Route 10.0.0.0 / 24
ip addr...: löscht die IP-Adresse 10.0.0.1 aus der virtuellen Netzwerkschnittstelle von tinc
ip link...: setzt den Status der virtuellen Netzwerkschnittstelle von tinc auf down
Speichern und schließen Sie die Datei und machen Sie dann diese beiden neuen Netzwerkskripte ausführbar:
Fügen Sie als letzten Schritt der Konfiguration von server-01 eine Firewall-Regel hinzu, die den Datenverkehr über Port 655, den Standardport von tinc, zulässt:
server-01 ist nun vollständig konfiguriert und Sie können mit der Einrichtung Ihrer Client-Knoten fortfahren.
Schritt 3 - Konfigurieren der Client-Knoten
Ihre beiden Client-Rechner benötigen eine etwas andere Konfiguration als der Server, obwohl der Prozess im Allgemeinen recht ähnlich ist.
Aufgrund der Einrichtung, die wir in diesem Leitfaden anstreben, werden wir client-01 und client-02 fast identisch, mit nur wenigen kleinen Unterschieden zwischen ihnen, konfigurieren.
Daher müssen viele der in diesem Schritt gegebenen Befehle auf beiden Rechnern ausgeführt werden.
Beachten Sie jedoch, dass, wenn client-01 oder client-02 einen bestimmten Befehl oder eine spezielle Konfiguration erfordern, diese Anweisungen in einem blauen bzw. roten Befehlsblock angezeigt werden.
Replizieren Sie sowohl auf client-01 als auch auf client-02 die auf server-01 erstellte Verzeichnisstruktur:
Erstellen Sie dann eine Datei tinc.conf:
Fügen Sie auf beiden Rechnern die folgenden Zeilen in die Datei ein:
Stellen Sie sicher, dass Sie < ^ > node _ name < ^ > durch den Namen des jeweiligen Client-Knotens ersetzen.
Auch hier ist darauf zu achten, dass dieser Name einen Unterstrich (_) und keinen Bindestrich verwendet.
Beachten Sie, dass diese Datei eine Anweisung ConnectTo enthält, die auf server _ 01 verweist, während die Datei tinc.conf von server-01 diese Anweisung nicht enthält.
Indem Sie keine Anweisung ConnectTo auf server-01 einbinden, bedeutet dies, dass server-01 nur auf eingehende Verbindungen lauscht.
Dies funktioniert für unsere Einrichtung, da es keine Verbindung mit anderen Rechnern herstellt.
Erstellen Sie als Nächstes auf jedem Client-Knoten eine Host-Konfigurationsdatei.
Achten Sie auch hier darauf, dass der Dateinamen mit einem Unterstrich statt eines Bindestrichs geschrieben wird:
Fügen Sie für client-01 diese Zeile hinzu:
Fügen Sie für client-02 diese Zeile hinzu:
Beachten Sie, dass jeder Client ein anderes Subnetz hat, das tinc bedienen wird.
Erzeugen Sie als Nächstes auf jedem Client-Rechner die Schlüsselpaare:
Wie auch bei server-01 drücken Sie bei der Aufforderung zur Dateiauswahl zum Speichern der RSA-Schlüssel die EINGABETASTE, um die Standardauswahl zu akzeptieren.
Erstellen Sie anschließend das Startskript für die Netzwerkschnittstelle auf jedem Client:
Fügen Sie für client-01 diese Zeilen hinzu:
Fügen Sie für client-02 Folgendes hinzu:
Speichern und schließen Sie jede Datei.
Erstellen Sie als Nächstes auf jedem Client das Stoppskript für die Netzwerkschnittstelle:
Fügen Sie auf client-01 den folgenden Inhalt in die leere Datei ein:
Fügen Sie auf client-02 Folgendes hinzu:
Speichern und schließen Sie die Dateien.
Machen Sie Netzwerkskripte ausführbar, indem Sie auf jedem Client-Rechner den folgenden Befehl ausführen:
Öffnen Sie zum Schluss Port 655 auf jedem Client:
Zu diesem Zeitpunkt sind die Client-Knoten fast, wenn auch nicht ganz, eingerichtet.
Sie benötigen noch den öffentlichen Schlüssel, den wir im vorherigen Schritt auf server-01 erstellt haben, um die Verbindung zum VPN zu authentifizieren.
Schritt 4 - Verteilen der Schlüssel
Jeder Knoten, der direkt mit einem anderen Knoten kommunizieren möchte, muss öffentliche Schlüssel ausgetauscht haben, die sich innerhalb der Host-Konfigurationsdateien befinden.
In unserem Fall muss server-01 öffentliche Schlüssel mit den anderen Knoten austauschen.
Austauschen von Schlüsseln zwischen server-01 und client-01
Kopieren Sie auf client-01 seine Host-Konfigurationsdatei auf server-01.
Da sich sowohl client-01 als auch server-01 im selben Datencenter befinden und beide über ein aktiviertes privates Netzwerk verfügen, können Sie hier die private IP-Adresse von server01 verwenden:
Kopieren Sie dann auf server-01 die Host-Konfigurationsdatei von client-01 in das Verzeichnis / etc / tinc / < ^ > netname < ^ > / hosts /:
Kopieren Sie dann, während Sie sich noch auf server-01 befinden, dessen Host-Konfigurationsdatei nach client-01:
Kopieren Sie auf client-01 die Datei von server-01 an den entsprechenden Ort:
Bearbeiten Sie auf client-01 die Host-Konfigurationsdatei von server-01, damit das Feld Address auf die private IP-Adresse von server-01 gesetzt wird.
Auf diese Weise verbindet sich client-01 über das private Netzwerk mit dem VPN:
Ändern Sie die Anweisung Address so, dass sie auf die private IP-Adresse von server-01 verweist:
Speichern und beenden.
Gehen wir jetzt zu unserem verbleibenden Knoten, client-02.
Austauschen von Schlüsseln zwischen server-01 und client-02
Kopieren Sie auf client-02 seine Host-Konfigurationsdatei auf server-01:
Kopieren Sie dann auf server-01 die Host-Konfigurationsdatei von client _ 02 an den entsprechenden Ort:
Kopieren Sie dann die Host-Konfigurationsdatei von server-01 nach client-02:
Kopieren Sie auf client-02 die Datei von server-01 an den entsprechenden Ort:
Angenommen, Sie richten nur zwei Client-Knoten ein, dann sind Sie mit der Verteilung der öffentlichen Schlüssel fertig.
Wenn Sie jedoch ein größeres VPN erstellen, ist jetzt ein guter Zeitpunkt, um die Schlüssel zwischen diesen anderen Knoten auszutauschen.
Denken Sie daran, dass, wenn zwei Knoten direkt miteinander kommunizieren sollen (ohne einen Weiterleitungsserver dazwischen), sie ihre Schlüssel / Host-Konfigurationsdateien ausgetauscht haben müssen und auf die realen Netzwerkschnittstellen des jeweils anderen zugreifen können müssen.
Es ist außerdem in Ordnung, die Konfigurationsdatei jedes Hosts einfach auf jeden Knoten im VPN zu kopieren.
Schritt 5 - Testen der Konfiguration
Starten Sie tinc auf jedem Knoten, beginnend mit server-01, mit dem folgenden Befehl:
Dieser Befehl enthält das Flag -n, das auf den Netznamen für unser VPN < ^ > netname < ^ > verweist.
Dies ist nützlich, wenn Sie mehr als ein VPN eingerichtet haben und Sie angeben müssen, welches Sie starten möchten.
Es enthält auch das Flag -D, das die Gabelung und Trennung von tinc verhindert sowie den automatischen Neustartmechanismus von tinc deaktiviert.
Schließlich enthält es das Flag -d, das tinc anweist, im Debug-Modus mit einem Debug-Level von 3 zu laufen.
< $> note Anmerkung: Wenn es um den tinc-Daemon geht, zeigt ein Debug-Level von 3 jede zwischen zwei beliebigen Servern ausgetauschte Anfrage, einschließlich Authentifizierungsanforderungen, Schlüsselaustausch und Aktualisierungen von Verbindungslisten.
Höhere Debug-Level zeigen mehr Informationen über den Netzwerkverkehr an, aber im Moment geht es nur darum, ob die Knoten miteinander kommunizieren können. Daher ist ein Level von 3 ausreichend.
In einem Produktivszenario würden Sie jedoch zu einem niedrigeren Debug-Level wechseln wollen, um die Festplatten nicht mit Protokolldateien zu füllen.
Sie können mehr über die Debug-Level von tinc erfahren, indem Sie die offizielle Dokumentation durchsehen.
Nachdem der Daemon auf jedem Knoten gestartet wurde, sollten Sie eine Ausgabe mit den Namen der einzelnen Knoten sehen, wenn sie sich mit server-01 verbinden.
Testen wir jetzt die Verbindung über das VPN.
Pingen Sie in einem separaten Fenster auf client-02 die VPN IP-Adresse von client-01 an.
Dieser haben wir zuvor 10.0.0.2 zugewiesen:
Der Ping sollte korrekt funktionieren und Sie sollten in den anderen Fenstern einige Debug-Ausgaben über die Verbindung im VPN sehen.
Dies zeigt an, dass client-02 über das VPN über server-01 mit client-01 kommunizieren kann.
Drücken Sie STRG + C, um den Ping zu beenden.
Sie können die VPN-Schnittstellen auch für jede andere Netzwerkkommunikation verwenden, z. B. für Anwendungsverbindungen, das Kopieren von Dateien und SSH.
Beenden Sie in jedem Debug-Fenster des tinc-Daemons den Daemon durch drücken von STRG +\.
Schritt 6 - Konfigurieren von Tinc zum Starten beim Booten
Ubuntu-Server verwenden systemd als standardmäßigen Systemmangager, um das Starten und Ausführen von Prozessen zu steuern.
Aus diesem Grund können wir das VPN < ^ > netname < ^ > so aktivieren, dass es beim Booten mit einem einzigen systemctl-Befehl automatisch gestartet wird.
Führen Sie den folgenden Befehl auf jedem Knoten aus, um das tinc-VPN so einzustellen, dass es bei jedem Booten des Rechners startet:
Tinc ist so konfiguriert, dass es auf jedem Rechner beim Booten startet, und Sie können es mit dem Befehl systemctl steuern.
Wenn Sie es jetzt starten möchten, führen Sie den folgenden Befehl auf jedem Ihrer Knoten aus:
< $> note Anmerkung: Wenn Sie mehrere VPN haben, aktivieren oder starten jedes sofort, so wie hier:
Damit ist Ihr tinc-VPN vollständig konfiguriert und wird auf jedem Ihrer Knoten ausgeführt.
Nachdem Sie dieses Tutorial durchgearbeitet haben, sollten Sie nun eine gute Grundlage haben, um Ihr VPN entsprechend Ihren Bedürfnissen auszubauen.
Tinc ist sehr flexibel und jeder Knoten kann so konfiguriert werden, dass er sich mit jedem anderen Knoten (auf den er über das Netzwerk zugreifen kann) verbindet, sodass er als Mesh-VPN fungieren kann, ohne sich auf einen einzelnen Knoten zu verlassen.
So installieren Sie WordPress mit OpenLiteSpeed unter Ubuntu 18.04
3273
WordPress ist ein Open-Source-Content-Management-System (CMS).
Als weltweit beliebtetes CMS ermöglicht WordPress die Einrichtung von Blogs und Websites auf einem MySQL-Datenbank-Backend, wobei PHP zur Ausführung von Skripten und zur Verarbeitung dynamischer Inhalte verwendet wird.
OpenLiteSpeed ist ein optimierter Open-Source-Webserver, den Sie zur Verwaltung und Bereitstellung von Websites verwenden können.
OpenLiteSpeed verfügt über einige nützliche Funktionen, die ihn zu einer soliden Wahl für viele Installationen machen: Apache-kompatible Neuschreibregeln, eine integrierte webbasierte Verwaltungsoberfläche und eine für den Server optimierte PHP-Verarbeitung.
Dieser Leitfaden führt durch den Prozess der Installation und Einrichtung einer WordPress-Instanz unter Ubuntu 18.04 mit dem OpenLiteSpeed-Webserver.
Da sowohl WordPress als auch OpenLiteSpeed über einen Webbrowser verwaltet werden können, ist diese Konfiguration ideal für diejenigen, die keinen regulären Zugriff zu einer SSH-Sitzung haben oder sich nicht wohlfühlen, einen Webserver über die Befehlszeile zu verwalten.
Einen Server, auf dem Ubuntu 18.04 mit einem administrativen Benutzer ohne Root-Berechtigung und einer mit ufw konfigurierten Firewall ausgeführt wird.
Zum Einrichten dieser Umgebung folgen Sie unserem Tutorial Ersteinrichtung des Servers für Ubuntu 18.04.
Auf Ihrem Server installiertes OpenLiteSpeed.
Anweisungen zur Installation und Konfiguration von OpenLiteSpeed finden Sie in unserem Leitfaden So installieren Sie den OpenLiteSpeed-Webserver unter Ubuntu 18.04.
Auf Ihrem Server installiertes MySQL.
Folgen Sie für die Einrichtung unserem Tutorial So installieren Sie MySQL unter Ubuntu 18.04.
Schritt 1 - Erstellen einer Datenbank und eines Datenbankbenutzers für WordPress
Sie haben MySQL bereits installiert, aber als vorbereitenden Schritt müssen Sie eine Datenbank und einen Benutzer für WordPress einrichten.
Stellen Sie zu Beginn eine Verbindung mit Ihrem Server unter Verwendung von SSH her:
Melden Sie sich dann an dem MySQL Root-Konto an:
< $> note Anmerkung: Wenn Sie Schritt 3 im vorbereitenden MySQL-Tutorial abgeschlossen und Ihren MySQL-Benutzer root konfiguriert haben, um sich mit dem Plugin mysql _ native _ password zu authentifizieren, müssen Sie sich mit dem folgenden Befehl anmelden:
Geben Sie das Passwort Ihres Root-Benutzers ein, wenn Sie dazu aufgefordert werden.
Erstellen Sie von der MySQL-Eingabeaufforderung aus eine Datenbank mit dem folgenden Befehl.
Hier nennen wir diese Datenbank der Einfachheit halber < ^ > wordpress < ^ >, aber Sie können ihr einen beliebigen Namen geben.
Erstellen Sie dann einen Benutzer und erteilen Sie ihm Berechtigungen über die von Ihnen gerade erstellte Datenbank.
Auch hier können Sie diesem Benutzer einen beliebigen Namen geben, aber der Einfachheit halber nennen wir ihn < ^ > wordpressuser < ^ >.
Denken Sie daran, < ^ > password < ^ > in ein starkes Passwort Ihrer Wahl zu ändern:
Danach können Sie die MySQL-Eingabeaufforderung schließen:
Sie haben die Einrichtung Ihrer MySQL-Installation für die Arbeit mit WordPress abgeschlossen.
Als Nächstes installieren wir einige PHP-Erweiterungen.
Schritt 2 - Installieren zusätzlicher PHP-Erweiterungen
In dem vorbereitenden Tutorial OpenLiteSpeed haben Sie das Paket lsphp73 installiert.
Dies ist eine für OpenLiteSpeed optimierte Zusammenstellung von PHP, die die LiteSpeed-SAPI zur Kommunikation mit externen Anwendungen nutzt.
Je nach Ihren Bedürfnissen kann WordPress andere bestimmte PHP-Erweiterungen benötigen, um wie gewünscht zu funktionieren.
Führen Sie den folgenden Befehl aus, um einige mit WordPress häufig verwendete PHP-Erweiterungen zu installieren:
< $> note Anmerkung: Die in diesem Befehl enthaltenen Pakete können nicht jeden Anwendungsfall abdecken.
Eine vollständige Liste der in dem LiteSpeed-Repository, das Sie in dem vorbereitenden Tutorial zu Ihrem Server hinzugefügt haben, verfügbaren PHP 7.3-Erweiterungen, finden Sie im LiteSpeed Wiki.
Danach können Sie mit dem Herunterladen und Einrichten von WordPress auf Ihrem Server fortfahren.
Schritt 3 - Herunterladen von WordPress
Nachdem Ihre Server-Software konfiguriert ist, können Sie WordPress installieren und einrichten.
Insbesondere aus Sicherheitsgründen ist es immer empfehlenswert, die neueste Version von WordPress direkt von ihrer Website zu beziehen.
Wir werden diese Dateien vorübergehend in unser Stammverzeichnis verschieben, doch zuerst erstellen wir einige Dateien und Verzeichnisse, von denen die Installation von WordPress abhängig ist.
OpenLiteSpeed unterstützt die Dateien .htaccess.
Dies ist für unsere Zwecke wichtig, da WordPress die Dateien .htaccess zur Erstellung und Verwaltung von Permalinks verwendet.
Fügen Sie eine Dummy-Datei .htaccess hinzu, damit sie später von WordPress verwendet werden kann:
Als Nächstes kopieren Sie die Beispielkonfigurationsdatei in den Dateinamen, den WordPress tatsächlich liest:
Erstellen Sie zusätzlich das Verzeichnis upgrade, damit Wordpress keine Probleme mit Berechtigungen hat, wenn es versucht, dies nach einer Aktualisierung seiner Software selbstständig zu tun:
Kopieren Sie dann den gesamten Inhalt des Verzeichnisses in Ihr Dokumenten-Stammverzeichnis.
OpenLiteSpeed verfügt standardmäßig über einen virtuellen Host namens Example in dem Verzeichnis / usr / local / lsws /.
Das Dokumenten-Stammverzeichnis für den virtuellen Host Example ist das Unterverzeichnis html:
Beachten Sie, dass dieser Befehl am Ende des Quellverzeichnisses einen Punkt enthält, um anzugeben, dass alles im Verzeichnis kopiert werden soll, einschließlich versteckter Dateien (wie die von Ihnen erstellte Datei .htaccess):
Damit haben Sie WordPress erfolgreich auf Ihrem Webserver installiert und einige der ersten Konfigurationsschritte ausgeführt.
Als Nächstes werden wir einige weitere Konfigurationsänderungen vornehmen, die WordPress die zur sicheren Funktion und zum Zugriff auf die zuvor erstellte MySQL-Datenbank und das Benutzerkonto erforderlichen Berechtigungen erteilen.
Schritt 4 - Konfigurieren des WordPress-Verzeichnisses
Bevor wir den webbasierten Einrichtungsvorgang für WordPress durchlaufen können, müssen wir einige Einträge in unserem WordPress-Verzeichnis anpassen.
Beginnen Sie damit, die Eigentümerschaft an allen Dateien im Verzeichnis an den Benutzer nobody und die Gruppe nogroup zu übergeben, die der OpenLiteSpeed-Webserver standardmäßig ausführt.
Der folgende Befehl chown gewährt OpenLiteSpeed die Fähigkeit zum Lesen und Schreiben von Dateien im Verzeichnis wordpress, sodass es die Website bedienen und automatisch Aktualisierungen durchführen kann:
Führen Sie als Nächstes zwei find-Befehle aus, um die richtigen Berechtigungen für die WordPress-Verzeichnisse und -Dateien festzulegen:
Diese sollten von Anfang an ein sinnvoller Berechtigungssatz sein, obwohl einige Plugins und Verfahren möglicherweise zusätzliche Anpassungen erfordern.
Anschließend müssen Sie einige Änderungen an der Hauptkonfigurationsdatei von WordPress vornehmen.
Wenn Sie die Datei öffnen, ist Ihr erstes Ziel die Anpassung einiger geheimer Schlüssel, um eine gewisse Sicherheit für Ihre Installation zu gewährleisten.
Sie werden nur intern verwendet, d. h. komplexe, sichere Werte haben keine negativen Auswirkungen auf die Benutzerfreundlichkeit.
Dies sind Konfigurationszeilen, die Sie direkt in Ihre Konfigurationsdatei einfügen, um sichere Schlüssel festzulegen.
Kopieren Sie die erhaltene Ausgabe in die Zwischenablage und öffnen Sie dann die WordPress-Konfigurationsdatei, die sich im Stammverzeichnis Ihres Dokuments befindet:
Als Nächstes ändern Sie die Einstellungen für die Datenbankverbindung im oberen Abschnitt der Datei.
Sie müssen den Datenbanknamen, den Datenbankbenutzer und das zugehörige Passwort, das Sie in MySQL konfiguriert haben, anpassen.
Die andere Änderung, die Sie vornehmen müssen, ist die Einstellung der Methode, die WordPress zum Schreiben in das Dateisystem verwenden soll.
Da wir dem Webserver die Berechtigung erteilt haben, an den erforderlichen Stellen zu schreiben, können wir die Dateisystemmethode ausdrücklich auf direct setzen.
Sollte dies nicht mit unseren aktuellen Einstellungen geschehen, würde WordPress bei bestimmten Aktionen nach FTP-Zugangsdaten fragen.
Zu diesem Zeitpunkt ist WordPress auf Ihrem System noch nicht vollständig konfiguriert, da Sie noch einige letzte Handgriffe vornehmen müssen, bevor Sie mit der Veröffentlichung von Inhalten beginnen können.
Dafür müssen Sie jedoch zunächst einige Konfigurationsänderungen an Ihrer OpenLiteSpeed-Installation vornehmen.
Schritt 6 - Konfigurieren von OpenLiteSpeed
Derzeit haben Sie WordPress auf Ihrem Ubuntu-Server installiert, aber Ihre OpenLiteSpeed-Installation wurde noch nicht für die Bedienung konfiguriert. In diesem Schritt greifen wir auf die Verwaltungsoberfläche von OpenLiteSpeed zu und nehmen einige Änderungen an der Konfiguration Ihres Servers vor.
Navigieren Sie in Ihrem bevorzugten Webbrowser zur Verwaltungsoberfläche von OpenLiteSpeed.
Sie finden diese, indem Sie die öffentliche IP-Adresse Ihres Servers oder den damit verbundenen Domänennamen, gefolgt von: 7080, in die Adressleiste Ihres Browsers eingeben:
Dort wird Ihnen ein Anmeldebildschirm angezeigt.
Geben Sie den Benutzernamen und das Passwort ein, das Sie in dem vorbereitenden Tutorial zur OpenLiteSpeed-Installation definiert haben:
OpenLiteSpeed Anmeldebildschirm
Suchen Sie in der OpenLiteSpeed-Konsole in der linken Seitenleiste nach Server Configuration und klicken Sie darauf.
Navigieren Sie dann zu der Registerkarte External App, suchen Sie die Zeile der LiteSpeed SAPI App und klicken Sie auf die zugehörige Schaltfläche Bearbeiten:
Server-Konfigurationsseite
Erinnern Sie sich daran, dass Sie in dem vorbereitenden OpenLiteSpeed-Tutorial das Paket lsphp73 installiert haben, eine Kompilierung von PHP, die für die Arbeit mit OpenLiteSpeed über die LiteSpeed-SAPI optimiert ist.
Die Standardeinstellungen auf der Seite External App verweisen jedoch auf lsphp und nicht lsphp73.
Aus diesem Grund ist Ihre OpenLiteSpeed-Installation nicht in der Lage, PHP-Skripte korrekt auszuführen.
Um dies zu korrigieren, ändern Sie das Feld Name auf lsphp73, ändern Sie das Feld Address auf uds: / / tmp / lshttpd / lsphp73.sock und ändern Sie das Feld Command auf $SERVER _ ROOT / lsphp73 / bin / lsphp:
Änderungen an External App
Klicken Sie auf das Symbol Speichern in der oberen rechten Ecke des LiteSpeed SAPI App-Fensters, nachdem Sie diese Änderungen vorgenommen haben.
Klicken Sie anschließend im linken Menü auf Virtual Hosts.
Suchen Sie auf der Seite Virtual Hosts den virtuellen Host, den Sie verwenden möchten, und klicken Sie auf das Symbol Anzeige.
Hier verwenden wir den virtuellen Standardhost Example:
Seite Virtual Hosts
Navigieren Sie zu der Registerkarte General des virtuellen Hosts. Finden Sie dort den Abschnitt General und klicken Sie auf die zugehörige Schaltfläche Bearbeiten:
Virtual Hosts Registerkarte General
OpenLiteSpeed prüft den Inhalt des Feldes Document Root auf den zu bedienenden Inhalt.
Da alle Ihre WordPress-Inhalte und -Dateien in dem zuvor erstellten Verzeichnis wordpress gespeichert sind, aktualisieren Sie das Feld Document Root, um auf dieses Verzeichnis zu verweisen.
Dazu müssen Sie lediglich wordpress / an das Ende des Standardwerts anhängen:
Virtual Hosts Änderungen an General
Klicken Sie zum Speichern dieser Änderung auf das Symbol Speichern.
Nun müssen Sie die Dateien index.php aktivieren, damit sie zur Bearbeitung von Anfragen verwendet werden können, die nicht von statischen Dateien bearbeitet werden.
Dies ermöglicht die korrekte Funktionsweise der Hauptlogik von WordPress.
Blättern Sie, während Sie sich noch auf der Registerkarte General befinden, nach unten, um den Abschnitt Index Files zu finden, und klicken Sie auf das Symbol Bearbeiten:
Virtual Hosts Seite Index Files
Stellen Sie in dem Feld Index Files index.html index.php voran.
Indem Sie index.php vor index.html setzen, erlauben Sie PHP-Indexdateien den Vorrang.
Nach der Aktualisierung dieses Feldes sieht es wie folgt aus:
Virtual Hosts geänderte Index Files
Achten Sie darauf, vor dem Fortfahren auf das Symbol Speichern zu klicken.
Navigieren Sie anschließend zu der Registerkarte Rewrite des virtuellen Hosts. Finden Sie den Abschnitt Rewrite Control und drücken Sie die Schaltfläche Bearbeiten:
Virtual Hosts Seite Rewrite
Setzen Sie sowohl die Optionen Enable Rewrite als auch Auto Load from .htaccess auf Yes, indem Sie die jeweiligen Optionsfelder anklicken.
Wenn Sie die Rewrite-Anweisungen auf diese Weise konfigurieren, können Sie innerhalb Ihrer WordPress-Installation Permalinks verwenden:
Virtual Hosts Änderungen an Rewrite
Klicken Sie nach der Änderung auf das Symbol Speichern.
Der in der OpenLiteSpeed-Installation enthaltene virtuelle Standardhost enthält einige passwortgeschützte Bereiche, um die Benutzerauthentifizierungsfunktionen von OpenLiteSpeed zu präsentieren.
WordPress enthält seine eigenen Authentifizierungsmechanismen und wir werden die in OpenLiteSpeed enthaltene dateibasierte Authentifizierung nicht verwenden.
Wir sollten diese entfernen, um die auf unserer WordPress-Installation aktiven verstreuten Konfigurationsfragmente zu minimieren.
Klicken Sie zuerst auf die Registerkarte Security und anschließend auf die Schaltfläche Löschen neben SampleProtectedArea in der Tabelle Realms List:
OpenLiteSpeed Security Realm List
Sie werden aufgefordert, die Löschung zu bestätigen.
Klicken Sie zum Fortfahren auf Löschen.
Klicken Sie anschließend auf die Registerkarte Context. Löschen Sie in der Context List den Inhalt / protected /, der mit dem soeben gelöschten Sicherheitsbereich verbunden war:
OpenLiteSpeed - Löschen des geschützten Kontexts
Auch hier müssen Sie die Löschung durch Klicken auf Löschen bestätigen.
Mit derselben Technik können Sie auch alle oder einen Teil der anderen Kontexte sicher löschen, da wir sie nicht benötigen.
Wir haben speziell den Kontext / protected / gelöscht, weil sonst ein Fehler durch die Löschung des zugehörigen Sicherheitsbereichs (den wir gerade auf der Registerkarte Security entfernt haben) entstehen würde.
Klicken Sie anschließend auf das grüne Symbol Graceful Restart in der oberen rechten Ecke der OpenLiteSpeed-Konsole.
Dadurch wird der OpenLiteSpeed-Server neu gestartet, sodass die von Ihnen vorgenommenen Änderungen angewendet werden:
Position des Symbols Graceful Restart
Damit ist Ihr OpenLiteSpeed-Server vollständig konfiguriert.
Sie sind jetzt zum Abschluss der Einrichtung von WordPress in Ihrem Browser bereit.
Schritt 7 - Abschließen der Installation über die WordPress-Oberfläche
Wenn Sie bereit sind, klicken Sie auf die Schaltfläche Install WordPress.
Sie werden zu einer Seite weitergeleitet, die Sie zur Anmeldung auffordert:
Vom Dashboard aus können Sie Änderungen am Thema Ihrer Website vornehmen und Inhalte veröffentlichen.
Durch die Ausführung dieses Leitfadens haben Sie eine WordPress-Instanz auf einem Ubuntu 18.04-Server, auf dem OpenLiteSpeed ausgeführt wird, installiert und konfiguriert.
Einige häufige nächste Schritte sind die Auswahl der Permalink-Einstellungen für Ihre Posts (diese sind unter Einstellungen > Permalinks zu finden) oder die Auswahl eines neuen Designs (in Darstellung > Design).
Um die Sicherheit Ihrer neuen WordPress-Website zu erhöhen, empfehlen wir Ihnen, sie für die Funktion mit SSL zu konfigurieren, damit sie Inhalte über HTTPS bereitstellen kann.
Schauen Sie sich dieses Tutorial aus der OpenLiteSpeed-Dokumentation an, um LetsEncrypt zu installieren und einzurichten.
So optimieren Sie MySQL-Abfragen mit ProxySQL-Caching unter Ubuntu 16.04
3339
Der Autor wählte die Free Software Foundation, um eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
ProxySQL ist ein SQL-fähiger Proxy-Server, der zwischen Ihrer Anwendung und Ihrer Datenbank positioniert werden kann.
Er bietet viele Funktionen, wie z. B. Lastverteilung zwischen mehreren MySQL-Servern, und dient als Caching-Layer für Abfragen.
Dieses Tutorial konzentriert sich auf die Cache-Funktion von ProxySQL und wie es Abfragen für Ihre MySQL-Datenbank optimieren kann.
MySQL-Caching tritt bei der Speicherung des Ergebnisses einer Abfrage auf, sodass bei wiederholter Abfrage das Ergebnis zurückgegeben werden kann, ohne dass die Datenbank durchsucht werden muss.
Dies kann die Geschwindigkeit gewöhnlicher Abfragen deutlich erhöhen.
Bei vielen Caching-Methoden müssen die Entwickler jedoch den Code ihrer Anwendung modifizieren, was einen Fehler in die Codebase einbringen könnte.
Um diese fehleranfällige Praxis zu vermeiden, erlaubt ProxySQL die Einrichtung eines transparenten Cachings.
Bei dem transparenten Caching müssen nur Datenbankadministratoren die ProxySQL-Konfiguration ändern, um das Caching für die gängigsten Abfragen zu aktivieren, und diese Änderungen können über die ProxySQL-Verwaltungsschnittstelle vorgenommen werden.
Der Entwickler muss sich nur mit dem protokollfähigen Proxy verbinden und der Proxy wird entscheiden, ob die Abfrage aus dem Cache bedient werden kann, ohne im Back-End-Server zu suchen.
In diesem Tutorial werden Sie ProxySQL verwenden, um transparentes Caching für einen MySQL-Server unter Ubuntu 16.04. einzurichten.
Anschließend werden Sie seine Leistung unter Verwendung von mysqlslap mit und ohne Caching testen, um die Wirkung des Caching zu demonstrieren und zu zeigen, wie viel Zeit es bei der Ausführung vieler ähnlicher Abfragen sparen kann.
Einen Ubuntu-16.04-Server mit mindestens 2 GB RAM, der mit einem Benutzer ohne Root- und mit sudo-Berechtigungen und einer Firewall eingerichtet ist, wie in unserem Leitfaden Ersteinrichtung des Servers für Ubuntu 16.04 beschrieben.
Schritt 1 - Installieren und Einrichten des MySQL-Servers
Zuerst werden Sie MySQL-Server installieren und so konfigurieren, dass es ProxySQL als Backend-Server zur Bedienung von Client-Anfragen verwendet.
Unter Ubuntu 16.04 kann mysql-server mit diesem Befehl installiert werden:
Drücken Sie Y, um die Installation zu bestätigen.
Sie werden dann zur Eingabe Ihres MySQL Root-Benutzer-Passwortes aufgefordert.
Geben Sie ein starkes Passwort ein und speichern Sie es zur späteren Verwendung.
Nachdem Ihr MySQL-Server nun bereit ist, konfigurieren Sie ihn so, dass ProxySQL korrekt zu arbeitet.
Sie müssen einen Benutzer monitor für ProxySQL hinzufügen, um den MySQL-Server zu überwachen, da ProxySQL über das SQL-Protokoll auf den Backend-Server lauscht, anstatt eine TCP-Verbindung oder HTTP-GET-Anforderungen zu benutzen, um sicherzustellen, dass das Backend läuft. Um festzustellen, ob der Server noch aktiv ist, verwendet monitor eine Dummy-SQL-Verbindung.
Melden Sie sich zuerst bei der MySQL-Shell an:
-uroot meldet Sie unter Verwendung des MySQL Root-Benutzers an und -p fordert zur Eingabe des Passworts des Root-Benutzers auf.
Dieser Root-Benutzer unterscheidet sich von dem Root-Benutzer Ihres Servers, und das Passwort ist dasjenige, das Sie bei der Installation des Pakets mysql-server eingegeben haben.
Geben Sie das Root-Passwort ein und drücken Sie die EINGABETASTE.
Nun erstellen Sie zwei Benutzer, einen namens monitor für ProxySQL und einen weiteren, den Sie zum Ausführen von Client-Abfragen und zur Gewährung der richtigen Berechtigungen verwenden.
In diesem Tutorial wird dieser Benutzer sammy genannt.
Erstellen Sie den Benutzer monitor:
Die Abfrage CREATE USER wird verwendet, um einen neuen Benutzer zu erstellen, der sich von bestimmten IPs aus verbinden kann.
Die Verwendung von% bedeutet, dass der Benutzer eine Verbindung von einer beliebigen IP-Adresse aus herstellen kann.
IDENTIFIED BY legt das Passwort für den neuen Benutzer fest; geben Sie ein beliebiges Passwort ein, aber merken Sie es sich für die spätere Verwendung.
Nachdem der Benutzer monitor erstellt ist, erstellen Sie als Nächstes den Benutzer sammy:
Als Nächstes gewähren Sie Ihren neuen Benutzern Berechtigungen.
Führen Sie den folgenden Befehl aus, um monitor zu konfigurieren:
Die Abfrage GRANT wird verwendet, um Benutzern Berechtigungen zu erteilen.
Hier haben Sie dem Benutzer monitor nur SELECT auf alle Tabellen in der Datenbank sys gewährt; er benötigt diese Berechtigung nur, um auf den Back-End-Server zu lauschen.
Gewähren Sie nun dem Benutzer sammy alle Berechtigungen für alle Datenbanken:
Dies ermöglicht sammy, die notwendigen Abfragen durchzuführen, um Ihre Datenbank später zu testen.
Wenden Sie die Änderungen der Berechtigungen an, indem Sie Folgendes ausführen:
Beenden Sie im Anschluss die mysql-Shell:
Sie haben nun mysql-server installiert und einen Benutzer zur Verwendung durch ProxySQL zur Überwachung Ihres MySQL-Servers und einen weiteren zur Ausführung von Client-Abfragen erstellt.
Als Nächstes installieren und konfigurieren Sie ProxySQL.
Schritt 2 - Installieren und Konfigurieren von ProxySQL
Jetzt können Sie den ProxySQL-Server installieren, der als Caching-Layer für Ihre Abfragen verwendet wird.
Ein Caching-Layer existiert als Zwischenstation zwischen Ihren Anwendungsservern und den Back-End-Servern der Datenbank. Er wird verwendet, um eine Verbindung zur Datenbank herzustellen und die Ergebnisse einiger Abfragen für den schnellen Zugriff zu einem späteren Zeitpunkt zu speichern.
Die Github-Seite ProxySQL Releases bietet Installationsdateien für gängige Linux-Distributionen.
Für dieses Tutorial verwenden Sie wget, um die ProxySQL Version 2.0.4 Debian-Installationsdatei herunterzuladen:
Installieren Sie als Nächstes das Paket unter Verwendung von dpkg:
Nach der Installation starten Sie ProxySQL mit diesem Befehl:
Ob ProxySQL korrekt gestartet ist, können Sie mit diesem Befehl überprüfen:
Jetzt ist es an der Zeit, Ihren ProxySQL-Server mit dem MySQL-Server zu verbinden.
Verwenden Sie hierzu die ProxySQL Admin-SQL-Schnittstelle, die standardmäßig Port 6032 auf localhost überwacht und als Benutzernamen und Passwort admin hat.
Stellen Sie die Verbindung mit der Schnittstelle her, indem Sie Folgendes ausführen:
Geben Sie admin ein, wenn Sie zur Eingabe des Passworts aufgefordert werden.
-uadmin legt den Benutzernamen als admin fest und das Flag -h gibt den Host als localhost an.
Der Port ist 6032, der mit dem Flag -P angegeben wird.
Hier mussten Sie den Host und den Port explizit angeben, da der MySQL-Client standardmäßig eine Verbindung über eine lokale Socket-Datei und Port 3306 herstellt.
Nachdem Sie nun als admin an der mysql-Shell angemeldet sind, konfigurieren Sie den Benutzer monitor, damit ProxySQL ihn verwenden kann. Verwenden Sie zuerst Standard-SQL-Abfragen, um die Werte von zwei globalen Variablen festzulegen:
Die Variable mysql-monitor _ username gibt den MySQL-Benutzernamen an, mit dem geprüft wird, ob der Backend-Server aktiv ist oder nicht.
Die Variable mysql-monitor _ password zeigt auf das Passwort, das bei der Verbindung mit dem Backend-Server verwendet wird.
Verwenden Sie das Passwort, das Sie für den Benutzernamen monitor erstellt haben.
Jedes Mal, wenn Sie eine Änderung in der ProxySQL Admin-Schnittstelle erstellen, müssen Sie den richtigen LOAD-Befehl verwenden, um Änderungen an der laufenden ProxySQL-Instanz vorzunehmen.
Sie haben die globalen MySQL-Variablen geändert. Laden Sie sie also in RUNTIME, um die Änderungen anzuwenden:
Führen Sie als Nächstes SAVE für die Änderungen an der Datenbank auf der Festplatte aus, um die Änderungen zwischen Neustarts persistent zu speichern.
ProxySQL verwendet seine eigene lokale SQLite-Datenbank, um seine eigenen Tabellen und Variablen zu speichern:
Nun werden Sie ProxySQL über den Backend-Server informieren.
Die Tabelle mysql _ servers enthält Informationen über jeden Backend-Server, mit dem ProxySQL eine Verbindung herstellen und Abfragen ausführen kann. Fügen Sie also einen neuen Datensatz mit einer Standard-SQL-INSERT-Anweisung mit den folgenden Werten für hostgroup _ id, hostname und Port hinzu:
Um die Änderungen anzuwenden, führen Sie LOAD und SAVE erneut aus:
Abschließend teilen Sie ProxySQL mit, welcher Benutzer sich mit dem Backend-Server verbinden wird; legen Sie sammy als Benutzer fest und ersetzen Sie < ^ > sammy _ password < ^ > mit dem zuvor erstellten Passwort:
Die Tabelle mysql _ users enthält Informationen über die Benutzer, die für die Verbindung zu den Backend-Servern verwendet werden; Sie haben den Benutzernamen, das Passwort und default _ hostgroup angegeben.
Führen Sie LOAD und SAVE aus, um die Änderungen zu laden und zu speichern:
Beenden Sie dann die mysql-Shell:
Um zu testen, ob Sie mit ProxySQL eine Verbindung zu Ihrem Backend-Server herstellen können, führen Sie die folgende Testanfrage aus:
In diesem Befehl haben Sie Flag -e verwendet, um eine Abfrage auszuführen und die Verbindung zu schließen.
Die Abfrage gibt den Hostnamen des Backend-Servers aus.
< $> note Anmerkung: ProxySQL verwendet standardmäßig Port 6033 zum Lauschen auf eingehende Verbindungen.
Die Ausgabe sieht wie folgt aus, wobei < ^ > your _ hostname < ^ > durch Ihren Hostnamen ersetzt ist:
Um mehr über ProxySQL zu erfahren, lesen Sie Schritt 3 unter So verwenden Sie ProxySQL als Lastenausgleich für MySQL unter Ubuntu 16.04.
Bisher haben Sie ProxySQL konfiguriert, um Ihren MySQL-Server als Backend zu verwenden und sich mit dem Backend unter Verwendung von ProxySQL zu verbinden.
Jetzt können Sie mysqlslap verwenden, um die Abfrageleistung ohne Caching zu vergleichen.
Schritt 3 - Testen unter Verwendung von mysqlslap ohne Caching
In diesem Schritt laden Sie eine Testdatenbank herunter, damit Sie mit mysqlslap Abfragen gegen diese Datenbank ausführen können, um die Latenz ohne Caching zu testen, wobei Sie eine Benchmark für die Geschwindigkeit Ihrer Abfragen festlegen.
Sie werden auch erfahren, wie ProxySQL Datensätze von Abfragen in der Tabelle stats _ mysql _ query _ digest führt.
mysqlslap ist ein Lastemulator-Client, der als Auslastungstest-Tool für MySQL verwendet wird.
Er kann einen MySQL-Server mit automatisch generierten Abfragen oder mit einigen benutzerdefinierten Abfragen, die in einer Datenbank ausgeführt werden, testen.
Er wird mit dem MySQL Client-Paket installiert, sodass Sie ihn nicht installieren müssen; laden Sie stattdessen eine Datenbank nur zu Testzwecken herunter, auf der Sie mysqlslap verwenden können.
In diesem Tutorial werden Sie eine Beispiel-Mitarbeiter-Datenbank verwenden.
Sie verwenden diese Mitarbeiterdatenbank, weil sie einen großen Datensatz enthält, der Unterschiede in der Abfrageoptimierung veranschaulichen kann.
Die Datenbank verfügt über sechs Tabellen, aber die darin enthaltenen Daten haben mehr als 300.000 Mitarbeitereinträge.
Dies wird Ihnen helfen, eine Auslastung in einer großen Produktivumgebung zu emulieren.
Um die Datenbank herunterzuladen, klonen Sie zunächst das Github-Repository mit diesem Befehl:
Geben Sie dann das Verzeichnis test _ db ein und laden Sie die Datenbank mit diesen Befehlen in den MySQL-Server:
Dieser Befehl verwendet eine Shell-Umleitung, um die SQL-Abfragen in der Datei employees.sql zu lesen und sie auf dem MySQL-Server auszuführen, um die Datenbankstruktur zu erstellen.
Sobald die Datenbank in Ihren MySQL-Server geladen ist, testen Sie, ob mysqlslap mit der folgenden Abfrage funktioniert:
mysqlslap hat ähnliche Flags wie der mysql-Client; hier sind die in diesem Befehl verwendeten aufgeführt:
-u gibt den Benutzer an, der für eine Verbindung mit dem Server verwendet wird.
-p fragt nach dem Passwort des Benutzers.
-P verbindet sich über den angegebenen Port.
-h verbindet sich mit dem angegebenen Host.
--auto-generate-sql lässt MySQL Auslastungstests unter Verwendung seiner eigenen erzeugten Abfragen durchführen.
--verbose lässt die Ausgabe mehr Informationen anzeigen.
In dieser Ausgabe sehen Sie die durchschnittliche, minimale und maximale Anzahl von Sekunden, die für die Ausführung aller Abfragen aufgewendet wurden.
Dies gibt Ihnen einen Hinweis auf die Zeit, die für die Ausführung der Abfragen durch eine Reihe von Clients benötigt wird.
In dieser Ausgabe wurde nur ein Client für die Ausführung von Abfragen verwendet.
Finden Sie als Nächstes heraus, welche Abfragen mysqlslap im letzten Befehl ausgeführt hat, indem Sie sich ProxySQLs stats _ mysql _ query _ digest ansehen.
Damit erhalten wird Informationen wie den Digest der Abfragen, der eine normalisierte Form der SQL-Anweisung ist, auf die später verwiesen werden kann, um Caching zu aktivieren.
Rufen Sie mit diesem Befehl die ProxySQL Admin-Schnittstelle auf:
Führen Sie dann diese Abfrage aus, um Informationen in der Tabelle stats _ mysql _ query _ digest zu finden:
Die vorherige Abfrage wählt Daten aus der Tabelle stats _ mysql _ query _ digest aus, die Informationen über alle ausgeführten Abfragen in ProxySQL enthält.
Hier haben Sie fünf Spalten ausgewählt:
count _ star: Die Anzahl, wie oft diese Abfrage ausgeführt wurde.
sum _ time: Die Gesamtzeit in Millisekunden, die diese Abfrage zur Ausführung benötigt hat.
hostgroup: Die Hostgruppe, die zur Ausführung der Abfrage verwendet wurde.
digest: Ein Digest der ausgeführten Abfrage.
digest _ text: Die eigentliche Abfrage.
In dem Beispiel dieses Tutorials wird die zweite Abfrage mit?
Markierungen anstelle von variablen Parametern parametrisiert. Daher werden select @ @ version _ comment limit 1 und select @ @ version _ comment limit 2 als die gleiche Abfrage mit dem gleichen Digest gruppiert.
Nachdem Sie nun wissen, wie Sie Abfragedaten in der Tabelle stats _ mysql _ query _ digest überprüfen, verlassen Sie die mysql-Shell:
Die von Ihnen heruntergeladene Datenbank enthält einige Tabellen mit Demo-Daten. Sie werden nun Abfragen an der Tabelle dept _ emp testen, indem Sie alle Datensätze auswählen, deren from _ date größer als 2000-04-20 ist, und die durchschnittliche Ausführungszeit aufzeichnen.
Verwenden Sie diesen Befehl zur Durchführung des Tests:
Hier verwenden Sie einige neue Flags:
--concurrency = 100: Damit wird die Anzahl der zu simulierenden Benutzer festgelegt, in diesem Fall 100.
--iterations = 20: Dadurch wird dieser Test 20-mal ausgeführt und Ergebnisse von allen berechnet.
--create-schema = employees: Hier haben Sie die Datenbank employees ausgewählt.
--query = "SELECT * from dept _ emp WHERE from _ date > '2000-04-20' ": Hier haben Sie die in dem Test ausgeführte Abfrage angegeben.
Der Test dauert einige Minuten.
Nachdem er durchgeführt wurde, erhalten Sie ähnliche Ergebnisse wie die folgenden:
Ihre Zahlen können etwas abweichen.
Bewahren Sie diese Zahlen auf, um sie mit den Ergebnissen nach dem Aktivieren von Caching zu vergleichen.
Nach dem Testen von ProxySQL ohne Caching ist es an der Zeit, den gleichen Test erneut durchzuführen, diesmal jedoch mit aktiviertem Caching.
Schritt 3 - Testen unter Verwendung von mysqlslap mit Caching
In diesem Schritt hilft uns das Caching, die Latenz bei der Ausführung ähnlicher Abfragen zu verringern.
Hier identifizieren Sie die ausgeführten Abfragen, deren Digests aus der Tabelle stats _ mysql _ query _ digest von ProxySQL entnehmen und diese zum Aktivieren des Cachings verwenden.
Dann führen Sie einen neuen Test aus, um den Unterschied zu überprüfen.
Zum Aktivieren des Caching müssen Sie die Digests der Abfragen kennen, die zwischengespeichert werden sollen.
Melden Sie sich mit diesem Befehl an der ProxySQL Admin-Schnittstelle an:
Führen Sie dann diese Abfrage erneut aus, um eine Liste der ausgeführten Abfragen und deren Digests zu erhalten:
Sie erhalten ein ähnliches Ergebnis wie dieses:
Schauen Sie sich die erste Zeile an.
Es handelt sich um eine Abfrage, die 2000-mal ausgeführt wurde.
Dies ist die zuvor ausgeführte Benchmarking-Abfrage:
Speichern Sie ihre Digests, um sie für das Hinzufügen einer Abfrageregel für das Caching zu verwenden.
Die nächsten Abfragen fügen eine neue Abfrageregel zu ProxySQL hinzu, die mit dem Digest der vorherigen Abfrage übereinstimmt und einen Wert cache _ ttl für sie setzt. cache _ ttl ist die Anzahl der Millisekunden, die das Ergebnis im Speicher zwischengespeichert wird:
In diesem Befehl fügen Sie einen neuen Datensatz zu der Tabelle mysql _ query _ rules hinzu; diese Tabelle enthält alle Regeln, die vor der Ausführung einer Abfrage angewendet wurden.
In diesem Beispiel fügen Sie einen Wert für die Spalte cache _ ttl hinzu, der bewirkt, dass die übereinstimmende Abfrage durch den gegebenen Digest für eine von dieser Spalte angegebene Anzahl von Millisekunden zwischengespeichert wird.
Fügen Sie in der Spalte Anwenden eine 1 ein, um sicherzustellen, dass die Regel auf die Abfragen angewendet wird.
Führen Sie LOAD und SAVE aus, um diese Änderungen zu laden und zu speichern, und beenden Sie dann die mysql-Shell:
Nachdem das Caching nun aktiviert ist, führen Sie den Test erneut aus, um das Ergebnis zu überprüfen:
Hier sehen Sie den großen Unterschied in der durchschnittlichen Ausführungszeit: sie fiel von < ^ > 18.117 < ^ > Sekunden auf < ^ > 7.020 < ^ >.
In diesem Artikel haben Sie das transparente Caching mit ProxySQL eingerichtet, um die Ergebnisse von Datenbankanfragen zwischenzuspeichern.
Sie haben auch die Abfragegeschwindigkeit mit und ohne Caching getestet, um den Unterschied zu sehen, den Caching bewirken kann.
In diesem Tutorial haben Sie eine Ebene des Caching verwendet.
Sie können auch das Web-Caching ausprobieren, das vor einem Webserver sitzt und die Antworten auf ähnliche Anfragen zwischenspeichert und die Antworten an den Client zurücksendet, ohne in den Backend-Servern zu suchen.
Dies ist dem ProxySQL Caching sehr ähnlich, aber auf einer anderen Ebene.
Um mehr über das Web-Caching zu erfahren, besuchen Sie unsere Einführung Web-Caching-Grundlagen: Terminologie, HTTP-Headers und Caching.
MySQL-Server verfügt auch über einen eigenen Abfrage-Cache; Sie können in unserem Tutorial So optimieren Sie MySQL mit Abfrage-Cache unter Ubuntu 18.04 mehr darüber erfahren.
3754
Der Apache-Webserver ist eine beliebte Methode zur Bereitstellung von Websites im Internet.
Ab 2019 wird er schätzungsweise 29% aller aktiven Websites bereitstellen und bietet Entwicklern Robustheit und Flexibilität.
Mit Apache kann ein Administrator einen Server einrichten, um mehrere Domänen oder Websites über eine einzige Schnittstelle oder IP zu hosten, indem er ein übereinstimmendes System verwendet.
Jede mit Apache konfigurierte Domäne oder individuelle Site - bekannt als "virtueller Host" - leitet den Besucher zu einem bestimmten Verzeichnis, das die Informationen dieser Website enthält.
Dies geschieht, ohne anzugeben, dass derselbe Server auch für andere Sites verantwortlich ist.
Dieses Schema ist ohne jegliche Softwarebeschränkung erweiterbar, solange Ihr Server die Last bewältigen kann.
Die Basiseinheit, die eine individuelle Website oder Domäne beschreibt, wird als virtueller Host bezeichnet.
In diesem Leitfaden führen wir Sie durch die Einrichtung von virtuellen Apache-Hosts auf einem Ubuntu 18.04-Server.
Bevor Sie mit diesem Tutorial beginnen, sollten Sie einen Benutzer ohne Rootberechtigung erstellen.
Sie müssen auch Apache installiert haben, um diese Schritte durcharbeiten zu können.
Falls Sie dies noch nicht getan haben, können Sie Apache mit der Paketmethode apt auf Ihrem Server installieren lassen.
Wenn Sie genauere Anweisungen sowie die Einrichtung einer Firewall wünschen, lesen Sie bitte unseren Leitfaden So installieren Sie den Apache-Webserver unter Ubuntu 18.04.
Für die Zwecke dieses Leitfadens wird unsere Konfiguration einen virtuellen Host für example.com und einen weiteren für test.com erstellen.
Auf diese wird im gesamten Leitfaden Bezug genommen, doch sollten Sie Ihre eigenen Domänen oder Werte einsetzen, während Sie dem Leitfaden folgen.
Wenn Sie DigitalOcean verwenden, können Sie anhand der Produktdokumentation So fügen Sie Domänen hinzu lernen, wie Sie Domänen einrichten.
Für andere Anbieter lesen Sie die entsprechende Produktdokumentation. Wenn Sie zu diesem Zeitpunkt keine Domäne zur Verfügung haben, können Sie Testwerte verwenden.
Wir zeigen Ihnen später, wie Sie Ihre lokale hosts-Datei zum Testen der Konfiguration bearbeiten können, wenn Sie Testwerte verwenden.
Auf diese Weise können Sie Ihre Konfiguration von Ihrem Heimcomputer validieren, auch wenn Ihr Inhalt über den Domänennamen nicht für andere Besucher verfügbar ist.
Schritt Eins - Erstellen der Verzeichnisstruktur
Im ersten Schritt werden wir eine Verzeichnisstruktur erstellen, die die Daten der Website enthält, die wir den Besuchern zur Verfügung stellen werden.
Unser Dokumentenstamm (das Verzeichnis auf oberster Ebene, in dem Apache nach den bereitzustellenden Inhalten sucht) wird unter dem Verzeichnis / var / www auf einzelne Verzeichnisse gesetzt.
Wir erstellen hier ein Verzeichnis für beide virtuellen Hosts, die wir erstellen wollen.
In jedem dieser Verzeichnisse erstellen wir einen Ordner public _ html, der unsere eigentlichen Dateien enthält.
Dies gibt uns eine gewisse Flexibilität bei unserem Hosting.
Zum Beispiel werden wir für unsere Sites unsere Verzeichnisse wie folgt erstellen.
Wenn Sie tatsächliche Domänen oder alternative Werte verwenden, tauschen Sie den hervorgehobenen Text gegen diese aus.
Die rot markierten Abschnitte stellen die Domänennamen dar, die wir von unserem VPS aus bereitstellen möchten.
Schritt Zwei - Erteilen von Berechtigungen
Jetzt haben wir die Verzeichnisstruktur für unsere Dateien, aber sie gehören unserem Root-Benutzer.
Wenn wir möchten, dass unser normaler Benutzer in der Lage ist, Dateien in unseren Webverzeichnissen zu ändern, können wir die Eigentümerschaft ändern, indem wir Folgendes tun:
Die Variable $USER nimmt den Wert des Benutzers an, als der Sie gerade angemeldet sind, wenn Sie die EINGABETASTE drücken.
Dadurch besitzt unser normaler Benutzer nun die Unterverzeichnisse public _ html, in denen wir unsere Inhalte speichern werden.
Wir sollten auch unsere Berechtigungen ändern, um sicherzustellen, dass der Lesezugriff auf das allgemeine Webverzeichnis und alle darin enthaltenen Dateien und Ordner erlaubt ist, damit die Seiten korrekt bereitgestellt werden können:
Ihr Webserver sollte nun über die erforderlichen Berechtigungen verfügen, um Inhalte bereitzustellen, und Ihr Benutzer sollte in der Lage sein, Inhalte in den erforderlichen Ordnern zu erstellen.
Schritt Drei - Erstellen von Demo-Seiten für jeden virtuellen Host
Wir haben nun unsere Verzeichnisstruktur eingerichtet.
Lassen Sie uns einige Inhalte erstellen, die wir bereitstellen können.
Zu Demonstrationszwecken werden wir für jede Website eine Seite index.html erstellen.
Beginnen wir mit example.com.
Wir können eine Datei index.html in einem Texteditor öffnen. In diesem Fall verwenden wir nano:
Erstellen Sie innerhalb dieser Datei ein HTML-Dokument, das die Website angibt, mit der sie verbunden ist, wie folgt:
Speichern und schließen Sie die Datei (in nano drücken Sie STRG + X, dann Y und dann die EINGABETASTE) wenn Sie fertig sind.
Wir können diese Datei kopieren und als Grundlage für unsere zweite Website verwenden, indem wir Folgendes eingeben:
Dann können wir die Datei öffnen und die relevanten Informationen ändern:
Sie verfügen nun über die erforderlichen Seiten zum Testen der Konfiguration des virtuellen Hosts.
Schritt Vier - Erstellen neuer virtuelle Host-Dateien
Virtuelle Host-Dateien sind die Dateien, die die tatsächliche Konfiguration unserer virtuellen Hosts spezifizieren und bestimmen, wie der Apache-Webserver auf verschiedene Domänenanfragen reagieren wird.
In Apache ist eine virtuelle Standarddatei namens 000-default.conf enthalten, die wir als Ausgangspunkt verwenden können.
Wir werden sie kopieren, um eine virtuelle Host-Datei für jede unserer Domänen zu erstellen.
Wir beginnen mit einer Domäne, konfigurieren sie, kopieren sie für unsere zweite Domäne und nehmen dann die wenigen weiteren erforderlichen Anpassungen vor.
Die Standardkonfiguration von Ubuntu erfordert, dass jede virtuelle Host-Datei auf .conf endet.
Öffnen Sie die neue Datei in Ihrem Editor mit Root-Berechtigungen:
Ohne Kommentare wird die Datei in etwa wie folgt aussehen:
Innerhalb dieser Datei werden wir Elemente für unsere erste Domäne anpassen und einige zusätzliche Anweisungen hinzufügen.
Dieser virtuelle Host-Abschnitt entspricht allen Anfragen, die auf Port 80, dem standardmäßigen HTTP-Port, gestellt werden.
Zuerst müssen wir die Anweisung ServerAdmin in eine E-Mail ändern, über die der Website-Administrator E-Mails empfangen kann.
Danach müssen wir zwei Anweisungen hinzufügen.
Der erste namens ServerName legt die Grunddomäne fest, die mit dieser Definition des virtuellen Hosts übereinstimmen sollte.
Dies wird höchstwahrscheinlich Ihre Domäne sein.
Die zweite namens ServerAlias definiert weitere Namen, die übereinstimmen sollten, als wären sie der Basisname.
Dies ist nützlich für die von Ihnen definierte Zuordnung von Hosts, wie beispielsweise www:
Das einzige, was wir für unsere virtuelle Host-Datei ändern müssen, ist der Speicherort des Dokumentenstamms für diese Domäne.
Wir haben das benötigte Verzeichnis bereits erstellt, also müssen wir nur die Anweisung DocumentRoot so ändern, dass sie das von uns erstellte Verzeichnis widerspiegelt:
Nach Fertigstellung sollte unsere virtuelle Host-Datei wie folgt aussehen:
Wenn Sie fertig sind, sollte sie wie folgt aussehen:
Schritt Fünf - Aktivieren der neuen virtuellen Host-Dateien
Nachdem wir nun unsere virtuellen Host-Dateien erstellt haben, müssen wir sie aktivieren.
Apache enthält einige Tools, mit denen wir dies tun können.
Wir verwenden das Tool a2ensite, um jede unserer Sites zu aktivieren.
Wenn Sie mehr über dieses Skript lesen möchten, können Sie die Dokumentation zu a2ensite einsehen.
Schritt Sechs - Einrichten der Datei "Local Hosts" (optional)
Wenn Sie zum Testen dieses Verfahrens keine tatsächlichen Domänennamen, die ihnen gehören, verwendet haben, sondern stattdessen einige Beispieldomänen, können Sie zumindest die Funktionalität dieses Prozesses testen, indem Sie die Datei hosts auf Ihrem lokalen Rechner vorübergehend ändern.
Dadurch werden alle Anfragen für die von Ihnen konfigurierten Domänen abgefangen und an Ihren VPS-Server verwiesen, so wie es das DNS-System tun würde, wenn Sie registrierte Domänen verwenden würden.
Dies funktioniert jedoch nur von Ihrem lokalen Rechner aus und nur zu Testzwecken.
Stellen Sie sicher, dass Sie für diese Schritte auf Ihrem lokalen Rechner arbeiten und nicht auf Ihrem VPS-Server.
Sie müssen das administrative Passwort des Rechners kennen oder anderweitig ein Mitglied der administrativen Gruppe sein.
Wenn Sie auf einem Mac- oder Linux-Rechner arbeiten, bearbeiten Sie Ihre lokale Datei mit administrativen Berechtigungen, indem Sie Folgendes eingeben:
Wenn Sie auf einem Windows-Rechner arbeiten, finden Sie hier Anweisungen zur Änderung Ihrer Datei hosts.
Die Angaben, die Sie hinzufügen müssen, sind die öffentliche IP-Adresse Ihres Servers, gefolgt von der Domäne, mit der Sie diesen Server erreichen möchten.
Genau das möchten wir, wenn wir nicht tatsächlich die Besitzer dieser Domänen sind, um unsere virtuellen Hosts zu testen.
Schritt Sieben - Testen Ihrer Ergebnisse
Wenn beide Sites wie erwartet funktionieren, haben Sie erfolgreich zwei virtuelle Hosts auf ein und demselben Server konfiguriert.
Wenn Sie die Datei hosts Ihres Heimcomputers angepasst haben, möchten Sie die hinzugefügten Zeilen möglicherweise löschen, nachdem Sie die Funktionalität Ihrer Konfiguration überprüft haben.
Dadurch wird verhindert, dass Ihre Datei hosts mit Einträgen gefüllt wird, die nicht mehr erforderlich sind.
Wenn Sie langfristig auf diese Datei zugreifen müssen, sollten Sie in Erwägung ziehen, für jede benötigte Website einen Domänennamen hinzuzufügen und sie so einzurichten, dass sie auf Ihren Server verweist.
Wenn Sie diesem Beispiel gefolgt sind, sollten Sie jetzt einen einzigen Server haben, der zwei separate Domänennamen verwaltet.
Sie können diesen Prozess erweitern, indem Sie die oben beschriebenen Schritte befolgen, um zusätzliche virtuelle Hosts zu erstellen.
Es gibt keine Softwareeinschränkung für die Anzahl der Domänennamen, die Apache verarbeiten kann, also zögern Sie nicht, so viele zu erstellen, wie Ihr Server verarbeiten kann.
3692
Für Sie als Web-Administrator könnte es eventuell sinnvoll sein, einige Teile einer Website vorübergehend oder dauerhaft für Besucher einzuschränken.
Obwohl Webanwendungen möglicherweise ihre eigenen Authentifizierungs- und Autorisierungsmethoden anbieten, können Sie sich auch auf den Webserver selbst stützen, um den Zugriff einzuschränken, wenn diese unzureichend oder nicht verfügbar sind.
Dieses Tutorial bietet Ihnen eine Vorgehensweise, um Assets auf einem Apache-Webserver, auf dem Ubuntu 18.04 ausgeführt wird, mit einem Passwort zu schützen, um Ihren Server mit zusätzlicher Sicherheit zu versehen.
Zusätzlich benötigen Sie die folgende Einrichtung, bevor Sie beginnen können:
Einen sudo-Benutzer auf Ihrem Server: Sie können einen Benutzer mit sudo-Berechtigungen erstellen, indem Sie der Anleitung für die Ersteinrichtung eines Servers unter Ubuntu 18.04 folgen.
Einen Apache2-Webserver: Sollten Sie noch keinen eingerichtet haben, können Sie eine Hilfestellung in dem Tutorial So installieren Sie den Apache-Webserver unter Ubuntu 18.04 finden.
Eine mit SSL gesicherte Website: Wie Sie diese einrichten, hängt davon ab, ob Sie einen Domänenamen für Ihre Website haben.
Wenn Sie einen Domänennamen haben, können Sie Ihre Site mit Let 's Encrypt sichern, das kostenlose, vertrauenswürdige Zertifikate bereitstellt.
Folgen Sie dem Leitfaden Let 's Encrypt für Apache, um diese Einrichtung vorzunehmen.
Wenn Sie keine Domäne haben und diese Konfiguration nur zu Testzwecken oder für den persönlichen Gebrauch verwenden, können Sie stattdessen ein selbstsigniertes Zertifikat verwenden.
Folgen Sie dem Leitfaden für selbstsignierte SSL für Apache, um diese Einrichtung vorzunehmen.
Wenn alle diese Voraussetzungen erfüllt sind, melden Sie sich als sudo-Benutzer an Ihrem Server an und fahren weiter unten fort.
Schritt 1 - Installieren des Apache-Dienstprogrammpakets
Beginnen wir, indem wir unseren Server aktualisieren und ein Paket installieren, das wir benötigen.
Um diesem Tutorial zu folgen, verwenden wir ein Dienstprogramm namens htpasswd, das Teil des Pakets apache2-utils ist, um die Datei zu erstellen und den Benutzernamen und die Passwörter zu verwalten, die für den Zugriff auf eingeschränkte Inhalte benötigt werden.
Wenn dies installiert ist, haben wir Zugriff auf den Befehl htpasswd.
Mit dem Befehl htpasswd können wir eine Passwortdatei erstellen, die Apache zur Authentifizierung von Benutzern verwenden kann.
Wir werden zu diesem Zweck eine versteckte Datei namens .htpasswd in unserem Konfigurationsverzeichnis / etc / apache2 erstellen.
Wenn wir dieses Dienstprogramm zum ersten Mal verwenden, müssen wir die Option -c hinzufügen, um die spezifizierte passwdfile zu erstellen.
Wir geben am Ende des Befehls einen Benutzernamen (in diesem Beispiel sammy) an, um einen neuen Eintrag in der Datei zu erstellen:
Wenn wir den Inhalt der Datei ansehen, können wir für jeden Datensatz den Benutzernamen und das verschlüsselte Passwort sehen:
Unsere Benutzer und Passwörter liegen jetzt in einem Format vor, das Apache lesen kann.
Schritt 3 - Konfigurieren der Apache Passwort-Authentifizierung
Wir können dies auf eine von zwei Arten tun: entweder direkt in der virtuellen Host-Datei einer Website oder indem wir .htaccess-Dateien in die Verzeichnisse übertragen, die eine Einschränkung benötigen.
In der Regel empfiehlt sich die Verwendung der virtuellen Host-Datei. Wenn Sie jedoch Benutzern ohne Rootberechtigung die Verwaltung ihrer eigenen Zugriffsbeschränkungen ermöglichen müssen, die Einschränkungen in die Versionskontrolle der Website integrieren müssen oder eine Webanwendung haben, die .htaccess-Dateien bereits für andere Zwecke verwendet, sollten Sie die zweite Option ausprobieren.
Wählen Sie die Option, die am besten für Ihre Bedürfnisse geeignet ist.
Option 1: Konfigurieren der Zugriffskontrolle innerhalb der virtuellen Host-Definition (bevorzugt)
Die erste Option besteht darin, die Apache-Konfiguration zu bearbeiten und den Passwortschutz zu der virtuellen Host-Datei hinzuzufügen.
Dadurch wird im Allgemeinen eine bessere Leistung erzielt, da die Kosten für das Lesen von verteilten Konfigurationsdateien vermieden werden.
Diese Option erfordert Zugriff auf die Konfiguration, der nicht immer verfügbar ist. Wenn Sie jedoch Zugriff haben, wird dies empfohlen.
Beginnen Sie, indem Sie die virtuelle Host-Datei öffnen, der Sie eine Einschränkung hinzufügen möchten.
Wir werden für unser Beispiel die Datei default-ssl.conf verwenden, die den virtuellen Standard-Host enthält, der über das Apache-Paket von Ubuntu installiert wurde.
Öffnen Sie die Datei mit einem Befehlszeilen-Texteditor wie nano:
Ohne die Kommentare sollte die Datei in etwa wie folgt aussehen:
Um die Authentifizierung einzurichten, müssen Sie das Verzeichnis, das Sie einschränken möchten, mit einem < Directory _ _ _ > -Block als Ziel angeben.
In unserem Beispiel schränken wir den gesamte Dokumentenstamm ein. Sie können diese Auflistung jedoch so ändern, dass sie nur auf ein bestimmtes Verzeichnis innerhalb des Webspace abzielt:
Innerhalb dieses Verzeichnisblocks legen wir fest, dass die Basic-Authentifizierung eingerichtet wird.
Wählen Sie für den AuthName einen Bereichsnamen, der dem Benutzer bei der Aufforderung zur Eingabe der Anmeldedaten angezeigt wird.
Verwenden Sie die Anweisung AuthUserFile, um Apache auf die von uns erstellte Passwort-Datei zu verweisen.
Machen Sie es zum Schluss zur Bedingung, dass nur ein valid-user auf diese Ressource zugreifen kann, was bedeutet, dass jede Person, die ihre Identität mit einem Passwort verifizieren kann, Zugriff erhält.
Wenn Sie nano verwenden, drücken Sie hierfür STRG + X, gefolgt von Y und dann der EINGABETASTE.
Bevor Sie den Webserver neu starten, können Sie die Konfiguration mit dem folgenden Befehl überprüfen:
Wenn alles in Ordnung ist und Sie als Ausgabe Syntax OK erhalten, können Sie den Server neu starten, um Ihre Passwort-Richtlinie zu implementieren.
Da systemctl nicht das Ergebnis aller Dienstverwaltungsbefehle anzeigt, verwenden wir status, um sicherzustellen, dass der Server ausgeführt wird:
Nun sollte das von Ihnen angegebene Verzeichnis passwortgeschützt sein.
Option 2: Konfigurieren der Zugriffskontrolle mit .htaccess-Dateien
Apache kann .htaccess-Dateien verwenden, um die Einstellung bestimmter Konfigurationselemente innerhalb eines Inhaltsverzeichnisses zu ermöglichen.
Da Apache diese Dateien bei jeder das Verzeichnis betreffenden Anfrage erneut lesen muss, was sich negativ auf die Leistung auswirken kann, wird Option 1 bevorzugt. Wenn Sie jedoch bereits die Datei .htaccess verwenden oder Benutzern ohne Rootberechtigung die Verwaltung von Einschränkungen ermöglichen müssen, sind .htaccess-Dateien sinnvoll.
Um den Passwortschutz unter Verwendung von .htaccess-Dateien zu aktivieren, öffnen Sie die Apache-Konfigurationsdatei mit einem Befehlszeilen-Texteditor wie nano:
Suchen Sie den Block < Directory > für das Verzeichnis / var / www, in dem sich der Dokumentenstamm befindet.
Aktivieren Sie die Verarbeitung von .htaccess, indem Sie die Anweisung AllowOverride innerhalb dieses Blocks von None zu All ändern:
Als Nächstes müssen wir dem Verzeichnis, das wir einschränken möchten, eine .htaccess-Datei hinzufügen.
In unserer Demonstration beschränken wir den gesamten Dokumentenstamm (die gesamte Website), der auf / var / www / html basiert. Sie können diese Datei jedoch in jedem Verzeichnis ablegen, in dem Sie den Zugriff einschränken möchten:
Innerhalb dieser Datei geben wir an, dass wir die Basic-Authentifizierung einrichten möchten.
Abschließend erfordern wir den Zugriff auf diese Ressource durch einen valid-user, d. h. jede Person, die ihre Identität mit einem Passwort verifizieren kann, erhält Zugriff:
Starten Sie den Webserver neu, um alle Inhalte in oder unterhalb des Verzeichnisses mit der .htaccess-Datei mit einem Passwort zu schützen, und verwenden Sie systemctl status, um den Erfolg des Neustarts zu überprüfen:
Das von Ihnen angegebene Verzeichnis sollte nun passwortgeschützt sein.
Schritt 4 - Bestätigen der Passwort-Authentifizierung
Sie sollten eine Aufforderung zur Eingabe von Benutzername und Passwort erhalten, die wie folgt aussieht:
Wenn Sie die richtigen Anmeldedaten eingeben, können Sie auf den Inhalt zugreifen.
Wenn Sie die falschen Anmeldedaten eingeben oder auf "Cancel" klicken, wird die Fehlerseite "Unauthorized" angezeigt:
Apache2 Fehler "unauthorized"
Wenn Sie diesem Tutorial gefolgt sind, haben Sie nun die standardmäßige Authentifizierung für Ihre Website eingerichtet.
Sie können mit der Apache-Konfiguration und .htaccess noch wesentlich mehr tun.
Um mehr über die Flexibilität und Leistungsfähigkeit der Apache-Konfiguration zu erfahren, probieren Sie eines dieser Tutorials aus:
Für ein besseres Verständnis der Hauptkonfigurationsdatei lesen Sie den Abschnitt Mit wichtigen Apache-Dateien und -Verzeichnissen vertraut werden in unserem Apache-Installationsleitfaden.
Erfahren Sie mehr über die virtuellen Host-Dateien in So richten Sie Apache Virtual Hosts unter Ubuntu 16.04 ein
Erfahren Sie mehr über das Neuschreiben von URLs, das Anpassen von Fehlerseiten wie die obige Meldung "Unauthorized" oder das Einfügen gemeinsamer Elemente auf allen Ihren Seiten mit Server Side Includes in unserem Leitfaden So verwenden Sie die Datei .htaccess.
So richten Sie die Code-Server-Cloud-IDE-Plattform unter CentOS 7 ein
3333
In diesem Tutorial richten Sie die Code-Server-Cloud-IDE-Plattform auf Ihrem CentOS-7-Rechner ein und machen sie auf Ihrer Domäne verfügbar, geschützt durch kostenlose Let 's Encrypt TLS-Zertifikate.
Zum Schluss wird Microsoft Visual Studio Code auf Ihrem CentOS-7-Server ausgeführt, ist auf Ihrer Domäne verfügbar und mit einem Passwort geschützt.
Ein Server mit CentOS 7 mit mindestens 2 GB RAM, Root-Zugriff und einem Sudo-Konto ohne Rootberechtigung.
Eine Anleitung hierzu finden Sie in So installieren Sie Nginx unter CentOS 7.
Laden Sie ihn mit curl herunter, indem Sie folgenden Befehl ausführen:
Speichern Sie die Dienstkonfiguration in einer Datei namens code-server.service im Verzeichnis / usr / lib / systemd / system, in dem systemd seine Dienste speichert.
Erstellen Sie diese mit dem vi-Editor:
Geben Sie: wq ein und drücken Sie die EINGABETASTE, um die Datei zu speichern und zu schließen.
Wie Sie im Schritt Nginx-Voraussetzung erfahren haben, werden seine Website-Konfigurationsdateien unter / etc / nginx / conf.d gespeichert und beim Start von Nginx automatisch geladen.
Sie speichern die Konfiguration, um Code-Server auf Ihrer Domäne in einer Datei namens Code-server.conf unter / etc / nginx / conf.d verfügbar zu machen.
Bei CentOS 7 ist SELinux eingeschaltet und hat einen strengen Regelsatz, der Nginx standardmäßig nicht erlaubt, sich an lokale TCP-Sockets anzuschließen.
Nginx muss dies tun, um als Reverseproxy für den Code-Server zu fungieren.
Führen Sie folgenden Befehl aus, um die Regel dauerhaft abzuschwächen:
Jetzt lassen Sie Certbot die Zertifikate automatisch erneuern, bevor diese ablaufen.
Verwenden Sie zur täglichen Überprüfung der Erneuerung cron, ein Standard-System-Dienst für das Ausführen regelmäßiger Aufträge.
Sie weisen cron an, indem Sie eine Datei namens crontab öffnen und bearbeiten:
Mit diesem Befehl öffnen Sie den Standard crontab, der derzeit eine leere Textdatei ist.
Fügen Sie folgende Zeile hinzu und speichern und schließen Sie diese im Anschluss:
15 3 * * * führt folgenden Befehl jeden Tag um 3: 15 Uhr aus. Sie können jede beliebige Uhrzeit einstellen.
Mit dem Befehl renew für Certbot werden alle auf dem System installierten Zertifikate überprüft und alle aktualisiert, die in weniger als dreißig Tagen ablaufen. --quiet ​ ​ ​ befiehlt Certbot, keine Informationen auszugeben oder auf die Eingabe von Benutzern zu warten.
cron wird diesen Befehl nun täglich ausführen.
Alle installierten Zertifikate werden automatisch erneuert und neu geladen, wenn sie in dreißig Tagen oder weniger ablaufen.
Code-Server, eine vielseitige Cloud-IDE, ist jetzt auf Ihrem CentOS-7-Server installiert, auf Ihrer Domäne verfügbar und mit Let 's-Encrypt-Zertifikaten gesichert.
So richten Sie die Code-Server-Cloud-IDE-Plattform unter Ubuntu 18.04 ein
3329
In diesem Tutorial richten Sie die Code-Server-Cloud-IDE-Plattform auf Ihrem Ubuntu-18.04-Rechner ein und machen sie auf Ihrer Domäne verfügbar, geschützt durch kostenlose Let 's Encrypt TLS-Zertifikate.
Zum Schluss wird Microsoft Visual Studio Code auf Ihrem Ubuntu-18.04-Server ausgeführt, ist auf Ihrer Domäne verfügbar und mit einem Passwort geschützt.
Um die neueste Version von Certbot zu installieren, müssen Sie Ihrem Server das entsprechende Paketrepository durch Ausführung des folgenden Befehls hinzufügen:
Drücken Sie zum Akzeptieren die EINGABETASTE.
Installieren Sie danach Certbot und das zugehörige Nginx-Plugin:
So richten Sie die Eclipse-Theia-Cloud-IDE-Plattform unter CentOS 7 ein
3532
Die Einführung von Cloud-IDE-Plattformen (IDE: Integrated Development Environment) nimmt zu, da Entwickler-Tools zunehmend cloudbasiert sind.
Aussehen und Verhalten sind ähnlich wie bei Microsoft Visual Studio Code ​ ​ ​, wodurch es viele Programmiersprachen unterstützt, ein flexibles Layout und ein integriertes Terminal hat.
In diesem Tutorial stellen Sie Eclipse Theia für Ihren CentOS-7-Server mit Docker Compose, einem Tool zur Container-Orchestrierung, bereit.
Sie machen es auf Ihrer Domäne mit nginx-proxy verfügbar, einem automatisierten System für Docker, das den Prozess der Konfiguration von Nginx vereinfacht, damit es als Reverseproxy für einen Container fungieren kann.
Zusätzlich sichern Sie es mit einem kostenlosen Let 's Encrypt TLS-Zertifikat, das Sie mithilfe des spezialisierten Add-on bereitstellen.
Zum Schluss wird Eclipse Theia auf Ihrem CentOS-7-Server ausgeführt, über HTTPS verfügbar sein und der Benutzer wird sich anmelden müssen.
Ein CentOS-7-Server mit Rootberechtigung und einem sekundären Konto ohne Rootberechtigung.
Folgen Sie zur Einrichtung unserem Leitfaden zur Ersteinrichtung des Servers für CentOS 7. Der Benutzer ohne Rootberechtigung ist für dieses Tutorial < ^ > sammy < ^ >.
Folgen Sie Schritt 1 und Schritt 2 in So installieren Sie Docker unter CentOS 7. Vergessen Sie nicht, sich nach Ausführung dieser Voraussetzung ab- und wieder anzumelden.
Eine Einführung zu Docker finden Sie in Das Docker-Ökosystem: Leitfaden für gemeinsame Komponenten.
Docker Compose, das auf Ihrem Server installiert ist.
Folgen Sie Schritt 1 in So installieren Sie Docker Compose unter CentOS 7.
Dieses Tutorial verwendet durchgehend < ^ > theia.your-domain < ^ >.
Ein DNS-Eintrag mit < ^ > theia.your-domain < ^ >, der auf die öffentliche IP-Adresse Ihres Servers verweist.
Schritt 1 - Einsatz von nginx-proxy mit Let 's Encrypt
In diesem Abschnitt stellen Sie nginx-proxy und sein Add-on Let 's Encrypt mit Docker Compose bereit.
Das aktiviert die automatische Bereitstellung und Erneuerung von TLS-Zertifikaten, damit Eclipse Theia auf Ihrer Domäne über HTTPS zugänglich sein wird.
Zum Zwecke dieses Tutorials speichern Sie alle Dateien unter ~ / eclipse-theia.
Speichern Sie die Docker-Compose-Konfiguration für nginx-proxy in einer Datei namens nginx-proxy-compose.yaml.
Hier definieren Sie zwei Dienste, die Docker Compose ausführen wird, nginx-proxy und seinen Let 's Encrypt-Begleiter.
Für den Proxy geben Sie jwilder / nginx-proxy als Image ein, ordnen Sie HTTP- und HTTPS-Ports zu und definieren Sie Volumes, die während der Laufzeit zugänglich sind.
Volumes sind Verzeichnisse auf Ihrem Server, auf die der definierte Dienst vollen Zugriff haben wird, was Sie später zum Einrichten der Benutzerauthentifizierung verwenden werden.
Dazu verwenden Sie das erste Volume aus der Liste, das das lokale Verzeichnis / etc / nginx / htpasswd dem gleichen im Container zuordnet.
In diesem Ordner erwartet nginx-proxy eine Datei mit dem gleichen Namen wie die Zieldomäne, die Anmeldeangaben für die Benutzerauthentifizierung im Format htpasswd enthält (​ ​ ​ username: hashed _ password ​ ​ ​).
Für das Add-on benennen Sie das Docker-Image und ermöglichen Zugriff auf das Docker-Socket, indem Sie ein Volume definieren.
Dann geben Sie an, dass das Add-on Zugriff auf die Volumes erben soll, die für nginx-proxy definiert sind.
Bei beiden Diensten ist restart auf always eingestellt, wodurch Docker angewiesen wird, die Container im Falle eines Absturzes oder System-Neustarts neu zu starten.
Stellen Sie die Konfiguration bereit, indem Sie Folgendes ausführen:
Hier übergeben Sie den Dateinamen nginx-proxy-compose.yaml an den Parameter -f des Befehls docker-compose, der die Datei angibt, die ausgeführt wird.
Dann übergeben Sie das Verb up, das die Anweisung zur Ausführung der Container gibt.
Das Flag -d ermöglicht den getrennten Modus. Das bedeutet, dass Docker Compose die Container im Hintergrund ausführt.
Die endgültige Ausgabe sieht ungefähr so aus:
Sie haben nginx-proxy und seinen Let 's-Encrypt-Begleiter mit Docker Compose bereitgestellt.
Jetzt richten Sie Eclipse Theia auf Ihrer Domäne ein und sichern es.
Schritt 2 - Einsatz des dockerisierten Eclipse Theia
In diesem Abschnitt erstellen Sie eine Datei mit allen erlaubten Anmelde-Kombinationen, die ein Benutzer eingeben muss.
Dann stellen Sie Eclipse Theia für Ihren Server mit Docker Compose bereit und machen es auf Ihrer gesicherten Domäne mit nginx-proxy verfügbar.
Wie im vorigen Schritt erklärt, geht nginx-proxy davon aus, dass Anmelde-Kombinationen sich in einer Datei befinden, die nach der verfügbar gemachten Domäne benannt wurde, das Format htpasswd hat und unter dem Verzeichnis / etc / nginx / htpasswd im Container gespeichert ist.
Das lokale Verzeichnis, das dem virtuellen zugeordnet ist, muss nicht das gleiche sein, das in der Konfiguration nginx-proxy angegeben wurde.
Um Anmelde-Kombinationen zu erstellen, müssen Sie zunächst htpasswd installieren, indem Sie folgenden Befehl ausführen:
Das Paket httpd-tools ​ ​ ​ enthält das Dienstprogramm htpasswd.
Erstellen Sie das Verzeichnis / etc / nginx / htpasswd:
Erstellen Sie eine Datei, welche die Anmeldungen für Ihre Domäne speichert:
Vergessen Sie nicht, < ^ > theia.your-domain < ^ > durch Ihre Eclipse-Theia-Domäne zu ersetzen.
Führen Sie den folgenden Befehl aus, um eine Kombination aus Benutzernamen und Passwort hinzuzufügen:
Ersetzen Sie < ^ > username < ^ > durch den Benutzernamen, den Sie hinzufügen möchten.
Sie werden zweimal nach einem Passwort gefragt.
Nachdem Sie es angegeben haben, fügt htpasswd den Benutzernamen und ein Passwort-Paar mit Hash am Ende der Datei hinzu.
Sie können diesen Befehl für so viele Anmeldungen wiederholen, wie Sie hinzufügen möchten.
Jetzt erstellen Sie eine Konfiguration, um Eclipse Theia bereitzustellen.
Speichern Sie diese in einer Datei namens eclipse-theia-compose.yaml.
In dieser Konfiguration definieren Sie einen einzelnen Dienst namens eclipse-theia, bei dem restart auf always eingestellt ist und theiaide / theia: next als das Container-Image.
Außerdem stellen Sie init auf true ein, um Docker anzuweisen, init als Haupt-Prozessmanager zu verwenden, wenn Eclipse Theia im Container ausgeführt wird.
Dann bestimmen Sie zwei Umgebungsvariablen im Umgebungsabschnitt: VIRTUAL _ HOST und LETSENCRYPT _ HOST.
Erstere wird an nginx-proxy übergeben und teilt ihm mit, auf welcher Domäne der Container verfügbar sein soll. Letztere wird von ihrem Let 's-Encrypt-Add-on benutzt und gibt an, für welche Domäne TLS-Zertifikate angefordert werden sollen.
Wenn Sie nicht eine Wildcard als Wert für VIRTUAL _ HOST angeben, müssen sie gleich sein.
Vergessen Sie nicht, < ^ > theia.your-domain < ^ > durch Ihre gewünschte Domäne zu ersetzen. Speichern und schließen Sie danach die Datei.
Stellen Sie jetzt Eclipse Theia bereit, indem Sie Folgendes ausführen:
Der endgültige Ausgabe sieht ungefähr so aus:
Navigieren Sie anschließend in Ihrem Browser zu der Domäne, die Sie für Eclipse Theia verwenden.
Ihr Browser zeigt Ihnen eine Eingabeaufforderung, mit der Sie zur Anmeldung aufgefordert werden.
Nachdem Sie die richtigen Anmeldeangaben bereitgestellt haben, steigen Sie in Eclipse Theia ein und sehen sofort seine Editor-GUI.
In Ihrer Adressleiste sehen Sie ein Schloss, das anzeigt, dass die Verbindung sicher ist.
Wenn Sie es nicht sofort sehen, warten Sie einige Minuten darauf, dass die TLS-Zertifikate bereitgestellt werden. Laden Sie dann die Seite neu.
Eclipse Theia GUI
Nachdem Sie nun sicher auf Ihre Cloud-IDE zugreifen können, beginnen Sie im nächsten Schritt mit der Verwendung des Editors.
Schritt 3 - Verwendung der Eclipse-Theia-Benutzeroberfläche
Nach der Erstellung einer neuen Datei über das Menü File öffnet sich eine leere Datei in einer neuen Registerkarte. Sobald sie gespeichert sind, können Sie den Namen der Datei im Seitenfeld Explorer anzeigen.
Eclipse Theia, eine vielseitige Cloud-IDE, ist nun auf Ihrem CentOS-7-Server mit Docker Compose und nginx-proxy installiert.
Sie haben es mit einem kostenlosen Let 's Encrypt TLS-Zertifikat gesichert und eine Instanz eingerichtet, um Anmeldedaten des Benutzers anzufordern.
So richten Sie die Eclipse-Theia-Cloud-IDE-Plattform unter Debian 10 ein
3682
In diesem Tutorial stellen Sie Eclipse Theia für Ihren Debian-10-Server mit Docker Compose, einem Tool zur Container-Orchestrierung, bereit.
Zum Schluss wird Eclipse Theia auf Ihrem Debian-10-Server ausgeführt, über HTTPS verfügbar sein und der Benutzer wird sich anmelden müssen.
Ein Debian-10-Server mit Rootberechtigung und einem sekundären Konto ohne Rootberechtigung.
Folgen Sie zur Einrichtung unserem Leitfaden zur Ersteinrichtung des Servers für Debian 10. Der Benutzer ohne Rootberechtigung ist für dieses Tutorial < ^ > sammy < ^ >.
Folgen Sie Schritt 1 und Schritt 2 in So installieren Sie Docker unter Debian 10. Eine Einführung in Docker finden Sie in Das Docker-Ökosystem: Leitfaden für gemeinsame Komponenten.
Folgen Sie Schritt 1 in So installieren Sie Docker Compose unter Debian 10.
Ein DNS-Eintrag mit < ^ > theia.your _ domain < ^ >, der auf die öffentliche IP-Adresse Ihres Servers verweist.
Das apache2-utils-Paket enthält das htpasswd-Dienstprogramm.
Vergessen Sie nicht, < ^ > theia.your _ domain < ^ > ​ ​ ​ durch Ihre Eclipse-Theia-Domäne zu ersetzen.
Vergessen Sie nicht, < ^ > theia.your _ domain < ^ > ​ ​ ​ durch Ihre gewünschte Domäne zu ersetzen. Speichern und schließen Sie danach die Datei.
Eclipse Theia, eine vielseitige Cloud-IDE, ist nun auf Ihrem Debian-10-Server mit Docker Compose und nginx-proxy installiert.
So richten Sie die Eclipse-Theia-Cloud-IDE-Plattform unter Ubuntu 18.04 ein Schnellstart
3758
Visual ist sie so konzipiert, dass sie ähnlich wie Microsoft Visual Studio aussieht und sich auch ähnlich verhält.
In diesem Tutorial verwenden Sie Eclipse Theia auf Ihrem Ubuntu-18.04-Server mit Docker Compose.
Sie machen ihn auf Ihrer Domäne mit nginx-proxy verfügbar und sichern ihn mit einem Let 's Encrypt TLS-Zertifikat, dem Sie ein Add-on hinzufügen.
Eine ausführlichere Version dieses Tutorials finden Sie unter So richten Sie die Eclipse-Theia-Cloud-IDE-Plattform unter Ubuntu 18.04 ein.
Ein Ubuntu-18.04-Server mit Rootberechtigung und ein sekundäres Konto ohne Rootberechtigung; folgen Sie dazu dem Leitfaden für die Ersteinrichtung des Servers für Ubuntu 18.04.
Docker muss auf Ihrem Server installiert sein. Folgen Sie dazu Schritt 1 und Schritt 2 in So installieren Sie Docker unter Ubuntu 18.04.
Docker Compose muss auf Ihrem Server installiert sein. Folgen Sie dazu Schritt 1 in So installieren Sie Docker Compose unter Ubuntu 18.04.
Erstellen Sie das Verzeichnis, um alle Daten für Eclipse Theia zu speichern:
Erstellen Sie nginx-proxy-compose.yaml, um die Docker Compose-Konfiguration für nginx-proxy zu speichern:
Stellen Sie die Konfiguration bereit:
nginx-proxy geht davon aus, dass Log-In-Kombinationen in einer Datei vorliegen, die nach der verfügbar gestellten Domäne benannt wurde, im Format htpasswd vorliegen und unter dem Verzeichnis / etc / nginx / htpasswd im Container gespeichert sind.
Installieren Sie htpasswd:
Erstellen Sie eine Datei, um die Logins für Ihre Domäne zu speichern:
Führen Sie den folgenden Befehl mit einem Benutzernamen und einer Passwort-Kombination aus:
htpasswd fügt den Benutzernamen und ein Passwort-Paar mit Hash am Ende der Datei hinzu.
Erstellen Sie die Konfiguration für die Bereitstellung von Eclipse Theia:
Sie definieren einen einzigen Dienst namens eclipse-theia mit einem restart, der auf always eingestellt ist und theiaide / theia: next als Container-Image.
Außerdem setzen Sie init auf true.
Navigieren Sie zur Domäne, die Sie für Eclipse Theia verwenden.
Sie rufen Eclipse Theia auf und sehen die Editor-GUI.
Außerdem sehen Sie ein Schloss, das anzeigt, dass die Verbindung sicher ist.
Sie haben jetzt Eclipse Theia, eine vielseitige Cloud IDE, auf Ihrem Ubuntu-18.04-Server mit Docker Compose und nginx-proxy installiert.
So richten Sie die Eclipse-Theia-Cloud-IDE-Plattform unter Ubuntu 18.04 ein
3342
In diesem Tutorial stellen Sie Eclipse Theia für Ihren Ubuntu-18.04-Server mit Docker Compose, einem Tool zur Container-Orchestrierung, bereit.
Zum Schluss wird Eclipse Theia auf Ihrem Ubuntu-18.04-Server ausgeführt, über HTTPS verfügbar sein und der Benutzer wird sich anmelden müssen.
Ein Ubuntu-18.04-Server mit Rootberechtigung und einem sekundären Konto ohne Rootberechtigung.
Sie können dies einrichten, indem Sie sich an den Leitfaden zur Ersteinrichtung des Servers für Ubuntu 18.04 halten.
Für dieses Tutorial ist der Benutzer ohne Rootberechtigung < ^ > sammy < ^ > ​ ​ ​.
Folgen Sie Schritt 1 und Schritt 2 in So installieren Sie Docker unter Ubuntu 18.04.
Folgen Sie Schritt 1 in So installieren Sie Docker Compose unter Ubuntu 18.04.
So installieren und richten Sie LAMP auf Ubuntu 18.04 mit Ansible ein
3328
Dieser Leitfaden erklärt, wie Sie die Schritte in unserem Leitfaden So installieren Sie Linux, Apache, MySQL und PHP (LAMP) auf Ubuntu 18.04 mit Ansible automatisieren können.
Ein "LAMP" -Stack ist eine aus Open-Source-Software bestehende Gruppe, die normalerweise zusammen installiert wird, damit ein Server dynamische Websites und Web-Apps hosten kann.
Dieses Ansible-Playbook bietet eine Alternative zur manuellen Ausführung des Verfahrens, das in unserem Leitfaden So installieren Sie Linux, Apache, MySQL und PHP (LAMP) auf Ubuntu 18.04 beschrieben ist.
Installieren Sie die erforderlichen LAMP-Pakete.
Erstellen Sie einen neuen Apache VirtualHost und richten Sie einen dedizierten Dokumentstamm dafür ein.
Aktivieren Sie den neuen VirtualHost.
Deaktivieren Sie die Apache-Standard-Website, wenn die Variable disable _ default auf true gesetzt ist.
Richten Sie ein PHP-Testskript mithilfe der bereitgestellten Vorlage ein.
Wenn die Ausführung des Playbooks abgeschlossen ist, wird eine Web-PHP-Umgebung basierend auf den Optionen, die Sie in Ihren Konfigurationsvariablen definiert haben, auf Apache ausgeführt.
Als Erstes müssen Sie das LAMP-Playbook und seine Abhängigkeiten aus dem Repository do-community / ansible-playbooks abrufen.
Die Dateien, die wir brauchen, befinden sich im Ordner lamp _ ubuntu1804, der die folgende Struktur hat:
files / info.php.j2: Vorlagendatei zur Einrichtung einer PHP-Testseite auf dem Stamm des Webservers
Bearbeiten Sie die Variablendatei des Playbooks zur Anpassung der Konfigurationen sowohl von MySQL als auch von Apache.
Greifen Sie auf das Verzeichnis lamp _ ubuntu1804 zu und öffnen Sie die Datei vars / default.yml mit dem Befehlszeilen-Editor Ihrer Wahl:
app _ user: Ein Remotebenutzer ohne Rootberechtigung auf dem Ansible-Host, der als Besitzer der Anwendungsdateien festgelegt wird.
disable _ default: Ob die Standard-Website von Apache deaktiviert werden soll oder nicht.
Wenn die Ausführung des Playbooks abgeschlossen ist, gehen Sie zu Ihrem Webbrowser und greifen Sie auf die Host- oder IP-Adresse des Servers zu, die in den Playbook-Variablen konfiguriert ist, gefolgt von / info.php:
phpinfo page
Da diese Seite sensible Informationen über Ihre PHP-Umgebung enthält, wird empfohlen, diese vom Server zu entfernen, indem Sie den Befehl rm -f / var / www / info.php ausführen, sobald Sie mit der Einrichtung fertig sind.
Sie finden die LAMP-Server-Einrichtung in diesem Tutorial im Ordner lamp _ ubuntu1804 im Repository DigitalOcean Community Playbooks.
Die Variablendatei default.yml enthält Werte, die in den Playbook-Aufgaben verwendet werden, wie z. B. das Passwort für das MySQL-Konto root und den Domänennamen für die Konfiguration mit Apache.
files / info.php.j2
Die Datei info.php.j2 ist eine weitere Jinja-Vorlage, die für die Einrichtung eines Test-PHP-Skripts im Dokumentstamm des neu konfigurierten LAMP-Servers verwendet wird.
In diesem Leitfaden haben wir den Prozess der Installation und Einrichtung einer LAMP-Umgebung auf einem Remoteserver mithilfe von Ansible automatisiert.
Da jeder bei der Arbeit mit MySQL-Datenbanken und -benutzern in der Regel unterschiedliche Bedürfnisse hat, raten wir Ihnen, sich die offizielle Ansible-Dokumentation für weitere Informationen und Anwendungsfälle des Ansible-Moduls mysql _ user anzusehen.
So automatisieren Sie Aufgaben mit Cron unter Ubuntu 18.04
3538
Um diesen Leitfaden zu absolvieren, benötigen Sie Zugriff auf einen Computer, der Ubuntu 18.04 ausführt.
Dies könnte Ihr lokalen Rechner, ein virtueller Rechner oder ein virtueller privater Server sein.
Um dies einzurichten, folgen Sie unserem Leitfaden für die Ersteinrichtung des Servers unter Ubuntu 18.04.
Wenn Sie jedoch einen Ubuntu-Rechner verwenden, auf dem Cron nicht installiert ist, können Sie es mit APT installieren.
Um Cron auf einem Ubuntu-Rechner zu installieren, aktualisieren Sie zuerst den lokalen Paketindex des Computers.
Installieren Sie dann Cron mit dem folgenden Befehl:
Vergewissern Sie sich, dass auch die Ausführung im Hintergrund festgelegt ist:
Jedes Benutzerprofil im System kann über ein eigenes crontab verfügen, bei dem Aufträge geplant werden können. Das ist unter / var / spool / cron / crontabs / gespeichert.
Wenn Sie den Befehl crontab das erste Mal unter diesem Benutzerprofil ausführen, werden Sie aufgefordert, einen Standard-Texteditor für die Bearbeitung Ihres crontabs auszuwählen.
Geben Sie die Zahl ein, die dem Editor Ihrer Wahl entspricht.
Alternativ können Sie einfach die EINGABETASTE drücken, um die Standardwahl nano zu akzeptieren.
Nachdem Sie Ihre Auswahl getroffen haben, werden Sie zu einem neuen crontab gebracht, das einige auskommentierte Anweisungen zu dessen Anwendung enthält:
Wenn Sie crontab -e in Zukunft ausführen, wird Ihr crontab automatisch in diesem Texteditor aufgerufen.
Andernfalls können Sie das crontab vorerst speichern und schließen (CTRL + X, Y, dann die EINGABETASTE, wenn Sie nano ausgewählt haben).
< $> ​ note Hinweis: Auf Linux-Systemen ist ein weiteres crontab unter dem Verzeichnis / etc / gespeichert.
Sie können beispielsweise ein Shell-Skript schreiben, um Daten-Backups zu einer Objekt-Speicherlösung zu senden, und es dann mit Cron automatisieren.
So verwenden Sie Datenbankmigrationen und Seeders für die abstrakte Datenbank-Einrichtung in Laravel
3695
Migrationen und Seeders sind leistungsfähige Datenbank-Dienstprogramme, die vom Laravel-PHP-Framework bereitgestellt werden, um Entwicklern einen schnellen Bootstrap, eine schnelle Löschung und neue Erstellung der Datenbank einer Anwendung zu ermöglichen.
Diese Dienstprogramme minimieren Probleme der Datenbankinkonsistenz, die auftreten können, wenn mehrere Entwickler an derselben Anwendung arbeiten: Neue Mitwirkende müssen nur ein paar artisan-Befehle ausführen, um die Datenbank auf einer neuen Installation einzurichten.
In diesem Leitfaden erstellen Sie Migrationen und Seeders, um eine Datenbank einer Laravel-Demo-Anwendung mit Beispieldaten zu füllen. Am Ende können Sie Ihre Datenbank-Tabellen nur mit artisan-Befehlen so oft Sie möchten löschen und neu erstellen.
Befolgen Sie zur Installation von Docker auf Ihrem Server Schritt 1 und Schritt 2 in So installieren und verwenden Sie Docker unter Ubuntu 18.04.
< $> note Anmerkung: In diesem Leitfaden verwenden wir eine containerisierte Entwicklungsumgebung, die von Docker Compose verwaltet wird, um die Anwendung auszuführen. Sie können die Anwendung jedoch auch auf einem LEMP-Server ausführen.
Um dies einzurichten, können Sie unseren Leitfaden So installieren und konfigurieren Sie Laravel mit LEMP unter Ubuntu 18.04 ausführen.
Wir beschäftigen uns mit dem Zweig tutorial-02, der eine Docker-Compose-Einrichtung beinhaltet, um die Anwendung auf Containern auszuführen.
In diesem Beispiel laden wir die Anwendung in unseren Home-Ordner herunter, aber Sie können jedes Verzeichnis Ihrer Wahl verwenden:
Da wir den Anwendungscode als .zip-Datei heruntergeladen haben, benötigen wir den Befehl unzip, um ihn zu entpacken. Wenn Sie es nicht vor Kurzem getan haben, aktualisieren den lokalen Paketindex Ihres Rechners:
Installieren Sie dann das unzip-Paket:
Entzippen Sie danach den Inhalt der Anwendung:
Benennen Sie dann das entpackte Verzeichnis für einen leichteren Zugriff in < ^ > travellist-demo < ^ > ​ ​ um:
In Laravel wird eine .env-Datei verwendet, um umgebungsabhängige Konfigurationen einzurichten, wie Anmeldeangaben und alle Informationen, die zwischen Bereitstellungen variieren können.
Jede Installation in einer neuen Umgebung erfordert eine maßgeschneiderte Umgebungsdatei, um Dinge wie Datenbank-Verbindungseinstellungen, Debug-Optionen, Anwendungs-URL und andere Objekte festzulegen, die je nach der Umgebung variieren können.
So sieht Ihre .env-Datei jetzt aus:
Die aktuelle .env-Datei aus der Demo-Anwendung travellist enthält Einstellungen, um die containerisierte Umgebung zu verwenden, die wir im letzten Teil dieser Serie mit Docker Compose erstellt haben.
Sie müssen keine dieser Werte ändern, aber Sie können gerne DB _ DATABASE, DB _ USERNAME und DB _ PASSWORD ändern, da diese von unserer Datei docker-compose.yml automatisch abgerufen werden, um die Entwicklungsdatenbank einzurichten.
Stellen Sie jedoch sicher, dass die Variable DB _ HOST unverändert bleibt, da sie auf den Namen unseres Datenbank-Dienstes innerhalb der Docker-Compose-Umgebung verweist.
Wenn Sie Änderungen an der Datei vornehmen, stellen Sie sicher, dass Sie sie durch Drücken von STRG + X, Y, dann der EINGABETASTE gespeichert und geschlossen haben.
< $> note Anmerkung: Wenn Sie sich dafür entschieden haben, die Anwendung auf einem LEMP-Server auszuführen, müssen Sie die hervorgehobenen Werte ändern, um Ihre eigenen Datenbankeinstellungen, einschließlich der Variablen DB _ HOST, zu berücksichtigen.
Schritt 3 - Installieren von Anwendungsabhängigkeiten mit Composer
Jetzt verwenden wir Composer, das Abhängigkeitsmanagement-Tool von PHP, um die Abhängigkeiten der Anwendung zu installieren und sicherzustellen, dass wir artisan-Befehle ausführen können.
Rufen Sie Ihre Docker-Compose-Umgebung mit dem folgenden Befehl auf.
Dadurch werden das Image travellist für den app-Dienst erstellt und die zusätzlichen Docker-Images eingefügt, die von den Diensten nginx und db benötigt werden, um die Anwendungsumgebung zu erstellen:
Sobald der Vorgang abgeschlossen ist, können wir Composer ausführen, um die Abhängigkeiten der Anwendung zu installieren.
Um composer und andere Befehle im app-Dienstcontainer auszuführen, verwenden wir docker-compose exec.
Der Befehl exec ermöglicht es uns, jeden Befehl unserer Wahl auf Containern auszuführen, die von Docker Compose verwaltet werden.
Es verwendet die folgende Syntax: docker-compose exec < ^ > service _ name command < ^ >.
< $> note Anmerkung: Falls Sie sich entschieden haben, einen LEMP-Server zu verwenden, um die Demo-Anwendung auszuführen, sollten Sie den Teil docker-compose exec app der Befehle in diesem Leitfaden ignorieren.
Statt beispielsweise den folgenden Befehl auszuführen, wie er geschrieben ist, würden Sie nur Folgendes ausführen:
Um composer install im app-Container auszuführen, führen Sie Folgendes aus:
Wenn Composer die Installation der Anwendungsabhängigkeiten abgeschlossen hat, können Sie artisan-Befehle ausführen.
Um zu testen, ob die Anwendung eine Verbindung mit der Datenbank herstellen kann, führen Sie folgenden Befehl aus, der alle bereits vorhandenen Tabellen bereinigen wird:
Dieser Befehl verwirft alle bereits vorhandenen Tabellen auf der konfigurierten Datenbank.
Wenn er erfolgreich ausgeführt wird und die Anwendung eine Verbindung mit der Datenbank herstellen kann, sehen Sie eine Ausgabe wie diese:
Nachdem Sie nun die Anwendungsabhängigkeiten mit Composer installiert haben, können Sie das artisan-Tool verwenden, um Migrationen und Seeders zu erstellen.
Schritt 4 - Datenbank-Migrationen erstellen
Das artisan-Befehlszeilentool, das mit Laravel geliefert wird, enthält eine Reihe von Hilfsbefehlen, die zur Verwaltung der Anwendung und dem Bootstrap neuer Klassen verwendet werden können.
Um eine neue Migrationsklasse zu generieren, können wir den Befehl make: migration wie folgt verwenden:
Laravel leitet die auszuführende Operation (< ^ > create < ^ >), den Namen der Tabelle (< ^ > places < ^ >) und ob diese Migration eine neue Tabelle erstellt oder nicht basierend auf dem beschreibenden Namen ab, der dem Befehl make: migration bereitgestellt wird.
Dadurch wird eine neue Datei im Verzeichnis database / migrations der Anwendung generiert.
Der in der automatisch generierten Datei enthaltene Zeitstempel wird von Laravel verwendet, um zu bestimmen, in welcher Reihenfolge Migrationen ausgeführt werden sollen.
Verwenden Sie den Texteditor Ihrer Wahl, um die generierte Migrationsdatei zu öffnen.
Vergessen Sie nicht, den hervorgehobenen Wert durch Ihren eigenen Migrationsdatei-Namen zu ersetzen:
Die generierte Migrationsdatei enthält eine Klasse namens CreatePlacesTable:
Diese Klasse verfügt über zwei Methoden: up und down.
Beide Methoden enthalten Bootstrap-Code, den Sie erweitern können, um das anzupassen, was geschieht, wenn die Migration ausgeführt wird und wenn sie zurückgesetzt wird.
Wir ändern die Methode up, damit die Tabelle places die Struktur wiedergibt, die wir bereits in der aktuellen Version der Anwendung verwenden:
id: primäres Schlüsselfeld
name: Name des Ortes
visited: ob dieser Ort bereits besucht wurde oder nicht
Der Laravel Schema Builder macht Methoden zum Erstellen, Aktualisieren und Löschen von Tabellen in einer Datenbank verfügbar.
Die Klasse Blueprint definiert die Struktur der Tabelle und bietet verschiedene Methoden, um die Definition jedes Tabellenfeldes zusammenzufassen.
Der automatisch generierte Code richtet ein primäres ID-Feld namens id ein. Die Methode timestamps erstellt zwei datetime-Felder, die automatisch von den zugrunde liegenden Datenbank-Klassen aktualisiert werden, wenn Daten in diese Tabelle eingefügt oder aktualisiert werden.
Zusätzlich müssen wir einen Namen und ein visited-Feld aufnehmen.
Unser Namens-Feld ist vom Typ Zeichenfolge und unser visited-Feld vom Typ Boolean.
Außerdem geben wir einen Standardwert von 0 für das visited-Feld ein. Wenn kein Wert übergeben wird, bedeutet es dann, dass der Ort noch nicht besucht wurde.
So sieht die Methode up jetzt aus:
< $> note Anmerkung: Die vollständige Liste der verfügbaren Spaltentypen finden Sie in der Laravel-Dokumentation.
Nachdem Sie die beiden hervorgehobenen Zeilen auf Ihrem eigenen Migrations-Skript inkludiert haben, speichern und schließen Sie die Datei.
Ihre Migration kann jetzt über artisan migrate ausgeführt werden.
Dies würde jedoch nur eine leere Tabelle erstellen; Sie müssen auch Beispieldaten für Entwicklung und Test einfügen können.
Im nächsten Schritt sehen Sie, wie Sie das mithilfe von Datenbank-Seeders tun können.
Schritt 5 - Databank-Seeders erstellen
Ein Seeder ist eine spezielle Klasse, die zum Generieren und Einfügen von Beispieldaten (Seeds) in einer Datenbank verwendet wird.
Dies ist ein wichtiges Merkmal in Entwicklungsumgebungen, denn damit können Sie die Anwendung mit einer frischen Datenbank neu erstellen - mit Beispielwerten, die Sie sonst jedes Mal manuell einfügen müssen, wenn die Datenbank neu erstellt wird.
Wir verwenden jetzt den Befehl artisan, um eine neue Seeder-Klasse für unsere Tabelle places namens PlacesTableSeeder zu generieren:
Der Befehl erstellt eine neue Datei namens PlacesTableSeeder.php im Verzeichnis database / seeds.
Öffnen Sie diese Datei mit dem Texteditor Ihrer Wahl:
So sieht die automatisch generierte Datei PlacesTableSeeder.php aus:
Unsere neue Seeder-Klasse enthält eine leere Methode namens run.
Diese Methode wird aufgerufen, wenn der Artisan-Befehl db: seed ausgeführt wird.
Sie müssen die Methode run bearbeiten, um Anweisungen zum Einfügen von Beispieldaten in die Datenbank aufzunehmen.
Wir verwenden den Laravel query builder ​ ​ ​, um diesen Vorgang zu optimieren.
Der Laravel query builder bietet eine fließende Schnittstelle für Datenbankoperationen wie Einfügen, Aktualisieren, Löschen und Abrufen von Daten. Er schützt zudem vor SQL-Injection-Angriffen.
Der query builder wird von der DB facade ​ ​ ​ verfügbar gemacht - ein statischer Proxy für zugrunde liegende Datenbank-Klassen im Dienstcontainer.
Jetzt erstellen wir eine statische Klassen-Variable, um alle Beispielorte aufzunehmen, die wir als Array in die Datenbank einfügen möchten.
Dadurch können wir eine foreach-Schleife verwenden, um alle Werte zu durchlaufen und jeden einzelnen mit dem query builder in die Datenbank einzufügen.
Wir nennen diese Variable $places:
Als Nächstes müssen wir eine use-Anweisung am Anfang unserer PlacesTableSeeder-Klasse aufnehmen, um die Referenzierung der DB facade im gesamten Code zu erleichtern:
Wir können jetzt die Array-Werte $places mit einer foreach-Schleife durchlaufen und jeden einzelnen mit dem query builder in unsere Tabelle places einfügen:
Die foreach-Schleife durchläuft jeden Wert des statischen Arrays $places.
Bei jedem Durchlauf verwenden wir die DB facade, um eine neue Zeile in der Tabelle places einzufügen.
In das Namens-Feld geben wir den Namen des Ortes ein, den wir gerade aus dem Array $places erhalten haben, und in das visited-Feld einen beliebigen Wert von entweder 0 oder 1.
So sieht die volle Klasse PlacesTableSeeder nach allen Aktualisierungen aus:
Speichern und schließen Sie die Datei, wenn Sie diese Änderungen vorgenommen haben.
Seeder-Klassen werden nicht automatisch im der Anwendung geladen.
Bearbeiten Sie die Hauptklasse von DatabaseSeeder, um einen Aufruf zu dem Seeder aufzunehmen, den wir gerade erstellt haben.
Öffnen Sie die Datei database / seeds / DatabaseSeeder.php mit nano oder Ihrem bevorzugten Editor:
Die Klasse DatabaseSeeder sieht wie jeder andere Seeder aus: sie erweitert die Seeder-Klasse und hat eine run-Methode.
Diese Methode aktualisieren wir, um einen Aufruf von PlacesTableSeeder aufzunehmen.
Aktualisieren Sie die aktuelle run-Methode in Ihrer Klasse DatabaseSeeder, indem Sie die auskommentierte Zeile löschen und sie durch den folgenden hervorgehobenen Code ersetzen:
So sieht die volle Klasse DatabaseSeeder nach der Aktualisierung aus:
Speichern und schließen Sie die Datei, wenn Sie mit der Aktualisierung des Inhalts fertig sind.
Jetzt sind wir mit der Einrichtung einer Migration und eines Seeders für unsere Tabelle places fertig.
Im nächsten Schritt sehen wir uns ihre Ausführung an.
Schritt 6 - Datenbank-Migrationen und Seeders ausführen
Vergewissern Sie sich zuerst, dass Ihre Anwendung ausgeführt wird.
Wir richten den Verschlüsselungsschlüssel der Anwendung ein und greifen dann von einem Browser auf die Anwendung zu, um den Webserver zu testen.
Um den für Laravel erforderlichen Verschlüsselungsschlüssel zu generieren, können Sie den Befehl artisan key: generate verwenden:
Sobald der Schlüssel generiert wurde, können Sie auf die Anwendung zugreifen, indem Sie Ihren Browser auf Ihren Server-Hostname oder die IP-Adresse auf Port 8000 verweisen:
MySQL error
Das bedeutet, dass die Anwendung eine Verbindung mit der Datenbank herstellen kann, jedoch keine Tabelle namens places finden konnte.
Wir erstellen jetzt die Tabelle places mit dem folgenden artisan-Befehl migrate:
Sie werden feststellen, dass einige weitere Migrationen zusammen mit der Migration create _ places _ table ausgeführt wurden, die wir eingerichtet haben.
Diese Migrationen werden automatisch generiert, wenn Laravel installiert ist.
Obwohl wir diese zusätzlichen Tabellen jetzt nicht verwenden, werden sie in Zukunft benötigt, wenn wir die Anwendung um registrierte Benutzer und geplante Aufträge erweitern.
Jetzt können Sie sie vorerst so belassen.
Zu diesem Zeitpunkt ist unsere Tabelle noch leer.
Wir müssen den Befehl db: seed ​ ​ ​ ausführen, um Seeding für die Datenbank mit unseren Beispielorten durchzuführen.
Dadurch werden unser Seeder ausgeführt und die Beispielwerte eingefügt, die wir in unserer Klasse PlacesTableSeeder definiert haben.
Laden Sie jetzt die Anwendungsseite in Ihrem Browser neu.
Wann immer Sie wieder von vorne beginnen müssen, können Sie alle Datenbank-Tabellen folgendermaßen verwerfen:
Um die Anwendungs-Migrationen auszuführen und Seeding für die Tabellen mit einem einzelnen Befehl durchzuführen, können Sie Folgendes verwenden:
Wenn Sie eine Migration zurücksetzen möchten, können Sie Folgendes ausführen:
Dadurch wird die Methode down für jede Migrationsklasse im Ordner migrations ausgelöst.
Üblicherweise werden alle Tabellen entfernt, die durch Migrations-Klassen erstellt wurden, und es bleiben nur jene Tabellen übrig, die von Hand erstellt wurden.
Der Befehl Zurücksetzen ist besonders nützlich, wenn Sie Änderungen an Anwendungsmodellen vornehmen und ein Befehl db: wipe nicht verwendet werden kann - beispielsweise wenn mehrere Systeme von der gleichen Datenbank abhängen.
In diesem Leitfaden haben Sie gelernt, wie Sie das Einrichten von Entwicklungs- und Test-Datenbänken für eine Laravel-6-Anwendung mithilfe von Datenbank-Migrationen und Seeders erleichtern können.
In der Laravel-Dokumentation finden Sie weitere Informationen zur Verwendung des query builder ​ ​ ​ und von Eloquent models, um das Datenbank-Schema Ihrer Anwendung noch weiter zusammenzufassen.
Installieren von Nginx auf CentOS 8
3859
Nginx ist einer der beliebtesten Webserver der Welt und Host einiger der größten und beliebtesten Websites im Internet.
Nginx ist in den meisten Fällen ressourcenschonender als Apache und kann als Webserver oder Reverse-Proxy verwendet werden.
Dieser Leitfaden zeigt Ihnen, wie Sie Nginx auf einem CentOS-8-Server installieren.
Um Nginx zu installieren, verwenden Sie den Paketmanager dnf, den neuen Standard-Paketmanager auf CentOS 8.
Danach wird dnf Nginx und alle erforderlichen Abhängigkeiten auf Ihrem Server installieren.
Wenn die Installation abgeschlossen ist, führen Sie die folgenden Befehle aus, um den Server zu aktivieren und zu starten:
Dadurch wird Nginx beim Boot des Systems gestartet.
Schritt 2 - Anpassen der Firewalleinstellungen
Wenn Sie die Firewall firewalld gemäß unserem Leitfaden zur Ersteinrichtung des Servers für CentOS 8 aktiviert haben, müssen Sie die Firewalleinstellungen anpassen, um externe Verbindungen auf Ihrem Nginx Webserver zu ermöglichen, der standardmäßig auf Port 80 läuft.
Führen Sie den folgenden Befehl aus, um dauerhaft HTTP-Verbindungen auf Port 80 zu aktivieren:
Um zu verifizieren, dass der http Firewall-Service korrekt hinzugefügt wurde, können Sie Folgendes ausführen:
Um die Änderungen anzuwenden, müssen Sie den Firewall-Service neu laden:
Nun ist Ihr Nginx Server komplett installiert und bereit für den Zugriff von externen Besuchern.
Sie können nun testen, ob Ihr Nginx Webserver installiert ist und läuft, indem Sie auf die öffentliche IP-Adresse oder den Domänennamen Ihres Servers von Ihrem Webbrowser aus zugreifen.
< $> note Anmerkung: Falls Sie DigitalOcean als DNS-Hosting-Anbieter verwenden, konsultieren Sie unsere Produktdokumente für detaillierte Anweisungen, wie Sie einen neuen Domänennamen einrichten und diesen ihrem Server zuweisen.
Schritt 4 - Verwalten der Nginx-Prozesse
Jetzt, wo Ihr Webserver in Betrieb ist, können Sie überprüfen, wie der Nginx-Service über systemctl verwaltet werden kann.
Wenn Sie Ihren Webserver anhalten möchten, können Sie Folgendes anwenden:
Um den Service anzuhalten und anschließend erneut zu starten, können Sie Folgendes anwenden:
Nginx kann auch Konfigurationsänderungen neu laden, ohne die Verbindungen zu unterbrechen.
Standardmäßig ist Nginx so konfiguriert, dass es beim Booten des Servers automatisch startet.
Zum Reaktivieren des Services, damit Nginx wieder beim Booten startet, können Sie Folgendes anwenden:
Schritt 5 - Kennenlernen der wichtigen Nginx-Dateien und -Verzeichnisse
Nachdem Sie nun wissen, wie Sie den Nginx-Service verwalten, nehmen Sie sich einige Minuten Zeit, um sich mit einigen wichtigen Verzeichnissen und Dateien vertraut zu machen.
Inhalt
/ usr / share / nginx / html: Der eigentliche Web-Inhalt, der standardmäßig nur aus der Nginx-Standardseite besteht, die Sie zuvor gesehen haben, wird aus dem Verzeichnis / usr / share / nginx / html bedient.
Das kann durch die Anpassung der Konfigurationsdateien von Nginx geändert werden.
Server-Konfiguration
/ etc / nginx: Das Nginx-Konfigurationsverzeichnis.
Hier befinden sich alle Konfigurationsdateien von Nginx.
/ etc / nginx / nginx.conf: Die Hauptkonfigurationsdatei von Nginx.
Dies kann modifiziert werden, um die globale Konfiguration von Nginx zu ändern.
/ etc / nginx / conf.d /: Dieses Verzeichnis enthält Serverblock-Konfigurationsdateien, in denen Sie die Websites definieren können, die innerhalb von Nginx gehostet werden.
Eine typische Vorgehensweise hierbei ist, jede Website in einer separaten Datei zu speichern, die nach dem Domänennamen der Website benannt wird, wie z. B. your _ domain.conf.
Serverprotokolle
/ var / log / nginx / access.log: Jede Anfrage auf Ihrem Webserver wird in dieser Protokolldatei aufgezeichnet, sofern Nginx nicht anders konfiguriert ist.
/ var / log / nginx / error.log: In diesem Protokoll werden alle Nginx-Fehler aufgezeichnet.
Schritt 6 - Einrichten von Serverblocks (optional)
Wenn Sie mehrere Websites auf dem gleichen Nginx Webserver hosten möchten, müssen Sie Serverblocks einrichten.
Nginx-Serverblocks arbeiten ähnlich wie virtuelle Apache-Hosts: Sie ermöglichen es einem einzelnen Server, auf mehrere Domänennamen zu antworten und für jeden von ihnen unterschiedliche Inhalte bereitzustellen.
Auf CentOS 8 werden Serverblocks in .conf-Dateien definiert, die sich in / etc / nginx / conf.d befinden.
Richten Sie einen Serverblock für eine Domäne namens your _ domain ein.
Standardmäßig ist Nginx auf CentOS 8 konfiguriert, um Dokumente aus einem Verzeichnis in / usr / share / nginx / html zu bedienen.
Das eignet sich gut für eine Website, kann aber umständlich werden, wenn Sie mehrere hosten.
Statt / usr / share / nginx / html zu ändern, erstellen Sie eine Verzeichnisstruktur innerhalb / var / www für Ihre Website your _ domain und lassen dabei / usr / share / nginx / html als Standardverzeichnis stehen, das bereitgestellt wird, wenn eine Client-Anfrage zu keinen übereinstimmenden Websites führt.
Erstellen Sie das Verzeichnis für your _ domain wie folgt, wobei Sie das Kennzeichen -p nutzen, um alle erforderlichen übergeordneten Verzeichnisse zu erstellen:
Als Nächstes weisen Sie die Eigentumsrechte des Verzeichnisses mit der Umgebungsvariablen $USER zu, die auf Ihren aktuellen Systembenutzer verweisen sollte:
Als Nächstes erstellen Sie eine index.html Probeseite, um die Serverblock-Konfiguration zu testen.
Nun können Sie nano verwenden, um die index.html Probedatei zu erstellen:
Fügen Sie den folgenden HTML-Code in die Datei ein:
Wenn Sie nano verwendet haben, können Sie hierzu STRG + X, Y, und dann ENTER drücken.
Damit Nginx den Inhalt bereitstellen kann, müssen Sie einen Serverblock mit den korrekten Direktiven erstellen, die auf Ihre benutzerdefinierte Web-Root verweisen.
Hierzu erstellen Sie einen neuen Serverblock in / etc / nginx / conf.d / < ^ > your _ domain.conf < ^ >:
Fügen Sie den folgenden Konfigurationsblock ein:
Speichern und schließen Sie die Datei, wenn Sie mit der Bearbeitung des Inhalts fertig sind.
Um sicherzustellen, dass es keine Syntaxfehler in Ihren Nginx-Dateien gibt, führen Sie Folgendes aus:
Wenn keine Probleme bestehen, wird die folgende Ausgabe angezeigt:
Wenn Ihr Konfigurationstest bestanden ist, starten Sie Nginx neu, um Ihre Änderungen zu aktivieren:
Bevor Sie die Änderungen von Ihrem Browser testen können, müssen Sie die SELinux Sicherheitskontexte Ihres Servers aktualisieren, damit Nginx den Inhalt aus dem Verzeichnis / var / www / < ^ > your _ domain < ^ > bereitstellen kann.
Mit dem folgenden Befehl können Sie Ihr benutzerdefiniertes Dokumentverzeichnis als HTTP-Inhalt bereitstellen:
Jetzt können Sie Ihr benutzerdefiniertes Domäne-Setup testen, indem Sie zu http: / / < ^ > your _ domain < ^ >, navigieren. Dort sehen Sie etwa Folgendes:
Nginx Serverblock
Diese Seite ist die Darstellung des HTML-Codes, den wir im benutzerdefinierten, für den Server erstellten Dokumentverzeichnis definiert haben.
Wenn Sie diese Seite sehen können, bedeutet dies, dass Ihr Nginx-Server korrekt für die Bedienung Ihrer Domäne konfiguriert ist.
In diesem Leitfaden haben Sie gelernt, wie man Nginx, einen Hochleistungs-Webserver und Reverse-Proxy, installiert und einrichtet.
Sie konnten überprüfen, wie der auf Ihrem Server laufende Nginx-Service verwaltet wird und welche Hauptverzeichnisse Nginx zur Speicherung von Konfigurationsdateien, Inhalten und Protokollen verwendet.
Nun haben Sie zahlreiche Möglichkeiten für verschiedene Inhalte und Technologien, die Sie auf den auf Ihrem Webserver gehosteten Websites anwenden können.
Verwendung von nsh zur sicheren Ausführung von Remotebefehlen auf Ubuntu 18.04
3825
Es kann oft schwierig sein, täglich mehrere Rechner zu betreiben.
Obwohl Secure Shell (SSH) eine gute Wahl für den Remote-Zugriff ist, hat die Ausführung selbst einige Nachteile, was die Bequemlichkeit und Sicherheit anbelangt.
Beispielsweise benötigen Remote-Rechner eine öffentliche IP-Adresse und einen weitergeleiteten Port, damit auf sie zugegriffen werden kann, wodurch sie dem Internet oder zumindest einem größeren Netzwerk ausgesetzt sind.
Dies ist insbesondere dann bedenklich, wenn Sie zur Authentifizierung ein Passwort anstelle eines öffentlichen und privaten Schlüsselpaares verwenden.
Wenn Sie außerdem den öffentlichen Schlüssel des Remote-Rechners nicht im Voraus kennen, könnten Sie für einen "Man-in-the-Middle-Angriff" anfällig sein.
Auch haben viele Remote-Rechner, auf die Sie zugreifen möchten, entweder keine öffentliche IP-Adresse oder eine dynamische IP-Adresse, die Sie möglicherweise nicht kennen.
Außerdem benötigt SSH jeweils eine Verbindung pro Remote-Sitzung.
Wenn ein Benutzer einen einzelnen Befehl über Hunderte oder sogar Tausende von Rechnern ausführen muss, muss er sich zunächst mit jedem Rechner mittels TCP-Handshake verbinden, was weniger effizient ist.
NKN Shell oder nsh ist eine Alternative zu SSH, die eine bequeme und sichere Möglichkeit bietet, Remote-Befehle auszuführen. Nsh nutzt das globale öffentliche Netzwerk von NKN, das eine sichere und dezentrale Datenübertragung bereitstellt.
Die Architektur verwendet individuelle Adressen, die einen öffentlichen Schlüssel sowohl zum Routing als auch zur End-to-End-Verschlüsselung ohne Public-Key-Infrastruktur (PKI) enthalten.
Für das Netzwerk ist es auch nicht erforderlich, dass der Remote-Server eine öffentliche IP-Adresse hat.
Der Remote-Server benötigt nur Zugang zum Internet und muss in der Lage sein, ausgehende HTTP- und Websocket-Verbindungen herzustellen.
Dadurch sind Ihre Remote-Rechner nicht dem offenen Internet ausgesetzt.
In diesem Tutorial nutzen Sie die Anwendungen NKN shell daemon und NKN Shell Client Xterm, um Befehle auf einem Remote-Rechner auszuführen.
Dazu installieren und konfigurieren Sie den NKN Shell Daemon auf einem Remote-Rechner mit Internet-Zugang, generieren ein Schlüsselpaar und erstellen Ihre Verbindung von einem Client.
Einen Webbrowser, der auf Ihrem lokalen Rechner installiert ist.
Schritt 1 - Installieren von NKN Shell auf einem Remote-Server
Installieren Sie zunächst den NKN shell daemon (nsd) auf Ihrem Server.
Diese Anwendung ruft nkn-multiclient auf, der sich mit dem öffentlichen Netzwerk von NKN verbindet und eine Adresse zum Routing erhält.
Der Daemon empfängt dann eingehende Shell-Befehle von authentifizierten und erlaubten Clients, führt diese Befehle aus und sendet danach Ergebnisse zurück.
Beginnen Sie mit dem Herunterladen des neuesten pre-built nshd binary from GitHub ​ ​:
Dekomprimieren Sie die Datei:
Verschieben Sie dann die Dateien in das Verzeichnis / usr / local / bin, damit sie systemweit verfügbar sind:
Als Nächstes konfigurieren Sie dies als Daemon-Prozess mit Systemd, damit es neu gestartet wird, wenn der Server zurückgesetzt wird.
Erstellen Sie eine Datei namens nshd.service in / etc / systemd / system:
Fügen Sie der Datei folgende Dienst-Definition hinzu, um den Dienst zu konfigurieren:
Erfahren Sie mehr über Systemd Unit-Dateien in Verstehen von Systemd Units und Unit-Dateien.
Aktivieren und starten Sie dann den nshd-Dienst mit den folgenden Befehlen:
Führen Sie den folgenden Befehl aus, um sicherzustellen, dass der Dienst aktiv und gestartet ist:
Sie sehen, dass der Status aktiv ist:
Um sich mit Ihrem Server zu verbinden, müssen Sie seine NKN-Adresse bereitstellen, die Sie im Output des vorherigen Befehls finden.
Sie können auch den folgenden Befehl ausführen, um die Adresse zu erhalten:
Ihre Adresse wird Ihnen angezeigt:
Notieren Sie sich diese Adresse, da Sie sie brauchen, um sich mit Ihrem Server zu verbinden.
Nachdem nun der Daemon läuft und empfängt, können Sie den webbasierten Client konfigurieren, um mit dem Server zu kommunizieren.
Schritt 2 - Konfigurieren von Berechtigungen für NKN Shell Client
Sie benötigen einen kompatiblen Client, der sich mit dem Remote-Rechner verbinden kann.
In diesem Tutorial verwenden Sie NKN Shell Client Xterm, einen webbasierten NKN Shell-Client.
Es gibt einige verschiedene Möglichkeiten, um diesen auszuführen:
Verwenden Sie die gehostete Version unter https: / / nsh.nkn.org /. Beachten Sie, dass diese Webseite zwar auf einem Server gehostet wird, aber eigentlich eine rein lokale Webanwendung ist, die in Ihrem Browser läuft.
Nehmen Sie den Quellcode und hosten Sie ihn selbst.
Verwenden Sie die nShell Chrome Extension.
In diesem Tutorial verwenden Sie die gehostete Version.
Öffnen Sie den Webbrowser auf Ihrem lokalen Rechner und navigieren Sie zu https: / / nsh.nkn.org. Sie sehen einen Startbildschirm:
The Shell Client
Klicken Sie Generate New Key Pair.
Ihre Schlüssel werden generiert und wie im folgenden Bild angezeigt:
Generiertes Schlüsselpaar
< $> note Anmerkung: Wenn Sie ein neues Schlüsselpaar generieren, sehen Sie einen Secret Seed.
Verwahren Sie diesen Secret Seed sicher auf - genauso, wie Sie es mit Ihrem SSH-Schlüssel tun würden.
Jeder, der über den Secret Seed verfügt, kann ihn zum Regenerieren Ihres öffentlichen Schlüssels und zum Ausführen von Befehlen auf Ihren Remote-Rechnern verwenden.
Ihr Browser speichert diesen Seed, aber Sie sollten ihn an einem sicheren Ort aufbewahren, um ihn bei Bedarf erneut auf einem neuen Rechner verwenden zu können.
Speichern Sie den Secret Seed an einem sicheren Ort.
Sie können ihn später verwenden, um Ihren öffentlichen Schlüssel zu regenerieren, damit Sie sich von einem anderen Client-Rechner aus verbinden können.
Da es sich um ein neues Schlüsselpaar handelt, müssen Sie den öffentlichen Schlüssel der Datei / etc / nshd / authorized _ pubkeys auf Ihrem Server hinzufügen.
/ etc / nshd / authorized _ pubkeys hat eine ähnliche Rolle wie die Datei ~ / authorized _ keys, die steuert, welche öffentlichen SSH-Schlüssel sich anmelden können.
Die Datei authorized _ pubkeys kann bestimmen, welcher Benutzer mit einem Schlüssel verbunden ist.
Loggen Sie sich in diesem Tutorial aus Sicherheitsgründen als Benutzer ohne Rootberechtigung ein, sodass Sie den generierten öffentlichen Schlüssel mit dem sammy-Benutzer, den Sie in der Anleitung zum Server-Setup kreiert haben, verbinden.
Um einen Benutzer mit dem öffentlichen Schlüssel zu verbinden, müssen Sie die User-ID (UID) und Group-ID (GID) dieses Benutzers bereitstellen.
Führen Sie den folgenden Befehl auf Ihrem Server aus, während Sie als sammy-Benutzer angemeldet sind:
Sie sehen die UID und GID des Benutzers:
Öffnen Sie nun die Datei authorized _ pubkeys in Ihrem Editor:
Fügen Sie eine einzelne Zeile hinzu, die den öffentlichen Schlüssel sowie die UID und GID enthält, die durch Leerstellen getrennt sind:
Überprüfen Sie, ob die Datei den richtigen Inhalt enthält:
Sie sehen Ihren Schlüssel auf dem Bildschirm:
Starten Sie dann den Daemon nshd neu, um die Änderungen anzuwenden:
Führen Sie nun einen Test aus, indem Sie sich mit dem Server verbinden und einen Befehl ausführen.
Schritt 3 - Senden eines Befehls an den Remote-Rechner und Empfangen einer Antwort
Geben Sie in NKN Shell Client Ihre Remote-nshd-Adresse aus Schritt 1 sowie einen optionalen Client-Identifikator ein:
Die nsh-Website mit der ausgefüllten Remote-Adresse
Klicken Sie auf Connect, um die Verbindung herzustellen.
Sie werden mit Ihrem Remote-Rechner verbunden und bekommen eine Terminal-Eingabeaufforderung im Browser angezeigt.
Sie können nun weiter vorgehen wie sonst mit SSH.
Führen Sie beispielsweise den folgenden Befehl aus, um in das Verzeichnis / etc / nshd zu wechseln:
Dann listen Sie den Inhalt auf:
Sie sehen den Inhalt des Verzeichnisses:
Sie können sich mit der Eingabe von exit trennen.
Wenn Sie sich erneut verbinden möchten, gehen Sie zurück zur Web-Oberfläche und geben Ihre Verbindungsdaten ein.
Wenn Sie ein neues Schlüsselpaar generieren, müssen Sie den neuen öffentlichen Schlüssel Ihrem Server hinzufügen.
In diesem Tutorial haben Sie nsh installiert und konfiguriert, um Befehle sicher und bequem zu einem Remote-Rechner zu senden. Nsh bietet eine hervorragende Möglichkeit, auf Ihre Remote-Rechner zuzugreifen, wenn Sie schnell einen Befehl ausführen müssen, um den neuesten Status eines Dienstes zu erhalten oder um einen Blick auf die Konfigurationseinstellungen zu werfen.
Die Anwendung befindet sich auf dem globalen öffentlichen Netzwerk von NKN und ist kostenfrei nutzbar, sodass Sie sie heute in Ihre eigene Anwendung oder Ihren Workflow einbinden können.
Erkunden Sie auch nkn-tunnel, das SSH und andere TCP-basierte Anwendungen unterstützt.
Grundlegendes zu Generatoren in JavaScript
3855
Im ECMAScript 2015 wurden Generatoren in die JavaScript-Sprache eingeführt.
Ein Generator ist ein Prozess, der angehalten und wieder aufgenommen und mehrere Werte liefern kann.
Ein Generator in JavaScript besteht aus einer Generator-Funktion, die ein iterierbares Generator-Objekt zurückgibt.
Generatoren können den Zustand aufrechterhalten, was eine effiziente Methode zur Erstellung von Iteratoren darstellt. Sie sind in der Lage, mit unendlichen Datenströmen umzugehen, was zur Implementierung von infinitem Scrollen auf dem Frontend einer Webanwendung zum Betrieb mit Schallwellendaten und mehr verwendet werden kann.
Außerdem können Generatoren, die mit sogenannten Promises (Versprechen) verwendet werden, die async / await-Funktionalität imitieren, was es uns ermöglicht, mit asynchronem Code auf einer direkteren und lesbaren Weise umzugehen.
Obwohl async / await eine häufigere Methode ist, mit gebräuchlichen, einfachen asynchronen Anwendungsfällen wie dem Abrufen von Daten von einer API umzugehen, verfügen Generatoren über fortgeschrittenere Funktionen, die es lohnenswert machen, ihre Verwendung zu erlernen.
Dieser Artikel behandelt die Erstellung von Generator-Funktionen, die Iterierung über Generator-Objekten, den Unterschied zwischen yield und return innerhalb eines Generators sowie andere Aspekte bezüglich des Arbeitens mit Generatoren.
Generator-Funktionen
Eine Generator-Funktion ist eine Funktion, die ein Generator-Objekt zurückgibt. Sie wird definiert durch das Schlüsselwort function gefolgt von einem Asterisk (*), so wie nachstehend gezeigt:
Gelegentlich befindet sich der Asterisk neben dem Funktionsnamen, im Gegensatz zum Schlüsselwort, wie z. B. function * generatorFunction ().
Das funktioniert genauso, aber function * ist eine allgemein bekanntere Syntax.
Generator-Funktionen können auch in einer Expression definiert sein, wie reguläre Funktionen:
Generatoren können sogar die Methoden von object und class sein:
Die Beispiele in diesem Artikel verwenden die Deklarationssyntax von Generator-Funktionen.
< $> note Anmerkung: Im Gegensatz zu regulären Funktionen können Generatoren weder mit dem Schlüsselwort new konstruiert noch in Verbindung mit Pfeilfunktionen verwendet werden.
Nachdem Sie nun wissen, wie Sie Generator-Funktionen deklarieren, sehen Sie sich die iterierbaren Generator-Objekte an, die diese zurückgeben.
Generator-Objekte
Traditionell laufen Funktionen in JavaScript bis zum Abschluss; das Aufrufen einer Funktion gibt einen Wert zurück, wenn sie beim Schlüsselwort return ankommt.
Wenn das Schlüsselwort return ausgelassen wird, wird eine Funktion implizit als undefined zurückgegeben.
Im folgenden Code z. B. deklarieren wir eine sum () -Funktion, die einen Wert zurückgibt, der die Summe von zwei ganzzahligen Argumenten ist:
Das Aufrufen der Funktion gibt einen Wert zurück, der die Summe der Argumente ist:
Eine Generator-Funktion hingegen gibt nicht sofort einen Wert zurück, sondern ein iterierbares Generator-Objekt.
Im folgenden Beispiel deklarieren wir eine Funktion und geben ihr einen einzelnen Rückgabewert, wie einer Standardfunktion:
Wenn wir die Generator-Funktion aufrufen, gibt sie das Generator-Objekt zurück, das wir einer Variable zuweisen können:
Wenn es eine reguläre Funktion wäre, würden wir erwarten, dass uns generator die in der Funktion zurückgegebene Zeichenfolge angibt.
Was wir jedoch tatsächlich erhalten, ist ein Objekt im Zustand suspended.
Das Aufrufen von generator ergibt daher ein Output, das dem Folgenden ähnelt:
Das von der Funktion zurückgegebene Generator-Objekt ist ein Iterator.
Ein Iterator ist ein Objekt, das über eine next () -Methode verfügt, die dazu dient, durch eine Sequenz von Werten zu iterieren.
Die next () -Methode gibt ein Objekt mit value- und done-Eigenschaften zurück. value repräsentiert den zurückgegebenen Wert und done zeigt an, ob der Iterator alle seine Werte durchlaufen hat oder nicht.
Mit diesem Wissen rufen wir nur next () auf unserem generator auf und erhalten den aktuellen Wert und den Zustand des Iterators:
Der durch das Aufrufen von next () zurückgegebene Wert ist Hello, Generator! und der Zustand von done ist true, da dieser Wert von einem return stammt, das den Iterator ausgeschlossen hat.
Da der Iterator ausgeführt ist, ändert sich der Status der Generator-Funktion von suspended zu closed.
Ein erneutes Aufrufen von generator ergibt Folgendes:
Bisher haben wir nur gezeigt, dass eine Generator-Funktion eine komplexere Methode sein kann, um den return-Wert einer Funktion zu erhalten.
Aber Generator-Funktionen verfügen auch über einzigartige Eigenschaften, die sie von normalen Funktionen unterscheiden.
Im nächsten Abschnitt erfahren wir mehr über den yield-Operator und sehen, wie ein Generator die Ausführung anhalten und wieder aufnehmen kann.
yield-Operatoren
Generatoren implementieren ein neues Schlüsselwort in JavaScript: yield.yield kann eine Generator-Funktion anhalten und den Wert, der auf yield folgt, zurückgeben. Es bietet somit eine einfache Methode, durch Werte zu iterieren.
In diesem Beispiel halten wir die Generator-Funktion dreimal mit verschiedenen Werten an und geben am Ende einen Wert zurück.
Anschließend weisen wir unser Generator-Objekt der generator-Variablen zu.
Wenn wir nun next () in der Generator-Funktion aufrufen, wird sie jedes Mal anhalten, wenn sie auf yield trifft. done wird auf false gesetzt nach jedem yield, was anzeigt, dass der Generator nicht abgeschlossen hat.
Trifft sie auf ein return oder wenn keine yields mehr in der Funktion zu finden sind, wechselt done auf true und der Generator schließt ab.
Verwenden Sie die next () -Methode viermal in einer Reihe:
Dies ergibt die folgenden vier Zeilen an Output in der Reihenfolge:
Beachten Sie, dass ein Generator kein return benötigt; bei dessen Auslassung gibt die letzte Iteration {value: undefined, done: true} zurück, ebenso wie etwaige weitere Anrufe an next (), nachdem ein Generator abgeschlossen hat.
Iterierung über einen Generator
Mithilfe der next () -Methode haben wir manuell über das Generator-Objekt iteriert und dabei alle value- und done-Eigenschaften des kompletten Objekts erhalten.
Doch so wie Array, Map und Set folgt ein Generator dem Iterationsprotokoll und kann durch for... of iteriert werden:
Dadurch erhalten wir:
Auch der Spread-Operator kann dazu verwendet werden, die Werte eines Generators einem Array zuzuweisen.
Dadurch ergibt sich das folgende Array:
Sowohl Spread als auch for... of beziehen nicht das return in die Werte mit ein (in diesem Fall wäre das Ergebnis 'The Oracle ').
< $> note Anmerkung: Während beide Methoden wirksam sind, um mit finiten Generatoren zu arbeiten, ist es nicht möglich, spread oder for... of direkt zu verwenden, ohne eine unendliche Schleife zu erzeugen, wenn ein Generator mit einem infiniten Datenstrom umgeht.
Schließen eines Generators
Wie wir sehen, kann in einem Generator beim Iterieren durch seine Werte sein done-Merkmal auf true und sein Zustand auf closed gesetzt sein.
Es gibt zwei zusätzliche Möglichkeiten, um einen Generator sofort zu annullieren: mit der return () -Methode und mit der throw () -Methode.
Mit return () kann der Generator an jedem Punkt beendet werden, so wie es bei einem return-Statement im Funktionskörper der Fall wäre.
Sie können ein Argument in return () einfügen oder es für einen undefinierten Wert freihalten.
Um return () anzuwenden, erstellen wir einen Generator mit einigen yield-Werten, aber ohne return, in der Funktionsdefinition:
Das erste next () ergibt 'Neo', mit done auf false gesetzt.
Wenn wir direkt danach eine return () -Methode auf das Generator-Objekt anwenden, erhalten wir nun den übergebenen Wert und done ist auf true gesetzt.
Jeder zusätzliche Aufruf an next () ergibt die Standardantwort Generator abgeschlossen, mit einem undefinierten Wert.
Um dies zu demonstrieren, führen Sie die folgenden drei Methoden auf generator aus:
Dadurch ergeben sich diese drei Ergebnisse:
Die return () -Methode hat das Generator-Objekt zum Abschließen und zum Ignorieren von etwaigen anderen yield-Schlüsselwörtern gezwungen.
Dies ist besonders nützlich, wenn Sie bei der asynchronen Programmierung Funktionen annullierbar machen müssen - z. B. das Unterbrechen einer Web-Anfrage, wenn ein Benutzer eine andere Aktion ausführen möchte - da es nicht möglich ist, ein Promise direkt zu annullieren.
Wenn der Körper einer Generator-Funktion eine Möglichkeit hat, Fehler abzufangen und mit ihnen umzugehen, können Sie die throw () -Methode verwenden, um einen Fehler in den Generator zu injizieren.
Das startet den Generator, injiziert den Fehler und beendet den Generator.
Um dies zu demonstrieren, geben wir ein try... catch in den Körper der Generator-Funktion ein und zeichnen einen Fehler auf, wenn dieser gefunden wird:
Jetzt führen wir die next () -Methode aus, gefolgt von throw ():
Wir haben mit throw () einen Fehler in den Generator injiziert, der vom try... catch aufgefangen und in der Konsole aufgezeichnet wurde.
Methoden und Zustände von Generator-Objekten
Die folgende Tabelle zeigt eine Liste von Methoden, die auf Generator-Objekte angewendet werden können:
next ()
Gibt den nächsten Wert in einem Generator zurück
return ()
Gibt einen Wert in einem Generator zurück und beendet den Generator
throw ()
Injiziert einen Fehler und beendet den Generator
Die nächste Tabelle listet die möglichen Zustände eines Generator-Objekts auf:
Zustand
suspended
Der Generator hat die Ausführung angehalten, ist aber nicht beendet
closed
Der Generator wurde beendet, indem er entweder auf einen Fehler gestoßen, zurückgekehrt oder durch alle Werte iteriert ist
yield Delegation
Außer dem regulären yield-Operator können Generatoren auch die yield * -Expression verwenden, um weitere Werte an einen anderen Generator zu delegieren.
Wenn yield * in einem Generator auftritt, geht es in den delegierten Generator und beginnt, durch alle yields zu iterieren, bis der Generator beendet ist.
Dies kann dazu genutzt werden, verschiedene Generator-Funktionen zu trennen, um Ihren Code semantisch zu organisieren, während alle ihre yields in der richtigen Reihenfolge iterierbar bleiben.
Um dies zu demonstrieren, können wir zwei Generator-Funktionen erstellen, von denen eine eine yield * -Operation auf die andere ausführt:
Als Nächstes iterieren wir durch die Generator-Funktion begin ():
Dadurch erhalten wir folgende Werte in der Reihenfolge, in der sie generiert werden:
Der äußere Generator lieferte die Werte 1 und 2, delegierte dann an den anderen Generator mit yield *, der 3 und 4 zurückgab.
yield * kann auch an jedes Objekt delegieren, das iterierbar ist, wie z. B. ein Array oder ein Map.
Die Yield-Delegation kann bei der Organisation von Code hilfreich sein, da jede Funktion innerhalb eines Generators, die yield verwenden will, auch ein Generator sein muss.
Infinite Datenströme
Eine der nützlichen Aspekte von Generatoren ist die Fähigkeit, mit unendlichen Datenströmen und -sammlungen zu arbeiten.
Dies kann durch das Erstellen einer Endlosschleife innerhalb einer Generator-Funktion demonstriert werden, die eine Zahl um eins erhöht.
Im folgenden Code-Block definieren wir diese Generator-Funktion und initiieren dann den Generator:
Iterieren Sie nun durch die Werte mit next ():
Die Funktion gibt in der Endlosschleife aufeinanderfolgende Werte zurück, während das done-Merkmal auf false bleibt, wodurch sichergestellt wird, dass der Vorgang nicht beendet wird.
Mit Generatoren müssen Sie sich nicht um die Erstellung einer Endlosschleife kümmern, da Sie die Ausführung nach Belieben anhalten und wieder aufnehmen können.
Sie müssen jedoch immer noch vorsichtig sein, wie Sie den Generator aufrufen.
Wenn Sie Spread oder for... of bei einem infiniten Datenstrom anwenden, iterieren Sie plötzlich immer noch über eine Endlosschleife, was zum Absturz der Umgebung führt.
Für ein komplexeres Beispiel eines infiniten Datenstroms können wir eine Fibonacci-Generator-Funktion erstellen.
Die Fibonacci-Sequenz, die die beiden vorherigen Werte kontinuierlich zusammenrechnet, kann mit einer Endlosschleife innerhalb eines Generators wie folgt geschrieben werden:
Um dies zu testen, können wir für eine Endlosschleife eine endliche Zahl verwenden und die Fibonacci-Sequenz in die Konsole eingeben.
Die Fähigkeit, mit unendlichen Datensätzen zu arbeiten, ist ein Teil dessen, was Generatoren so leistungsstark macht.
Dies kann nützlich sein für Beispiele wie infinites Scrollen auf dem Frontend einer Webanwendung.
Übergeben von Werten in Generatoren
In diesem Artikel haben wir Generatoren als Iteratoren verwendet und Werte in jeder Iteration geliefert.
Neben der Erzeugung von Werten können Generatoren auch Werte aus next () konsumieren.
In diesem Fall beinhaltet yield einen Wert.
Es ist wichtig zu beachten, dass das erste aufgerufene next () keinen Wert übergibt, sondern nur den Generator startet.
Um das zu demonstrieren, können wir den Wert von yield aufzeichnen und einige Male next () mit einigen Werten aufrufen.
Es ist auch möglich, den Generator mit einem Anfangswert zu versehen.
Im folgenden Beispiel erstellen wir eine for-Schleife und übergeben jeden Wert in die next () -Methode, versehen die ürsprüngliche Funktion aber auch mit einem Argument:
Wir rufen den Wert von next () ab und geben der nächsten Iteration einen neuen Wert, der dem vorherigen Wert plus Zehn entspricht.
Eine weitere Möglichkeit des Startens eines Generators besteht darin, den Generator mit einer Funktion zu umgeben, die vor jeder anderen Handlung immer einmal next () anruft.
async / await mit Generatoren
Eine asynchrone Funktion ist ein in ES6 + JavaScript verfügbarer Funktionstyp, der das Arbeiten mit asynchronen Daten erleichtert, indem er diese synchron erscheinen lässt.
Generatoren verfügen über eine umfangreichere Palette von Fähigkeiten als asynchrone Funktionen, sind jedoch in der Lage, ein ähnliches Verhalten zu replizieren.
Die Implementierung asynchroner Programmierung auf diese Weise kann die Flexibilität Ihres Codes erhöhen.
In diesem Abschnitt zeigen wir ein Beispiel zur Reproduktion von async / await mit Generatoren.
Wir erstellen eine asynchrone Funktion, die die Fetch API verwendet, um Daten über die JSONPlaceholder API (die JSON-Beispieldaten zu Testzwecken bereitstellt) zu erhalten, und die Rückmeldung in der Konsole aufzeichnet.
Beginnen Sie mit dem Definieren einer asynchronen Funktion namens getUsers, die Daten von der API abruft und eine Reihe von Objekten zurückgibt, und rufen Sie dann getUsers auf:
Dadurch ergeben sich JSON-Daten, die ähnlich sind wie folgt:
Mit Generatoren können wir etwas nahezu Identisches erstellen, das die Schlüsselwörter async / await nicht verwendet.
Es verwendet stattdessen eine neue, von uns erstellte Funktion und liefert yield-Werte anstatt von await-Promises.
Im folgenden Code-Block definieren wir eine Funktion namens getUsers, die unsere neue asyncAlt-Funktion (wir schreiben diese später) nutzt, um async / await zu imitieren.
Wie wir sehen können, sieht diese fast genauso aus wie die async / await-Implementierung - außer dass eine Generator-Funktion vorhanden ist, die Werte liefert.
Nun können wir eine asyncAlt-Funktion erstellen, die einer asynchronen Funktion ähnelt. asyncAlt hat eine Generator-Funktion als Parameter - das ist unsere Funktion, die die Promises liefert, die fetch zurückgibt. asyncAlt gibt selbst eine Funktion zurück und löst jedes Promise, das es findet, bis hin zum letzten:
Das ergibt dasselbe Output wie bei der async / await-Version:
Beachten Sie, dass diese Implementierung demonstriert, wie Generatoren anstelle von async / await verwendet werden können. Sie ist jedoch kein produktreifer Entwurf.
Sie verfügt weder über eine Einrichtung zur Fehlerbehandlung noch über die Fähigkeit, Parameter in die gelieferten Werte zu übergeben.
Obwohl diese Methode Ihrem Code mehr Flexibilität verleihen kann, ist async / await oft die bessere Wahl, da es Implementierungsdetails abstrahiert, sodass Sie sich auf das Schreiben von produktivem Code konzentrieren können.
Generatoren sind Prozesse, die Ausführungen anhalten und wieder aufnehmen können.
Sie sind ein leistungsfähiger, vielseitiger Bestandteil von JavaScript, obwohl sie nicht häufig verwendet werden.
In diesem Tutorial haben Sie mehr über Generator-Funktionen und Generator-Objekte, für Generatoren verfügbare Methoden, yield- und yield * -Operatoren sowie Generatoren mit finiten und infiniten Datensätzen erfahren.
Behandelt wurde auch eine Möglichkeit, asynchronen Code ohne geschachtelte Callbacks oder lange Promise-Ketten zu implementieren.
Wenn Sie mehr über die Syntax von JavaScript lernen möchten, besuchen Sie unsere Tutorials Grundlegendes zu This, Bind, Call und Apply in JavaScript sowie Grundlegendes zu den Objekten Map und Set in JavaScript.
Wie war die Übersetzungsqualität?
Sie haben diese Übersetzung markiert.
Sie haben diese Übersetzung als hilfreich bewertet.
Installieren und Verwenden von TimescaleDB unter Ubuntu 18.04
3219
Die Verwaltung von Zeitreihendaten ist mit dem Aufstieg des Internets der Dinge (IoT) und des industriellen Internets der Dinge zu einer grundlegenden Fähigkeit geworden.
Durch Befolgung dieses Tutorials richten Sie TimescaleDB unter Ubuntu 18.04 ein und lernen, wie Sie damit arbeiten. Sie werden Zeitreihendatenbanken erstellen und einfache Abfragen vornehmen.
Am Ende erfahren Sie, wie Sie unnötige Daten entfernen können.
Einen Ubuntu 18.04-Server, der gemäß unserem Leitfaden zur Ersteinrichtung des Servers für Ubuntu 18.04 eingerichtet wurde, einschließlich eines non-root users, aber mit sudo-Berechtigungen, und einer Firewall.
Folgen Sie Schritt 1 von Installieren und Verwenden von PostgreSQL unter Ubuntu 18.04, um die Datenbank zu installieren.
TimescaleDB ist in den Repositorys des Standardpakets von Ubuntu nicht verfügbar. Darum installieren Sie in diesem Schritt die Datenbank aus dem TimescaleDB PPA (Privatpaket-Repository).
Fügen Sie zunächst das APT-Repository von Timescale hinzu:
Bestätigen Sie diese Aktion durch Betätigung der Eingabetaste.
Als Nächstes aktualisieren Sie Ihren APT-Cache, um Ihre Paketlisten zu aktualisieren:
In diesem Tutorial kommt PostgreSQL Version 10 zum Einsatz. Falls Sie eine andere Version von PostgreSQL verwenden (z. B. 9.6 oder 11), ersetzen Sie den Wert im folgenden Befehl und führen Sie den Befehl aus:
Das TimescaleDB-Modul funktioniert gut mit den Standard-Konfigurationseinstellungen für PostgreSQL, jedoch schlagen die Entwickler von TimescaleDB eine Konfiguration bestimmter Parameter vor, um die Leistung zu erhöhen und die Ressourcen von Prozessor, Arbeitsspeicher und Festplatte besser zu nutzen.
In diesem Tutorial verwenden Sie das Tool timescaledb-tune, das die Datei postgresql.conf lesen und interaktiv Änderungen vorschlagen wird.
Als Nächstes werden Sie dazu aufgefordert, die Variable shared _ preload _ libraries zu ändern, um beim Starten des PostgreSQL-Servers das TimescaleDB-Modul im Voraus zu laden:
shared _ preload _ libraries akzeptiert eine durch Trennzeichen getrennte Liste der Module als Wert, die angibt, welche Module PostgreSQL vor dem Start des Datenbankservers laden soll.
Durch Vornehmen dieser Änderung wird das Modul timescaledb der Liste hinzugefügt.
< $> note Anmerkung: Wenn eine von shared _ preload _ libraries angegebene Bibliothek nicht gefunden werden kann, wird der Datenbankserver nicht gestartet.
Denken Sie daran, wenn Sie Anwendungen debuggen, die shared _ preload _ libraries verwenden.
Weitere Informationen dazu finden Sie in diesem PostgresqlCO.NF-Artikel zu shared _ preload _ libraries.
Aktivieren Sie das TimescaleDB-Modul, indem Sie in dieser Eingabeaufforderung y eingeben und die Eingabetaste drücken:
Basierend auf den Eigenschaften Ihres Servers und der PostgreSQL-Version wird Ihnen das Skript dann anbieten, Ihre Einstellungen zu optimieren.
timescaledb-tune erkennt automatisch den verfügbaren Arbeitsspeicher des Servers und berechnet empfohlene Werte für eine Reihe von Einstellungen. shared _ buffers zum Beispiel ermittelt die für das Zwischenspeichern von Daten zugewiesene Menge an Arbeitsspeicher. Standardmäßig ist dieser Wert relativ niedrig, um einer größeren Palette von Plattformen gerecht zu werden; darum schlägt timescaledb-tune vor, diesen Wert von 128 MB auf 1994 MB zu erhöhen, sodass Sie Ressourcen besser nutzen können, indem mehr Platz zum Speichern zwischengespeicherter Informationen (z. B. wiederholte Abfragen) zur Verfügung steht.
Die Variable work _ mem wurde ebenfalls erhöht, um kompliziertere Sortiervorgänge zuzulassen.
Wenn Sie mehr über die Feinabstimmung der Arbeitsspeichereinstellungen für PostgreSQL erfahren möchten, konsultieren Sie den Artikel Optimierung Ihres PostgreSQL-Servers in der PostgreSQL-Wiki.
Geben Sie y ein, um die Werte zu akzeptieren:
Diese Einstellungen bestimmen darüber, wie mehrere CPUs simultane Abfragen parallel ausführen können, um Datenbanken zu durchsuchen und die angeforderten Daten schneller zurückzugeben.
Diese Einstellungen steuern die Anzahl der Worker, die Anfragen und Hintergrundaufgaben verarbeiten.
WAL ist eine Protokollierungsmethode, mit der PostgreSQL Änderungen an Datendateien protokolliert, bevor die Änderungen an der Datenbank vorgenommen werden.
Durch Priorisierung eines aktuellen Datensatzes mit Datenänderungen sorgt WAL dafür, dass Sie Ihre Datenbank im Falle eines Absturzes rekonstruieren können.
So wird die Datenintegrität gewahrt.
Die Standardeinstellungen können jedoch ineffiziente Input / Output (I / O) -Operationen verursachen, was die Schreibperformance verringert.
Um dieses Problem zu beheben, geben Sie y ein und bestätigen Sie mit der Eingabetaste:
Als Ergebnis erhalten Sie eine gebrauchsfertige Konfigurationsdatei unter / etc / postgresql / < ^ > 10 < ^ > / main / postgresql.conf.
< $> note Anmerkung: Wenn Sie die Installation automatisieren, könnten Sie den anfänglichen Befehl auch mit den Flags --quiet und --yes ausführen, wodurch automatisch alle Empfehlungen angewendet und Änderungen an der Konfigurationsdatei postgresql.conf vorgenommen werden:
Zum Demonstrieren dieser Möglichkeit verwenden Sie PostgreSQL-Befehle, um eine Datenbank zu erstellen und dann die TimescaleDB-Erweiterung zu aktivieren. Dadurch erstellen Sie eine Hypertabelle, die eine Abstraktion vieler individueller Tabellen auf höherer Ebene ist.
Hypertabellen sind die Hauptstrukturen, mit denen Sie in TimescaleDB arbeiten.
Melden Sie sich bei Ihrer PostgreSQL-Datenbank an:
Wie bereits erwähnt, sind die primären Interaktionspunkte mit Ihren Zeitreihendaten Hypertabellen, die aus vielen einzelnen Tabellen namens Chunks bestehen.
Sie können Daten in die Hypertabelle einfügen, indem Sie den SQL-Standardbefehl INSERT verwenden.
Verwenden Sie nach dem Löschvorgang den Befehl VACUUM, um Platz freizugeben, der noch von gelöschten Daten verwendet wird.
Dies ist ein offizieller TimescaleDB-Datensatz, der zum Testen der Datenbank dient.
Zusätzlich zu den SQL-Standardbefehlen bietet TimescaleDB auch eine Reihe spezieller Funktionen, die für die Analyse von Zeitreihendaten nützlich sind.
Sie haben TimescaleDB auf Ihrem Ubuntu 18.04-Server eingerichtet.
Nachdem Sie wissen, wie sich Zeitreihendaten speichern lassen, können Sie die Daten nun zum Erstellen von Diagrammen verwenden.
TimescaleDB ist mit Visualisierungstools kompatibel, die mit PostgreSQL funktionieren (wie Grafana).
Sie können unser Tutorial Installieren und Schützen von Grafana unter Ubuntu 18.04 konsultieren, um mehr über dieses beliebte Visualisierungstool zu erfahren.
Einrichten eines Objektspeicherservers mit Minio unter Ubuntu 18.04
3275
Von cloudbasierten Backup-Lösungen bis zu Content Delivery Networks (CDNs) mit Hochverfügbarkeit ist die Möglichkeit, unstrukturierte Blobs von Objektdaten zu speichern und über HTTP-APIs zugänglich zu machen (auch als Objektspeicher bezeichnet), zu einem integralen Bestandteil der modernen Technologielandschaft geworden.
Minio ist ein beliebter Open-Source-basierter Objektspeicherserver, der mit dem Amazon S3-Cloud-Speicherdienst kompatibel ist.
Anwendungen, die so konfiguriert sind, dass sie mit Amazon S3 kommunizieren können, lassen sich auch so konfigurieren, dass Minio eine tragfähige Alternative zu S3 ist, wenn Sie mehr Kontrolle über Ihren Objektspeicherserver wünschen.
Der Dienst speichert unstrukturierte Daten wie Fotos, Videos, Protokolldateien, Backups sowie Container / VM-Images und kann sogar einen einzelnen Objektspeicherserver bereitstellen, der mehrere, auf verschiedene Server verteilte Laufwerke in Pools zusammenfasst.
Minio ist in Go geschrieben, bietet einen Befehlszeilenclient sowie eine Browseroberfläche und unterstützt einen einfachen Queuing-Dienst für Advanced Message Queuing Protocol (AMQP) -, Elasticsearch-, Redis-, NATS- und PostgreSQL-Ziele.
Aus allen diesen Gründen kann das Erlernen der Einrichtung eines Minio-Objektspeicherservers die Flexibilität und Nützlichkeit Ihres Projekts deutlich erhöhen.
In diesem Tutorial werden Sie folgende Aufgaben erledigen:
Installieren des Minio-Servers auf Ihrem Ubuntu 18.04-Server und Konfigurieren des Servers als systemd-Dienst
Einrichten eines SSL / TLS-Zertifikats mit Let 's Encrypt, um die Kommunikation zwischen Server und Client zu schützen
Zugreifen auf die Browseroberfläche von Minio über HTTPS zum Verwenden und Verwalten des Servers
Einen Ubuntu 18.04-Server, der gemäß unseres Tutorials zur Ersteinrichtung des Ubuntu 18.04-Servers eingerichtet wurde, einschließlich eines sudo non-root users und einer Firewall.
Sie können einen Domänennamen bei Namecheap kaufen bzw. kostenlos bei Freenom erhalten.
In diesem Tutorial wird Ihre Domäne als < ^ > your _ domain < ^ > dargestellt.
Richten Sie die folgenden DNS-Einträge für Ihren Minio-Server ein.
Informationen zum Hinzufügen von DNS-Einträgen für ein DigitalOcean-Droplet finden Sie in unserer Dokumentation zu DNS-Einträgen.
Einen A-Eintrag mit Ihrem Servernamen (z. B. < ^ > minio-server.your _ domain < ^ >), der auf die IPv4-Adresse Ihres Objektservers verweist.
(Optional) Wenn Sie möchten, dass Ihr Server über IPv6 erreichbar ist, benötigen Sie einen AAAA-Eintrag, wobei Ihr Servername auf die IPv6-Adresse Ihres Objektservers verweist.
Schritt 1 - Installieren und Konfigurieren des Minio-Servers
Sie können den Minio-Server installieren, indem Sie den Quellcode kompilieren oder eine Binärdatei verwenden.
Um den Server von der Quelle zu installieren, müssen Sie in Ihrem System mindestens Go 1.12 installiert haben.
In diesem Schritt installieren Sie den Server über die vorkompilierte Binärdatei und konfigurieren dann den Minio-Server.
Melden Sie sich zunächst bei Ihrem Server an, indem Sie < ^ > sammy < ^ > durch Ihren Benutzernamen und < ^ > your _ server _ ip < ^ > durch die IP-Adresse Ihres Ubuntu 18.04-Servers ersetzen:
Wenn Sie die Paketdatenbank in letzter Zeit nicht aktualisiert haben, tun Sie es nun:
Laden Sie als Nächstes die Binärdatei des Minio-Servers von der offiziellen Website herunter:
Nach Abschluss des Downloads befindet sich in Ihrem Arbeitsverzeichnis eine Datei namens minio.
Verwenden Sie folgenden Befehl, um sie ausführbar zu machen:
Verschieben Sie die Datei nun in das Verzeichnis / usr / local / bin, in dem das Startskript systemd von Minio sie erwartet:
Das erlaubt es uns später in dem Tutorial, eine Diensteinheitdatei zu schreiben, damit Minio beim Start automatisch ausgeführt wird.
Aus Sicherheitsgründen ist es besser, den Minio-Server als root auszuführen.
So lassen sich Schäden, die bei einer Kompromittierung Ihres Systems entstehen können, begrenzen.
Da das in Schritt 2 verwendete Skript systemd nach einem Benutzerkonto und einer Gruppe namens minio-user sucht, erstellen Sie einen neuen Benutzer mit diesem Namen:
In diesem Befehl haben Sie das Flag -s verwendet, um / sbin / nologin als Shell für minio-user festzulegen.
Dies ist ein Shell, das keine Benutzeranmeldung erlaubt, was für minio-user auch nicht benötigt wird.
Ändern Sie als Nächstes den Besitz an der Minio-Binärdatei in minio-user:
Als Nächstes erstellen Sie ein Verzeichnis, in dem Minio Dateien speichert.
Dies ist der Speicherort für die Buckets, die Sie später zum Organisieren von Objekten, die Sie auf Ihrem Minio-Server speichern, nutzen werden.
In diesem Tutorial heißt das Verzeichnis < ^ > minio < ^ >:
Vergeben Sie den Besitz dieses Verzeichnisses an minio-user:
Die meisten Serverkonfigurationsdateien werden im Verzeichnis / etc gespeichert. Erstellen Sie also dort Ihre Minio-Konfigurationsdatei.
Übergeben Sie den Besitz dieses Verzeichnisses ebenfalls an minio-user:
Verwenden Sie Nano oder Ihren bevorzugten Texteditor, um die Umgebungsdatei zu erstellen, die zum Ändern der Standardkonfiguration benötigt wird:
Wenn die Datei geöffnet ist, fügen Sie die folgenden Zeilen hinzu, um in Ihrer Umgebungsdatei einige wichtige Umgebungsvariablen festzulegen:
Sehen wir uns diese Variablen sowie die Werte an, die Sie festlegen:
MINIO _ ACCESS _ KEY: Dadurch wird der Zugriffsschlüssel festgelegt, den Sie für den Zugriff auf die Benutzeroberfläche des Minio-Browsers nutzen werden.
MINIO _ SECRET _ KEY: Damit wird der private Schlüssel festgelegt, den Sie zum Vervollständigen Ihrer Anmeldeinformationen in der Minio-Oberfläche verwenden werden.
In diesem Tutorial ist der Wert auf < ^ > miniostorage < ^ > festgelegt; wir empfehlen jedoch die Auswahl eines anderen, komplexeren Passworts zum Schutz Ihres Servers.
MINIO _ VOLUMES: Damit wird das Speicherverzeichnis angegeben, das Sie für Ihre Buckets erstellt haben.
MINIO _ OPTS: Damit lässt sich ändern, wo und wie der Server Daten bereitstellt. Das Flag -C verweist auf das zu verwendende Konfigurationsverzeichnis, während das Flag --address Minio über die IP-Adresse und den Port für die Bindung informiert.
Wenn keine IP-Adresse angegeben ist, bindet sich Minio an jede auf dem Server konfigurierte Adresse, einschließlich localhost und mit Docker verknüpfter IP-Adressen; daher wird empfohlen, die IP-Adresse hier anzugeben.
Der Standardport 9000 kann bei Bedarf geändert werden.
Speichern und schließen Sie abschließend die Umgebungsdatei, nachdem Sie alle Änderungen vorgenommen haben.
Sie haben Minio jetzt installiert und einige wichtige Umgebungsvariablen festgelegt.
Als Nächstes konfigurieren Sie den Server so, dass er als Systemdienst ausgeführt wird.
Schritt 2 - Installieren des Minio Systemd-Startskripts
In diesem Schritt konfigurieren Sie den Minio-Server so, dass er als systemd-Dienst verwaltet wird.
Laden Sie zunächst mit folgendem Befehl die offizielle Deskriptordatei für den Minio-Dienst herunter:
Nach Abschluss des Downloads befindet sich in Ihrem Arbeitsverzeichnis eine Datei namens minio.service.
Um den Inhalt von minio.service vor der Anwendung zu überprüfen, öffnen Sie ihn in einem Texteditor:
Diese Diensteinheitdatei startet den Minio-Server mit dem Benutzer minio-user, den Sie zuvor erstellt haben.
Sie implementiert außerdem die im letzten Schritt festgelegten Umgebungsvariablen und sorgt dafür, dass der Server beim Start automatisch ausgeführt wird.
Weitere Informationen zu systemd-Unit-Dateien finden Sie in unserem Leitfaden Informationen zu systemd-Units und Unit-Dateien.
Nach Prüfung der Inhalte des Skripts schließen Sie den Texteditor.
Systemd setzt voraus, dass Unit-Dateien im Konfigurationsverzeichnis systemd gespeichert werden. Verschieben Sie minio.service also dort hin:
Führen Sie dann den folgenden Befehl zum Neuladen aller systemd-Units aus:
Aktivieren Sie zum Schluss das Starten von Minio beim Booten:
Nachdem Sie das Skript systemd nun installiert und konfiguriert haben, ist es Zeit zum Starten des Servers.
Schritt 3 - Starten des Minio-Servers
In diesem Schritt starten Sie den Server und ändern die Firewall, um Zugriff über die Browseroberfläche zuzulassen.
Starten Sie zunächst den Minio-Server:
Überprüfen Sie als Nächstes den Status des Minio-Servers, die IP-Adresse, an die er gebunden ist, die Arbeitsspeichernutzung und mehr, indem Sie folgenden Befehl ausführen:
Aktivieren Sie als Nächstes den Zugriff auf den Minio-Server über die Firewall am konfigurierten Port. In diesem Tutorial ist das der Port < ^ > 9000 < ^ >.
Fügen Sie zunächst die Regel hinzu:
Aktivieren Sie dann die Firewall:
Sie erhalten folgende Eingabeaufforderung:
Drücken Sie auf y und bestätigen Sie mit der Eingabetaste:
Sie erhalten dann folgende Ausgabe:
Minio ist jetzt bereit dazu, Datenverkehr zu akzeptieren. Bevor Sie aber eine Verbindung mit dem Server herstellen, sichern Sie die Kommunikation durch Installation eines SSL / TLS-Zertifikats.
Schritt 4 - Sichern des Zugriffs auf Ihren Minio-Server mit einem TLS-Zertifikat
In diesem Schritt sichern Sie den Zugriff auf Ihren Minio-Server mit einem privaten Schlüssel und einem öffentlichen Zertifikat, das von einer Zertifizierungsstelle (CA) empfangen wurde - in diesem Fall von Let 's Encrypt.
Um ein kostenloses SSL-Zertifikat zu erhalten, verwenden Sie Certbot.
Lassen Sie zunächst HTTP- und HTTPS-Zugriff über Ihre Firewall zu.
Öffnen Sie dazu Port 80, was der Port für HTTP ist:
Öffnen Sie als Nächstes Port 443 für HTTPS:
Überprüfen Sie nach dem Hinzufügen dieser Regeln den Status Ihrer Firewall mit folgendem Befehl:
Dadurch wird bestätigt, dass Ports 80 und 443 geöffnet sind, was bedeutet, dass Ihr Server Anfragen aus dem Internet akzeptiert.
Als Nächstes installieren Sie Certbot.
Da Certbot ein separates PPA-Repository unterhält, müssen Sie es zunächst zu Ihrer Liste von Repositorys hinzufügen, bevor Sie Certbot wie dargestellt installieren:
Um das Hinzufügen des PPA-Repository vorzubereiten, installieren Sie zunächst software-properties-common, ein Paket zum Verwalten von PPAs:
Dieses Paket bietet einige nützliche Skripts zum Hinzufügen und Entfernen von PPAs, damit Sie es nicht manuell tun müssen.
Fügen Sie jetzt das Repository Universe hinzu:
Dieses Repository enthält kostenlose Open-Source-Software, die von der Ubuntu-Community gepflegt wird, jedoch nicht offiziell von Canonical, den Entwicklern von Ubuntu.
Hier finden wir das Repository for Certbot.
Fügen Sie als Nächstes das Certbot-Repository hinzu:
Drücken Sie die Eingabetaste, um fortzufahren.
Aktualisieren Sie dann die Paketliste:
Installieren Sie zum Schluss certbot:
Als Nächstes verwenden Sie certbot zum Erstellen eines neuen SSL-Zertifikats.
Da Ubuntu 18.04 noch keine automatische Installation unterstützt, verwenden Sie den Befehl certonly und --standalone, um das Zertifikat abzurufen:
--standalone bedeutet, dass dies ein Zertifikat für einen integrierten Standalone-Webserver ist.
Weitere Informationen dazu finden Sie in unserem Tutorial Verwenden von Certbot im Standalone-Modus zum Abrufen von Let 's Encrypt-SSL-Zertifikaten unter Ubuntu 18.04.
Fügen Sie Ihre E-Mail-Adresse hinzu und drücken Sie die Eingabetaste.
Certbot fordert Sie dann dazu auf, sich bei Let 's Encrypt zu registrieren:
Geben Sie A ein und drücken Sie die Eingabetaste, um zu akzeptieren.
Als Nächstes werden Sie gefragt, ob Sie damit einverstanden sind, Ihre E-Mail-Adresse mit der Electronic Frontier Foundation zu teilen:
Nachdem Sie mit Y oder N geantwortet haben, werden Ihre öffentlichen und privaten Schlüssel generiert und im Verzeichnis / etc / letsencrypt / live / minio-server. < ^ > your _ domain _ name < ^ > gespeichert.
Kopieren Sie als Nächstes diese beiden Dateien (privkey.pem und fullchain.pem) in das Verzeichnis certs unter dem Konfigurationsverzeichnis des Minio-Servers, das in diesem Tutorial / etc / minio lautet.
Verwenden Sie folgenden Befehl zum Kopieren von privkey.pem und zum Umbenennen der Datei private.key:
Tun Sie dann das Gleiche für fullchain.pem und nennen Sie das Ergebnis public.crt:
Ändern Sie jetzt den Besitz an den Dateien in minio-user.
Erledigen Sie diese Aufgabe zunächst für private.key:
Dann für public.crt:
Starten Sie den Minio-Server neu, damit er sich des Zertifikats bewusst wird und mit HTTPS startet:
Zertifikate von Let "s Encrypt sind nur für neunzig Tage gültig.
Das soll Benutzer dazu ermutigen, die Erneuerung ihrer Zertifikate zu automatisieren.
Das Certbot-Paket, das Sie automatisch installiert haben, fügt / etc / cron.d ein Erneuerungsskript hinzu.
Damit ist die Verbindung von Minio jetzt sicher und das SSL / TLS-Zertifikat wird automatisch für Sie erneuert.
Im nächsten Schritt stellen Sie über den Browser eine Verbindung mit Minio her, um den Server zu verwenden.
Schritt 5 - Sicheres Herstellen einer Verbindung mit der Weboberfläche von Minio über HTTPS
In diesem Schritt stellen Sie eine sichere Verbindung mit der Minio-Weboberfläche über HTTPS her und erstellen dann Buckets, in die Sie Objekte hochladen.
Greifen Sie auf die Weboberfläche zu, indem Sie in Ihrem Browser https: / / minio-server. < ^ > your _ domain < ^ >: < ^ > 9000 < ^ > aufrufen.
Sie sehen den Anmeldebildschirm des Minio-Servers:
Minio-Anmeldebildschirm
Melden Sie sich jetzt bei der Hauptoberfläche an, indem Sie Ihre Anmeldeinformationen eingeben.
Geben Sie für Access Key (Zugriffsschlüssel) den MINIO _ ACCESS _ KEY ein, den Sie in der Umgebungsdatei / etc / default / < ^ > minio < ^ > in Schritt 1 festgelegt haben. Geben Sie für Secret Key (Geheimer Schlüssel) den MINIO _ SECRET _ KEY ein, den Sie in derselben Datei festgelegt haben.
Klicken Sie nach Eingabe der Anmeldeinformationen auf die runde Schaltfläche mit dem Pfeil direkt unterhalb der Eingabefelder.
Nun wird die Benutzeroberfläche von Minio angezeigt.
Um einen neuen Bucket zu erstellen, in dem Sie Objekte speichern können, klicken Sie auf die hellrote Schaltfläche + unten rechts in der Hauptoberfläche; nun werden zwei zusätzliche gelbe Schaltflächen angezeigt.
Hauptoberfläche von Minio
Klicken Sie auf die mittlere gelbe Schaltfläche und geben Sie in der Eingabeaufforderung einen Namen für Ihren neuen Bucket ein, bevor Sie die Eingabetaste drücken, um Ihre Antwort zu speichern.
Ihr neuer Bucket kann jetzt als Speicher verwendet werden.
< $> note Anmerkung: Stellen Sie bei der Benennung Ihres Minio-Buckets sicher, dass der Name nur Kleinbuchstaben, Zahlen und Bindestriche enthält.
Minio begrenzt die Namenskonventionen für Buckets, um mit AWS S3-Standards vereinbar zu sein.
Wenn Sie Objekte in Ihren Bucket hinzufügen möchten, klicken Sie auf dieselbe hellrote Schaltfläche wie zuvor und klicken Sie dann auf die obere gelbe Schaltfläche, um eine Eingabeaufforderung für den Datei-Upload zu öffnen.
Jetzt haben Sie mit dem Erstellen von Buckets und Hochladen von Objekten alle grundlegenden Funktionen der Weboberfläche verwendet.
Sie verfügen jetzt über einen eigenen Minio-Server, mit dem Sie über die Weboberfläche unter Verwendung eines Let 's Encrypt-SSL / TLS-Zertifikats eine sichere Verbindung herstellen können.
Optional können Sie sich die Minio-Desktopclients für FreeBSD, Linux, Mac und Windows als Alternative für das Verwenden und Verwalten Ihres Objektspeicherservers ansehen.
Wenn Sie außerdem die Speicherkapazität Ihrer Minio-Installation über die Datenträgergröße Ihres Servers hinaus erhöhen möchten, können Sie den Blockspeicherdienst von DigitalOcean nutzen, um ein Volume mit Ihrem Server zu verbinden und die Speicherkapazität um bis zu 80 TB zu erweitern.
Weitere Informationen zu Minio finden Sie auf der Dokumentationswebsite des Projekts.
Wenn Sie mehr über Objektspeicher erfahren möchten, sehen Sie sich unsere Tutorials zu Objektspeicher an.
Erstellen eines Hashicorp-Vault-Servers mit Packer und Terraform in DigitalOcean Schnellstart
3926
Eine ausführlichere Version dieses Tutorials finden Sie in Erstellen eines Hashicorp Vault-Servers mit Packer und Terraform in DigitalOcean.
Besuchen Sie Erstellen eines persönlichen Zugriffstokens, um ein Token zu erstellen.
Erstellen und wechseln Sie in das Verzeichnis ~ / vault-orchestration, um Ihre Vault-Dateien zu speichern:
Erstellen Sie separate Verzeichnisse für die Packer- und Terraform-Konfiguration, indem Sie Folgendes ausführen:
Navigieren Sie zum Verzeichnis Packer:
Erstellen Sie eine variables.json in Ihrem packer-Unterverzeichnis, um Ihre privaten variablen Daten zu speichern:
Sie können die Werte Basisbild, Region und Droplet-Größe entsprechend den developer docs bearbeiten.
Ersetzen Sie < ^ > your _ do _ api _ key < ^ > durch Ihren API-Schlüssel. Speichern und schließen Sie dann die Datei.
Erstellen Sie Ihre Packer-Vorlage für Vault in einer Datei namens template.json:
Sie definieren einen einfachen digitalocean-Builder.
Packer erstellt ein temporäres Droplet der definierten Größe, des definierten Bilds und der definierten Region, wobei der bereitgestellte API-Schlüssel verwendet wird.
Der Provisioner stellt über SSH eine Verbindung mit dem angegebenen Benutzernamen her und führt nacheinander alle definierten Provisioners aus, bevor ein DigitalOcean-Snapshot aus dem Droplet erstellt und gelöscht wird.
Konsultieren Sie die offizielle Vault-Downloadseite, um die aktuellste Version für Linux zu erhalten.
Überprüfen Sie die Gültigkeit Ihrer Vorlage:
Erstellen Sie Ihren Snapshot mit dem Packer-Befehl build:
Die letzte Zeile enthält den Namen des Snapshots (wie packer-1581537927) und seine ID in Klammern, wie hier hervorgehoben.
Notieren Sie sich die ID des Snapshots, da Sie diese im nächsten Schritt benötigen.
Navigieren Sie zum Unterverzeichnis terraform:
Erstellen Sie eine Datei namens do-provider.tf, um den Provider zu speichern:
Diese Datei stellt dem digitalocean-Provider einen API-Schlüssel zur Verfügung.
Um die Werte dieser Variablen anzugeben, erstellen Sie eine Variablendefinitionsdatei, ähnlich wie bei Packer.
Erstellen Sie eine Variablendefinitionsdatei:
Ersetzen Sie < ^ > your _ do _ api _ key < ^ >, < ^ > your _ ssh _ key _ fingerprint < ^ > und < ^ > your _ do _ snapshot _ id < ^ > (die Snapshot-ID, die Sie im vorherigen Schritt notiert haben).
Erstellen Sie die folgende Datei, um die Bereitstellungskonfiguration des Vault-Snapshots zu speichern:
Sie definieren eine einzelne resource vom Typ digitalocean _ droplet namens vault.
Sie legen ihre Parameter entsprechend den Variablenwerten fest und fügen (mit seinem Fingerabdruck) einen SSH-Schlüssel von Ihrem DigitalOcean-Konto zur Droplet-Ressource hinzu.
Sie geben die IP-Adressen aller neu erstellten Instanzen an die Konsole aus.
Initialisieren Sie das Verzeichnis als Terraform-Projekt:
Testen Sie die Gültigkeit Ihrer Konfiguration:
Führen Sie den Befehl plan aus, um zu sehen, was Terraform bei der Bereitstellung der Infrastruktur versucht:
Führen Sie den Plan aus:
Das Droplet wird die Bereitstellung abschließen und Sie sehen eine Ausgabe, die etwa folgendermaßen aussieht:
Führen Sie Folgendes aus, um sich mit Ihrem neuen Droplet zu verbinden:
Nach der Anmeldung führen Sie Vault aus mit:
Sie sehen die entsprechende "Hilfe" -Ausgabe:
3205
Bei der Entwicklung einer Ruby-on-Rails-Anwendung stellen Sie möglicherweise fest, dass Sie Anwendungsaufgaben haben, die asynchron ausgeführt werden sollten.
Die Verarbeitung von Daten, das Versenden von Batch-E-Mails oder die Interaktion mit externen APIs sind Beispiele für Arbeiten, die asynchron mit Hintergrundjobs durchgeführt werden können.
Die Verwendung von Hintergrundjobs kann die Leistung Ihrer Anwendung verbessern, indem möglicherweise zeitintensive Aufgaben in eine Hintergrundverarbeitungs-Warteschlange ausgelagert werden, wodurch der ursprüngliche Anfrage- / Antwort-Zyklus entlastet wird.
Sidekiq ist eines der am häufigsten verwendeten Hintergrundjob-Frameworks, das Sie in einer Rails-Anwendung implementieren können.
Es wird von Redis unterstützt, einem In-Memory-Schlüsselwertspeicher, der für seine Flexibilität und Leistung bekannt ist.
Sidekiq verwendet Redis als Aufgabenmanagementspeicher, um pro Sekunde Tausende von Aufgaben zu verarbeiten.
In diesem Tutorial fügen Sie Redis und Sidekiq zu einer bestehenden Rails-Anwendung hinzu.
Sie werden einen Satz von Sidekiq Worker-Klassen und -Verfahren erstellen, um Folgendes zu bewältigen:
Einen Batch-Upload von Informationen über bedrohte Haie in die Anwendungsdatenbank aus einer CSV-Datei im Projekt-Repository.
Die Entfernung dieser Daten.
Wenn Sie fertig sind, haben Sie eine Demo-Anwendung, die Workers und Jobs verwendet, um Aufgaben asynchron zu verarbeiten.
Dieses Tutorial ist ein guter Startpunkt, damit Sie später Worker und Jobs zu Ihrer eigenen Anwendung hinzufügen können.
Einen lokalen Rechner oder einen Entwicklungsserver, auf dem Ubuntu 18.04. ausgeführt wird.
Ihr Entwicklungsrechner sollte über einen non-root user mit Administratorberechtigungen und eine mit ufw konfigurierte Firewall verfügen.
Anleitungen zur Einrichtung finden Sie in unserem Tutorial Ersteinrichtung des Servers unter Ubuntu 18.04.
Auf Ihrem lokalen Rechner oder dem Entwicklungsserver installiertes Node.js und npm.
Dieses Tutorial verwendet Node.js Version < ^ > 10.17.0 < ^ > und npm Version < ^ > 6.11.3 < ^ >.
Eine Anleitung zur Installation von Node.js und npm unter Ubuntu 18.04 finden Sie im Abschnitt "Installation unter Verwendung eines PPA" in "Installieren von Node.js unter Ubuntu 18.04".
Den auf Ihrem lokalen Rechner oder dem Entwicklungsserver installierten Paketmanager Yarn.
Sie können den Installationsanweisungen in der offiziellen Dokumentation folgen.
Auf Ihrem lokalen Rechner oder dem Entwicklungsserver gemäß den Schritten 1-4 von "Installieren von Ruby-on-Rails mit rbenv unter Ubuntu 18.04" installiertes Ruby, rbenv und Rails.
Dieses Tutorial verwendet Ruby < ^ > 2,5.1 < ^ >, rbenv < ^ > 1.1.2 < ^ > und Rails < ^ > 5.2.3 < ^ >.
Nach dem Schritt 1 von "Erstellen einer Ruby-on-Rails-Anwendung" installiertes SQLite.
Dieses Tutorial verwendet SQLite 3 < ^ > 3.22.0 < ^ >.
Nach den Schritten 1-3 von "Installieren und Sichern von Redis unter Ubuntu 18.04." installiertes Redis.
Dieses Tutorial verwendet Redis < ^ > 4.0.9 < ^ >.
Schritt 1 - Klonen des Projekts und Installieren von Abhängigkeiten
Unser erster Schritt besteht darin, das Repository rails-bootstrap aus dem DigitalOcean Community GitHub-Konto zu klonen.
Dieses Repository enthält den Code aus dem Setup, das in "Hinzufügen von Sidekiq und Redis zu einer Ruby-on-Rails-Anwendung" beschrieben wird. Dort wird erklärt, wie Bootstrap zu einem bestehenden Rails 5-Projekt hinzugefügt wird.
Klonen Sie das Repository in ein Verzeichnis namens < ^ > rails-sidekiq < ^ >:
Navigieren Sie zum Verzeichnis < ^ > rails-sidekiq < ^ >:
Um mit dem Code arbeiten zu können, müssen Sie zunächst die Abhängigkeiten des Projekts installieren, die in seiner Gemfile aufgelistet sind.
Um mit Sidekiq und Redis arbeiten zu können, müssen Sie dem Projekt auch das Gem sidekiq hinzufügen.
Öffnen Sie das Gemfile des Projekts zur Bearbeitung mit nano oder Ihrem bevorzugten Editor:
Speichern und schließen Sie die Datei, wenn Sie mit dem Hinzufügen des Gems fertig sind.
Verwenden Sie den folgenden Befehl, um die Gems zu installieren:
Sie werden in der Ausgabe sehen, dass das Gem redis auch als Erfordernis für sidekiq installiert ist.
Als Nächstes installieren Sie Ihre Yarn-Abhängigkeiten.
Da dieses Rails 5-Projekt modifiziert wurde, um Assets mit webpack bereitzustellen, werden seine JavaScript-Abhängigkeiten nun von Yarn verwaltet.
Das bedeutet es ist notwendig, die in der Datei package.json des Projekts aufgeführten Abhängigkeiten zu installieren und zu verifizieren.
Führen Sie yarn install aus, um diese Abhängigkeiten zu installieren:
Führen Sie als Nächstes Ihre Datenbankmigration durch:
Sobald Ihre Migrationen abgeschlossen sind, können Sie die Anwendung testen, um sicherzustellen, dass sie wie erwartet funktioniert.
Starten Sie Ihren Server im Kontext Ihres lokalen Bundles mit dem folgenden Befehl, wenn Sie lokal arbeiten:
Wenn Sie auf einem Entwicklungsserver arbeiten, können Sie die Anwendung starten mit:
Navigieren Sie zu localhost: 3000 oder http: / / < ^ > your _ server _ ip < ^ >: 3000.
Um einen neuen Hai zu erstellen, klicken Sie auf die Schaltfläche Get Shark Info, die Sie zu der Route sharks / index führt:
Sharks Index-Route
Geben Sie auf der Seite New Shark unter Name "Great White" und unter Facts "Scary" ein:
Hai erstellen
Sobald Sie sehen, dass Ihr Hai erstellt wurde, können Sie den Server mit STRG + C beenden.
Sie haben nun die notwendigen Abhängigkeiten für Ihr Projekt installiert und seine Funktionalität getestet.
Als Nächstes können Sie einige Änderungen der Rails-Anwendung vornehmen, um mit Ihren Ressourcen für bedrohte Haie zu arbeiten.
Schritt 2 - Generieren eines Controllers für Ressourcen für bedrohte Haie
Um mit unseren Ressourcen für bedrohte Haie zu arbeiten, fügen wir der Anwendung ein neues Modell und einen Controller hinzu, der steuert, wie den Benutzern Informationen über bedrohte Haie präsentiert werden.
Unser ultimatives Ziel ist, den Benutzern das Hochladen einer großen Menge von Informationen über bedrohte Haie zu ermöglichen, ohne die Gesamtfunktionalität unserer Anwendung zu blockieren, und diese Informationen zu löschen, wenn sie sie nicht mehr benötigen.
Erstellen wir zunächst ein Modell Endangered für unsere bedrohten Haie.
Wir werden ein Zeichenfolgenfeld für den Hainamen in unsere Datenbanktabelle aufnehmen und ein weiteres Zeichenfolgenfeld für die Kategorien der International Union for the Conservation of Nature (IUCN), die den Grad der Gefährdung der einzelnen Haie bestimmen.
Letztendlich entspricht unsere Modellstruktur den Spalten in der CSV-Datei, die wir zur Erstellung unseres Batch-Uploads verwenden werden.
Diese Datei befindet sich im Verzeichnis db und Sie können ihren Inhalt mit dem folgenden Befehl überprüfen:
Die Datei enthält eine Liste von 73 bedrohten Haien und ihren IUCN-Status - vu für gefährdet, en für bedroht und cr für vom Aussterben bedroht.
Unser Modell Endangered wird mit diesen Daten übereinstimmen, sodass wir neue Instanzen Endangered aus dieser CSV-Datei erstellen können.
Erstellen Sie das Modell mit dem folgenden Befehl:
Erstellen Sie als Nächstes einen Controller Endangered mit einer Aktion index:
Dadurch erhalten wir einen Ausgangspunkt, um die Funktionalität unserer Anwendung auszubauen, obwohl wir auch benutzerdefinierte Methoden zu der Controller-Datei hinzufügen müssen, die Rails für uns generiert hat.
Öffnen Sie nun diese Datei:
Rails hat uns ein Grundgerüst zur Verfügung gestellt, das wir nun ausfüllen können.
Zunächst müssen wir bestimmen, welche Routen wir für die Arbeit mit unseren Daten benötigen. Dank dem Befehl generate controller haben wir eine Methode index, mit der wir beginnen können.
Diese wird mit einer Ansicht index korrelieren, in der wir den Benutzern die Möglichkeit bieten werden, bedrohte Haie hochzuladen.
Wir werden aber auch Fälle bearbeiten wollen, in denen Benutzer die Haie möglicherweise bereits hochgeladen haben. In diesem Fall benötigen sie keine Upload-Option.
Wir werden irgendwie beurteilen müssen, wie viele Instanzen der Klasse Endangered bereits existieren, da mehr als eine darauf hinweist, dass der Batch-Upload bereits stattgefunden hat.
Beginnen wir mit der Erstellung einer Methode private set _ endangered, die jede Instanz unserer Klasse Endangered aus der Datenbank abrufen wird.
Beachten Sie, dass der Filter before _ action sicherstellt, dass der Wert von @ endangered nur für die Routen index und data festgelegt ist. Dort verarbeiten wir die Daten über bedrohte Haie.
Fügen Sie als Nächstes den folgenden Code zu der Methode index hinzu, um den richtigen Pfad für Benutzer zu bestimmen, die diesen Teil der Anwendung besuchen:
Wenn mehr als 0 Instanzen unserer Klasse Endangered vorhanden sind, leiten wird die Benutzer auf die Route data um, wo sie Informationen über die von ihnen erstellten Haie einsehen können.
Andernfalls sehen sie die Ansicht index.
Fügen Sie als Nächstes unter die Methode index eine Methode data hinzu, die mit der Ansicht data korreliert:
Als Nächstes fügen wir eine Methode hinzu, um den Daten-Upload selbst zu verarbeiten.
Wir nennen diese Methode upload und sie wird eine Sidekiq Worker-Klasse und -methode aufrufen, um den Daten-Upload aus der CSV-Datei durchzuführen.
Im nächsten Schritt erstellen wir die Definition für diese Worker-Klasse, AddEndangeredWorker.
Für den Moment fügen Sie den folgenden Code zu der Datei hinzu, um den Sidekiq Worker aufzurufen, der den Upload ausführt:
Durch den Aufruf der Methode perform _ async in der Klasse AddEndangeredWorker mit der CSV-Datei als Argument, stellt dieser Code sicher, dass die Haidaten und der Upload-Job an Redis übergeben werden.
Die Sidekiq Workers, die wir einrichten werden, überwachen die Job-Warteschlange und reagieren, wenn neue Jobs entstehen.
Nach dem Aufruf von perform _ async leitet unsere Methode upload zu dem Pfad data um, wo Benutzer die hochgeladenen Haie einsehen können.
Als Nächstes fügen wir eine Methode destroy zur Zerstörung der Daten hinzu. Fügen Sie den folgenden Code unter die Methode upload hinzu:
Wie unsere Methode upload beinhaltet unsere Methode destroy einen Aufruf perform _ async der Klasse RemoveEndangeredWorker - den anderen Sidekiq Worker, den wir erstellen werden.
Nach dem Aufruf dieser Methode leitet sie die Benutzer zu dem Stammanwendungspfad um.
Als letzten Schritt zur Verfestigung der Routen unserer Anwendung werden wir den Code config / routes.rb der Datei, in der sich unsere Routendeklarationen befinden, ändern.
Die Datei sieht derzeit wie folgt aus:
Wir müssen die Datei aktualisieren, um die in unserem Controller definierten Routen aufzunehmen: data, upload und destroy.
Unsere Route data wird mit einer GET-Anforderung zum Abrufen der Haifischdaten übereinstimmen, während unsere Routen upload und destroy den POST-Anforderungen zugeordnet werden, die diese Daten hochladen und zerstören.
Fügen Sie den folgenden Code zu der Datei hinzu, um diese Routen zu definieren:
Nachdem Sie Ihr Modell Endangered und Ihre Controller eingerichtet haben, können Sie nun mit der Definition Ihrer Sidekiq Worker-Klassen fortfahren.
Schritt 3 - Definieren von Sidekiq Workers
Wir haben die Methode perform _ async für unsere Sidekiq Worker in unserem Controller aufgerufen, aber wir müssen noch die Worker selbst erstellen.
Erstellen Sie zunächst ein Verzeichnis workers für die Worker:
Öffnen Sie eine Datei für den Worker AddEndangeredWorker:
Wir fügen in dieser Datei Code ein, der uns die Arbeit mit den Daten in unserer CSV-Datei ermöglicht.
Zunächst fügen wir der Datei Code hinzu, der die Klasse erstellt, die Ruby-CSV-Bibliothek einbezieht und sicherstellt, dass diese Klasse als Sidekiq Worker funktioniert:
Wir fügen auch die Option retry: false ein, um sicherzustellen, dass Sidekiq den Upload im Falle eines Fehlschlags nicht erneut versucht.
Fügen Sie als Nächstes den Code für die Funktion perform hinzu:
Die Methode perform empfängt Argumente von der im Controller definierten Methode perform _ async. Daher ist es wichtig, dass die Argumentwerte abgeglichen sind.
Hier übergeben wir in csv _ file, die in dem Controller definierte Variable und verwenden die Methode foreach aus der CSV-Bibliothek, um die Werte in der Datei zu lesen.
Das Setzen von headers: true für diese Schleife stellt sicher, dass die erste Zeile der Datei als eine Zeile von Headers behandelt wird.
Der Block liest dann die Werte von der Datei in die Spalten, die wir für unser Modell Endangered festgelegt haben: name und iucn.
Durch Ausführung dieser Schleife werden für jeden der Einträge in unserer CSV-Datei Instanzen Endangered erstellt.
Als Nächstes erstellen wir einen Worker, der sich um das Löschen dieser Daten kümmert. Öffnen Sie eine Datei für die Klasse RemoveEndangeredWorker:
Fügen Sie den Code zur Definition der Klasse hinzu und stellen Sie sicher, dass sie die CSV-Bibliothek verwendet und als Sidekiq Worker funktioniert:
Fügen Sie als Nächstes eine Methode perform hinzu, um die Zerstörung der Daten von bedrohten Haien zu handhaben:
Die Methode perform ruft destroy _ all in der Klasse Endangered auf, wodurch alle Instanzen der Klasse aus der Datenbank entfernt werden.
Nachdem Sie Ihre Worker eingerichtet haben, können Sie mit der Erstellung eines Layouts für Ihre Ansichten endangered und von Vorlagen für Ihre Ansichten index und data fortfahren, sodass Benutzer bedrohte Haie hochladen und ansehen können.
Schritt 4 - Hinzufügen von Layouts und Ansichtsvorlagen
Damit Benutzer in den Genuss ihrer Informationen über bedrohte Haie kommen, müssen wir uns mit zwei Dingen befassen: dem Layout für die in unserem Controller endangered definierten Ansichten und mit den Ansichtsvorlagen für die Ansichten index und data.
Unsere Anwendung verwendet derzeit ein anwendungsweites Layout, das sich unter app / views / layouts / application.html.erb befindet, eine Navigations-Partiale und ein Layout für die Ansichten sharks.
Das Anwendungslayout prüft, ob ein Inhaltsblock vorhanden ist, der das Laden verschiedener Layouts ermöglicht, je nachdem, mit welchem Teil der Anwendung sich unsere Benutzer beschäftigen: für die Seite home index sehen sie ein Layout und für alle Ansichten über individuelle Haie ein anderes.
Wir können das Layout sharks für unsere Ansichten endangered neu verwenden, da dieses Format auch für die Darstellung von Haidaten in großen Mengen funktioniert.
Kopieren Sie die Layoutdatei sharks, um ein Layout für endangered zu erstellen:
Als Nächstes werden wir an der Erstellung von Ansichtenvorlagen für unsere Ansichten index und data arbeiten.
Öffnen Sie zuerst die Vorlage index:
Löschen Sie den Boilerplate-Code und fügen Sie stattdessen den folgenden Code hinzu, der den Benutzern einige allgemeine Informationen über die bedrohten Kategorien gibt und ihnen die Möglichkeit bietet, Informationen über bedrohte Haie hochzuladen:
Ein form _ tag macht den Daten-Upload möglich, indem eine Post-Aktion auf den endangered _ upload _ path verweist - die Route, die wir für unsere Uploads definiert haben.
Eine Submit-Schaltfläche, die mit dem submit _ tag erstellt wurde, fordert die Benutzer zum "Import Endangered Sharks" (Importieren bedrohter Haie) auf.
Zusätzlich zu diesem Code haben wir einige allgemeine Informationen über ICUN-Codes eingefügt, damit die Benutzer die Daten, die sie sehen werden, interpretieren können.
Öffnen Sie als Nächstes eine Datei für die Ansicht data:
Fügen Sie den folgenden Code hinzu, der eine Tabelle mit den Daten der bedrohten Haie hinzufügt:
Dieser Code enthält noch einmal die ICUN-Statuscodes und eine Bootstrap-Tabelle für die ausgegebenen Daten. Indem wir unsere Variable @ endangered durchschleifen, geben wir den Namen und den ICUN-Status jedes Hais in die Tabelle aus.
Unterhalb der Tabelle haben wir einen weiteren Satz von form _ tags und submit _ tags, die auf den Pfad destroy verweisen, indem Benutzern die Option zum "Delete Endangered Sharks" (Löschen bedrohter Haie) angeboten wird.
Die letzte Änderung, die wir für unsere Ansichten vornehmen, wird in der Ansicht index vorgenommen, die mit unserem Controller home verknüpft ist.
Sie erinnern sich vielleicht daran, dass diese Ansicht als das Stammverzeichnis der Anwendung in config / routes.rb festgelegt ist.
Öffnen Sie diese Datei zur Bearbeitung:
Finden Sie die Spalte in der Zeile, die besagt Sharks are ancient (Haie sind uralt):
Wir haben einen Handlungsaufruf für Benutzer aufgenommen, um mehr über bedrohte Haie zu erfahren, indem wir zuerst eine starke Botschaft weitergeben und dann einen Helfer button _ to hinzufügen, der eine GET-Anfrage an die Route endangered index sendet und Benutzern Zugriff auf diesen Teil der Anwendung gewährt.
Von dort aus können sie Informationen über bedrohte Haie hochladen und anzeigen.
Nachdem Sie nun Ihren Code eingegeben haben, können Sie die Anwendung starten und einige Haie hochladen!
Schritt 5 - Starten von Sidekiq und Testen der Anwendung
Bevor wir die Anwendung starten, müssen wir Migrationen in unserer Datenbank durchführen und Sidekiq starten, um unsere Worker zu aktivieren.
Redis sollte bereits auf dem Server ausgeführt werden, aber wir können das überprüfen, um sicherzugehen.
Sobald all diese Dinge eingerichtet sind, sind wir bereit, die Anwendung zu testen.
Überprüfen Sie zunächst, ob Redis ausgeführt wird:
Sie können nun Sidekiq im Kontext Ihres aktuellen Projekt-Bundles starten, indem Sie den Befehl bundle exec sidekiq verwenden:
Sie werden eine ähnliche Ausgabe wie diese sehen, die anzeigt, dass Sidekiq zur Bearbeitung von Jobs bereit ist:
Öffnen Sie ein zweites Terminalfenster, navigieren Sie zum Verzeichnis < ^ > rails-sidekiq < ^ > und starten Sie Ihren Anwendungsserver.
Wenn Sie die Anwendung lokal ausführen, verwenden Sie den folgenden Befehl:
Wenn Sie mit einem Entwicklungsserver arbeiten, führen Sie den folgenden Befehl aus:
Navigieren Sie im Browser zu localhost: 3000 oder http: / / < ^ > your _ server _ ip < ^ >: 3000.
Damit gelangen Sie nun direkt zu der Ansicht data, da Sie die Haie bereits hochgeladen haben.
Um die Löschfunktionalität zu testen, klicken Sie auf die Schaltfläche Delete Endangered Sharks unterhalb der Tabelle.
Sie sollten nun erneut zu der Hauptseite der Anwendung umgeleitet werden.
Wenn Sie ein letztes Mal auf Which Sharks Are in Danger? klicken,
gelangen Sie zu der Ansicht index zurück, in der Sie die Option haben, die Haie erneut hochzuladen:
Ihre Anwendung wird nun mit Sidekiq Workers ausgeführt, die zur Bearbeitung von Jobs bereit sind und sicherstellen, dass die Benutzer eine gute Erfahrung mit Ihrer Anwendung haben.
Sie haben nun eine funktionierende Rails-Anwendung mit aktiviertem Sidekiq, die es Ihnen ermöglicht, kostspielige Operationen in eine von Sidekiq verwaltete und von Redis unterstützte Job-Warteschlange auszulagern.
Dadurch können Sie die Geschwindigkeit und Funktionalität Ihrer Website im Laufe der Entwicklung verbessern.
Wenn Sie mehr über Sidekiq erfahren möchten, sind die Docs ein guter Ausgangspunkt.
Weitere Informationen über Redis finden Sie in unserer Bibliothek der Redis Ressourcen.
Sie können auch mehr über die Ausführung eines verwalteten Redis-Clusters auf DigitalOcean erfahren, indem Sie sich die Produktdokumentation ansehen.
So verwalten Sie sortierte Sets in Redis
3142
Redis ist ein Open-Source-, In-Memory-Datenspeicher mit Schlüsselwerte-Datenstruktur.
Sortierte Sets in Redis sind ein Datentyp, der den Sets ähnelt. Beide sind Gruppen von Zeichenfolgen, die keine Wiederholungen zulassen.
Der Unterschied besteht darin, dass in einem sortierten Set jedes Mitglied mit einem Score verknüpft ist, wodurch diese vom kleinsten bis zum größten Score sortiert werden können.
So wie bei Sets darf jedes Mitglied eines sortieren Sets in diesem nur einmalig vorkommen, wobei mehrere Mitglieder den gleichen Score haben können.
Dieses Tutorial erklärt, wie man sortierte Sets erstellt, Ihre Mitglieder abruft und entfernt und wie man neue sortierte Sets aus bestehenden Sets erstellen kann.
Dieser Leitfaden ist wie ein Spickzettel mit eigenständigen Beispielen geschrieben.
Sie können beliebig zu jedem Abschnitt springen, der zu der Aufgabe passt, die Sie ausführen möchten.
Die in diesem Leitfaden verwendeten Befehle wurden auf einem Ubuntu 18.04-Server mit der Redis-Version < ^ > 4.0.9 < ^ > getestet.
Um eine ähnliche Umgebung einzurichten, können Sie Schritt 1 unseres Leitfadens Installieren und Sichern von Redis unter Ubuntu 18.04 folgen.
Wir zeigen, wie diese Befehle sich verhalten, wenn sie mit redis-cli, der Befehlszeilenschnittstelle von Redis, angewendet werden.
Beachten Sie, dass bei der Anwendung einer anderen Redis-Schnittstelle - z. B. Redli - die genaue Ausgabe bestimmter Befehle abweichen kann.
Alternativ können Sie ein verwaltetes Exemplar einer Redis-Datenbank bereitstellen, um diese Befehle zu testen. Beachten Sie jedoch, dass abhängig vom Kontrollniveau, das Ihr Datenbankanbieter erlaubt, einige Befehle in diesem Leitfaden nicht wie beschrieben funktionieren könnten.
Um eine von DigitalOcean verwaltete Datenbank bereitzustellen, folgen Sie bitte unserer Produktdokumentation über verwaltete Datenbanken.
Dann müssen Sie entweder Redli installieren oder einen TLS-Tunnel einrichten, um sich mit der verwalteten Datenbank über TLS zu verbinden.
Erstellen von sortierten Sets und Hinzufügen von Mitgliedern
Um ein sortiertes Set zu erstellen, verwenden Sie den Befehl zadd. zadd akzeptiert als Argumente den Namen des Schlüssels, der das sortierte Set speichert, gefolgt vom Score des Mitglieds, das Sie hinzufügen, sowie dem Wert des Mitglieds selbst.
Der folgende Befehl erstellt den Schlüssel eines sortierten Sets namens faveGuitarists mit einem Mitglied, "Joe Pass", das einen Score von 1 hat:
zadd gibt eine ganze Zahl (Integer) aus, die anzeigt, wie viele Mitglieder dem sortierten Set hinzugefügt wurden, wenn es erfolgreich erstellt wurde.
Mit zadd können Sie einem sortierten Set mehr als ein Mitglied hinzufügen.
Beachten Sie, dass deren Scores nicht sequenziell sein müssen. Zwischen den Scores kann es Lücken geben und mehrere Mitglieder aus demselben sortierten Set können den gleichen Score haben:
zadd kann die folgenden Optionen akzeptieren, die Sie nach dem Schlüsselnamen und vor dem ersten Score eines Mitglieds eingeben müssen:
NX oder XX: Diese Optionen haben gegensätzliche Effekte, daher können Sie nur eine davon in eine zadd-Operation einfügen:
NX weist zadd an, die bestehenden Mitglieder nicht zu aktualisieren.
Mit dieser Option fügt zadd nur neue Elemente hinzu.
XX weist zadd an, nur bestehende Elemente zu aktualisieren.
Mit dieser Option fügt zadd niemals neue Mitglieder hinzu.
CH: Normalerweise gibt zadd nur die Anzahl der neuen Elemente aus, die dem sortierten Set hinzugefügt werden. Doch mit dieser Option gibt zadd die Anzahl der geänderten Elemente aus.
Dazu gehören neu hinzugefügte Mitglieder sowie Mitglieder, deren Scores geändert wurden.
INCR: Führt dazu, dass der Befehl den Score-Wert des Mitglieds inkrementiert.
Wenn das Mitglied noch nicht existiert, fügt der Befehl es mit dem Inkrement als Score in das sortierte Set ein, so als wäre sein ursprünglicher Score 0. Mit INCR gibt zadd bei erfolgreicher Ausführung den neuen Score des Mitglieds aus.
Beachten Sie, dass Sie mit dieser Option jeweils nur ein Score / Mitglied-Paar einbeziehen können.
Statt INCR mit zadd einzugeben, können Sie stattdessen den Befehl zincrby verwenden, der sich genauso verhält.
Statt so wie zadd dem Mitglied des sortierten Sets den durch den Score-Wert angezeigten Wert zu geben, inkrementiert er den Score des Mitglieds auf diesen Wert.
Beispiel: Der folgende Befehl inkrementiert den Score des Mitglieds "Stephen Malkmus", der ursprünglich 4 war, um 5 auf 9.
So wie bei der zadd-Befehlsoption INCR wird zincrby, falls das angegebene Mitglied nicht existiert, es mit dem inkrementellen Wert als seinen Score kreieren.
Abrufen von Mitgliedern aus sortierten Sets
Die grundlegendste Möglichkeit, die in einem sortierten Set enthaltenen Mitglieder abzurufen, ist die Verwendung des Befehls zrange.
Dieser Befehl akzeptiert als Argumente den Namen des Schlüssels, dessen Mitglieder Sie abrufen möchten und eine Reihe von darin enthaltenen Mitgliedern. Die Reihe ist durch zwei Zahlen definiert, die nullbasierte Indexe repräsentieren. Das bedeutet, dass 0 das erste Mitglied im sortierten Set (oder das Mitglied mit dem niedrigsten Score) repräsentiert, 1 repräsentiert das nächste, und so fort.
Das folgende Beispiel gibt die ersten vier Mitglieder aus dem im vorherigen Abschnitt erstellten Set faveGuitarists aus:
Beachten Sie: Falls das sortierte Set, das Sie an zrange übergeben, zwei oder mehr Elemente hat, die den gleichen Score teilen, werden diese Elemente in lexikografischer oder alphabetischer Reihenfolge sortiert.
Die Start- und Stopp-Indizes können auch negative Zahlen sein, wobei -1 das letzte Mitglied repräsentiert, -2 das vorletzte Mitglied, und so fort:
zrange akzeptiert zudem das Argument WITHSCORES. Wenn es enthalten ist, gibt es auch die Scores der Mitglieder aus:
zrange kann eine Reihe von Mitgliedern nur in aufsteigender numerischer Reihenfolge ausgeben.
Um das umzukehren und eine Reihe in absteigender Reihenfolge auszugeben, müssen Sie den Befehl zrevrange verwenden.
Stellen Sie sich diesen Befehl als vorübergehende Umkehrung der Reihenfolge des gegebenen sortierten Sets vor der Ausgabe der Mitglieder vor, die in den angegebenen Bereich fallen.
Bei zrevrange repräsentiert 0 das letzte im Schlüssel enthaltene Mitglied, 1 das vorletzte, und so fort:
zrevrange akzeptiert auch die Option WITHSCORES.
Mit dem Befehl zrangebyscore können Sie eine Reihe von Mitgliedern auf der Grundlage ihrer Scores ausgeben.
Im folgenden Beispiel gibt der Befehl jedes Mitglied, das im Schlüssel faveGuitarists enthalten ist und einen Score von 2, 3 oder 4 hat, aus:
In diesem Beispiel ist die Reihe integrativ. Das bedeutet, dass sie Mitglieder mit Scores von 2 oder 4 ausgibt. Sie können jedes Ende der Reihe ausschließen, indem Sie eine offene Klammer davorsetzen (().
Das folgende Beispiel gibt jedes Mitglied mit einem Score, der größer oder gleich 2, jedoch kleiner als 4 ist, aus:
So wie bei zrange akzeptiert zrangebyscore das Argument WITHSCORES.
Außerdem akzeptiert es die Option LIMIT, die Sie verwenden können, um nur eine Auswahl von Elementen aus der Ausgabe von zrangebyscore abzurufen.
Diese Option akzeptiert ein Offset, das das erste Mitglied in der Reihe markiert, die der Befehl zurückgibt, sowie einen Zählstand, der festlegt, wie viele Mitglieder der Befehl insgesamt ausgibt.
Der folgende Befehl z. B. konzentriert sich auf die ersten sechs Mitglieder des sortierten Sets faveGuitarists, gibt aber nur 3 dieser Mitglieder aus, beginnend mit dem zweiten Mitglied in der Reihe, repräsentiert durch 1:
Der Befehl zrevrangebyscore gibt eine umgekehrte Reihe von Mitgliedern auf der Grundlage ihrer Scores aus.
Der folgende Befehl gibt jedes Mitglied des Sets mit einem Score zwischen 10 und 6 aus:
So wie bei zrangebyscore kann zrevrangebyscore sowohl die Option WITHSCORES als auch die Option LIMIT akzeptieren.
Außerdem können Sie jedes Ende der Reihe ausschließen, indem Sie vor dieses eine offene Klammer setzen.
Es kann vorkommen, dass alle Mitglieder in einem sortierten Set den gleichen Score haben.
In einem solchen Fall können Sie redis zwingen, mit dem Befehl zrangebylex eine Reihe von Elementen, die in lexikografischer oder in alphabetischer Reihenfolge sortiert sind, auszugeben.
Um diesen Befehl auszuprobieren, führen Sie den folgenden zadd-Befehl aus, um ein sortiertes Set zu erstellen, in dem jedes Mitglied den gleichen Score hat:
Auf zrangebylex müssen ein Schlüsselname, ein Startintervall und ein Stoppinterval folgen.
Die Start- und Stoppintervalle müssen mit einer offenen runden Klammer (() oder einer offenen eckigen Klammer ([) beginnen:
Beachten Sie, dass dieses Beispiel nur vier der acht Mitglieder im Set ausgegeben hat, obwohl der Befehl eine Reihe von a bis z suchte. Das liegt daran, dass Redis-Werte groß- / kleinschreibungsabhängig sind. Daher wurden die Mitglieder, die mit Großbuchstaben beginnen, aus der Ausgabe ausgeschlossen.
Um diese auszugeben, können Sie Folgendes ausführen:
zrangebylex akzeptiert auch die Sonderzeichen -, das negative Unendlichkeit, und +, das positive Unendlichkeit darstellt.
Daher gibt die folgende Befehlssyntax auch jedes Mitglied des sortierten Sets aus:
Beachten Sie, dass zrangebylex Mitglieder eines sortierten Sets nicht in umgekehrter lexikografischer (ansteigend alphabetischer) Reihenfolge ausgeben kann.
Hierfür verwenden Sie zrevrangebylex:
Da es für die Verwendung mit sortierten Sets bestimmt ist, bei denen jedes Mitglied den gleichen Score hat, akzeptiert zrangebylex nicht die Option WITHSCORES.
Es akzeptiert jedoch die Option LIMIT.
Abrufen von Informationen aus sortierten Sets
Um herauszufinden, wie viele Mitglieder sich in einem gegebenen sortierten Set befinden (oder anders ausgedrückt, um seine Kardinalität zu bestimmen), verwenden Sie den Befehl zcard.
Das folgende Beispiel zeigt, wie viele Mitglieder im Schlüssel faveGuitarists aus dem ersten Abschnitt dieses Leitfadens enthalten sind:
zcount kann Ihnen zeigen, wie viele Elemente in einem gegebenen sortierten Set enthalten sind, die in einen Bereich von Scores fallen.
Die erste Zahl, die auf den Schlüssel folgt, ist der Beginn des Bereichs und die zweite das Ende des Bereichs:
zscore gibt den Score eines angegebenen Mitglieds eines sortierten Sets aus:
Wenn entweder das angegebene Mitglied oder der Schlüssel nicht existieren, gibt zscore als Ausgabe (nil) an.
zrank ist ähnlich wie zscore, aber anstatt den Score des gegebenen Mitglieds auszugeben, gibt es stattdessen seinen Rang aus.
In Redis ist Rang - rank - ein nullbasierter Index der Mitglieder eines sortierten Sets, der nach ihrem Score geordnet wird.
Beispielsweise hat "Joe Pass" einen Score von 1, aber weil dies der niedrigste Score aller Mitglieder im Schlüssel ist, hat er den Rang 0:
Ein weiterer Redis-Befehl namens zrevrank führt die gleiche Funktion wie zrank aus, kehrt aber stattdessen den Rang der Mitglieder im Set um. Im folgenden Beispiel hat das Mitglied "Joe Pass" den niedrigsten Score und daraus folgend den höchsten umgekehrten Rang:
Die einzige Beziehung zwischen dem Score eines Mitglieds und seinem Rang besteht darin, wo sein Score im Verhältnis zu dem der anderen Mitglieder steht.
Wenn zwischen zwei aufeinanderfolgenden Mitgliedern eine Lücke beim Score besteht, reflektiert sich diese nicht in ihrem Rang.
Beachten Sie: Wenn zwei Mitglieder den gleichen Score haben, nimmt das Mitglied, das alphabetisch zuerst kommt, den unteren Rang ein.
Wie zscore geben zrank und zrevrank als Ausgabe (nil) an, wenn der Schlüssel oder das Mitglied nicht existiert.
zlexcount kann Ihnen zeigen, wie viele Mitglieder in einem sortierten Set innerhalb eines lexikografischen Bereichs enthalten sind.
Das folgende Beispiel verwendet das sortierte Set SomervilleSquares aus dem vorherigen Abschnitt:
Dieser Befehl folgt derselben Syntax wie der Befehl zrangebylex. Einzelheiten zum Definieren eines Stringbereichs entnehmen Sie dem vorherigen Abschnitt.
Entfernen von Mitgliedern aus sortierten Sets
Der Befehl zrem kann ein oder mehrere Mitglieder aus einem sortierten Set entfernen:
zrem gibt eine ganze Zahl aus, die anzeigt, wie viele Mitglieder aus dem sortierten Set entfernt wurden:
Es gibt drei Redis-Befehle, die es Ihnen ermöglichen, Mitglieder eines sortierten Sets auf der Grundlage eines Bereichs zu entfernen.
Wenn z. B. jedes Mitglied in einem sortierten Set den gleichen Score hat, können Sie Mitglieder auf der Grundlage eines lexikografischen Bereichs mit zremrangebylex entfernen.
Dieser Befehl verwendet die gleiche Syntax wie zrangebylex.
Das folgende Beispiel entfernt jedes Mitglied, das mit einem Großbuchstaben beginnt, aus dem im vorherigen Abschnitt erstellten Schlüssel SomervilleSquares:
zremrangebylex gibt eine ganze Zahl an, die anzeigt, wie viele Mitglieder entfernt wurden:
Außerdem können Sie Mitglieder auf der Grundlage eines Score-Bereichs mit dem Befehl zremrangebyscore entfernen, der die gleiche Syntax wie der Befehl zrangebyscore verwendet.
Das folgende Beispiel entfernt jedes in faveGuitarists enthaltene Mitglied mit einem Score von 4, 5 oder 6:
Sie können Mitglieder aus einem Set auf der Grundlage eines Rangbereichs mit dem Befehl zremrangebyrank entfernen, der die gleiche Syntax wie zrangebyrank verwendet.
Der folgende Befehl entfernt die drei Mitglieder des sortierten Sets mit den niedrigsten Rängen, die durch einen Bereich von nullbasierten Indizes definiert sind:
Beachten Sie, dass an remrangebyrank übergebene Zahlen auch negativ sein können, wobei -1 den höchsten Rang repräsentiert, -2 den nächsthöheren, und so fort.
Erstellen von neuen sortierten Sets aus bestehenden Sets
Redis enthält zwei Befehle, die es Ihnen ermöglichen, Mitglieder mehrerer sortierter Sets zu vergleichen und auf der Grundlage dieser Vergleiche neue Sets zu erstellen: zinterstore und zunionstore.
Um diese Befehle auszuprobieren, führen Sie die folgenden zadd-Befehle aus, um einige sortierte Beispielsets zu erstellen.
zinterstore findet die Mitglieder, die von zwei oder mehreren sortierten Sets geteilt werden - deren Schnittmenge - und erzeugt ein neues sortiertes Set, das nur diese Mitglieder enthält.
Dieser Befehl muss den Namen eines Zielschlüssels, in dem die sich überschneidenden Mitglieder als sortiertes Set gespeichert werden, die Anzahl der an zinterstore übergebenden Schlüssel und die Namen der Schlüssel, die Sie analysieren möchten, in der Reihenfolge enthalten:
zinterstore gibt dann eine ganze Zahl aus, die die Anzahl der im sortierten Zielset gespeicherten Elemente anzeigt. Da NewKids und Nsync nur ein Mitglied teilen - "Joey" - gibt der Befehl 1 aus:
Bitte beachten Sie: Falls der Zielschlüssel bereits existiert, überschreibt zinterstore dessen Inhalt.
zunionstore erstellt ein neues sortiertes Set, das jedes Mitglied der an den Befehl übergebenen Schlüssel enthält. Dieser Befehl verwendet die gleiche Syntax wie zinterstore und erfordert den Namen eines Zielschlüssels, die Anzahl der an den Befehl übergebenen Schlüssel und die Namen der Schlüssel:
Wie zinterstore gibt zunionstore eine ganze Zahl aus, die die Anzahl der im Zielschlüssel gespeicherten Elemente anzeigt.
Obwohl beide der ursprünglich sortierten Sets fünf Mitglieder enthalten, die sortierten Sets jedoch keine sich wiederholenden Mitglieder enthalten können und jeder Schlüssel ein Mitglied mit dem Namen "Joey" beinhaltet, ist die resultierende ganze Zahl 9:
Wie zinterstore überschreibt zunionstore den Inhalt des Zielschlüssels, wenn dieser bereits existiert.
Um Ihnen beim Erstellen von neuen sortierten Sets mit zinterstore und zunionstore mehr Kontrolle über die Scores der Mitglieder zu geben, akzeptieren beide Befehle die Optionen WEIGHTS und AGGREGATE.
Auf die Option WEIGHTS folgt eine Zahl für jedes im Befehl enthaltene sortierte Set, welche die Scores jedes Mitglieds bewertet oder vervielfacht.
Die erste Zahl nach der Option WEIGHTS bewertet die Scores des ersten an den Befehl übergebenen Schlüssels, die zweite Zahl den zweiten Schlüssel, und so fort.
Das folgende Beispiel erstellt ein neues sortiertes Set, das die überschneidenden Schlüssel aus den sortierten Sets NewKids und Nsync enthält.
Es bewertet die Scores im Schlüssel NewKids mit dem Faktor drei und die im Schlüssel Nsync mit dem Faktor sieben:
Wenn die Option WEIGHTS nicht enthalten ist, wird die Bewertung standardmäßig auf 1 gesetzt, sowohl für zinterstore als auch für zunionstore.
AGGREGATE akzeptiert drei Suboptionen.
Die erste davon, SUM, implementiert das Standardverhalten von zinterstore und zunionstore, indem sie die Scores von übereinstimmenden Mitgliedern den kombinierten Sets hinzufügt.
Wenn Sie in zwei sortierten Sets, die ein Mitglied teilen, eine zinterstore- oder zunionstore-Operation ausführen, dieses Mitglied aber in jedem Set einen anderen Score hat, können Sie die Operation mit der Suboption MIN zwingen, den niedrigeren der beiden Punktstände zuzuweisen.
Da die beiden sortierten Sets nur ein übereinstimmendes Mitglied mit dem gleichen Score (3) haben, erstellt dieser Befehl ein neues Set mit einem Mitglied, das den niedrigeren der beiden bewerteten Scores hat:
Ebenso kann AGGREGATE zinterstore oder zunionstore zwingen, mit der Option MAX den höheren der beiden Scores zuzuweisen:
Dieser Befehl erstellt ein neues Set mit einem Mitglied, "Joey", das den höheren der beiden bewerteten Scores hat:
Es kann hilfreich sein, sich WEIGHTS als eine Möglichkeit vorzustellen, die Scores von Mitgliedern vorübergehend zu manipulieren, bevor sie analysiert werden.
Ebenso ist es hilfreich, sich die Option AGGREGATE als Entscheidungsmöglichkeit vorzustellen, wie die Scores der Mitglieder kontrolliert werden können, bevor sie in ihre neuen Sets aufgenommen werden.
Dieser Leitfaden erläutert eine Reihe von Befehlen, die zur Erstellung und Verwaltung von sortierten Sets in Redis verwendet werden.
Wenn es andere verwandte Befehle, Argumente oder Verfahren gibt, die Sie in diesem Leitfaden erklärt sehen möchten, schreiben Sie bitte eine Frage oder einen Vorschlag in das Kommentarfeld unten.
Weitere Informationen zu Redis finden Sie in unserer Tutorialserie Verwalten einer Redis-Datenbank.
So verwenden Sie Node.js-Module mit npm und package.json
3209
Aufgrund von solchen Eigenschaften wie ihrer schnellen Performance bezüglich des Input / Output (I / O) und ihrer bekannten JavaScript-Syntax ist Node.js schnell zu einer beliebten Laufzeitumgebung für die Backend-Webentwicklung geworden.
Doch mit wachsendem Interesse werden immer größere Anwendungen erstellt und die Verwaltung der Komplexität der Codebasis und ihrer Abhängigkeiten wird immer schwieriger.
Node.js organisiert diese Komplexität mithilfe von Modulen. Das sind beliebige einzelne JavaScript-Dateien mit Funktionen oder Objekten, die von anderen Programmen oder Modulen verwendet werden können.
Eine Sammlung von einem oder mehreren Modulen wird allgemein als Paket bezeichnet. Die Pakete selbst werden von Paketmanagern organisiert.
Der Node.js-Paketmanager (npm) ist der standardmäßige und beliebteste Paketmanager im Node.js.-Ökosystem und wird in erster Linie zur Installation und Verwaltung externer Module in einem Node.js-Projekt verwendet.
Er wird auch häufig genutzt, um eine breite Palette von CLI-Tools zu installieren und Projektskripte auszuführen. npm verfolgt die in einem Projekt installierten Module mit der Datei package.json, die sich im Verzeichnis eines Projekts befindet und Folgendes enthält:
Alle für ein Projekt benötigten Module und ihre installierten Versionen
Alle Metadaten zu einem Projekt, wie z. B. Autor, Lizenz usw.
Skripts, die ausgeführt werden können, um Aufgaben im Projekt zu automatisieren
Wenn Sie komplexere Node.js-Projekte erstellen, bietet Ihnen das Verwalten Ihrer Metadaten und Abhängigkeiten mit der package.json-Datei besser vorhersehbare Builds, da alle externen Abhängigkeiten gleich gehalten werden.
Die Datei verfolgt diese Informationen automatisch; während Sie die Datei direkt ändern können, um die Metadaten Ihres Projekts zu aktualisieren, müssen Sie selten direkt mit ihr interagieren, um Module zu verwalten.
In diesem Tutorial verwalten Sie Pakete mit npm.
Der erste Schritt besteht darin, die package.json-Datei zu erstellen und zu verstehen.
Diese verwenden Sie dann, um alle Module zu verfolgen, die Sie in Ihrem Projekt installieren.
Zum Schluss listen Sie Ihre Paketabhängigkeiten auf, aktualisieren Ihre Pakete, deinstallieren Ihre Pakete und führen eine Prüfung zum Auffinden von Sicherheitsmängeln in Ihren Paketen durch.
Schritt 1 - Erstellen einer package.json-Datei
Beginnen wir dieses Tutorial, indem wir das Beispielprojekt einrichten - ein fiktives Node.js-locator-Modul, das die IP-Adresse des Benutzers ermittelt und das Herkunftsland ausgibt.
In diesem Tutorial codieren Sie das Modul nicht.
Die von Ihnen verwalteten Pakete wären jedoch relevant, wenn Sie es entwickeln würden.
Zuerst erstellen Sie eine package.json-Datei, um nützliche Metadaten über das Projekt zu speichern und um Ihnen die Verwaltung der abhängigen Node.js-Module des Projekts zu erleichtern.
Wie die Endung bereits andeutet, handelt es sich um eine JSON-Datei (JavaScript Object Notation).
JSON ist ein Standardformat für die gemeinsame Nutzung, das auf JavaScript-Objekten basiert und aus Daten besteht, die als Schlüssel-Wert-Paare gespeichert sind.
Wenn Sie mehr über JSON erfahren möchten, lesen Sie unseren Artikel Einführung zu JSON.
Da eine package.json-Datei zahlreiche Eigenschaften enthält, kann ihre manuelle Erstellung umständlich sein, ohne eine Vorlage von einer anderen Stelle zu kopieren und einzufügen.
Um das zu erleichtern, bietet npm den Befehl init.
Das ist ein interaktiver Befehl, der Ihnen eine Reihe von Fragen stellt und eine package.json-Datei auf der Grundlage Ihrer Antworten erstellt.
Verwenden des Befehls init
Erstellen Sie zuerst ein Projekt, mit dem Sie das Verwalten von Modulen praktizieren können.
Erstellen Sie in Ihrer Shell einen neuen Ordner namens locator:
Gehen Sie dann in den neuen Ordner:
Initialisieren Sie nun die interaktive Eingabeaufforderung, indem Sie Folgendes eingeben:
< $> note Anmerkung: Wenn Ihr Code Git zur Versionskontrolle verwendet, erstellen Sie zuerst das Git-Repository und führen Sie dann npm init aus.
Der Befehl versteht automatisch, dass er in einem Git-fähigen Ordner ist.
Wenn ein Git-Remote eingestellt ist, füllt es automatisch die Felder repository, bugs und homepage für Ihre package.json-Datei aus.
Wenn Sie das Repo nach Erstellung der package.json-Datei initialisiert haben, müssen Sie diese Informationen selbst eingeben.
Weitere Informationen zur Git-Versionskontrolle finden Sie in unserer Serie Einführung zu Git: Installation, Verwendung und Zweige.
Zuerst werden Sie mit name zur Eingabe des Namens Ihres neuen Projekts aufgefordert.
Standardmäßig nimmt der Befehl an, dass dies der Name des Ordners ist, in dem Sie sind.
Standardwerte für jede Eigenschaft werden in Klammern () gezeigt.
Da der Standardwert für name die Zwecke dieses Tutorials erfüllt, drücken Sie die Eingabetaste, um ihn zu akzeptieren.
Der nächste einzugebende Wert ist version.
Neben name ist dieses Feld erforderlich, wenn Ihr Projekt mit anderen im npm-Paket-Repository geteilt wird.
< $> note Anmerkung: Es wird erwartet, dass Node.js-Pakete dem Leitfaden Semantic Versioning (semver) folgen, einem Leitfaden für semantische Versionierung.
Die erste Ziffer ist daher die MAJOR-Versionsziffer, die sich nur ändert, wenn sich die API ändert.
Die letzte Ziffer ist die PATCH-Version, die sich ändert, wenn Fehler behoben werden.
Drücken Sie die Eingabetaste, damit die Standardversion akzeptiert wird.
Das nächste Feld ist description - eine nützliche Zeichenfolge, um zu erklären, was Ihr Node.js-Modul tut.
Unser fiktive locator würde die IP-Adresse des Benutzers ermitteln und das Herkunftsland ausgeben.
Eine passende description wäre Findet das Ursprungsland der eingehenden Anfrage. Geben Sie etwas in dieser Art ein und drücken Sie die Eingabetaste.
Die description ist sehr nützlich, wenn jemand nach Ihrem Modul sucht.
Die nächste Eingabeaufforderung fragt Sie nach dem entry point.
Wenn jemand Ihr Modul installiert und mit requires anfordert, wird das, was Sie in entry point festlegen, als erster Teil Ihres Programms geladen.
Der Wert muss der relative Speicherort einer JavaScript-Datei sein und wird der main-Eigenschaft des package.json hinzugefügt.
Drücken Sie die Eingabetaste, um den Standardwert zu speichern.
< $> note Anmerkung: Die meisten Module haben eine index.js-Datei als Haupteingabepunkt.
Das ist der Standardwert für die main-Eigenschaft des package.json, die der Eingangspunkt für npm-Module ist.
Wenn kein package.json vorhanden ist, wird Node.js standardmäßig versuchen, index.js zu laden.
Als Nächstes werden Sie nach einem test command gefragt, einem ausführbaren Skript oder Befehl zur Ausführung Ihres Projekttests.
In vielen beliebten Node.js-Modulen werden Tests mit Mocha, Jest, Jasmine oder anderen Test-Frameworks geschrieben und ausgeführt.
Da das Testen über den Umfang dieses Artikels hinausgeht, lassen Sie diese Option jetzt leer und drücken Sie die Eingabetaste, um fortzufahren.
Der Befehl init fragt dann nach dem GitHub-Repository des Projekts.
Sie werden es in diesem Beispiel nicht verwenden, also lassen Sie auch das leer.
Nach der Eingabeaufforderung des Repository fragt der Befehl nach keywords.
Diese Eigenschaft ist ein Array von Zeichenfolgen mit nützlichen Begriffen, die Personen zum Finden Ihres Repositorys verwenden können.
Am besten ist es, wenn Sie eine kleine Anzahl von Wörtern haben, die für Ihr Projekt wirklich relevant sind, damit die Suche gezielter erfolgen kann.
Erstellen Sie eine Liste dieser Schlüsselwörter als Zeichenfolge mit Kommas, die die einzelnen Werte trennen.
Geben Sie für dieses Beispielprojekt ip, geo, country bei der Eingabeaufforderung ein.
Das fertige package.json verfügt nun über drei Elemente im Array für keywords.
Das nächste Feld in der Eingabeaufforderung ist author.
Das ist nützlich für die Benutzer Ihres Moduls, die Kontakt mit Ihnen aufnehmen möchten.
Wenn zum Beispiel jemand ein Exploit in Ihrem Modul entdeckt, kann er auf diesem Wege das Problem melden, damit Sie es beheben können. Das author-Feld ist eine Zeichenfolge im folgenden Format: "< ^ > Name < ^ >\ < < ^ > Email < ^ >\ > (< ^ > Website < ^ >)".
Beispielsweise ist "Sammy\ < sammy @ your _ domain\ > (https: / / your _ domain)" ein gültiger Autor.
Die Daten zur E-Mail und Website sind optional - ein gültiger Autor kann auch nur ein Name sein.
Fügen Sie Ihre Kontaktdaten als Autor hinzu und bestätigen Sie mit der Eingabetaste.
Schließlich werden Sie mit license zur Eingabe der Lizenz aufgefordert.
Dadurch werden die rechtlichen Berechtigungen und Beschränkungen für Benutzer bei der Verwendung Ihres Moduls festgelegt.
Viele Node.js-Module sind Open-Source-Module, sodass npm die Standardeinstellung auf ISC setzt.
Zu diesem Zeitpunkt würden Sie Ihre Lizenzoptionen überprüfen und entscheiden, was für Ihr Projekt am besten geeignet ist.
Weitere Informationen zu verschiedenen Arten von Open-Source-Lizenzen finden Sie in dieser Lizenzliste aus der Open Source Initiative.
Wenn Sie keine Lizenz für ein privates Repository bereitstellen möchten, können Sie bei der Eingabeaufforderung UNLICENSED eingeben.
Verwenden Sie für dieses Beispiel die standardmäßige ISC-Lizenz und drücken Sie die Eingabetaste, um den Prozess abzuschließen.
Der Befehl init zeigt nun die package.json-Datei, die erstellt wird.
Das wird ähnlich aussehen wie folgt:
Sobald die Informationen mit dem übereinstimmen, was Sie hier sehen, drücken Sie die Eingabetaste, um den Prozess abzuschließen und die Datei package.json zu erstellen.
Mit dieser Datei können Sie ein Verzeichnis der Module führen, die Sie für Ihr Projekt installieren.
Nachdem Ihre package.json-Datei nun vorhanden ist, können Sie im nächsten Schritt die Installation von Modulen testen.
Schritt 2 - Installieren von Modulen
In der Softwareentwicklung ist es üblich, externe Bibliotheken zu nutzen, um untergeordnete Aufgaben in Projekten auszuführen.
Dadurch kann sich der Entwickler auf die Geschäftslogik konzentrieren und die Anwendung schneller und effizienter gestalten.
Wenn beispielsweise unser locater-Beispielmodul eine externe API-Anfrage stellen muss, um geografische Daten zu erhalten, könnten wir eine HTTP-Bibliothek verwenden, um diese Aufgabe zu erleichtern.
Da unser Hauptziel darin besteht, sachbezogene geografische Daten an den Benutzer auszugeben, könnten wir ein Paket installieren, das uns HTTP-Anfragen erleichtert - anstatt diesen Code für uns selbst neu zu schreiben, was eine Aufgabe wäre, die den Rahmen unseres Projekts sprengt.
Wir führen dieses Beispiel nun durch.
In Ihrer locator-Anwendung nutzen Sie die axios-Bibliothek, die Ihnen bei der Erstellung von HTTP-Anfragen hilft.
Installieren Sie sie, indem Sie Folgendes in Ihre Shell eingeben:
Sie beginnen diesen Befehl mit npm install, wodurch das Paket installiert wird (der Einfachheit halber können Sie npm i verwenden).
Anschließend listen Sie die Pakete, die installiert werden sollen, durch ein Leerzeichen getrennt auf.
In diesem Fall ist das axios.
Schließlich beenden Sie den Befehl mit dem optionalen Parameter --save, der angibt, dass axios als Projektabhängigkeit gespeichert wird.
Wenn die Bibliothek installiert ist, sehen Sie eine Ausgabe, die der folgenden ähnelt:
Öffnen Sie nun die Datei package.json mit einem Texteditor Ihrer Wahl.
Sie sehen eine neue Eigenschaft, wie im Folgenden hervorgehoben:
Die Option --save hat npm angewiesen, die Datei package.json mit dem Modul und der Version zu aktualisieren, die gerade installiert wurden.
Das ist sehr gut, da andere Entwickler, die an Ihren Projekten arbeiten, leicht erkennen können, welche externen Abhängigkeiten benötigt werden.
< $> note Anmerkung: Vielleicht ist Ihnen das ^ vor der Versionsnummer für die axios-Abhängigkeit aufgefallen.
Erinnern Sie sich daran, dass die semantische Versionierung aus drei Ziffern besteht: MAJOR, MINOR und PATCH.
Das Symbol ^ bedeutet, dass jede höhere MINOR- oder PATCH-Version diese Versionseinschränkung erfüllen würde.
Wenn Sie zu Beginn einer Versionsnummer ~ sehen, erfüllen nur höhere PATCH-Versionen die Einschränkung.
Wenn Sie mit der Prüfung von package.json fertig sind, beenden Sie die Datei.
Entwicklungsabhängigkeiten
Pakete, die für die Entwicklung eines Projekts verwendet werden, nicht aber für die Zusammenstellung oder die Ausführung in der Produktion, werden als Entwicklungsabhängigkeiten bezeichnet.
Sie sind nicht notwendig, damit Ihr Modul oder Ihre Anwendung in der Produktion funktioniert, können aber beim Schreiben des Codes hilfreich sein.
Beispielsweise ist es üblich, dass Entwickler Code-Linter verwenden, um sicherzustellen, dass ihr Code bewährte Praktiken verfolgt und um den Stil konsistent zu halten.
Das ist für die Entwicklung nützlich, erhöht aber die Menge des Verteilbaren, ohne beim Einsatz in der Produktion einen spürbaren Nutzen zu bieten.
Installieren Sie einen Linter als Entwicklungsabhängigkeit für Ihr Projekt.
Probieren Sie Folgendes in Ihrer Shell aus:
In diesem Befehl haben Sie das Flag --save-dev verwendet.
Dadurch wird eslint als Abhängigkeit gespeichert, die nur für die Entwicklung benötigt wird.
Beachten Sie auch, dass Sie Ihrem Abhängigkeitsnamen @ 6.0.0 hinzugefügt haben.
Wenn Module aktualisiert werden, werden sie mit einer Version getaggt.
Das @ weist npm an, nach einem bestimmten Tag des Moduls, das Sie installieren, zu suchen.
Ohne ein bestimmtes Tag installiert npm die letzte getaggte Version.
Öffnen Sie package.json erneut:
eslint wurde als eine devDependencies gespeichert, zusammen mit der zuvor von Ihnen angegebenen Versionsnummer.
Beenden Sie package.json.
Automatisch generierte Dateien: node _ modules und package-lock.json
Wenn Sie zum ersten Mal ein Paket in einem Node.js-Projekt installieren, erstellt npm automatisch den Ordner node _ modules, um die für Ihr Projekt erforderlichen Module zu speichern, und die Datei package-lock.json, die Sie zuvor geprüft haben.
Bestätigen Sie, dass sich diese in Ihrem Arbeitsverzeichnis befinden.
Geben Sie in Ihrer Shell ls ein und drücken Sie die Eingabetaste.
Sie sehen folgende Ausgabe:
Der Ordner node _ modules enthält jede installierte Abhängigkeit für Ihr Projekt.
In den meisten Fällen sollten Sie diesen Ordner nicht in Ihr versionskontrolliertes Repository übertragen.
Wenn Sie weitere Abhängigkeiten installieren, wird die Größe dieses Ordners schnell anwachsen.
Darüber hinaus werden in der Datei package-lock.json die genauen installierten Versionen in einer knapperen Form aufgezeichnet, sodass das Einfügen von node _ modules nicht notwendig ist.
Während die Datei package.json Abhängigkeiten auflistet, die uns die geeigneten Versionen zeigen, die für das Projekt installiert werden sollten, verfolgt die Datei package-lock.json alle Änderungen in package.json oder node _ modules und zeigt uns die genaue Version des installierten Pakets.
Normalerweise übergeben Sie diese an Ihr versionskontrolliertes Repository statt an node _ modules, da dies eine sauberere Darstellung all Ihrer Abhängigkeiten bietet.
Installieren aus package.json
Mit Ihren Dateien package.json und package-lock.json können Sie schnell die gleichen Projektabhängigkeiten einrichten, bevor Sie die Entwicklung eines neuen Projekts starten.
Um dies zu demonstrieren, gehen Sie in Ihrem Verzeichnisbaum eine Ebene höher und erstellen Sie einen neuen Ordner namens cloned _ locator auf derselben Verzeichnisebene wie locator:
Gehen Sie in Ihr neues Verzeichnis:
Kopieren Sie nun die Dateien package.json und package-lock.json von locator zu cloned _ locator:
Um die erforderlichen Module für dieses Projekt zu installieren, geben Sie Folgendes ein:
npm wird nach einer package-lock.json-Datei suchen, um die Module zu installieren.
Wenn keine Lock-Datei verfügbar ist, würde npm aus der Datei package.json lesen, um die Installationen zu bestimmen.
Normalerweise ist es schneller, aus package-lock.json zu installieren, da die Lock-Datei die genaue Version von Modulen und deren Abhängigkeiten enthält. So muss npm nicht lange nach einer geeigneten Version für die Installation suchen.
Beim Einsatz in der Produktion können Sie die Entwicklungsabhängigkeiten überspringen.
Erinnern Sie sich daran, dass Entwicklungsabhängigkeiten im Abschnitt devDependencies von package.json gespeichert werden und keinen Einfluss auf die Ausführung Ihrer Anwendung haben. Wenn Sie Module als Teil des CI / CD-Prozesses zur Bereitstellung Ihrer Anwendung installieren, lassen Sie die dev-Abhängigkeiten weg, indem Sie Folgendes ausführen:
Das Flag --production ignoriert den Abschnitt devDependencies während der Installation.
Bleiben Sie vorerst bei Ihrem Entwicklungs-Build.
Bevor Sie zum nächsten Abschnitt übergehen, kehren Sie zum Ordner locator zurück:
Globale Installationen
Bisher haben Sie npm-Module für das locator-Projekt installiert. npm bietet Ihnen auch die Möglichkeit, Pakete global zu installieren.
Das bedeutet, dass das Paket wie jeder andere Shell-Befehl für Ihren Benutzer im weiteren System verfügbar ist.
Diese Fähigkeit ist für die vielen Node.js-Module nützlich, die CLI-Tools sind.
Vielleicht möchten Sie zum Beispiel über das locator-Projekt bloggen, an dem Sie gerade arbeiten.
Dazu können Sie eine Bibliothek wie Hexo verwenden, um Ihren statischen Website-Blog zu erstellen und zu verwalten.
Installieren Sie die Hexo-CLI wie folgt:
Um ein Paket global zu installieren, hängen Sie das Flag -g an den Befehl an.
< $> note Anmerkung: Wenn Sie beim Versuch, dieses Paket global zu installieren, eine Fehlermeldung bezüglich der Berechtigung erhalten, benötigt Ihr System möglicherweise Superuser-Rechte, um den Befehl auszuführen.
Versuchen Sie es erneut mit sudo npm i hexo-cli -g.
Testen Sie, ob das Paket erfolgreich installiert wurde, indem Sie Folgendes eingeben:
Bisher haben Sie gelernt, wie Sie Module mit npm installieren.
Sie können Pakete lokal in ein Projekt installieren, entweder als Produktions- oder Entwicklungsabhängigkeit.
Sie können auch Pakete installieren, die auf bereits existierenden package.json- oder package-lock.json-Dateien basieren. So können Sie mit den gleichen Abhängigkeiten entwickeln wie Ihre Peers.
Außerdem können Sie das Flag -g verwenden, um Pakete global zu installieren und auf diese zuzugreifen - unabhängig davon, ob Sie sich in einem Node.js-Projekt befinden oder nicht.
Nachdem Sie nun Module installieren können, werden Sie im nächsten Abschnitt Techniken zur Verwaltung Ihrer Abhängigkeiten anwenden.
Schritt 3 - Verwalten von Modulen
Ein vollständiger Paketmanager kann viel mehr als nur Module installieren. npm verfügt über mehr als 20 Befehle bezüglich der Verwaltung von Abhängigkeiten.
In diesem Schritt werden Sie folgende Aufgaben erledigen:
Module auflisten, die Sie installiert haben.
Module auf eine neuere Version aktualisieren.
Module deinstallieren, die Sie nicht mehr benötigen.
Eine Sicherheitsprüfung in Ihren Modulen ausführen, um Sicherheitsmängel zu finden und zu beheben.
Obwohl diese Beispiele in Ihrem locator-Ordner angewendet werden, können alle diese Befehle auch global ausgeführt werden, indem Sie das Flag -g am Ende anhängen - genau wie bei der globalen Installation.
Module auflisten
Wenn Sie wissen möchten, welche Module in einem Projekt installiert sind, ist es einfacher, den list- oder ls-Befehl zu verwenden, anstatt das package.json direkt zu lesen.
Standardmäßig zeigt ls den gesamten Abhängigkeitenbaum - die Module, auf die Ihr Projekt angewiesen ist, und die Module, auf die Ihre Abhängigkeiten angewiesen sind.
Das kann etwas unhandlich sein, wenn Sie einen Überblick auf hoher Ebene über die installierten Systeme haben wollen.
Um nur die von Ihnen installierten Module ohne deren Abhängigkeiten zu zeigen, geben Sie in Ihrer Shell Folgendes ein:
Mit der Option --depth können Sie angeben, welche Ebene des Abhängigkeitenbaums Sie sehen möchten.
Wenn diese 0, ist, sehen Sie nur Ihre Abhängigkeiten auf höchster Ebene.
Module aktualisieren
Das ist eine gute Praxis, Ihre npm-Module auf dem neuesten Stand zu halten.
Es erhöht die Wahrscheinlichkeit, die neuesten Sicherheitskorrekturen für ein Modul zu erhalten.
Verwenden Sie den Befehl outdated, um zu prüfen, ob Module aktualisiert werden können:
Sie erhalten eine ähnliche Ausgabe:
Der Befehl listet zuerst unter Package das installierte Paket und unter Current die aktuelle Version auf.
Die Spalte Wanted zeigt, welche Version Ihre Versionsanforderung in package.json erfüllt.
Die Spalte Latest zeigt die neueste veröffentlichte Version des Moduls.
Die Spalte Location gibt an, wo sich das Paket im Abhängigkeitenbaum befindet.
Der Befehl outdated hat so wie ls das Flag --depth.
Die Standardeinstellung für depth ist 0.
Scheinbar können Sie eslint auf eine neuere Version aktualisieren.
Verwenden Sie den update- oder up-Befehl wie folgt:
Die Ausgabe des Befehls enthält die installierte Version:
Wenn Sie alle Module auf einmal aktualisieren möchten, geben Sie ein:
Module deinstallieren
Der npm-Befehl uninstall kann Module aus Ihren Projekten entfernen.
Das bedeutet, dass das Modul nicht länger im Ordner node _ modules installiert sein wird. Auch wird es in den Dateien package.json und package-lock.json nicht mehr zu sehen sein.
Die Entfernung von Abhängigkeiten aus einem Projekt ist eine normale Aktivität im Zyklus der Softwareentwicklung.
Vielleicht löst eine Abhängigkeit ein Problem nicht so wie zuvor angepriesen oder es bietet keine zufriedenstellenden Entwicklungsergebnisse.
In diesen Fällen kann es besser sein, die Abhängigkeit zu deinstallieren und Ihr eigenes Modul zu erstellen.
Stellen Sie sich vor, dass axios nicht das Entwicklungsergebnis bietet, das Sie sich für die Erstellung von HTTP-Anfragen vorgestellt haben.
Desinstallieren Sie axios mit dem uninstall- oder un-Befehl, indem Sie Folgendes eingeben:
Sie gibt nicht ausdrücklich an, dass axios entfernt wurde.
Um zu überprüfen, ob es deinstalliert wurde, listen Sie erneut die Abhängigkeiten auf:
Jetzt sehen wir, dass nur eslint installiert ist:
Das zeigt, dass Sie das Paket axios erfolgreich desinstalliert haben.
Module prüfen
npm bietet einen audit-Befehl, um auf potenzielle Sicherheitsmängel in Ihren Abhängigkeiten zu verweisen.
Um den Prüfbefehl audit in Aktion zu sehen, installieren Sie eine veraltete Version des Moduls request, indem Sie Folgendes ausführen:
Wenn Sie diese veraltete Version von request installieren, erhalten Sie eine Ausgabe, die der folgenden ähnelt:
npm teilt Ihnen mit, dass Sie Schwachstellen in Ihren Abhängigkeiten haben.
Um weitere Informationen zu erhalten, prüfen Sie Ihr gesamtes Projekt mit:
Der Befehl audit zeigt in der Ausgabe Tabellen, in denen Sicherheitsmängel hervorgehoben werden:
Sie können den Pfad der Schwachstelle sehen und manchmal bietet npm Ihnen Möglichkeiten, sie zu beheben. Sie können wie vorgeschlagen den Befehl zur Aktualisierung oder den Unterbefehl fix des audit ausführen.
Geben Sie Folgendes in Ihre Shell ein:
Sie sehen eine ähnliche Ausgabe:
npm konnte zwei der Pakete sicher aktualisieren und Ihre Schwachstellen um die gleiche Anzahl verringern.
Sie haben jedoch immer noch vier Schwachstellen in Ihren Abhängigkeiten.
Der Befehl audit fix korrigiert nicht immer jedes Problem.
Obwohl eine Version eines Moduls eine Sicherheitsschwachstelle aufweisen kann, könnte eine Aktualisierung auf eine Version mit einer anderen API den Code weiter oben im Abhängigkeitenbaum aufbrechen.
Sie können den Parameter --force auf folgende Weise verwenden, um sicherzustellen, dass die Schwachstellen nicht mehr vorhanden sind:
Wie bereits erwähnt, wird dies nicht empfohlen, solange Sie sich nicht sicher sind, dass die Funktionalität hiervon nicht beeinträchtigt wird.
In diesem Tutorial haben Sie verschiedene Übungen durchlaufen, die zeigen, wie Node.js-Module in Paketen organisiert sind und wie diese Pakete von npm verwaltet werden.
Sie haben in einem Node.js-Projekt npm-Pakete als Abhängigkeiten verwendet, indem Sie eine package.json-Datei erstellt und verwaltet haben. Diese Datei beinhaltet eine Aufzeichnung der Metadaten Ihres Projekts einschließlich der Module, die Sie installiert haben.
Neben dem Auflisten Ihres Abhängigkeitenbaums Ihrer Projekte sowie dem Prüfen und Aktualisieren veralteter Module haben Sie das npm-CLI-Tool zum Installieren, Aktualisieren und Entfernen von Modulen verwendet.
In Zukunft können Sie Module verwenden, um vorhandenen Code einzusetzen und dadurch die Entwicklungszeit beschleunigen, da Sie die Funktionalität nicht wiederholen müssen.
Sie werden zudem in der Lage sein, eigene npm-Module zu erstellen, die wiederum von anderen über npm-Befehle verwaltet werden.
Experimentieren Sie als Nächstes mit dem, was Sie in diesem Tutorial gelernt haben, indem Sie einige der vielen existierenden Pakete installieren und testen.
Sehen Sie sich an, was das Ökosystem bietet, um die Problemlösung zu erleichtern.
Sie könnten zum Beispiel TypeScript, eine übergeordnete Programmiersprache von JavaScript, ausprobieren oder Ihre Website mit Cordova in mobile Anwendungen verwandeln.
Wenn Sie mehr über Node.js lernen möchten, lesen Sie unsere anderen Node.js-Tutorials.
Hosten einer Website mit Caddy unter Ubuntu 18.04
3942
Caddy ist ein auf Einfachheit und Sicherheit ausgelegter Webserver, der mit einer Reihe von Funktionen ausgestattet ist, die für das Hosting von Websites nützlich sind.
Beispielsweise kann er automatisch TLS-Zertifikate von Let 's Encrypt beziehen und verwalten, um HTTPS zu aktivieren, und er bietet Unterstützung für HTTP / 2.
HTTPS ist ein System zur Sicherung des Datenverkehrs zwischen Ihren Benutzern und Ihrem Server und entwickelte sich schnell zu einer grundlegenden Erwartung für jede Website, die in Produktion ausgeführt wird - ohne HTTPS warnen Chrome und Firefox, dass Ihre Website "Nicht sicher" ist, wenn Benutzer versuchen, Anmeldeinformationen einzugeben.
Früher wurde als Methode zur Installation von Caddy empfohlen, vorgefertigte Binärdateien von der Caddy-Projekt-Website herunterzuladen.
Änderungen in der Lizenzierungsweise von Caddy bedeuten jedoch, dass Sie diese vorgefertigten Binärdateien nicht mehr für kommerzielle Zwecke verwenden dürfen, es sei denn, Sie zahlen eine Lizenzgebühr, auch wenn Sie Caddy nur intern innerhalb eines Unternehmens verwenden.
Glücklicherweise ist der Quellcode von Caddy immer noch vollständig Open-Source und Sie können Caddy selbst erstellen, um Lizenzprobleme zu vermeiden.
In diesem Tutorial erstellen Sie Caddy aus dem Quellcode und verwenden ihn zum Hosten einer mit HTTPS gesicherten Website.
Dazu müssen Sie ihn kompilieren, mit einer Caddyfile konfigurieren und Plugins installieren.
Am Ende lernen Sie, wie Sie Ihre Installation aktualisieren, wenn eine neue Version verfügbar ist.
Die auf Ihrem Server installierte Go language-Toolchain.
Folgen Sie unserem Leitfaden "Installieren von Go und Einrichten einer lokalen Programmierungsumgebung unter Ubuntu 18.04" zur Einrichtung von Go.
Sie müssen keine Beispielprojekte erstellen.
Schritt 1 - Erstellen von Caddy
In diesem Schritt erstellen Sie den Caddy aus dem Quellcode mit der Möglichkeit, später Plugins hinzuzufügen, ohne den Quellcode von Caddy zu ändern.
Für die Zwecke dieses Tutorials werden Sie den Quellcode unter ~ / caddy speichern.
Erstellen Sie das Verzeichnis durch Ausführen des folgenden Befehls:
Speichern Sie den Quellcode zur Ausführung und Anpassung von Caddy in einer Datei namens caddy.go.
Dieser Code importiert Caddy direkt von Github (mit Git) und startet ihn von der Eingangsfunktion main.
Wenn Sie die Telemetrie aktivieren möchten, entkommentieren Sie die Zeile caddymain.EnableTelemetry und setzen den Wert auf true.
Damit caddy.go die importierten Abhängigkeiten verwenden kann, müssen Sie es als Modul initialisieren:
An diesem Punkt sind Sie bereit, die Stock-Version von Caddy aus dem obigen Quellcode zu erstellen, indem Sie Folgendes ausführen:
Es werden zahlreiche Ausgaben erscheinen, die detailliert aufführen, welche Bibliotheken von Go als Abhängigkeiten heruntergeladen werden, die zum Kompilieren notwendig sind.
Die resultierende ausführbare Datei wird unter $GOPATH / bin gespeichert, wie in den Voraussetzungen erklärt.
Führen Sie Caddy nach Abschluss aus:
Dies bedeutet, dass Caddy erfolgreich gestartet wurde und auf dem Port 2015 verfügbar ist. Sie können die Warnmeldung ignorieren, da diese Grenze in späteren Schritten ohne Ihr Zutun angepasst wird.
Drücken Sie zum Beenden STRG + C.
Sie haben Caddy nun erstellt und ausgeführt.
Im nächsten Schritt installieren Sie Caddy als Dienst, damit er automatisch beim Booten startet, und passen dann seine Eigentums- und Berechtigungseinstellungen an, um die Sicherheit des Servers zu gewährleisten.
Schritt 2 - Installieren von Caddy
Nachdem Sie nun bestätigt haben, dass Sie Caddy erstellen und ausführen können, ist es an der Zeit, einen systemd-Dienst zu konfigurieren, damit Caddy automatisch beim Systemstart gestartet werden kann.
Um mehr über systemd zu erfahren, besuchen Sie unser Tutorial Systemd-Grundlagen.
Verschieben Sie zunächst die Caddy-Binärdatei nach / usr / local / bin, den Standardspeicherort für Binärdateien, die nicht vom Ubuntu-Paketmanager verwaltet werden und für den Systembetrieb nicht entscheidend sind:
Übertragen Sie anschließend die Eigentümerschaft der Caddy-Binärdatei an den root user:
Dadurch wird verhindert, dass andere Konten die ausführbare Datei modifizieren.
Aber selbst wenn der root user Eigentümer von Caddy ist, ist es ratsam, ihn nur mit anderen, Nicht-Root-Konten auszuführen, die auf dem System vorhanden sind.
Dadurch wird sichergestellt, dass im Falle einer Kompromittierung von Caddy (oder eines anderen Programms) der Angreifer nicht in der Lage sein wird, die Binärdatei zu verändern oder Befehle als root auszuführen.
Legen Sie anschließend die Berechtigungen der Binärdatei auf 755 fest - dies gibt root volle Lese- / Schreib- / Ausführungsberechtigungen für die Datei, während andere Benutzer nur in der Lage sind, sie zu lesen und auszuführen:
Da der Caddy-Prozess nicht als root ausgeführt wird, verhindert Linux, dass er an die Ports 80 und 443 (die Standardports für HTTP bzw. HTTPS) gebunden wird, da es sich hierbei um mit Berechtigungen versehene Operationen handelt.
Um in Ihrer Domäne leicht zugänglich zu sein, muss Caddy abhängig vom Protokoll an einen dieser Ports gebunden werden.
Andernfalls müssten Sie der Domain-URL in Ihrem Browser eine bestimmte Port-Nummer hinzufügen, um die Inhalte anzuzeigen, die er bereitstellen wird.
Damit Caddy an niedrige Ports gebunden werden kann, ohne als root ausgeführt zu werden, führen Sie den folgenden Befehl aus:
Das Dienstprogramm setcap legt die Dateifähigkeiten fest.
In diesem Befehl weist es der Caddy-Binärdatei die Fähigkeit CAP _ NET _ BIND _ SERVICE zu, die es einer ausführbaren Datei ermöglicht, sich an einen niedrigeren Port als 1024 zu binden.
Sie haben nun die Einrichtung der Caddy-Binärdatei abgeschlossen und können mit dem Schreiben der Caddy-Konfiguration beginnen.
Erstellen Sie ein Verzeichnis, in dem Sie die Konfigurationsdateien von Caddy speichern, indem Sie den folgenden Befehl ausführen:
Legen Sie dann die richtigen Benutzer- und Gruppenberechtigungen dafür fest:
Die Einstellung des Benutzers als root und der Gruppe als www-data stellt sicher, dass Caddy Lese- und Schreibzugriff auf den Ordner hat (über die Gruppe www-data) und dass nur das Superuser-Konto die gleichen Rechte zum Lesen und Ändern hat. www-data ist der Standardbenutzer und die Standardgruppe für Webserver unter Ubuntu.
In einem späteren Schritt werden Sie die automatische TLS-Zertifikatsbereitstellung von Let 's Encrypt aktivieren.
Erstellen Sie in Vorbereitung darauf ein Verzeichnis, in dem alle TLS-Zertifikate gespeichert werden, die Caddy erhalten wird, und geben ihm die gleichen Eigentumsregeln wie das Verzeichnis / etc / caddy:
Caddy muss Zertifikate in dieses Verzeichnis schreiben und aus diesem Verzeichnis lesen können, um Anfragen zu verschlüsseln.
Aus diesem Grund sollten Sie die Berechtigungen für das Verzeichnis / etc / ssl / caddy so ändern, dass es nur für root und www-data zugänglich ist:
Erstellen Sie anschließend ein Verzeichnis, um die Dateien zu speichern, die Caddy hosten wird:
Setzten Sie dann den Eigentümer und die Gruppe des Verzeichnisses auf www-data:
Caddy liest seine Konfiguration aus einer Datei namens Caddyfile, die unter / etc / caddy gespeichert ist.
Erstellen Sie die Datei auf der Festplatte, indem Sie Folgendes ausführen:
Um den Caddy-Dienst zu installieren, laden Sie die Unit-Datei systemd aus dem Caddy Github-Repository in / etc / systemd / system herunter, indem Sie Folgendes ausführen:
Ändern Sie die Berechtigungen der Dienstdatei, damit sie nur durch ihren Eigentümer root geändert werden kann:
Dann laden Sie systemd neu, um den Caddy-Dienst zu erkennen:
Prüfen Sie, ob systemd den Caddy-Dienst erkannt hat, durch Ausführen von systemctl status:
Wenn Sie die gleiche Ausgabe sehen, wurde der neue Dienst korrekt von systemd erkannt.
Als Teil der anfänglichen Server-Einrichtungsvoraussetzung haben Sie ufw, die unkomplizierte Firewall, aktiviert und SSH-Verbindungen zugelassen.
Damit Caddy HTTP- und HTTPS-Verkehr von Ihrem Server aus bereitstellen kann, müssen Sie diese in ufw zulassen, indem Sie den folgenden Befehl ausführen:
Überprüfen Sie mit ufw status, ob Ihre Änderungen funktioniert haben:
Ihre Installation von Caddy ist nun abgeschlossen, aber noch nicht konfiguriert, um etwas bereitzustellen.
Im nächsten Schritt konfigurieren Sie Caddy, um Dateien aus dem Verzeichnis / var / www bereitzustellen.
Schritt 3 - Konfigurieren von Caddy
In diesem Abschnitt schreiben Sie die grundlegende Caddy-Konfiguration für die Bereitstellung von statischen Dateien von Ihrem Server aus.
Beginnen Sie mit der Erstellung einer grundlegenden HTML-Datei in / var / www namens index.html:
Wenn sie in einem Webbrowser angezeigt wird, zeigt sie eine Kopfzeile mit dem Text This page is being served via Caddy.
Öffnen Sie die zuvor erstellte Konfigurationsdatei Caddyfile für die Bearbeitung:
Hierbei handelt es sich um eine grundlegende Caddy-Konfiguration, die besagt, dass der Port 80 Ihres Servers mit Dateien aus / var / www bedient und mit gzip komprimiert werden soll, um die Seitenladezeiten auf der Client-Seite zu reduzieren.
In den meisten Fällen können Sie die Konfigurationsanweisungen mit Caddy weiter anpassen.
Beispielsweise können Sie die gzip-Komprimierung auf HTML- und PHP-Dateien beschränken und die Komprimierungsstufe auf 6 setzen (1 ist die niedrigste und 9 die höchste Komprimierungsstufe), indem Sie die Anweisung mit geschweiften Klammern erweitern und Unteranweisungen darunter auflisten:
Caddy verfügt über eine große Anzahl verschiedener Richtlinien für viele Anwendungsfälle.
Beispielsweise könnte die Anweisung fastcgi für das Aktivieren von PHP nützlich sein.
Die Anweisung markdown könnte verwendet werden, um Markdown-Dateien vor der Bereitstellung automatisch in HTML zu konvertieren, was für die Erstellung eines einfachen Blogs nützlich sein könnte.
Um zu testen, ob alles korrekt funktioniert, starten Sie den Caddy-Dienst:
Führen Sie als Nächstes systemctl status aus, um Informationen über den Status des Caddy-Dienstes zu erhalten:
Sie sehen dann Folgendes:
Sie können nun in einem Webbrowser zu der IP Ihres Servers browsen.
Ihre Beispiel-Webseite zeigt an:
Nachricht von Caddy
Sie haben Caddy nun konfiguriert, um statische Dateien von Ihrem Server aus bereitzustellen.
Im nächsten Schritt erweitern Sie die Funktionalität von Caddy durch die Verwendung von Plugins.
Schritt 4 - Verwenden von Plugins
Plugins bieten eine Möglichkeit, das Verhalten von Caddy zu ändern und zu erweitern.
Sie bieten allgemein mehr Konfigurationsanweisungen, die Sie je nach Anwendungsfall verwenden können.
In diesem Abschnitt fügen Sie Plugins hinzu und verwenden sie durch die Installation des Plugin minify, das überschüssige Leerzeichen entfernt und den Code, der an den Client gesendet wird, aufräumt, wodurch der Platzbedarf und die Ladezeiten weiter reduziert werden.
Das GitHub-Repository des Plugins minify ist hacdias / caddy-minify.
Navigieren Sie zum Verzeichnis mit dem Quellcode, den Sie in Schritt Eins erstellt haben:
Um Caddy ein Plugin hinzuzufügen, müssen Sie es in die Datei caddy.go importieren, die Sie zum Erstellen von Caddy verwendet haben.
Öffnen Sie caddy.go zum Bearbeiten:
Importieren Sie das Plugin minify, indem Sie die hervorgehobene Zeile hinzufügen, wie hier beschrieben:
Einige Plugins erfordern möglicherweise einige geringfügige Konfigurationsanpassungen, lesen Sie also unbedingt die Dokumentation des Plugins, das Sie installieren.
Eine Liste beliebter Plugins finden Sie im linken Fenster der Caddy-Dokumentation unter Plugins.
Sie müssen Caddy bei jedem Hinzufügen eines neuen Plugins neu erstellen.
Das liegt daran, dass Go eine kompilierte Programmiersprache ist, d. h. der Quellcode wird vor der Ausführung in Computercode umgewandelt.
Ihre Änderung an der Importdeklaration hat den Quellcode verändert, wirkt sich aber erst nach der Kompilierung auf die Binärdatei aus.
Verwenden Sie den Befehl go install, um Caddy zu kompilieren:
Verschieben Sie nach dem Abschluss die erzeugte Binärdatei nach / usr / local / bin und richten Sie Berechtigungen für die Binärdatei ein, wie Sie es zuvor getan haben.
Sie müssen diese Schritte bei jeder Neuerstellung von Caddy durchführen, um dessen Funktionalität und Sicherheit zu gewährleisten:
Um das Plugin minify zu verwenden, müssen Sie die Anweisung minify zu Ihrer Caddyfile hinzufügen.
Aktivieren Sie das Plugin, indem Sie die folgende Zeile zum Konfigurationsblock hinzufügen:
Starten Sie nun Ihren Server mit systemctl neu:
Caddy wird nun ausgeführt und wird alle bereitgestellten Daten, einschließlich der zuvor erstellten Datei index.html, "minifizieren".
Sie können die "Minifizierung" bei der Arbeit beobachten, indem Sie den Inhalt Ihrer Domäne mit curl abrufen:
Sie werden die folgende Ausgabe sehen:
Beachten Sie, dass alle unnötigen Leerzeichen entfernt wurden, was zeigt, dass das Plugin minify funktioniert.
In diesem Schritt haben Sie gelernt, wie Sie Caddy mit Plugins erweitern können.
Als Nächstes aktivieren Sie HTTPS, indem Sie das Plugin tls.dns.digitalocean installieren.
Schritt 5 - Aktivieren von automatischem TLS mit Let 's Encrypt
In diesem Abschnitt aktivieren Sie die automatische Bereitstellung und Erneuerung von Zertifikaten durch Let 's Encrypt, wobei TXT-DNS-Datensätze zur Verifizierung verwendet werden.
Um die Verwendung von TXT-DNS-Datensätzen zu verifizieren, installieren Sie mit der DigitalOcean-API namens tls.dns.digitalocean ein Plugin für die Schnittstelle.
Das Verfahren zur Installation dieses Plugins ist fast identisch mit der Installation des Plugins minify im vorherigen Schritt.
Öffnen Sie zunächst caddy.go:
Fügen Sie das Repository des Plugins zu imports hinzu:
Kompilieren Sie es durch Ausführen von:
Stellen Sie sicher, dass Caddy über systemctl angehalten wird, und beenden Sie dann die Installation des Plugins, indem Sie die neu erstellte Caddy-Binärdatei kopieren und noch einmal die Eigentümerschaft und die Berechtigungen festlegen:
Konfigurieren Sie anschließend Caddy, um mit der API von DigitalOcean zum Festlegen von DNS-Einträgen zu arbeiten.
Caddy muss auf dieses Token als Umgebungsvariable zugreifen, um den DNS von DigitalOcean zu konfigurieren, damit Sie die Unit-Datei systemd bearbeiten können:
Suchen Sie die Zeile, die mit Environment = beginnt, im Abschnitt [Service].
Diese Zeile definiert die Umgebungsvariablen, die an den Caddy-Prozess übergeben werden sollen.
Fügen Sie am Ende dieser Zeile ein Leerzeichen ein, fügen Sie dann eine Variable DO _ AUTH _ TOKEN hinzu, gefolgt von dem Token, das Sie gerade generiert haben:
Speichern und schließen Sie diese Datei und laden Sie dann den Daemon systemd wie zuvor neu, um sicherzustellen, dass die Konfiguration aktualisiert wird:
Führen Sie systemctl status aus, um zu überprüfen, ob Ihre Konfigurationsänderungen in Ordnung sind:
Sie müssen einige geringfügige Änderungen an Ihrer Caddyfile vornehmen, also öffnen Sie diese zur Bearbeitung:
Fügen Sie die hervorgehobenen Zeilen in die Caddyfile ein. Achten Sie darauf, dass Sie dabei < ^ > your _ domain < ^ > durch Ihre Domäne ersetzen (statt nur Port: 80) und gzip kommentieren:
Die Verwendung einer Domäne statt nur eines Ports für den Hostnamen führt dazu, dass Caddy Anfragen über HTTP bedient.
Die Anweisung tls konfiguriert das Verhalten von Caddy bei Verwendung von TLS, und die Unteranweisung dns gibt an, dass Caddy das DNS-01-System anstelle des HTTP-01 verwenden soll.
Damit ist Ihre Website für die Bereitstellung bereit.
Starten Sie Caddy mit systemctl und aktivieren Sie es dann mit enable, damit es beim Starten ausgeführt wird.
Wenn Sie zu Ihrer Domäne browsen, werden Sie automatisch zu HTTPS umgeleitet, wobei dieselbe Meldung angezeigt wird.
Ihre Installation von Caddy ist nun abgeschlossen und gesichert und Sie können je nach Anwendungsfall weitere Anpassungen vornehmen.
Wenn Sie Caddy aktualisieren möchten, wenn eine neue Version verfügbar ist, müssen Sie die Datei go.mod (im gleichen Verzeichnis gespeichert) aktualisieren, die wie folgt aussieht:
Der hervorgehobene Bereich ist die von Ihnen verwendete Version von Caddy.
Wenn eine neue Version auf Github veröffentlicht wird (siehe die Seite mit den Versions-Tags), können Sie die bestehende Version in go.mod durch diese ersetzen und Caddy entsprechend der ersten beiden Schritte kompilieren.
Sie können dasselbe für alle importierten Plugins tun.
Sie haben Caddy nun auf Ihrem Server installiert und konfiguriert, sodass statische Seiten auf Ihrer gewünschten Domäne bereitgestellt werden, die mit kostenlosen Let 's Encrypt TLS-Zertifikaten gesichert sind.
Ein guter nächster Schritt wäre, einen Weg für die Benachrichtigung zu finden, wenn neue Versionen von Caddy veröffentlicht werden.
Sie können beispielsweise den Atom-Feed für Caddy-Veröffentlichungen oder den dedizierten Dienst dependencies.io verwenden.
Weitere Informationen zur Konfiguration von Caddy finden Sie in der Dokumentation von Caddy.
Installieren und Konfigurieren eines SNMP-Daemons und -Clients unter Ubuntu 18.04
3940
Der Autor hat das Internet Archive dazu ausgewählt, im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
Ein großer Teil der Tätigkeit eines Systemadministrators besteht darin, genaue Informationen über Ihre Server und Infrastruktur zu sammeln.
Es gibt eine Reihe von Tools und Optionen, die zur Sammlung und Verarbeitung dieser Informationen dienen.
Viele von ihnen basieren auf einer Technologie namens SNMP.
SNMP steht für Simple Network Management Protokol, einfaches Netzwerkverwaltungs-Protokoll.
Es ist eine Methode, mit der Server Informationen über ihren aktuellen Status austauschen können, sowie ein Kanal, über den ein Administrator vordefinierte Werte ändern kann.
Während das Protokoll selbst schlank ist, kann die Struktur von Programmen, die SNMP implementieren, schnell an Komplexität zunehmen.
Weitere Informationen zu den Grundlagen des SNMP-Protokolls finden Sie in unserem Artikel Eine Einführung in SNMP.
In diesem Tutorial richten Sie die Tools zur Kommunikation mit SNMP ein.
Zur Demonstration verwenden Sie zwei Ubuntu 18.04-Server.
Einer enthält den SNMP-Manager, der mit dem Agenten zur Implementierung von Netzwerkgeräten spricht.
Dieser wird als Manager-Server bezeichnet.
Der andere Server enthält den SNMP-Agenten, der auf die Befehle des Manager-Servers reagiert.
Dieser wird als Agenten-Server bezeichnet.
Sie könnten sich dafür entscheiden, den Agenten auch auf dem Manager-Rechner zu installieren, aber wenn sie getrennt gehalten werden, lässt sich leichter zeigen, welche Funktionalität von jeder Komponente bereitgestellt wird.
Zwei Ubuntu 18.04-Server, die gemäß des Leitfadens zur Ersteinrichtung des Servers für Ubuntu 18.04 eingerichtet wurde, einschließlich eines non-root users, aber mit sudo-Berechtigungen, und einer mit ufw konfigurierten Firewall.
Schritt 1 - Installieren des SNMP-Daemons und der Dienstprogramme
Um herauszufinden, wie SNMP auf einem System implementiert werden kann, beginnen Sie mit der Installation des Daemons und der Tools auf Ihren Ubuntu-Servern.
Melden Sie sich von Ihrem lokalen Rechner aus als Ihr non-root user am Manager-Server an:
Aktualisieren Sie den Paketindex für den APT-Paketmanager:
Installieren Sie anschließend die SNMP-Software:
Das Paket snmp bietet eine Sammlung von Befehlszeilen-Tools zur Ausgabe von SNMP-Anfragen an Agenten.
Das Paket snmp-mibs-downloader hilft bei der Installation und Verwaltung von Management Information Base (MIB) -Dateien, die Netzwerkobjekte verfolgen.
Öffnen Sie dann ein neues Terminal auf Ihrem lokalen Rechner und melden Sie sich bei dem Agenten-Server an:
Aktualisieren Sie auf dem Agenten-Server den Paketindex:
Installieren Sie dann den SNMP-Daemon
Beachten Sie, dass Sie das Paket snmp-mibs-downloader nicht benötigen, da der Agenten-Server keine MIB-Dateien verwaltet.
Nachdem Sie nun diese Komponenten installiert haben, konfigurieren Sie Ihren Manager-Server.
Schritt 2 - Konfigurieren des SNMP Manager-Servers
Wie bereits erwähnt, wird der Großteil der Arbeit auf dem Agenten-Server stattfinden, sodass Ihre Konfiguration auf dem Manager-Server weniger aufwendig ist.
Um sicherzustellen, dass SNMP-Tools die von Ihnen installierten zusätzlichen MIB-Daten verwenden können, müssen Sie nur eine Datei ändern.
Öffnen Sie auf Ihrem Manager-Server die Datei / etc / snmp / snmp.conf in Ihrem Texteditor mit sudo-Berechtigungen.
In dieser Datei gibt es einige Kommentare und eine einzige unkommentierte Zeile.
Damit der Manager die MIB-Dateien importieren kann, kommentieren Sie die Zeile mibs: aus:
Speichern und schließen Sie snmp.conf, indem Sie STRG + X drücken, gefolgt von Y und dann der Eingabetaste, wenn Sie nano verwenden.
Die Konfiguration des Manager-Servers ist nun abgeschlossen, aber Sie müssen diesen Server immer noch verwenden, um bei der Konfiguration Ihres Agenten-Servers zu helfen, was Sie im nächsten Schritt tun werden.
Schritt 2 - Konfigurieren des SNMP Agenten-Servers
Als echtes Client-Server-System verfügt der Agenten-Server über keine der externen Tools, die zur Konfiguration seiner eigenen SNMP-Einrichtung erforderlich sind.
Sie können einige Konfigurationsdateien ändern, um einige Änderungen vorzunehmen, aber die meisten Änderungen, die Sie vornehmen müssen, werden durch eine Verbindung zu Ihrem Agenten-Server von Ihrem Management-Server aus vorgenommen.
In diesem Tutorial verwenden Sie Version 3 des SNMP-Protokolls.
Im Gegensatz zu SNMPv1 und v2 enthält bei SNMPv3 jede Nachricht Sicherheitsparameter, die verschlüsselt sind.
In diesem Schritt konfigurieren Sie die SNMPv3-Authentifizierungs- und Zugriffskontrollregeln.
Um zu beginnen, öffnen Sie auf Ihrem Agenten-Server die Konfigurationsdatei des Daemons mit sudo-Berechtigungen:
Sie müssen einige Änderungen darin vornehmen.
Diese werden hauptsächlich für das Bootstrapping Ihrer Konfiguration verwendet, damit Sie diese von Ihrem anderen Server aus verwalten können.
Zuerst müssen Sie die Anweisung agentAddress ändern.
Momentan ist sie so eingestellt, dass sie nur vom lokalen Rechner ausgehende Verbindungen zulässt.
Sie müssen die aktuelle Zeile auskommentieren und die Zeile darunter, die alle Verbindungen ermöglicht, entkommentieren.
< $> note Anmerkung: Da das Zulassen aller Verbindungen auf diese Art keine optimale Sicherheitspraxis ist, empfiehlt es sich, diese bald wieder zu sperren, nachdem das Bootstrapping abgeschlossen ist.
Als Nächstes fügen Sie vorübergehend eine Zeile createUser ein.
Diese Anweisungen werden normalerweise nicht in dieser Datei aufbewahrt; Sie werden sie in Kürze wieder entfernen.
Der Benutzer, den Sie erstellen, wird als bootstrap bezeichnet und wird als Vorlage verwendet, in der Sie Ihren ersten tatsächlichen Benutzer erstellen.
Die SNMP-Pakete führen dies durch das Klonen der Eigenschaften des Benutzers aus.
Wenn Sie einen neuen Benutzer definieren, müssen Sie den Authentifizierungstyp (MD5 oder SHA) angeben und eine Passphrase mit mindestens acht Zeichen bereitstellen.
Wenn Sie vorhaben, die Übertragung zu verschlüsseln, wie Sie es in diesem Tutorial tun werden, müssen Sie auch das Datenschutzprotokoll (DES oder AES) und optional eine Datenschutzprotokoll-Passphrase angeben.
Wenn keine Passphrase für das Datenschutzprotokoll angegeben wird, wird die Authentifizierungs-Passphrase auch für das Datenschutzprotokoll verwendet.
Fügen Sie diese Zeile createUser am Ende der Datei hinzu:
Nachdem Sie nun einen neuen Benutzer angegeben haben, können Sie die Zugriffsebene einrichten, die dieser Benutzer haben wird.
In diesem Tutorial richten Sie diesen für Ihren Benutzer bootstrap als auch für den neuen Benutzer demo ein, den Sie anlegen werden.
Sie werden Ihnen Lese- und Schreibberechtigungen erteilen, indem Sie die Anweisung rwuser verwenden (die Alternative ist rouser für den Nur-Lese-Zugriff).
Sie werden auch die Verwendung der Verschlüsselung erzwingen, indem Sie nach dem Benutzer priv angeben.
Wenn Sie den Benutzer auf einen bestimmten Teil der MIB beschränken möchten, können Sie am Ende der Zeile den höchsten Objektidentifikator (OID) angeben, auf den der Benutzer Zugriff haben soll.
Für die Zwecke dieses Tutorials werden Ihre beiden Zeilen wie folgt aussehen:
Wenn Sie mit diesen Änderungen fertig sind, speichern und schließen Sie die Datei.
Um diese Änderungen zu implementieren, starten Sie den Dienst snmpd auf Ihrem Agenten-Server neu:
Der SNMP-Daemon horcht auf Port: 161 auf Verbindungen.
Konfigurieren Sie UFW, um Verbindungen vom Manager-Server zu diesem Port zu erlauben:
Mehr über UFW erfahren Sie in Einrichten einer Firewall mit UFW unter Ubuntu 18.04.
Nachdem nun der Agenten-Server konfiguriert ist, können Sie sich über den Manager-Server mit Ihrem Agenten-Server verbinden, um die Verbindung zu überprüfen.
Schritt 4 - Verifizieren der Authentifizierung zum Agenten-Server
In diesem Schritt führen Sie einen Test durch, um sicherzustellen, dass Sie mit Ihrem Konto bootstrap eine Verbindung zum Agenten-Server herstellen können.
Zuvor wird in diesem Tutorial jedoch ein wenig über die allgemeine Struktur des Sendens eines SNMP-Befehls gesprochen.
Wenn Sie die in dem Paket snmp (die net-snmp Software Suite) enthaltenen Tools verwenden, gibt es einige Muster, wie Sie die Befehle aufrufen müssen.
Als Erstes müssen Sie sich bei dem SNMP-Daemon authentifizieren, mit dem Sie kommunizieren möchten.
Dazu müssen in der Regel einige Informationen bereitgestellt werden.
Die häufigsten sind die folgenden:
-v: Dieses Flag wird zur Angabe der Version des SNMP-Protokolls verwendet, die Sie verwenden möchten.
Dieses Tutorial verwendet v3.
-c: Dieses Flag wird verwendet, wenn Sie SNMP v1 oder v2-ähnliche Community-Strings zur Authentifizierung verwenden.
Da Sie eine benutzerbasierte Authentifizierung im Stil von v3 verwenden, brauchen Sie dies nicht zu tun.
-u: Dieser Parameter wird verwendet, um den Benutzernamen anzugeben, als den Sie sich authentifizieren möchten.
Um mit SNMP etwas zu lesen oder zu ändern, müssen Sie sich mit einem bekannten Benutzernamen authentifizieren.
-l: Dieser Parameter wird verwendet, um die Sicherheitsebene anzugeben, mit der Sie sich verbinden.
Die möglichen Werte sind noAuthNoPriv für keine Authentifizierung und keine Verschlüsselung, authNoPriv für die Authentifizierung aber keine Verschlüsselung sowie authPriv für die Authentifizierung und Verschlüsselung.
Der Benutzername, den Sie verwenden, muss so konfiguriert sein, dass er auf der von Ihnen angegebenen Sicherheitsstufe funktioniert, sonst wird die Authentifizierung nicht erfolgreich sein.
-a: Dieser Parameter wird verwendet, um das verwendete Authentifizierungsprotokoll anzugeben.
Die möglichen Werte sind MD5 oder SHA.
Dies muss mit den Informationen übereinstimmen, die beim Anlegen des Benutzers angegeben wurden.
-x: Dieser Parameter wird verwendet, um das verwendete Verschlüsselungsprotokoll anzugeben.
Die möglichen Werte sind DES oder AES.
Dies ist immer dann erforderlich, wenn die Berechtigungsspezifikation des Benutzers priv dahinter enthält, wodurch die Verschlüsselung obligatorisch wird.
-A: Wird verwendet, um die Authentifizierungs-Passphrase anzugeben, die bei der Erstellung des Benutzers angegeben wurde.
-X: Dies ist die Verschlüsselungs-Passphrase, die bei der Erstellung des Benutzers angegeben wurde.
Wenn keine Passphrase aber ein Verschlüsselungsalgorithmus angegeben wurde, wird die Authentifizierungs-Passphrase verwendet.
Dies ist erforderlich, wenn der Parameter -x angegeben wird oder wenn die Berechtigungsspezifikation eines Benutzers ein priv dahinter hat, das eine Verschlüsselung erfordert.
Anhand dieser Informationen können Sie Ihre Befehle erstellen.
In Anbetracht der Art und Weise, wie Sie Ihren Benutzer bootstrap einrichten, werden die Befehle, die Sie mit diesem Konto verwenden, wie folgt aussehen:
Testen Sie von Ihrem Manager-Server aus, um sicherzustellen, dass Ihr Konto bootstrap verfügbar ist.
Geben Sie Folgendes ein, um die Systeminformationen für den Agenten-Server anzuzeigen:
Die Zeichenfolge 1.3.6.1.2.1.1.1.0 ist die OID, die für die Anzeige von Systeminformationen verantwortlich ist.
Sie gibt die Ausgabe von uname -a auf dem Remote-System zurück.
Nachdem Sie nun überprüft haben, dass Sie sich auf dem Server, auf dem der SNMP-Daemon ausgeführt wird, authentifizieren können, können Sie mit der Erstellung Ihres regulären Benutzerkontos fortfahren.
Schritt 5 - Einrichten des regulären Benutzerkontos
Obwohl Sie die Berechtigungen für das Benutzerkonto demo in der Datei snmpd.conf angegeben haben, haben Sie diesen Benutzer noch nicht wirklich erstellt.
In diesem Schritt verwenden Sie den Benutzer bootstrap als Vorlage für Ihren neuen Benutzer.
Sie werden dies mit dem Tool snmpusm tun, das für die Benutzerverwaltung verwendet wird.
Auf dem Manager-Server können Sie den Benutzer aus der Vorlage mithilfe des Tools snmpusm und der folgenden allgemeinen Syntax anlegen.
Mit dem, was Sie über die zu übermittelnden Authentifizierungs-Flags wissen, und unter Nutzung des bereits vorhandenen Benutzerkontos (bootstrap) können Sie einen Benutzer erstellen, der zu den bereits definierten Benutzerrechten passt (demo).
Der Befehl sieht wie folgt aus:
Sie erhalten die folgende Meldung:
Sie haben nun einen voll funktionsfähigen Benutzer namens demo auf Ihrem Agenten-Server.
Es verwendet jedoch nach wie vor die gleichen Authentifizierungsinformationen wie das Konto bootstrap.
Um die Sicherheit zu erhöhen, können Sie das Passwort ändern.
Dieses Mal verwenden Sie das Konto demo zur Authentifizierung.
Denken Sie daran, dass Passwörter mindestens acht Zeichen lang sein müssen:
Sie erhalten die folgende Meldung zurück:
Sie können Ihre neuen Berechtigungsnachweise und Ihr Passwort testen, indem Sie den Agenten-Server fragen, wie lange der SNMP-Dienst bereits ausgeführt wird.
Sie verwenden den Befehl snmpget, um einen einzelnen Wert von dem Agenten-Server zu erhalten.
Nutzen Sie diesmal die von Ihnen heruntergeladenen zusätzlichen MIB-Definitionen, um den Wert nach Namen anstelle der numerischen ID von OID zu erfragen.
Sie erhalten einen Wert zurück, der das letzte Mal repräsentiert, als der entfernte SNMP-Daemon neu gestartet wurde:
Sie haben nun ein funktionierendes Benutzerkonto namens demo.
Im nächsten Schritt vereinfachen Sie das Arbeiten mit SNMP-Befehlen durch die Konfiguration des Client.
Schritt 6 - Erstellen einer Client-Konfigurationsdatei
Sie haben zu diesem Punkt wahrscheinlich bemerkt, dass die Authentifizierungsdetails für alle Ihre SNMP-Befehle bei jeder Anfrage ziemlich statisch sind.
Anstatt diese jedes Mal einzugeben, können Sie eine clientseitige Konfigurationsdatei erstellen, die die Berechtigungsnachweise enthält, mit denen Sie sich verbinden.
Die Client-Konfigurationsdatei kann an zwei verschiedenen Stellen abgelegt werden, je nachdem, wie weit Sie sie freigeben möchten.
Wenn Sie Ihre Anmeldedaten für jeden gültigen Benutzer auf Ihrem Management-Rechner freigeben möchten, können Sie Ihre Konfigurationsdetails in der globalen Datei snmp.conf auf dem Manager-Server ablegen.
Sie müssten diese Datei mit sudo-Berechtigungen öffnen:
Wenn Sie jedoch die Berechtigungsnachweise für die Authentifizierung nur für Ihren Benutzer definieren möchten, können Sie ein verstecktes Verzeichnis .snmp im Stammverzeichnis Ihres Benutzers auf dem Manager-Server erstellen und die Datei dort anlegen:
Unabhängig von Ihrer Entscheidung, wo Sie Ihre Konfiguration ablegen, bleibt der Inhalt derselbe.
Die Befehle, die Sie zur Authentifizierung verwenden, sind in der folgenden Tabelle aufgeführt.
In der rechten Spalte sehen Sie die Namen der Anweisungen, die zur Erstellung dieser Konfigurationsdetails innerhalb der Datei snmp.conf verwendet werden:
Befehls-Flag
Übersetzte snmp.conf-Anweisung
-u < ^ > username < ^ >
Der SNMPv3-Benutzername, mit dem sich authentifiziert wird.
defSecurityName < ^ > username < ^ >
-l authPriv
Die Sicherheitsstufe, mit der sich authentifiziert wird.
defSecurityLevel authPriv
-a MD5
Das zu verwendende Authentifizierungsprotokoll.
defAuthType MD5
-x DES
Das zu verwendende Datenschutz (Verschlüsselungs) -Protokoll.
defPrivType DES
-A < ^ > passphrase < ^ >
Die Authentifizierungs-Passphrase für den angegebenen Benutzernamen.
defAuthPassphrase < ^ > passphrase < ^ >
-X < ^ > passphrase < ^ >
Die Datenschutz-Passphrase aus dem angegebenen Benutzernamen.
defPrivPassphrase < ^ > passphrase < ^ >
Anhand dieser Informationen können Sie eine entsprechende Datei snmp.conf erstellen.
Für diesen Leitfaden sieht sie wie folgt aus:
Sie können nun Befehle ausgeben, ohne die Authentifizierungsdetails anzugeben.
Sie benötigen nur den SNMP-Befehl, den Host und die Befehlsargumente.
Anstatt einzugeben:
Können Sie eingeben:
Wie Sie sehen können, reduziert sich dadurch die Menge der Informationen, die Sie in jeder Anfrage bereitstellen müssen.
Als Nächstes entfernen Sie das Konto bootstrap, um die Netzwerksicherheit zu erhöhen.
Schritt 7 - Entfernen des Kontos Bootstrap
Nachdem Ihr reguläres Konto nun korrekt konfiguriert ist, können Sie das unsichere Konto bootstrap entfernen.
Öffnen Sie auf Ihrem Agenten-Server erneut die Datei / etc / snmp / snmpd.conf mit sudo-Berechtigungen.
Suchen und kommentieren (oder entfernen) Sie beide Zeilen aus, die Sie zuvor hinzugefügt haben und die auf den Benutzer bootstrap verweisen:
Starten Sie nun den SNMP-Daemon neu:
Damit wird die Empfehlung erfüllt, keine Anweisung createUser in der normalen Datei snmpd.conf zu haben.
Hierdurch werden auch Berechtigungen von dem temporären Benutzer entfernt.
Wenn Sie den Benutzer bootstrap vollständig aus der usmUserTable entfernen möchten, können Sie dies tun, indem Sie diesen Befehl vom Manager-Server aus erteilen:
Sie erhalten die folgende Antwort:
Zu diesem Zeitpunkt haben Sie eine vollständig konfigurierte Client-Server-Einrichtung, die über das SNMP-Protokoll sicher kommunizieren kann.
Sie können nun zusätzliche Daemons auf anderen Hosts hinzufügen und den Kontozugriff über Ihre gesamte Infrastruktur konfigurieren.
Für weitere Studien können Sie unser Tutorial Verwenden der Net-SNMP Tool-Suite zur Verwaltung und Überwachung von Servern verwenden, um mehr über SNMP-Tools und ihre Verwendung zum Abrufen von Werten nacheinander oder in großen Mengen sowie zum Ändern von Daten zu erfahren.
Aufnehmen und Teilen von Terminalsitzungen mit Terminalizer unter Ubuntu 18.04
3939
Terminalizer ist eine Terminal-Rekorderanwendung, mit der Sie Ihre Terminalsitzung in Echtzeit aufnehmen und dann zu einem späteren Zeitpunkt wiedergeben können.
Sie funktioniert genauso wie ein Desktop-Bildschirmrekorder, läuft aber stattdessen in Ihrem Terminal.
Die Aufzeichnung Ihrer Terminalsitzung ist nützlich, wenn Sie eine bestimmte Aktivität erneut überprüfen oder einen besonders kniffligen Fehler beheben möchten.
Die mit Terminalizer erstellten Aufnahmen können auch als animierte GIFs exportiert werden, was Ihnen eine gute Möglichkeit bietet, diese online zu teilen oder dem Marketingmaterial Ihrer Software hinzuzufügen.
In diesem Tutorial installieren und verwenden Sie Terminalizer zur Aufnahme und Wiedergabe von Terminalsitzungen, zur benutzerdefinierten Anpassung Ihrer Aufnahmen und zum anschließenden Exportieren, um die Aufnahmen online zu teilen.
Einen Ubuntu 18.04-Server, der gemäß der Ersteinrichtung eines Servers unter Ubuntu 18.04 eingerichtet wurde, einschließlich eines sudo non-root users.
Node.js und npm, die Sie installieren, indem Sie dem Abschnitt Installation der Distro-Stable Version für Ubuntu in Installation von Node.js unter Ubuntu 18.04 folgen.
Wenn Sie Ihre Aufnahmen online teilen möchten, benötigen Sie außerdem:
Einen kostenlosen Account auf der Terminalizer-Website.
Schritt 1 - Installieren von Terminalizer
In diesem Schritt laden Sie Terminalizer herunter und installieren es auf Ihrem System.
Terminalizer ist mit Node.js geschrieben und kann mit dem npm-Paketmanger installiert werden.
Um Terminalizer global auf Ihrem System zu installieren, führen Sie den folgenden Befehl aus:
Terminalizer verwendet das Electron-Anwendungsframework, um aufgenommene Terminalsitzungen in GIF-Format zu exportieren.
Für die globale Installation von Electron auf Ihrem System ist das Befehlsargument --unsafe-perms = true erforderlich.
Sobald Terminalizer installiert ist, sehen Sie eine Ausgabe ähnlich wie die folgende:
Als Nächstes überprüfen Sie die Installation von Terminalizer, indem Sie Folgendes ausführen:
Sie erhalten eine Ausgabe, die der Folgenden ähnelt:
Erstellen Sie zum Schluss eine standardmäßige Terminalizer-Konfigurationsdatei, die Sie zur erweiterten Anpassung des Terminalizers nutzen können (Einzelheiten hierzu in Schritt 4):
Nachdem Sie Terminalizer nun installiert haben, können Sie Ihre erste Terminalaufnahme erstellen.
Schritt 2 - Aufnahme und Wiedergabe einer Terminalsitzung
In diesem Schritt nehmen Sie eine Terminalsitzung auf und geben diese wieder.
Zuerst erstellen Sie eine neue Terminalizer-Aufnahme mit einem Namen Ihrer Wahl:
Die darauf folgende Ausgabe zeigt an, dass die Aufnahme gestartet wurde:
Sie können nun in Ihrem Terminal alles ausführen, was Sie möchten.
Jede Tasteneingabe und jeder Befehl wird von Terminalizer in Echtzeit aufgezeichnet.
Wenn Sie die Aufnahme stoppen möchten, drücken Sie STRG + D.
Nun speichert Terminalizer die Aufnahme in die angegebene Datei im Format YAML, z. B. < ^ > your-recording < ^ > .yml.
Sie können von Terminalizer aufgefordert werden, Ihre Aufnahme online zu teilen.
Drücken Sie einfach STRG + C, um dies vorerst abzubrechen, da Sie die Terminalaufnahme zunächst lokal wiedergeben können.
Als Nächstes geben Sie die aufgenommene Terminalsitzung mit dem folgenden Befehl wieder:
Dadurch wird die aufgenommene Sitzung in Echtzeit in Ihrem Terminal wiedergegeben:
Sie können die Wiedergabegeschwindigkeit Ihrer Aufnahme auch mit der Option --speed-factor anpassen.
Folgendes z. B. gibt Ihre Aufnahme halb so schnell wieder (halbe Geschwindigkeit):
Alternativ können Sie Ihre Aufnahme doppelt so schnell (doppelte Geschwindigkeit) wiedergeben:
Sie haben eine Terminalsitzung aufgenommen und wiedergegeben.
Als Nächstes können Sie eine aufgenommene Terminalsitzung online teilen.
Schritt 3 - Teilen einer aufgenommenen Terminalsitzung
In diesem Schritt teilen Sie Ihre aufgenommene Terminalsitzung online auf der Webseite Terminalizer Explore.
Wählen Sie zuerst die aufgenommene Sitzung aus, die Sie teilen möchten:
Sie werden dann aufgefordert, einige grundlegende Metadaten über Ihre Aufnahme bereitzustellen, wie z. B. den Titel und die Beschreibung:
< $> warning Warnung: Terminalizer-Aufnahmen werden standardmäßig öffentlich gemeinsam genutzt. Achten Sie also darauf, dass es in Ihrer Terminalaufnahme keine persönlichen Daten gibt, die Sie nicht teilen möchten.
Wenn Sie zum ersten Mal eine aufgenommene Sitzung mit Terminalizer teilen, müssen Sie Ihren Terminalizer-Account verlinken.
Terminalizer wird, falls erforderlich, einen Verifizierungslink anzeigen:
< $> warning Warnung: Stellen Sie sicher, dass Sie Ihr Terminalizer-Token privat halten, da jeder, der es besitzt, auf Ihren Terminalizer-Account zugreifen kann.
Sobald Sie den Link in Ihrem Webbrowser geöffnet und sich in Ihren Terminalizer-Account eingeloggt haben, drücken Sie eine beliebige Taste, um fortzufahren.
Terminalizer lädt Ihre Aufnahme nun hoch und stellt Ihnen den Link zum Ansehen zur Verfügung:
Durch das Öffnen des Links in einem Desktop-Webbrowser können Sie Ihre geteilte Aufnahme ansehen:
Screenshot der Terminalizer-Website mit dem Beispiel einer geteilten Terminalaufnahme
Sie haben eine aufgenommene Terminalsitzung auf der Terminalizer-Website geteilt und in Ihrem Webbrowser angesehen.
Schritt 4 - Einstellen der erweiterten Terminalizer-Konfiguration
Nachdem Sie sich nun mit Terminalizer vertraut gemacht haben, können Sie beginnen, einige der erweiterten Anpassungsoptionen zu testen. Sie haben z. B. die Möglichkeit, die Farben und den Stil des Displays anzupassen.
Jede Aufnahme erhält die Standardkonfiguration aus der globalen Terminalizer-Konfigurationsdatei, die sich unter ~ / .terminalizer / config.yml befindet.
Das bedeutet, dass Sie die Konfiguration für einzelne Aufnahmen direkt bearbeiten können, indem Sie die Aufnahmedatei bearbeiten (z. B. < ^ > your-recording < ^ > .yml).
Alternativ können Sie die globale Konfiguration bearbeiten, was sich auf alle neuen Aufnahmen auswirkt.
In diesem Beispiel bearbeiten Sie die globale Konfigurationsdatei, aber die gleiche Anleitung gilt auch für Konfigurationsdateien individueller Aufnahmen.
Öffnen Sie zuerst die globale Terminalizer-Konfiguration in Ihrem Texteditor, z. B. nano:
Jede der verfügbaren Konfigurationsoptionen innerhalb der Datei ist mit einem erklärenden Kommentar versehen.
Es gibt verschiedene gängige Konfigurationsoptionen, die Sie Ihren Wünschen anpassen können:
cols: Legen Sie die genaue Anzahl der für Ihre Aufnahme verwendeten Terminalspalten fest.
rows: Legen Sie die genaue Anzahl der für Ihre Aufnahme verwendeten Terminalreihen fest.
frameDelay: Heben Sie die Verzögerung zwischen jeder Tastatureingabe während der Wiedergabe auf.
maxIdleTime: Geben Sie eine maximale Zeit zwischen den Tastatureingaben während der Wiedergabe an.
cursorStyle: Legen Sie den standardmäßigen Cursor-Stil des Terminals auf block, bar oder underline fest.
fontFamily: Legen Sie eine Liste der bevorzugten Wiedergabeschriftarten in der bevorzugten Reihenfolge fest.
theme: Stellen Sie das Farbschema der Wiedergabe ein, z. B. für ein Schwarz-auf-Weiß-Terminal, etc.
Beispielsweise können Sie ein Weiß-auf-Schwarz-Terminaldisplay erhalten, indem Sie die folgenden Optionen konfigurieren:
Sie erhalten ein Ergebnis, das dem Folgenden ähnelt:
Screenshot der Terminalizer-Website mit dem Beispiel einer Aufnahme in einem Schwarz-auf-Weiß-Thema
Sie können den Cursor-Stil anpassen, um die Aufnahme leichter verständlich zu machen, z. B. indem Sie den standardmäßigen Cursor im Block-Stil mit einem unterstrichenen tauschen:
Screenshot der Terminalizer-Website mit dem Beispiel einer Aufnahme mit einem Cursor im Unterstrich-Stil
Sobald Sie alle gewünschten Änderungen vorgenommen haben, speichern Sie die Datei und gehen zu Ihrem Terminal zurück.
Wenn Sie die globale Terminalizer-Konfiguration bearbeitet haben, gelten diese Einstellungen für alle neuen Aufnahmen, die Sie zukünftig erstellen.
Wenn Sie eine individuelle Aufnahmekonfiguration bearbeiten, wendet Terminalizer die Änderungen sofort auf die jeweilige Aufnahme an.
Beachten Sie, dass der benutzerdefinierte Wiedergabestil nur für geteilte Aufnahmesitzungen gilt.
Die direkte Wiedergabe in Ihrem Terminal verwendet immer Ihre standarmäßigen Terminalstile und Farbschemen.
In diesem letzten Schritt haben Sie einige der erweiterten Konfigurationsoptionen von Terminalizer getestet.
In diesem Artikel haben Sie Terminalizer zum Aufnehmen und Teilen einer Terminalsitzung verwendet.
Sie verfügen nun über das erforderliche Wissen, um aufgenommene Demos Ihrer Software zu erstellen, die Sie für Marketingmaterial verwenden können, oder um Befehlszeilenkniffe mit Ihren Freunden zu teilen.
Wenn Sie Terminalizer-Aufnahmen in GIF-Format rendern und exportieren möchten, können Sie Terminalizer auf einem Rechner mit einer grafischen Benutzeroberfläche / Desktop installieren und die integrierten Rendering-Funktionen verwenden:
Erstellen von GIFs mit Terminalizer
Sie können auch die Terminalizer-Website besuchen, um von anderen Benutzern aufgenommene, geteilte Terminalsitzungen zu sehen:
Explore Terminalizer Recordings ​ ​
So testen Sie ein Node.js-Modul mit Mocha und Assert
3930
Testen ist ein integraler Bestandteil der Softwareentwicklung.
Es ist üblich, dass Programmierer Code ausführen, der ihre Anwendung testet, während sie darin Änderungen vornehmen. So können sie bestätigen, dass sich die Anwendung so verhält, wie sie es gerne hätten.
Mit dem richtigen Test-Setup kann dieser Prozess sogar automatisiert sein und somit eine Menge Zeit sparen.
Das Ausführen von Tests nach dem Schreiben von neuem Code stellt sicher, dass neue Änderungen keine bereits vorhandenen Funktionen brechen.
Das gibt dem Entwickler Vertrauen in seine Code-Basis, insbesondere dann, wenn der Code produktiv eingesetzt wird, damit die Benutzer mit ihm interagieren können.
Ein Test-Framework strukturiert die Art, wie wir Testfälle erstellen.
Mocha ist ein beliebtes JavaScript-Framework, das unsere Testfälle organisiert und für uns ausführt.
Mocha verifiziert jedoch nicht das Verhalten unseres Codes.
Um Werte in einem Test zu vergleichen, können wir das Node.js-assert-Modul verwenden.
In diesem Artikel schreiben Sie Tests für ein Node.js-TODO-Listenmodul.
Sie richten das Testframework von Mocha ein und nutzen es, um Ihre Tests zu strukturieren.
Dann verwenden Sie das Node.js-assert-Modul, um die Tests selbst zu erstellen.
In diesem Sinne verwenden Sie Mocha als Planersteller und assert zur Umsetzung des Plans.
Dieses Tutorial verwendet die Node.js-Version 10.16.0.
Grundkenntnisse über JavaScript, die Sie in unserer Serie Codieren in JavaScript finden können.
Schritt 1 - Schreiben eines Node-Moduls
Beginnen wir diesen Artikel mit dem Schreiben des Node.js-Moduls, das wir testen möchten.
Dieses Modul verwaltet eine Liste von TODO-Elementen.
Mithilfe dieses Moduls können wir alle TODOs auflisten, die wir verfolgen, sowie neue Elemente hinzufügen und einige als abgeschlossen markieren.
Zusätzlich können wir eine Liste von TODO-Elementen in eine CSV-Datei exportieren.
Wenn Sie eine Auffrischung über das Schreiben von Node.js-Modulen wünschen, können Sie unseren Artikel Erstellen eines Node.js-Moduls lesen.
Zuerst müssen wir die Codierungsumgebung einrichten.
Erstellen Sie einen Ordner mit dem Namen Ihres Projekts in Ihrem Terminal.
Dieses Tutorial verwendet den Namen < ^ > todos < ^ >:
Initialisieren Sie nun npm, da wir später zum Ausführen der Tests seine CLI-Funktionalität verwenden:
Wir haben nur eine Abhängigkeit, Mocha, das wir zur Organisation und Durchführung unserer Tests verwenden.
Zum Herunterladen und Installieren von Mocha verwenden Sie Folgendes:
Wir installieren Mocha als dev-Abhängigkeit, da es für das Modul in einer Produktionsumgebung nicht erforderlich ist.
Wenn Sie mehr über Node.js-Packages oder npm erfahren möchten, lesen Sie den Leitfaden Verwenden von Node.js-Modulen mit npm und package.json.
Abschließend erstellen wir unsere Datei, die den Code unseres Moduls enthalten wird:
Damit können wir nun unser Modul erstellen.
Öffnen Sie index.js in einem Texteditor wie nano:
Zuerst definieren wir die Todos-Klasse.
Diese Klasse enthält alle Funktionen, die wir zur Verwaltung unserer TODO-Liste benötigen.
Fügen Sie index.js die folgenden Zeilen von Code hinzu:
Wir beginnen mit der Datei, indem wir eine Todos-Klasse erstellen.
Seine Funktion constructor () nimmt keine Argumente an, daher müssen wir keine Werte bereitstellen, um ein Objekt für diese Klasse zu instanziieren.
Wenn wir ein Todos-Objekt initialisieren, erstellen wir lediglich eine todos-Funktion, bei der es sich um ein leeres Array handelt.
Die modules-Zeile ermöglicht es anderen Node.js-Modulen, unsere Todos-Klasse zu verlangen.
Wenn wir sie nicht ausdrücklich exportieren, könnte die Testdatei, die wir später erstellen, sie nicht verwenden.
Wir fügen nun eine Funktion hinzu, um das Array von todos, das wir gespeichert haben, auszugeben.
Fügen Sie die folgenden hervorgehobenen Zeilen ein:
Unsere list () -Funktion gibt eine Kopie des Arrays aus, die von der Klasse verwendet wird.
Sie erstellt die Kopie des Arrays mit der destrukturierten Syntax von JavaScript.
Wir erstellen eine Kopie des Arrays, damit die Änderungen, die der Benutzer an dem von list () ausgegebenen Array vornimmt, nicht das vom Todos-Objekt verwendete Array beeinträchtigen.
< $> note Anmerkung: JavaScript-Arrays sind Referenztypen.
Das bedeutet, dass sich JavaScript bei jeder Variablenzuweisung an ein Array oder bei Funktionsaufrufen mit einem Array als Parameter auf das ursprüngliche Array bezieht, das erstellt wurde.
Wenn wir zum Beispiel ein Array mit drei Elementen namens x haben und eine neue Variable y erstellen, sodass y = x, y und x sich beide auf dieselbe Sache beziehen.
Alle Änderungen, die wir im Array an y vornehmen, wirken sich auf die Variable x aus und umgekehrt.
Schreiben wir nun die add () -Funktion, die ein neues TODO-Element hinzufügt:
Unsere add () -Funktion nimmt eine Zeichenfolge und platziert sie bei einem neuen JavaScript-Object in die title-Funktion.
Das neue Objekt hat auch eine completed-Funktion, die standardmäßig auf false gesetzt ist.
Dann fügen wir unser neues Objekt unserem Array von TODOs hinzu.
Eine wichtige Funktionalität in einem TODO-Manager ist die Markierung von Elementen als abgeschlossen.
Um dies umzusetzen, durchlaufen wir unser todos-Array, um das TODO-Element zu finden, nach dem der Benutzer sucht.
Wenn eines gefunden wird, markieren wir es als abgeschlossen.
Wenn keines gefunden wird, geben wir einen Fehler aus.
Fügen Sie die complete () -Funktion wie hier gezeigt hinzu:
Wir verfügen nun über einen grundlegenden TODO-Manager, mit dem wir experimentieren können.
Als Nächstes testen wir den Code manuell, um zu sehen, ob die Anwendung funktioniert.
Schritt 2 - Manuelles Testen des Codes
In diesem Schritt führen wir die Funktionen unseres Codes aus und beobachten die Ausgabe, um sicherzustellen, dass sie unseren Erwartungen entspricht.
Das nennt sich manuelles Testen.
Es ist wahrscheinlich die gebräuchlichste Testmethodik, die Programmierer anwenden.
Obwohl wir unsere Tests später mit Mocha automatisieren, testen wir unseren Code zunächst manuell, um ein besseres Gefühl dafür zu bekommen, wie sich manuelles Testen von Test-Frameworks unterscheidet.
Wir fügen unserer Anwendung zwei TODO-Elemente hinzu und markieren eines als abgeschlossen.
Sie sehen die Eingabeaufforderung > in der REPL, die uns anzeigt, dass wir JavaScript-Code eingeben können.
Geben Sie Folgendes an der Eingabeaufforderung ein:
Mit require () laden wir das TODOs-Modul in eine Todos-Variable.
Erinnern Sie sich, dass unser Modul die Todos-Klasse standardmäßig ausgibt.
Lassen Sie uns nun ein Objekt für diese Klasse instanziieren.
Fügen Sie diese Zeile von Code in der REPL hinzu:
Wir können das todos-Objekt verwenden, um unsere Umsetzungsarbeiten zu verifizieren.
Fügen wir unser erstes TODO-Element hinzu:
Bisher haben wir noch keine Ausgabe in unserem Terminal gesehen.
Verifizieren wir, dass wir unser "run code" -TODO-Element gespeichert haben, indem wir eine Liste aller unserer TODOs abrufen:
Sie sehen diese Ausgabe in Ihrer REPL:
Das ist das erwartete Ergebnis: Wir haben ein TODO-Element in unserem Array von TODOs und es ist nicht standardmäßig abgeschlossen.
Wir fügen ein weiteres TODO-Element hinzu:
Markieren Sie das erste TODO-Element als abgeschlossen:
Unser todos-Objekt verwaltet nun zwei Elemente: "run code" und "test everything".
Das "run code" -TODO wird ebenfalls abgeschlossen sein.
Bestätigen wir dies, indem wir list () erneut aufrufen:
Beenden Sie nun die REPL wie folgt:
Wir haben bestätigt, dass sich unser Modul erwartungsgemäß verhält.
Wir haben unseren Code nicht in eine Testdatei gestellt oder eine Testbibliothek verwendet, sondern manuell getestet.
Leider ist diese Testform zeitaufwendig, wenn wir sie bei jeder Änderung, die wir vornehmen, durchführen.
Als Nächstes verwenden wir automatisiertes Testen in Node.js und sehen, ob wir dieses Problem mit dem Mocha lösen können.
Schritt 3 - Schreiben des ersten Tests mit Mocha und Assert
Im letzten Schritt haben wir unsere Anwendung manuell getestet.
Das funktioniert für individuelle Anwendungsfälle, aber wenn unser Modul skaliert, wird diese Methode weniger praktikabel.
Wenn wir neue Eigenschaften testen, müssen wir sicher sein, dass die hinzugefügte Funktionalität keine Probleme in der alten Funktionalität verursacht.
Wir möchten jede Eigenschaft bei jeder Änderung des Codes erneut testen, aber dies von Hand zu tun wäre sehr aufwendig und fehleranfällig.
Eine effizientere Praxis wäre die Einrichtung automatisierter Tests.
Das sind skriptbasierte Tests, die wie jeder andere Codeblock geschrieben sind.
Wir führen unsere Funktionen mit definierten Eingaben aus und inspizieren ihre Effekte, um sicherzustellen, dass sie sich wie erwartet verhalten.
Mit dem Anwachsen unserer Codebasis wächst auch der Umfang der automatisierten Tests.
Wenn wir neue Tests zusammen mit den Eigenschaften schreiben, können wir überprüfen, ob das gesamte Modul noch funktioniert - ohne sich jedesmal daran erinnern zu müssen, wie jede einzelne Funktion genutzt wird.
In diesem Tutorial verwenden wir das Testframework von Mocha mit dem Node.js-assert-Modul.
Wir sammeln ein paar praktische Erfahrungen, um zu sehen, wie sie zusammenarbeiten.
Erstellen Sie zunächst eine neue Datei, um unseren Testcode zu speichern:
Verwenden Sie nun Ihren bevorzugten Texteditor, um die Testdatei zu öffnen.
Sie können wie zuvor nano verwenden:
In der ersten Zeile der Textdatei laden wir das TODOs-Modul, so wie wir es mit der Node.js-Shell getan haben.
Dann laden wir das assert-Modul für das Schreiben unserer Tests.
Die strict-Funktion des assert-Moduls erlaubt uns, spezielle Gleichheitstests anzuwenden, die von Node.js empfohlen werden und gut für das zukünftige Prüfen geeignet sind, da sie mehr Anwendungsfälle berücksichtigen.
Bevor wir mit dem Schreiben von Tests beginnen, behandeln wir, wie Mocha unseren Code organisiert.
Die in Mocha strukturierten Tests folgen normalerweise dieser Vorlage:
Beachten Sie zwei Schlüsselfunktionen: describe () und it ().
Die describe () -Funktion wird zur Gruppierung ähnlicher Tests genutzt.
Es ist nicht erforderlich, für Mocha Tests auszuführen, aber die Gruppierung von Tests erleichtert die Pflege unseres Testcodes.
Es wird empfohlen, Ihre Tests so zu gruppieren, dass Sie ähnliche Tests leicht zusammen aktualisieren können.
Die Funktion it () enthält unseren Testcode.
Hier würden wir mit den Funktionen unseres Moduls interagieren und die assert-Bibliothek verwenden.
Viele it () -Funktionen können als describe () -Funktion definiert werden.
Unser Ziel in diesem Abschnitt ist die Verwendung von Mocha und assert zur Automatisierung unserer manuellen Tests.
Wir führen dies Schritt für Schritt aus und beginnen mit unserem Beschreibungsblock.
Fügen Sie Folgendes nach den Modulzeilen in Ihre Datei ein:
Mit diesem Codeblock haben wir eine Gruppierung für unsere integrierten Tests erstellt.
Komponententests würden jeweils nur eine Funktion testen.
Integrationstests verifizieren, wie gut Funktionen innerhalb oder über Module zusammenarbeiten.
Wenn Mocha unseren Test ausführt, laufen alle Tests innerhalb des Beschreibungsblocks unter der "integration test" -Gruppe.
Wir fügen nun eine it () -Funktion hinzu, damit wir mit dem Testen des Codes unseres Moduls beginnen können:
Beachten Sie, wie deskriptiv wir den Namen des Tests gemacht haben.
Wenn jemand unseren Test ausführt, wird sofort klar, was passiert oder fehlgeschlagen ist.
Eine gut getestete Anwendung ist typischerweise eine gut dokumentierte Anwendung und Tests können manchmal eine effektive Art der Dokumentation sein.
Für unseren ersten Test erstellen wir ein neues Todos-Objekt und verifizieren, dass es keine Elemente enthält:
Die erste neue Zeile des Codes instanziierte ein neues Todos-Objekt, wie wir es in der Node.js-REPL oder einem anderen Modul tun würden.
In der zweiten neuen Zeile verwenden wir das assert-Modul.
Aus dem assert-Modul verwenden wir die notStrictEqual () -Methode.
Diese Funktion nimmt zwei Parameter: den Wert, den wir testen möchten (genannt actual-Wert) und den Wert, den wir erhalten möchten (genannt expected-Wert).
Wenn beide Argumente gleich sind, gibt notStrictEqual () einen Fehler aus, damit der Test fehlschlägt.
Speichern und beenden Sie index.test.js.
Der Basisfall wird wahr sein, da die Länge 0 sein sollte, was nicht 1 ist. Bestätigen wir das, indem wir Mocha ausführen.
Dazu müssen wir unsere package.json-Datei ändern.
Öffnen Sie Ihre package.json-Datein mit Ihrem Texteditor:
Ändern Sie diese nun in der scripts-Funktion so wie hier gezeigt:
Wir haben nun das Verhalten des npm-CLI-Befehls test geändert.
Wenn wir npm test ausführen, überprüft npm den gerade eingegebenen Befehl in package.json.
Die Ausführung sucht nach der Mocha-Bibliothek in unserem node _ modules-Ordner und führt den mocha-Befehl mit unserer Testdatei aus.
Speichern und beenden Sie package.json.
Nun sehen wir uns an, was passiert, wenn wir unseren Test ausführen.
Geben Sie Folgendes in Ihr Terminal ein:
Der Befehl erzeugt die folgende Ausgabe:
Diese Ausgabe zeigt uns zunächst, welche Testgruppe sie nun ausführen wird.
Für jeden einzelnen Test innerhalb einer Gruppe ist der Testfall einbezogen.
Wir sehen unseren Testnamen wie in der Funktion it () beschrieben.
Das Häkchen auf der linken Seite des Testfalls zeigt an, dass der Test bestanden ist.
Am Ende erhalten wir eine Zusammenfassung aller Tests.
In unserem Fall ist unser einzelner Test bestanden und wurde in 16 ms abgeschlossen (die Zeit variiert von Computer zu Computer).
Unsere Testung hat erfolgreich begonnen.
Der aktuelle Testfall kann jedoch falsch-positive Meldungen liefern.
Ein falsch-positiver Testfall ist ein Testfall, der bestanden wird, wenn er fehlschlagen sollte.
Wir überprüfen gerade, dass die Länge des Arrays nicht gleich 1 ist. Wir werden den Test nun so ändern, dass dieser Zustand auch dann zutrifft, wenn er es nicht sollte.
Fügen Sie der index.test.js folgende Zeilen hinzu:
Wir haben zwei TODO-Elemente hinzugefügt.
Wir führen nun den Test aus, um zu sehen, was passiert:
Wie ewartet besteht der Test, da die Länge größer als 1 ist. Jedoch wird der ursprüngliche Zweck des ersten Tests verfehlt.
Der erste Test ist dazu gedacht, zu bestätigen, dass wir mit einem Leerzustand beginnen.
Ein besserer Test bestätigt dies in allen Fällen.
Wir ändern nun den Test, damit er nur dann bestanden wird, wenn wir absolut keine TODOs im Speicher haben.
Führen Sie die folgenden Änderungen in der index.test.js aus:
Sie haben notStrictEqual () auf strictEqual () geändert, eine Funktion, die die Gleichheit zwischen dem tatsächlichen und erwarteten Argument überprüft.
Die Strict-Equal-Funktion schlägt fehl, wenn unsere Argumente nicht genau gleich sind.
Speichern und beenden Sie und führen Sie dann den Test aus, damit wir sehen können, was passiert:
Dieses Mal zeigt die Ausgabe einen Fehler:
Dieser Text wird uns helfen, herauszufinden, warum der Test fehlgeschlagen ist.
Beachten Sie, dass zu Beginn des Testfalls kein Häkchen vorhanden ist, da der Test fehlgeschlagen ist.
Unsere Testzusammenfassung befindet sich nicht mehr am Ende der Ausgabe, sondern direkt nach der Anzeige unserer Liste von Testfällen:
Die verbleibende Ausgabe gibt uns Daten über unsere fehlgeschlagenen Tests.
Zuerst sehen wir, welcher Testfall fehlgeschlagen ist:
Dann sehen wir, warum unser Test fehlgeschlagen ist:
Es wird ein AssertionError gemeldet, wenn strictEqual () fehlschlägt.
Wir sehen, dass der expected-Wert, 0, vom actual-Wert, 2, abweicht.
Dann sehen wir die Zeile in unserer Testdatei, in der der Code fehlschlägt.
In diesem Fall ist es Zeile 10.
Nun haben wir selbst gesehen, dass unser Test fehlschlägt, wenn wir fehlerhafte Werte erwarten.
Wir ändern unseren Testfall wieder auf seinen richtigen Wert.
Dann nehmen Sie die todos.add-Zeilen heraus, sodass Ihr Code wie folgt aussieht:
Führen Sie ihn erneut aus, um zu bestätigen, dass er ohne potenzielle falsch-positive Meldungen besteht:
Wir haben nun die Belastbarkeit unseres Tests deutlich verbessert.
Fahren wir mit unserem Integrationstest fort.
Der nächste Schritt ist das Hinzufügen eines neuen TODO-Elements in index.test.js:
Nach der Verwendung der add () -Funktion bestätigen wir, dass wir nun ein TODO haben, das von unserem todos-Objekt mit strictEqual () verwaltet wird.
Unser nächster Test bestätigt die Daten in den todos mit deepStrictEqual ().
Die Funktion deepStrictEqual () prüft rekursiv, ob unsere erwarteten und tatsächlichen Objekte die gleichen Eigenschaften haben.
In diesem Fall testet sie, dass die von uns erwarteten Arrays beide ein JavaScript-Objekt beinhalten.
Dann überprüft sie, dass ihre JavaScript-Objekte die gleichen Eigenschaften haben, d. h., ihre beiden title-Eigenschaften "run code" und ihre beiden completed-Eigenschaften false sind.
Dann schließen wir die restlichen Tests unter Verwendung dieser beiden Gleichheitsprüfungen nach Bedarf durch Hinzufügen der folgenden hervorgehobenen Zeilen ab:
Unser Test imitiert nun unseren manuellen Test.
Mit diesen programmatischen Tests müssen wir die Ausgabe nicht kontinuierlich überprüfen, um zu sehen, ob unsere Tests bei der Ausführung bestehen.
Üblicherweise möchte man jeden Aspekt der Verwendung testen, um sicherzustellen, dass der Code ordnungsgemäß getestet wird.
Wir führen unseren Test erneut mit npm test aus, um diese bekannte Ausgabe zu erhalten:
Sie haben nun einen integrierten Test mit dem Mocha-Framework und der assert-Bibliothek eingerichtet.
Gehen wir nun von einer Situation aus, in der wir unser Modul mit einigen anderen Entwicklern geteilt haben und diese uns jetzt Feedback geben.
Viele unserer Benutzer würden sich wünschen, dass die Funktion complete () einen Fehler meldet, wenn bisher noch keine TODOs hinzugefügt wurden.
Wir fügen diese Funktionalität in unserer Funktion complete () ein.
Öffnen Sie index.js in Ihrem Texteditor:
Fügen Sie der Funktion Folgendes hinzu:
Nun fügen wir einen neuen Test für diese neue Eigenschaft hinzu.
Wir wollen verifizieren, ob ein Todos-Objekt, das keine Elemente enthält, unseren speziellen Fehler ausgibt, wenn wir es mit complete aufrufen.
Gehen Sie in die index.test.js zurück:
Fügen Sie am Ende der Datei den folgenden Code hinzu:
Wie zuvor verwenden wir describe () und it ().
Wir beginnen unseren Test mit der Erstellung eines neuen todos-Objekts.
Dann definieren wir den Fehler, dessen Meldung wir erwarten, wenn wir die Funktion complete () aufrufen.
Als Nächstes verwenden wir die Funktion throws () des assert-Moduls.
Diese Funktion wurde erstellt, damit wir die Fehler, die unser Code ausgibt, verifizieren können.
Sein erstes Argument ist eine Funktion, die den Code enthält, der den Fehler ausgibt.
Das zweite Argument ist der Fehler, dessen Meldung wir erwarten.
Führen Sie in Ihrem Terminal erneut die Tests mit npm test aus und Sie sehen die folgende Ausgabe:
Diese Ausgabe zeigt den Nutzen, warum wir automatisiertes Testen mit Mocha und assert durchführen.
Da unsere Tests schriftlich ausgearbeitet sind, verifizieren wir bei jeder Ausführung von npm test, dass alle unsere Tests bestehen.
Wir mussten nicht manuell überprüfen, ob der andere Code noch funktioniert - wir wissen, dass es so ist, da der Test, den wir haben, bestand.
Bisher haben unsere Tests die Ergebnisse von synchronem Code verifiziert.
Wir behandeln nun, wie wir unsere neu gewonnenen Testgewohnheiten anpassen müssten, um mit asynchronem Code arbeiten zu können.
Schritt 4 - Testen von asynchronem Code
Eine der Eigenschaften, die wir in unserem TODO-Modul benötigen, ist eine CSV-Exportfunktion.
Damit werden alle gespeicherten TODOs zusammen mit dem abgeschlossenen Status in einer Datei ausgegeben.
Das erfordert die Verwendung des Moduls fs - eines integrierten Node.js-Moduls für die Arbeit mit dem Dateisystem.
Das Schreiben in eine Datei ist eine asynchrone Operation.
Es gibt viele Möglichkeiten, in eine Datei in Node.js zu schreiben.
Wir können Callbacks, Promises oder die Schlüsselworte async / await verwenden.
In diesem Abschnitt behandeln wir, wie wir Tests für diese verschiedenen Methoden schreiben.
Callbacks
Eine callback-Funktion ist eine Funktion, die als Argument in einer asynchronen Funktion verwendet wird.
Sie wird aufgerufen, wenn die asynchrone Operation abgeschlossen ist.
Wir fügen unserer Todos-Klasse eine Funktion namens saveToFile () hinzu.
Diese Funktion erstellt eine Zeichenfolge, indem Sie alle unsere TODO-Elemente durchläuft, und schreibt diese Zeichenfolge in eine Datei.
Öffnen Sie Ihre index.js-Datei:
Fügen Sie den folgenden hervorgehobenen Code in die Datei ein:
Zunächst müssen wir das fs-Modul in unsere Datei importieren.
Dann haben wir unsere neue Funktion saveToFile () hinzugefügt.
Unsere Funktion übernimmt eine Callback-Funktion, die genutzt wird, sobald die Schreiboperation der Datei abgeschlossen ist.
In dieser Funktion erstellen wir eine fileContents-Variable, die die gesamte zu speichernde Zeichenfolge als Datei speichert.
Sie wird mit den CSV-Titeln initialisiert.
Dann durchlaufen wir jedes TODO-Element mit der forEach () -Methode des internen Arrays.
Beim Durchlaufen fügen wir die title- und completed-Eigenschaften der einzelnen todos hinzu.
Zum Schluss verwenden wir das fs-Modul zum Schreiben der Datei mit der writeFile () -Funktion.
Unser erstes Argument ist der Dateiname: todos.csv.
Das zweite ist der Inhalt der Datei, in diesem Fall unsere fileContents-Variable.
Das letzte Argument ist unsere Callback-Funktion, die alle Schreibfehler der Datei behandelt.
Wir schreiben nun einen Test für unsere Funktion saveToFile.
Unser Test führt zwei Dinge aus: Er überprüft die Existenz der Datei und verifiziert, dass sie den richtigen Inhalt hat.
Öffnen Sie die Datei index.test.js:
Beginnen wir damit, das fs-Modul am Anfang der Datei zu laden, da wir es zum Testen unserer Ergebnisse verwenden werden:
Am Ende der Datei fügen wir unseren neuen Testfall hinzu:
Wie zuvor verwenden wir describe (), um unseren Test getrennt von den anderen zu gruppieren, da er eine neue Funktionalität enthält.
Die it () -Funktion unterscheidet sich leicht von unseren anderen.
Normalerweise hat die von uns verwendete Callback-Funktion keine Argumente.
Dieses Mal haben wir done als Argument.
Wir benötigen dieses Argument, wenn wir Funktionen mit Callbacks testen.
Die Callback-Funktion done () wird von Mocha verwendet, um ihr anzugeben, wenn eine asynchrone Funktion abgeschlossen ist.
Alle in Mocha getesteten Callback-Funktionen müssen den Callback done () aufrufen.
Wäre dies nicht der Fall, würde Mocha nie wissen, ob die Funktion abgeschlossen ist und würde festgefahren auf ein Signal warten.
Wir erstellen nun unsere Todos-Instanz und fügen ihr ein einzelnes Element hinzu. Wir rufen die Funktion saveToFile () mit einem Callback auf, der einen Dateischreibfehler findet.
Beachten Sie, wie unser Test für diese Funktion im Callback enthalten ist.
Wenn unser Testcode außerhalb des Callbacks wäre, würde er fehlschlagen, solange der Code aufgerufen würde, bevor das Schreiben der Datei abgeschlossen wäre.
In unserer Callback-Funktion überprüfen wir zunächst, dass unsere Datei existiert:
Die Funktion fs.existsSync () gibt true aus, wenn der Dateipfad in ihrem Argument existiert, und andernfalls false.
< $> note Anmerkung: Die Funktionen des fs-Moduls sind standardmäßig asynchron.
Sie bildeten jedoch für Schlüsselfunktionen synchrone Gegenstücke.
Dieser Test ist einfacher, wenn synchrone Funktionen verwendet werden, da wir den asynchronen Code nicht schachteln müssen, um sicherzustellen, dass er funktioniert.
Im fs-Modul enden synchrone Funktionen normalerweise mit "Sync" am Ende ihrer Namen.
Dann erstellen wir eine Variable, um unseren erwarteten Wert zu speichern:
Wir verwenden readFileSync () des fs-Moduls zum synchronen Lesen der Datei:
Wir geben readFileSync () den richtigen Pfad für die Datei: todos.csv.
Da readFileSync () ein Buffer-Objekt ausgibt, das Binärdaten speichert, verwenden wir seine toString () -Methode, damit wir seinen Wert mit der Zeichenfolge vergleichen können, die wir voraussichtlich gespeichert haben.
Wie zuvor verwenden wir das strictEqual des assert-Moduls, um einen Vergleich durchzuführen:
Wir beenden unseren Test durch das Aufrufen des done () -Callbacks, sodass Mocha weiß, dass der Test dieses Falls gestoppt wird:
Wir geben das err-Objekt zu done (), sodass der Test mit Mocha fehlschlägt, falls ein Fehler vorhanden ist.
Wie zuvor führen wir diesen Test mit npm test durch.
Ihre Konsole zeigt dann diese Ausgabe:
Sie haben nun Ihre erste asynchrone Funktion mit Mocha unter der Verwendung von Callbacks getestet.
Zum Zeitpunkt des Schreibens dieses Tutorials sind Promises jedoch verbreiteter als Callbacks in neuem Node.js-Code, wie auch in unserem Artikel Schreiben von asynchronem Code in Node.js beschrieben.
Als Nächstes lernen wir, wie wir auch diese mit Mocha testen können.
Promises
Ein Promise ist ein JavaScript-Objekt, das letztendlich einen Wert ausgibt.
Wenn ein Promise erfolgreich ist, ist es gelöst.
Wenn es auf einen Fehler trifft, wird es verworfen.
Wir ändern die saveToFile () -Funktion, damit sie Promises anstelle von Callbacks verwendet.
Öffnen Sie index.js:
Zuerst müssen wir ändern, wie das fs-Modul geladen ist.
Ändern Sie in Ihrer index.js-Datei die require () -Aussage am Anfang der Datei, sodass sie aussieht wie folgt:
Wir haben nun das fs-Modul importiert, das Promises anstelle von Callbacks verwendet.
Nun müssen wir einige Änderungen an saveToFile () vornehmen, damit es stattdessen mit Promises arbeitet.
Führen Sie in Ihrem Texteditor die folgenden Änderungen an der Funktion saveToFile () aus, um die Callbacks zu entfernen:
Der erste Unterschied besteht darin, dass unsere Funktion keine Argumente mehr akzeptiert.
Mit Promises benötigen wir keine Callback-Funktion.
Die zweite Änderung betrifft die Weise, wie die Datei geschrieben ist.
Wir geben nun das Ergebnis des writeFile () -Promises aus.
Speichern und schließen Sie index.js.
Wir passen unseren Test so an, dass er mit Promises funktioniert.
Öffnen Sie index.test.js:
Ändern Sie den saveToFile () -Test auf Folgendes:
Als erste Änderung müssen wir den done () -Callback aus den Argumenten entfernen.
Wenn Mocha das done () -Argument durchläuft, muss es aufgerufen werden oder es gibt einen Fehler wie folgt aus:
Schließen Sie beim Testen von Promises den done () -Callback nicht in it () ein.
Um unser Promise zu testen, müssen wir unseren Assertionscode in die then () -Funktion einfügen.
Beachten Sie, dass wir dieses Promise im Test ausgeben. Wir haben keine catch () -Funktion, um das Promise auszufangen, wenn es verworfen wird.
Wir geben das Promise aus, sodass alle Fehler, die in der then () -Funktion ausgegeben werden, in der it () -Funktion heraustreten.
Wenn die Fehler nicht heraustreten, wird der Testfall mit Mocha nicht fehlschlagen.
Beim Testen von Promises müssen Sie return auf das getestete Promise verwenden.
Andernfalls besteht das Risiko, ein falsch-positives Ergebnis zu erhalten.
Wir lassen auch die catch () -Klausel aus, da Mocha erkennen kann, wenn ein Promise verworfen wird.
Im Fall einer Verwerfung schlägt der Test automatisch fehl.
Da unser Test nun fertig ist, speichern und beenden Sie die Datei und führen Sie anschließend Mocha mit npm test aus. Als Bestätigung erhalten wir ein erfolgreiches Ergebnis:
Wir haben unseren Code und Test zur Verwendung von Promises geändert und wissen nun sicher, dass es funktioniert.
Die neuesten asynchronen Muster verwenden jedoch async / await-Schlüsselwörter, damit wir nicht mehrere then () -Funktionen erstellen müssen, um erfolgreiche Ergebnisse zu bearbeiten.
Sehen wir als Nächstes, wie wir mit async / await testen können.
async / await
Die Schlüsselwörter async / await erleichtern die Arbeit mit Promises, da sie nicht so ausführlich sind.
Sobald wir eine Funktion als asynchron mit dem Schlüsselwort async definieren, können wir alle zukünftigen Ergebnisse in dieser Funktion mit dem Schlüsselwort await erhalten.
Auf diese Weise können wir Promises verwenden, ohne die Funktionen then () oder catch () verwenden zu müssen.
Wir können unseren auf Promises basierenden saveToFile () -Test mit async / await vereinfachen.
Führen Sie in Ihrem Texteditor diese kleineren Änderungen im saveToFile () -Test in der index.test.js aus:
Die erste Änderung besteht darin, dass die von der it () -Funktion verwendete Funktion jetzt das Schlüsselwort async zur Definierung verwendet.
Dadurch können wir das Schlüsselwort await in ihrem Körper verwenden.
Die zweite Änderung tritt auf, wenn wir saveToFile () aufrufen.
Bevor es aufgerufen wird, wird das Schlüsselwort await verwendet.
Node.js wird nun warten, bis diese Funktion gelöst ist, bevor es den Test fortsetzt.
Da wir den Code aus der then () -Funktion in den it () -Funktionskörper verschoben haben, ist unser Funktionscode leichter zu lesen.
Die Ausführung dieses Codes mit npm test erzeugt diese Ausgabe:
Wir können jetzt asynchrone Funktionen testen, indem wir ein beliebiges von drei asynchronen Paradigmen entsprechend verwenden.
Wir haben mit dem Testen von synchronem und asynchronem Code mit Mocha schon einen breiten Bereich abgedeckt.
Als Nächstes tauchen wir tiefer ein in einige andere Funktionalitäten, die Mocha bietet, um unsere Testerfahrung zu verbessern. Besonders interessant ist hierbei auch, wie Hooks die Testumgebungen verändern können.
Schritt 5 - Verwenden von Hooks zur Verbesserung von Testfällen
Hooks sind ein nützlicher Bestandteil von Mocha, der es uns ermöglicht, die Umgebung vor und nach einem Test zu konfigurieren.
Wir fügen Hooks typischerweise in einen describe () -Funktionsblock, da diese eine für einige Testfälle spezifische Auf- und Abbaulogik enthalten.
Mocha bietet vier Hooks, die wir in unseren Tests verwenden können:
before: Dieser Hook wird einmal ausgeführt, bevor der erste Test beginnt.
beforeEach: Dieser Hook wird vor jedem Testfall ausgeführt.
after: Dieser Hook wird einmal ausgeführt, nachdem der letzte Testfall abgeschlossen ist.
afterEach: Dieser Hook wird nach jedem Testfall ausgeführt.
Hooks sind sehr nützlich, wenn wir eine Funktion oder Eigenschaft mehrmals testen, da sie uns erlauben, den Einrichtungscode des Tests (wie das Erstellen des todos-Objekts) vom Assertionscode des Tests zu trennen.
Um den Wert von Hooks zu sehen, fügen wir dem Testblock saveToFile () weitere Tests hinzu.
Obwohl wir bestätigt haben, dass wir unsere TODO-Elemente in eine Datei speichern können, haben wir nur ein Element gespeichert.
Außerdem wurde das Element nicht als abgeschlossen markiert.
Wir fügen weitere Tests hinzu, um sicherzustellen, dass die verschiedenen Aspekte unseres Moduls funktionieren.
Wir fügen zunächst einen zweiten Test hinzu, um zu bestätigen, dass unsere Datei korrekt gespeichert wird, wenn wir ein abgeschlossenes TODO-Element haben.
Öffnen Sie Ihre Datei index.test.js mit Ihrem Texteditor:
Ändern Sie den letzten Test folgendermaßen:
Der Test ist ähnlich wie zuvor.
Die wichtigsten Unterschiede sind, dass wir die complete () -Funktion vor saveToFile () aufrufen, und unsere expectedFileContents nun true anstatt false für den Wert der completed-Kolumne haben.
Wir führen unseren neuen Test und alle anderen mit npm test aus:
Es funktioniert wie erwartet.
Es gibt jedoch Raum für Verbesserungen.
Sie müssen ein Todos-Objekt zu Beginn des Tests instanziieren.
Beim Hinzufügen von mehr Testfällen wird dies schnell repetitiv und vergeudet Speicherplatz.
Außerdem erstellt der Test bei jeder Ausführung eine Datei.
Das kann von jemandem, der sich mit dem Modul nicht so gut auskennt, mit einer tatsächlichen Ausgabe verwechselt werden.
Es wäre schön, wenn wir unsere Ausgabedateien nach dem Testen bereinigen würden.
Führen wir nun diese Verbesserungen mit Test-Hooks aus.
Wir verwenden den Hook beforeEach (), um unsere Testvorrichtung von TODO-Elementen einzurichten.
Eine Testvorrichtung ist jeder konsistente Zustand, der in einem Test verwendet wird.
In unserem Fall ist unsere Testvorrichtung ein neues todos-Objekt, dem bereits ein TODO-Element hinzugefügt wurde.
Wir verwenden afterEach (), um die vom Test erstellte Datei zu entfernen.
Führen Sie in index.test.js die folgenden Änderungen an Ihrem letzten Test für saveToFile () aus:
Entschlüsseln wir alle vorgenommenen Änderungen.
Wir haben dem Testblock ein beforeEach () hinzugefügt:
Diese beiden Zeilen Code erstellen ein neues Todos-Objekt, das in jedem unserer Tests verfügbar ist.
Mit Mocha verweist das this-Objekt in beforeEach () auf dasselbe this-Objekt in it (). this ist für jeden Codeblock im describe () -Block gleich.
Weitere Informationen über this finden Sie in unserem Tutorial Verstehen Sie This, Bind, Call und Apply in JavaScript.
Diese leistungsstarke gemeinsame Nutzung des Kontexts ist der Grund, warum wir schnell Testvorrichtungen erstellen können, die für unsere beiden Tests funktionieren.
Dann bereinigen wir unsere CSV-Datei in der afterEach () -Funktion:
Wenn unser Test fehlgeschlagen ist, hat der Test möglicherweise keine Datei erstellt.
Aus diesem Grund überprüfen wir, ob die Datei vorhanden ist, bevor wir die Funktion unlinkSync () verwenden, um diese zu löschen.
Die verbleibenden Änderungen wechseln die Referenz von todos, die zuvor in der Funktion it () erstellt wurden, zu this.todos, das im Mocha-Kontext verfügbar ist.
Wir haben auch die Zeilen gelöscht, die zuvor todos in den einzelnen Testfällen instanziierten.
Führen wir nun diese Datei aus, um zu bestätigen, dass unsere Tests noch funktionieren.
Geben Sie npm test in Ihr Terminal ein, um Folgendes zu erhalten:
Die Ergebnisse sind gleich, und als zusätzlichen Vorteil haben wir die Einrichtungszeit für neue Tests für die Funktion saveToFile () leicht reduziert sowie eine Lösung für die zurückbleibende CSV-Datei gefunden.
In diesem Tutorial haben Sie ein Node.js-Modul geschrieben, um TODO-Elemente zu verwalten und den Code manuell mit der Node.js-REPL getestet.
Dann haben Sie eine Testdatei erstellt und das Mocha-Framework zur Ausführung automatisierter Tests verwendet.
Mit dem assert-Modul konnten Sie verifizieren, ob Ihr Code funktioniert.
Sie haben mit Mocha auch synchrone und asynchrone Funktionen getestet.
Schließlich haben Sie Hooks mit Mocha erstellt, die das Schreiben mehrerer verwandter Testfälle wesentlich lesbarer und wartungsfreundlicher machen.
Mit diesen neuen Kenntnissen können Sie nun versuchen, Tests für neue Node.js-Module zu schreiben, die Sie gerade erstellen.
Können Sie über die Ein- und Ausgaben Ihrer Funktion nachdenken und Ihren Test schreiben, bevor Sie Ihren Code erstellen?
Wenn Sie weitere Informationen über das Mocha-Framework erhalten möchten, besuchen Sie unsere offizielle Mocha-Dokumentation.
Wenn Sie gerne noch mehr über Node.js lernen möchten, können Sie zu der Serienseite Codieren in Node.js zurückkehren.
Ein DigitalOcean Workshop Kit
3985
< $> note label Automatisieren der Servereinrichtung mit Ansible Workshop Kit-Materialien Dieses Workshop Kit ist so konzipiert, dass es einem technischen Fachpublikum erlaubt, sich mit Konfigurationsmanagementkonzepten sowie der Verwendung von Ansible zur Automatisierung der Serverinfrastruktureinrichtung vertraut zu machen.
Das Ziel des Kits besteht darin, einem Redner einen kompletten Ressourcensatz zum Abhalten einer Veranstaltung und Halten einer Einführungsrede über Ansible bereitzustellen.
Das Kit beinhaltet:
Folien und Redemanuskripte, einschließlich kurzer Demovideos und Anweisungen zur Ausführung einer optionalen Live-Demo.
Dieser Vortrag hat eine Länge von etwa 50 Minuten.
Ein GitHub-Repository, das den Demo-App-Code und die erforderlichen Ansible-Skripte für die Bereitstellung dieser Anwendung auf einem Ubuntu-Server enthält.
Dieses Tutorial, das einen Benutzer durch die Bereitstellung der Travellist-Demo der Laravel-Anwendung auf einem Remoteserver leitet.
Dieses Tutorial ist als Ergänzung für die Vortrags-Demo inklusive zusätzlicher Details und Erläuterungen gedacht.
Außerdem dient es als Referenz für Leser, die unter Einsatz von Ansible eine Laravel-Anwendung auf einem Ubuntu-Remoteserver bereitstellen möchten.
Üblicherweise werden zur Optimierung der automatisierten Einrichtung von Servern Tools für das Konfigurationsmanagement wie z. B. Ansible verwendet, um Standardverfahren für neue Server festzulegen.
Das hat den Vorteil, dass sich mit manuellen Konfigurationen verbundene menschliche Fehler verringern lassen.
Dieses Tutorial, das als Begleitung zu den Folien und Redemanuskripten für die Automatisierung der Servereinrichtung mit Ansible Workshop Kit gedacht ist, zeigt Ihnen, wie Sie eine Inventurdatei einrichten und eine Reihe von Bereitstellungsskripten ausführen können, um das Verfahren der Einrichtung eines LEMP-Remoteservers (Linux, (E) Nginx, MariaDB und PHP-FPM) unter Ubuntu 18.04 vollständig zu automatisieren und in diesem System eine Laravel-Demoanwendung bereitzustellen.
< $> note Anmerkung: Dieses Material soll zeigen, wie Sie mit Ansible Playbooks zur Automatisierung der Servereinrichtung verwenden können.
Unsere Demo besteht zwar aus einer Laravel-Anwendung, die auf einem LEMP-Server ausgeführt wird, doch werden Leser dazu ermutigt, die enthaltene Einrichtung ihren Anforderungen entsprechend zu ändern und anzupassen.
Stellen Sie sicher, dass der Steuerknoten einen regulären Benutzer mit sudo-Berechtigungen und einer aktivierten Firewall (wie in unserem Leitfaden Ersteinrichtung eines Servers erläutert) sowie einen Satz an gültigen SSH-Schlüsseln aufweist.
Einen oder mehrere Ansible-Hosts: einen oder mehrere Ubuntu 18.04-Remoteserver.
Jedem Host muss der öffentliche Schlüssel des Steuerknotens dessen Datei authorized _ keys hinzugefügt werden, wie in Schritt 2 des Leitfadens Einrichten von SSH-Schlüsseln unter Ubuntu 18.04 beschrieben.
Wenn Sie DigitalOcean-Droplets als Knoten verwenden, können Sie das Bedienfeld nutzen, um Ihren öffentlichen Schlüssel zu Ihren Ansible-Hosts hinzuzufügen.
Schritt 1 - Klonen des Demo-Repository
Als Erstes müssen wir das Repository, das die Ansible-Bereitstellungsskripte enthält, und die Laravel-Demoanwendung klonen, die wir auf den Remoteservern bereitstellen werden.
Alle erforderlichen Dateien finden Sie im Github-Repository do-community / ansible-laravel-demo.
Nachdem Sie sich als sudo-Benutzer bei Ihrem Ansible-Steuerknoten angemeldet haben, klonen Sie das Repository und navigieren Sie zu dem Verzeichnis, das Sie mit dem Befehl git erstellt haben:
Nun können Sie einen ls-Befehl ausführen, um den Inhalt des geklonten Repository zu inspizieren:
Hier ist eine Übersicht über die einzelnen Ordner und Dateien sowie über deren Funktion:
application /: Dieses Verzeichnis enthält die Laravel-Demoanwendung, die im Laufe des Workshops auf dem Remoteserver bereitgestellt wird.
group _ vars /: Dieses Verzeichnis enthält Variablendateien, die benutzerdefinierte Optionen für die Anwendungseinrichtung enthalten, z. B. Datenbankanmeldeinformationen sowie Angaben zum Speicherort der Anwendungsdateien auf dem Remoteserver.
roles /: Dieses Verzeichnis enthält die verschiedenen Ansible-Rollen, die die Bereitstellung eines Ubuntu-LEMP-Servers handhaben.
inventory-example: Diese Datei kann als Basis zur Erstellung eines benutzerdefinierten Inventars für Ihre Infrastruktur verwendet werden.
laravel-deploy.yml: Dieses Playbook stellt die Laravel-Demoanwendung auf dem Remoteserver bereit.
laravel-env.j2: Diese Vorlage wird vom Playbook laravel-deploy.yml zur Einrichtung der Anwendungsumgebungsdatei verwendet.
readme.md: Diese Datei enthält allgemeine Informationen über die in diesem Repository enthaltene Bereitstellung.
server-setup.yml: Dieses Playbook stellt unter Verwendung der im Verzeichnis roles / definierten Rollen einen LEMP-Server bereit.
Schritt 2 - Einrichten der Inventardatei und Testen der Verbindung zu Knoten
Jetzt erstellen wir eine Inventardatei, um die Hosts aufzulisten, die wir mit Ansible verwalten möchten.
Kopieren Sie zunächst die Datei inventory-example in eine neue Datei namens hosts:
Verwenden Sie nun einen Texteditor Ihrer Wahl zum Öffnen der neuen Inventardatei und zum Aktualisieren Ihrer eigenen Server.
Das Beispielinventar, das Teil des Workshop Kits ist, enthält zwei Ansible-Gruppen: dev und production.
Dadurch soll gezeigt werden, wie sich Gruppenvariablen verwenden lassen, um die Bereitstellung in verschiedenen Umgebungen anzupassen.
Wenn Sie diese Einrichtung mit einem einzigen Knoten testen möchten, können Sie entweder die Gruppe dev oder production verwenden und die andere aus der Inventardatei entfernen.
< $> note Anmerkung: Die Variable ansible _ python _ interpreter definiert den Pfad zur ausführbaren Python-Datei auf dem Remotehost.
Hier weisen wir Ansible an, diese Variable für alle Hosts in dieser Inventardatei festzulegen.
Wenn Sie nano verwenden, können Sie Strg + X drücken, dann Y eingeben und zur Bestätigung ENTER drücken.
Sobald Sie Ihre Inventardatei angepasst haben, können Sie das ping-Ansible-Modul ausführen, um zu testen, ob der Steuerknoten eine Verbindung mit den Hosts herstellen kann:
Lassen Sie uns den Befehl aufschlüsseln:
all: Diese Option weist Ansible an, den folgenden Befehl auf allen Hosts in der designierten Inventardatei auszuführen.
-i hosts: Gibt an, welches Inventar verwendet werden soll.
Wenn diese Option nicht angegeben ist, wird Ansible versuchen, das standardmäßige Inventar zu verwenden, das sich typischerweise unter / etc / ansible / hosts befindet.
-m ping: Damit wird das ping-Ansible-Modul ausgeführt, das die Verbindung mit Knoten testen wird und prüft, ob sich die ausführbare Python-Datei auf den Remotesystemen finden lässt.
-u root: Diese Option gibt an, welcher Remotebenutzer zur Verbindung mit den Knoten verwendet werden soll.
Wir verwenden hier das Root-Konto als Beispiel, da dies in der Regel das einzige Konto ist, das auf neu eingerichteten Servern verfügbar ist.
Je nach Ihrem Infrastrukturanbieter und der SSH-Konfiguration können weitere Verbindungsoptionen erforderlich sein.
Wenn Ihre SSH-Verbindung zu den Knoten richtig eingerichtet ist, erhalten Sie folgende Ausgabe:
Die pong-Antwort bedeutet, dass sich Ihr Steuerknoten mit den verwalteten Knoten verbinden kann und Ansible dazu in der Lage ist, Python-Befehle auf den Remotehosts auszuführen.
Schritt 3 - Einrichten von Variablendateien
Bevor Sie die in diesem Workshop Kit enthaltenen Playbooks ausführen, müssen Sie zunächst die Variablendatei bearbeiten, die Einstellungen wie den Namen des zu erstellenden Remotebenutzers sowie die mit MariaDB einzurichtenden Datenbankanmeldeinformationen enthält.
Öffnen Sie mit Ihrem bevorzugten Texteditor die Datei group _ vars / all:
Die Variablen, die Ihre Aufmerksamkeit benötigen, sind:
remote _ user: Der angegebene Benutzer wird auf dem Remoteserver erstellt und erhält sudo-Berechtigungen.
mysql _ root _ password: Diese Variable definiert das Datenbank-Rootpasswort für den MariaDB-Server.
Beachten Sie, dass es sich dabei um ein sicheres Passwort Ihrer Wahl handeln sollte.
mysql _ app _ db: Der Name der Datenbank, die für die Laravel-Anwendung erstellt wird.
Sie müssen diesen Wert nicht ändern, können es aber auf Wunsch tun.
Dieser Wert wird zur Einrichtung der Laravel-Konfigurationsdatei .env verwendet.
mysql _ app _ user: Der Name des Datenbankbenutzers für die Laravel-Anwendung.
Auch hier müssen Sie den Wert nicht ändern, können es aber auf Wunsch tun.
mysql _ app _ pass: Das Datenbankpasswort für die Laravel-Anwendung.
Das sollte ein sicheres Passwort Ihrer Wahl sein.
http _ host: Der Domänenname oder die IP-Adresse des Remotehosts.
Hier verwenden wir ein Ansible-Fakt, das die IPv4-Adresse für die Netzwerkschnittstelle eth0 enthält.
Wenn Sie über Domänennamen verfügen, die auf Ihre Remotehosts verweisen, möchten Sie ggf. für jeden dieser Namen separate Variablendateien erstellen, um diesen Wert zu überschreiben, damit die Nginx-Konfiguration den richtigen Hostnamen für jeden Server enthält.
Wenn Sie mit der Bearbeitung dieser Werte fertig sind, speichern und schließen Sie die Datei.
Erstellen zusätzlicher Variablendateien für verschiedene Umgebungen
Wenn Sie Ihre Inventardatei mit mehreren Knoten eingerichtet haben, wollen Sie möglicherweise zusätzliche Variablendateien erstellen, um jeden Knoten entsprechend zu konfigurieren.
In unserem Beispielinventar haben wir zwei verschiedene Gruppen erstellt: dev und production.
Um zu vermeiden, dass in beiden Umgebungen dieselben Datenbank-Anmeldeinformationen und andere Einstellungen verwendet werden, müssen wir eine separate Variablendatei für die Produktionswerte erstellen.
Vielleicht möchten Sie die standardmäßige Variablendatei kopieren und sie als Basis für Ihre Produktionswerte nutzen:
Da die Datei all.yml die Standardwerte enthält, die für alle Umgebungen gültig sein sollen, können Sie alle Variablen entfernen, die in der neuen Datei production.yml keine Änderung benötigen.
Die Variablen, die Sie für die jeweilige Umgebung aktualisieren sollten, sind hier hervorgehoben:
Beachten Sie, dass wir den Wert app _ env in prod geändert haben und den Wert app _ debug auf false gesetzt haben.
Das sind die empfohlenen Laravel-Einstellungen für Produktionsumgebungen.
Wenn Sie die Anpassung Ihrer Produktionsvariablen abgeschlossen haben, speichern und schließen Sie die Datei.
Verschlüsseln von Variablendateien mit Ansible Vault
Wenn Sie planen, Ihre Ansible-Einrichtung mit anderen Benutzern zu teilen, ist es wichtig, die Datenbank-Anmeldeinformationen und andere sensible Daten in Ihren Variablendateien sicher aufzubewahren.
Das ist mit Ansible Vault möglich, einer Funktion, die standardmäßig in Ansible enthalten ist.
Mit Ansible Vault können Sie Variablendateien verschlüsseln, damit nur Benutzer, die Zugriff auf das Vault-Passwort haben, die Dateien anzeigen, bearbeiten oder entschlüsseln können.
Außerdem wird das Vault-Passwort für die Ausführung eines Playbooks oder Befehls benötigt, das bzw. der verschlüsselte Dateien nutzt.
Um Ihre Produktionsvariablendatei zu verschlüsseln, führen Sie Folgendes aus:
Sie werden dazu aufgefordert, ein Vault-Passwort anzugeben und zu bestätigen. Wenn Sie damit fertig sind und die Inhalte dieser Datei überprüfen, sehen Sie, dass die Daten nun verschlüsselt sind.
Wenn Sie die Variablendatei anzeigen möchten, ohne ihren Inhalt zu ändern, können Sie den Befehl view verwenden:
Sie werden dazu aufgefordert, das gleiche Passwort einzugeben, das Sie festgelegt haben, als Sie diese Datei mit ansible-vault verschlüsselt haben.
Nach Eingabe des Passworts wird der Inhalt der Datei in Ihrem Terminal angezeigt.
Geben Sie q ein, um die Dateiansicht zu beenden.
Um eine zuvor mit Ansible Vault verschlüsselte Datei zu bearbeiten, verwenden Sie den Vault-Befehl edit:
Bei diesem Befehl werden Sie dazu aufgefordert, das Vault-Passwort für diese Datei anzugeben.
Dann wird Ihr standardmäßiger Terminal-Editor genutzt, um die Datei zur Bearbeitung zu öffnen.
Speichern und schließen Sie die Datei nach dem Vornehmen der gewünschten Änderungen, woraufhin sie durch Ansible Vault automatisch verschlüsselt wird.
Jetzt haben Sie die Einrichtung Ihrer Variablendateien abgeschlossen.
Im nächsten Schritt führen wir das Playbook aus, um Nginx, PHP-FPM und MariaDB (die zusammen mit einem Linux-basierten Betriebssystem wie Ubuntu den LEMP-Stack bilden) auf Ihrem bzw. Ihren Remoteservern einzurichten.
Schritt 4 - Ausführen des LEMP-Playbook
Bevor wir die Laravel-Demoanwendung auf den Remoteservern bereitstellen, müssen wir eine LEMP-Umgebung einrichten, die die Anwendung bereitstellen wird.
Das Playbook server-setup.yml enthält die zur Einrichtung erforderlichen Ansible-Rollen.
Um dessen Inhalte zu inspizieren, führen Sie Folgendes aus:
Hier ist eine Übersicht über alle in diesem Playbook enthaltenen Rollen:
setup: Enthält die Aufgaben, die zur Erstellung eines neuen Systembenutzers und Gewährung von sudo-Berechtigungen sowie zur Aktivierung der ufw-Firewall erforderlich sind.
mariadb: Installiert den MariaDB-Datenbankserver und erstellt die Anwendungsdatenbank sowie den Benutzer.
php: Installiert php-fpm und PHP-Module, die zur Ausführung einer Laravel-Anwendung benötigt werden.
nginx: Installiert den Nginx-Webserver und ermöglicht Zugriff an Port 80.
composer: Installiert Composer global.
Beachten Sie, dass wir in jeder Rolle einige Tags eingerichtet haben.
Dies soll es erlauben, bei Bedarf nur Teile dieses Playbooks neu auszuführen.
Wenn Sie Änderungen an Ihrer Nginx-Vorlagendatei vornehmen, möchten Sie möglicherweise nur die Nginx-Rolle ausführen.
Der folgende Befehl führt dieses Playbook aus Ihrer Inventardatei auf allen Servern aus.
--ask-vault-pass ist nur erforderlich, falls Sie im vorherigen Schritt ansible-vault verwendet haben, um verschiedene Variablendateien zu verschlüsseln:
Ihre Knoten sind nun bereit, PHP-Anwendungen mit Nginx + PHP-FPM bereitzustellen, wobei MariaDB als Datenbankserver dient.
Im nächsten Schritt werden wir die enthaltene Laravel-Demoanwendung mit dem Ansible-Playbook laravel-deploy.yml bereitstellen.
Schritt 5 - Bereitstellen der Laravel-Anwendung
Nachdem Sie auf Ihrem bzw. Ihren Remoteservern nun über eine funktionierende LEMP-Umgebung verfügen, können Sie das Playbook laravel-deploy.yml ausführen.
Dieses Playbook führt folgende Aufgaben aus:
Erstellen Sie den Anwendungsdokument-Root auf dem Remoteserver, wenn dies noch nicht geschehen ist.
Synchronisieren Sie den lokalen Anwendungsordner zum Remoteserver mit dem Modul sync.
Verwenden Sie das Modul acl, um Berechtigungen für den Benutzer www-data für den Speicherordner festzulegen.
Richten Sie die Anwendungsdatei .env auf Grundlage der Vorlage laravel-env.j2 ein.
Installieren Sie Anwendungsabhängigkeiten mit Composer.
Erstellen Sie den Anwendungssicherheitsschlüssel.
Richten Sie einen öffentlichen Link für den Ordner storage ein.
Führen Sie Datenbankmigrationen und Seeder aus.
Dieses Playbook sollte von einem non-root user, aber mit sudo-Berechtigungen ausgeführt werden.
Dieser Benutzer sollte erstellt worden sein, als Sie im vorherigen Schritt das Playbook server-setup.yml ausgeführt haben - unter Verwendung des Namens, der von der Variable remote _ user definiert wird.
Wenn Sie bereit sind, führen Sie das Playbook laravel-deploy.yml aus mit:
--ask-vault-pass ist nur erforderlich, falls Sie im vorherigen Schritt ansible-vault verwendet haben, um Variablendateien zu verschlüsseln:
Sobald die Ausführung abgeschlossen ist, können Sie auf die Demoanwendung zugreifen, indem Sie Ihren Browser auf den Domänennamen oder die IP-Adresse Ihres Knotens verweisen:
Laravel Travellist-Demo
Dieses Tutorial zeigt, wie Sie eine Ansible-Inventardatei einrichten, eine Verbindung mit Remoteknoten herstellen und Ansible-Playbooks ausführen, um einen LEMP-Server einzurichten und eine Laravel-Demoanwendung darauf bereitzustellen. Dieser Leitfaden ergänzt die Automatisierung der Servereinrichtung um Folien und Redemanuskripte des Ansible Workshop Kit und wird von einem beispielhaften GitHub-Repository begleitet, das alle erforderlichen Dateien zum Befolgen der Demokomponente des Workshops enthält.
Erstellen eines URL-Shorteners mit Django und GraphQL
3983
GraphQL ist ein API-Standard, der von Facebook als Alternative zu REST-APIs entwickelt wurde und im Open-Source-Modell bereitgestellt wird.
Im Gegensatz zu REST-APIs verwendet GraphQL ein typisiertes System zur Definition seiner Datenstruktur, wobei alle Daten, die gesendet und empfangen werden, einem vordefinierten Schema entsprechen müssen.
Außerdem macht die Sprache einen einzigen Endpunkt für die gesamte Kommunikation verfügbar (anstelle unterschiedlicher URLs für verschiedene Ressourcen) und löst das Overfetching-Problem dadurch, dass nur die Daten zurückgegeben werden, die der Client angefordert hat. Somit werden kleinere und präzisere Antworten generiert.
In diesem Tutorial erstellen Sie ein Backend für einen URL-Shortener - einen Dienst, der aus einer beliebigen URL eine kürzere, besser lesbare Version generiert. Außerdem erfahren Sie mehr über GraphQL-Konzepte wie Abfragen und Mutationen sowie Tools (wie die GraphiQL-Oberfläche).
Vielleicht haben Sie solche Dienste zuvor bereits verwendet, wie z. B. bit.ly.
Da GraphQL eine sprachunabhängige Technologie ist, wird sie auf verschiedene Sprachen und Frameworks aufgesetzt.
Hier verwenden Sie die allgemeine Python-Programmiersprache, das Django-Web-Framework sowie die Graphene-Django-Bibliothek als GraphQL-Python-Implementierung mit spezifischen Integrationen für Django.
Um mit diesem Tutorial fortzufahren, muss auf Ihrem Entwicklungsrechner Python Version 3.5 oder höher installiert sein.
Um Python zu installieren, folgen Sie unserem Tutorial Installieren und Einrichten einer lokalen Programmierumgebung für Python 3 für Ihr Betriebssystem.
Stellen Sie sicher, dass Sie auch eine virtuelle Umgebung einrichten und starten; um den Vorgaben in diesem Tutorial zu folgen, können Sie Ihr Projektverzeichnis < ^ > shorty < ^ > nennen.
Grundlegende Kenntnisse über Django sind erwünscht, aber nicht zwingend.
Wenn Sie neugierig sind, können Sie dieser Django-Entwicklungsreihe folgen, die von der DigitalOcean-Community erstellt wurde.
Schritt 1 - Einrichten des Django-Projekts
In diesem Schritt installieren Sie alle erforderlichen Tools für die Anwendung und Einrichtung Ihres Django-Projekts.
Sobald Sie Ihr Projektverzeichnis erstellt und Ihre virtuelle Umgebung gestartet haben (wie in den Voraussetzungen beschrieben), installieren Sie die erforderlichen Pakete mit pip, dem Python-Paketmanager.
In diesem Tutorial werden Django Version 2.1.7 und Graphene-Django Version 2.2.0 oder höher installiert:
Sie haben nun alle Werkzeuge, die Sie zur Erledigung Ihrer Aufgaben benötigen.
Als Nächstes erstellen Sie ein Django-Projekt mit dem Befehl django-admin.
Ein Projekt ist der standardmäßige Django-Baustein: ein Satz von Ordnern und Dateien mit allem, was für die Entwicklung einer Webanwendung erforderlich ist.
In diesem Fall nennen Sie Ihr Projekt < ^ > shorty < ^ > und erstellen es in Ihrem aktuellen Ordner, indem Sie am Ende. angeben:
Nach der Erstellung Ihres Projekts führen Sie die Django-Migrationen durch.
Diese Dateien enthalten von Django generierten Python-Code und sind für die Änderung der Struktur der Anwendung entsprechend den Django-Modellen verantwortlich.
Änderungen können beispielsweise die Erstellung einer Tabelle beinhalten.
Standardmäßig verfügt Django über einen eigenen Satz von Migrationen, die für Subsysteme wie Django-Authentifizierung zuständig sind. Daher müssen Sie sie mit dem folgenden Befehl ausführen:
Dieser Befehl verwendet den Python-Interpreter, um ein Django-Skript namens manage.py aufzurufen, das für die Verwaltung verschiedener Aspekte Ihres Projekts verantwortlich ist, wie das Erstellen von Anwendungen oder die Ausführung von Migrationen.
Sobald die Datenbank von Django bereit ist, starten Sie ihren lokalen Entwicklungsserver:
Dieser Befehl übernimmt die Eingabeaufforderung in Ihrem Terminal und startet den Server.
Besuchen Sie die Seite http: / / 127.0.0.1: 8000 in Ihrem lokalen Browser.
Sie sehen diese Seite:
Titelseite des lokalen Django-Servers
Um den Server zu stoppen und zu Ihrem Terminal zurückzukehren, drücken Sie Strg + C.
Wann immer Sie auf den Browser zugreifen müssen, stellen Sie sicher, dass der vorhergehende Befehl ausgeführt wird.
Als Nächstes beenden Sie diesen Schritt, indem Sie im Projekt die Django-Graphene-Bibliothek aktivieren.
Django nutzt das Konzept von Apps, von Webanwendungen mit einer spezifischen Aufgabe.
Ein Projekt besteht aus einer oder mehreren Apps.
Öffnen Sie jetzt die Datei < ^ > shorty < ^ > / settings.py in Ihrem bevorzugten Text-Editor.
In diesem Tutorial wird vim verwendet:
Die Datei settings.py verwaltet alle Einstellungen in Ihrem Projekt.
Suchen Sie darin nach dem Eintrag INSTALLED _ APPS und fügen Sie die Zeile 'graphene _ django' hinzu:
Dadurch erfährt Django, dass Sie eine App namens graphene _ django verwenden werden, die Sie in Schritt 1 installiert haben.
Fügen Sie am unteren Ende der Datei die folgende Variable hinzu:
Diese letzte Variable verweist auf Ihr zentrales Schema, das Sie später erstellen werden.
In GraphQL enthält ein Schema alle Objekttypen, wie Ressourcen, Abfragen und Mutationen.
Stellen Sie sich ein Schema als Dokumentation aller Daten und Funktionen vor, die in Ihrem System verfügbar sind.
Speichern und schließen Sie die Datei nach Vornahme der Änderungen.
Jetzt haben Sie das Django-Projekt konfiguriert.
Im nächsten Schritt werden Sie eine Django-App und ihre Modelle erstellen.
Schritt 2 - Einrichten einer Django-App und ihrer Modelle
Eine Django-Plattform besteht normalerweise aus einem Projekt und vielen Anwendungen oder Apps.
Eine App beschreibt eine Reihe von Funktionen innerhalb eines Projekts und kann, falls gut gestaltet, in Django wiederverwendet werden.
In diesem Schritt erstellen Sie eine App namens < ^ > shortener < ^ >, die für die tatsächliche URL-Verkürzung verantwortlich ist.
Geben Sie für die Erstellung des Fundaments den folgenden Befehl in Ihr Terminal ein:
Hier haben Sie den Parameter startapp < ^ > app _ name < ^ > verwendet, um manage.py anzuweisen, eine App namens shortener zu erstellen.
Um die App-Erstellung abzuschließen, öffnen Sie die Datei < ^ > shorty < ^ > / settings.py.
Fügen Sie den Namen der App dem gleichen INSTALLED _ APPS-Eintrag hinzu, den Sie zuvor geändert haben:
Nachdem Sie Ihren shortener zu < ^ > shorty < ^ > / settings.py hinzugefügt haben, können Sie mit der Erstellung der Modelle für Ihr Projekt fortfahren.
Modelle sind eine der Schlüsselfunktionen in Django.
Sie werden verwendet, um eine Datenbank auf eine "Python-artige" Weise darzustellen, sodass Sie Daten mit Python-Code verwalten, abfragen und speichern können.
Bevor die Datei models.py für Änderungen geöffnet wird, bietet Ihnen dieses Tutorial einen Überblick über die Änderungen, die Sie vornehmen werden.
Ihre Modelldatei (shortener / models.py) wird nach dem Austausch des vorhandenen Codes den folgenden Inhalt umfassen:
Hier importieren Sie die erforderlichen Pakete, die von Ihrem Code benötigt werden.
Sie fügen oben die Zeile from hashlib import md5 hinzu, um die Standardbibliothek von Python zu importieren, die zur Erstellung eines Hash der URL verwendet wird.
Die Zeile from django.db import models ist eine Django-Hilfe für das Erstellen von Modellen.
< $> warning Warnung: Dieses Tutorial verweist auf Hash als das Ergebnis einer Funktion, die eine Eingabe nimmt und immer dieselbe Ausgabe zurückgibt.
In diesem Tutorial wird für Demonstrationszwecke die MD5-Hash-Funktion verwendet.
Beachten Sie, dass MD5 Kollisionsprobleme aufweist und in der Produktion vermieden werden sollte.
Als Nächstes fügen Sie ein Modell namens URL mit den folgenden Feldern hinzu:
full _ url: die URL, die verkürzt werden soll.
url _ hash: ein kurzer Hashwert, der die vollständige URL repräsentiert.
clicks: gibt darüber Auskunft, wie oft auf die kurze URL zugegriffen wurde.
created _ at: Datum und Uhrzeit der Erstellung der URL.
Sie generieren den url _ hash, indem Sie den MD5-Hash-Algorithmus auf das Feld full _ url anwenden und nur die ersten 10 Zeichen nutzen, die bei der save () -Methode des Modells zurückgegeben werden; diese wird jedes Mal ausgeführt, wenn Django einen Eintrag in der Datenbank speichert.
Außerdem verfolgen URL-Shortener meist, wie oft auf einen Link geklickt wurde.
Sie können dies erreichen, indem Sie die Methode clicked () aufrufen, wenn die URL von einem Benutzer besucht wird.
Die genannten Operationen werden mit diesem Code in Ihrem URL-Modell hinzugefügt:
Nachdem Sie den Code überprüft haben, öffnen Sie jetzt die Datei shortener / models.py:
Ersetzen Sie den Code mit dem folgenden Inhalt:
Um diese Änderungen in der Datenbank anzuwenden, müssen Sie die entsprechenden Migrationen erstellen, indem Sie den folgenden Befehl ausführen:
Dann führen Sie die Migrationen aus:
Nachdem Sie nun die Modelle eingerichtet haben, werden Sie im nächsten Schritt den GraphQL-Endpunkt und eine Abfrage erstellen.
Schritt 3 - Erstellen von Abfragen
Die REST-Architektur macht verschiedene Ressourcen in verschiedenen Endpunkten verfügbar, jeweils mit einer genau definierten Datenstruktur.
Zum Beispiel können Sie unter / api / users eine Benutzerliste abrufen und dabei immer dieselben Felder erwarten.
GraphQL hingegen verfügt über einen einzigen Endpunkt für alle Interaktionen und verwendet Abfragen für den Zugriff auf Daten. Der größte und wichtigste Unterschied besteht darin, dass Sie eine Abfrage verwenden können, um alle Ihre Benutzer im Rahmen einer einzigen Anfrage abzurufen.
Erstellen Sie zunächst eine Abfrage, um alle URLs abzurufen.
Sie benötigen mehrere Dinge:
Einen URL-Typ, der mit Ihrem zuvor definierten Modell verknüpft ist.
Eine Abfrageanweisung namens urls.
Eine Methode zum Auflösen Ihrer Abfrage, was bedeutet, dass alle URLs aus der Datenbank abgerufen und an den Client zurückgegeben werden.
Erstellen Sie eine neue Datei namens shortener / schema.py:
Fügen Sie zunächst die import-Anweisungen von Python hinzu:
Die erste Zeile sorgt für den Import der wichtigsten graphene-Bibliothek, die die grundlegenden GraphQL-Typen enthält, z. B. List.
Der DjangoObjectType ist ein Helfer, der Sie bei der Erstellung einer Schemadefinition aus einem beliebigen Django-Modell unterstützt, während die dritte Zeile dafür sorgt, dass Ihr zuvor erstelltes URL-Modell importiert wird.
Danach erstellen Sie einen neuen GraphQL-Typ für das Modell URL, indem Sie die folgenden Zeilen hinzufügen:
Fügen Sie schließlich folgende Zeilen hinzu, um einen Abfragetyp für das URL-Modell zu erstellen:
Dieser Code erstellt eine Query-Klasse mit einem Feld namens urls, das eine Liste des zuvor definierten URLType ist.
Wenn Sie die Abfrage mit der Methode resolve _ urls auflösen, geben Sie alle in der Datenbank gespeicherten URLs zurück.
Die vollständige Datei shortener / schema.py wird hier angezeigt:
Alle Abfragen müssen jetzt dem Hauptschema hinzugefügt werden.
Stellen Sie sich das Schema als Behälter für alle Ihre Ressourcen vor.
Erstellen Sie eine neue Datei im Pfad < ^ > shorty < ^ > / schema.py und öffnen Sie sie mit Ihrem Editor:
Importieren Sie die folgenden Python-Pakete, indem Sie folgende Zeilen hinzufügen.
Die erste Zeile enthält, wie bereits erwähnt, die grundlegenden GraphQL-Typen.
Die zweite Zeile importiert die zuvor erstellte Schemadatei.
Fügen Sie als Nächstes die Haupt-Query-Klasse hinzu.
Sie wird via Vererbung alle Abfragen und künftig erstellten Operationen enthalten:
Erstellen Sie schließlich die Variable schema:
Die SCHEMA-Einstellung, die Sie in Schritt 2 definiert haben, verweist auf die Variable schema, die Sie gerade erstellt haben.
Die vollständige Datei < ^ > shorty < ^ > / schema.py wird hier angezeigt:
Aktivieren Sie als Nächstes den GraphQL-Endpunkt und die GraphiQL-Oberfläche, die eine grafische Weboberfläche ist, die der Interaktion mit dem GraphQL-System dient.
Öffnen Sie die Datei < ^ > shorty < ^ > / urls.py:
Zu Lernzwecken löschen Sie die Dateiinhalte und speichern sie, damit Sie ganz neu anfangen können.
Die ersten Zeilen, die Sie hinzufügen, sind Python-Importanweisungen:
Die Funktion path wird von Django verwendet, um eine aufrufbare URL für die GraphiQL-Oberfläche zu erstellen.
Danach importieren Sie csrf _ exempt, was es Clients ermöglicht, Daten an den Server zu senden.
Eine vollständige Erklärung finden Sie in der Graphene-Dokumentation.
In der letzten Zeile haben Sie über GraphQLView den tatsächlichen Code importiert, der für die Oberfläche verantwortlich ist.
Erstellen Sie als Nächstes eine Variable namens urlpatterns.
Diese wird den gesamten Code zusammenfügen, der erforderlich ist, um die GraphiQL-Oberfläche im Pfad graphql / verfügbar zu machen:
Die vollständige Datei shortener / urls.py wird hier angezeigt:
Führen Sie zurück im Terminal den Befehl python manage.py runserver aus (wenn er nicht bereits ausgeführt wird):
Öffnen Sie Ihren Webbrowser unter der Adresse http: / / localhost: 8000 / graphql.
Ihnen wird der folgende Bildschirm angezeigt:
GraphiQL-Oberfläche
GraphiQL ist eine Oberfläche, in der Sie GraphQL-Anweisungen ausführen und die Ergebnisse anzeigen können.
Eine Funktion ist der Abschnitt Docs oben rechts.
Da alles in GraphQL typisiert ist, erhalten Sie eine freie Dokumentation für alle Ihre Typen, Abfragen, Mutationen usw.
Nach Erkundung der Seite fügen Sie im Haupttextbereich Ihre erste Abfrage ein:
Dieser Inhalt zeigt, wie eine GraphQL-Abfrage strukturiert ist: Erstens verwenden Sie das Schlüsselwort query, um dem Server zu sagen, dass Sie nur einige Daten zurückgeben möchten.
Als Nächstes verwenden Sie das Feld urls, das in der Datei shortener / schema.py in der Klasse Query definiert ist.
Daraus fordern Sie ausdrücklich an, dass alle im URL-Modell definierten Felder mit Höckerschrift definiert werden, was die Standardeinstellung für GraphQL ist.
Klicken Sie nun oben links auf die Wiedergabepfeiltaste.
Sie erhalten folgende Antwort, in der steht, dass Sie immer noch keine URLs haben:
Dies zeigt, dass GraphQL funktioniert.
Drücken Sie in Ihrem Terminal Strg + C, um Ihren Server anzuhalten.
In diesem Schritt haben Sie viel erreicht: Sie haben den GraphQL-Endpunkt erstellt, eine Abfrage ausgeführt, um alle URLs abzurufen, und die GraphQL-Oberfläche aktiviert.
Jetzt erstellen Sie Mutationen, um die Datenbank zu ändern.
Schritt 4 - Erstellen von Mutationen
Die meisten Anwendungen bieten eine Möglichkeit, den Datenbankzustand zu ändern, indem sie Daten hinzufügen, aktualisieren oder löschen. In GraphQL werden diese Operationen Mutationen genannt.
Sie sehen wie Abfragen aus, verwenden aber Argumente, um Daten an den Server zu senden.
Um Ihre erste Mutation zu erstellen, öffnen Sie shortener / schema.py:
Am Ende der Datei fügen Sie zunächst eine neue Klasse namens CreateURL hinzu:
Diese Klasse erbt das Hilfsprogramm graphene.Mutation, um über die Fähigkeiten einer GraphQL-Mutation zu verfügen.
Außerdem verfügt sie über einen Eigenschaftsnamen url, der den Inhalt definiert, der vom Server nach Fertigstellung der Mutation zurückgegeben wird.
In diesem Fall ist es die Datenstruktur URLType.
Fügen Sie als Nächstes in der bereits definierten Klasse eine Unterklasse namens Arguments hinzu:
Damit wird definiert, welche Daten vom Server akzeptiert werden.
Hier erwarten Sie einen Parameter namens full _ url mit einem String-Inhalt:
Fügen Sie jetzt folgende Zeilen hinzu, um die mutate-Methode zu erstellen:
Diese mutate-Methode erledigt viele Aufgaben, indem sie Daten vom Client empfängt und in der Datenbank speichert.
Am Ende gibt sie die Klasse selbst zurück, die das neu erstellte Element enthält.
Erstellen Sie schließlich eine Klasse Mutation, die alle Mutationen für Ihre App enthält, indem Sie folgende Zeilen hinzufügen:
Bisher verfügen Sie nur über eine Mutation namens create _ url.
Schließen und speichern Sie die Datei.
Um das Hinzufügen der Mutation abzuschließen, ändern Sie die Datei < ^ > shorty < ^ > / schema.py:
Ändern Sie die Datei, indem Sie den folgenden hervorgehobenen Code einfügen:
Wenn Sie den lokalen Server noch nicht ausführen, starten Sie ihn:
Navigieren Sie in Ihrem Webbrowser zu http: / / localhost: 8000 / graphql.
Führen Sie Ihre erste Mutation in der GraphiQL-Weboberfläche aus, indem Sie folgende Anweisung ausführen:
Sie haben die Mutation mit dem Namen createURL, das Argument fullUrl und die Daten erstellt, die Sie in der im Feld url definierten Antwort erhalten möchten.
Die Ausgabe enthält die URL-Informationen, die Sie gerade im GraphQL-Feld data erstellt haben, wie hier gezeigt:
Damit wurde der Datenbank eine URL mit ihrer Hashversion hinzugefügt, wie Sie im Feld urlHash sehen können.
Versuchen Sie, die im letzten Schritt erstellte Abfrage auszuführen, um ihr Ergebnis zu sehen:
Die Ausgabe zeigt die gespeicherte URL:
Sie können auch versuchen, dieselbe Abfrage auszuführen, wobei Sie aber nur nach den gewünschten Feldern fragen.
Probieren Sie es als Nächstes noch einmal mit einer anderen URL:
Das System ist jetzt in der Lage, Kurz-URLs zu erstellen und aufzulisten.
Im nächsten Schritt aktivieren Sie Benutzer, um eine URL anhand ihrer Kurzversion aufzurufen und die Benutzer auf die korrekte Seite weiterzuleiten.
Schritt 5 - Erstellen des Zugriffs-Endpunkts
In diesem Schritt verwenden Sie Django Views - eine Methode, die eine Anfrage übernimmt und eine Antwort zurückgibt -, um alle Benutzer, die auf den Endpunkt http: / / localhost: 8000 / < ^ > url _ hash < ^ > zugreifen, zur vollständigen URL umzuleiten.
Öffnen Sie die Datei shortener / views.py mit Ihrem Editor:
Importieren Sie zunächst zwei Pakete, indem Sie den Inhalt durch die folgenden Zeilen ersetzen:
Diese werden später noch genauer erläutert.
Als Nächstes erstellen Sie eine Django-Ansicht namens root.
Fügen Sie am Ende Ihrer Datei den für die Ansicht verantwortlichen Codeausschnitt hinzu:
Dadurch wird ein Argument namens url _ hash von der URL empfangen, die von einem Benutzer angefordert wurde.
Innerhalb der Funktion versucht die erste Zeile, mit dem Argument url _ hash die URL aus der Datenbank abzurufen.
Wenn die URL nicht gefunden wird, gibt sie den Fehler HTTP 404 an den Client zurück, was bedeutet, dass die Ressource fehlt.
Anschließend wird die Eigenschaft clicked des URL-Eintrags inkrementiert, sodass verfolgt wird, wie oft auf die URL zugegriffen wird.
Am Ende leitet sie den Client zur angeforderten URL weiter.
Die vollständige Datei shortener / views.py wird hier angezeigt:
Öffnen Sie als Nächstes < ^ > shorty < ^ > / urls.py:
Fügen Sie den folgenden hervorgehobenen Code hinzu, um die root-Ansicht zu aktivieren.
Die root-Ansicht wird im Pfad / Ihres Servers zugänglich sein und akzeptiert einen url _ hash als Zeichenfolgenparameter.
Wenn Sie den lokalen Server nicht ausführen, starten Sie ihn durch Ausführung des Befehls python manage.py runserver.
Um Ihre neue Hinzufügung zu testen, öffnen Sie Ihren Webbrowser und rufen Sie die URL http: / / localhost: 8000 / 077880af78 auf.
Beachten Sie, dass der letzte Teil der URL der Hash ist, der von der Mutation in Schritt 5 erstellt wurde. Sie werden zur URL-Seite des Hash, in diesem Fall die Website der DigitalOcean-Community, umgeleitet.
Nachdem nun die URL-Umleitung funktioniert, machen Sie die Anwendung sicherer, indem Sie bei Ausführung der Mutation eine Fehlerbehandlung implementieren.
Schritt 6 - Implementieren der Fehlerbehandlung
Die Behandlung von Fehlern ist bei allen Anwendungen eine bewährte Methode, da Entwickler normalerweise nicht kontrollieren, was an den Server gesendet wird.
Auf diese Weise können Sie versuchen, Fehler vorauszusehen und ihre Auswirkungen zu minimieren.
In einem komplexen System wie GraphQL könnten viele Dinge schiefgehen: von Clients, die die falschen Daten anfragen, bis hin zum Server, der den Zugriff auf die Datenbank verliert.
Als typisiertes System kann GraphQL mit einer Operation namens Schemaüberprüfung alles überprüfen, was ein Client anfragt und empfängt.
Sie können dies testen, indem Sie eine Abfrage mit einem nicht vorhandenen Feld erstellen.
Navigieren Sie in Ihrem Browser erneut zu http: / / localhost: 8000 / graphql und führen Sie die nächste Abfrage innerhalb der GraphiQL-Oberfläche mit dem Feld iDontExist aus:
Da in Ihrer Abfrage kein iDontExist-Feld vorhanden ist, gibt GraphQL eine Fehlermeldung zurück:
Dies ist wichtig, da im typisierten GraphQL-System das Ziel darin besteht, nur die Daten zu senden und zu empfangen, die bereits im Schema definiert sind.
Die aktuelle Anwendung akzeptiert jede beliebige Zeichenfolge im Feld full _ url.
Das Problem besteht darin, dass Sie, wenn jemand eine schlecht konstruierte URL versendet, den Benutzer beim Versuch, die gespeicherten Daten abzurufen, ins Nirgendwo umleiten.
In diesem Fall müssen Sie überprüfen, ob die full _ url richtig formatiert ist, bevor sie in der Datenbank gespeichert wird; falls es Fehler gibt, lösen Sie die Ausnahme GraphQLError mit einer benutzerdefinierten Nachricht aus.
Lassen Sie uns diese Funktion in zwei Schritten implementieren.
Öffnen Sie zunächst die Datei shortener / models.py:
Fügen Sie die hervorgehobenen Zeilen im import-Abschnitt hinzu:
Der URLValidator ist ein Django-Hilfsprogramm zur Überprüfung einer URL-Zeichenfolge, während GraphQLError von Graphene dazu verwendet wird, um Ausnahmen mit einer benutzerdefinierten Nachricht auszulösen.
Überprüfen Sie als Nächstes die vom Benutzer empfangene URL, bevor Sie sie in der Datenbank speichern.
Aktivieren Sie diese Operation, indem Sie in der Datei shortener / models.py den hervorgehobenen Code hinzufügen:
Zunächst instanziiert dieser Code den URLValidator in der Variable validate.
Innerhalb des Blocks try / except überprüfen Sie mit validate () die empfangene URL und lösen einen GraphQLError mit der benutzerdefinierten Nachricht Ungültige URL aus, wenn etwas schiefgegangen ist.
Die vollständige Datei shortener / models.py wird hier angezeigt:
Wenn Sie den lokalen Server noch nicht ausführen, starten Sie ihn mit dem Befehl python manage.py runserver.
Testen Sie als Nächstes Ihre neue Fehlerbehandlung unter http: / / localhost: 8000 / graphql.
Versuchen Sie, in der GraphiQL-Oberfläche eine neue URL mit einer ungültigen full _ url zu erstellen:
Wenn Sie eine ungültige URL senden, wird Ihre Ausnahme mit der benutzerdefinierten Nachricht ausgelöst:
Wenn Sie in Ihrem Terminal nachsehen, wo der Befehl python manage.py runserver ausgeführt wird, wird ein Fehler angezeigt:
Ein GraphQL-Endpunkt wird immer mit einem HTTP 200-Statuscode fehlschlagen, was in der Regel Erfolg bedeutet.
Denken Sie daran, dass GraphQL zwar auf HTTP aufbaut, jedoch nicht die Konzepte von HTTP-Statuscodes oder HTTP-Methoden verwendet, wie REST es tut.
Mit implementierter Fehlerbehandlung können Sie nun ein Verfahren zum Filtern Ihrer Abfrage einrichten, um die Daten zu minimieren, die vom Server zurückgegeben werden.
Schritt 7 - Implementieren von Filtern
Stellen Sie sich vor, Sie haben den URL-Shortener genutzt, um eigene Links hinzuzufügen.
Nach einer Weile gibt es so viele Einträge, dass es schwierig wird, die richtigen zu finden.
Sie können dieses Problem durch Einsatz von Filtern lösen.
Filtern ist ein gängiges Konzept in REST-APIs, bei dem in der Regel ein Abfrageparameter mit einem Feld und Wert an die URL angehängt wird.
Zum Beispiel können Sie zum Filtern aller Benutzer namens jojo GET / api / users? name = jojo verwenden.
In GraphQL verwenden Sie Abfrageparameter als Filter.
Sie sorgen für eine schöne und saubere Oberfläche.
Sie können das Problem des Auffindens einer URL lösen, indem Sie es dem Client mithilfe des Felds full _ url ermöglichen, URLs anhand von Namen zu filtern.
Öffnen Sie dazu die Datei shortener / schema.py in Ihrem bevorzugten Editor.
Importieren Sie zunächst die Methode Q in die hervorgehobene Zeile:
Dies dient zum Filtern Ihrer Datenbankanfrage.
Als Nächstes schreiben Sie die gesamte Query-Klasse mit dem folgenden Inhalt neu:
Die von Ihnen vorgenommenen Änderungen lauten:
Hinzufügen des Filterparameters url innerhalb der Variable urls und der Methode resolve _ url.
Wenn in resolve _ urls ein Parameter namens url angegeben wird, werden beim Filtern der Datenbank mit der Methode Q (full _ url _ _ icontains = url) nur URLs zurückgegeben, die den angegebenen Wert enthalten.
Testen Sie Ihre letzten Änderungen unter http: / / localhost: 8000 / graphql.
Schreiben Sie in der GraphiQL-Oberfläche die folgende Anweisung.
Dadurch werden alle URLs mit dem Wort community gefiltert:
Die Ausgabe umfasst nur einen Eintrag, da Sie nur eine URL mit der Zeichenfolge community hinzugefügt haben. Wenn Sie zuvor weitere URLs hinzugefügt haben, kann Ihre Ausgabe anders aussehen.
Nun haben Sie die Möglichkeit, Ihre URLs zu durchsuchen.
Bei zu vielen Links beschweren sich Ihre Clients ggf. jedoch, dass die URL-Liste mehr Daten zurückgibt, als ihre Apps bewältigen können.
Um dieses Problem zu lösen, werden Sie Paginierung implementieren.
Schritt 8 - Implementieren von Paginierung
Clients, die Ihr Backend verwenden, beschweren sich möglicherweise darüber, dass die Antwortzeit zu lange oder die Größe übermäßig ist, falls es zu viele URL-Einträge gibt.
Außerdem kann Ihre Datenbank Probleme damit bekommen, eine riesige Menge von Daten zusammenzustellen.
Um dieses Problem zu lösen, können Sie es dem Client anhand eines Verfahrens namens Paginierung gestatten festzulegen, wie viele Elemente innerhalb einer Anfrage zurückgegeben werden.
Es gibt keine Standardmethode zur Implementierung dieser Funktion.
Auch in REST-APIs sehen Sie sie ggf. in HTTP-Headern oder Abfrageparametern - mit verschiedenen Namen und Verhaltensweisen.
In dieser Anwendung werden Sie Paginierung implementieren, indem Sie zunächst zwei weitere Argumente für die URL-Abfrage aktivieren: first und skip. first wählt die erste Variablenzahl von Elementen aus, während skip angibt, wie viele Elemente vom Anfang aus übersprungen werden sollen.
Wenn Sie zum Beispiel first = = 10 und skip = = 5 verwenden, werden die ersten 10 URLs abgerufen, jedoch 5 von ihnen übersprungen, sodass nur die verbleibenden 5 zurückgegeben werden.
Die Implementierung dieser Lösung ähnelt dem Hinzufügen eines Filters.
Öffnen Sie die Datei shortener / schema.py:
Ändern Sie in der Datei die Query-Klasse, indem Sie die beiden neuen Parameter in der Variable urls und der Methode resolve _ urls hinzufügen, wie im folgenden Code hervorgehoben:
Dieser Code verwendet die neu erstellten Parameter first und skip innerhalb der Methode resolve _ urls zum Filtern der Datenbankabfrage.
Um die Paginierung zu testen, geben Sie in der GraphiQL-Oberfläche unter http: / / localhost: 8000 / graphql folgende Abfrage ein:
Ihr URL-Shortener gibt die zweite in Ihrer Datenbank erstellte URL zurück:
Das bedeutet, dass die Paginierung funktioniert.
Probieren Sie die Funktion aus, indem Sie weitere URLs hinzufügen und verschiedene Sätze von first und skip testen.
Das gesamte GraphQL-Ökosystem wächst von Tag zu Tag und beruht auf einer aktiven Community. Bei Firmen wie GitHub und Facebook hat sich bereits gezeigt, dass es bereit für die Produktion ist; jetzt können Sie diese Technologie also für Ihre eigenen Projekte verwenden.
In diesem Tutorial haben Sie mit GraphQL, Python und Django unter Verwendung von Konzepten wie Abfragen und Mutationen einen URL-Shortener erstellt.
Darüber hinaus wissen Sie jetzt, wie Sie diese Technologien nutzen können, um mit dem Django-Web-Framework Webanwendungen zu erstellen.
Weitere Informationen über GraphQL und die hier verwendeten Tools finden Sie auf der GraphQL-Website sowie der Website mit der Graphene-Dokumentation.
Außerdem verfügt DigitalOcean über zusätzliche Tutorials für Python und Django, die Sie verwenden können, falls Sie mehr über diese beiden Lösungen erfahren möchten.
Installieren und Verwenden von PostgreSQL unter CentOS 8
4007
Relationale Datenbank-Managementsysteme (RDBMS) sind ein wichtiger Bestandteil zahlreicher Websites und Anwendungen.
PostgreSQL, auch als Postgres bekannt, ist ein relationales Datenbank-Managementsystem, das eine Implementierung von Structured Query Language (besser bekannt als SQL) bietet.
Es wird in vielen beliebten Projekten verwendet (ob groß oder klein), ist standardkonform und stellt viele fortgeschrittene Funktionen bereit, z. B. zuverlässige Transaktionen und Parallelität ohne Lesesperre.
Indem Sie diesem Leitfaden folgen, installieren Sie die neueste Version von PostgreSQL auf einem CentOS 8-Server.
PostgreSQL ist über das standardmäßige AppStream-Software-Repository von CentOS 8 verfügbar; es gibt mehrere Versionen, die Sie installieren können.
Sie können zwischen diesen Versionen wählen, indem Sie die entsprechende Sammlung von Paketen und Abhängigkeiten aktivieren, die zu der Version passt, die Sie installieren möchten. Einzelne Sammlungen werden dabei als Module Stream bezeichnet.
In DNF, dem standardmäßigen Paketmanager von CentOS 8, sind Module spezielle Sammlungen von RPM-Paketen, die zusammen eine größere Anwendung bilden.
Damit soll die Installation von Paketen und ihren Abhängigkeiten für Benutzer intuitiver gestaltet werden.
Listen Sie mit dem Befehl dnf die verfügbaren Streams für das postgresql-Modul auf:
In dieser Ausgabe sehen Sie, dass es im AppStream-Repository drei Versionen von PostgreSQL gibt: 9.6, 10 und 12. Der Stream, der Postgres Version 10 bereitstellt, ist der Standard, wie durch das darauffolgende [d] angegeben. Wenn Sie diese Version installieren möchten, können Sie einfach sudo dnf install postgresql-server ausführen und mit dem nächsten Schritt fortfahren.
Zwar wird die Version 10 weiter verwaltet, doch wird in diesem Tutorial Postgres Version 12, die neueste Version zum Zeitpunkt der Verfassung dieses Texts, installiert.
Um PostgreSQL Version 12 zu installieren, müssen Sie den Module Stream dieser Version aktivieren.
Wenn Sie einen Module Stream aktivieren, überschreiben Sie den standardmäßigen Stream und machen alle Pakete, die mit dem aktivierten Stream verbunden sind, im System verfügbar.
Beachten Sie, dass nur ein Stream eines bestimmten Moduls auf einmal in einem System aktiviert werden kann.
Um den Module Stream für Postgres Version 12 zu aktivieren, führen Sie den folgenden Befehl aus:
Wenn Sie dazu aufgefordert werden, drücken Sie y und dann ENTER, um zu bestätigen, dass Sie den Stream aktivieren möchten:
Nach dem Aktivieren des Module Stream von Version 12 können Sie das Paket postgresql-server installieren, um PostgreSQL 12 und alle seine Abhängigkeiten zu installieren:
Wenn die Eingabeaufforderung erscheint, bestätigen Sie die Installation, indem Sie y und dann ENTER drücken:
Nachdem die Software installiert ist, führen Sie nun einige Initialisierungsschritte aus, um einen neuen Datenbankcluster für PostgreSQL vorzubereiten.
Schritt 2 - Erstellen eines neuen PostgreSQL-Datenbankclusters
Sie müssen einen neuen PostgreSQL-Datenbankcluster einrichten, bevor Sie Tabellen erstellen und Daten in diese Tabellen laden können. Ein Datenbankcluster ist eine Sammlung von Datenbanken, die von einer einzigen Serverinstanz verwaltet wird.
Das Einrichten eines Datenbankclusters besteht aus der Erstellung der Verzeichnisse, in der die Datenbankdaten platziert werden. Dabei werden die gemeinsamen Katalogtabellen sowie die Datenbanken template1 und postgres erstellt.
template1 database ist eine Vorlage, die zur Erstellung neuer Datenbanken dient. Alles, was in template1 gespeichert wird (auch Objekte, die Sie selbst hinzufügen), wird bei der Erstellung in neuen Datenbanken abgelegt.
Die Datenbank postgres ist eine standardmäßige Datenbank, die für die Verwendung durch Benutzer, Dienstprogramme und Anwendungen anderer Anbieter entwickelt wurde.
Das im vorherigen Schritt installierte Postgres-Paket bietet ein praktisches Skript namens postgresql-setup, das bei der grundlegenden Verwaltung von Datenbankclustern hilft.
Um einen Datenbankcluster zu erstellen, führen Sie das Skript mit sudo und der Option --initdb aus:
Starten Sie nun mit systemctl den PostgreSQL-Dienst:
Verwenden Sie dann erneut systemctl, um den Dienst so zu aktivieren, dass er bei jedem Start des Servers gestartet wird:
Nachdem PostgreSQL nun ausgeführt wird, werden wir uns die Verwendung von Rollen ansehen, um zu erfahren, wie Postgres funktioniert und wie sich das System von ähnlichen Datenbank-Managementsystemen unterscheidet, die Sie möglicherweise in der Vergangenheit verwendet haben.
Schritt 3 - Verwenden von PostgreSQL-Rollen und -Datenbanken
PostgreSQL nutzt ein Konzept namens Rollen, um die Authentifizierung und Autorisierung von Clients zu handhaben.
Diese sind auf gewisse Weise herkömmlichen Konten im Unix-Stil ähnlich, Postgres unterscheidet jedoch nicht zwischen Benutzern und Gruppen und bevorzugt eher den flexiblen Begriff "Rolle".
Um PostgreSQL zu verwenden, können Sie sich bei diesem Konto anmelden.
Es gibt verschiedene Möglichkeiten, um dieses Konto für den Zugriff auf die PostgreSQL-Eingabeaufforderung zu verwenden.
Damit kehren Sie zur Linux-Eingabeaufforderung des Postgres-Kontos zurück.
Kehren Sie nun wie folgt zu Ihrem ursprünglichen Konto zurück:
Außerdem können Sie unter Einsatz von sudo Befehle für das Postgres-Konto direkt ausführen.
So wurden Sie z. B. im letzten Beispiel angewiesen, zur Postgres-Eingabeaufforderung zu gelangen, indem Sie zunächst zum Postgres-Benutzer wechseln und dann psql ausführen, um die Postgres-Eingabeaufforderung zu öffnen.
Alternativ könnten Sie dies in einem Schritt erledigen, indem Sie den einzelnen psql-Befehl als Postgres-Benutzer mit sudo ausführen, wie zum Beispiel:
Damit melden Sie sich direkt in Postgres ohne die zwischengeschaltete Bash-Shell an.
In diesem Schritt haben Sie das Postgres-Konto verwendet, um zur psql-Eingabeaufforderung zu gelangen.
Zahlreiche Anwendungsfälle erfordern jedoch mehr als eine Postgres-Rolle.
Lesen Sie weiter, um mehr über die Konfiguration neuer Rollen zu erfahren.
Schritt 4 - Erstellen einer neuen Rolle
Das Skript enthält einige Auswahlmöglichkeiten und führt basierend auf Ihren Antworten die erforderlichen Postgres-Befehle aus, um einen Benutzer im Sinne Ihrer Spezifikationen zu erstellen.
Erstellen Sie für dieses Tutorial eine Rolle namens sammy und weisen Sie Superuser-Berechtigungen zu, indem Sie bei Aufforderung y eingeben:
Sehen Sie sich die Optionen auf der man-Seite für createuser an:
Ihre Installation von Postgres hat jetzt eine neue Rolle, aber Sie haben noch keine Datenbanken hinzugefügt.
Schritt 5 - Erstellen einer neuen Datenbank
Wenn der von Ihnen im letzten Abschnitt erstellte Benutzer sammy heißt, wird diese Rolle demnach versuchen, standardmäßig eine Verbindung mit einer Datenbank namens sammy herzustellen.
Nachdem Sie nun eine neue Datenbank erstellt haben, melden Sie sich mit Ihrer neuen Rolle bei ihr an.
Schritt 6 - Öffnen einer Postgres-Eingabeaufforderung mit der neuen Rolle
Sobald dieses neue Konto verfügbar ist, können Sie entweder die Datenbank wechseln und sich dann mit ihr verbinden, indem Sie zunächst Folgendes eingeben:
Mit diesem Befehl werden Sie automatisch angemeldet.
Wenn sich Ihr Benutzer mit einer anderen Datenbank verbinden soll, können Sie dies erreichen, indem Sie das Flag -d einbinden und die Datenbank wie folgt angeben:
Damit wird die folgende Ausgabe angezeigt:
Nachdem Sie mit Ihrer Datenbank verbunden sind, können Sie nun versuchen, Tabellen zu erstellen und zu löschen.
Schritt 7 - Erstellen und Löschen von Tabellen
Diese Befehle geben der Tabelle einen Namen und definieren dann die Spalten, den Spaltentyp und die maximale Länge der Felddaten. Sie können wahlweise auch Beschränkungen für jede Spalte angeben.
Sie haben dieser Spalte auch die Einschränkung PRIMARY KEY (Primärschlüssel) gegeben, was bedeutet, dass die Werte einzigartig sein müssen und nicht Null sein dürfen.
Damit wird der Typ Serie angegeben, den Sie Ihrer Spalte equip _ id zugeordnet haben.
In diesem Schritt haben Sie eine Beispieltabelle erstellt.
Im nächsten Schritt versuchen Sie, Einträge in dieser Tabelle hinzuzufügen, abzufragen und zu löschen.
Schritt 8 - Hinzufügen, Abfragen und Löschen von Daten in einer Tabelle
Sie können zum Beispiel eine Rutsche und eine Schaukel hinzufügen, indem Sie die Tabelle aufrufen, der Sie diese Geräte hinzufügen möchten, die Spalten bezeichnen und dann Daten in die einzelnen Spalten eingeben:
Das liegt daran, dass dieser Wert automatisch generiert wird, wenn eine neue Zeile in der Tabelle erstellt wird.
Beachten Sie, dass Ihre Rutsche nicht mehr Teil der Tabelle ist.
Nachdem Sie nun Einträge in Ihrer Tabelle hinzugefügt und gelöscht haben, können Sie versuchen, Spalten hinzuzufügen und zu löschen.
Schritt 9 - Hinzufügen und Löschen von Spalten in einer Tabelle
Nach dem Erstellen einer Tabelle können Sie sie ändern, indem Sie Spalten hinzufügen oder entfernen.
Wenn Sie feststellen, dass Ihre Arbeitskräfte ein anderes Tool verwendet haben, um die Wartungsarbeiten zu erfassen, können Sie die Spalte löschen, indem Sie Folgendes eingeben:
Nachdem Sie nun Spalten hinzugefügt und gelöscht haben, können Sie im letzten Schritt versuchen, bestehende Daten zu aktualisieren.
Schritt 10 - Aktualisieren der Daten in einer Tabelle
Sie können eine Abfrage nach dem Schaukel-Eintrag vornehmen (damit erhalten Sie alle Einträge mit dem Wort Schaukel in der Tabelle) und die Farbe in red (rot) ändern:
Sie haben PostgreSQL nun auf Ihrem CentOS 8-Server eingerichtet.
Erfahren Sie mehr über die Ausführung von Abfragen in PostgreSQL
Installieren von Node.js unter CentOS 8
4006
In diesem Leitfaden stellen wir Ihnen drei verschiedene Möglichkeiten vor, um Node.js auf einem CentOS 8-Server zu installieren:
durch Verwendung von dnf zum Installieren des nodejs-Pakets aus dem standardmäßigen AppStream-Repository von CentOS
durch Installation von nvm, dem Node Version Manager, und dessen Verwendung zum Installieren und Verwalten von verschiedenen node-Versionen
durch Erstellen und Installieren von node aus dem Quellcode
Die meisten Benutzer sollten dnf verwenden, um die integrierten vorverpackten Versionen von Node zu installieren.
Wenn Sie Entwickler sind oder aus anderen Gründen verschiedene installierte Versionen von Node verwalten müssen, verwenden Sie die Methode nvm.
Das Erstellen aus dem Quellcode benötigen nur die wenigsten Benutzer.
Option 1 - Installieren von Node über das CentOS AppStream-Repository
Node.js ist im standardmäßigen AppStream-Repository von CentOS 8 verfügbar.
Es gibt verschiedene Versionen; Sie können die passende Version wählen, indem Sie den entsprechenden Module Stream aktivieren.
Listen Sie zunächst mit dem Befehl dnf die verfügbaren Streams für das nodejs-Modul auf:
Es sind zwei Streams verfügbar: 10 und 12. < ^ > [d] < ^ > gibt an, dass die Version 10 der Standard-Stream ist.
Wenn Sie Node.js 12 vorziehen, wechseln Sie jetzt den Module Stream:
Sie werden zur Bestätigung Ihrer Entscheidung aufgefordert.
Anschließend wird der Version 12-Stream aktiviert und wir können mit der Installation fortfahren.
Weitere Informationen zur Arbeit mit Module Streams finden Sie in der offiziellen CentOS AppStream-Dokumentation.
Installieren Sie das nodejs-Paket mit dnf:
Auch hier fordert Sie dnf dazu auf, die vorgenommenen Aktionen zu bestätigen.
Drücken Sie auf y und dann ENTER, woraufhin die Software installiert wird.
Ihre --version-Ausgabe wird sich davon unterscheiden, wenn Sie stattdessen Node.js 10 installiert haben.
< $> note Anmerkung: Beide verfügbaren Versionen von Node.js sind Releases mit langfristigem Support. Das bedeutet, dass sie ein längeres garantiertes Wartungsfenster haben.
Weitere Informationen zum Lebenszyklus finden Sie auf der offiziellen Node.js-Release-Seite.
Durch Installieren des nodejs-Pakets sollte auch das Dienstprogramm npm Node Package Manager als Abhängigkeit installiert werden.
Überprüfen Sie, ob es richtig installiert wurde:
Zu diesem Zeitpunkt haben Sie Node.js und npm mit den CentOS-Software-Repositorys erfolgreich installiert.
Der nächste Abschnitt zeigt, wie Sie dazu den Node Version Manager verwenden können.
Option 2 - Installieren von Node mit dem Node Version Manager
Um NVM auf Ihrem CentOS 8-Rechner zu installieren, besuchen Sie die GitHub-Seite des Projekts.
Um es zu verwenden, müssen Sie zunächst Ihre .bash _ profile-Datei aufrufen:
< $> note Anmerkung: Wenn Sie auch über die CentOS-Software-Repositorys eine Version von Node installiert haben, sehen Sie hier eine Zeile mit system - > v12.13.1 (oder einer anderen Versionsnummer).
Sie können mit nvm use system immer die Systemversion von Node aktivieren.
Option 3 - Installieren von Node aus dem Quellcode
Eine weitere Möglichkeit, Node.js zu installieren, besteht darin, den Quellcode herunterzuladen und selbst zu kompilieren.
Verwenden Sie dazu Ihren Webbrowser, um zur offiziellen Node.js-Download-Seite zu navigieren, klicken Sie mit der rechten Maustaste auf den Link Quellcode und klicken Sie dann auf Linkadresse kopieren oder eine ähnliche Option in Ihrem Browser.
Stellen Sie zurück in Ihrer SSH-Sitzung zunächst sicher, dass Sie sich in einem Verzeichnis befinden, in dem Sie schreiben können.
Wir verwenden das Stammverzeichnis des aktuellen Benutzers:
Geben Sie dann curl ein, fügen Sie den Link ein, den Sie von der Website kopiert haben, und folgen Sie mit | tar xz:
Dadurch wird das Dienstprogramm curl zum Herunterladen des Quellcodes verwendet, der dann direkt an das Dienstprogramm tar weitergeleitet und im aktuellen Verzeichnis extrahiert wird.
Rufen Sie das neu erstellte Quellverzeichnis auf:
Es gibt einige Pakete, die wir zum Kompilieren des Codes aus den CentOS-Repositorys herunterladen müssen.
Verwenden Sie dnf, um diese Pakete nun zu installieren:
Sie werden zur Bestätigung der Installation aufgefordert.
Geben Sie dazu y ein und drücken Sie ENTER.
Jetzt können wir die Software konfigurieren und kompilieren:
Die Kompilierung dauert eine Weile (rund 30 Minuten bei einem Server mit vier Kernen).
Wir haben die Option -j4 verwendet, um vier parallele Kompilierungsprozesse auszuführen.
Sie können diese Option auslassen oder die Zahl aktualisieren, je nach der Anzahl der verfügbaren Prozessorkerne.
Wenn die Kompilierung abgeschlossen ist, können Sie die Software durch folgende Eingabe in Ihrem System installieren:
Um zu prüfen, ob die Installation erfolgreich war, weisen Sie Node an, seine Versionsnummer anzuzeigen:
Wenn Sie die korrekte Versionsnummer sehen, wurde die Installation erfolgreich abgeschlossen.
Standardmäßig installiert Node auch eine kompatible Version von npm, sodass diese ebenfalls verfügbar sein sollte.
In diesem Tutorial haben wir gelernt, wie wir Node.js mit dem CentOS AppStream-Software-Repository durch Verwendung des Node Version Manager und Kompilieren des Quellcodes installieren können.
Wenn Sie weitere Informationen zum Programmieren in JavaScript wünschen, lesen Sie unsere verwandte Tutorial-Reihe:
Kodieren in JavaScript: eine umfassende Übersicht über die JavaScript-Sprache, die sowohl für den Browser als auch für Node.js gilt.
Kodieren in Node.js: eine Reihe von Übungen, die die Grundlagen der Verwendung von Node.js vermitteln.
< $> note Anmerkung: Wenn Ihre Server auf DigitalOcean ausgeführt werden, können Sie optional DigitalOcean Cloud Firewalls anstelle von firewalld verwenden.
Server zum Laufen bringen
3995
Server zum Laufen bringen: Ein praktischer Leitfaden für Systemadministrator eBook im EPUB-Format
Server zum Laufen bringen: Ein praktischer Leitfaden für Systemadministrator eBook im PDF-Format < $>
Dieses Buch behandelt praktische Fähigkeiten von Systemadministratoren, gängige Architekturen, denen Sie begegnen werden, und bewährte Verfahren, die für die Automatisierung und den Betrieb von Systemen jeder Größenordnung gelten, von einem Laptop oder Server bis hin zu 1.000 oder mehr.
Es soll Ihnen helfen, sich innerhalb des Fachgebiets zu orientieren, und ermutigt Sie hoffentlich dazu, mehr über Systemadministration zu lernen.
Einführende Themen
LAMP- und LEMP-Technologie-Stapel
Sicherung Ihrer Server
Automatisierung mit Ansible
Versionskontrolle und kontinuierliche Integration
Suchen Sie sich in diesem Buch Themen aus, die Sie interessieren, und erkunden Sie sie anhand dieser Kapitel als Leitfaden.
Beim Durcharbeiten dieses Buches werden Sie mit einer Vielzahl von Technologien, technischen Begriffen und konzeptionellen Ansätzen zur Verwaltung von Linux-Servern konfrontiert.
Sie können jedes Kapitel oder jeden Abschnitt in Ihrem eigenen Tempo und in der von Ihnen gewählten Reihenfolge durcharbeiten.
Wenn Sie zusätzliche Sysadmin-Ressourcen benötigen, die Ihnen den Einstieg erleichtern, und wenn Sie an der DigitalOcean-Community anderer Entwickler und Administratoren teilnehmen möchten, sehen Sie sich unsere wachsende Bibliothek mit Tutorials, Fragen und Projekten mit dem Tag Erste Schritte an.
Ausführen serverloser Funktionen mit OpenFaaS auf DigitalOcean Kubernetes
4025
Üblicherweise erfordert das Hosting einer Softwareanwendung im Internet Infrastrukturmanagement, Planung und Überwachung für ein monolithisches System.
Im Gegensatz zu diesem traditionellen Ansatz gliedert die serverless Architektur (auch bekannt als Function as a Service oder FaaS) Ihre Anwendung in Funktionen auf.
Bei diesen Funktionen handelt es sich um zustandslose, in sich geschlossene, ereignisausgelöste, funktional vollständige Entitäten, die über von Ihnen verwaltete APIs kommunizieren, statt über die zugrunde liegende Hardware und die explizite Bereitstellung der Infrastruktur.
Die Funktionen sind vom Design her skalierbar, übertragbar, schneller einzurichten und einfacher zu testen als gewöhnliche Anwendungen.
Damit die serverlose Architektur im Prinzip funktionieren kann, ist eine plattformunabhängige Art der Paketierung und Orchestrierung von Funktionen erforderlich.
OpenFaaS ist ein Open-Source-Framework zur Implementierung der serverlosen Architektur auf Kubernetes, wobei Docker-Container zur Speicherung und Ausführung von Funktionen verwendet werden.
Es ermöglicht die Paketierung eines beliebigen Programms als Container und die Verwaltung einer Funktion über die Befehlszeile oder die integrierte Web-Benutzeroberfläche.
OpenFaaS verfügt über eine ausgezeichnete Unterstützung für Metriken und bietet eine automatische Skalierung der Funktionen bei steigender Nachfrage.
In diesem Tutorial werden Sie OpenFaaS auf Ihrem DigitalOcean Kubernetes-Cluster in Ihrer Domäne einsetzen und es mit kostenlosen Let "s Encrypt TLS-Zertifikaten sichern.
Sie werden auch die Web-Benutzeroberfläche erkunden und bestehende und neue Funktionen mithilfe von faas-cli, dem offiziellen Befehlszeilen-Tool, implementieren.
Am Ende verfügen Sie über ein flexibles System für die Bereitstellung serverloser Funktionen.
Der Cluster muss über mindestens 8 GB RAM und 4 für OpenFaaS verfügbare CPU-Cores verfügen (bei stärkerer Nutzung sind mehr erforderlich).
Auf Ihrem lokalen Rechner installiertes Docker.
Folgen Sie den Schritten 1 und 2 für Ihre Distribution in Installieren von Docker.
Einen Account bei Docker Hub zur Speicherung von Docker-Images, die Sie in diesem Tutorial erstellen werden.
Auf Ihrem lokalen Rechner installiertes faas-cli, das offizielle CLI-Tool zur Verwaltung von OpenFaaS.
Anweisungen für mehrere Plattformen finden Sie in den offiziellen Dokumentationen.
Auf Ihrem lokalen Rechner installierten Helm-Paketmanager.
Schließen Sie dazu Schritt 1 ab und fügen Sie das Repository stable von Schritt 2 des Tutorials Installieren von Software auf Kubernetes-Clustern mit dem Helm 3-Paketmanager hinzu.
In Ihrem Cluster mit Helm installierten Nginx Ingress Controller und Cert Manager, um OpenFaaS mit Ingress-Ressourcen freizugeben.
Ein vollständig registrierter Domänenname zum Hosten von OpenFaaS, der auf den vom Nginx Ingress verwendeten Load Balancer verweist.
Dieses Tutorial verwendet durchgehend < ^ > openfaas.your _ domain < ^ >.
< $> note Anmerkung: Der von Ihnen in diesem Tutorial verwendete Domänenname muss sich von dem Domänennamen unterscheiden, der in dem vorbereitenden Tutorial Einrichten eines Nginx Ingress auf DigitalOcean Kubernetes verwendet wird.
Schritt 1 - Installieren von OpenFaaS mit Helm
In diesem Schritt installieren Sie OpenFaaS mit Helm auf Ihrem Kubernetes-Cluster und stellen es in Ihrer Domäne zur Verfügung.
Als Teil der Voraussetzung des Nginx Ingress Controllers haben Sie Beispieldienste und einen Ingress erstellt.
Sie werden sie in diesem Tutorial nicht benötigen, sodass Sie diese durch Ausführen der folgenden Befehle löschen können:
Da Sie Funktionen als Kubernetes-Objekte bereitstellen werden, ist es hilfreich, sie und OpenFaaS selbst in getrennten Namespaces in Ihrem Cluster zu speichern.
Der Namespace von OpenFaaS wird als openfaas bezeichnet und der Namespace der Funktionen ist openfaas-fn.
Erstellen Sie diese in Ihrem Cluster, indem Sie den folgenden Befehl ausführen:
Als Nächstes müssen Sie das OpenFaaS Helm-Repository hinzufügen, das das OpenFaaS-Chart hostet.
Helm wird die folgende Ausgabe anzeigen:
Aktualisieren Sie den Chart-Cache von Helm:
Bevor Sie OpenFaaS installieren, müssen Sie Chart-Parameter anpassen.
Sie werden diese auf Ihrem lokalen Rechner in einer Datei namens values.yaml speichern.
Erstellen und öffnen Sie die Datei mit Ihrem Texteditor:
Zuerst geben Sie den Namespace an, in dem Funktionen gespeichert werden, indem Sie openfaas-fn der Variablen functionNamespace zuweisen.
Indem Sie generateBasicAuth auf true setzen, weisen Sie Helm an, eine obligatorische Authentifizierung beim Zugriff auf die OpenFaaS Web-Benutzeroberfläche einzurichten und eine Kombination von Admin-Benutzername und Passwort für Sie zu erzeugen.
Dann aktivieren Sie die Ingress-Erstellung und konfigurieren sie weiter, um den Nginx Ingress-Controller zur Bereitstellung des OpenFaaS-Dienstes gateway in Ihrer Domäne zu verwenden.
Denken Sie daran, < ^ > openfaas.your _ domain < ^ > durch Ihre gewünschte Domäne aus den Voraussetzungen zu ersetzen.
Installieren Sie zum Schluss OpenFaaS in den Namespace openfaas mit den angepassten Werten:
Die Ausgabe zeigt, dass die Installation erfolgreich war.
Führen Sie den folgenden Befehl aus, um das Passwort für den Account admin zu ermitteln:
Das entschlüsselte Passwort wird in die Ausgabe und gleichzeitig mit tee in eine Datei namens openfaas-password.txt geschrieben.
Notieren Sie sich die Ausgabe, bei der es sich um Ihr OpenFaaS-Passwort für das Konto admin handelt.
Durch Ausführen des folgenden Befehls können Sie beobachten, wie OpenFaaS-Container verfügbar werden:
Wenn alle aufgelisteten Bereitstellungen ready sind, geben Sie zum Beenden STRG + C ein.
Sie können nun in Ihrem Webbrowser zu der angegebenen Domäne navigieren.
Geben Sie bei Aufforderung admin als Benutzername und das zugehörige Passwort ein.
Sie sehen dann die Web-Benutzeroberfläche von OpenFaaS:
OpenFaaS - Leeres Bedienfeld
Sie haben OpenFaaS erfolgreich installiert und das Bedienfeld in Ihrer Domäne verfügbar gemacht.
Als Nächstes sichern Sie es mit kostenlosen TLS-Zertifikaten von Let "s Encrypt.
Schritt 2 - Aktivieren von TLS für Ihre Domäne
In diesem Schritt sichern Sie Ihre ungeschützte Domäne mit Let "s Encrypt-Zertifikaten, die vom Zertifikatsmanager bereitgestellt werden.
Dazu müssen Sie die Ingress-Konfiguration in values.yaml bearbeiten.
Fügen Sie die hervorgehobenen Zeilen hinzu:
Der Block tls definiert, in welchem Secret die Zertifikate für ihre (unter hosts aufgelisteten) Seiten ihre Zertifikate speichern, die der ClusterIssuer letsencrypt-prod ausstellt.
In der Regel muss das angegebene Secret für jeden Ingress in Ihrem Cluster unterschiedlich sein.
Vergessen Sie nicht, < ^ > openfaas.your _ domain < ^ > ​ ​ ​ durch Ihre gewünschte Domäne zu ersetzen. Speichern und schließen Sie danach die Datei.
Sie müssen einige Minuten warten, bis die Server von Let "s Encrypt ein Zertifikat für Ihre Domäne ausstellen.
In der Zwischenzeit können Sie den Fortschritt verfolgen, indem Sie sich die Ausgabe des folgenden Befehls ansehen:
Das Ende der Ausgabe wird ähnlich wie dieses aussehen:
Wenn die letzte Ausgabezeile Certificate issued successfully lautet, können Sie die Ausgabe durch Drücken von STRG + C beenden.
Sie sehen das Vorhängeschloss links neben der Adressleiste in Ihrem Browser, das anzeigt, dass Ihre Verbindung sicher ist.
Sie haben Ihre OpenFaaS-Domäne mit kostenlosen TLS-Zertifikaten von Let "s Encrypt gesichert.
Jetzt werden Sie die Web-Benutzeroberfläche verwenden und Funktionen von dort aus verwalten.
Schritt 3 - Bereitstellen von Funktionen über die Web-Benutzeroberfläche
In diesem Abschnitt werden Sie die Web-Benutzeroberfäche von OpenFaaS erkunden und dann Funktionen von dort bereitstellen, verwalten und aufrufen.
Die Web-Benutzeroberfläche von OpenFaaS verfügt über zwei Hauptteile: Auf der linken Seite eine Spalte, in der die bereitgestellten Funktionen aufgelistet werden, und das zentrale Bedienfeld, in dem Sie detaillierte Informationen über eine ausgewählte Funktion sehen und mit dieser interagieren können.
Um eine neue Funktion bereitzustellen, klicken Sie oben links unter dem OpenFaaS-Logo auf die Schaltfläche Deploy New Function.
Es wird ein Dialogfeld angezeigt, in dem Sie aufgefordert werden, eine Funktion auszuwählen:
OpenFaaS - Dialogfeld Bereitstellen einer neuen Funktion
Die Registerkarte FROM STORE listet vorgefertigte Funktionen aus dem offiziellen OpenFaaS Function Store auf, die Sie sofort bereitstellen können.
Jede Funktion wird mit einer kurzen Beschreibung angezeigt und Sie können das Link-Symbol rechts neben einer Funktion auswählen, um einen Blick auf ihren Quellcode zu werfen.
Um eine Store-Funktion aus der Liste bereitzustellen, wählen Sie diese aus und klicken Sie dann auf die Schaltfläche DEPLOY.
Sie können auch Ihre eigene Funktion bereitstellen, indem Sie zu der Registerkarte CUSTOM wechseln:
OpenFaaS - Bereitstellen einer benutzerdefinierten Funktion
Hier müssen Sie ein Docker-Image Ihrer Funktion angeben, das speziell für OpenFaaS konfiguriert und in einer Docker-Registrierung (wie z. B. Docker Hub) verfügbar ist.
In diesem Schritt stellen Sie eine vorgefertigte Funktion aus dem OpenFaaS Store bereit. Im nächsten Schritt erstellen Sie benutzerdefinierte Funktionen und stellen diese im Docker Hub bereit.
Sie stellen die Funktion NodeInfo bereit, die Informationen über den Rechner zurückgibt, auf dem sie bereitgestellt wird, wie CPU-Architektur, Anzahl der Cores, gesamt verfügbarer RAM-Speicher und Betriebszeit (in Sekunden).
Wählen Sie aus der Liste der Store-Funktionen NodeInfo und klicken Sie auf DEPLOY.
Es wird bald in der Liste der bereitgestellten Funktionen angezeigt.
OpenFaaS - NodeInfo bereitgestellt
Wählen Sie sie aus. Im mittleren Teil des Bildschirms sehen sie grundlegende Informationen über die bereitgestellte Funktion.
OpenFaaS - Info über bereitgestellte Funktion
Der Status der Funktion wird in Echtzeit aktualisiert und sollte schnell zu Ready wechseln.
Wenn über längere Zeit Not Ready angezeigt wird, ist es sehr wahrscheinlich, dass Ihrem Cluster die Ressourcen fehlen, um einen neuen Pod zu akzeptieren.
Im Abschnitt Größenänderung von Droplets finden Sie Informationen darüber, wie Sie dies beheben können.
Sobald Ready erscheint, ist die bereitgestellte Funktion unter der angegebenen URL zugänglich.
Um sie zu testen, können Sie in Ihrem Browser zu der URL navigieren oder sie über das Feld Invoke function aufrufen, das sich unter der Funktionsinfo befindet.
OpenFaaS - Bereitgestellte Funktion aufrufen
Sie können zwischen Text, JSON und Download wählen, um die Art der Antwort anzugeben, die Sie erwarten.
Wenn Sie möchten, dass die Abfrage ein POST anstelle von GET ist, können Sie im Feld Request body Abfragedaten bereitstellen.
Um die Funktion nodeinfo aufzurufen, klicken Sie auf die Schaltfläche INVOKE.
OpenFaaS erstellt und führt eine HTTP-Abfrage gemäß den gewählten Optionen aus und füllt die Antwortfelder mit den empfangenen Daten aus.
OpenFaaS - Antwort der Funktion nodeinfo
Der Antwortstatus ist HTTP 200 OK. Das bedeutet, dass die Abfrage erfolgreich ausgeführt wurde.
Response body enthält Systeminformationen, die die Funktion NodeInfo sammelt, d. h. sie ist zugänglich und funktioniert korrekt.
Um eine Funktion zu löschen, wählen Sie sie aus der Liste aus und klicken Sie auf das Mülleimer-Symbol in der rechten oberen Ecke der Seite.
Wenn Sie dazu aufgefordert werden, klicken Sie zur Bestätigung auf OK.
Der Status der Funktion wechselt zu Not Ready (was bedeutet, dass sie aus dem Cluster entfernt wird) und die Funktion wird bald nicht mehr auf der Benutzeroberfläche vorhanden sein.
In diesem Schritt haben Sie die Web-Benutzeroberfläche von OpenFaaS verwendet sowie Funktionen von dort bereitgestellt und verwaltet. Sie werden nun sehen, wie Sie OpenFaaS-Funktionen über die Befehlszeile bereitstellen und verwalten können.
Schritt 4 - Verwalten von Funktionen mit faas-cli
In diesem Abschnitt konfigurieren Sie die faas-cli für die Arbeit mit Ihrem Cluster.
Anschließend werden Sie Ihre vorhandenen Funktionen über die Befehlszeile bereitstellen und verwalten.
Um zu vermeiden, dass Sie Ihre OpenFaaS-Domäne bei jeder Ausführung der faas-cli angeben müssen, speichern Sie sie in einer Umgebungsvariable namens OPENFAAS _ URL, deren Wert die faas-cli automatisch abholt und während der Ausführung verwendet.
Öffnen Sie .bash _ profile in Ihrem Stammverzeichnis zur Bearbeitung:
Denken Sie daran, < ^ > openfaas.your _ domain < ^ > ​ ​ ​ durch Ihre gewünschte Domäne zu ersetzen. Speichern und schließen Sie danach die Datei.
Um sich nicht erneut anmelden zu müssen, werten Sie die Datei manuell aus:
Stellen Sie nun sicher, dass Sie faas-cli auf Ihrem lokalen Rechner installiert haben.
Falls Sie es noch nicht installiert haben, folgen Sie den Anleitungen in den offiziellen Dokumentationen.
Richten Sie dann Ihre Anmeldedaten ein, indem Sie den folgenden Befehl ausführen:
Um eine Funktion aus dem Store bereitzustellen, führen Sie den folgenden Befehl aus:
Sie können die Bereitstellung von nodeinfo versuchen, indem Sie Folgendes ausführen:
Um bereitgestellte Funktionen auflisten zu können, führen Sie faas list aus:
Ihre vorhandenen Funktionen werden angezeigt:
Um detaillierte Informationen über eine bereitgestellte Funktion zu erhalten, verwenden Sie faas describe:
Die Ausgabe wird ähnlich dieser sein:
Sie können eine Funktion mit faas invoke aufrufen:
Dann können Sie einen Request Body bereitstellen.
Wenn Sie dies tun, ist die Methode POST anstelle von GET.
Wenn Sie die Dateneingabe abgeschlossen haben oder die Anforderung GET sein soll, drücken Sie STRG + D. faas-cli führt dann die abgeleitete Abfrage aus und gibt die Antwort aus, ähnlich wie bei der Web-Benutzeroberfläche.
Zum Löschen einer Funktion führen Sie faas remove aus:
Führen Sie faas list erneut aus, um zu sehen, dass nodeinfo entfernt wurde.
In diesem Schritt haben Sie Funktionen in Ihrem Cluster von der Befehlszeile aus mit Hilfe von faas-cli bereitgestellt, aufgelistet, aufgerufen und entfernt.
im nächsten Schritt erstellen Sie Ihre eigene Funktion und stellen sie in Ihrem Cluster bereit.
Schritt 5 - Erstellen und Bereitstellen einer neuen Funktion
Erstellen Sie nun eine Beispielfunktion von Node.JS unter Verwendung von faas-cli und stellen Sie diese auf Ihrem Cluster bereit.
Die daraus resultierende Funktion wird als Docker-Container gepackt und auf Docker Hub veröffentlicht.
Um Container veröffentlichen zu können, müssen Sie sich anmelden, indem Sie den folgenden Befehl ausführen:
Geben Sie Ihren Docker Hub-Benutzernamen und Ihr Passwort ein, wenn Sie zum Beenden des Anmeldevorgangs aufgefordert werden.
Speichern Sie die Beispielfunktion Node.JS in einem Ordner namens sample-js-function.
Füllen Sie das Verzeichnis mit der Vorlage einer JS-Funktion, indem Sie den folgenden Befehl ausführen:
Wie in der Ausgabe geschrieben, befindet sich der Code für die Funktion selbst im Ordner sample-js, während sich die OpenFaaS-Konfiguration für die Funktion in der Datei sample-js.yaml befindet.
Unter dem Verzeichnis sample-js (das einem regulären Node.JS-Projekt ähnelt) befinden sich zwei Dateien, handler.js und package.json.
handler.js enthält tatsächlichen JS-Code, der eine Antwort zurückgibt, wenn die Funktion aufgerufen wird.
Der Inhalt des Handlers sieht wie folgt aus:
Er exportiert eine Lambda-Funktion mit zwei Parametern, einem Kontext mit Abfragedaten und einem Callback, mit dem Sie Antwortdaten zurückübergeben können, anstatt sie nur zurückzugeben.
Ändern Sie die hervorgehobene Zeile wie folgt:
Diese OpenFaaS-Funktion wird, wenn sie aufgerufen wird, Hello Sammy!
in die Antwort schreiben.
Öffnen Sie als Nächstes die Konfigurationsdatei zur Bearbeitung:
Sie wird wie folgt aussehen:
Für den provider gibt sie openfaas und ein Standard-Gateway an.
Dann definiert sie die Funktion sample-js, gibt ihre Sprache (node), ihren Handler und den Namen des Docker-Image an, den Sie ändern müssen, um den Benutzernamen Ihres Docker Hub-Accounts einzufügen:
Erstellen Sie dann das Docker-Image, verschieben Sie es zum Docker-Hub und stellen Sie es in Ihrem Cluster bereit, und zwar gleichzeitig, indem Sie den folgenden Befehl ausführen:
Es wird eine umfangreiche Ausgabe (hauptsächlich von Docker) erfolgen, die so endet:
Rufen Sie Ihre neu bereitgestellte Funktion auf, um sicherzustellen, dass sie funktioniert:
Drücken Sie STRG + D. Sie sehen die folgende Ausgabe:
Das bedeutet, dass die Funktion gepackt und korrekt bereitgestellt wurde.
Sie können die Funktion entfernen, indem Sie Folgendes ausführen:
Sie haben nun erfolgreich eine benutzerdefinierte Node.JS-Funktion auf Ihrer OpenFaaS-Instanz in Ihrem Cluster erstellt und bereitgestellt.
Sie haben OpenFaaS auf Ihrem DigitalOcean Kubernetes-Cluster bereitgestellt und können nun sowohl vorgefertigte als auch benutzerdefinierte Funktionen bereitstellen.
Jetzt sind Sie in der Lage, die Function-as-a-Service-Architektur zu implementieren, was die Ressourcennutzung erhöhen und Leistungsverbesserungen für Ihre Anwendungen bewirken kann.
Wenn Sie mehr über erweiterte OpenFaaS-Funktionen erfahren möchten, wie z. B. die automatische Skalierung für Ihre eingesetzten Funktionen und die Überwachung ihrer Leistung, besuchen Sie die offizielle Dokumentation.
Erstellen eines Webservers in Node.js mit dem HTTP-Modul
4024
Wenn Sie eine Webseite in Ihrem Browser anzeigen, stellen Sie eine Anfrage über das Internet an einen anderen Computer, der Ihnen dann die Webseite als Antwort bereitstellt.
Dieser Computer, mit dem Sie über das Internet kommunizieren, ist ein Webserver.
Ein Webserver empfängt HTTP-Anfragen von einem Client (wie Ihrem Browser) und stellt eine HTTP-Antwort bereit, z. B. eine HTML-Seite oder JSON aus einer API.
Am Zurückgeben einer Webseite durch einen Server sind verschiedene Softwareanwendungen beteiligt.
Diese Software fällt im Allgemeinen in zwei Kategorien: Frontend und Backend.
Frontend-Code befasst sich damit, wie Inhalte dargestellt werden, z. B. die Farbe einer Navigationsleiste und der Textstil.
Backend-Code befasst sich damit, wie Daten ausgetauscht, verarbeitet und gespeichert werden.
Code, der Netzwerkanfragen von Ihrem Browser behandelt oder mit der Datenbank kommuniziert, wird in erster Linie mit Backend-Code verwaltet.
Mit Node.js können Entwickler JavaScript zum Schreiben von Backend-Code nutzen, auch wenn die Umgebung im Browser traditionell verwendet wurde, um Frontend-Code zu schreiben.
Wenn Frontend und Backend so nah beieinander sind, verringert sich der Aufwand bei der Einrichtung eines Webservers. Das ist ein Grund dafür, warum Node.js eine beliebte Wahl zum Schreiben von Backend-Code ist.
In diesem Tutorial lernen Sie, wie Sie Webserver mit dem in Node.js enthaltenen http-Modul einrichten können.
Sie werden Webserver erstellen, die JSON-Daten, CSV-Dateien und HTML-Webseiten zurückgeben können.
Stellen Sie sicher, dass Node.js auf Ihrem Entwicklungscomputer installiert ist.
Dieses Tutorial verwendet die Node.js-Version 10.19.0.
Die Node.js-Plattform unterstützt eine standardmäßige Erstellung von Webservern.
Bevor Sie beginnen, sollten Sie sich mit den Grundlagen von Node.js vertraut machen.
Sie können damit starten, indem Sie unseren Leitfaden zu Schreiben und Ausführen Ihres ersten Programms in Node.js lesen.
Außerdem verwenden wir asynchrone Programmierung für einen unserer Abschnitte.
Wenn Sie mit der asynchronen Programmierung in Node.js oder dem fs-Modul für die Interaktion mit Dateien noch nicht vertraut sind, können Sie in unserem Artikel mehr erfahren über das Schreiben von asynchronem Code in Node.js.
Schritt 1 - Einrichten eines grundlegenden HTTP-Servers
Beginnen wir zunächst mit der Einrichtung eines Servers, der dem Benutzer Klartext zurückgibt.
Damit werden die zur Einrichtung eines Servers erforderlichen Schlüsselkonzepte behandelt, was als Grundlage für die Rückgabe komplexer Datenformate wie JSON dient.
Zuerst müssen wir eine zugängliche Codierungsumgebung einrichten, um unsere Übungen sowie andere Aufgaben in dem Artikel ausführen zu können.
Erstellen Sie im Terminal einen Ordner namens < ^ > first-servers < ^ >:
Erstellen Sie nun die Datei, die den Code beinhalten wird:
Öffnen Sie die Datei in einem Texteditor.
Wir verwenden nano, da es im Terminal verfügbar ist:
Wir starten mit dem Laden des http-Moduls, das bei allen Node.js-Installationen Standard ist.
Fügen Sie die folgende Zeile zu < ^ > hello.js < ^ > hinzu:
Das Modul http enthält die Funktion zur Erstellung des Servers, wie wir später sehen werden.
Wenn Sie mehr über Module in Node.js erfahren möchten, lesen Sie unseren Artikel Erstellen eines Node.js-Moduls.
Unser nächster Schritt besteht darin, zwei Konstanten zu definieren, also den Host und den Port, an die unser Server gebunden ist:
Wie bereits erwähnt, akzeptieren Webserver Anfragen von Browsern und anderen Clients.
Wir können mit einem Webserver interagieren, indem wir einen Domänennamen eingeben, der über einen DNS-Server in eine IP-Adresse übersetzt wird.
Eine IP-Adresse ist eine eindeutige Sequenz von Zahlen, die einen Rechner in einem Netzwerk wie dem Internet identifiziert.
Weitere Informationen zu Domänennamenkonzepten finden Sie in unserem Artikel Eine Einführung in DNS-Begriffe, -Komponenten und -Konzepte.
Der Wert localhost ist eine spezielle private Adresse, die Computer zum Verweisen auf sich selbst verwenden.
Es handelt sich dabei typischerweise um das Äquivalent der internen IP-Adresse 127.0.0.1 und ist nur für den lokalen Computer verfügbar, nicht für die lokalen Netzwerke, mit denen wir verbunden sind, oder mit dem Internet.
Der Port ist eine Zahl, die Server als Endpunkt oder "Tür" zu unserer IP-Adresse verwenden.
In unserem Beispiel verwenden wir Port 8000 für unseren Webserver.
Ports 8080 und 8000 dienen typischerweise als Standardports für die Entwicklung, und meist nutzen Entwickler sie anstelle anderer Ports für HTTP-Server.
Wenn wir unseren Server an diesen Host und Port binden, können wir unseren Server erreichen, indem wir in einem lokalen Browser http: / / localhost: 8000 aufrufen.
Lassen Sie uns eine spezielle Funktion hinzufügen, die wir in Node.js einen Request Listener nennen.
Diese Funktion dient dazu, eine eingehende HTTP-Anfrage zu bearbeiten und eine HTTP-Antwort zurückzugeben.
Diese Funktion muss zwei Argumente aufweisen: ein Anfrageobjekt und ein Antwortobjekt.
Das Anfrageobjekt erfasst alle eingehenden Daten der HTTP-Anforderung.
Das Antwortobjekt dient der Rückgabe von HTTP-Antworten an den Server.
Wir möchten, dass unser erster Server folgende Nachricht zurückgibt, wenn jemand darauf zugreift: "My first server!".
Fügen wir als Nächstes diese Funktion hinzu:
Die Funktion würde normalerweise nach dem benannt, was sie tut.
Wenn wir zum Beispiel eine Request Listener-Funktion erstellen, um eine Liste von Büchern zurückzugeben, würden wir sie wahrscheinlich listBooks () nennen.
Da dies ein Beispielfall ist, verwenden wir den allgemeinen Namen requestListener.
Alle Request Listener-Funktionen in Node.js akzeptieren zwei Argumente: req und res (wir können sie anders nennen, wenn wir möchten).
Die HTTP-Anfrage, die der Benutzer sendet, wird in einem Anfrageobjekt erfasst. Das entspricht dem ersten Argument req.
Die HTTP-Antwort, die wir an den Benutzer zurückgeben, wird gebildet, indem wir mit dem Antwortobjekt im zweiten Argument res interagieren.
Die erste Zeile res.writeHead (200); legt den HTTP-Statuscode der Antwort fest.
HTTP-Statuscodes geben an, wie gut eine HTTP-Anfrage vom Server bearbeitet wurde.
In diesem Fall entspricht der Statuscode 200 "OK".
Wenn Sie mehr über die verschiedenen HTTP-Codes, die Ihre Webserver zurückgeben können, und ihre jeweilige Bedeutung erfahren möchten, ist unser Leitfaden Fehlerbehebung für gängige HTTP-Fehlercodes ein perfekter Ausgangspunkt.
Die nächste Zeile der Funktion res.end (" My fist server! ")
; schreibt die HTTP-Antwort zurück an den Client, der sie angefordert hat. Diese Funktion gibt alle Daten zurück, die der Server zurückgeben muss.
In diesem Fall werden Textdaten zurückgegeben.
Abschließend können wir unseren Server erstellen und unseren Request Listener verwenden:
Speichern und schließen Sie nano, indem Sie Strg + X drücken.
In der ersten Zeile erstellen wir ein neues Server-Objekt über die Funktion createServer () des http-Moduls.
Dieser Server akzeptiert HTTP-Anfragen und übergibt sie an unsere Funktion requestListener ().
Nachdem wir unseren Server erstellt haben, müssen wir ihn nun an eine Netzwerkadresse binden.
Das tun wir mit der Methode server.listen ().
Sie akzeptiert drei Argumente: Port, Host und eine Rückruffunktion, die ausgelöst wird, sobald der Server mit dem Lauschen beginnt.
Alle diese Argumente sind optional; es ist aber eine gute Idee, explizit zu bestätigen, welchen Port und Host ein Webserver verwenden soll.
Wenn Sie Webserver in verschiedenen Umgebungen bereitstellen, müssen Sie den Port und den Host kennen, bei denen sie ausgeführt werden, um den Lastausgleich oder einen DNS-Alias einzurichten.
Die Rückruffunktion protokolliert eine Nachricht an unsere Konsole, damit wir wissen, wann der Server mit dem Lauschen nach Verbindungen begonnen hat.
< $> note Anmerkung: Obwohl requestListener () das Objekt req nicht nutzt, muss es dennoch das erste Argument der Funktion sein.
Mit weniger als fünfzehn Codezeilen verfügen wir nun über einen Webserver.
Lassen Sie uns das in der Praxis ansehen und durchgängig testen, indem wir folgendes Programm ausführen:
In der Konsole sehen wir diese Ausgabe:
Beachten Sie, dass die Eingabeaufforderung verschwindet.
Das liegt daran, dass ein Node.js-Server ein lang laufender Vorgang ist.
Der Server wird nur dann beendet, wenn ein Fehler auftritt, der zum Absturz des Servers führt, oder wenn wir den Node.js-Prozess anhalten, der den Server ausführt.
In einem separaten Terminalfenster kommunizieren wir mit dem Server über cURL, ein CLI-Tool zur Übertragung von Daten an und aus einem Netzwerk.
Geben Sie den Befehl ein, um eine HTTP-GET-Anfrage an unseren laufenden Server zu stellen:
Wenn wir ENTER drücken, zeigt unser Terminal die folgende Ausgabe:
Wir haben nun einen Server eingerichtet und unsere erste Serverantwort erhalten.
Lassen Sie uns genau ansehen, was passiert ist, als wir unseren Server getestet haben.
Mit cURL haben wir eine GET-Anfrage an den Server bei http: / / localhost: 8000 gesendet.
Unser Node.js-Server hat nach Verbindungen von dieser Adresse gelauscht.
Der Server hat diese Anfrage an die Funktion requestListener () übergeben.
Die Funktion hat Textdaten mit dem Statuscode 200 zurückgegeben. Der Server hat diese Antwort dann an cURL zurückgesendet, wodurch die Nachricht in unserem Terminal angezeigt wurde.
Bevor wir fortfahren, beenden wir unseren laufenden Server, indem wir Strg + C drücken.
Dadurch wird die Ausführung unseres Servers unterbrochen und wir kehren zur Befehlszeilenaufforderung zurück.
Bei den wenigstens Websites, die wir besuchen, oder APIs, die wir verwenden, werden Serverantworten in Klartext dargestellt.
HTML-Seiten und JSON-Daten sind gängige Antwortformate.
Im nächsten Schritt erfahren wir, wie sich HTTP-Antworten in gängigen Datenformaten zurückgeben lassen, die im Web genutzt werden.
Schritt 2 - Zurückgeben verschiedener Arten von Inhalten
Die Antwort, die wir von einem Webserver zurückgeben, kann eine Vielzahl von Formaten aufweisen.
JSON und HTML wurden bereits erwähnt; wir können aber auch andere Textformate wie XML und CSV zurückgeben.
Schließlich können Webserver Nicht-Text-Daten wie PDFs, gezippte Dateien, Audio und Video zurückgeben.
In diesem Artikel lernen Sie, wie Sie neben dem gerade zurückgegebenen Klartext die folgenden Arten von Daten zurückgeben:
JSON
CSV
HTML
Diese drei Datentypen sind alle textbasiert und beliebte Formate für die Bereitstellung von Inhalten im Web.
Viele serverseitige Entwicklungssprachen und Tools bieten Unterstützung für die Rückgabe dieser verschiedenen Datentypen.
Im Kontext von Node.js müssen wir zwei Dinge tun:
Den Header Content-Type in unseren HTTP-Antworten mit dem entsprechenden Wert festlegen.
Sicherstellen, dass res.end () die Daten im richtigen Format erhält.
Lassen Sie uns dies mit einigen Beispielen in der Praxis ansehen.
Der Code, den wir in diesem Abschnitt schreiben werden, sowie spätere Codes haben viele Ähnlichkeiten mit dem zuvor geschriebenen Code.
Die meisten Änderungen gibt es innerhalb der Funktion requestListener ().
Lassen Sie uns Dateien mit diesem "Vorlagencode" erstellen, um zukünftige Abschnitte leichter folgbar zu machen.
Erstellen Sie eine neue Datei namens html.js:
Diese Datei wird später verwendet, um in einer HTTP-Antwort HTML-Text zurückzugeben.
Wir platzieren den Vorlagencode hier und kopieren ihn auf die anderen Server, die verschiedene Arten zurückgeben.
Geben Sie im Terminal Folgendes ein:
Öffnen Sie diese Datei nun in einem Texteditor:
Kopieren wir den "Vorlagencode" ​ ​.
Geben Sie Folgendes in nano ein:
Speichern und schließen Sie html.js mit Strg + X und kehren Sie dann zum Terminal zurück.
Lassen Sie uns diese Datei nun in zwei neue Dateien kopieren.
Die erste Datei wird dazu dienen, CSV-Daten in der HTTP-Antwort zurückzugeben:
Die zweite Datei wird eine JSON-Antwort im Server zurückgegeben:
Die verbleibenden Dateien werden für spätere Übungen benötigt:
Wir sind nun bereit, um unsere Übungen fortzusetzen.
Lassen Sie uns mit der Rückgabe von JSON beginnen.
Bereitstellen von JSON
JavaScript Object Notation, allgemein als JSON bezeichnet, ist ein textbasiertes Format für den Datenaustausch.
Wie der Name bereits andeutet, wird es von JavaScript-Objekten abgeleitet, ist aber sprachunabhängig und lässt sich von jeder Programmiersprache verwenden, die seine Syntax parsen kann.
JSON wird häufig von APIs verwendet, um Daten zu akzeptieren und zurückzugeben. Die hohe Popularität beruht auf einer geringeren Datenübertragungsgröße als bei älteren Datenaustauschstandards wie XML sowie auf dem vorhandenen Tooling, mit dem Programme JSON ohne übermäßigen Aufwand parsen können.
Wenn Sie mehr über JSON erfahren möchten, können Sie unseren Leitfaden Arbeiten mit JSON in JavaScript lesen.
Öffnen Sie die Datei json.js mit nano:
Wir möchten eine JSON-Antwort zurückgeben.
Lassen Sie uns die Funktion requestListener () so anpassen, dass der entsprechende Header aller JSON-Antworten zurückgegeben wird, indem wir die hervorgehobenen Zeilen folgendermaßen ändern:
Die Methode res.setHeader () fügt der Antwort einen HTTP-Header hinzu.
HTTP-Header sind zusätzliche Informationen, die an eine Anfrage oder Antwort angehängt werden können.
Die Methode res.setHeader () benötigt zwei Argumente: den Namen des Headers und seinen Wert.
Der Header Content-Type dient zum Anzeigen des Formats der Daten (auch als Medientyp bekannt), die mit der Anfrage oder Antwort gesendet werden.
In diesem Fall ist unser Content-Type application / json.
Lassen Sie uns nun JSON-Inhalt an den Benutzer zurückgeben.
Ändern Sie json.js, damit die Datei folgendermaßen aussieht:
Wie zuvor sagen wir dem Benutzer, dass seine Anfrage erfolgreich war, indem wir einen Statuscode von 200 zurückgeben. Dieses Mal enthält unser Zeichfolgenargument im Aufruf response.end () gültiges JSON.
Speichern und schließen Sie json.js, indem Sie Strg + X drücken.
Lassen Sie uns nun den Server mit dem Befehl node ausführen:
In einem anderen Terminal stellen wir eine Verbindung zum Server her, indem wir cURL verwenden:
Wenn wir ENTER drücken, sehen wir folgendes Ergebnis:
Wir haben nun erfolgreich eine JSON-Antwort zurückgegeben, genauso wie viele der beliebten APIs, mit denen wir Apps einrichten.
Beenden Sie den laufenden Server unbedingt mit Strg + C, damit wir zur standardmäßigen Terminalaufforderung zurückkehren können.
Lassen Sie uns als Nächstes ein weiteres beliebtes Format zum Zurückgeben von Daten ansehen: CSV.
Bereitstellen von CSV
Das Dateiformat Comma Separated Values (CSV) ist ein Textstandard, der häufig für die Bereitstellung von Tabellendaten verwendet wird. In den meisten Fällen wird jede Zeile durch ein Neue-Zeile-Symbol und jedes Element in der Zeile durch ein Komma getrennt.
Öffnen Sie in unserem Arbeitsbereich mit einem Texteditor die Datei csv.js:
Lassen Sie uns die folgenden Zeilen in unsere Funktion requestListener () einfügen:
Dieses Mal zeigt unser Content-Type an, dass eine CSV-Datei zurückgegeben wird, da der Wert text / csv lautet.
Der zweite Header, den wir hinzufügen, ist Content-Disposition.
Dieser Header teilt dem Browser mit, wie die Daten angezeigt werden sollen, insbesondere im Browser oder als separate Datei.
Wenn wir CSV-Antworten zurückgeben, laden die meisten modernen Browser die Datei automatisch herunter, selbst wenn der Header Content-Disposition nicht gesetzt ist. Wenn wir aber eine CSV-Datei zurückgeben, sollten wir diesen Header weiterhin hinzufügen, da er uns ermöglicht, den Namen der CSV-Datei festzulegen.
In diesem Fall signalisieren wir dem Browser, dass diese CSV-Datei ein Anhang ist und heruntergeladen werden soll.
Dann teilen wir dem Browser mit, dass der Name der Datei oceanpals.csv lautet.
Lassen Sie uns die CSV-Daten in die HTTP-Antwort schreiben:
Wie zuvor geben wir mit unserer Antwort einen 200 / OK-Status zurück.
Dieses Mal verfügt unser Aufruf an res.end () über eine Zeichenfolge, die eine gültige CSV ist.
Das Komma trennt den Wert in den einzelnen Spalten, während das Neue-Zeile-Zeichen (\ n) die Zeilen trennt.
Wir verfügen über zwei Zeilen, eine für den Tabellen-Header und eine für die Daten.
Wir testen diesen Server im Browser.
Speichern Sie csv.js und schließen Sie den Editor mit Strg + X.
Führen Sie den Server mit dem Befehl Node.js aus:
Das wird in der Konsole angezeigt:
Wenn wir in unserem Browser http: / / localhost: 8000 aufrufen, wird eine CSV-Datei heruntergeladen.
Ihr Dateiname wird oceanpals.csv lauten.
Beenden Sie den laufenden Server mit Strg + C, um zur standardmäßigen Terminalaufforderung zurückzukehren.
Mit der Rückgabe von JSON und CSV haben wir zwei Fälle behandelt, die bei APIs beliebt sind.
Lassen Sie uns mit der Ansicht fortfahren, die Benutzer von Websites in einem Browser sehen.
Bereitstellen von HTML
HTML (HyperText Markup Language) ist das gängigste Format, das verwendet wird, wenn wir wollen, dass Benutzer über einen Webbrowser mit unserem Server interagieren.
Es dient dazu, Webinhalte zu strukturieren.
Webbrowser dienen zum Anzeigen von HTML-Inhalten sowie beliebiger Stile, die wir mit CSS hinzufügen, einer anderen Frontend-Webtechnologie, mit der wir das Aussehen unserer Websites ändern können.
Lassen Sie uns die Datei html.js mit unserem Texteditor erneut öffnen:
Ändern Sie die Funktion requestListener (), um den entsprechenden Content-Type-Header für eine HTML-Antwort zurückzugeben:
Lassen Sie uns nun HTML-Inhalt an den Benutzer zurückgeben.
Fügen Sie die hervorgehobenen Zeilen in html.js hinzu, damit die Datei folgendermaßen aussieht:
Wir fügen zuerst den HTTP-Statuscode ein.
Dann rufen wir response.end () mit einem Zeichenfolgenargument auf, das gültiges HTML enthält.
Wenn wir im Browser auf unseren Server zugreifen, sehen wir eine HTML-Seite mit einem Header-Tag, das den Inhalt This is HTML aufweist.
Speichern und schließen Sie Datei, indem Sie Strg + X drücken.
Wir sehen Server is running on http: / / localhost: 8000, sobald unser Programm gestartet wurde.
Rufen Sie nun im Browser http: / / localhost: 8000 auf.
Unsere Seite wird so aussehen:
Bild der vom Node.js-Server zurückgegebenen HTML-Antwort
Es ist üblich, dass HTML in einer Datei geschrieben wird, getrennt vom serverseitigen Code (wie unsere Node.js-Programme).
Lassen Sie uns als Nächstes sehen, wie wir HTML-Antworten aus Dateien zurückgeben können.
Schritt 3 - Bereitstellen einer HTML-Seite aus einer Datei
Wir können dem Benutzer HTML als Zeichenfolgen in Node.js bereitstellen, es ist jedoch empfehlenswert, HTML-Dateien zu laden und ihren Inhalt bereitzustellen.
Wenn die HTML-Datei wächst, müssen wir so keine langen Zeichenfolgen in unserem Node.js-Code pflegen. Dadurch bleibt der Code kurz und können wir getrennt an den einzelnen Aspekten unserer Website arbeiten.
Diese "Trennung von Aufgaben" ist in vielen Webentwicklungs-Setups üblich; daher ist es gut zu wissen, wie HTML-Dateien zur Unterstützung in Node.js geladen werden.
Um HTML-Dateien bereitzustellen, laden wir die HTML-Datei mit dem fs-Modul und verwenden ihre Daten beim Schreiben unserer HTTP-Antwort.
Zuerst erstellen wir eine HTML-Datei, die der Webserver zurückgeben wird.
Erstellen Sie eine neue HTML-Datei:
Öffnen Sie nun index.html in einem Texteditor:
Unsere Webseite wird minimal sein.
Sie wird über einen orangen Hintergrund verfügen und in der Mitte einen Grußtext anzeigen.
Fügen Sie diesen Code in die Datei ein:
Diese einzelne Webseite zeigt zwei Textzeilen an: Hello Again!
und This is served from a file.
Die Zeilen befinden sich in der Mitte der Seite übereinander.
Die erste Zeile des Texts wird als Überschrift angezeigt. Das bedeutet, dass sie größer ist.
Die zweite Zeile des Texts wird etwas kleiner erscheinen.
Der gesamte Text wird weiß dargestellt, während die Webseite einen orangen Hintergrund hat.
Es ist zwar nicht Inhalt dieses Artikel oder dieser Reihe, Sie können bei Bedarf aber mehr über HTML, CSS und andere Frontend-Webtechnologien erfahren, indem Sie einen Blick in den Leitfaden Erste Schritte mit dem Web von Mozilla werfen.
Das ist alles, was wir für HTML benötigen. Speichern und schließen Sie die Datei also mit Strg + X.
Wir können nun mit dem Servercode fortfahren.
Für diese Übung werden wir mit htmlFile.js arbeiten.
Öffnen Sie die Datei mit dem Texteditor:
Da wir eine Datei lesen müssen, beginnen wir, indem wir das fs-Modul importieren:
Dieses Modul enthält eine readFile () -Funktion, die wir zum Laden der HTML-Datei verwenden werden.
Wir importieren die Zusagevariante anhand aktueller Best Practices für JavaScript.
Wir verwenden Zusagen, da sie syntaktisch prägnanter sind als Rückrufe. Diese müssten wir verwenden, wenn wir fs lediglich require (' fs ') zuordnen.
Weitere Informationen über Best Practices für asynchrone Programmierung finden Sie in unserem Leitfaden Schreiben von asynchronem Code in Node.js.
Wir möchten, dass unsere HTML-Datei gelesen wird, wenn ein Benutzer eine Anfrage an unser System stellt.
Lassen Sie uns mit dem Ändern von requestListener () zum Lesen der Datei beginnen:
Wir verwenden die Methode fs.readFile (), um die Datei zu laden.
Ihr Argument lautet _ _ dirname + "/ index.html"
Der spezielle Variable _ _ dirname weist den absoluten Pfad auf, an dem der Node.js-Code ausgeführt wird.
Dann hängen wir / index.html an, damit wir die zuvor erstellte HTML-Datei laden können.
Lassen Sie uns nun die HTML-Seite zurückgeben, sobald sie geladen ist:
Wenn das Versprechen fs.readFile () erfolgreich aufgelöst wird, werden deren Daten zurückgegeben. Wir verwenden die Methode then (), um diesen Fall zu handhaben.
Der Parameter contents enthält die Daten der HTML-Datei.
Wir setzen zunächst den Header Content-Type auf text / html, um dem Client zu sagen, dass wir HTML-Daten zurückgeben. Wir schreiben dann den Statuscode, um anzuzeigen, dass die Anfrage erfolgreich war.
Abschließend senden wir dem Client die geladene HTML-Seite mit den Daten in der Variablen contents.
Die Methode fs.readFile () kann manchmal fehlschlagen. Daher sollten wir den Fall handhaben, wenn wir einen Fehler erhalten.
Fügen Sie Folgendes in die Funktion requestListener () ein:
Speichern Sie die Datei und schließen Sie nano mit Strg + X.
Wenn ein Versprechen auf einen Fehler trifft, wird es verworfen.
Wir behandeln diesen Fall mit der Methode catch ().
Sie akzeptiert den Fehler, den fs.readFile () zurückgibt, legt den Statuscode auf 500 fest, was bedeutet, dass ein interner Fehler aufgetreten ist, und gibt den Fehler an den Benutzer zurück.
Führen Sie unseren Server mit dem Befehl node aus:
Rufen Sie im Webbrowser http: / / localhost: 8000 auf.
Bild der aus einer Datei in Node.js geladenen HTML-Seite
Sie haben nun eine HTML-Seite vom Server an den Benutzer zurückgegeben.
Sie können den laufenden Server mit Strg + C beenden.
Wenn Sie dies tun, wird Ihnen die Eingabeaufforderung des Terminals angezeigt.
Wenn Sie in der Produktion Code wie hier verfassen, wollen Sie vielleicht nicht jedesmal eine HTML-Seite laden, wenn Sie eine HTTP-Anfrage erhalten.
Zwar ist diese HTML-Seite nur etwa 800 Byte groß, doch können komplexere Websites Größen im Megabytebereich aufweisen.
Große Dateien können eine Weile zum Laden benötigen.
Wenn Ihre Website eine Menge Datenverkehr erwartet, kann es am besten sein, HTML-Dateien beim Starten zu laden und ihren Inhalt zu speichern.
Nachdem sie geladen wurden, können Sie den Server einrichten und ihn an einer Adresse auf Anfragen lauschen lassen.
Um diese Methode zu demonstrieren, sehen wir uns an, wie wir unseren Server so anpassen können, dass er effizienter und besser skalierbar wird.
Effizientes Bereitstellen von HTML
Anstatt die HTML für jede Anfrage zu laden, laden wir sie in diesem Schritt nur einmal am Anfang.
Die Anfrage gibt die beim Starten geladenen Daten zurück.
Öffnen Sie im Terminal mit einem Texteditor erneut das Skript Node.js:
Lassen Sie uns zunächst eine neue Variable hinzufügen, bevor wir die Funktion requestListener () erstellen:
Wenn wir dieses Programm ausführen, wird diese Variable den Inhalt der HTML-Datei enthalten.
Lassen Sie uns nun die Funktion requestListener () anpassen.
Anstatt die Datei zu laden, gibt sie nun den Inhalt von indexFile zurück:
Als Nächstes verschieben wir die Logik zum Dateilesen von der Funktion requestListener () in den Start unseres Servers.
Nehmen Sie beim Erstellen des Servers die folgenden Änderungen vor:
Der Code, der die Datei liest, ähnelt dem, was wir bei unserem ersten Versuch geschrieben haben.
Nachdem wir die Datei erfolgreich gelesen haben, speichern wir den Inhalt jedoch in unserer globalen Variable indexFile.
Dann starten wir den Server mit der Methode listen ().
Das Entscheidende ist, dass die Datei geladen wird, bevor der Server ausgeführt wird.
Auf diese Weise wird die Funktion requestListener () eine HTML-Seite zurückgeben, da indexFile keine leere Variable mehr ist.
Unsere Fehlerhandlung hat sich auch geändert.
Wenn die Datei nicht geladen werden kann, erfassen wir den Fehler und geben ihn in unserer Konsole aus.
Dann beenden wir das Node.js-Programm mit der Funktion exit (), ohne den Server zu starten.
So können wir sehen, warum das Lesen der Datei fehlgeschlagen ist, das Problem adressieren und dann den Server neu starten.
Wir haben nun verschiedene Webserver erstellt, die verschiedene Arten von Daten an einen Benutzer zurückgeben.
Bisher haben wir keine Anfragedaten verwendet, um zu bestimmen, was zurückgegeben werden soll.
Wir müssen Anfragedaten verwenden, wenn wir verschiedene Routen oder Wege in einem Node.js-Server einrichten. Lassen Sie uns also als Nächstes betrachten, wie diese zusammenarbeiten.
Schritt 4 - Verwalten von Routen mit einem HTTP-Anfrageobjekt
Die meisten Websites, die wir besuchen, oder APIs, die wir verwenden, weisen normalerweise mehr als einen Endpunkt auf, sodass wir auf verschiedene Ressourcen zugreifen können.
Ein gutes Beispiel dafür wäre ein Buchverwaltungssystem, das in einer Bücherei zum Einsatz kommt.
Es müsste nicht nur Buchdaten verwalten, sondern auch Autorendaten, um das Angebot zu katalogisieren und Suchen möglichst einfach zu gestalten.
Zwar sind die Daten für Bücher und Autoren verwandt, doch sind sie zwei verschiedene Objekte.
In diesem Fall codieren Softwareentwickler meist jedes Objekt auf verschiedenen Endpunkten, um den API-Benutzern zu zeigen, mit welchen Arten von Daten sie interagieren.
Lassen Sie uns einen neuen Server für eine kleine Bücherei erstellen, der zwei verschiedene Arten von Daten zurückgibt. Wenn Benutzer die Adresse unseres Servers unter / books aufrufen, erhalten Sie eine Liste von Büchern in JSON.
Wenn sie zu / authors gehen, erhalten sie eine Liste von Autorendaten in JSON.
Bisher haben wir auf jede Anfrage, die wir erhalten haben, die gleiche Antwort zurückgegeben. Lassen Sie uns das kurz illustrieren.
Führen Sie unser JSON-Antwortbeispiel erneut aus:
Lassen Sie uns wie zuvor in einem anderen Terminal eine cURL-Anfrage ausführen:
Lassen Sie uns nun einen weiteren cURL-Befehl ausprobieren:
Nach dem Drücken von Enter sehen Sie das gleiche Ergebnis:
Wir haben keine spezielle Logik in unserer Funktion requestListener () zur Bearbeitung einer Anfrage erstellt, deren URL / todos enthält, sodass Node.js standardmäßig die gleiche JSON-Nachricht zurückgibt.
Da wir einen Miniaturserver zur Buchverwaltung erstellen möchten, trennen wir nun die Art von Daten, die zurückgegeben werden sollen, je nach Endpunkt der Benutzerzugriffe.
Beenden Sie zuerst den laufenden Server mit Strg + C.
Öffnen Sie nun routes.js in Ihrem Texteditor:
Lassen Sie uns zunächst unsere JSON-Daten in Variablen vor der Funktion requestListener () speichern:
Die Variable books ist eine Zeichenfolge, die JSON für eine Gruppe von Buchobjekten enthält.
Jedes Buch hat einen Titel oder Namen, einen Autor und ein Veröffentlichungsjahr.
Die Variable authors ist eine Zeichenfolge, die JSON für eine Reihe von Autorenobjekten enthält.
Jeder Autor hat einen Namen, ein Geburtsland und ein Geburtsjahr.
Nachdem wir nun über die Daten verfügen, die unsere Antworten zurückgeben werden, beginnen wir, die Funktion requestListener () zu ändern, um sie auf die richtigen Routen zurückzuleiten.
Zuerst stellen wir sicher, dass jede Antwort von unserem Server den richtigen Content-Type-Header aufweist:
Jetzt möchten wir je nach URL-Pfad, den der Benutzer besucht, die richtige JSON zurückgeben.
Lassen Sie uns eine switch-Anweisung zur URL der Anfrage erstellen:
Um den URL-Pfad eines Anfrageobjekts zu erhalten, müssen wir auf seine url-Eigenschaft zugreifen.
Wir können nun Fälle in die switch-Anweisung einfügen, um die entsprechende JSON zurückzugeben.
Die switch-Anweisung von JavaScript bietet eine Möglichkeit zu steuern, welcher Code je nach Wert eines Objekts oder JavaScript-Ausdrucks (z. B. Ergebnis mathematischer Operationen) ausgeführt wird.
Wenn Sie eine Lektion oder Erinnerung zu ihrer Verwendung benötigen, lesen Sie unseren Leitfaden Verwenden der switch-Anweisung in JavaScript.
Lassen Sie uns einen case (Fall) für die Situation einfügen, dass der Benutzer unsere Liste von Büchern erhalten möchte:
Wir setzen unseren Statuscode auf 200, um anzuzeigen, dass die Anfrage in Ordnung ist, und geben die JSON zurück, die die Liste unserer Bücher enthält.
Lassen Sie uns nun einen weiteren case (Fall) für unsere Autoren einfügen:
Wie zuvor wird der Statuscode 200 betragen, da die Anfrage in Ordnung ist.
Dieses Mal geben wir die JSON zurück, die die Liste unserer Autoren enthält.
Wir wollen einen Fehler zurückgeben, wenn der Benutzer einen beliebigen anderen Pfad aufrufen möchte.
Lassen Sie uns dazu den Standardfall einfügen:
Wir verwenden das default-Stichwort in einer switch-Anweisung, um alle anderen Szenarien zu erfassen, die von unseren vorherigen Fällen nicht abgedeckt werden.
Wir setzen den Statuscode auf 404, um anzuzeigen, dass die gewünschte URL nicht gefunden wurde.
Dann legen wir ein JSON-Objekt fest, das eine Fehlermeldung enthält.
Lassen Sie uns unseren Server testen, um zu sehen, ob er sich wie erwartet verhält.
Führen wir in einem anderen Terminal zuerst einen Befehl aus, um zu sehen, ob wir unsere Liste mit Büchern erhalten:
Drücken Sie Enter, um die folgende Ausgabe zu sehen:
Das funktioniert schon einmal gut.
Lassen Sie uns das gleiche für / authors ausprobieren.
Geben Sie den folgenden Befehl im Terminal ein:
Sie sehen die folgende Ausgabe, sobald der Befehl abgeschlossen wurde:
Lassen Sie uns zuletzt eine fehlerhafte URL ausprobieren, um sicherzustellen, dass requestListener () die Fehlerantwort zurückgibt:
Bei Eingabe dieses Befehls wird folgende Nachricht angezeigt:
Wir haben nun verschiedene Wege für Benutzer erstellt, um verschiedene Daten zu erhalten. Wir haben auch eine Standardantwort hinzugefügt, die einen HTTP-Fehler zurückgibt, wenn der Benutzer eine URL eingibt, die wir nicht unterstützen.
In diesem Tutorial haben Sie eine Reihe von Node.js-HTTP-Servern erstellt.
Zuerst haben Sie eine einfache Textantwort zurückgegeben.
Dann haben Sie von unserem Server verschiedene Arten von Daten zurückgegeben: JSON, CSV und HTML.
Ab da konnten Sie das Laden von Dateien mit HTTP-Antworten kombinieren, um eine HTML-Seite vom Server an den Benutzer zurückzugeben, und eine API erstellen, die Informationen über die Anfrage des Benutzers verwendet, um zu bestimmen, welche Daten in der Antwort gesendet werden sollen.
Sie sind nun dazu in der Lage, Webserver zu erstellen, die eine Vielzahl von Anfragen und Antworten bearbeiten können.
Mit diesem Wissen können Sie einen Server einrichten, der an verschiedenen Endpunkten viele HTML-Seiten an den Benutzer zurückgibt.
Außerdem können Sie Ihre eigene API erstellen.
Um mehr über weitere HTTP-Webserver in Node.js zu erfahren, können Sie die Node.js-Dokumentation im http-Modul lesen.
Wenn Sie noch mehr über Node.js erfahren möchten, können Sie zu der Serienseite Codieren in Node.js zurückkehren.
Erstellen von React-Elementen mit JSX
4048
In diesem Tutorial lernen Sie, wie Sie Elemente mit JSX beschreiben.
JSX ist eine Abstraktion, die es Ihnen erlaubt, eine HTML-ähnliche Syntax in Ihren JavaScript-Code zu schreiben und ermöglicht es Ihnen, React-Komponenten zu erstellen, die wie Standard-HTML-Markup aussehen.
JSX ist die Vorlagensprache der React-Elemente und bildet daher die Grundlage für jedes Markup, das React in Ihrer Anwendung rendert.
Da Sie mit JSX auch JavaScript in Ihr Markup schreiben können, können Sie die Vorteile von JavaScript-Funktionen und -Methoden nutzen, einschließlich Array-Mapping und Kurzschlussevaluierung für Bedingungen.
Im Rahmen des Tutorials werden Sie Klick-Ereignisse auf Schaltflächen direkt im Markup erfassen und Instanzen abfangen, wenn die Syntax nicht exakt mit Standard-HTML übereinstimmt, wie zum Beispiel bei CSS-Klassen.
Am Ende dieses Tutorials haben Sie eine funktionierende Anwendung, die eine Vielzahl von JSX-Funktionen verwendet, um eine Liste von Elementen anzuzeigen, die über einen integrierten Klick-Listener verfügen.
Dies ist ein übliches Muster in React-Anwendungen, das Sie während des Erlernens des Frameworks häufig verwenden werden.
Sie werden auch in der Lage sein, Standard-HTML-Elemente mit JavaScript zu mischen und dabei sehen, wie React Ihnen ermöglicht, kleine, wiederverwendbare Code-Stücke zu erstellen.
Sie benötigen eine Entwicklungsumgebung, die Node.js ausführt; dieses Tutorial wurde mit der Node.js-Version 10.19.0 und der npm-Version 6.13.4 getestet.
Schritt 1 - Hinzufügen eines Markups zu einem React-Element
Wie bereits erwähnt, verfügt React über eine spezielle Markup-Sprache namens JSX.
Diese vermischt HTML- und JavaScript-Syntax und sieht in etwa so aus:
Sie werden einige JavaScript-Funktionalität wie .filter und .map sowie Standard-HTML wie < div > erkennen.
Es gibt aber auch andere Teile, die wie HTML und JavaScript aussehen, wie < Card > und className.
Dabei handelt es sich um JSX, die spezielle Markup-Sprache, die React-Komponenten den Anschein von HTML mit der Leistungsfähigkeit von JavaScript verleiht.
In diesem Schritt lernen Sie, grundlegende HTML-ähnliche Syntax zu einem bestehenden React-Element hinzuzufügen.
Zu Beginn fügen Sie Standard-HTML-Elemente in eine JavaScript-Funktion ein und sehen dann den kompilierten Code in einem Browser.
Außerdem gruppieren Sie Elemente, damit React diese mit minimalem Markup kompilieren kann und dadurch eine klare HTML-Ausgabe hinterlässt.
Beginnen Sie mit der Erstellung eines neuen Projekts.
Führen Sie auf Ihrer Befehlszeile das folgende Skript aus, um ein neues Projekt mit create-react-app zu installieren:
Nachdem das Projekt erstellt ist, wechseln Sie in das Verzeichnis:
Starten Sie das Projekt in einer neuen Terminal-Registerkarte oder einem -fenster mit dem Create React App start-Skript.
Der Browser aktualisiert Änderungen automatisch; lassen Sie also dieses Skript während der gesamten Dauer Ihrer Arbeit laufen:
Sie erhalten einen funktionierenden lokalen Server.
Falls sich das Projekt nicht in einem Browserfenster geöffnet hat, finden Sie es unter http: / / localhost: 3000 /.
Wenn Sie dies von einem Remote-Server aus ausführen, ist die Adresse http: / / < ^ > your _ IP _ address < ^ >: 3000.
Ihr Browser wird mit einer React-Anwendung geladen, die als Teil von Create React App enthalten ist.
Sie werden ein vollständig neues Set von benutzerdefinierten Komponenten erstellen, sodass Sie zunächst etwas Standardcode löschen müssen, um ein leeres Projekt zu erhalten.
Zuerst öffnen Sie App.js in einem Texteditor.
Das ist die Stammkomponente, die in der Seite injiziert wird.
Alle Komponenten beginnen von hier.
Gehen Sie in einem neuen Terminal in den Projektordner und öffnen Sie src / App.js mit dem folgenden Befehl:
Sie sehen eine Datei wie diese:
Löschen Sie nun die Zeile import logo from '. / logo.svg sowie alles nach der return-Anweisung in der Funktion.
Ändern Sie diese zu return null.
Der endgültige Code sieht so aus:
Speichern und schließen Sie den Texteditor.
Abschließend löschen Sie das Logo.
Geben Sie im Terminalfenster den folgenden Befehl ein:
Sie werden diese SVG-Datei nicht in Ihrer Anwendung verwenden und Sie sollten unbenutzte Dateien entfernen, während Sie arbeiten.
So wird Ihr Code langfristig besser organisiert.
Nachdem diese Teile Ihres Projekts nun entfernt sind, können Sie die Facetten von JSX erkunden.
Diese Markup-Sprache wird von React kompiliert und wird schließlich zu dem HTML, das Sie auf einer Webseite sehen.
Ohne zu sehr in die Einzelheiten zu gehen, nimmt React die JSX und erstellt ein Modell Ihrer zukünftigen Seite sowie anschließend die erforderlichen Elemente, die dann in Ihre Seite eingefügt werden.
Das bedeutet, dass Sie etwas schreiben können, das wie HTML aussieht, und erwarten können, dass das gerenderte HTML ähnlich sein wird.
Es gibt jedoch einige Fallstricke.
Wenn Sie sich zunächst die Registerkarte oder das Fenster ansehen, auf dem Ihr Server läuft, sehen Sie Folgendes:
Das ist der Linter, der Ihnen sagt, dass Sie den importierten React-Code nicht verwenden.
Wenn Sie die Zeile import React from 'react' in Ihren Code einfügen, importieren Sie JavaScript-Code, der die JSX in React-Code konvertiert.
Wenn keine JSX vorhanden ist, ist das Importieren nicht erforderlich.
Wir ändern dies, indem wir einen kleinen Teil JSX einfügen.
Beginnen Sie, indem Sie null durch das Beispiel Hello, World ersetzen:
Wenn Sie sich das Terminal mit dem laufenden Server ansehen, ist die Warnmeldung verschwunden.
Wenn Sie Ihren Browser besuchen, sehen Sie die Nachricht als h1-Element.
Bildschirmanzeige des Browsers mit "Hello, World"
Fügen Sie als Nächstes unter dem Tag < h1 > ein Absatz-Tag mit der Zeichenfolge I am writing JSX ein.
Der Code sieht dann so aus:
Da JSX mehrere Zeilen umfasst, müssen Sie den Ausdruck in Klammern setzten.
Wenn Sie dies tun, sehen Sie einen Fehler in dem Terminal, auf dem Ihr Server läuft:
Wenn Sie JSX von einer Funktion oder Anweisung zurückgeben, müssen Sie ein einzelnes Element zurückgeben.
Dieses Element kann verschachtelte Unterelemente haben, aber es muss ein einzelnes Element der obersten Ebene vorhanden sein.
In diesem Fall geben Sie zwei Elemente zurück.
Die Lösung ist eine kleine Codeänderung.
Umgeben Sie den Code mit einem leeren Tag.
Ein leeres Tag ist ein HTML-Element ohne Wörter.
Es sieht so aus: < > < / >.
Gehen Sie zu. / src / App.js in Ihrem Editor zurück und fügen Sie das leere Tag ein:
Das leere Tag erstellt ein einzelnes Element, aber wenn der Code kompiliert wird, wird es nicht in das endgültige Markup eingefügt.
Dadurch wird Ihr Code sauber gehalten und dennoch ein einzelnes Element an React gegeben.
< $> note Anmerkung: Sie könnten den Code auch mit einem div anstatt eines leeren Tags umgeben, solange der Code ein Element zurückgibt.
In diesem Beispiel hat ein leeres Tag den Vorteil, dass kein zusätzliches Markup in die gegliederte Ausgabe eingefügt wird.
Speichern Sie den Code und schließen Sie die Datei.
Ihr Browser aktualisiert sich und zeigt die aktualisierte Seite mit dem Absatz-Element an.
Zusätzlich werden die leeren Tags bei der Konvertierung des Codes ausgelassen:
Browser mit Anzeige des Markups und DevTools mit Anzeige des Markups ohne leere Tags
Sie haben nun einige grundlegende JSX in Ihre Komponente eingefügt und gelernt, wie die gesamte JSX in einer einzelnen Komponente geschachtelt werden muss.
Im nächsten Schritt geben Sie Ihre Komponente ein Styling.
Schritt 2 - Hinzufügen eines Stylings zu einem Element mit Attributen
In diesem Schritt stylen Sie die Elemente in Ihrer Komponente, um zu lernen, wie HTML-Attribute mit JSX arbeiten.
Es gibt zahlreiche Styling-Optionen in React.
Einige von ihnen erfordern das Schreiben von CSS in Javascript, andere verwenden Präprozessoren.
In diesem Tutorial arbeiten Sie mit importierten CSS und CSS-Klassen.
Sie haben nun Ihren Code und können diesem nun ein Styling hinzufügen.
Öffnen Sie App.css in Ihrem Texteditor:
Da Sie mit neuer JSX beginnen, bezieht sich das aktuelle CSS auf Elemente, die nicht mehr existieren.
Da Sie kein CSS benötigen, können Sie es löschen.
Nach der Löschung des Codes haben Sie eine leere Datei.
Als Nächstes fügen Sie ein Styling hinzu, um den Text zu zentrieren.
Fügen Sie den folgenden Code in src / App.css ein:
In diesem Codeblock haben Sie einen CSS-Klassenselektor namens .container erstellt und dazu verwendet, den Inhalt mit display: flex zu zentrieren.
Der Browser wird aktualisiert, aber es wird sich nichts ändern.
Bevor Sie die Änderung sehen können, müssen Sie die CSS-Klasse zu Ihrer React-Komponente hinzufügen.
Öffnen Sie den JavaScript-Code der Komponente:
Das bedeutet, dass webpack den Code einzieht, um ein endgültiges Stylesheet zu erstellen, aber um das CSS auf Ihre Elemente anzuwenden, müssen Sie die Klassen hinzufügen.
Ändern Sie zuerst in Ihrem Texteditor die leeren Tags, < >, in < div >.
In diesem Code haben Sie die leeren Tags - < > - mit div-Tags ersetzt.
Leere Tags sind nützlich, um Ihren Code zu gruppieren, ohne zusätzliche Tags einzufügen, aber hier müssen Sie ein div verwenden, da leere Tags keine HTML-Attribute akzeptieren.
Als Nächstes müssen Sie den Klassennamen hinzufügen.
Hier beginnt JSX, von HTML abzuweichen.
Wenn Sie eine Klasse zu einem gängigen HTML-Element hinzufügen wollten, würden Sie das so tun:
Da JSX jedoch JavaScript ist, hat es einige Beschränkungen.
Eine der Beschränkungen besteht darin, dass JavaScript reservierte Schlüsselwörter hat.
Das bedeutet, dass Sie bestimmte Wörter in keinem JavaScript-Code verwenden können.
Sie können zum Beispiel keine Variable mit der Bezeichnung null erstellen, da dieses Wort bereits reserviert ist.
Eines der reservierten Wörter ist class.
React umgeht dieses reservierte Wort, indem es leicht geändert wird.
Anstelle des Attributs class fügen Sie das Attribut className hinzu.
Wenn ein Attribut nicht wie erwartet funktioniert, können Sie generell versuchen, die Version mit einer Binnenmajuskel hinzufügen.
Ein weiteres Attribut, das leicht abweicht, ist das Attribut for, das Sie für Labels verwenden würden.
Es gibt einige weitere Fälle, aber glücklicherweise ist die Liste ziemlich kurz.
< $> note Anmerkung: In React werden Attribute oft props genannt.
Props sind Datenstücke, die Sie an andere benutzerdefinierte Komponenten übergeben können.
Sie sehen so aus wie Attribute, außer dass sie keinen HTML-Spezifikationen entsprechen.
In diesem Tutorial nennen wir sie Attribute, da sie hauptsächlich wie Standard-HTML-Attribute verwendet werden.
Das unterscheidet sie von den Props, die sich nicht wie HTML-Attribute verhalten, was später in dieser Serie behandelt wird.
Nachdem Sie nun wissen, wie das Attribut class in React verwendet wird, können Sie Ihren Code aktualisieren, um das Styling einzubeziehen.
Fügen Sie in Ihrem Texteditor className = "container" in Ihr öffnendes div-Tag ein:
Wenn Sie dies tun, wird die Seite neu geladen und der Inhalt wird zentriert.
Zentrierte HTML-Elemente in einem Browser.
Das Attribut className ist in React einzigartig.
Sie können die meisten HTML-Attribute ohne Änderung in JSX einfügen.
Gehen Sie als Beispiel zurück in Ihren Texteditor und fügen Sie Ihrem < h1 > -Element eine id von greeting hinzu.
Es wird aussehen wie Standard-HTML:
Speichern Sie die Seite und laden Sie den Browser neu.
Sie wird gleich bleiben.
Bis hierher sieht JSX wie Standard-Markup aus. Aber der Vorteil von JSX ist, dass es, obwohl es wie HTML aussieht, die Leistungsfähigkeit von JavaScript hat.
Das bedeutet, dass Sie Variablen zuweisen und in Ihren Attributen referenzieren können.
Um ein Attribut zu referenzieren, umschließen Sie es mit geschweiften Klammern - {} - anstelle von Anführungszeichen.
Fügen Sie in Ihrem Texteditor die folgenden hervorgehobenen Zeilen ein, um ein Attribut zu referenzieren:
In diesem Code haben Sie eine Variable über der return-Anweisung namens greeting mit dem Wert von "greeting" erstellt und dann die Variable im Attribut id Ihres < h1 > -Tags referenziert.
Die Seite wird gleich sein, hat aber ein id-Tag.
Seite mit hervorgehobenem id-Tag in den Entwickler-Tools
Bisher haben Sie mit einigen wenigen Elementen einzeln gearbeitet, aber Sie können JSX auch verwenden, um viele HTML-Elemente zur Erstellung komplexer Seiten hinzuzufügen und zu verschachteln.
Um dies zu demonstrieren, erstellen Sie eine Seite mit einer Liste von Emojis.
Diese Emojis werden von einem < button > -Element umgeben.
Wenn Sie auf das Emoji klicken, erhalten Sie dessen CLDR-Kurznamen.
Zu Beginn müssen Sie einige weitere Elemente in die Seite einfügen.
Öffnen Sie src / App.js in Ihrem Texteditor.
Lassen Sie ihn während dieses Schritts geöffnet.
Fügen Sie zunächst eine Liste von Emojis hinzu, indem Sie die folgenden hervorgehobenen Zeilen einfügen:
Sie haben hier ein < ul > -Tag erstellt, um eine Liste von Emojis zu umfassen.
Jedes Emoji ist in einem separaten < li > -Element und von einem < button > -Element umgeben.
Im nächsten Schritt fügen Sie diesem Button ein Ereignis hinzu.
Zudem haben Sie das Emoji mit einem < span > -Tag umgeben, das einige weitere Attribute hat.
Jedes span hat das Attribut role, das auf die Rolle img gesetzt ist.
Dadurch wird barrierefreier Software signalisiert, dass sich das Element wie ein Bild verhält.
Außerdem verfügt jedes < span > auch über ein aria-label und ein id-Attribut mit dem Namen des Emojis.
Das aria-label sagt Besuchern mit Bildschirmlesern, was angezeigt wird.
Beim Schreiben von Ereignissen im nächsten Schritt verwenden Sie die id.
Wenn Sie auf diese Weise Code schreiben, verwenden Sie semantische Elemente, die dazu beitragen, dass die Seite barrierefrei und verständlich für Bildschirmleser ist.
Ihr Browser wird aktualisiert und Sie sehen Folgendes:
Browser mit Emojis im Stil einer Liste
Fügen Sie nun etwas Styling hinzu.
Öffnen Sie den CSS-Code in Ihrem Texteditor:
Fügen Sie den folgenden hervorgehobenen Code hinzu, um den Standard-Hintergrund und die Ränder der Buttons zu entfernen und um die Schriftgröße zu erhöhen:
In diesem Code haben Sie font-size, border und andere Parameter verwendet, um das Aussehen Ihrer Buttons anzupassen und die Schriftart zu ändern.
Sie haben auch den Listenstil entfernt und display: flex in das < ul > -Element eingefügt, um eine horizontale Ausführung zu erhalten.
Speichern und schließen Sie die CSS-Datei.
Liste mit Standardstilen entfernt
Sie haben nun mit mehreren JSX-Elementen gearbeitet, die wie reguläres HTML aussehen.
Sie haben Klassen, IDs und aria-Tags eingefügt sowie mit Daten als Zeichenfolgen und Variablen gearbeitet.
React verwendet aber auch Attribute, um zu definieren, wie Ihre Elemente auf Benutzerereignisse reagieren sollen.
Im nächsten Schritt beginnen Sie, die Seite interaktiver zu machen, indem Sie dem Button Ereignisse hinzufügen.
Schritt 3 - Hinzufügen von Ereignissen zu Elemente
In diesem Schritt fügen Sie mit speziellen Attributen Ereignisse zu Elementen hinzu und erfassen ein Klick-Ereignis auf einem Button-Element.
Sie lernen, wie Sie Informationen von dem Ereignis erfassen, um eine andere Aktion zu senden oder andere Informationen im Rahmen der Datei zu verwenden.
Sie haben nun eine grundlegende Seite mit Informationen und können nun einige Ereignisse zu diesen hinzufügen. Es gibt zahlreiche Event-Handler, die Sie zu HTML-Elementen hinzufügen können.
Mit React können Sie auf all diese zugreifen.
Da Ihr JavaScript-Code mit Ihrem Markup gekoppelt ist, können Sie die Ereignisse schnell einfügen und gleichzeitig Ihren Code gut organisiert halten.
Zunächst fügen Sie den Event Handler onclick hinzu.
Dadurch können Sie einen JavaScript-Code direkt in Ihr Element einfügen, anstatt einen Event-Listener anzufügen:
Da es sich um JSX handelt, haben Sie onclick mit einer Binnenmajuskel versehen, d. h. Sie haben es als onClick eingefügt.
Das Attribut onClick verwendet eine anonyme Funktion, um Informationen über das Element abzurufen, das angeklickt wurde.
Sie haben eine anonyme Pfeilfunktion hinzugefügt, die das Ereignis von dem angeklickten Button abruft. Das Ereignis wird als Ziel das < span > -Element haben.
Die Information, die Sie benötigen, ist im id-Attribut, auf das Sie mit event.target.id zugreifen können.
Sie können den Hinweis mit der Funktion alert () auslösen.
Klicken Sie in Ihrem Browser auf ein Emoji und Sie erhalten einen Hinweis mit dem Namen.
Hinweis für Party Popper
Sie können Wiederholungen reduzieren, indem Sie die Funktion einmal deklarieren und sie an jede onClick-Aktion übergeben.
Da die Funktion nur auf Ein- und Ausgaben angewiesen ist, können Sie sie außerhalb der Hauptkomponentenfunktion deklarieren.
Mit anderen Worten, die Funktion muss nicht auf den Anwendungsbereich der Komponente zugreifen.
Der Vorteil einer Auseinanderhaltung besteht darin, dass Ihre Komponentenfunktion etwas kürzer ist und Sie die Funktion später in eine separate Datei verschieben können, wenn Sie möchten.
Erstellen Sie in Ihrem Texteditor eine Funktion namens displayEmojiName, die das Ereignis nimmt und die Funktion alert () mit einer ID aufruft. Dann übergeben Sie die Funktion an jedes onClick-Attribut:
Klicken Sie in Ihrem Browser auf ein Emoji und Sie werden den gleichen Hinweis sehen.
In diesem Schritt haben Sie jedem Element Ereignisse hinzugefügt.
Sie haben auch gesehen, wie JSX leicht abweichende Namen für Elementereignisse verwendet und begonnen, wiederverwendbaren Code zu schreiben, indem Sie die Funktion genommen und in mehreren Elementen erneut verwendet haben.
Im nächsten Schritt schreiben Sie eine wiederverwendbare Funktion, die JSX-Elemente ausgibt, anstatt jedes Element per Hand zu schreiben. Dadurch werden Wiederholungen noch weiter reduziert.
Schritt 4 - Mapping von Daten zur Erstellung von Elementen
In diesem Schritt gehen Sie über die Verwendung von JSX als einfaches Markup hinaus.
Sie lernen, es mit JavaScript zu kombinieren, um dynamisches Markup zu erstellen, das Code reduziert und die Lesbarkeit verbessert.
Sie gestalten Ihren Code in ein Array um, über das Sie eine Schleife ausführen, um HTML-Elemente zu erstellen.
JSX beschränkt Sie nicht auf eine HTML-ähnliche Syntax.
Es bietet Ihnen auch die Möglichkeit, JavaScript direkt in Ihrem Markup zu verwenden.
Das haben Sie bereits ausprobiert, indem Sie Funktionen an Attribute übergeben haben.
Sie haben auch Variablen zur Wiederverwendung von Daten verwendet. Nun können Sie JSX direkt aus Daten unter Verwendung von Standard-JavaScript-Code erstellen.
Sie müssen in Ihrem Texteditor ein Array der Emoji-Daten in der Datei src / App.js erstellen.
Öffnen Sie die Datei, falls Sie diese geschlossen haben:
Fügen Sie ein Array ein, das Objekte enthält, die das Emoji und den Namen des Emojis beinhalten.
Beachten Sie, dass Emojis von Anführungszeichen umgeben sein müssen.
Erstellen Sie dieses Array über der App-Funktion:
Nachdem Sie nun über die Daten verfügen, können Sie eine Schleife über diese ausführen. Um JavaScript innerhalb von JSX zu verwenden, müssen Sie es mit geschweiften Klammern umschließen: {}.
Es ist das Gleiche wie bei den Funktionen, die Sie in Attribute eingefügt haben.
Um React-Komponenten zu erstellen, müssen Sie die Daten in JSX-Elemente konvertieren.
Dazu führen Sie ein Mapping der Daten aus und geben ein JSX-Element aus.
Es gibt ein paar Dinge, die Sie beim Schreiben des Codes beachten müssen.
Erstens muss eine Gruppe von Elementen von einem < div > -Container umgeben sein.
Zweitens benötigt jedes Element eine spezielle Eigenschaft namens key - Schlüssel.
Der key muss ein eindeutiges Datenstück sein. Mit diesem kann React die Elemente verfolgen und dadurch erfahren, wann die Komponente aktualisiert werden muss.
Der Schlüssel wird aus dem kompilierten HTML ausgelassen, da er nur internen Zwecken dient.
Wann immer Sie mit Schleifen arbeiten, müssen Sie eine einfache Zeichenfolge als Schlüssel einfügen.
Hier ist ein vereinfachtes Beispiel, das eine Liste von Namen auf ein enthaltendes < div > abbildet:
Das resultierende HTML würde so aussehen:
Die Konvertierung der Emoji-Liste ist ähnlich.
Das < ul > ist der Container.
Sie führen ein Mapping von Daten aus und geben ein < li > mit einem Schlüssel des Kurznamens des Emojis aus.
Sie ersetzen die hartcodierten Daten in den Tags < button > und < span > durch Informationen aus der Schleife.
Fügen Sie Folgendes in Ihrem Texteditor ein:
Im Code haben Sie über das emojis-Array im < ul > -Tag gemappt und ein < li > ausgegeben.
In jedem < li > haben Sie den Namen des Emojis als key-Prop verwendet.
Der Button wird die gleiche Funktion wie normalerweise haben.
Ersetzen Sie im Element < span > das aria-label und id mit dem name.
Der Inhalt des < span > -Tags sollte das Emoji sein.
Ihr Fenster wird aktualisiert und Sie sehen die Daten. Beachten Sie, dass der Schlüssel nicht in dem generierten HTML vorhanden ist.
Browser mit Entwickler-Tools, die das aktualisierte HTML ohne key-Props zeigen
Das Kombinieren von JSX mit Standard JavaScript bietet Ihnen eine Menge Werkzeuge, um Inhalte dynamisch zu erstellen, und Sie können jedes gewünschte Standard-JavaScript verwenden.
In diesem Schritt haben Sie hartkodiertes JSX durch ein Array und eine Schleife ersetzt, um HTML dynamisch zu erstellen.
Im nächsten Schritt zeigen Sie Informationen bedingt an, indem Sie die Kurzschlussevaluierung anwenden.
Schritt 5 - Bedingtes Anzeigen von Elementen mit Kurzschlussevaluierung
In diesem Schritt verwenden Sie die Kurzschlussevaluierung, um bestimmte HTML-Elemente unter bestimmten Bedingungen anzuzeigen.
Auf diese Weise können Sie Komponenten erstellen, die HTML auf der Grundlage zusätzlicher Informationen aus- oder einblenden können, wodurch Ihre Komponenten flexibel auf verschiedene Situationen reagieren können.
In einigen Situationen kann es notwendig sein, dass eine Komponente Informationen in bestimmten Fällen anzeigt und in anderen nicht.
Beispielsweise möchten Sie vielleicht nur dann eine Hinweismeldung für den Benutzer anzeigen, wenn bestimmte Fälle zutreffen, oder Sie möchten eventuell einige Account-Informationen für einen Admin anzeigen, die ein normaler Benutzer nicht sehen soll.
Dazu verwenden Sie die Kurzschlussevaluierung.
Das bedeutet, dass Sie eine Bedingung verwenden, und wenn der erste Teil zutrifft, werden die Informationen im zweiten Teil ausgegeben.
Hier folgt ein Beispiel.
Wenn Sie nur dann einen Button anzeigen wollen, wenn der Benutzer eingeloggt ist, umschließen Sie das Element mit geschweiften Klammern und fügen die Bedingung davor ein.
In diesem Beispiel verwenden Sie den & & -Operator, der den letzten Wert ausgibt, wenn alles zutrifft.
Andernfalls gibt es false aus, was React anweist, kein zusätzliches Markup auszugeben.
Wenn isLoggedIn zutrifft ist, zeigt React den Button.
Wenn isLoggedIn nicht zutrifft, wird der Button nicht angezeit.
Fügen Sie zum Ausprobieren die folgenden hervorgehobenen Zeilen ein:
Sie haben in Ihrem Texteditor eine Variable namens displayAction mit einem Wert von false erstellt.
Dann haben Sie das < p > -Tag mit geschweiften Klammern umschlossen.
Am Anfang der geschweiften Klammern haben Sie displayAction & & eingefügt, um die Bedingung zu erstellen.
Speichern Sie die Datei und Sie sehen, dass das Element in Ihrem Browser verschwindet.
Wichtig ist auch, dass es auch nicht in dem generierten HTML erscheint.
Dies ist nicht dasselbe wie das Ausblenden eines Elements mit CSS.
Es wird im endgültigen Markup überhaupt nicht vorhanden sein.
Browser mit Entwickler-Tools, die kein Paragrafenelement anzeigen
Im Moment ist der Wert von displayAction hartkodiert, aber Sie können diesen Wert auch als einen State speichern oder als Prop aus einer übergeordneten Komponente übergeben.
In diesem Schritt haben Sie gelernt, wie man bedingt Elemente anzeigt.
Dadurch können Sie Komponenten erstellen, die auf der Grundlage anderer Informationen anpassbar sind.
Bis hierher haben Sie eine benutzerdefinierte Anwendung mit JSX erstellt.
Sie haben gelernt, wie man HTML-ähnliche Elemente in Ihre Komponente einfügt, diesen Elementen ein Styling gibt, Attribute übergibt, um semantisches und barrierefreies Markup zu erstellen, und Events in die Komponenten eingefügt.
Dann haben Sie JavaScript in Ihr JSX gemischt, um Wiederholungen im Code zu verringern und um Elemente bedingt ein- und auszublenden.
Das ist die Grundlage, die Sie benötigen, um zukünftige Komponenten zu erstellen.
Mit einer Kombination aus JavaScript und HTML können Sie dynamische Komponenten erstellen, die flexibel sind und es Ihrer Anwendung ermöglichen, zu wachsen und sich zu verändern.
Wenn Sie mehr über React erfahren möchten, besuchen Sie unsere Themenseite React.
Der CSS-Code wurde bereits mit der Zeile import '. / App.css' importiert.
Installieren von Git unter CentOS 8
4084
Versionsverwaltungssysteme sind ein unverzichtbarer Teil der modernen Softwareentwicklung.
Durch Versionierung können Sie Ihre Software auf der Quellebene verfolgen.
Sie können Änderungen verfolgen, zu früheren Stadien zurückkehren und verzweigen, um alternative Versionen von Dateien und Verzeichnissen zu erstellen.
Eines der beliebtesten Versionsverwaltungssysteme derzeit ist Git.
Dateien aus vielen Projekten werden in einem Git-Repository und Sites wie GitHub, GitLab und Bitbucket verwaltet, um das Teilen und Zusammenarbeiten bei Softwareentwicklungsprojekten zu erleichtern.
In diesem Leitfaden erfahren Sie, wie Sie Git auf einem CentOS 8-Server installieren und konfigurieren können.
Wir behandeln die Installation der Software auf zwei verschiedene Arten: über den integrierten Paketmanager und über Quellcode.
Jeder dieser Ansätze bietet je nach Ihren Bedürfnissen eigene Vorteile.
Sie benötigen einen CentOS 8-Server mit einem non-root-Superuser-Konto.
Installieren von Git mit Standardpaketen
Unsere erste Option zur Installation von Git umfasst CentOS-Standardpakete.
Diese Option eignet sich am besten für Benutzer, die schnell mit Git loslegen möchten, eine verbreitete stabile Version bevorzugen oder keine der neuesten verfügbaren Funktionen benötigen.
Wenn Sie nach der aktuellsten Version suchen, sollten Sie zum Abschnitt Installieren aus Quellcode springen.
Wir verwenden das Open-Source-basierte Paketmanager-Tool DNF, was für Dandified YUM, die nächste Generation des Yellowdog Updater, Modified (YUM), steht.
DNF ist ein Paketmanager, der nun der Standard-Paketmanager für Red Hat-basierte Linux-Systeme wie CentOS ist.
Damit können Sie Softwarepakete auf Ihrem Server installieren, aktualisieren und entfernen.
Verwenden Sie zunächst die APT-Paketmanagement-Tools zur Aktualisierung Ihres lokalen Paketindex.
Das Flag -y weist das System darauf hin, das wir wissen, dass wir Änderungen vornehmen. Damit wird verhindert, dass uns das Terminal zur Bestätigung der Änderungen auffordert.
Nach abgeschlossener Aktualisierung können Sie Git installieren:
Sie können prüfen, ob Sie Git richtig installiert haben, indem Sie folgenden Befehl ausführen:
Nach erfolgreicher Installation von Git können Sie nun mit dem Abschnitt Einrichten von Git dieses Tutorials fortfahren, um Ihre Einrichtung fertigzustellen.
Installieren von Git aus Quellcode
Eine flexiblere Methode zur Installation von Git besteht darin, die Software aus Quellcode zu kompilieren.
Das dauert länger und wird nicht von Ihrem Paketmanager verwaltet, ermöglicht Ihnen aber, die neueste Version herunterzuladen. Außerdem können Sie zwischen einigen Optionen wählen, wenn Sie Anpassungen vornehmen möchten.
Bevor Sie beginnen, müssen Sie die Software installieren, auf die Git angewiesen ist.
Diese ist vollständig in den Standard-Repositorys verfügbar, sodass wir unseren lokalen Paketindex aktualisieren und dann die Pakete installieren können.
Nachdem Sie die erforderlichen Abhängigkeiten installiert haben, erstellen Sie ein temporäres Verzeichnis und wechseln dorthin. Hier laden wir unseren Git-Tarball herunter.
Von der Git-Projekt-Website können wir zu der Tarball-Liste der Red Hat Linux-Distribution navigieren, die unter https: / / mirrors.edge.kernel.org / pub / software / scm / git / verfügbar ist, und die gewünschte Version herunterladen.
Zum Zeitpunkt der Verfassung dieses Textes ist die aktuellste Version 2.26.0, sodass wir diese zu Veranschaulichungszwecken herunterladen.
Wir verwenden curl und geben die Datei, die wir herunterladen, in git.tar.gz aus.
Entpacken Sie die komprimierte Tarball-Datei:
Als Nächstes gehen Sie in das neue Git-Verzeichnis:
Jetzt können Sie das Paket erstellen und installieren, indem Sie diese beiden Befehle eingeben:
Nach abgeschlossener Ausführung können Sie sehen, ob Ihre Installation erfolgreich war, indem Sie die Version überprüfen.
Nach erfolgreicher Installation von Git können Sie nun Ihre Einrichtung abschließen.
Einrichten von Git
Nachdem Sie Git installiert haben, sollten Sie es nun konfigurieren, damit die erzeugten commit-Nachrichten Ihre richtigen Informationen enthalten.
Dies lässt sich durch Verwendung des Befehls git config erreichen.
Insbesondere müssen wir unseren Namen und unsere E-Mail-Adresse angeben, da Git diese Informationen in jedem unserer Commits einbettet.
Wir können fortfahren und diese Informationen einfügen, indem wir Folgendes eingeben:
Wir können alle Konfigurationselemente anzeigen, die wir festgelegt haben, indem wir Folgendes eingeben:
Die Informationen, die Sie eingeben, werden in Ihrer Git-Konfigurationsdatei gespeichert. Diese können Sie bei Bedarf mit einem Texteditor wie dem folgenden bearbeiten:
Drücken Sie ESC und dann: q, um den Texteditor zu beenden.
Es gibt viele weitere Optionen, die Sie festlegen können. Diese beiden sind aber obligatorisch.
Wenn Sie diesen Schritt überspringen, werden wahrscheinlich Warnungen angezeigt, wenn Sie einen Commit für Git ausführen.
Dadurch werden Sie mehr Arbeit haben, da Sie dann die von Ihnen vorgenommenen Commits mit den korrigierten Informationen korrigieren müssen.
Sie sollten Git nun installiert haben und bereit sein, Git in Ihrem System zu benutzen.
Weitere Informationen zur Verwendung von Git finden Sie in diesen Artikeln und Reihen:
Installieren von Python 3 und Einrichten einer Programmierumgebung unter CentOS 8
4083
Python ist eine vielseitige Programmiersprache, die für viele verschiedene Programmierprojekte verwendet werden kann.
Inspiriert von der britischen Comedy-Gruppe Monty Python wollte das Entwicklungsteam von Python eine Sprache kreieren, die Spaß macht.
Python ist eine zunehmend beliebte Sprache für verschiedenste Anwendungen und sowohl für Anfänger als auch erfahrene Entwickler bestens geeignet.
Dieses Tutorial führt Sie durch die Installation von Python 3 auf einem CentOS 8-Cloudserver und die Einrichtung einer Programmierumgebung über die Befehlszeile.
Schritt 1 - Vorbereiten des Systems
Bevor wir mit der Installation beginnen, sollten wir die Standardanwendungen des Systems aktualisieren, um sicherzustellen, dass wir über die neuesten Versionen verfügen.
Lassen Sie uns zunächst sicherstellen, dass unser Paketmanager aktuell ist, indem wir diesen Befehl ausführen:
Sobald alles installiert ist, ist unsere Einrichtung verfügbar und wir können Python 3 installieren.
Schritt 2 - Installieren und Einrichten von Python 3
CentOS ist von RHEL (Red Hat Enterprise Linux) abgeleitet und zeichnet sich durch hohe Stabilität aus.
Daher sind getestete und stabile Versionen von Anwendungen das, was im System und in herunterladbaren Paketen am häufigsten zu finden ist. Wenn Sie den CentOS-Paketmanager nutzen, finden Sie also ältere Versionen von Python (und nicht die aktuellste Version).
Sobald dieser Vorgang abgeschlossen ist, können wir überprüfen, ob die Installation erfolgreich war, indem wir mit dem Befehl python3 die Versionsnummer überprüfen:
Nach erfolgreicher Installation einer Python 3-Version erhalten wir die folgende Ausgabe:
Als Nächstes installieren wir die CentOS-Entwicklungstools, die dazu dienen, Software aus Quellcode zu erstellen und zu kompilieren:
Nach der Installation sehen wir uns im nächsten Abschnitt an, wie sich Python-Entwicklungsprojekte einrichten lassen.
Schritt 3 - Einrichten einer virtuellen Umgebung
Nachdem Python installiert und unser System eingerichtet ist, können wir unter Verwendung von venv nun mit der Einrichtung unserer Programmierumgebung fortfahren.
Mit virtuellen Umgebungen können Sie isolierten Platz auf Ihrem Computer für Python-Projekte schaffen, sodass jedes Ihrer Projekte eine eigene Gruppe von Abhängigkeiten aufweist, die andere Projekte nicht stören.
Die Einrichtung einer Programmierumgebung bietet uns mehr Kontrolle über unsere Python-Projekte sowie über verschiedene Pakete und Versionen.
Jede Umgebung ist im Grunde genommen ein Verzeichnis oder Ordner auf Ihrem Server, der Skripte zum Einrichten einer Umgebung enthält.
Wählen Sie ein Verzeichnis, in das Sie Ihre Python-Programmierumgebungen einfügen möchten, oder erstellen mit mkdir wie folgt ein neues Verzeichnis:
Sobald Sie sich im Verzeichnis befinden, in dem Sie die Umgebungen einrichten möchten, können Sie eine Umgebung erstellen, indem Sie den folgenden Befehl ausführen.
Sie sollten einen Umgebungsnamen verwenden, der für Sie sinnvoll ist. Hier nennen wir die Umgebung my _ env.
In diesem Fall heißt die Umgebung < ^ > my _ env < ^ > und das neue Verzeichnis enthält einige Elemente, die wir anzeigen können, indem wir in diesem Verzeichnis den Befehl ls verwenden:
Zusammen erlauben es diese Dateien, Ihre Python-Arbeit vom breiteren Kontext Ihres lokalen Rechners zu isolieren, damit sich Systemdateien und Projektdateien nicht vermischen.
Um diese Umgebung zu benutzen, müssen Sie sie aktivieren. Das können Sie tun, indem Sie den folgenden Befehl eingeben, der das Skript activate im bin-Verzeichnis aufruft:
Ihrer Eingabeaufforderung wird nun der Name Ihrer Umgebung vorangestellt. In diesem Fall heißt sie < ^ > my _ env < ^ >:
Der Python-Paketmanager pip ist bereits installiert.
Als Tool für die Verwendung mit Python nutzen wir pip, um Programmierpakete zu installieren und zu verwalten, die wir ggf. in unseren Entwicklungsprojekten verwenden möchten.
Sie können Python-Pakete installieren, indem Sie Folgendes eingeben:
Wenn Sie NumPy installieren möchten, können Sie dies mit dem Befehl pip install numpy tun.
< $> note Anmerkung: Innerhalb der virtuellen Umgebung können Sie den Befehl python anstelle von python3 und pip anstelle von pip3 verwenden.
Wenn Sie Python 3 oder pip3 auf Ihrem Rechner außerhalb einer Umgebung verwenden, müssen Sie ausschließlich die Befehle python3 und pip3 verwenden.
Schritt 4 - Erstellen eines "Hello, World!" -
Programms
Nachdem wir unsere virtuelle Umgebung eingerichtet haben, erstellen wir nun das traditionelle "Hello, World!" -
Programm zum Testen unserer Installation.
Dadurch wird sichergestellt, dass unsere Umgebung funktioniert, und wir können uns mit Python vertrauter machen, wenn wir es nicht bereits sind.
Dazu öffnen wir einen Befehlszeileneditor wie vi und erstellen eine neue Datei:
Sobald sich die Textdatei in unserem Terminalfenster öffnet, müssen wir i eingeben, um den Einfügemodus zu öffnen. Dann können wir unser erstes Programm schreiben:
Drücken Sie nun ESC, um den Einfügemodus zu verlassen.
Geben Sie als Nächste: x ein und drücken Sie ENTER, um die Datei zu speichern und zu schließen.
Wir sind nun bereit, unser Programm auszuführen:
Das von Ihnen erstellte Programm hello.py sollte dazu führen, dass im Terminal die folgende Ausgabe angezeigt wird:
Sie haben auf Ihrem CentOS 8-Server eine Python 3-Programmierumgebung eingerichtet und können nun mit einem Programmierprojekt beginnen!
Da Ihr Rechner bereit für die Softwareentwicklung ist, können Sie mehr über die Codierung in Python erfahren, indem Sie unseren Reihen Codieren in Python folgen oder das kostenlose eBook Codieren in Python herunterladen.
Um mehr über Projekte insbesondere für maschinelles Lernen zu erfahren, konsultieren Sie unser eBook Machine-Learning-Projekte mit Python.
Ausführen verschiedener PHP-Versionen auf einem Server unter Verwendung von Apache und PHP-FPM unter Debian 10
4049
Ein Debian 10-Server mit mindestens 1 GB RAM, der anhand der Ersteinrichtung des Servers unter Debian 10 eingerichtet wurde, einschließlich eines sudo non-root user und einer Firewall.
Ein Apache-Webserver, der anhand von Installieren des Apache-Webservers unter Debian 10 eingerichtet und konfiguriert wurde.
Ein Domänenname, der so konfiguriert ist, dass er auf Ihren Debian 10-Server verweist.
Dazu müssen Sie jedoch zunächst das sury php-Repository zu Ihrem System hinzufügen.
Installieren Sie zunächst mehrere erforderliche Pakete, einschließlich curl, wget und gnupg2:
Mit den obigen Paketen können Sie sicher auf das sury php-Repository zugreifen. sury php ist ein Drittanbieter-Repository oder PPA (Personal Package Archive).
Es stellt PHP 7.4, 7.3, 7.2, 7.1 und 7.0 für das Debian-Betriebssystem bereit.
Außerdem bietet es aktuellere Versionen von PHP als die offiziellen Debian 10-Repositorys und Sie können verschiedene Versionen von PHP im gleichen System installieren.
Als Nächstes importieren Sie den Schlüssel des Pakets:
Fügen Sie nun das Repository sury php Ihrem System hinzu:
Installieren Sie php7.2, php7.2-fpm, php7.2-mysql und libapache2-mod-php7.2.
Und überprüfen Sie dann den Status des php7.2-fpm-Dienstes:
Jetzt verfügen Sie über einen einzelnen Debian 10-Server, der zwei Websites mit zwei verschiedenen PHP-Versionen handhabt.
Einrichten und Konfigurieren eines OpenVPN-Servers unter CentOS 8
3993
In diesem Tutorial richten Sie OpenVPN auf einem CentOS 8-Server ein und konfigurieren es anschließend so, dass von einem Client-Computer aus darauf zugegriffen werden kann.
Einen CentOS 8-Server mit einem sudo non-root user und eine aktivierte Firewall.
Um ihn einzurichten, können Sie unserem Tutorial Ersteinrichtung des Servers unter CentOS 8 folgen.
Einen separaten CentOS 8-Server, der als private Zertifizierungsstelle (" Certificate Authority ", CA) eingerichtet ist. Diesen bezeichnen wird in diesem Leitfaden als CA-Server.
Nachdem Sie die Schritte aus dem Leitfaden zur Ersteinrichtung des Servers auf diesem Server ausgeführt haben, können Sie dazu den Schritten 1 bis 3 unseres Leitfadens Einrichten und Konfigurieren einer Zertifizierungsstelle (CA) unter CentOS 8 folgen.
Aus diesem Grund wird in diesem Leitfaden davon ausgegangen, dass sich Ihre CA auf einem separaten CentOS 8-Server befindet, der auch einen non-root user mit sudo-Berechtigungen und eine einfache Firewall aufweist.
Wenn diese Voraussetzungen erfüllt sind, können Sie mit der Einrichtung und Konfiguration eines OpenVPN-Servers unter CentOS 8 beginnen.
Anweisungen zur Ausführung dieser Lösungen finden Sie unter Einrichten von SSH-Schlüsseln unter CentOS 8.
Jedoch sind OpenVPN und Easy-RSA in CentOS 8 nicht standardmäßig verfügbar, daher müssen Sie das Repository "Extra Packages for Enterprise Linux" (EPEL) aktivieren.
Melden Sie sich bei Ihrem OpenVPN-Server als der non-root sudo user an, den Sie während der anfänglichen Einrichtungsschritte erstellt haben, und führen Sie Folgendes aus:
Zuerst wechseln Sie mit cd in das Verzeichnis easy-rsa, dann erstellen und bearbeiten Sie die Datei vars mit nano oder Ihrem bevorzugten Texteditor.
Diese Zeilen stellen sicher, dass Ihre privaten Schlüssel und Zertifikatsanforderungen so konfiguriert sind, dass sie moderne Elliptische-Kurven-Kryptografie (ECC) nutzen, um Schlüssel und sichere Signaturen für Ihre Clients und OpenVPN-Server zu erzeugen.
Melden Sie sich jetzt beim CA-Server als non-root user an, dem das easy-rsa-Verzeichnis gehört, in dem Sie Ihre PKI erstellt haben.
Importieren Sie die Zertifikatsanforderung mit dem easyrsa-Skript:
Als Nächstes möchten wir, dass OpenVPN nach seinem Start ohne Berechtigungen läuft. Daher müssen wir ihm sagen, dass es mit einem Benutzer und einer Gruppe von nobody läuft.
Um dies zu aktivieren, finden Sie die Zeilen mit user nobody und group nobody und kommentieren Sie sie aus, indem Sie das; -Zeichen am Anfang jeder Zeile entfernen:
Fügen Sie dann am Anfang der Datei die folgende Zeile hinzu:
Wenn Sie den Voraussetzungen am Anfang dieses Tutorials gefolgt sind, sollte firewalld bereits installiert sein und auf Ihrem Server laufen.
Um OpenVPN durch die Firewall zu lassen, müssen Sie wissen, welche Ihre aktive firewalld-Zone ist.
Finden Sie das mit dem folgenden Befehl:
Wenn Sie keine trusted-Zone sehen, die die Schnittstelle tun0 listet, führen Sie die folgenden Befehle aus, um das VPN-Gerät zu dieser Zone hinzuzufügen:
Fügen Sie anschließend den Dienst openvpn in die Liste von Diensten ein, die von firewalld in Ihrer aktiven Zone zugelassen werden. Legen Sie diese Einstellung dauerhaft fest, indem Sie den Befehl erneut ausführen, diesmal jedoch mit der hinzugefügten Option --permanent:
Um die Änderungen an der Firewall anzuwenden, führen Sie Folgendes aus:
Sie können nun mit dem folgenden Befehl überprüfen, ob der Dienst korrekt hinzugefügt wurde:
Als Nächstes fügen wir der Firewall eine Masquerade-Regel hinzu.
Masquerading ermöglicht Ihrem OpenVPN-Server, die Adressen Ihrer OpenVPN-Clients in die eigene öffentliche Adresse des Servers zu übersetzen und dann das Gegenteil mit dem Verkehr zu tun, der an die Clients zurückgeschickt wird.
Dieser Vorgang ist auch als Netzwerkadressübersetzung (NAT), bekannt.
Fügen Sie Masquerade-Regeln mit den folgenden Befehlen hinzu:
Mit diesem Befehl können Sie überprüfen, ob die Masquerade korrekt hinzugefügt wurde:
Als Nächstes müssen Sie die spezifische Masquerade-Regel nur für Ihr OpenVPN-Subnetz erstellen.
Sie können dies tun, indem Sie zunächst eine Shell-Variable (in unserem Beispiel < ^ > DEVICE < ^ >) erstellen, die die primäre Netzwerkschnittstelle repräsentiert, die von Ihrem Server verwendet wird, und dann diese Variable verwenden, um die Leitungsregel dauerhaft hinzuzufügen:
Achten Sie darauf, firewalld neu zu laden, damit alle Ihre Änderungen wirksam werden:
Die Befehle mit dem Flag --permanent stellen sicher, dass die Regeln über Reboots hinweg bestehen bleiben.
Der Befehl firewall-cmd --reload stellt sicher, dass alle ausstehenden Änderungen an der Firewall angewendet werden.
Folgen Sie dazu dem Beispiel im Abschnitt Widerrufen eines Zertifikats im voraussetzenden Tutorial Erstellen und Konfigurieren einer Zertifizierungsstelle unter CentOS 8.
Sobald das Profil hinzugefügt wurde, sehen Sie einen Bildschirm wie diesen:
Sie sehen Echtzeitstatistiken der Verbindung und des Datenverkehrs, der über Ihren OpenVPN-Server geleitet wird:
Die mit dem VPN verbundene OpenVPN-App für Android
Installieren und Konfigurieren von Ansible unter Ubuntu 18.04 Schnellstart
4763
In diesem Leitfaden erörtern wir, wie Sie Ansible auf einem Ubuntu 18.04-Server installieren und konfigurieren können.
Eine ausführlichere Version dieses Tutorials mit genaueren Erklärungen zu den einzelnen Schritten finden Sie unter Installieren und Konfigurieren von Ansible unter Ubuntu 18.04.
Einen Ansible-Kontrollknoten: ein Ubuntu 18.04-System, in dem Ansible installiert wird.
Das kann ein Remote-Server oder ein lokaler Computer sein.
Einen oder mehrere Ansible-Hosts: ein oder mehrere Ubuntu 18.04-Server, die von Ihrem Kontrollknoten über SSH zugänglich sind.
Schritt 1 - Installieren von Ansible
Führen Sie von Ihrem Kontrollknoten den folgenden Befehl aus, um das offizielle PPA (persönliche Paketarchiv) des Projekts in die Liste von Quellen Ihres Systems einzuschließen:
Aktualisieren Sie den Paketindex Ihres Systems mit:
Nach diesem Update können Sie die Ansible-Software installieren mit:
Schritt 2 - Einrichten der Inventardatei
Um den Inhalt Ihres standardmäßigen Ansible-Inventars zu bearbeiten, öffnen Sie die Datei / etc / ansible / hosts mit Ihrem bevorzugten Texteditor:
Die bei der Ansible-Installation bereitgestellte Standardinventardatei enthält eine Reihe von Beispielen, die Sie als Referenz zur Einrichtung Ihres Inventars verwenden können.
Das folgende Beispiel definiert eine Gruppe namens [servers] mit drei verschiedenen Servern darin, die jeweils durch einen benutzerdefinierten Alias identifiziert werden: server1, server2 und server3.
Achten Sie darauf, die hervorgehobenen IP-Adressen durch die IP-Adressen Ihrer Ansible-Hosts zu ersetzen.
Die Untergruppe all: vars legt den ansible _ python _ interpreter-Hostparameter fest, der für alle Hosts in diesem Inventar gültig ist.
Dieser Parameter stellt sicher, dass der Remote-Server die ausführbare Datei / usr / bin / python3 anstelle von / usr / bin / python (Python 2.7) verwendet, die in neuen Ubuntu-Versionen nicht mehr vorhanden sind.
Vergessen Sie nicht, die Datei zu speichern und zu schließen, wenn Sie fertig sind.
Schritt 3 - Testen der Verbindung
Sie können das Argument -u verwenden, um den Remote-Systembenutzer anzugeben.
Bei fehlender Angabe wird Ansible versuchen, sich auf dem Kontrollknoten als Ihr aktueller Systembenutzer zu verbinden.
Führen Sie von Ihrem Ansible-Kontrollknoten Folgendes aus:
Sie sollten in etwa folgende Ausgabe sehen:
Wenn Sie zum ersten Mal über SSH eine Verbindung mit diesen Servern herstellen, werden Sie aufgefordert, die Authentizität der Hosts zu bestätigen, mit denen Sie über Ansible eine Verbindung herstellen.
Geben Sie in der Eingabeaufforderung yes ein und drücken Sie dann ENTER zur Bestätigung.
Sobald Sie von einem Host einen "pong" zurückerhalten, sind Sie bereit, Ansible-Befehle und -Playbooks auf diesem Server auszuführen.
Installieren und Konfigurieren von Ansible unter Ubuntu 18.04
Verwenden von Ansible: ein Referenzhandbuch
Konfigurationsmanagement 101: Verfassen von Ansible-Playbooks
Installieren und Konfigurieren von Elasticsearch unter Ubuntu 18.04
5242
Einen Ubuntu 18.04-Server mit 4 GB RAM und 2 CPUs, eingerichtet mit einem non-root user, der über sudo-Berechtigungen verfügt.
Sie können dies erreichen, indem Sie Ersteinrichtung eines Servers unter Ubuntu 18.04 ausführen.
Um Elasticsearch zu konfigurieren, werden wir die Konfigurationsdateien bearbeiten.
Elasticsearch verfügt über drei Konfigurationsdateien:
elasticsearch.yml zur Konfiguration von Elasticsearch (die Hauptkonfigurationsdatei)
jvm.options zur Konfiguration der Java Virtual Machine (JVM) -Einstellungen von Elasticsearch
log4j2.properties zur Konfiguration der Protokollierung von Elasticsearch
In diesem Tutorial interessieren wir uns für die Datei elasticsearch.yml, in der die meisten Konfigurationsoptionen gespeichert sind.
Diese Firewall sollte bereits aktiviert sein, wenn Sie die Schritte im vorbereitenden Tutorial Ersteinrichtung eines Servers mit Ubuntu 18.04 ausgeführt haben.
Wenn Sie die Regeln richtig angegeben haben, sollte die Ausgabe wie folgt aussehen:
Seit der ersten Version von Elasticsearch hat Elastic drei zusätzliche Tools (Logstash, Kabana und Beats) entwickelt, die im Rahmen des Elastic Stack zusammen mit Elasticsearch eingesetzt werden können.
Bei gemeinsamer Nutzung können Sie mit diesen Tools Protokolle, die von beliebigen Quellen in beliebigen Formaten erzeugt wurden, durchsuchen, analysieren und visualisieren, was in der Praxis als zentralisierte Protokollierung bekannt ist.
Informationen zu den ersten Schritten mit dem Elastic Stack unter Ubuntu 18.04 finden Sie in unserem Leitfaden Installieren von Elasticsearch, Logstash und Kibana (Elastic Stack) unter Ubuntu 18.04.
Installieren eines Linux-, Apache-, MariaDB- und PHP- (LAMP-) Stacks unter CentOS 8 Schnellstart
4762
In diesem Tutorial installieren Sie einen LAMP-Stack auf einem CentOS 8-Server.
Zwar ist MySQL über die Standard-Repositorys in CentOS 8 verfügbar, doch beschreibt dieser Leitfaden den Prozess der Einrichtung eines LAMP-Stacks mit MariaDB als Datenbank-Managementsystem.
Eine detailliertere Version dieses Tutorials mit weiteren Erklärungen zu jedem Schritt finden Sie unter Installieren eines Linux-, Apache-, MySQL- und PHP- (LAMP-) Stacks unter CentOS 8.
Schritt 4 - Testen von PHP mit Apache
Der folgende Befehl ändert die Eigentümerschaft des standardmäßigen Apache-Dokumentstamms auf einen Benutzer und eine Gruppe namens < ^ > sammy < ^ >:
Zuerst möchten Sie ggf. nano installieren, einen benutzerfreundlicheren Texteditor, da dieser von CentOS 8 nicht standardmäßig installiert wird:
Um zu testen, ob unser Webserver von einem PHP-Skript generierte Inhalte richtig anzeigen kann, rufen Sie in Ihrem Browser den Hostnamen oder die IP-Adresse Ihres Servers auf, gefolgt von / info.php:
Installieren eines Linux-, Apache-, MySQL- und PHP- (LAMP-) Stacks unter CentOS 8.
Installieren von MySQL unter Ubuntu 20.04 Schnellstart
5244
MySQL ist ein Open-Source-basiertes Datenbank-Managementsystem, das üblicherweise als Teil des beliebten LAMP-Stacks (Linux, Apache, MySQL, PHP / Python / Perl) installiert wird.
Es implementiert das relationale Modell und verwendet zur Verwaltung der Daten Structured Query Language (besser bekannt als SQL).
In diesem Schnellstart-Tutorial wird beschrieben, wie Sie MySQL-Version 8 auf einem Ubuntu 20.04-Server installieren.
Einen Ubuntu 20.04-Server mit einem administrativen non-root user und eine mit UFW konfigurierte Firewall.
Folgen Sie zur Einrichtung unserem Leitfaden zur Ersteinrichtung eines Servers für Ubuntu 20.04.
Um MySQL zu installieren, aktualisieren Sie zuerst den Paketindex Ihres Servers, wenn Sie dies in letzter Zeit nicht getan haben:
Installieren Sie anschließend das Paket mysql-server:
Schritt 2 - Konfigurieren von MySQL
Führen Sie mit sudo das in MySQL enthaltene Sicherheitsskript aus:
Dieses Skript führt Sie durch eine Reihe von Aufforderungen, in denen Sie verschiedene Änderungen an den Sicherheitseinstellungen Ihrer MySQL-Einrichtung vornehmen können.
Wenn Sie sich für die Einrichtung das Plugin Validate Password entscheiden, werden Sie von dem Skript gefragt, ob Sie eine Password-Validierungsstufe auswählen möchten, wobei 0 die schwächste und 2 die stärkste ist:
In der nächsten Eingabeaufforderung geben Sie ein Passwort für den MySQL-root user ein und bestätigen es:
Wenn Sie mit Ihrem Passwort zufrieden sind, geben Sie Y ein, um mit dem Skript fortzufahren:
Um als root ein Passwort für die Verbindung mit MySQL zu verwenden, müssen Sie die Authentifizierungsmethode vom standardmäßigen Plugin auth _ socket in ein anderes Plugin ändern, wie zum Beispiel caching _ sha2 _ password oder mysql _ native _ password.
Führen Sie von dort eine ALTER USER-Anweisung aus, um das verwendete Authentifizierungs-Plugin zu ändern, und legen Sie ein neues Passwort fest.
Vergessen Sie nicht, < ^ > password < ^ > in ein starkes Passwort Ihrer Wahl zu ändern, und beachten Sie, dass dieser Befehl das root-Passwort, das Sie in Schritt 2 festgelegt haben, ändern wird:
< $> note Anmerkung: caching _ sha2 _ password ist das bevorzugte Authentifizierungs-Plugin von MySQL, da es eine sicherere Passwortverschlüsselung bietet als das ältere, aber immer noch verbreitete mysql _ native _ password.
Allerdings funktionieren viele PHP-Anwendungen (beispielsweise phpMyAdmin) mit caching _ sha2 _ password nicht zuverlässig.
Wenn Sie diese Datenbank mit einer PHP-Anwendung verwenden möchten, wollen Sie root ggf. so festlegen, dass stattdessen mit mysql _ native _ password authentifiziert wird:
Alternativ können Sie mit einem dedizierten Benutzer anstelle von root eine Verbindung zu MySQL herstellen.
Um einen solchen Benutzer zu erstellen, öffnen Sie erneut die MySQL-Shell:
< $> note Anmerkung: Wenn Sie für root die Passwortauthentifizierung aktiviert haben (wie in den vorherigen Absätzen beschrieben), müssen Sie stattdessen Folgendes ausführen:
Erstellen Sie von dort einen neuen Benutzer und geben Sie ihm ein starkes Passwort:
Erteilen Sie dann Ihrem neuen Benutzer die entsprechenden Berechtigungen.
Mit diesem Befehl können Sie dem Benutzer zum Beispiel Berechtigungen für alle Tabellen in der Datenbank sowie Berechtigungen zum Hinzufügen, Ändern und Entfernen von Benutzerberechtigungen erteilen:
Danach können Sie die MySQL-Shell verlassen:
Sie haben nun eine grundlegende MySQL-Einrichtung auf Ihrem Server installiert.
Hier sehen Sie einige Beispiele für nächste Schritte:
Einrichten eines LAMP-Stacks
Installieren von MySQL unter Ubuntu 20.04
5240
Eine frühere Version dieses Tutorials wurde von Hazel Virdó verfasst.
In diesem Tutorial wird beschrieben, wie Sie MySQL-Version 8 auf einem Ubuntu 20.04-Server installieren können.
Nach Abschluss des Tutorials verfügen Sie über eine funktionierende relationale Datenbank, mit der Sie Ihre nächste Website oder Anwendung erstellen können.
Unter Ubuntu 20.04 können Sie MySQL mithilfe des APT-Paket-Repository installieren.
Zum Zeitpunkt der Verfassung dieses Dokuments ist die im standardmäßigen Ubuntu-Repository verfügbare Version < ^ > 8.0.19 < ^ >.
Aktualisieren Sie zum Installieren den Paketindex auf Ihrem Server, wenn Sie es in letzter Zeit nicht getan haben:
Dadurch wird MySQL installiert; Sie werden jedoch nicht dazu aufgefordert, ein Passwort festzulegen oder andere Konfigurationsänderungen vorzunehmen.
Da damit Ihre Installation von MySQL unsicher bleibt, kümmern wir uns als Nächstes um das Thema Sicherheit.
Bei neuen Installationen von MySQL sollten Sie das im DBMS enthaltene Sicherheitsskript ausführen.
Das Skript ändert gewisse weniger sichere Standardeinstellungen bei Aspekten wie Remote-Root-Login und Beispielbenutzern.
Führen Sie das Sicherheitsskript mit sudo aus:
Beachten Sie, dass Sie zwar ein Passwort für den MySQL-Benutzer root festgelegt haben, dieser Benutzer jedoch nicht so konfiguriert ist, dass er beim Verbinden mit der MySQL-Shell per Passwort authentifiziert wird.
Wenn Sie möchten, können Sie diese Einstellung durch Ausführung von Schritt 3 anpassen.
In Ubuntu-Systemen, die MySQL 5.7 (oder höher) ausführen, ist der MySQL-Benutzer root so konfiguriert, dass er nicht per Passwort, sondern standardmäßig mit dem Plugin auth _ socket authentifiziert wird.
Um als root ein Passwort für die Verbindung mit MySQL zu verwenden, müssen Sie die Authentifizierungsmethode vom Plugin auth _ socket in ein anderes Plugin ändern, wie zum Beispiel caching _ sha2 _ password oder mysql _ native _ password.
Um das root-Konto so zu konfigurieren, dass eine Authentifizierung mit Passwort erfolgt, führen Sie eine ALTER USER-Anweisung aus, um zu ändern, welches Authentifizierungs-Plugin verwendet wird, und legen Sie ein neues Passwort fest.
< $> note Anmerkung: Die vorherige ALTER USER-Anweisung konfiguriert den MySQL-Benutzer root so, dass er mit dem Plugin caching _ sha2 _ password authentifiziert wird.
Gemäß der offiziellen MySQL-Dokumentation ist caching _ sha2 _ password das bevorzugte Authentifizierungs-Plugin von MySQL, da es eine sicherere Passwortverschlüsselung bietet als das ältere, aber immer noch verbreitete Plugin mysql _ native _ password.
In dieser Beispielausgabe können Sie sehen, dass der MySQL-Benutzer root jetzt mit caching _ sha2 _ password authentifiziert wird.
Manchmal ist es hilfreicher, sich über einen dedizierten Benutzer mit MySQL zu verbinden.
< $> note Anmerkung: Wenn Sie für root die Passwortauthentifizierung aktiviert haben, wie in den vorherigen Absätzen beschrieben, müssen Sie einen anderen Befehl für den Zugriff auf die MySQL-Shell verwenden.
Der folgende Befehl sorgt dafür, dass Ihr MySQL-Client mit regulären Benutzerberechtigungen ausgeführt wird; Sie erhalten nur dann Administratorberechtigungen in der Datenbank, wenn Sie sich authentifizieren:
Beachten Sie, dass Sie an dieser Stelle den Befehl FLUSH PRIVILEGES nicht erneut ausführen müssen.
Der Befehl wird nur benötigt, wenn Sie die Berechtigungstabellen mit Anweisungen wie INSERT, UPDATE oder DELETE ändern.
Da Sie einen neuen Benutzer erstellt haben, anstatt einen vorhandenen Benutzer zu verändern, ist FLUSH PRIVILEGES hier nicht erforderlich.
Beenden Sie anschließend die MySQ-Shell:
Lassen Sie uns die MySQL-Installation abschließend testen.
Schritt 4 - Testen von MySQL
Unabhängig davon, wie Sie es installiert haben, sollte MySQL automatisch gestartet worden sein.
Wenn MySQL nicht ausgeführt wird, können Sie es mit sudo systemctl start mysql starten.
Dieser Befehl besagt zum Beispiel, dass eine Verbindung mit MySQL als root (-u root) hergestellt, nach einem Passwort gefragt (-p) und die Version zurückgegeben wird.
Das bedeutet, dass MySQL jetzt ausgeführt wird.
Erstellen einer Webanwendung mit Flask in Python 3
4100
Flask ist ein kleines und schlankes Python-Web-Framework mit nützlichen Tools und Funktionen, die das Erstellen von Webanwendungen in Python erleichtern.
Es bietet Entwicklern mehr Flexibilität und ist ein besser zugängliches Framework für neue Entwickler, da Sie Webanwendungen schnell unter Verwendung einer einzigen Python-Datei erstellen können.
Außerdem ist Flask erweiterbar und setzt keine bestimmte Verzeichnisstruktur oder komplizierte Codebausteine voraus, bevor Sie loslegen können.
Als Teil dieses Tutorials nutzen Sie das Bootstrap-Toolkit, um Ihre Anwendung so zu gestalten, dass sie optisch ansprechender aussieht.
Bootstrap wird Ihnen helfen, responsive Webseiten in Ihre Webanwendung zu integrieren, sodass sie problemlos auch mit mobilen Browsern funktioniert, ohne dass Sie dafür eigenen HTML-, CSS- und JavaScript-Code schreiben müssen.
Mit dem Toolkit können Sie sich darauf konzentrieren, die Funktionsweise von Flask zu erlernen.
Flask verwendet die Jinja-Vorlagen-Engine für das dynamische Einrichten von HTML-Seiten mit bekannten Python-Konzepten wie Variablen, Schleifen, Listen usw.
Sie werden diese Vorlagen im Rahmen dieses Projekts nutzen.
In diesem Tutorial entwickeln Sie einen kleinen Weblog mit Flask und SQLite in Python 3. Benutzer der Anwendung können alle Beiträge in Ihrer Datenbank anzeigen und auf den Titel eines Beitrags klicken, um dessen Inhalt anzuzeigen und bei Bedarf der Datenbank einen neuen Beitrag hinzuzufügen bzw. einen bestehenden Beitrag zu bearbeiten oder zu löschen.
Bevor Sie mit diesem Leitfaden fortfahren, benötigen Sie Folgendes:
Eine lokale Python-3-Programmierumgebung; folgen Sie dem Tutorial für Ihre Distribution in der Reihe Installieren und Einrichten einer lokalen Programmierumgebung für Python 3 für Ihren lokalen Rechner.
In diesem Tutorial nennen wir unser Projektverzeichnis flask _ blog.
Ein Verständnis von Python 3-Konzepten, darunter von Datentypen, Bedingungsanweisungen, for-Schleifen, Funktionen und anderen solchen Konzepten.
Wenn Sie nicht mit Python vertraut sind, sehen Sie sich unsere Reihe Codieren in Python 3 an.
Schritt 1 - Installieren von Flask
In diesem Schritt aktivieren Sie Ihre Python-Umgebung und installieren Flask mit dem Package Installer pip.
Wenn Sie Ihre Programmierumgebung noch nicht aktiviert haben, stellen Sie sicher, dass Sie sich in Ihrem Projektverzeichnis befinden (< ^ > flask _ blog < ^ >) und den folgenden Befehl nutzen, um die Umgebung zu aktivieren:
Sobald Ihre Programmierumgebung aktiviert ist, weist Ihre Eingabeaufforderung das Präfix < ^ > env < ^ > auf, was wie folgt aussieht:
Dieses Präfix ist ein Hinweis darauf, dass die Umgebung env derzeit aktiv ist; sie trägt möglicherweise einen anderen Namen, je nach dem Namen, den Sie der Umgebung bei der Erstellung gegeben haben.
< $> note Anmerkung: Sie können Git, ein System für die Versionskontrolle, verwenden, um den Entwicklungsprozess für Ihr Projekt zu verwalten und zu verfolgen.
Um mehr über die Verwendung von Git zu erfahren, lesen Sie unseren Artikel Einleitung zu Installation, Nutzung und Verzweigungen von Git.
Wenn Sie Git verwenden, ist es eine gute Idee, das neu erstellte Verzeichnis env in Ihrer Datei .gitignore zu ignorieren, um eine Verfolgung von Dateien zu vermeiden, die nicht mit dem Projekt in Verbindung stehen.
Nun installieren Sie Python-Pakete und isolieren Ihren Projektcode abseits von der Hauptinstallation des Python-Systems.
Sie werden das mithilfe von pip und python tun.
Um Flask zu installieren, führen Sie den folgenden Befehl aus:
Sobald die Installation abgeschlossen ist, führen Sie den folgenden Befehl aus, um die Installation zu überprüfen:
Sie nutzen die python-Befehlszeilenschnittstelle mit der Option -c, um Python-Code auszuführen.
Als Nächstes importieren Sie das flask-Paket mit import flask und drucken dann die Flask-Version, die über die Variable flask. _ _ version _ _ verfügbar ist.
Die Ausgabe wird eine Versionsnummer sein, die der folgenden ähnelt:
Sie haben den Projektordner und eine virtuelle Umgebung eingerichtet sowie Flask installiert.
Sie können nun mit der Einrichtung Ihrer Basisanwendung fortfahren.
Schritt 2 - Erstellen einer Basisanwendung
Nachdem Sie Ihre Programmierumgebung eingerichtet haben, beginnen Sie nun mit der Verwendung von Flask.
In diesem Schritt erstellen Sie eine kleine Webanwendung in einer Python-Datei und führen sie aus, um den Server zu starten. Dadurch werden im Browser verschiedene Informationen angezeigt.
Öffnen Sie in Ihrem Verzeichnis flask _ blog eine Datei namens hello.py zum Bearbeiten; verwenden Sie dazu nano oder Ihren bevorzugten Texteditor:
Die Datei hello.py wird als Minimalbeispiel für die Handhabung von HTTP-Anfragen dienen.
Darin werden Sie das Flask-Objekt importieren und eine Funktion erstellen, die eine HTTP-Antwort zurückgibt.
Schreiben Sie den folgenden Code in hello.py:
Im vorherigen Codeblock importieren Sie zuerst das Flask-Objekt aus dem flask-Paket.
Anschließend nutzen Sie es, um Ihre Flask-Anwendungsinstanz mit dem Namen app zu erstellen. Sie übergeben die spezielle Variable _ _ name _ _, die den Namen des aktuellen Python-Moduls enthält.
Damit wird der Instanz mitgeteilt, wo sie sich befindet. Das ist nötig, da Flask einige Pfade im Hintergrund einrichtet.
Nachdem Sie die Instanz app erstellt haben, verwenden Sie sie zum Handhaben eingehender Webanfragen und zum Senden von Antworten an den Benutzer. @ app.route ist ein Decorator, der eine reguläre Python-Funktion in eine Flask-Anzeigefunktion verwandelt. Diese verwandelt den Rückgabewert der Funktion in eine HTTP-Antwort, die von einem HTTP-Client (wie einem Webbrowser) angezeigt wird.
Sie übergeben den Wert '/' an @ app.route (), um anzugeben, dass diese Funktion auf Webanfragen bezüglich der URL / reagiert. Dabei handelt es sich um die Haupt-URL.
Die Anzeigefunktion hello () gibt die Zeichenfolge 'Hello, World!'
als Antwort zurück.
Um Ihre Webanwendung auszuführen, teilen Sie Flask zuerst mit, wo sich die Anwendung (in Ihrem Fall die Datei hello.py) befindet. Nutzen Sie dazu die Umgebungsvariable FLASK _ APP:
Führen Sie sie dann im Entwicklungsmodus mit der Umgebungsvariable FLASK _ ENV aus:
Führen Sie die Anwendung abschließend mit dem Befehl flask run aus:
Sobald die Anwendung ausgeführt wird, sieht die Ausgabe in etwa wie folgt aus:
Die vorangehende Ausgabe weist mehrere Informationen auf, zum Beispiel:
Der Name der Anwendung, die Sie ausführen.
Die Umgebung, in der die Anwendung ausgeführt wird.
Debug mode: on bedeutet, dass der Flask-Debugger ausgeführt wird.
Das ist bei der Entwicklung nützlich, da wir detaillierte Fehlermeldungen erhalten, wenn etwas schiefgeht. Das erleichtert die Fehlerbehebung.
Die Anwendung wird lokal an der URL http: / / 127.0.0.1: 5000 / ausgeführt; 127.0.0.1 ​ ​ ist die IP-Adresse, die den localhost Ihres Computers darstellt, während: 5000 die Portnummer ist.
Öffnen Sie einen Browser und geben Sie die URL http: / / 127.0.0.1: 5000 / ein; Sie erhalten die Zeichenfolge Hello, World!
als Antwort. Das bestätigt, dass Ihre Anwendung erfolgreich ausgeführt wird.
< $> warning Warnung Flask verwendet einen einfachen Webserver, um unsere Anwendung in einer Entwicklungsumgebung bereitzustellen. Das bedeutet auch, dass der Flask-Debugger ausgeführt wird, um die Erkennung von Fehlern zu erleichtern.
Dieser Entwicklungsserver sollte nicht in Produktionsbereitstellungen verwendet werden.
Weitere Informationen finden Sie auf der Seite Deploymment Options in der Flask-Dokumentation. Sie können sich auch dieses Flask-Bereitstellungs-Tutorial ansehen.
Sie können den Entwicklungsserver nun im Terminal laufen lassen und ein anderes Terminalfenster öffnen.
Gehen Sie zum Projektordner, in dem sich hello.py befindet, aktivieren Sie die virtuelle Umgebung, setzen Sie die Umgebungsvariablen FLASK _ ENV und FLASK _ APP und fahren Sie mit den nächsten Schritten fort.
(Diese Befehle sind in diesem Schritt zuvor aufgelistet.)
< $> note Anmerkung: Beim Öffnen eines neuen Terminals ist es wichtig, die virtuelle Umgebung zu aktivieren und die Umgebungsvariablen FLASK _ ENV und FLASK _ APP festzulegen.
Wenn bereits ein Entwicklungsserver einer Flask-Anwendung ausgeführt wird, ist es nicht möglich, eine weitere Flask-Anwendung mit dem gleichen Befehl flask run auszuführen.
Das liegt daran, dass flask run die Portnummer 5000 standardmäßig verwendet. Sobald sie vergeben ist, ist sie nicht mehr zur Ausführung einer weiteren Anwendung verfügbar. Sie erhalten einen Fehler, der dem folgenden ähnelt:
Um dieses Problem zu lösen, stoppen Sie entweder den gerade ausgeführten Server mit STRG + C und führen flask run erneut aus. Wenn Sie beide Server gleichzeitig ausführen möchten, können Sie eine andere Portnummer an das Argument -p übergeben. Um zum Beispiel eine weitere Anwendung an Port 5001 auszuführen, verwenden Sie den folgenden Befehl:
Sie verfügen nun über eine kleine Flask-Webabwendung.
Sie haben Ihre Anwendung ausgeführt und Informationen im Webbrowser angezeigt.
Als Nächstes nutzen Sie in Ihrer Anwendung HTML-Dateien.
Schritt 3 - Verwenden von HTML-Vorlagen
Derzeit zeigt Ihre Anwendung nur eine einfache Meldung ohne HTML an.
Webanwendungen nutzen HTML hauptsächlich zum Anzeigen von Informationen für den Besucher. Daher arbeiten Sie nun an der Einbindung von HTML-Dateien in Ihre App, die im Webbrowser angezeigt werden können.
Flask bietet eine render _ template () -Hilfsfunktion, die eine Verwendung der Jinja-Vorlagen-Engine ermöglicht.
Dadurch wird das Verwalten von HTML wesentlich erleichtert, da Ihr HTML-Code in .html-Dateien geschrieben und in Ihrem HTML-Code Logik verwendet wird.
Sie werden diese HTML-Dateien (templates) zum Erstellen aller Ihrer Anwendungsseiten nutzen. Dazu gehören zum Beispiel die Hauptseite, auf der Sie die aktuellen Blogbeiträge anzeigen, die Seite des Blogbeitrags, die Seite, auf der Benutzer einen neuen Beitrag hinzufügen können, und so weiter.
In diesem Schritt erstellen Sie Ihre Flask-Hauptanwendung in einer neuen Datei.
Verwenden Sie zuerst in Ihrem Verzeichnis flask _ blog nano oder Ihren bevorzugten Editor, um Ihre Datei app.py zu erstellen und zu bearbeiten.
Diese wird den gesamten Code enthalten, den Sie zum Erstellen der Bloganwendung verwenden.
In dieser neuen Datei importieren Sie das Flask-Objekt, um eine Flask-Anwendungsinstanz zu erstellen, wie Sie dies zuvor getan haben.
Außerdem importieren Sie die render _ template () -Hilfsfunktion, mit der Sie im Ordner templates, den Sie gleich erstellen werden, vorhandene HTML-Vorlagendateien rendern können.
Die Datei wird eine Einzelansichtfunktion aufweisen, die für die Bearbeitung von Anfragen an die Hauptroute / zuständig ist ​ ​.
Die Ansichtsfunktion index () gibt das Ergebnis des Aufrufs von render _ template () mit index.html als Argument zurück. Das teilt render _ template () mit, nach einer Datei namens index.html im Ordner templates zu suchen.
Sowohl der Ordner als auch die Datei existieren noch nicht. Sie erhalten einen Fehler, wenn Sie die Anwendung an dieser Stelle ausführen würden.
Sie werden sie trotzdem ausführen, damit Sie sich mit dieser häufig auftretenden Ausnahme vertraut machen können.
Dann beheben Sie die Ausnahme durch Erstellung des erforderlichen Ordners und der Datei.
Stoppen Sie den Entwicklungsserver im anderen Terminal, in dem die Anwendung hello ausgeführt wird, mit STRG + C.
Bevor Sie die Anwendung ausführen, stellen Sie sicher, dass Sie den Wert für die Umgebungsvariable FLASK _ APP richtig angeben, da Sie nicht mehr die Anwendung hello verwenden:
Durch Öffnen der URL http: / / 127.0.0.1: 5000 / in Ihrem Browser wird die Debugger-Seite angezeigt, die Ihnen mitteilt, dass die Vorlage index.html nicht gefunden wurde.
Die Hauptzeile im Code, die für diesen Fehler verantwortlich war, wird hervorgehoben.
In diesem Fall ist es die Zeile return render _ template (' index.html ') ​.
Wenn Sie auf diese Zeile klicken, zeigt der Debugger weiteren Code an, damit Sie über mehr Kontext zum Lösen des Problems verfügen.
Der Flask-Debugger
Um diesen Fehler zu beheben, erstellen Sie ein Verzeichnis namens templates in Ihrem Verzeichnis flask _ blog.
Öffnen Sie dann darin eine Datei namens index.html zum Bearbeiten:
Fügen Sie anschließend in index.html den folgenden HTML-Code hinzu:
Speichern Sie die Datei und navigieren Sie in Ihrem Browser erneut zu http: / / 127.0.0.1: 5000 / oder aktualisieren Sie die Seite.
Dieses Mal sollte der Browser den Text Welcome to FlaskBlog in einem < h1 > -Tag anzeigen.
Zusätzlich zum Ordner templates weisen Flask-Webanwendungen typischerweise einen Ordner static zum Hosting von statischen Dateien auf, wie z. B. CSS-Dateien, JavaScript-Dateien und Bilder, die die Anwendung verwendet.
Sie können eine Stylesheetdatei style.css erstellen, um CSS Ihrer Anwendung hinzuzufügen.
Erstellen Sie zuerst ein Verzeichnis namens static im Hauptverzeichnis flask _ blog:
Erstellen Sie dann ein anderes Verzeichnis namens css im Verzeichnis static, um .css-Dateien zu hosten.
Das dient normalerweise der Organisation von statischen Dateien in dedizierten Ordnern. So befinden sich JavaScript-Dateien meist in einem Verzeichnis namens js, Bilder in einem Verzeichnis namens images (oder img) und so weiter.
Der folgende Befehl erstellt das Verzeichnis css im Verzeichnis static:
Öffnen Sie dann eine style.css-Datei im css-Verzeichnis zur Bearbeitung:
Fügen Sie Ihrer Datei style.css die folgende CSS-Regel hinzu:
Der CSS-Code fügt einen Rahmen hinzu, ändert die Farbe in braun, zentriert den Text und fügt < h1 > -Tags ein wenig Abstand hinzu.
Öffnen Sie als Nächstes die Vorlagendatei index.html zur Bearbeitung:
Sie fügen der Datei style.css im Abschnitt < head > der Vorlagendatei index.html einen Link hinzu:
Hier verwenden Sie die Hilfsfunktion url _ for (), um den entsprechenden Ort der Datei zu generieren.
Das erste Argument gibt an, dass Sie mit einer statischen Datei verknüpfen, und das zweite Argument ist der Pfad der Datei im static-Verzeichnis.
Wenn Sie die Indexseite Ihrer Anwendung aktualisieren, werden Sie bemerken, dass der Text Welcome to FlaskBlog jetzt braun, zentriert und in einen Rahmen eingeschlossen ist.
Sie können die CSS-Sprache verwenden, um den Stil der Anwendung zu verändern und nach Ihrem eigenen Geschmack attraktiver zu gestalten.
Wenn Sie jedoch kein Webdesigner sind oder nicht mit CSS vertraut sind, können Sie das Bootstrap-Toolkit verwenden, das anwenderfreundliche Komponenten für die Anpassung des Stils Ihrer Anwendung bereitstellt.
In diesem Projekt verwenden wir Bootstrap.
Vielleicht haben Sie sich schon gedacht, dass die Einrichtung einer weiteren HTML-Vorlage eine Wiederholung des größten Teils des HTML-Codes bedeutet, den Sie in der Vorlage index.html bereits geschrieben haben.
Sie können mithilfe einer Basisvorlagen-Datei, aus der alle Ihre HTML-Dateien erben werden, unnötige Codewiederholungen vermeiden.
Weitere Informationen finden Sie in Vorlagenvererbung in Jinja.
Um eine Basisvorlage einzurichten, erstellen Sie zuerst eine Datei namens base.html in Ihrem Verzeichnis templates:
Geben Sie in Ihrer Vorlage base.html den folgenden Code ein:
Der größte Teil des Codes im vorherigen Block ist Standard-HTML und Code, der für Bootstrap benötigt wird.
Die < meta > -Tags bieten Informationen für den Webbrowser, das < link > -Tag verknüpft die Bootstrap CSS-Dateien und die < script > -Tags sind Links zu JavaScript-Code, der einige zusätzliche Bootstrap-Funktionen bietet. Konsultieren Sie die Bootstrap-Dokumentation, um mehr zu erfahren.
Die folgenden hervorgehobenen Teile sind jedoch für die Jinja-Vorlagen-Engine spezifisch:
< ^ > {% block title%} {% endblock%} < ^ >: Ein Block, der als Platzhalter für einen Titel dient; Sie werden ihn später in anderen Vorlagen nutzen, um einen benutzerdefinierten Titel für jede Seite in Ihrer Anwendung zu vergeben, ohne den gesamten Abschnitt < head > neu schreiben zu müssen.
< ^ > {{url _ for (' index ')}} < ^ >: Ein Funktionsaufruf, der die URL für die Ansichtsfunktion index () zurückgibt.
Das unterscheidet sich von dem vorherigen Aufruf url _ for (), den Sie verwendet haben, um eine statische CSS-Datei zu verknüpfen. Er weist nur ein Argument auf, bei dem es sich um den Namen der Ansichtsfunktion handelt, und verknüpft mit der Route, die mit der Funktion verbunden ist (anstelle einer statischen Datei).
< ^ > {% block content%} {% endblock%} < ^ >: Ein weiterer Block, der durch Inhalt ersetzt wird, je nach der untergeordneten Vorlage (Vorlagen, die von base.html erben), die ihn überschreiben wird.
Nachdem Sie nun über eine Basisvorlage verfügen, können Sie sie mithilfe von Vererbung nutzen.
Öffnen Sie die Datei index.html:
Ersetzen Sie die Inhalte mit Folgendem:
In dieser neuen Version der Vorlage index.html verwenden Sie das {% extends%} -Tag, um von der Vorlage base.html zu erben.
Dann erweitern Sie sie, indem Sie den content-Block in der Basisvorlage durch das ersetzen, was innerhalb des content-Blocks im vorherigen Codeblock enthalten ist.
Dieser content-Block enthält ein < h1 > -Tag mit dem Text Welcome to FlaskBlog in einem title-Block, der wiederum den ursprünglichen title-Block in der Vorlage base.html durch den Text Welcome to FlaskBlog ersetzt.
So müssen Sie den gleichen Text nicht zweimal wiederholen, da er sowohl als Titel für die Seite als auch Überschrift dienen kann, die unterhalb der Navigationsleiste erscheint, geerbt von der Basisvorlage.
Außerdem bietet Ihnen Vererbung mit Vorlagen die Möglichkeit, den HTML-Code, über den Sie in anderen Vorlagen verfügen (in diesem Fall base.html), wiederzuverwenden, ohne ihn jedes Mal wiederholen zu müssen, wenn er benötigt wird.
Speichern und schließen Sie die Datei und aktualisieren Sie die Indexseite in Ihrem Browser.
Sie werden Ihre Seite mit einer Navigationsleiste und einem formatierten Titel sehen.
Indexseite mit Bootstrap
Sie haben in Flask HTML-Vorlagen und statische Dateien verwendet.
Außerdem haben Sie Bootstrap genutzt, um die Optik Ihrer Seite und eine Basisvorlage anzupassen, um Codewiederholungen zu vermeiden.
Im nächsten Schritt richten Sie eine Datenbank ein, die Ihre Anwendungsdaten speichern wird.
Schritt 4 - Einrichten der Datenbank
In diesem Schritt richten Sie eine Datenbank zur Speicherung von Daten ein, d. h. die Blogbeiträge für Ihre Anwendung.
Außerdem befüllen Sie die Datenbank mit einigen Beispieleinträgen.
Sie werden eine SQLite-Datenbankdatei verwenden, um Ihre Daten zu speichern, da das Modul sqlite3, das wir zur Interaktion mit der Datenbank verwenden, in der Python-Standardbibliothek einsatzbereit ist.
Weitere Informationen zu SQLite finden Sie in diesem Tutorial.
Da Daten in SQLite in Tabellen und Spalten gespeichert werden und Ihre Daten hauptsächlich aus Blogbeiträgen bestehen, müssen Sie zunächst eine Tabelle namens posts mit den erforderlichen Spalten erstellen.
Sie werden eine .sql-Datei erstellen, die SQL-Befehle enthält, um die Tabelle posts mit einigen Spalten zu erstellen.
Dann verwenden Sie diese Datei zur Erstellung der Datenbank.
Öffnen Sie eine Datei namens schema.sql in Ihrem Verzeichnis flask _ blog:
Geben Sie in dieser Datei die folgenden SQL-Befehle ein:
Der erste SQL-Befehl ist DROP TABLE IF EXISTS posts; - damit werden alle bereits vorhandenen Tabellen namens posts gelöscht, damit Sie nicht durcheinander kommen.
Beachten Sie, dass bei Nutzung dieser SQL-Befehle stets alle in der Datenbank vorhandenen Inhalte gelöscht werden. Schreiben Sie also keine wichtigen Inhalte in die Webanwendung, bis Sie dieses Tutorial abgeschlossen und mit dem Endergebnis experimentiert haben.
Als Nächstes verwenden Sie CREATE TABLE posts zur Erstellung der Tabelle posts mit den folgenden Spalten:
id: Eine ganze Zahl, die einen Primärschlüssel darstellt; diesem wird von der Datenbank für jeden Eintrag (also jeden Blogbeitrag) ein eindeutiger Wert zugewiesen.
created: Die Zeit, zu der der Blogbeitrag erstellt wurde.
NOT NULL bedeutet, dass diese Spalte nicht leer sein darf und der DEFAULT-Wert der CURRENT _ TIMESTAMP-Wert ist. Das ist der Zeitpunkt, zu dem der Blogbeitrag der Datenbank hinzugefügt wurde.
Genau wie bei id müssen Sie keinen Wert für diese Spalte angeben, da er automatisch ausgefüllt wird.
title: Der Titel des Beitrags.
content: Der Inhalt des Beitrags.
Nachdem Sie in der Datei schema.sql über ein SQL-Schema verfügen, verwenden Sie es nun zur Erstellung der Datenbank mit einer Python-Datei, die eine SQLite .db-Datenbank generieren wird.
Öffnen Sie mit Ihrem bevorzugten Editor eine Datei namens init _ db.py im Verzeichnis flask _ blog:
Und fügen Sie dann den folgenden Code hinzu.
Zuerst importieren Sie das Modul sqlite3 und öffnen dann eine Verbindung zu einer Datenbankdatei namens database.db, die erstellt wird, sobald Sie die Python-Datei ausführen.
Dann verwenden Sie die Funktion open (), um die Datei schema.sql zu öffnen.
Als Nächstes führen Sie ihre Inhalte mit der Methode executescript () aus, die mehrere SQL-Anweisungen auf einmal ausführt. Dadurch wird die Tabelle posts erstellt.
Sie erstellen ein Cursor-Objekt, mit dem Sie die Methode execute () verwenden können, um zwei INSERT SQL-Anweisungen auszuführen und Ihrer Tabelle posts zwei Blogbeiträge hinzuzufügen.
Schließlich committen Sie die Änderungen und schließen die Verbindung.
Speichern und schließen Sie die Datei und führen Sie sie dann im Terminal mit dem python-Befehl aus:
Sobald die Ausführung der Datei beendet ist, wird in Ihrem Verzeichnis flask _ blog eine neue Datei namens database.db angezeigt.
Das bedeutet, dass Sie Ihre Datenbank erfolgreich eingerichtet haben.
Im nächsten Schritt rufen Sie die Beiträge ab, die Sie in Ihre Datenbank eingefügt haben, und zeigen sie auf der Homepage Ihrer Anwendung an.
Schritt 5 - Anzeigen aller Beiträge
Nachdem Sie Ihre Datenbank eingerichtet haben, können Sie die Ansichtsfunktion index () nun so ändern, dass sie alle Beiträge anzeigt, die Sie in Ihrer Datenbank haben.
Öffnen Sie die Datei app.py, um die folgenden Änderungen vorzunehmen:
Als erste Änderung importieren Sie das Modul sqlite3 an den Anfang der Datei:
Als Nächstes erstellen Sie eine Funktion, die eine Datenbankverbindung erstellt und zurückgibt. Fügen Sie sie direkt nach den Importen hinzu:
Diese get _ db _ connection () -Funktion öffnet eine Verbindung zur Datenbankdatei database.db und legt dann das Attribut row _ factory auf sqlite3.
Row fest, damit Sie namenbasierten Zugriff auf Spalten erhalten.
Das bedeutet, dass die Datenbankverbindung Zeilen zurückgibt, die sich wie reguelmäßige Python-Wörterbücher verhalten.
Schließlich gibt die Funktion das Verbindungsobjekt conn zurück, das Sie zum Zugriff auf die Datenbank verwenden werden.
Nach der Definition der get _ db _ connection () -Funktion ändern Sie die Funktion index (), damit sie wie folgt aussieht:
In dieser neuen Version der Funktion index () öffnen Sie zuerst mit der Funktion get _ db _ connection (), die Sie zuvor definiert haben, eine Datenbankverbindung.
Dann führen Sie eine SQL-Abfrage aus, um alle Einträge aus der Tabelle posts auszuwählen.
Sie implementieren die Methode fetchall (), um alle Zeilen des Abfrageergebnisses abzurufen. Dadurch wird eine Liste der Beiträge, die Sie der Datenbank im vorherigen Schritt hinzugefügt haben, zurückgegeben.
Sie schließen die Datenbankverbindung mit dem Befehl close () und geben das Ergebnis vom Rendern der Vorlage index.html zurück.
Außerdem übergeben Sie das Objekt posts als Argument, das die Ergebnisse enthält, die Sie aus der Datenbank erhalten haben. Das ermöglicht Ihnen, auf die Blogbeiträge in der Vorlage index.html zuzugreifen.
Speichern und schließen Sie die Datei app.py nach Vornahme der Änderungen.
Nachdem Sie die Beiträge, die Sie von der Datenbank abgerufen haben, an die Vorlage index.html übergeben haben, können Sie eine for-Schleife verwenden, um einzelne Beiträge auf Ihrer Indexseite anzuzeigen.
Ändern Sie sie dann so, dass sie wie folgt aussieht:
Hier ist die Syntax {% for post in posts%} eine Jinja-for-Schleife, die einer Python-for-Schleife ähnelt; sie muss allerdings später mit der {% endfor%} -Syntax geschlossen werden.
Sie verwenden diese Syntax, um für jedes Element in der Liste posts, die von der Funktion index () in der Zeile return render _ template (' index.html ', posts = posts) übergeben wurde, eine Schleife zu durchlaufen.
Innerhalb dieser for-Schleife zeigen Sie den Titel des Beitrags in einer < h2 > -Überschrift in einem < a > -Tag an (Sie verwenden dieses Tag später, um Verknüpfungen für die einzelnen Beiträge zu erstellen).
Sie zeigen den Titel an mit einem Literal-Variablen-Trennzeichen ({{...
}}).
Denken Sie daran, dass post ein wörterbuchähnliches Objekt sein wird, damit Sie mit post ['title'] auf den Beitragstitel zugreifen können.
Außerdem zeigen Sie mit der gleichen Methode das Erstellungsdatum des Beitrags an.
Sobald Sie die Bearbeitung der Datei abgeschlossen haben, speichern und schließen Sie sie. Navigieren Sie dann in Ihrem Browser zur Indexseite.
Sie werden die beiden Beiträge, die Sie der Datenbank hinzugefügt haben, auf Ihrer Seite sehen.
Indexseite mit den angezeigten Beiträgen
Nachdem Sie die Ansichtsfunktion index () modifiziert haben, um alle Beiträge, die Sie in der Datenbank haben, auf der Homepage Ihrer Anwendung anzeigen, zeigen Sie nun jeden Beitrag auf einer einzelnen Seite an und ermöglichen es Benutzern, Links zu den einzelnen Beiträgen zu nutzen.
Schritt 6 - Anzeigen eines einzelnen Beitrags
In diesem Schritt erstellen Sie eine neue Flask-Route mit einer Ansichtsfunktion und eine neue HTML-Vorlage zur Anzeige eines einzelnen Blogeintrags anhand seiner ID.
Am Ende dieses Schritts wird die URL http: / / 127.0.0.1: 5000 / 1 eine Seite sein, die den ersten Beitrag anzeigt (weil dieser die ID 1 hat).
Die URL http: / / 127.0.0.1: 5000 / < ^ > ID < ^ > zeigt den Beitrag mit der zugehörigen < ^ > ID < ^ > -Nummer an, so sie vorhanden ist.
Öffnen Sie app.py zum Bearbeiten:
Da Sie in diesem Projekt noch an unterschiedlichen Stellen einen Blogeintrag aus der Datenbank abrufen müssen, erstellen Sie eine Standalone-Funktion namens get _ post ().
Sie können sie aufrufen, indem Sie ihr eine ID übergeben, und erhalten den Blogbeitrag zurück, der mit der bereitgestellten ID verknüpft ist; oder Sie sorgen dafür, dass Flask mit einer 404 Nicht gefunden-Nachricht antwortet, wenn der Blogbeitrag nicht existiert.
Um mit einer 404-Seite zu antworten, müssen Sie die Funktion abort () aus der Werkzeug-Bibliothek, die zusammen mit Flask installiert wurde, am Anfang der Datei importieren:
Fügen Sie die Funktion get _ post () direkt nach der Funktion get _ db _ connection () hinzu, die Sie im vorherigen Schritt erstellt haben:
Diese neue Funktion verfügt über ein post _ id-Argument, das bestimmt, welcher Blogbeitrag zurückgegeben wird.
Innerhalb der Funktion verwenden Sie die Funktion get _ db _ connection () zum Öffnen einer Datenbankverbindung und Ausführen einer SQL-Abfrage, um den Blogbeitrag zu erhalten, der mit dem angegebenen post _ id-Wert verknüpft ist.
Sie fügen die Methode fetchone () hinzu, um das Ergebnis zu erhalten und in der Variable post zu speichern. Dann schließen Sie die Verbindung.
Wenn die Variable post den Wert None (Keine) hat, was bedeutet, dass in der Datenbank kein Ergebnis gefunden wird, verwenden Sie die Funktion abort (), die Sie zuvor importiert haben, um mit einem 404-Fehlercode zu reagieren; die Ausführung der Funktion wird beendet.
Wenn jedoch ein Beitrag gefunden wurde, geben Sie den Wert der Variable post zurück.
Fügen Sie als Nächstes die folgende Ansichtsfunktion am Ende der Datei app.py hinzu:
In dieser neuen Ansichtsfunktion fügen Sie eine Variablenregel < int: post _ id > hinzu, um anzugeben, dass der Teil nach dem Schrägstrich (/) eine positive ganze Zahl ist (markiert mit dem int-Konverter), die Sie in Ihrer Ansichtsfunktion aufrufen müssen.
Flask erkennt das und übergibt ihren Wert an das Schlüsselwortargument post _ id Ihrer post () -Ansichtsfunktion.
Dann verwenden Sie die Funktion get _ post (), um den Blogbeitrag abzurufen, der mit der angegebenen ID verknüpft ist, und speichern das Ergebnis in der Variable post, die Sie an eine post.html-Vorlage, die Sie bald erstellen werden, übergeben.
Speichern Sie die Datei app.py und öffnen Sie eine neue post.html-Vorlagendatei zur Bearbeitung:
Geben Sie in dieser neuen post.html-Datei den folgenden Code ein.
Das wird ähnlich aussehen wie bei der Datei index.html; es wird jedoch nur ein einziger Blogbeitrag angezeigt, und zudem mit dem Inhalt des Beitrags:
Sie fügen den title-Block hinzu, den Sie in der Vorlage base.html definiert haben, um den Titel der Seite an den Titel des Beitrags anzupassen, der gleichzeitig in einer < h2 > -Überschrift angezeigt wird.
Sie können nun zu den folgenden URLs navigieren, um die beiden Beiträge, die Sie in Ihrer Datenbank haben, sowie eine Seite anzuzeigen, die dem Benutzer mitteilt, dass der angeforderte Blogbeitrag nicht gefunden wurde (da bisher kein Beitrag mit der ID-Nummer 3 vorhanden ist):
Zurück auf der Indexseite verknüpfen Sie jeden Titel eines Beitrags mit seiner jeweiligen Seite.
Dazu verwenden Sie die Funktion url _ for ().
Öffnen Sie zuerst die Vorlage index.html zur Bearbeitung:
Ändern Sie dann den Wert des Attributs href von # in {{url _ for (' post ', post _ id = post [' id '])}}, damit die for-Schleife genau wie folgt aussieht:
Hier übergeben Sie 'post' an die Funktion url _ for () als erstes Argument.
Das ist der Name der post () -Ansichtsfunktion; da sie ein post _ id-Argument akzeptiert, geben Sie den Wert post ['id'].
Die Funktion url _ for () gibt für jeden Beitrag auf Grundlage seiner ID die richtige URL zurück.
Die Links auf der Indexseite funktionieren nun wie erwartet.
Damit haben Sie die Erstellung des Teils der Anwendung abgeschlossen, der für die Anzeige der Blogbeiträge in Ihrer Datenbank verantwortlich ist.
Als Nächstes fügen Sie die Möglichkeit zur Erstellung, Bearbeitung und Löschung von Blogbeiträgen zu Ihrer Anwendung hinzu.
Schritt 7 - Bearbeiten von Beiträgen
Nachdem Sie die Anzeige der Blogbeiträge, die in der Datenbank vorhanden sind, in der Webanwendung abgeschlossen haben, müssen Sie es den Benutzern Ihrer Anwendung nun ermöglichen, neue Blogbeiträge zu schreiben und der Datenbank hinzuzufügen, vorhandene Beiträge zu bearbeiten und unnötige Blogbeiträge zu löschen.
Erstellen eines neuen Beitrags
Bislang verfügen Sie über eine Anwendung, die die Beiträge in Ihrer Datenbank anzeigt, aber keine Möglichkeit zum Hinzufügen eines neuen Beitrags, es sei denn, Sie verbinden sich direkt mit der SQLite-Datenbank und fügen manuell einen Beitrag hinzu.
In diesem Abschnitt richten Sie eine Seite ein, auf der Sie einen Beitrag erstellen können, indem Sie den Titel und den Inhalt bereitstellen.
Öffnen Sie die Datei app.py zur Bearbeitung:
Zuerst importieren Sie Folgendes aus dem Flask-Framework:
Das globale request-Objekt für den Zugriff auf eingehende Anfragedaten, die über ein HTML-Formular übermittelt werden.
Die Funktion url _ for () zur Erstellung von URLs.
Die flash () -Funktion zur Anzeige einer Meldung, wenn eine Anfrage verarbeitet wird.
Die Funktion redirect () zum Umleiten des Clients an einen anderen Ort.
Fügen Sie die Importe wie folgt in Ihre Datei ein:
Die flash () -Funktion speichert geflashte Nachrichten in der Browsersitzung des Clients, was die Einrichtung eines geheimen Schlüssels erfordert.
Dieser geheime Schlüssel dient zur Sicherung von Sitzungen, sodass sich Flask Informationen von einer Anfrage zu einer anderen merken kann, zum Beispiel beim Navigieren von der Seite für neue Beiträge zur Indexseite.
Der Benutzer kann auf die in der Sitzung gespeicherten Daten zugreifen, sie aber nicht ändern, es sei denn, er verfügt über den geheimen Schlüssel; das bedeutet, dass Sie niemandem Zugriff auf Ihren geheimen Schlüssel gewähren dürfen.
Weitere Informationen finden Sie in der Flask-Dokumentation für Sitzungen.
Um einen geheimen Schlüssel einzurichten, fügen Sie Ihrer Anwendung eine SECRET _ KEY-Konfiguration über das Objekt app.config hinzu.
Fügen Sie sie unmittelbar nach der app-Definition hinzu, bevor Sie die Ansichtsfunktion index () definieren:
Denken Sie daran, dass der geheime Schlüssel eine lange Zufallszeichenfolge sein sollte.
Nach der Einrichtung eines geheimen Schlüssels erstellen Sie eine Ansichtsfunktion, die eine Vorlage rendern wird, um ein Formular anzuzeigen, das Sie zum Erstellen eines neuen Blogbeitrags ausfüllen können.
Fügen Sie diese neue Funktion am Ende der Datei hinzu:
Dadurch wird eine / create-Route erstellt, die sowohl GET- als auch POST-Anfragen akzeptiert.
GET-Anfragen werden standardmäßig akzeptiert.
Um auch POST-Anfragen zu akzeptieren, die vom Browser beim Übermitteln von Formularen gesendet werden, übergeben Sie ein Tupel mit den akzeptierten Arten von Anfragen an das methods-Argument des @ app.route () -Decorators.
Um die Vorlage zu erstellen, öffnen Sie eine Datei namens create.html in Ihrem Ordner templates:
Fügen Sie in dieser neuen Datei den folgenden Code hinzu:
Der größte Teil dieses Codes ist Standard-HTML.
Er zeigt ein Eingabefeld für den Titel des Beitrags, einen Textbereich für den Inhalt des Beitrags und eine Schaltfläche zum Übermitteln des Formulars an.
Der Wert der Beitragstiteleingabe ist {{request.form ['title']}} und der Textbereich hat den Wert {{request.form ['content']}}; das sorgt dafür, dass die eingegebenen Daten nicht verloren gehen, wenn etwas schiefläuft.
Wenn Sie zum Beispiel einen langen Beitrag verfassen und vergessen, ihm einen Titel zu geben, wird Ihnen eine Meldung angezeigt, die Sie daran erinnert, dass der Titel erforderlich ist.
Das geschieht, ohne dass Sie den geschriebenen Beitrag verlieren, da er im globalen request-Objekt gespeichert wird, auf das Sie in Ihren Vorlagen Zugriff haben.
Nachdem der Entwicklungsserver ausgeführt wird, verwenden Sie nun Ihren Browser, um zur Route / create zu navigieren:
Sie werden eine Seite für Create a New Post mit einem Feld für Titel und Inhalt sehen.
Seite zum Erstellen eines neuen Beitrags
Dieses Formular übermittelt eine POST-Anfrage an Ihre create () -Funktion.
In der Funktion gibt es jedoch noch keinen Code, um eine POST-Anfrage zu bearbeiten; also wird nach dem Ausfüllen und Übermitteln des Formulars nichts geschehen.
Sie werden die eingehende POST-Anfrage bearbeiten, wenn ein Formular übermittelt wird.
Das tun Sie in der create () -Ansichtsfunktion.
Sie können die POST-Anfrage separat bearbeiten, indem Sie den Wert von request.method überprüfen.
Wenn der Wert auf 'POST' festgelegt ist, bedeutet das, dass die Anfrage eine POST-Anfrage ist. Dann fahren Sie mit dem Extrahieren der übermittelten Daten sowie dem Validieren und Einfügen der Daten in Ihre Datenbank fort.
Ändern Sie die create () -Ansichtsfunktion, damit sie genau wie folgt aussieht:
Stellen Sie in der if-Anweisung sicher, dass der darauf folgende Code nur ausgeführt wird, wenn die Anfrage eine POST-Anfrage ist. Nutzen Sie dazu den Vergleich request.method = = 'POST'.
Dann extrahieren Sie den übermittelten Titel und Inhalt aus dem request.from-Objekt, das Ihnen Zugriff auf die Formulardaten in der Anfrage bietet.
Wenn der Titel nicht angegeben ist, wäre die Bedingung if not title erfüllt. In dem Fall wird dem Benutzer eine Meldung angezeigt, die ihm mitteilt, dass ein Titel erforderlich ist.
Wenn der Titel hingegen angegeben ist, öffnen Sie eine Verbindung mit der Funktion get _ db _ connection () und fügen den Titel und Inhalt, die Sie erhalten haben, in die Tabelle posts ein.
Dann übergeben Sie die Änderungen mit "commit" an die Datenbank und schließen die Verbindung.
Nachdem Sie den Blogbeitrag in die Datenbank eingefügt haben, leiten Sie den Client mit der redirect () -Funktion auf die Indexseite um. Dazu übergeben Sie ihr die von der Funktion url _ for () mit dem Wert 'index' als Argument generierte URL.
Navigieren Sie nun mit Ihrem Webbrowser zur Route / create:
Füllen Sie das Formular mit einem Titel Ihrer Wahl und einem Inhalt aus.
Sobald Sie das Formular übermitteln, sehen Sie, dass der neue Beitrag auf der Indexseite aufgelistet wird.
Schließlich zeigen Sie geflashte Nachrichten an und fügen zur Navigationsleiste in der Vorlage base.html eine Verknüpfung hinzu, um einfachen Zugriff auf diese neue Seite zu ermöglichen.
Öffnen Sie die Vorlagendatei:
Bearbeiten Sie die Datei, indem Sie ein neues < li > -Tag nach dem Link About im Tag < nav > hinzufügen.
Fügen Sie dann direkt über dem content-Block eine neue for-Schleife hinzu, um die geflashten Nachrichten unterhalb der Navigationsleiste anzuzeigen.
Diese Nachrichten sind in der speziellen Funktion get _ flashed _ messages () von Flask verfügbar:
Die Navigationsleiste verfügt nun über ein Element New Post, das mit der Route / create verknüpft ist.
Bearbeiten eines Beitrags
Damit Ihr Blog aktuell bleibt, müssen Sie Ihre vorhandenen Beiträge bearbeiten können.
Dieser Abschnitt führt Sie durch die Erstellung einer neuen Seite in Ihrer Anwendung, um das Bearbeiten eines Beitrags zu vereinfachen.
Zuerst fügen Sie der Datei app.py eine neue Route hinzu.
Ihre Ansichtsfunktion empfängt die ID des Beitrags, der bearbeitet werden soll; die URL wird im Format / < ^ > post _ id < ^ > / edit vorliegen, wobei die Variable < ^ > post _ id < ^ > die ID des Beitrags ist.
Fügen Sie als Nächstes die folgende Ansichtsfunktion edit () am Ende der Datei hinzu:
Die Bearbeitung eines bestehenden Beitrags ähnelt der Erstellung eines neuen Beitrags. Daher wird diese Ansichtsfunktion ähnlich aussehen wie die create () -Ansichtsfunktion:
Der Beitrag, den Sie bearbeiten, wird von der URL bestimmt; Flask übergibt die ID-Nummer an die Funktion edit () über das Argument id.
Sie fügen diesen Wert der Funktion get _ post () hinzu, um den Beitrag, der mit der bereitgestellten ID verknüpft ist, aus der Datenbank abzurufen.
Die neuen Daten kommen in einer POST-Anfrage, die in der Bedingung if request.method = = 'POST' verwaltet wird.
Genau wie bei der Erstellung eines neuen Beitrags extrahieren Sie die Daten zuerst aus dem request.form-Objekt und flashen dann eine Nachricht, wenn der Titel einen leeren Wert hat; andernfalls öffnen Sie eine Datenbankverbindung.
Dann aktualisieren Sie die Tabelle posts durch Festlegen eines neuen Titels und eines neuen Inhalts, wobei die ID des Beitrags in der Datenbank gleich der ID ist, die in der URL enthalten war.
Im Fall einer GET-Anfrage rendern Sie eine Vorlage edit.html, indem Sie die Variable post übergeben, die den zurückgegebenen Wert der Funktion get _ post () enthält.
Sie tun das, um den bestehenden Titel und Inhalt auf der Bearbeitungsseite anzuzeigen.
Speichern und schließen Sie die Datei und erstellen Sie dann eine neue edit.html-Vorlage:
Schreiben Sie in dieser neuen Datei den folgenden Code:
Dieser Code folgt dem gleichen Muster, außer der Syntax {{request.form ['title'] or post ['title']}} und {{request.form ['content'] or post ['content']}}.
Das zeigt die in der Anfrage gespeicherten Daten an, so vorhanden. Andernfalls werden die Daten aus der Variablen post angezeigt, die mit den aktuellen Datenbankdaten an die Vorlage übergeben wurde.
Navigieren Sie nun zur folgenden URL, um den ersten Beitrag zu bearbeiten:
Sie werden eine Seite Edit "First Post" sehen.
Seite zum Bearbeiten eines Beitrags
Bearbeiten Sie den Beitrag und übermitteln Sie das Formular; überprüfen Sie dann, ob der Beitrag aktualisiert wurde.
Nun müssen Sie für jeden Beitrag auf der Indexseite einen Link hinzufügen, der auf die Bearbeitungsseite verweist.
Öffnen Sie die Vorlagendatei index.html:
Bearbeiten Sie die Datei, damit sie genau wie folgt aussieht:
Hier fügen Sie ein < a > -Tag hinzu, um eine Verknüpfung zur Ansichtsfunktion edit () herzustellen. Dazu übergeben Sie den Wert post ['id'], um mit dem Link Edit eine Verknüpfung zur Bearbeitungsseite der einzelnen Beiträge herzustellen.
Löschen eines Beitrags
Manchmal müssen Beiträge nicht mehr öffentlich verfügbar sein. Dafür gibt es die Funktion zum Löschen von Beiträgen.
In diesem Schritt fügen Sie Ihrer Anwendung die Löschfunktion hinzu.
Zuerst fügen Sie eine neue Route / < ^ > ID < ^ > / delete hinzu, die POST-Anfragen akzeptiert, ähnlich wie bei der Ansichtsfunktion edit ().
Ihre neue delete () -Ansichtsfunktion empfängt die ID des Beitrags, der gelöscht werden soll, aus der URL.
Öffnen Sie die Datei app.py:
Fügen Sie am Ende der Datei die folgende Ansichtsfunktion hinzu:
Diese Ansichtsfunktion akzeptiert nur POST-Anfragen.
Das bedeutet, dass, wenn Sie in Ihrem Browser zur Route / < ^ > ID < ^ > / delete navigieren, ein Fehler zurückgegeben wird, da Webbrowser standardmäßig GET-Anfragen nutzen.
Sie können diese Route jedoch über ein Formular aufrufen, das eine POST-Anfrage sendet, mit der die ID des Beitrags übergeben wird, den Sie löschen möchten.
Die Funktion empfängt den ID-Wert und verwendet ihn, um den Beitrag mit der Funktion get _ post () aus der Datenbank abzurufen.
Dann öffnen Sie eine Datenbankverbindung und führen einen DELETE FROM-SQL-Befehl aus, um den Beitrag zu löschen.
Sie übergeben die Änderung an die Datenbank und schließen die Verbindung, während Sie dem Benutzer eine Nachricht flashen, dass der Beitrag erfolgreich gelöscht wurde, und leiten ihn zur Indexseite weiter.
Beachten Sie, dass Sie eine Vorlagendatei nicht rendern, da Sie der Bearbeitungsseite einfach eine Delete-Schaltfläche hinzufügen.
Öffnen Sie die Vorlagendatei edit.html:
Fügen Sie dann das folgende < form > -Tag nach dem Tag < hr > und direkt vor der Zeile {% endblock%} hinzu:
Sie verwenden die Methode confirm (), um eine Bestätigungsmeldung anzuzeigen, bevor die Anfrage übermittelt wird.
Navigieren Sie nun erneut zur Bearbeitungsseite eines Blogbeitrags und versuchen Sie, ihn zu löschen:
Am Ende dieses Schritts sieht der Quellcode Ihres Projekts wie der Code auf dieser Seite aus.
Damit können die Benutzer Ihrer Anwendung nun neue Blogbeiträge schreiben und der Datenbank hinzufügen sowie bestehende Beiträge bearbeiten und löschen.
Dieses Tutorial hat eine Einleitung in grundlegende Konzepte des Flask-Python-Frameworks geboten.
Sie haben gelernt, wie Sie eine kleine Webanwendung erstellen und auf einem Entwicklungsserver ausführen sowie dem Benutzer erlauben, über URL-Parameter und Webformulare benutzerdefinierte Daten anzugeben.
Außerdem haben Sie die Jinja-Vorlagen-Engine verwendet, um HTML-Dateien wiederzuverwenden und Logik darin zu nutzen.
Am Ende dieses Tutorials verfügen Sie nun über einen voll funktierende Weblog, der mit einer SQLite-Datenbank interagiert, um mithilfe der Python-Sprache und SQL-Abfragen das Erstellen, Anzeigen, Bearbeiten und Löschen von Blogbeiträgen zu ermöglichen.
Sie können diese Anwendung weiter entwickeln, indem Sie Benutzerauthentifizierung hinzufügen, damit nur registrierte Benutzer Blogbeiträge erstellen und ändern können. Außerdem können Sie zu jedem Blogbeitrag Kommentare und Tags sowie Datei-Upload-Funktionen hinzufügen, damit Benutzer die Möglichkeit haben, Bilder im Beitrag zu verwenden.
Weitere Informationen finden Sie in der Flask-Dokumentation.
Flask verfügt über eine Vielzahl von Flask-Erweiterungen, die von der Community entwickelt wurden.
Im Folgenden finden Sie eine Liste von Erweiterungen, die Sie verwenden können, um Ihren Entwicklungsprozess zu erleichtern:
Flask-Login: verwaltet die Benutzersitzung und übernimmt die An- und Abmeldung sowie das Erinnern angemeldeter Benutzer.
Flask-SQLAlchemy: vereinfacht die Verwendung von Flask mit SQLAlchemy, einem Python-SQL-Toolkit, und Object Relational Mapper zur Interaktion mit SQL-Datenbanken.
Flask-Mail: hilft beim Versand von E-Mail-Nachrichten in Ihrer Flask-Anwendung.
Überwachen des Serverzustands mit Checkmk unter Ubuntu 18.04
4102
Als Systemadministrator ist es eine bewährte Vorgehensweise, den aktuellen Zustand Ihrer Infrastruktur und Diensten zu kennen.
Im Idealfall möchten Sie ausfallende Festplatten oder Anwendungsausfälle möglichst vor den Benutzern erkennen.
Überwachungs-Tools wie Checkmk helfen Administratoren, diese Probleme zu erkennen und für einwandfreien Serverbetrieb zu sorgen.
Im Allgemeinen kann Überwachungssoftware die Hardware, die Betriebszeit und den Servicestatus Ihrer Server verfolgen und Warnmeldungen ausgeben, wenn ein Fehler auftritt.
In einem sehr einfachen Szenario würden Sie von einem Überwachungssystem alarmiert, wenn Dienste ausfallen.
In einem robusteren Szenario würden Sie kurz nach dem Auftreten verdächtiger Anzeichen benachrichtigt werden, z. B. bei erhöhter Speichernutzung oder einer ungewöhnlichen Anzahl von TCP-Verbindungen.
Es gibt verschiedene kostenlose und kommerzielle Überwachungslösungen mit unterschiedlichen Komplexitätsgraden und Funktionalitäten.
Häufig ist die Installation, Konfiguration und Verwaltung dieser Tools kompliziert und zeitaufwändig.
Checkmk ist jedoch eine robuste und installationsfreundliche Überwachungslösung.
Als in sich geschlossenes Softwarepaket kombiniert es Nagios (populärer Open-Source-Alarmierungsdienst) mit Add-Ons zur Erfassung, Überwachung und grafischen Darstellung von Daten. Außerdem ist eine Weboberfläche von Checkmk mit dabei: ein umfassendes Tool, das viele der von Nagios bekannten Mängel ausgleicht.
Es bietet ein benutzerfreundliches Dashboard, ein voll funktionsfähiges Benachrichtigungssystem und ein Repository mit einfach zu installierenden Überwachungsagenten für viele Linux-Distributionen.
Ohne die Checkmk Weboberfläche müssten wir für verschiedene Aufgaben verschiedene Ansichten verwenden. All diese Funktionen zu konfigurieren, wäre ohne umfangreiche Dateiänderungen nicht möglich.
In diesem Leitfaden richten wir Checkmk auf einem Ubuntu 18.04 Server ein und überwachen zwei separate Hosts.
Wir überwachen sowohl den Ubuntu-Server selbst als auch einen separaten CentOS 7-Server. Das gleiche Prinzip lässt sich anwenden, um eine beliebige Anzahl weiterer Hosts zu einer Überwachungskonfiguration hinzuzufügen.
Ein Ubuntu 18.04-Server mit einem regulären non-root user mit sudo-Berechtigungen.
In diesem Tutorial zur Server-Ersteinrichtung erfahren Sie, wie Sie den Server richtig vorbereiten.
Ein CentoOS7 Server mit einem regulären non-root user mit sudo-Berechtigungen.
Schritt 1 - Installieren von Checkmk unter Ubuntu
Um unsere Überwachungsseite nutzen zu können, müssen wir zunächst Checkmk auf dem Ubuntu-Server installieren.
Dadurch erhalten wir alle benötigten Tools.
Checkmk liefert offizielle, sofort einsatzbereite Ubuntu-Paketdateien zur Installation des Softwarepakets.
Für die neueste Version der Repository-Listen aktualisieren wir als erstes die Paketeliste:
Auf der Paketelistenseite können die Pakete durchsucht werden.
Im Seitenmenü kann unter anderem Ubuntu 18.04 ausgewählt werden.
Laden Sie jetzt das Paket herunter:
Installieren Sie dann das neu heruntergeladene Paket:
Dieser Befehl installiert das Checkmk-Paket mit allen erforderlichen Abhängigkeiten, u. a. auch den Apache-Webserver, der für den Webzugriff auf die Überwachungsoberfläche verwendet wird.
Nach Abschluss der Installation können wir auf den Befehl omd zugreifen.
Versuchen Sie es:
Der Befehl omd gibt Folgendes aus:
Der Befehl omd kann alle Checkmk Instanzen auf unserem Server verwalten.
Er kann alle Überwachungsdienste auf einmal starten und anhalten. Außerdem nutzen wir ihn zur Erstellung unserer Checkmk Instanz.
Zuerst müssen wir allerdings die Firewall-Einstellungen aktualisieren, um den externen Zugriff auf Standard-Web-Ports zu ermöglichen.
Schritt 2 - Anpassen der Firewall-Einstellungen
Bevor wir mit Checkmk arbeiten können, muss der externe Zugriff auf den Webserver in der Firewall-Konfiguration erlaubt werden.
Wenn Sie die einzelnen Schritte zur Konfiguration der Firewall in den Voraussetzungen befolgt haben, haben Sie eine UFW-Firewalleinrichtung, die den Serverzugriff einschränkt.
Während der Installation registriert sich Apache bei UFW, damit der Apache-Zugriff ganz einfach über die Firewall aktiviert und deaktiviert werden kann.
Verwenden Sie den folgenden Befehl, um den Zugriff auf Apache zu ermöglichen:
Überprüfen Sie die Änderungen:
Sie sehen, dass Apache unter den erlaubten Diensten genannt ist:
Das ermöglicht uns den Zugriff auf die Checkmk Weboberfläche.
Im nächsten Schritt erstellen wir die erste Checkmk Überwachungsinstanz.
Schritt 3 - Erstellen einer Checkmk Überwachungsinstanz
Checkmk nutzt das Prinzip von Instanzen oder einzelnen Installationen, um verschiedene Checkmk-Kopien auf einem Server zu isolieren.
In den meisten Fällen genügt eine Kopie von Checkmk, und so werden wir die Software auch in diesem Leitfaden konfigurieren.
Zuerst müssen wir unserer neuen Instanz einen Namen geben. Wir verwenden in dem gesamten Text < ^ > monitoring < ^ >.
Geben Sie Folgendes ein, um die Instanz zu erstellen:
Das Tool omd richtet alles automatisch für uns ein.
Die Befehlsausgabe sieht etwa wie folgt aus:
In dieser Ausgabe sind URL-Adresse, Standard-Benutzername und Passwort für den Zugriff auf unsere Überwachungsoberfläche hervorgehoben.
Die Instanz ist jetzt erstellt, muss aber noch gestartet werden.
Geben Sie Folgendes ein, um die Instanz zu starten:
Jetzt werden alle erforderlichen Tools und Dienste sofort gestartet.
Am Ende sehen wir eine Ausgabe, die bestätigt, dass alle unsere Dienste erfolgreich gestartet wurden:
Die Instanz wird ausgeführt.
Öffnen Sie http: / / < ^ > your _ ubuntu _ server _ ip < ^ > / monitoring / im Webbrowser, um auf die Checkmk-Instanz zuzugreifen.
Sie werden zur Eingabe eines Passworts aufgefordert.
Verwenden Sie die auf dem Bildschirm voreingestellten Standardanmeldeinformationen. Wir werden diese später ändern.
Der Checkmk-Bildschirm öffnet sich mit einem Dashboard, auf dem der Status aller unserer Dienste und Server aufgelistet ist. Es bietet zudem praktische Diagramme, die der Erde ähneln.
Direkt nach der Installation sind diese leer. Wir werden uns aber in Kürze den Status unserer Dienste und Systeme anzeigen lassen.
Leeres Checkmk Dashboard
Im nächsten Schritt ändern wir das Standard-Passwort, um die Seite über diese Oberfläche zu sichern.
Schritt 4 - Ändern des Administratorpassworts
Während der Installation generiert Checkmk ein Zufallspasswort für den administrativen Benutzer cmkadmin.
Das Passwort ist meist kurz und nicht sicher und sollte nach der Installation geändert werden.
Wir können es über die Weboberfläche ändern.
Öffnen Sie zuerst die Seite Users im Menü WATO - Configuration links.
Die Liste zeigt alle Benutzer, die derzeit Zugriff auf die Seite Checkmk haben.
Bei einer neuen Installation sind es nur zwei Benutzer.
Der erste, automation, ist für die Verwendung mit Automatisierungstools vorgesehen. Der zweite ist der Benutzer cmkadmin, mit dem wir uns auf der Seite anmelden.
Liste der Checkmk-Benutzer
Klicken Sie auf das Stift-Symbol neben dem cmkadmin-Benutzer, um die Benutzerangaben, einschließlich des Passworts zu ändern.
Bearbeitungsformular für administrative Checkmk-Benutzer
Aktualisieren Sie das Passwort, fügen Sie eine Administrator-E-Mailadresse ein und nehmen Sie alle weiteren gewünschten Änderungen vor.
Nach Speichern der Änderungen werden wir aufgefordert, uns erneut mit den neuen Zugangsdaten anzumelden.
Tun Sie das und kehren Sie dann zum Dashboard zurück, auf dem wir noch etwas ausführen müssen, um unsere neue Konfiguration vollständig anwenden zu können.
Öffnen Sie erneut die Seite Users im Menü WATO - Configuration links.
Die orange Schaltfläche oben links mit der Bezeichnung 1 Change zeigt uns an, dass wir einige Änderungen an der Konfiguration von Checkmk vorgenommen haben, die wir noch speichern und aktivieren müssen.
Das geschieht jedes Mal, wenn wir die Konfiguration des Überwachungssystems ändern, nicht nur nach der Bearbeitung der Anmeldedaten eines Benutzers.
Um ausstehende Änderungen zu speichern und zu aktivieren, müssen wir auf diese Schaltfläche klicken und zustimmen, die aufgeführten Änderungen mit der Option Activate affected auf dem folgenden Bildschirm zu aktivieren.
Liste der Checkmk-Benutzer nach Änderungen Ansicht Aktivieren der Konfigurationsänderungen bestätigen Erfolgreich aktivierte Konfigurationsänderungen
Nachdem die Änderungen aktiviert sind, werden die Daten des neuen Benutzers in die Konfigurationsdateien geschrieben und von allen Systemkomponenten verwendet.
Checkmk benachrichtigt automatisch die einzelnen Komponenten des Überwachungssystems, lädt sie bei Bedarf neu und verwaltet alle notwendigen Konfigurationsdateien.
Die Checkmk Installation ist jetzt einsatzbereit.
Im nächsten Schritt fügen wir unserem Überwachungssystem den ersten Host hinzu.
Schritt 5 - Überwachen des ersten Hosts
Wir sind jetzt bereit, den ersten Host zu überwachen.
Hierzu installieren wir zunächst check-mk-agent auf dem Ubuntu-Server.
Dann beschränken wir den Zugriff auf die Überwachungsdaten mit xinetd.
Die mit Checkmk installierten Komponenten sind für Empfang, Speicherung und Darstellung von Überwachungsdaten verantwortlich.
Sie liefern die Daten nicht selbst.
Um die eigentlichen Daten zu sammeln, verwenden wir den Checkmk Agent.
Der Checkmk Agent, der speziell für diese Aufgabe entwickelt wurde, ist in der Lage, alle wichtigen Systemkomponenten auf einmal zu überwachen und diese Informationen an die Checkmk-Instanz zurückzumelden.
Installieren des Agenten
Der erste Host, der überwacht wird, ist < ^ > your _ ubuntu _ server < ^ >, also der Server, auf dem wir die Checkmk-Instanz installiert haben.
Zunächst müssen wir den Checkmk Agent installieren.
Pakete für alle wichtigen Distributionen, wie z. B. Ubuntu, sind direkt auf der Weboberfläche verfügbar.
Öffnen Sie die Seite Monitoring Agents im Menü WATO - Configuration links.
Sie sehen die verfügbaren Agent-Downloads mit den beliebtesten Paketen unter dem ersten Abschnitt Packaged agents.
Liste der verfügbaren Überwachungsagenten-Pakete
Das Paket check-mk-agent _ 1.6.0p8-1 _ all. < ^ > deb < ^ > ist geeignet für Debian-basierte Distributionen, einschließlich Ubuntu.
Kopieren Sie den Download-Link für dieses Paket aus dem Webbrowser und laden Sie das Paket unter dieser Adresse herunter.
Installieren Sie das Paket nach dem Herunterladen:
Überprüfen Sie, ob der Agent erfolgreich installiert wurde:
Der Befehl gibt einen sehr langen Text aus, der unverständlich wirkt, aber alle wichtigen Informationen über das System kombiniert.
Checkmk nutzt die Ausgabe dieses Befehls, um die Statusdaten der überwachten Hosts zu sammeln.
Mit xinetd beschränken wir den Zugriff auf die Überwachungsdaten.
Zugriffsbeschränkungen auf Überwachungsdaten mit xinetd
Standardmäßig werden die Daten von check _ mk _ agent mit Hilfe von xinetd bereitgestellt, einem Mechanismus, der Daten beim Zugriff auf einen bestimmten Netzwerk-Port ausgibt. Das bedeutet, dass wir auf check _ mk _ agent mit Hilfe von telnet auf Port 6556 (Standard-Port für Checkmk) von jedem Computer im Internet zugreifen können, sofern die Firewall-Konfiguration dies nicht untersagt.
Wichtige Informationen über Server sollten aus Sicherheitsgründen nicht im Internet veröffentlicht werden.
Wir sollten nur Hosts, die Checkmk ausführen und von uns überwacht werden, den Datenzugriff erlauben, damit nur unser Überwachungssystem diese sammeln kann.
Wenn Sie dem Tutorial zur Server-Ersteinrichtung gefolgt sind, darunter den Schritten zur Einrichtung einer Firewall, ist der Zugriff auf den Checkmk Agent standardmäßig blockiert.
Es ist jedoch empfehlenswert, diese Zugriffsbeschränkungen direkt in der Dienstkonfiguration vorzunehmen und sich bei deren Überwachung nicht allein auf die Firewall zu verlassen.
Um den Zugriff auf die Agentendaten zu beschränken, müssen wir die Konfigurationsdatei unter / etc / xinetd.d / check _ mk bearbeiten.
Öffnen Sie die Konfigurationsdatei in einem beliebigen Editor.
Um Nano zu verwenden, geben Sie Folgendes ein:
Die Einstellung only _ from schränkt den Zugang auf bestimmte IP-Adressen ein.
Da wir nun daran arbeiten, den gleichen Server zu überwachen, auf dem Checkmk ausgeführt wird, ist es in Ordnung, nur localhost das Verbinden zu erlauben.
Entfernen Sie den Kommentar und aktualisieren Sie die Konfigurationseinstellung in:
Der xinetd Daemon muss neu gestartet werden, damit die Änderungen übernommen werden.
Tun Sie das jetzt:
Der Agent wird nun ausgeführt und ist darauf beschränkt, nur lokale Verbindungen zu akzeptieren.
Wir können jetzt die Überwachung für diesen Host mit Hilfe von Checkmk konfigurieren.
Konfigurieren des Host auf der Checkmk Weboberfläche
Um einen neuen zu überwachenden Host hinzuzufügen, gehen wir in das Menü Hosts im Menü WATO - Configuration links.
Hier klicken Sie auf Create new host.
Wir werden nach einigen Informationen über den Host gefragt.
Erstellen eines neuen Hosts in Checkmk
Der Hostname ist der bekannte Name, den Checkmk für die Überwachung verwenden wird.
Es kann sich um einen vollwertigen Domänennamen handeln, muss aber nicht.
In diesem Beispiel nennen wir den Host monitoring, genau wie der Name der Checkmk-Instanz selbst.
Da monitoring auf unsere IP-Adresse nicht aufgelöst werden kann, müssen wir auch die IP-Adresse unseres Servers angeben.
Und da der lokale Host überwacht wird, lautet die IP einfach 127.0.0.1.
Markieren Sie das Feld IPv4 Address, um die manuelle IP-Eingabe zu aktivieren und geben Sie den Wert in das Textfeld ein.
Die Standardkonfiguration des Abschnitts Data Sources benötigt Checkmk Agent, um die Überwachungsdaten bereitzustellen.
Die Einstellung Segment Networking wird verwendet, um Hosts in Remote-Netzwerken zu kennzeichnen, die eine höhere erwartete Latenz haben, was keine Fehlfunktion darstellt.
Da es sich um einen lokalen Host handelt, ist die Standardeinstellung auch hier in Ordnung.
Um den Host zu speichern und zu konfigurieren, welche Dienste überwacht werden, klicken Sie auf die Schaltfläche Save & go to services.
Liste der verfügbaren Dienste zur Überwachung
Checkmk führt eine automatische Bestandsaufnahme durch.
Das bedeutet, dass es die Ausgabe des Agenten sammelt und entschlüsselt, um festzustellen, welche Arten von Diensten überwacht werden können.
Alle verfügbaren Dienste zur Überwachung finden sich in der Liste, einschließlich der CPU-Speichernutzung und des freien Festplattenspeicherplatzes.
Um die Überwachung aller ermittelten Dienste zu aktivieren, müssen wir unter dem Abschnitt Undecided services (currently not monitored) auf die Schaltfläche Monitor klicken.
Dadurch wird die Seite aktualisiert, aber alle Dienste werden im Abschnitt Monitored services aufgelistet. Das zeigt uns, dass sie tatsächlich überwacht werden.
So wie bei der Änderung unseres Benutzerpassworts müssen diese neuen Änderungen gespeichert und aktiviert werden, bevor sie live gehen.
Betätigen Sie die Schaltfläche 2 changes und nehmen Sie die Änderungen über die Schaltfläche Activate affected an.
Danach wird die Host-Überwachung ausgeführt.
Sie können nun mit den Serverdaten arbeiten. Sehen Sie sich das Haupt-Dashboard über den Menüpunkt Overview / Main Overview links an.
Arbeiten mit Überwachungsdaten
Sehen wir uns das Haupt-Dashboard über den Menüpunkt Overview / Main Overview links an:
Überwachungs-Dashboard mit allen fehlerfreien Diensten
Die Erdkugel ist komplett grün und die Tabelle zeigt an, dass ein Host problemlos funktioniert.
Wir können (über das Menü links) die vollständige Host-Liste in der Ansicht Hosts / All hosts sehen, die jetzt einen einzelnen Host enthält.
Liste der Hosts mit allen fehlerfreien Diensten
Dort sehen wir, wie viele Dienste fehlerfrei funktionieren (grün dargestellt), wie viele gestört sind und wie viele noch zu überprüfen sind.
Nachdem wir auf den Hostnamen geklickt haben, sehen wir die Liste aller Dienste mit ihrem vollständigen Status und ihren Perf-O-Meters.
Perf-O-Meter zeigt die Leistung eines einzelnen Dienstes im Verhältnis zu dem, was Checkmk als guten Zustand betrachtet.
Details eines Host-Dienstestatus
Alle Dienste, die diagrammfähige Daten liefern, zeigen ein Diagrammsymbol neben ihrem Namen.
Über dieses Symbol lassen sich die mit dem Dienst verknüpften Diagramme öffnen.
Da die Host-Überwachung noch neu ist, zeigen die Diagramme noch fast nichts. Nach einiger Zeit liefern sie jedoch wertvolle Informationen darüber, wie sich unsere Dienstleistung verändert.
Diagramme zur Darstellung der CPU-Last auf dem Server
Wenn einer der Dienste ausfällt oder wiederhergestellt wird, wird diese Information im Dashboard angezeigt.
Ausgefallene Dienste werden mit einer roten Fehleranzeige gekennzeichnet und sind auch im Erddiagramm sichtbar.
Dashboard mit einem Host mit Problemen
Nach erfolgreicher Wiederherstellung wechselt die Anzeige auf Grün für ordnungsgemäßen Betrieb. Das Ereignisprotokoll rechts zeigt jedoch alle vergangenen Störungen an.
Dashboard mit einem Host, der nach den Problemen wiederhergestellt wurde
Nachdem wir uns das Dashboard jetzt etwas genauer angesehen haben, fügen wir einen zweiten Host zu unserer Überwachungsinstanz hinzu.
Schritt 6 - Überwachung eines zweiten CentOS Hosts
Überwachung ist vor allem für mehrere Hosts sehr nützlich.
Wir fügen zu unserer Checkmk-Instanz einen zweiten Server hinzu. Dieser läuft unter CentOS 7.
Wie bei unserem Ubuntu-Server ist die Installation von Checkmk Agent erforderlich, um Überwachungsdaten auf CentOS zu sammeln.
Dieses Mal benötigen wir jedoch ein rpm Paket von der Seite Monitoring Agents auf der Weboberfläche, das so genannte check-mk-agent-1.6.0p8-1.noarch. < ^ > rpm < ^ >.
Zuerst müssen wir jedoch xinetd installieren, das standardmäßig nicht auf der CentOS Installation verfügbar ist.
Wir erinnern uns: Xinetd ist ein Daemon, der dafür verantwortlich ist, die von check _ mk _ agent bereitgestellten Überwachungsdaten über das Netzwerk verfügbar zu machen.
Auf Ihrem CentOS-Server installieren Sie zuerst xinetd:
Jetzt können wir das für unseren CentOS-Server benötigte Überwachungsagenten-Paket herunterladen und installieren:
Wie zuvor können wir überprüfen, ob der Agent richtig funktioniert, indem wir check _ mk _ agent ausführen:
Die Ausgabe ist ähnlich wie die vom Ubuntu-Server.
Jetzt beschränken wir den Zugriff auf den Agenten.
Dieses Mal überwachen wir keinen lokalen Host. Deshalb muss xinetd Verbindungen vom Ubuntu-Server erlauben, auf dem Checkmk installiert ist, um Daten zu sammeln. Um das zu erlauben, öffnen Sie zunächst Ihre Konfigurationsdatei:
Hier sehen Sie die Konfiguration für Ihren check _ mk Dienst, die spezifiziert, wie auf Checkmk Agent über den xinetd Daemon zugegriffen werden kann.
Suchen Sie nach den folgenden zwei kommentierten Zeilen:
Entfernen Sie den Kommentar der zweiten Zeile und ersetzen Sie die lokalen IP-Adressen durch < ^ > your _ ubuntu _ server _ ip < ^ >:
Speichern und schließen Sie die Datei. indem Sie: x und dann ENTER eingeben.
Starten Sie den xinetd Dienst neu mit:
Wir können jetzt fortfahren, um Checkmk so zu konfigurieren, dass er unseren CentOS 7-Host überwacht.
Konfigurieren des neuen Host in Checkmk
Um zusätzliche Hosts zu Checkmk hinzuzufügen, nutzen wir das gleiche Hosts Menü wie zuvor.
Dieses Mal nennen wir den Host centos, konfigurieren seine IP-Adresse und wählen WAN (high-latency) unter dem Auswahlfeld Networking Segment, da sich der Host in einem anderen Netzwerk befindet.
Wenn wir den Punkt überspringen und ihn als lokal belassen würden, würde uns Checkmk bald darauf aufmerksam machen, dass der Host nicht funktioniert, da erwartet würde, dass er auf Agentenanfragen wesentlich schneller reagierte als über das Internet möglich.
Bildschirm Erstellen einer zweiten Host-Konfiguration
Klicken Sie auf Save & go to services, um die für die Überwachung auf dem CentOS-Server verfügbaren Dienste anzuzeigen.
Die Liste ist der des ersten Hosts sehr ähnlich.
Auch diesmal müssen wir auf Monitor klicken und die Änderungen mit der orangen Schaltfläche oben links aktivieren.
Nach Aktivierung der Änderungen können wir überprüfen, dass der Host auf der Seite All hosts überwacht wird.
Gehen Sie dorthin.
Es sind jetzt zwei Hosts, monitoring und centos, sichtbar.
Liste der Hosts mit zwei überwachten Hosts
Sie überwachen nun einen Ubuntu-Server und einen CentOS-Server mit Checkmk.
Es ist möglich, noch mehr Hosts zu überwachen.
Die einzige Einschränkung ist die Serverleistung. Wenn es sich jedoch nicht um mehrere hundert Hosts handelt, sollte dies kein Problem darstellen.
Das Verfahren ist für jeden weiteren Host identisch.
Checkmk Agenten in deb und rpm Paketen arbeiten auf Ubuntu, CentOS und den meisten anderen Linux-Distributionen.
In diesem Leitfaden haben wir Ihnen gezeigt, wie Sie zwei Server mit zwei verschiedenen Linux-Distributionen einrichten: Ubuntu und CentOS.
Wir haben dann Checkmk zur Überwachung der Server installiert und konfiguriert und uns die leistungsfähige Weboberfläche von Checkmk näher angesehen.
Checkmk ermöglicht das einfache Einrichten eines kompletten, vielseitigen Überwachungssystems, das den Aufwand der manuellen Konfiguration in eine einfach zu bedienende Weboberfläche mit vielen Optionen und Funktionen verwandelt.
Mit diesen Tools können Sie mehrere Hosts überwachen, E-Mail-, SMS- oder Push-Benachrichtigungen für Probleme einrichten, zusätzliche Kontrollen für weitere Dienste einrichten, Erreichbarkeit und Leistung überwachen usw.
Um mehr über Checkmk zu erfahren, sehen Sie sich auch die offizielle Dokumentation an.
Ausführen mehrerer PHP-Versionen auf einem Server mit Apache und PHP-FPM unter CentOS 7
5237
Einen CentOS 7-Server mit mindestens 1 GB RAM, der gemäß der Ersteinrichtung eines Servers mit CentOS 7 eingerichtet wurde, einschließlich eines sudo non-root users und einer Firewall.
Einen Apache-Webserver, der nach Installieren des Apache-Webservers unter CentOS 7 eingerichtet und konfiguriert wurde.
Einen Domänennamen, der so konfiguriert ist, dass er auf Ihren CentOS 7-Server verweist.
Wenn die Vorkehrungen abgeschlossen sind, können Sie jetzt die PHP-Versionen 7.0 und 7.2 installieren.
Das SCL (Software Collections) -Repository verwaltet zahlreiche Versionen des PHP-Stacks für das CentOS 7-System.
Wenn Sie die neueste Version von PHP benötigen und diese nicht auf SCL verfügbar ist, sehen Sie stattdessen im remi PPA (persönliches Paketarchiv) nach.
Beginnen Sie mit der Installation des SCL Repository in Ihrem System:
Wir wollen uns zunächst ansehen, welche PHP 7-Versionen auf SCL verfügbar sind:
Sie sehen eine Ausgabe wie diese:
Sie sehen, dass die neueste Version, PHP 7.3, auch verfügbar ist.
Für unsere Beispiele werden wir jedoch die Versionen 7.0 und 7.2 installieren.
Beginnen wir mit der älteren Version.
Installieren Sie rh-php70 und rh-php70-php-fpm:
rh-php70 ist ein Metapaket, das PHP-Anwendungen ausführt.
rh-php7.0-php-fpm bietet den Fast Process Manager-Interpreter, der als Daemon ausgeführt wird und Fast / CGI-Anfragen empfängt.
Installieren Sie rh-php72 und rh-php72-php-fpm.
Führen Sie als Nächstes die folgenden Befehle aus, um mit der Verwendung der beiden Software-Sammlungen zu beginnen:
Standardmäßig hören beide PHP-Versionen Port 9000 ab.
In diesem Tutorial möchten wir jedoch zwei Versionen gleichzeitig ausführen.
Wir benennen daher zwei neue Ports:
Dazu können Sie / etc / opt / rh / rh-php70 / php-fpm.d / www.conf in einem beliebigen Texteditor öffnen und 9000 überall in 9002 ändern.
Speichern und schließen Sie die Datei und wiederholen Sie den Vorgang für / etc / opt / rh / rh-php72 / php-fpm.d / www.conf, aber ersetzen Sie jetzt 9000 durch 9003.
Alternativ können Sie diese beiden sed-Befehle für die Ersetzungen verwenden:
Sie haben nun einen eigenen Port für jeden Ihrer PHP Dienste angegeben.
Bevor die Änderungen jedoch funktionieren, müssen Sie die Ports zu Ihrer SELinux Konfiguration hinzufügen.
SELinux ist die Kurzform für Security Enhanced Linux und ist standardmäßig in CentOS 7 aktiviert. Damit Ihre Anwendungen ausgeführt werden können, müssen Sie die neuen Ports 9002 und 9003 Ihrer SELinux Datenbank hinzufügen und den httpd Diensten zuweisen.
Verwenden Sie den Befehl semanage, um diese Aufgabe auszuführen:
Das Flag -a gibt an, dass Sie der Datenbank ein Objekt hinzufügen.
Das Flag -t gibt die Art des Objekts an. In diesem Fall handelt es sich um http _ port _ t.
Flag -p ordnet das tcp-Protokoll zu.
Sie können mehr über SELinux und den semanage-Befehl in diesem Tutorial oder durch einen Besuch der offiziellen SELinux-Dokumentation erfahren.
Sie können nun die PHP Dienste starten und aktivieren.
Beginnen Sie mit dem Dienst rh-php70-php-fpm und aktivieren Sie ihn zum Starten beim Booten:
Überprüfen Sie als Nächstes den Status des rh-php70-php-fpm-Dienstes:
Wiederholen Sie den Vorgang, starten Sie den rh-php72-php-fpm-Dienst und aktivieren Sie ihn, sodass er beim Booten gestartet wird:
Der Apache-Webserver wird standardmäßig als apache-Benutzer und apache-Gruppe ausgeführt.
So sollten / var / www / und alle zugehörigen Dateien und Unterverzeichnisse ebenfalls unter deren Eigentümerschaft fallen.
Führen Sie die folgenden Befehle aus, um die richtigen Eigentümerschaften und Berechtigungen der Website-Stammverzeichnisse zu überprüfen:
Der Befehl chown ändert die Eigentümerschaften der beiden Website-Verzeichnisse auf den apache-Benutzer und die apache-Gruppe.
Der Befehl chmod ändert die Berechtigungen, die mit diesem Benutzer und der Gruppe verknüpft sind, sowie mit anderen.
Sie erstellen zwei neue virtuelle Host-Konfigurationsdateien im Verzeichnis / etc / httpd / conf.d /.
Hier weisen Sie Apache an, Inhalte mit PHP 7.0 zu rendern:
Stellen Sie sicher, dass der Website-Verzeichnispfad, der Servername, der Port und die PHP-Version mit Ihrer Einrichtung übereinstimmen:
Für DocumentRoot geben Sie den Pfad Ihres Website-Stammverzeichnisses ein.
Für ServerAdmin geben Sie eine E-Mail an, auf die der Administrator der Site < ^ > your _ domain < ^ > zugreifen kann.
Für ServerName fügen Sie die URL für Ihre erste Subdomäne hinzu.
Für SetHandler geben Sie Port < ^ > 9002 < ^ > an.
Die verbleibenden Anweisungen konfigurieren Ihren Dienst zudem für die Bereitstellung von PHP 7.0.
Geben Sie diese Subdomäne für die Bereitstellung von PHP 7.2 an:
Stellen Sie auch hier sicher, dass der Website-Verzeichnispfad, der Servername und die PHP-Version mit Ihren eindeutigen Informationen übereinstimmen:
Sie sehen eine Ausgabe von Syntax OK:
Da sie sensible Informationen über Ihren Server enthalten und für unberechtigte Benutzer zugänglich sind, stellen sie ein Sicherheitsrisiko dar.
Entfernen Sie die Dateien:
Sie haben nun einen einzigen CentOS 7-Server, der zwei Websites mit zwei verschiedenen PHP-Versionen verwaltet.
Von hier aus können Sie die fortgeschritteneren Funktionen von PHP-FPM erkunden, wie den adaptiven Spawning-Prozess oder die Protokollierung von sdtout und stderr. Oder Sie sichern jetzt Ihre Websites.
So installieren Sie Drupal mit Docker Compose
5299
Der Autor hat die United Nations Foundation dazu ausgewählt, im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
Die ursprüngliche WordPress-Version dieses Tutorials wurde von Kathleen Juell verfasst.
Drupal ist ein Content-Management-System (CMS), das in PHP geschrieben und unter der Open-Source-Lizenz GNU General Public License vergeben wird.
Menschen und Organisationen in der ganzen Welt verwenden Drupal, um öffentliche Websites, persönliche Blogs, Unternehmen und mehr zu betreiben.
Was Drupal von anderen CMS-Frameworks abhebt, ist seine wachsende Community und eine Reihe von Funktionen, die sichere Prozesse, zuverlässige Leistung, Modularität und Flexibilität zur Anpassung umfassen.
Drupal erfordert die Installation des LAMP-Stacks (Linux, Apache, MySQL und PHP) oder des LEMP-Stacks (Linux, Nginx, MySQL und PHP), aber das Installieren einzelner Komponenten ist eine zeitaufwendige Aufgabe.
Wir können Tools wie Docker und Docker Compose verwenden, um den Prozess der Installation von Drupal zu vereinfachen.
Dieses Tutorial verwendet Docker-Images zur Installation einzelner Komponenten in den Docker-Containern.
Mit der Verwendung von Docker Compose können wir mehrere Container für die Datenbank, die Anwendung und für die Vernetzung / Kommunikation zwischen ihnen definieren und verwalten.
In diesem Tutorial installieren wir Drupal mit Docker Compose, damit wir die Containerisierung nutzen und unsere Drupal-Website auf Servern bereitstellen können.
Wir führen Container für eine MySQL-Datenbank, einen Nginx-Webserver und Drupal aus.
Außerdem sichern wir unsere Installation, indem wir TLS / SSL Zertifikate mit Let 's Encrypt für die Domäne erlangen, die wir mit unserer Website verknüpfen möchten.
Schließlich richten wir einen cron-Job ein, um unsere Zertifikate zu erneuern, damit unsere Domäne sicher bleibt.
Einen Server, auf dem Ubuntu 18.04 ausgeführt wird, zusammen mit einem non-root user mit sudo-Privilegien und einer aktiven Firewall.
Dieses Tutorial wurde unter der Version 19.03.8 getestet.
Dieses Tutorial wurde unter der Version 1.21.2 getestet.
Falls Sie ein DigitalOcean-Konto nutzen, können Sie in der Einführung in DigitalOcean-DNS im Einzelnen nachlesen, wie Sie diese hinzufügen:
Schritt 1 - Definieren der Webserver-Konfiguration
Bevor wir Container ausführen, müssen wir die Konfiguration für unseren Nginx-Webserver definieren.
Unsere Konfigurationsdatei enthält einige Drupal-spezifische Location-Blocks, zusammen mit einem Location-Block zur Weiterleitung von Verifizierungsanforderungen von Let 's Encrypt an den Certbot-Client für automatisierte Zertifikatserneuerungen.
Zuerst erstellen wir ein Projektverzeichnis für unser Drupal-Setup namens drupal:
Rufen Sie das neu erstellte Verzeichnis auf:
Nun können wir ein Verzeichnis für unsere Konfigurationsdatei erstellen:
Öffnen Sie die Datei mit nano oder Ihrem bevorzugten Texteditor:
In dieser Datei fügen wir einen Serverblock mit Anweisungen für unseren Servernamen und die Dokumenten-root hinzu sowie Location-Blocks, um die Anforderung des Certbot-Clients nach Zertifikaten, PHP-Verarbeitung und statischen Asset-Anforderungen zu leiten.
Fügen Sie den folgenden Code in die Datei ein.
Achten Sie darauf, < ^ > your _ domain < ^ > durch Ihren eigenen Domänennamen zu ersetzen:
Unser Serverblock enthält folgende Informationen:
Anweisungen:
listen: Diese weist Nginx an, an Port 80 zu lauschen, sodass wir das Webroot-Plugin von Certbot für unsere Zertifikatsanforderungen nutzen können.
Beachten Sie, dass wir noch nicht Port 443 einschließen - wir aktualisieren unsere Konfiguration, um SSL einzuschließen, sobald wir erfolgreich unsere Zertifikate erlangt haben.
server _ name: Damit definieren Sie den Servernamen und den Serverblock, der für Anfragen an den Server verwendet werden sollte.
Achten Sie darauf, < ^ > your _ domain < ^ > in dieser Zeile durch Ihren eigenen Domänennamen zu ersetzen.
index: Die Anweisung index definiert die Dateien, die als Indizes bei der Verarbeitung von Anfragen an unseren Server dienen.
Wir haben hier die Standardreihenfolge der Priorität geändert und index.php vor index.html geschoben, damit Nginx Dateien namens index.php wenn möglich priorisiert.
root: Unsere root-Anweisung benennt das root-Verzeichnis für Anfragen an unseren Server.
Dieses Verzeichnis, / var / www / html, wird als Bereitstellungspunkt in der Erstellungszeit durch Anweisungen in unserer Drupal Dockerfile erstellt.
Diese Dockerfile-Anweisungen stellen auch sicher, dass die Dateien aus der Drupal-Version auf diesem Volume bereitgestellt werden.
rewrite: Wenn die angegebene reguläre Expression (< ^ > ^ / core / authorize.php / core / authorize.php (. *) $< ^ >) mit einer URI-Anfrage übereinstimmt, wird die URI wie in der Ersatzzeichenfolge (< ^ > / core / authorize.php $1 < ^ >) angegeben geändert.
Location-Blocks:
location ~ / .well-known / acme-challenge: Dieser Location-Block verwaltet Anfragen an das Verzeichnis .well-known, in dem Certbot eine temporäre Datei ablegt, um zu validieren, dass das DNS für unsere Domäne auf unserem Server aufgelöst wird.
Mit dieser Konfiguration können wir das Webroot-Plugin von Certbot verwenden, um Zertifikate für unsere Domäne zu erlangen.
location /: In diesem Location-Block verwenden wir eine try _ files-Anweisung, um nach Dateien zu suchen, die individuellen URI-Anfragen entsprechen.
Anstatt jedoch einen 404 Not Found-Status auszugeben, übergeben wir die Steuerung an die Datei index.php von Drupal mit den Anforderungsargumenten.
location ~\ .php $: Dieser Location-Block verwaltet die PHP-Verarbeitung und überträgt diese Anfragen an unseren drupal-Container.
Da unser Drupal-Docker-Image auf dem php: fpm-Image basiert, schließen wir auch Konfigurationsoptionen ein, die in diesem Block für das FastCGI-Protokoll spezifisch sind.
Nginx erfordert einen unabhängigen PHP-Prozessor für PHP-Anfragen: In unserem Fall werden diese Anfragen vom php-fpm-Prozessor verwaltet, der im php: fpm-Image enthalten ist.
Außerdem enthält dieser Location-Block FastCGI-spezifische Anweisungen, Variablen und Optionen, die Anfragen an die in unserem Drupal-Container ausgeführte Drupal-Anwendung überträgt, den bevorzugten Index für die analysierte Anfrage-URI festlegt und URI-Anfragen analysiert.
location ~ /\ .ht: Dieser Block verwaltet .htaccess-Dateien, da Nginx diese nicht bedient.
Die Anweisung deny _ all stellt sicher, dass .htaccess-Dateien nie für Benutzer bereitgestellt werden.
location = / favicon.ico, location = / robots.txt: Diese Blocks stellen sicher, dass Anfragen an / favicon.ico und / robots.txt nicht protokolliert werden.
location ~ *\. (css | gif | ico | jpeg | jpg | js | png) $: Dieser Block schaltet die Protokollierung für statische Asset-Anfragen ab und stellt sicher, dass diese Assets in hohem Maße zwischenspeicherbar sind, da ihre Bereitstellung in der Regel aufwendig ist.
Weitere Informationen zu FastCGI-Proxying finden Sie in Verstehen und Implementieren von FastCGI-Proxying in Nginx.
Informationen zu Server- und Location-Blocks finden Sie in Verstehen von Nginx-Server- und Location-Block-Auswahlalgorithmen.
Nach Einrichtung der Nginx-Konfiguration können Sie Umgebungsvariablen erstellen, die zur Laufzeit an Ihre Anwendungs- und Datenbankcontainer übergeben werden.
Schritt 2 - Definieren der Umgebungsvariablen
Unsere Drupal-Anwendung benötigt eine Datenbank (MySQL, PostgresSQL, etc.) zum Speichern von Informationen im Zusammenhang mit der Website.
Der Drupal-Container benötigt Zugriff auf bestimmte Umgebungsvariablen zur Laufzeit, um Zugriff auf den Datenbank-Container (MySQL) zu ermöglichen.
Diese Variablen enthalten sensible Informationen wie die Zugangsdaten der Datenbank, sodass wir sie nicht direkt in der Docker-Compose-Datei offenlegen können - der Hauptdatei, die Informationen darüber enthält, wie unsere Container ausgeführt werden.
Es wird immer empfohlen, die sensiblen Werte in der .env-Datei festzulegen und ihre Zirkulation zu beschränken.
Dadurch werden diese Werte nicht in unsere Projekt-Repositorys kopiert und öffentlich verfügbar gemacht.
Erstellen und öffnen Sie im Haupt-Projektverzeichnis ~ / drupal eine Datei namens .env:
Fügen Sie die folgenden Variablen in die .env-Datei ein und ersetzen Sie dabei die hervorgehobenen Abschnitte mit den Zugangsdaten, die Sie verwenden möchten:
Wir haben nun das Passwort für das MySQL-root-Administratorkonto sowie unseren bevorzugten Benutzernamen und das Passwort für unsere Anwendungsdatenbank hinzugefügt.
Unsere .env-Datei enthält sensible Informationen. Es wird daher immer empfohlen, sie in die .gitignore- und .dockerignore-Dateien des Projekts einzufügen, damit sie nicht in unseren Git-Repositorys und Docker-Images hinzugefügt werden.
Wenn Sie mit Git zur Versionskontrolle arbeiten möchten, initialisieren Sie Ihr aktuelles Arbeitsverzeichnis als ein Repository mit git init:
Öffnen Sie die .gitignore-Datei:
Öffnen Sie in ähnlicher Weise die .dockerignore-Datei:
Wir haben nun die Maßnahmen zur Sicherung unserer Zugangsdaten als Umgebungsvariablen ergriffen. Im nächsten Schritt definieren wir unsere Dienste in einer docker-compose.yml-Datei.
Schritt 3 - Definieren der Dienste mit Docker Compose
Docker Compose ist ein Tool zum Definieren und Ausführen von Docker-Anwendungen mit mehreren Containern.
Wir definieren eine YAML-Datei, um die Dienste unserer Anwendung zu konfigurieren.
Ein Dienst oder service in Docker Compose ist ein Container, der ausgeführt wird. Compose ermöglicht uns, diese Dienste zusammen mit geteilten Volumes und Netzwerken zu verknüpfen.
Wir erstellen verschiedene Container für die Drupal-Anwendung, die Datenbank und den Webserver.
Außerdem erstellen wir einen Container, der Certbot ausführt, um Zertifikate für unseren Webserver zu erlangen.
Erstellen Sie eine docker-compose.yml-Datei:
Fügen Sie den folgenden Code hinzu, um die Compose-Dateiversion und den mysql-Datenbankdienst zu definieren:
Wir behandeln diese nacheinander mit allen Konfigurationsoptionen des mysql-Dienstes:
image: Hiermit wird das Image festgelegt, das wir zum Erstellen des Containers verwenden / pullen.
Es wird immer empfohlen, das Image mit dem richtigen Versions-Tag unter Ausschluss des latest-Tags zu verwenden, um zukünftige Konflikte zu vermeiden.
Lesen Sie dazu mehr in Best Practices für Dockerfiles in der Docker-Dokumentation.
container _ name: Hiermit wird der Name des Containers definiert.
command: Diese Option wird verwendet, um den Standardbefehl (CMD-Instruktion) im Image zu überschreiben.
MySQL hat verschiedene Authentifizierungs-Plugins unterstützt, aber mysql _ native _ password ist die traditionelle Methode zur Authentifizierung.
Da PHP und damit Drupal nicht die neuere MySQL-Authentifizierung unterstützen, müssen wir das --default-authentication-plugin = mysql _ native _ password als standardmäßigen Authentifizierungsmechanismus festlegen.
restart: Diese Option wird verwendet, um die Neustartregel des Containers festzulegen.
Die Regel unless-stopped startet einen Container neu, bis er manuell gestoppt wird.
env _ file: Hiermit werden die Umgebungsvariablen aus einer Datei hinzugefügt.
In unserem Fall werden die Umgebungsvariablen aus der im vorherigen Schritt definierten .env-Datei gelesen.
volumes: Hiermit werden Host-Pfade oder benannte Volumes, die als Suboptionen eines Dienstes angegeben sind, bereitgestellt.
Wir stellen ein benanntes Volume namens db-data im Verzeichnis / var / lib / mysql des Containers bereit, in der MySQL standardmäßig seine Datendateien schreibt.
networks: Hiermit wird das internal-Netzwerk, an das sich unser Anwendungsdienst anschließt, definiert.
Wir definieren die Netzwerke am Ende der Datei.
Wir haben unsere mysql-Dienstdefiniton definiert. Nun fügen wir die Definition des drupal-Anwendungsdienstes am Ende der Datei hinzu:
In dieser Dienstdefinition benennen wir unseren Container und definieren eine Neustartregel, wie wir es beim mysql-Dienst getan haben.
Außerdem fügen wir einige Optionen hinzu, die für diesen Container spezifisch sind:
image: Hier verwenden wir Drupal-Image 8.7.8-fpm-alpine.
Dieses Image verfügt über den php-fpm-Prozessor, den unser Nginx-Webserver zur Verwaltung der PHP-Verarbeitung benötigt.
Darüber hinaus verwenden wir das aus dem Alpine-Linux-Projekt erlangte alpine-Image, das die Größe des Gesamtimages reduziert und in den Best Practices für Dockerfiles empfohlen wird.
Drupal verfügt über weitere Versionen von Images, die Sie auf Dockerhub finden können.
depends _ on: Diese Option wird verwendet, um Abhängigkeit zwischen Diensten auszudrücken.
Das Definieren des mysql-Dienstes als Abhängigkeit unseres drupal-Containers stellt sicher, dass unser drupal-Container nach dem mysql-Container erstellt wird und ermöglicht einen reibungslosen Start unserer Anwendung.
networks: Hier haben wir diesen Container zusammen mit dem internal-Netzwerk dem external-Netzwerk hinzugefügt.
Dadurch wird sichergestellt, dass unser mysql-Dienst nur über das internal-Netzwerk aus dem drupal-Container zugänglich ist, während dieser Container über das external-Netzwerk für andere Container weiterhin zugänglich bleibt.
volumes: Wir stellen ein benanntes Volume namens drupal-data im Bereitstellungspunkt / var / www / html bereit, der vom Drupal-Image erstellt wurde.
Wenn wir ein benanntes Volume auf diese Weise verwenden, können wir unseren Anwendungscode mit anderen Containern teilen.
Als Nächstes fügen wir die Nginx-Dienstdefinition nach der drupal-Dienstdefinition hinzu:
Auch hier benennen wir unseren Container und machen ihn in der Reihenfolge des Starts vom Drupal-Container abhängig.
Außerdem verwenden wir ein alpine-Image - das Nginx-Image 1.17.4-alpine.
Diese Dienstdefinition enthält auch die folgenden Optionen:
ports: Hiermit wird Port 80 zur Verfügung gestellt, um die Konfigurationsoptionen zu aktivieren, die wir in unserer Datei nginx.conf in Schritt 1 definiert haben.
volumes: Hier definieren wir sowohl das benannte Volume als auch den Host-Pfad:
drupal-data: / var / www / html: Hiermit wird unser Drupal-Anwendungscode im Verzeichnis / var / www / html, das wir als die root in unserem Nginx-Serverblock festgelegt haben, bereitgestellt.
. / nginx-conf: / etc / nginx / conf.d: Hiermit wird das Nginx-Konfigurationsverzeichnis auf dem Host in dem entsprechenden Verzeichnis im Container bereitgestellt. Dadurch wird sichergestellt, dass alle Änderungen, die wir an Dateien auf dem Host vornehmen, im Container reflektiert werden.
certbot-etc: / etc / letsencrypt: Hiermit werden die relevanten Let 's-Encrypt-Zertifikate und die Schlüssel für unsere Domäne in dem entsprechenden Verzeichnis des Containers bereitgestellt.
networks: Wir haben das external-Netzwerk nur definiert, damit dieser Container mit dem drupal-Container, aber nicht mit dem mysql-Container kommunizieren kann.
Schließlich fügen wir unsere letzte Dienstdefinition für den certbot-Dienst hinzu.
Achten Sie darauf, < ^ > sammy @ your _ domain < ^ > und < ^ > your _ domain < ^ > durch Ihre eigene E-Mail-Adresse und den Domänennamen zu ersetzen:
Diese Definition weist Compose an, das Image certbot / certbot von Docker Hub zu pullen.
Außerdem werden benannte Volumes verwendet, um Ressourcen für den Nginx-Container freizugeben, einschließlich der Domänenzertifikate und des Schlüssels in certbot-etc und des Anwendungscodes in drupal-data.
Wir haben auch depends _ on verwendet, um sicherzustellen, dass der certbot-Container gestartet wird, nachdem der webserver-Dienst ausgeführt wird.
Wir haben hier keine networks angegeben, da dieser Container mit keinem Dienst über das Netzwerk kommunizieren wird.
Es werden nur die Domänenzertifikate und der Schlüssel hinzugefügt, die wir mit den benannten Volumes bereitgestellt haben.
Außerdem haben wir die Option command eingeschlossen, die einen Unterbefehl angibt, der mit dem Standardbefehl certbot des Containers ausgeführt werden soll.
Der Certbot-Client unterstützt Plugins zur Erstellung und Installation von Zertifikaten.
Wir verwenden das webroot-Plugin, um ein Zertifikat zu erlangen, indem wir certonly und --webroot in die Befehlszeile einfügen.
Lesen Sie mehr über das Plugin und zusätzliche Befehle in der offiziellen Certbot-Dokumentation.
Fügen Sie nach der certbot-Dienstdefinition die Netzwerk- und Volume-Definitionen hinzu:
Mit dem networks-Schlüssel der obersten Ebene können wir die zu erstellenden Netzwerke angeben. networks ermöglicht die Kommunikation über die Dienste / Container auf allen Ports, da sie sich auf demselben Docker-Daemon-Host befinden.
Wir haben zwei Netzwerke, internal und external, definiert, um die Kommunikation der Dienste webserver, drupal und mysql zu sichern.
Der volumes-Schlüssel wird verwendet, um die benannten Volumes drupal-data, db-data und certbot-etc zu definieren.
Wenn Docker Volumes erstellt, werden die Inhalte des Volumes in einem Verzeichnis des Host-Dateisystems, / var / lib / docker / volumes /, gespeichert, das von Docker verwaltet wird.
Die Inhalte jedes Volumes werden dann von diesem Verzeichnis in jedem Container bereitgestellt, der das Volume verwendet.
Auf diese Weise können Code und Daten zwischen Containern geteilt werden.
Die fertige docker-compose.yml-Datei sieht so aus:
Nun sind unsere Dienstdefinitionen fertig.
Als Nächstes starten wir den Container und testen unsere Zertifikatsanforderungen.
Schritt 4 - Erlangen der SSL-Zertifikate und Zugangsdaten
Wir können unsere Container mit dem Befehl docker-compose up starten, der die Container in der von uns angegebenen Reihenfolge erstellt und ausführt.
Wenn unsere Domain-Anfragen erfolgreich sind, sehen wir den richtigen Exit-Status in unserer Ausgabe und die richtigen Zertifikate, die im Ordner / etc / letsencrypt / live im Webservercontainer bereitgestellt werden.
Um die Container im Hintergrund auszuführen, verwenden Sie den Befehl docker-compose up mit dem Flag -d:
Sie sehen eine ähnliche Ausgabe, die bestätigt, dass Ihre Dienste erstellt wurden:
Überprüfen Sie den Status der Dienste mit dem Befehl docker-compose ps:
Wir sehen die Dienste mysql, drupal und webserver mit einem State von Up, während certbot mit einer Statusmeldung von 0 beendet wird:
Wenn Sie in der Spalte State für die Dienste mysql, drupal oder webserver etwas anderes als Up sehen, oder für den certbot-Container einen anderen Exit-Status als 0, überprüfen Sie die Dienstprotokolle mit dem Befehl docker-compose logs:
Wir können nun mit dem Befehl docker-compose exec überprüfen, dass unsere Zertifikate auf dem webserver bereitgestellt wurden:
Nachdem nun alles erfolgreich ausgeführt wird, können wir unsere certbot-Dienstdefinition bearbeiten, um das Flag --staging zu entfernen.
Öffnen Sie die Datei docker-compose.yml, gehen Sie zur certbot-Dienstdefinition und ersetzen Sie das Flag --staging in der Befehlsoption mit dem Flag --force-renewal, das Certbot mitteilt, dass Sie ein neues Zertifikat mit denselben Domänen wie ein vorhandenes Zertifikat anfordern möchten.
Die aktualisierte Definition von certbot sieht wie folgt aus:
Wir müssen docker-compose erneut ausführen, um den certbot-Container neu zu erstellen.
Wir schließen auch die Option --no-deps ein, um Compose mitzuteilen, dass das Starten des webserver-Dienstes übersprungen werden kann, da dieser bereits ausgeführt wird:
In der Ausgabe sehen wir, dass unsere Zertifikatsanforderung erfolgreich war:
Nachdem wir unsere Zertifikate erfolgreich generiert haben, können wir unsere Nginx-Konfiguration aktualisieren, um SSL einzuschließen.
Schritt 5 - Ändern der Webserver-Konfiguration und Dienstdefinition
Nach der Installation von SSL-Zertifikaten in Nginx müssen wir alle HTTP-Anfragen an HTTPS umleiten.
Außerdem müssen wir unser SSL-Zertifikat und unsere Schlüsselpositionen angeben und Sicherheitsparameter sowie Header hinzufügen.
Da Sie den webserver-Dienst neu erstellen, um diese Ergänzungen einzuschließen, können Sie diesen jetzt stoppen:
Als Nächstes entfernen wir die zuvor erstellte Nginx-Konfigurationsdatei:
Öffnen Sie eine andere Version der Datei:
Fügen Sie der Datei den folgenden Code hinzu, um HTTP an HTTPS umzuleiten und SSL-Zugangsdaten, Protokolle und Sicherheitsheader hinzuzufügen.
Denken Sie daran, < ^ > your _ domain < ^ > durch Ihre eigene Domäne zu ersetzen:
Der HTTP-Serverblock gibt das Webroot-Plugin für Certbot-Erneuerungsanfragen an das Verzeichnis .well-known / acme-challenge an.
Er enthält auch eine rewrite-Anweisung, die HTTP-Anfragen an das root-Verzeichnis an HTTPS leitet.
Der HTTPS-Serverblock aktiviert ssl und http2.
Weitere Informationen dazu, wie HTTP / 2 über HTTP-Protokolle iteriert und welche Vorteile es für die Website-Leistung haben kann, lesen Sie bitte in der Einführung Einrichten von Nginx mit HTTP / 2-Support unter Ubuntu 18.04.
Diese Blocks aktivieren SSL, da wir unser SSL-Zertifikat und die Schlüsselpositionen zusammen mit den empfohlenen Headern eingefügt haben.
Diese Header ermöglichen uns eine A-Bewertung auf den Server-Testseiten SSL-Labs und Security Headers.
Unsere Anweisungen root und index befinden sich ebenfalls in diesem Block, ebenso wie die restlichen Drupal-spezifischen Location-Blocks, die in Schritt 1 behandelt wurden.
Speichern und schließen Sie die aktualisierte Nginx-Konfigurationsdatei.
Bevor wir den webserver-Container neu erstellen, müssen wir unserem webserver-Dienst ein 443-Port-Mapping hinzufügen, da wir SSL-Zertifikate aktiviert haben.
Führen Sie die folgenden Änderungen in der webserver-Dienstdefinition aus:
Nach dem Aktivieren der SSL-Zertifikate sieht unsere docker-compose.yml wie folgt aus:
Wir erstellen den webserver-Dienst mit unserer aktualisierten Konfiguration nun neu:
Überprüfen Sie die Dienste mit docker-compose ps:
Wir sehen die Dienste mysql, drupal und webserver als Up, während certbot mit der Statusmeldung 0 beendet wird:
Jetzt werden alle unsere Dienste ausgeführt und wir können mit der Installation von Drupal über die Web-Oberfläche fortfahren.
Schritt 6 - Abschließen der Installation über die Weboberfläche
Beenden wir die Installation über die Weboberfläche von Drupal.
Navigieren Sie in einem Webbrowser zur Domäne des Servers.
Denken Sie daran, your _ domain hier mit Ihrem eigenen Domänennamen zu ersetzen:
Sprachauswahlseite auf der Drupal-Weboberfläche
Klicken Sie auf Speichern und fortfahren.
Wir werden zur Seite der Installationsprofile geleitet.
Drupal bietet verschiedene Profile. Wählen Sie das Profil Standard und klicken Sie auf Speichern und fortfahren.
Profilauswahlseite auf der Drupal-Weboberfläche
Nach Auswahl des Profils gehen wir weiter zur Seite der Datenbankkonfiguration.
Wählen Sie als Datenbanktyp MySQL, MariaDB, Percona Server oder äquivalent aus. Geben Sie die Werte für Datenbankname, Benutzername und Passwort entsprechend der Werte von MYSQL _ DATABASE, MYSQL _ USER und MYSQL _ PASSWORD ein, die jeweils in der .env-Datei in Schritt 2 definiert wurden. Klicken Sie auf Erweiterte Optionen und setzen Sie den Wert für Host auf den Namen des mysql-Dienstcontainers.
Datenbankeinrichtungsseite auf der Drupal-Weboberfläche
Nach Konfiguration der Datenbank beginnt die Installation der Standardmodule und Themen von Drupal:
Installationsseite der Website auf der Drupal-Weboberfläche
Nachdem die Website installiert ist, werden wir auf die Einrichtungsseite der Drupal-Website geleitet, um den Namen der Website, die E-Mail-Adresse, den Benutzernamen, das Passwort und die Ländereinstellungen zu konfigurieren.
Geben Sie die Informationen an und klicken Sie auf Speichern und fortfahren:
Konfigurationsseite der Website auf der Drupal-Weboberfläche
Nachdem wir auf Speichern und fortfahren geklickt haben, sehen wir die Seite Willkommen bei Drupal. Diese zeigt uns, dass unsere Drupal-Website eingerichtet ist und erfolgreich ausgeführt wird.
Seite Willkommen bei Drupal auf der Drupal-Weboberfläche
Nachdem die Installation von Drupal nun abgeschlossen ist, müssen wir sicherstellen, dass unsere SSL-Zertifikate automatisch erneuert werden.
Schritt 7 - Erneuern der Zertifikate
Let "s-Encrypt-Zertifikate sind 90 Tage gültig. Daher müssen wir einen automatisierten Erneuerungsprozess einrichten, um sicherzustellen, dass sie nicht ablaufen.
Eine Möglichkeit hierzu ist das Erstellen eines Jobs mit dem Planungsprogramm cron.
In diesem Fall erstellen wir einen cron-Job, der in regelmäßigen Abständen ein Skript ausführt, das unsere Zertifikate erneuert und die Nginx-Konfiguration neu lädt.
Wir erstellen die Datei ssl _ renew.sh, um unsere Zertifikate zu erneuern:
Fügen Sie folgenden Code hinzu.
Denken Sie daran, den Verzeichnisnamen durch Ihren eigenen non-root user zu ersetzen:
Das Skript wechselt zum ~ / drupal-Projektverzeichnis und führt die folgenden docker-compose-Befehle aus.
docker-compose run: Hiermit wird ein certbot-Container gestartet und der in unserer certbot-Dienstdefinition verfügbare command überschrieben.
Anstatt des Unterbefehls certonly verwenden wir hier den Unterbefehl renew, mit dem Zertifikate, die kurz vor Ablauf stehen, erneuert werden.
Wir haben hier die Option --dry-run zum Testen unseres Skripts hinzugefügt.
docker-compose kill: Hiermit wird ein SIGHUP-Signal an den webserver-Container gesendet, um die Nginx-Konfiguration neu zu laden.
Schließen Sie die Datei und machen Sie sie mit folgendem Befehl ausführbar:
Öffnen Sie als Nächstes die Datei root crontab, um das Erneuerungsskript in einem angegebenen Intervall auszuführen:
Wenn Sie die Datei zum ersten Mal bearbeiten, werden Sie dazu aufgefordert, einen Texteditor zum Öffnen der Datei zu wählen:
Fügen Sie am Ende der Datei die folgende Zeile hinzu und ersetzen Sie sammy mit Ihrem Benutzernamen:
Hiermit wird das Job-Intervall auf alle fünf Minuten festgelegt, sodass wir testen können, ob unsere Erneuerungsanfrage wie beabsichtigt funktioniert hat oder nicht.
Außerdem haben wir eine Protokolldatei, cron.log, zum Aufzeichnen der relevanten Ausgabe erstellt.
Verwenden Sie nach fünf Minuten den Befehl tail, um cron.log zu prüfen und zu sehen, ob die Erneuerungsanfrage erfolgreich war oder nicht:
Sie sehen eine Ausgabe, die eine erfolgreiche Erneuerung bestätigt:
Drücken Sie STRG + C, um den tail-Prozess zu beenden.
Wir können jetzt die Datei crontab ändern, um das Skript jeden zweiten Tag der Woche um 02: 00 Uhr auszuführen.
Ändern Sie die letzte Zeile der crontab wie folgt:
Nun entfernen wir die Option --dry-run aus dem Skript ssl _ renew.sh.
Öffnen Sie es zunächst:
Ändern Sie dann den Inhalt wie folgt:
Unser cron-Job kümmert sich nun um den Ablauf unserer SSL-Zertifikate, indem er sie erneuert, wenn sie hierfür infrage kommen.
In diesem Tutorial haben wir Docker Compose verwendet, um eine Drupal-Installation mit einem Nginx-Webserver zu erstellen.
Als Teil des Workflows haben wir TLS / SSL-Zertifikate für die Domäne erlangt, mit der wir unsere Drupal-Website verknüpfen wollten, und einen cron-Job erstellt, um diese Zertifikate bei Bedarf zu erneuern.
Wenn Sie mehr über Docker erfahren möchten, besuchen Sie unsere Docker-Themenseite.
So installieren Sie Linux, Nginx, MySQL und PHP (LEMP-Stack) unter Ubuntu 20.04
5389
Es handelt sich um ein Akronym, das ein Linux-Betriebssystem mit einem Nginx-Webserver (ausgesprochen wie "Engine-X ") beschreibt.
Die Backend-Daten werden in einer MySQL-Datenbank gespeichert und die dynamische Verarbeitung wird mit PHP gehandhabt.
Dieser Leitfaden zeigt, wie Sie einen LEMP-Stack auf einem Ubuntu 20.04-Server installieren.
Das Ubuntu-Betriebssystem erfüllt die erste Anforderung.
Wir beschreiben, wie Sie den Rest der Komponenten einrichten und ausführen.
Um dieses Tutorial zu absolvieren, benötigen Sie Zugriff auf einen Ubuntu-20.04-Server als ein regulärer non-root sudo user und eine auf Ihrem Server aktivierte Firewall.
Sie können zur Einrichtung unserem Leitfaden zur Ersteinrichtung eines Servers für Ubuntu 20.04 folgen.
Schritt 1 - Installieren des Nginx-Webservers
Wir verwenden den apt-Paketmanager, um diese Software zu erlangen.
Da wir apt für diese Sitzung zum ersten Mal verwenden, beginnen Sie mit der Aktualisierung des Paketindexes Ihres Servers.
Danach können Sie apt install zum Installieren von Nginx verwenden:
Geben Sie bei der entsprechenden Aufforderung Y ein, um zu bestätigen, dass Sie Nginx installieren möchten.
Sobald die Installation abgeschlossen ist, ist der Nginx-Webserver aktiv und wird auf Ihrem Ubuntu-20.04-Server ausgeführt.
Wenn Sie die in unserem Leitfaden zur Ersteinrichtung eines Servers empfohlene ufw aktiviert haben, müssen Sie Verbindungen zu Nginx zulassen.
Nginx registriert bei der Installation einige verschiedene UFW-Anwendungsprofile.
Um zu prüfen, welche UFW-Profile verfügbar sind, führen Sie nun Folgendes aus:
Es wird empfohlen, das restriktivste Profil zu aktivieren, das den von Ihnen benötigten Datenverkehr immer noch zulässt.
Da Sie SSL für Ihren Server in diesem Leitfaden nicht konfiguriert haben, müssen Sie nur den regulären HTTP-Verkehr auf Port 80 zulassen.
Aktivieren Sie dies durch die Eingabe von:
Sie können die Änderung überprüfen, indem Sie Folgendes ausführen:
Die Ausgabe des Befehls zeigt, dass der HTTP-Verkehr jetzt zugelassen wird:
Nachdem die neue Firewall-Regel hinzugefügt wurde, können Sie testen, ob der Server ausgeführt wird, indem Sie auf Ihren Domänennamen oder die öffentliche IP-Adresse Ihres Servers in Ihrem Webbrowser zugreifen.
Wenn Sie keinen Domänennamen haben, der auf den Server verweist, und Sie die öffentliche IP-Adresse Ihres Servers nicht kennen, können Sie diese mit dem folgenden Befehl finden:
Nginx-Standardseite
Wenn Sie diese Seite sehen können, haben Sie Nginx erfolgreich installiert und den HTTP-Verkehr für Ihren Webserver aktiviert.
Als Nächstes installieren wir PHP, die letzte Komponente im LEMP-Stack.
Sie haben Nginx installiert, um Ihre Inhalte bereitzustellen, und MySQL, um Ihre Daten zu speichern und zu verwalten. Jetzt können Sie PHP installieren, um Code zu verarbeiten und dynamische Inhalte für den Webserver zu generieren.
Während Apache den PHP-Interpreter in jede Anfrage einbindet, benötigt Nginx ein externes Programm, das die PHP-Verarbeitung handhabt und als Brücke zwischen dem PHP-Interpreter selbst und dem Webserver fungiert.
Außerdem benötigen Sie php-mysql, ein PHP-Modul, das PHP ermöglicht, mit MySQL-basierten Datenbanken zu kommunizieren.
Wenn Sie dazu aufgefordert werden, geben Sie Y und ENTER ein, um die Installation zu bestätigen.
Sie haben nun Ihre PHP-Komponenten installiert.
Als Nächstes konfigurieren Sie Nginx, um sie zu verwenden.
Schritt 4 - Konfigurieren von Nginx zum Verwenden des PHP-Prozessors
Bei Verwendung des Nginx-Webservers können wir Serverblocks (ähnlich wie virtuelle Hosts in Apache) erstellen, um Konfigurationsdetails einzuschließen und mehr als eine Domäne auf einem einzelnen Server zu hosten.
In diesem Leitfaden verwenden wir your _ domain als Beispielnamen für die Domäne.
Auf Ubuntu 20.04 hat Nginx einen Serverblock standardmäßig aktiviert und ist so konfiguriert, dass Dokumente aus einem Verzeichnis in / var / www / html bereitgestellt werden.
Statt / var / www / html zu ändern, erstellen wir eine Verzeichnisstruktur innerhalb von / var / www für die Website your _ domain und belassen dabei / var / www / html als Standardverzeichnis, das genutzt wird, wenn eine Clientanfrage keine übereinstimmenden Websites ergibt.
Erstellen Sie das root-Webverzeichnis für your _ domain wie folgt:
Öffnen Sie dann mit Ihrem bevorzugten Befehlszeileneditor eine neue Konfigurationsdatei im Verzeichnis sites-available von Nginx.
Hier eine Auflistung der Funktionen dieser Anweisungen und Location-Blocks:
listen - Hiermit wird definiert, auf welchem Port Nginx lauscht.
In diesem Fall lauscht er auf Port 80, dem Standardport für HTTP.
root - Hiermit wird die Dokumentenroot definiert, in der die von der Website bereitgestellten Dateien gespeichert werden.
index - Hiermit wird die Reihenfolge definiert, in der Nginx die Indexdateien für die Website priorisiert.
Es ist gängige Praxis, index.html-Dateien mit einer höheren Priorität als index.php-Dateien aufzulisten, um die schnelle Einrichtung einer Wartungsstartseite in PHP-Anwendungen zu ermöglichen.
Sie können diese Einstellungen so anpassen, dass sie Ihren Anwendungsanforderungen entsprechen.
server _ name - Hiermit wird definiert, auf welche Domänennamen und / oder IP-Adressen dieser Serverblock antworten soll.
Verweisen Sie diese Anweisung auf den Domänennamen oder die öffentliche IP-Adresse Ihres Servers.
location / - Der erste Location-Block enthält eine Anweisung try _ files, die überprüft, ob Dateien oder Verzeichnisse vorhanden sind, die einer URI-Anfrage entsprechen.
Wenn Nginx die entsprechende Ressource nicht finden kann, wird der Fehler 404 ausgegeben.
location ~\ .php $- Dieser Location-Block handhabt die eigentliche PHP-Verarbeitung, indem er Nginx auf die Konfigurationsdatei fastcgi-php.conf und die Datei php7.4-fpm.sock verweist, wodurch deklariert wird, welche Socket mit php-fpm verknüpft ist.
location ~ /\ .ht - Der letzte Location-Block befasst sich mit .htaccess-Dateien, die Nginx nicht prozessiert.
Durch Hinzufügen der Anweisung deny all werden .htaccess-Dateien, die ihren Weg in die Dokumentenroot finden, nicht für Besucher bereitgestellt.
Wenn Sie mit der Bearbeitung fertig sind, speichern und schließen Sie die Datei.
Wenn Sie nano verwenden, geben Sie hierfür STRG + X ein und dann y und ENTER zur Bestätigung.
Aktivieren Sie Ihre Konfiguration, indem Sie eine Verknüpfung mit der Konfigurationsdatei aus dem sites-enabled-Verzeichnis von Nginx herstellen:
Dadurch wird Nginx angewiesen, die Konfiguration beim nächsten Neuladen zu verwenden.
Sie können Ihre Konfiguration auf Syntaxfehler testen, indem Sie Folgendes eingeben:
Wenn Fehler gemeldet werden, gehen Sie zurück zu Ihrer Konfigurationsdatei, um den Inhalt vor dem Fortfahren zu überprüfen.
Wenn Sie fertig sind, laden Sie Nginx neu, um die Änderungen anzuwenden:
Erstellen Sie an diesem Ort eine index.html-Datei, um zu testen, ob Ihr neuer Serverblock wie erwartet funktioniert:
Gehen Sie nun zu Ihrem Browser und greifen Sie auf den Domänennamen oder die IP-Adresse Ihres Servers zu, wie sie in der server _ name-Anweisung in Ihrer Serverblock-Konfigurationsdatei aufgeführt sind:
Wenn Sie diese Seite sehen, bedeutet das, dass Ihr Nginx-Serverblock wie erwartet funktioniert.
Ihr LEMP-Stack ist nun vollständig konfiguriert.
Im nächsten Schritt erstellen wir ein PHP-Skript, um zu testen, ob Nginx tatsächlich in der Lage ist, .php-Dateien in Ihrer neu konfigurierten Website zu verarbeiten.
Schritt 5 - Testen von PHP mit Nginx
Ihr LEMP-Stack sollte nun vollständig eingerichtet sein.
Sie können ihn testen, um zu bestätigen, dass Nginx .php-Dateien korrekt an Ihren PHP-Prozessor übergeben kann.
Hierfür können Sie eine PHP-Testdatei in Ihrer Dokumentenroot erstellen.
Öffnen Sie innerhalb Ihrer Dokumentenroot in Ihrem Texteditor eine neue Datei namens info.php:
Schreiben oder fügen Sie die folgenden Zeilen in die neue Datei ein.
Es handelt sich um einen gültigen PHP-Code, der Informationen über Ihren Server ausgibt:
Wenn Sie fertig sind, speichern und schließen Sie die Datei durch Eingabe von STRG + X und dann y und ENTER zur Bestätigung.
Sie können nun in Ihrem Webbrowser auf diese Seite zugreifen, indem Sie den Domänennamen oder die öffentliche IP-Adresse besuchen, die Sie in Ihrer Nginx-Konfigurationsdatei eingerichtet haben, gefolgt von / info.php:
Sie sehen dann eine Webseite, die detaillierte Informationen über Ihren Server enthält:
PHPInfo Ubuntu 20.04
Sie können diese Seite nun in Ihrem Webbrowser aufrufen, indem Sie den Domänennamen oder die öffentliche IP-Adresse für Ihre Website besuchen, gefolgt von / todo _ list.php:
In diesem Leitfaden haben wir eine flexible Basis für die Bereitstellung von PHP-Websites und Anwendungen für Ihre Besucher eingerichtet, wobei Nginx als Webserver und MySQL als Datenbanksystem dienen.
Von hier aus können Sie nun verschiedene weitere Schritte ausführen.
Sie sollten beispielsweise sicherstellen, dass Verbindungen zu Ihrem Server gesichert sind.
Zu diesem Zweck könnten Sie Ihre Nginx-Installation mit Let 's Encrypt sichern.
Wenn Sie diesem Leitfaden folgen, erwerben Sie ein kostenloses TLS / SSL-Zertifikat für Ihren Server, mit dem er Inhalte über HTTPS bereitstellen kann.
So installieren Sie Nginx unter Ubuntu 20.04
5298
Nginx ist einer der beliebtesten Webserver der Welt und verantwortlich für das Hosting einiger der größten und datenverkehrsreichsten Websites im Internet.
Er ist eine leichtgewichtige Wahl, die als Webserver oder Reverse-Proxy verwendet werden kann.
Dieser Leitfaden zeigt Ihnen, wie Sie Nginx auf Ihrem Ubuntu-20.04-Server installieren, die Firewall anpassen, den Nginx-Prozess verwalten und Serverblocks für das Hosting von mehreren Domänen von einem einzelnen Server aus einrichten.
Bevor Sie mit diesem Leitfaden beginnen, sollten Sie auf Ihrem Server einen regulären non-root user mit sudo-Berechtigungen konfiguriert haben.
In unserem Leitfaden zur Ersteinrichtung des Servers unter Ubuntu 20.04. erfahren Sie, wie Sie ein reguläres Benutzerkonto für einen solchen user konfigurieren.
Schritt 1 - Installieren von Nginx
Da Nginx in den Standard-Repositorys von Ubuntu verfügbar ist, kann es mithilfe des apt-Paketsystems aus diesen Repositorys installiert werden.
Da es unsere erste Interaktion mit dem apt-Paketsystem in dieser Sitzung ist, aktualisieren wir unseren lokalen Paketindex, damit wir auf die neuesten Paketlisten zugreifen können.
Danach können wir nginx installieren:
Nachdem das Verfahren akzeptiert wurde, installiert apt Nginx und alle erforderlichen Abhängigkeiten auf Ihren Server.
Schritt 2 - Anpassen der Firewall
Vor dem Testen von Nginx muss die Firewall-Software angepasst werden, um den Zugriff auf den Dienst zu ermöglichen.
Nginx registriert sich bei der Installation als Dienst mit ufw, wodurch ein unkomplizierter Zugriff von Nginx ermöglicht wird.
Listen Sie die Anwendungskonfigurationen auf, mit denen ufw arbeiten kann, indem Sie Folgendes eingeben:
Sie sollten eine Auflistung der Anwendungsprofile erhalten:
Wie in der Ausgabe angezeigt, gibt es drei Profile für Nginx:
Nginx Full: Dieses Profil öffnet sowohl Port 80 (normaler, unverschlüsselter Webverkehr) als auch Port 443 (mit TLS / SSL verschlüsselter Verkehr)
Nginx HTTP: Dieses Profil öffnet nur Port 80 (normaler, unverschlüsselter Verkehr)
Nginx HTTPS: Dieses Profil öffnet nur Port 443 (mit TLS / SSL verschlüsselter Datenverkehr)
Es wird empfohlen, das restriktivste Profil zu aktivieren, das den von Ihnen konfigurierten Datenverkehr immer noch zulässt.
Im Moment müssen wir nur den Datenverkehr auf Port 80 zulassen.
Sie können dies aktivieren, indem Sie Folgendes eingeben:
Sie können die Änderung überprüfen, indem Sie Folgendes eingeben:
Die Ausgabe zeigt an, welcher HTTP-Datenverkehr zugelassen ist:
Schritt 3 - Testen Ihres Webservers
Am Ende des Installationsprozesses startet Ubuntu 20.04 Nginx.
Der Webserver sollte bereits ausgeführt werden.
Sie können eine Überprüfung mit dem Init-System systemd durchführen, um sicherzustellen, dass der Dienst ausgeführt wird, indem Sie Folgendes eingeben:
Wie diese Ausgabe bestätigt, wurde der Dienst erfolgreich gestartet.
Der beste Weg, dies zu testen, besteht jedoch darin, eine Seite von Nginx anzufordern.
Sie können auf die Standardstartseite von Nginx zugreifen, um zu bestätigen, dass die Software ordnungsgemäß ausgeführt wird, indem Sie zur IP-Adresse Ihres Servers navigieren.
Wenn Sie die IP-Adresse Ihres Servers nicht kennen, können Sie sie mithilfe des Tools icanhazip.com finden. Mit diesem erhalten Sie Ihre IP-Adresse, wie sie von einem anderen Ort im Internet empfangen wurde:
Sie sollten nun die Standardstartseite von Nginx erhalten:
Wenn Sie auf dieser Seite sind, wird Ihr Server korrekt ausgeführt und steht zur Verwaltung bereit.
Nachdem Ihr Webserver nun ausgeführt wird, werfen wir einen Blick auf einige grundlegende Verwaltungsbefehle.
Wenn Sie nur Konfigurationsänderungen vornehmen, kann Nginx oft neu laden, ohne Verbindungen abzubrechen.
Geben Sie Folgendes ein, um den Start des Dienstes beim Booten wieder zu aktivieren:
Sie haben nun grundlegende Verwaltungsbefehle kennengelernt und können beginnen, die Website so zu konfigurieren, dass sie mehr als eine Domäne hosten kann.
Schritt 5 - Einrichten von Serverblocks (empfohlen)
Bei Verwendung des Nginx-Webservers können Serverblocks (ähnlich wie virtuelle Hosts in Apache) verwendet werden, um Konfigurationsdetails einzuschließen und mehr als eine Domäne auf einem einzigen Server zu hosten.
Auf Ubuntu 20.04 hat Nginx standardmäßig einen Serverblock aktiviert. Dieser ist so konfiguriert, dass er Dokumente aus einem Verzeichnis in / var / www / html bereitstellt.
Die Berechtigungen Ihrer Webroots sollten korrekt sein, wenn Sie Ihren umask-Wert, der die Standard-Dateiberechtigungen festlegt, nicht geändert haben.
Um sicherzustellen, dass Ihre Berechtigungen korrekt sind und der Eigentümer die Dateien lesen, schreiben und ausführen kann, während Gruppen und anderen nur Lese- und Ausführungsberechtigungen erteilt werden, können Sie den folgenden Befehl eingeben:
Wenn Sie fertig sind, speichern und schließen Sie die Datei, indem Sie STRG und X und dann Y und ENTER eingeben.
Sie müssen einen Serverblock mit den richtigen Anweisungen erstellen, damit Nginx diesen Inhalt bereitstellen kann.
Anstatt die Standardkonfigurationsdatei direkt zu ändern, erstellen wir eine neue in / etc / nginx / sites-available / < ^ > your _ domain < ^ >:
Beachten Sie, dass wir die root-Konfiguration mit unserem neuen Verzeichnis und den server _ name mit unserem Domänennamen aktualisiert haben.
Als Nächstes aktivieren wir die Datei, indem wir sie mit dem sites-enabled-Verzeichnis verknüpfen, das Nginx beim Starten liest:
Zwei Serverblocks sind nun aktiviert und konfiguriert, um Anfragen auf Grundlage ihrer Anweisungen listen und server _ name zu beantworten (mehr darüber, wie Nginx diese Anweisungen prozessiert, lesen Sie hier):
your _ domain: Antwortet auf Anfragen für your _ domain und www.your _ domain.
default: Antwortet auf alle Anfragen an Port 80, die nicht mit den anderen beiden Blocks übereinstimmen.
Um ein mögliches Hash-Bucket-Speicherproblem zu vermeiden, das aus dem Hinzufügen zusätzlicher Servernamen entstehen kann, ist es erforderlich, einen einzelnen Wert in der Datei / etc / nginx / nginx.conf anzupassen.
Suchen Sie die Anweisung server _ names _ hash _ bucket _ size und entfernen Sie das Symbol #, um die Kommentierung der Zeile aufzuheben.
Wenn Sie nano verwenden, können Sie mit STRG und w schnell nach Wörtern in der Datei suchen.
Als Nächstes testen Sie, um sicherzustellen, dass es in keiner Ihrer Nginx-Dateien Syntaxfehler gibt:
Wenn es keine Probleme gibt, starten Sie Nginx neu, um Ihre Änderungen zu aktivieren:
Nginx sollte jetzt für Ihren Domänennamen eingerichtet sein.
Erster Nginx-Serverblock
Schritt 6 - Kennenlernen der wichtigen Nginx-Dateien und -Verzeichnisse
Nachdem Sie nun wissen, wie Sie den Nginx-Dienst verwalten, nehmen Sie sich einige Minuten Zeit, um sich mit einigen wichtigen Verzeichnissen und Dateien vertraut zu machen.
/ var / www / html: Der eigentliche Webinhalt, der standardmäßig nur aus der Nginx-Standardseite besteht, die Sie zuvor gesehen haben, wird aus dem Verzeichnis / var / www / html bedient.
/ etc / nginx / sites-available /: Das Verzeichnis, in dem Serverblocks pro Website gespeichert werden können.
Nginx verwendet die in diesem Verzeichnis gefundenen Konfigurationsdateien nicht, es sei denn, sie sind mit dem Verzeichnis sites-enabled verknüpft.
Üblicherweise wird die gesamte Serverblockkonfiguration in diesem Verzeichnis ausgeführt und durch Verknüpfung mit dem anderen Verzeichnis aktiviert.
/ etc / nginx / sites-enabled /: Das Verzeichnis, in dem aktivierte Serverblocks pro Website gespeichert werden.
Üblicherweise werden diese erstellt, indem Sie mit Konfigurationsdateien verknüpft werden, die im Verzeichnis sites-available zu finden sind.
/ etc / nginx / snippets: Dieses Verzeichnis enthält Konfigurationsfragmente, die an anderer Stelle in der Nginx-Konfiguration enthalten sein können.
Potenziell wiederholbare Konfigurationssegmente sind gute Kandidaten für das Refactoring in Snippets.
Wenn Sie einen vollständigeren Anwendungsstapel erstellen möchten, lesen Sie den Artikel Installieren von Linux, Nginx, MySQL und PHP (LEMP-Stack) unter Ubuntu 20.04.
So installieren Sie den Apache-Webserver unter Ubuntu 20.04
5354
Apache ist innerhalb der Standard-Software-Repositorys von Ubuntu verfügbar und kann mit herkömmlichen Paketverwaltungstools installiert werden.
Beginnen Sie damit, den lokalen Paketindex zu aktualisieren, um alle neuen Änderungen im Upstream widerzuspiegeln:
Installieren Sie dann das Paket apache2:
Nach der Bestätigung der Installation wird apt Apache und alle erforderlichen Abhängigkeiten installieren.
Vor dem Testen von Apache müssen die Firewall-Einstellungen geändert werden, um den externen Zugriff auf die Standard-Webports zu ermöglichen.
Angenommen, Sie haben die Anweisungen in den Voraussetzungen befolgt, sollte eine UFW-Firewall konfiguriert sein, um den Zugriff auf Ihren Server einzuschränken.
Während der Installation registriert sich Apache bei UFW, um einige Anwendungsprofile bereitzustellen, mit denen der Zugriff auf Apache über die Firewall aktiviert oder deaktiviert werden kann.
Listen Sie die Anwendungsprofile ufw auf, indem Sie Folgendes eingeben:
Sie erhalten eine Liste der Anwendungsprofile:
Wie durch die Ausgabe angezeigt, gibt es drei Profile für Apache:
Apache: Dieses Profil öffnet nur Port 80 (normaler, nicht verschlüsselter Webverkehr).
Da wir in diesem Leitfaden noch kein SSL für unseren Server konfiguriert haben, müssen wir nur den Datenverkehr auf Port 80 zulassen:
Die Ausgabe stellt eine Liste mit erlaubtem HTTP-Datenverkehr bereit:
Wie durch die Ausgabe angezeigt, wurde das Profil aktiviert, um den Zugriff auf den Apache-Webserver zu ermöglichen.
Am Ende des Installationsprozesses startet Ubuntu 20.04 Apache.
Wie durch diese Ausgabe bestätigt, ist der Dienst erfolgreich gestartet.
Geben Sie dies bei der Eingabeaufforderung Ihres Servers ein:
Sie erhalten einige Adressen, die durch Leerzeichen getrennt sind.
Eine andere Möglichkeit ist die Verwendung des Icanhazip-Tools, mit dem Sie Ihre öffentliche IP-Adresse erhalten, die von einem anderen Ort im Internet gelesen wurde:
Sie sollten die Apache-Standardseite für Ubuntu 20.04 sehen:
Schritt 4 - Verwalten des Apache-Prozesses
Nachdem Sie Ihren Webserver nun ausgeführt haben, gehen wir nun mit systemctl über einige grundlegende Verwaltungsbefehle.
Apache sollte jetzt automatisch gestartet werden, wenn der Server erneut gestartet wird.
Schritt 5 - Einrichten von virtuellen Hosts (Empfohlen)
Statt / var / www / html zu ändern, erstellen wir eine Verzeichnisstruktur innerhalb / var / www für eine Seite your _ domain und lassen dabei / var / www / html als Standardverzeichnis, das bereitgestellt wird, wenn eine Client-Anfrage keine übereinstimmenden Sites hat.
Die Berechtigungen Ihrer Webstämme sollten korrekt sein, wenn Sie Ihren umask-Wert nicht geändert haben, wodurch die Standarddateiberechtigungen festgelegt werden.
Schritt 6 - Kennenlernen der wichtigen Apache-Dateien und -Verzeichnisse
Nachdem Sie nun wissen, wie Sie den Apache-Dienst verwalten, nehmen Sie sich einige Minuten Zeit, um sich mit einigen wichtigen Verzeichnissen und Dateien vertraut zu machen.
/ var / www / html: Der eigentliche Web-Inhalt, der standardmäßig nur aus der Apache-Standardseite besteht, die Sie zuvor gesehen haben, wird aus dem Verzeichnis / var / www / html bedient.
Dies kann durch die Anpassung der Konfigurationsdateien von Apache geändert werden.
/ etc / apache2: Das Apache-Konfigurationsverzeichnis.
Hier befinden sich alle Konfigurationsdateien von Apache.
/ etc / apache2 / apache2.conf: Die Hauptkonfigurationsdatei von Apache.
Diese kann modifiziert werden, um die globale Konfiguration von Apache zu ändern.
Diese Datei ist für das Laden vieler der anderen Dateien im Konfigurationsverzeichnis verantwortlich.
/ etc / apache2 / ports.conf: Diese Datei gibt die Ports an, die Apache überwacht.
Standardmäßig überwacht Apache Port 80 und zusätzlich Port 443, wenn ein Modul mit SSL-Funktionen aktiviert ist.
/ etc / apache2 / sites-available /: Das Verzeichnis, in dem virtuelle Hosts pro Standort gespeichert werden können.
Apache verwendet die in diesem Verzeichnis gefundenen Konfigurationsdateien nicht, es sei denn, sie sind mit dem Verzeichnis sites-enabled verknüpft.
In der Regel wird die gesamte Serverblockkonfiguration in diesem Verzeichnis durchgeführt und dann durch Verknüpfen mit dem anderen Verzeichnis mit dem Befehl a2ensite aktiviert.
/ etc / apache2 / sites-enabled /: Das Verzeichnis, in dem aktivierte virtuelle Hosts pro Standort gespeichert sind.
In der Regel werden diese durch Verknüpfen mit Konfigurationsdateien erstellt, die sich im Verzeichnis sites-available mit a2ensite befinden.
Apache liest die Konfigurationsdateien und Links in diesem Verzeichnis, wenn es gestartet oder neu geladen wird, um eine vollständige Konfiguration zu kompilieren.
/ etc / apache2 / conf-available /, / etc / apache2 / conf-enabled /: Diese Verzeichnisse verfügen über die gleiche Beziehung wie die Verzeichnisse sites-available- und sites-enabled, dienen aber zur Speicherung von Konfigurationsfragmenten, die nicht in einen virtuellen Host gehören.
Dateien im Verzeichnis conf available können mit dem Befehl a2enconf aktiviert und mit dem Befehl a2disconf deaktiviert werden.
/ etc / apache2 / mods-available /, / etc / apache2 / mods-enabled /: Diese Verzeichnisse enthalten jeweils die verfügbaren und aktivierten Module.
Dateien, die in .load enden, enthalten Fragmente zum Laden bestimmter Module, während in .conf endende Dateien die Konfiguration für diese Module enthalten.
Module können mit dem Befehl a2enmod und a2dismod aktiviert und deaktiviert werden.
/ var / log / apache2 / access.log: Standardmäßig wird jede Anforderung an Ihren Webserver in dieser Protokolldatei aufgezeichnet, sofern Apache nicht anders konfiguriert ist.
/ var / log / apache2 / error.log: Standardmäßig werden alle Fehler in dieser Datei aufgezeichnet.
Die LogLevel-Anweisung in der Apache-Konfiguration gibt an, wie detailliert die Fehlerprotokolle sein werden.
Wenn Sie einen vollständigeren Anwendungsstapel erstellen möchten, lesen Sie diesen Artikel unter So konfiguriert man einen LAMP-Stapel unter Ubuntu 20.04
Überwachen von BGP-Ankündigungen und -Routen mit BGPalerter unter Ubuntu 18.04
5236
BGP (Border Gateway Protocol) ist eines der Kernprotokolle, die für das Weiterleiten von Paketen über das Internet verantwortlich sind. Wenn es also schief geht, können erhebliche Ausfälle auftreten.
Beispielsweise hat ein kleiner ISP im Jahr 2019 eine BGP-Fehlkonfiguration vorgenommen, die sich leider im Upstream verbreitet und große Teile von Cloudflare und AWS über eine Stunde lang offline geschaltet hat.
Ein Jahr zuvor fand außerdem ein BGP-Hijack statt, um den Datenverkehr zu einem bekannten Kryptowährungs-Wallet-Anbieter abzufangen und die Gelder ahnungsloser Kunden zu stehlen.
BGPalerter ist ein Open-Source-Tool zur Überwachung des BGP-Netzwerks, das Echtzeit-Warnungen zu BGP-Aktivitäten, einschließlich Routensichtbarkeit und Ankündigungen neuer Routen, sowie potenziell schädliche Aktivitäten wie Routen-Hijacking oder Routenlecks bereitstellen kann.
BGPalerter nimmt automatisch öffentlich verfügbare Netzwerkroutinginformationen auf, was bedeutet, dass es keinen privilegierten Zugriff oder keine Integration in die Netzwerke haben muss, die Sie überwachen möchten.
< $> note Hinweis: BGPalerter nimmt automatisch öffentlich verfügbare Netzwerkroutinginformationen auf, was bedeutet, dass es keinen privilegierten Zugriff oder keine Integration in die Netzwerke haben muss, die Sie überwachen möchten.
Alle Überwachungen entsprechen vollständig dem Gesetz über Computermissbrauch, dem Gesetz über Computerbetrug und -missbrauch und anderen ähnlichen Gesetzen.
Es wird jedoch empfohlen, relevante Ergebnisse dem betroffenen Netzwerkbetreiber verantwortungsbewusst mitzuteilen.
In diesem Tutorial installieren und konfigurieren Sie BGPalerter, um Ihre wichtigen Netzwerke auf potenziell verdächtige Aktivitäten zu überwachen.
Ein oder mehrere Netzwerke oder Geräte, die Sie überwachen möchten, zum Beispiel:
Einen Server, den Sie verwalten
Ihr Firmennetzwerk
Ihre lokale ISP
Für jedes Gerät oder Netzwerk müssen Sie entweder die individuelle IP-Adresse, den IP-Adressbereich oder die Nummer des autonomen Systems angeben, zu der es gehört.
Dies ist in Schritt 1 abgedeckt.
Schritt 1 - Identifizieren der zu überwachenden Netzwerke
In diesem Schritt sehen Sie die relevanten Details der Netzwerke, die Sie überwachen möchten.
BGPalerter kann anhand einzelner IP-Adressen oder Netzwerkpräfixe überwachen.
Es kann auch ganze Netzwerke anhand ihrer AS-Nummer (Autonomous System) überwachen. Dies ist eine global eindeutige Kennung für ein Netzwerk, das einer bestimmten Verwaltungseinheit gehört.
Um diese Informationen zu finden, können Sie den Suchdienst IP-to-ASN WHOIS verwenden, der vom Bedrohungsnachrichtendienst Team Cymru bereitgestellt wird.
Dies ist ein benutzerdefinierter WHOIS-Server, mit dem IP-Adressen und Netzwerkroutinginformationen abgerufen werden können.
Wenn Sie whois noch nicht installiert haben, können Sie es mit den folgenden Befehlen installieren:
Sobald Sie die Installation von whois bestätigt haben, wird nun die IP-Adresse Ihres eigenen Servers anhand des Arguments -h angezeigt, um einen benutzerdefinierten Server anzugeben:
Dies gibt ein Ergebnis ähnlich dem folgenden aus, das den AS-Namen und die AS-Nummer anzeigt, zu der Ihr Server gehört.
Dies ist normalerweise der AS Ihres Server-Hosting-Anbieters, z. B. DigitalOcean.
Als Nächstes können Sie eine Suche durchführen, um das Netzwerkpräfix / den Netzwerkbereich zu ermitteln, zu dem Ihr Server gehört.
Dazu fügen Sie Ihrer Anfrage das Argument -p hinzu:
Die Ausgabe ist dem vorherigen Befehl sehr ähnlich, zeigt jedoch jetzt das IP-Adresspräfix an, zu dem die IP-Adresse Ihres Servers gehört:
Schließlich können Sie weitere Details des AS nachschlagen, zu dem Ihr Server gehört, einschließlich der geografischen Region und des Zuordnungsdatums.
Ersetzen Sie die AS-Nummer, die Sie mit den vorherigen Befehlen identifiziert haben.
Mit dem Argument -v aktivieren Sie die ausführliche Ausgabe, um sicherzustellen, dass alle relevanten Details angezeigt werden:
Die Ausgabe zeigt weitere Informationen zur AS:
Sie haben wichtige Details zu den Netzwerken identifiziert, die Sie überwachen möchten.
Notieren Sie sich diese Details irgendwo, da Sie sie später benötigen.
Als Nächstes beginnen Sie mit der Einrichtung von BGPalerter.
Schritt 2 - Erstellen eines nicht privilegierten Benutzers für BGPalerter
In diesem Schritt erstellen Sie ein neues nicht privilegiertes Benutzerkonto für BGPalerter, da das Programm nicht mit sudo- / root-Berechtigungen ausgeführt werden muss.
Erstellen Sie zunächst einen neuen Benutzer mit einem deaktivierten Passwort:
Sie müssen weder ein Passwort noch SSH-Schlüssel einrichten, da Sie diesen Benutzer nur als Dienstkonto zum Ausführen / Verwalten von BGPalerter verwenden.
Melden Sie sich mit su beim neuen Benutzer an:
Sie werden jetzt als neuer Benutzer angemeldet:
Verwenden Sie den Befehl cd, um in das Ausgangsverzeichnis Ihres neuen Benutzers zu wechseln:
Sie haben einen neuen nicht privilegierten Benutzer für BGPalerter erstellt.
Als Nächstes installieren und konfigurieren Sie BGPalerter auf Ihrem System.
Schritt 3 - Installieren und Konfigurieren von BGPalerter
In diesem Schritt installieren und konfigurieren Sie BGPalerter.
Stellen Sie sicher, dass Sie als Ihr neuer nicht privilegierter Benutzer noch angemeldet sind.
Zuerst müssen Sie die neueste Version von BGPalerter identifizieren, um sicherzustellen, dass Sie die aktuellste Version herunterladen.
Navigieren Sie zur Seite BGPalerter Releases und kopieren Sie den Download-Link für die neueste Linux x64-Version.
Sie können jetzt eine Kopie von BGPalerter mit wget herunterladen und dabei sicherstellen, dass Sie den richtigen Download-Link verwenden:
Markieren Sie die heruntergeladene Datei als ausführbar:
Überprüfen Sie als Nächstes, ob BGPalerter erfolgreich heruntergeladen und installiert wurde, indem Sie die Versionsnummer überprüfen:
Dadurch wird die aktuelle Versionsnummer ausgegeben:
Bevor Sie BGPalerter ordnungsgemäß ausführen können, müssen Sie die Netzwerke, die Sie überwachen möchten, in einer Konfigurationsdatei definieren.
Erstellen und öffnen Sie die Datei prefixes.yml in Ihrem bevorzugten Texteditor:
In dieser Konfigurationsdatei geben Sie jede der einzelnen IP-Adressen, IP-Adressbereiche und AS-Nummern an, die Sie überwachen möchten.
Fügen Sie das folgende Beispiel hinzu und passen Sie die Konfigurationswerte mithilfe der in Schritt 1 angegebenen Netzwerkinformationen nach Bedarf an:
Sie können beliebig viele IP-Adressbereiche oder AS-Nummern überwachen.
Um einzelne IP-Adressen zu überwachen, stellen Sie sie mit / 32 für IPv4 und / 128 für IPv6 dar.
Der Wert ignoreMorespecifics wird verwendet, um zu steuern, ob BGPalerter Aktivitäten für Routen ignorieren soll, die spezifischer (kleiner) sind als die, die Sie überwachen.
Wenn Sie beispielsweise eine / 20 überwachen und eine Routingänderung für eine / 24 darin festgestellt wird, wird dies als spezifischer angesehen.
In den meisten Fällen möchten Sie diese nicht ignorieren. Wenn Sie jedoch ein großes Netzwerk mit mehreren delegierten Kundenpräfixen überwachen, kann dies dazu beitragen, Hintergrundgeräusche zu reduzieren.
Sie können BGPalerter jetzt zum ersten Mal ausführen, um mit der Überwachung Ihrer Netzwerke zu beginnen:
Wenn BGPalerter erfolgreich gestartet wird, wird eine Ausgabe ähnlich der folgenden angezeigt.
Beachten Sie, dass es manchmal einige Minuten dauern kann, um die Überwachung zu beginnen:
BGPalerter wird so lange ausgeführt, bis Sie es mit Strg + C stoppen.
Im nächsten Schritt interpretieren Sie einige der Warnungen, die BGPalerter generieren kann.
Schritt 4 - Interpretieren von BGPalerter-Warnungen
In diesem Schritt werden einige beispielhafte BGPalerter-Warnungen überprüft.
BGPalerter gibt Warnungen an den Hauptausgabe-Feed und optional an zusätzliche Berichtsendpunkte aus, die in config.yml konfiguriert werden können, wie in der BGPalerter-Dokumentation beschrieben.
Standardmäßig überwacht und alarmiert BGPalerter Folgendes:
Routen-Hijacks: treten auf, wenn eine AS ein Präfix ankündigt, das nicht zulässig ist, wodurch der Datenverkehr fälschlicherweise weitergeleitet wird.
Dies kann entweder ein absichtlicher Angriff oder ein versehentlicher Konfigurationsfehler sein.
Verlust der Routensichtbarkeit: Eine Route wird als sichtbar angesehen, wenn die meisten BGP-Router im Internet zuverlässig darauf routen können. Ein Verlust der Sichtbarkeit bezieht sich darauf, dass Ihr Netzwerk möglicherweise nicht verfügbar ist, z. B. wenn Ihr BGP-Peering nicht mehr funktioniert.
Neue Unterpräfixankündigungen: Wenn eine AS beginnt, ein Präfix anzukündigen, das kleiner ist als erwartet.
Dies kann auf eine beabsichtigte Konfigurationsänderung, eine versehentliche Fehlkonfiguration oder in einigen Fällen auf einen Angriff hinweisen.
Aktivität innerhalb Ihrer AS: bezieht sich normalerweise auf neue Routenankündigungen.
Eine Route wird als "neu" angesehen, wenn BGPalerter noch nichts von ihr weiß.
Im Folgenden finden Sie einige Beispielwarnungen sowie eine kurze Beschreibung ihrer Bedeutung:
Diese Warnung zeigt Hinweise auf einen Routen-Hijack, bei dem AS64496 203.0.113.0 / 24 angekündigt hat, wenn erwartet wird, dass diese Route von AS65540 angekündigt wird.
Dies ist ein starker Indikator für eine Fehlkonfiguration, die zu einem Routenleck oder zu absichtlichem Hijacking durch einen Angreifer führt.
Diese Warnung zeigt an, dass das Netzwerk 203.0.113.0 / 24 nicht mehr sichtbar ist.
Dies kann an einem Upstream-Routing-Problem liegen oder an einem Stromausfall eines Routers.
Diese Warnung zeigt an, dass ein spezifischeres Präfix angekündigt wurde, wenn es nicht erwartet wird, z. B. indem eine / 25 angekündigt wird, wenn nur eine / 24 erwartet wird.
Dies ist höchstwahrscheinlich eine Fehlkonfiguration, kann jedoch in einigen Fällen ein Hinweis auf einen Routen-Hijack sein.
Schließlich zeigt diese Warnung, dass AS64496 ein Präfix angekündigt hat, über das BGPalerter noch nichts weiß.
Dies kann daran liegen, dass Sie zu Recht ein neues Präfix ankündigen, oder an einer Fehlkonfiguration, die dazu führt, dass Sie versehentlich ein Präfix ankündigen, das einer anderen Person gehört.
In diesem Schritt haben Sie einige Beispiel-BGPalerter-Warnungen überprüft.
Als Nächstes konfigurieren Sie BGPalerter so, dass es beim Booten automatisch ausgeführt wird.
Schritt 5 - Starten von BGPalerter beim Booten
In diesem letzten Schritt konfigurieren Sie BGPalerter so, dass es beim Booten ausgeführt wird.
Stellen Sie sicher, dass Sie weiterhin als Ihr neuer nicht privilegierter Benutzer angemeldet sind, und öffnen Sie dann den crontab-Editor:
Fügen Sie als Nächstes den folgenden Eintrag am Ende der crontab-Datei hinzu:
Bei jedem Systemstart wird eine getrennte Bildschirmsitzung mit dem Namen 'bgpalerter' erstellt und BGPalerter darin gestartet.
Speichern und schließen Sie den crontab-Editor.
Vielleicht möchten Sie Ihr System jetzt neu starten, um sicherzustellen, dass BGPalerter beim Booten richtig startet.
Sie müssen sich zuerst von Ihrem BGPalerter-Benutzer abmelden:
Fahren Sie dann mit einem normalen Systemneustart fort:
Melden Sie sich nach dem Neustart Ihres Systems erneut bei Ihrem Server an und verwenden Sie su, um erneut auf Ihren BGPalerter-Benutzer zuzugreifen:
Sie können dann jederzeit eine Verbindung zur Sitzung herstellen, um die Ausgabe von BGPalerter anzuzeigen:
In diesem Artikel haben Sie BGPalerter eingerichtet und damit Netzwerke auf Änderungen des BGP-Routings überwacht.
Wenn Sie BGPalerter benutzerfreundlicher gestalten möchten, können Sie es so konfigurieren, dass Warnungen über einen Webhook an einen Slack-Kanal gesendet werden:
Konfigurieren Sie Slack Reporting für BGPalerter
Wenn Sie mehr über BGP selbst erfahren möchten, aber keinen Zugriff auf eine Produktions-BGP-Umgebung haben, können Sie DN42 verwenden, um mit BGP in einer sicheren, isolierten Umgebung zu experimentieren:
Dezentralisiertes Netzwerk 42
Einrichten und Konfigurieren einer Zertifizierungsstelle (CA) unter Ubuntu 20.04
5387
In diesem Leitfaden lernen wir, wie eine private Zertifizierungsstelle auf einem Ubuntu 20.04-Server eingerichtet und wie mit Ihrer neuen CA ein Testzertifikat erzeugt und signiert wird.
Um dieses Tutorial zu absolvieren, benötigen Sie Zugriff auf einen Ubuntu 20.04-Server, der Ihren CA-Server hosten kann.
Sie können unseren Leitfaden zur Ersteinrichtung des Servers mit Ubuntu 20.04 befolgen, um einen Benutzer mit entsprechenden Berechtigungen einzurichten.
Wenn Sie sich entscheiden, diese Übungsschritte durchzuführen, benötigen Sie einen zweiten Ubuntu 20.04-Server, oder Sie können Ihren eigenen lokalen Linux-Computer verwenden, auf dem Ubuntu oder Debian oder davon abgeleitete Distributionen ausgeführt werden.
Führen Sie auf Ubuntu- und Debian-basierten Systemen die folgenden Befehle als Ihr non-root user aus, um das Zertifikat zu importieren:
Die folgenden Schritte werden auf Ihrem zweiten Ubuntu- oder Debian-System oder einer Distribution, die von einem dieser Systeme abgeleitet ist, ausgeführt.
In diesem Tutorial haben Sie eine private Zertifizierungsstelle mit dem Easy-RSA-Paket auf einem eigenständigen Ubuntu 20.04-Server erstellt.
So verwenden Sie Go mit MongoDB mithilfe des MongoDB Go-Treibers
5369
Nachdem MongoDB sich viele Jahre lang auf von der Community entwickelte Lösungen verlassen hatte, gab MongoDB bekannt, dass sie an einem offiziellen Treiber für Go arbeiten.
Im März 2019 erreichte dieser neue Treiber mit der Veröffentlichung von v1.0.0 den Status "Produktionsbereit" und wurde seitdem kontinuierlich aktualisiert.
Wie die anderen offiziellen MongoDB-Treiber ist der Go-Treiber für die Go-Programmiersprache typisch und bietet eine einfache Möglichkeit, MongoDB als Datenbanklösung für ein Go-Programm zu verwenden.
Er ist vollständig in die MongoDB-API integriert und stellt alle Abfrage-, Indexierungs- und Aggregationsfunktionen der API sowie andere erweiterte Funktionen zur Verfügung.
Im Gegensatz zu Bibliotheken von Drittanbietern wird er von MongoDB-Ingenieuren vollständig unterstützt, sodass Sie sicher sein können, dass er weiterentwickelt und gewartet wird.
In diesem Tutorial lernen Sie den offiziellen MongoDB Go-Treiber kennen.
Sie installieren den Treiber, stellen eine Verbindung zu einer MongoDB-Datenbank her und führen mehrere CRUD-Vorgänge aus.
Dabei erstellen Sie ein Task-Manager-Programm zum Verwalten von Aufgaben über die Befehlszeile.
Go auf Ihrem Computer installiert und einen Go-Arbeitsbereich, der wie folgt konfiguriert wird: Installieren von Go und Einrichten einer lokalen Programmierumgebung.
In diesem Tutorial wird das Projekt als tasker bezeichnet.
Sie müssen Go v1.11 oder höher auf Ihrem Computer mit aktivierten Go-Modulen installiert haben.
MongoDB für Ihr Betriebssystem gemäß der Installation von MongoDB installiert.
MongoDB 2.6 oder höher ist die Mindestversion, die vom MongoDB Go-Treiber unterstützt wird.
Wenn Sie Go v1.11 oder 1.12 verwenden, stellen Sie sicher, dass Go Modules aktiviert ist, indem Sie die Umgebungsvariable GO111MODULE wie folgt auf on setzen:
Weitere Informationen zum Implementieren von Umgebungsvariablen finden Sie in diesem Tutorial zum Lesen und Festlegen von Umgebungs- und Shell-Variablen.
Die in diesem Leitfaden gezeigten Befehle und Codes wurden mit Go v1.14.1 und MongoDB v3.6.3 getestet.
Schritt 1 - Installieren des MongoDB Go-Treibers
In diesem Schritt installieren Sie das Paket Go Driver für MongoDB und importieren es in Ihr Projekt.
Außerdem stellen Sie eine Verbindung zu Ihrer MongoDB-Datenbank her und überprüfen den Status der Verbindung.
Fahren Sie fort und erstellen Sie ein neues Verzeichnis für dieses Tutorial in Ihrem Dateisystem:
Sobald Ihr Projektverzeichnis eingerichtet ist, ändern Sie es mit dem folgenden Befehl:
Initialisieren Sie als Nächstes das Go-Projekt mit einer go.mod-Datei.
Diese Datei definiert die Projektanforderungen und die Abhängigkeiten ihrer richtigen Versionen:
Wenn Ihr Projektverzeichnis außerhalb von $GOPATH ist, müssen Sie den Importpfad für Ihr Modul wie folgt angeben:
Nun sieht Ihre Datei go.mod wie folgt aus:
Fügen Sie mit dem folgenden Befehl den MongoDB Go-Treiber als eine Abhängigkeit für Ihr Projekt hinzu:
Erstellen Sie als Nächstes eine main.go-Datei in Ihrem Projektstamm und öffnen Sie sie in Ihrem Texteditor:
Importieren Sie die folgenden Pakete in Ihre main.go-Datei, um mit dem Treiber zu beginnen:
Hier fügen Sie die Pakete mongo und options hinzu, die der MongoDB Go-Treiber bereitstellt.
Erstellen Sie nach Ihren Importen einen neuen MongoDB-Client und stellen Sie eine Verbindung zu Ihrem laufenden MongoDB-Server her:
mongo.Connect () akzeptiert ein Objekt Context und ein options.ClientOptions-Objekt, mit denen die Verbindungszeichenfolge und andere Treibereinstellungen festgelegt werden.
In der Dokumentation zum Optionspaket finden Sie Informationen zu den verfügbaren Konfigurationsoptionen.
Context ist wie eine Zeitüberschreitung oder eine Frist, die angibt, wann ein Vorgang nicht mehr ausgeführt und zurückgegeben werden soll.
Dies hilft, Leistungseinbußen auf Produktionssystemen zu vermeiden, wenn bestimmte Vorgänge langsam ausgeführt werden.
In diesem Code übergeben Sie context.TODO (), um anzuzeigen, dass Sie nicht sicher sind, welchen Kontext Sie derzeit verwenden sollen, aber Sie planen, in Zukunft einen hinzuzufügen.
Stellen Sie als Nächstes sicher, dass Ihr MongoDB-Server mithilfe der Ping-Methode gefunden und erfolgreich verbunden wurde.
Fügen Sie den folgenden Code in die init-Funktion ein:
Wenn beim Herstellen einer Verbindung zur Datenbank Fehler auftreten, sollte das Programm abstürzen, während Sie versuchen, das Problem zu beheben, da es keinen Sinn macht, das Programm ohne aktive Datenbankverbindung auszuführen.
Fügen Sie den folgenden Code hinzu, um eine Datenbank zu erstellen:
Sie erstellen eine tasker-Datenbank und eine task-Sammlung, um die zu erstellenden Aufgaben zu speichern.
Sie richten collection auch als Variable auf Paketebene ein, damit Sie die Datenbankverbindung im gesamten Paket wiederverwenden können.
Das vollständige main.go ist nun wie folgt:
Sie haben Ihr Programm eingerichtet, um über den Go-Treiber eine Verbindung zu Ihrem MongoDB-Server zu erhalten.
Im nächsten Schritt erstellen Sie Ihr Task-Manager-Programm.
Schritt 2 - Erstellen eines CLI-Programms
In diesem Schritt installieren Sie das bekannte cli-Paket, um die Entwicklung Ihres Task-Manager-Programms zu unterstützen.
Es bietet eine Schnittstelle, über die Sie schnell moderne Befehlszeilentools erstellen können.
Dieses Paket bietet beispielsweise die Möglichkeit, Unterbefehle für Ihr Programm zu definieren, um eine git-ähnliche Befehlszeilenerfahrung zu erzielen.
Führen Sie den folgenden Befehl aus, um das Paket als Abhängigkeit hinzuzufügen:
Öffnen Sie als Nächstes Ihre main.go-Datei erneut:
Fügen Sie Ihrer main.go-Datei den folgenden hervorgehobenen Code hinzu:
Sie importieren das cli-Paket wie erwähnt.
Außerdem importieren Sie das Paket os, das Sie verwenden, um Befehlszeilenargumente an Ihr Programm zu übergeben:
Fügen Sie nach Ihrer init-Funktion den folgenden Code hinzu, um Ihr CLI-Programm zu erstellen und den Code zu kompilieren:
Dieses Snippet erstellt ein CLI-Programm namens tasker und fügt eine kurze Verwendungsbeschreibung hinzu, die beim Ausführen des Programms ausgedruckt wird.
Im Befehlsfenster fügen Sie Befehle für Ihr Programm hinzu.
Der Befehl Run analysiert die Argumente auf den entsprechenden Befehl.
Hier ist der Befehl, den Sie zum Erstellen und Ausführen des Programms benötigen:
Das Programm wird ausgeführt und zeigt einen Hilfetext an, in dem Sie erfahren, was das Programm kann und wie es verwendet wird.
In den nächsten Schritten verbessern Sie die Nützlichkeit Ihres Programms, indem Sie Unterbefehle hinzufügen, um Ihre Aufgaben in MongoDB zu verwalten.
Schritt 3 - Erstellen einer Aufgabe
In diesem Schritt fügen Sie Ihrem CLI-Programm mithilfe des cli-Pakets einen Unterbefehl hinzu.
Am Ende dieses Abschnitts können Sie Ihrer MongoDB-Datenbank eine neue Aufgabe hinzufügen, indem Sie einen neuen add-Befehl in Ihrem CLI-Programm verwenden.
Öffnen Sie zunächst Ihre main.go-Datei:
Importieren Sie als Nächstes die Pakete go.mongodb.org / mongo-driver / bson / primitive, time und errors:
Erstellen Sie dann eine neue Struktur, um eine einzelne Aufgabe in der Datenbank darzustellen, und fügen Sie sie unmittelbar vor der Hauptfunktion ein:
Sie verwenden das primitive Paket, um den Typ der ID jeder Aufgabe festzulegen, da MongoDB standardmäßig ObjectIDs für das Feld _ id verwendet.
Ein weiteres Standardverhalten von MongoDB besteht darin, dass der Feldname in Kleinbuchstaben als Schlüssel für jedes exportierte Feld verwendet wird, wenn es serialisiert wird. Dies kann jedoch mithilfe von bson struct-Tags geändert werden.
Erstellen Sie als Nächstes eine Funktion, die eine Instanz der Aufgabe empfängt und in der Datenbank speichert.
Fügen Sie dieses Snippet nach der Hauptfunktion hinzu:
Die Methode collection.InsertOne () fügt die bereitgestellte Aufgabe in die Datenbanksammlung ein und gibt die ID des eingefügten Dokuments zurück.
Da Sie diese ID nicht benötigen, verwerfen Sie sie, indem Sie sie dem Unterstrichoperator zuweisen.
Der nächste Schritt besteht darin, Ihrem Task-Manager-Programm einen neuen Befehl zum Erstellen neuer Aufgaben hinzuzufügen.
Nennen wir ihn add:
Jeder neue Befehl, der Ihrem CLI-Programm hinzugefügt wird, wird im Fenster Befehle platziert.
Jeder besteht aus einem Namen, einer Verwendungsbeschreibung und einer Aktion.
Dies ist der Code, der bei der Befehlsausführung ausgeführt wird.
In diesem Code sammeln Sie das erste add-Argument und verwenden es, um die Texteigenschaft einer neuen Aufgabeninstanz festzulegen, während Sie die entsprechenden Standardeinstellungen für die anderen Eigenschaften zuweisen.
Die neue Aufgabe wird anschließend an createTask weitergeleitet, die die Aufgabe in die Datenbank einfügt und nil zurückgibt, wenn alles gut geht und der Befehl beendet wird.
Testen Sie es, indem Sie mit dem Befehl add einige Aufgaben hinzufügen.
Bei Erfolg werden keine Fehler auf Ihrem Bildschirm angezeigt:
Nachdem Sie nun erfolgreich Aufgaben hinzufügen können, implementieren wir eine Möglichkeit, alle Aufgaben anzuzeigen, die Sie der Datenbank hinzugefügt haben.
Schritt 4 - Auflisten aller Aufgaben
Das Auflisten der Dokumente in einer Sammlung kann mit der Methode collection.Find () erfolgen, die einen Filter sowie einen Zeiger auf einen Wert erwartet, in den das Ergebnis dekodiert werden kann.
Der Rückgabewert ist ein Cursor, der eine Reihe an Dokumenten bereitstellt, die einzeln durchlaufen und dekodiert werden können.
Der Cursor wird dann geschlossen, sobald er erschöpft ist.
Öffnen Sie Ihre main.go-Datei:
Stellen Sie sicher, dass das Paket bson importiert wird:
Erstellen Sie dann unmittelbar nach createTask die folgenden Funktionen:
Mit BSON (Binary-coded JSON) werden Dokumente in einer MongoDB-Datenbank dargestellt, und das bson-Paket hilft uns bei der Arbeit mit BSON-Objekten in Go.
Der in der Funktion getAll () verwendete Typ bson.D stellt ein BSON-Dokument dar und wird dort verwendet, wo die Reihenfolge der Eigenschaften von Bedeutung ist.
Indem Sie bson.D {{}} als Filter an filterTasks () übergeben, geben Sie an, dass Sie mit allen Dokumenten in der Sammlung übereinstimmen möchten.
In der Funktion filterTasks () iterieren Sie über den von der collection.Find () -Methode zurückgegebenen Cursor und dekodieren jedes Dokument in eine Instanz der Aufgabe.
Jede Aufgabe wird dann an den zu Beginn der Funktion erstellten Aufgabenbereich angehängt.
Sobald der Cursor erschöpft ist, wird er geschlossen und das Aufgabenfenster zurückgegeben.
Bevor Sie einen Befehl zum Auflisten aller Aufgaben erstellen, erstellen wir eine Hilfsfunktion, die einen Teil der Aufgaben übernimmt und in die Standardausgabe druckt.
Sie verwenden das Paket color, um die Ausgabe zu färben.
Bevor Sie dieses Paket verwenden können, installieren Sie es mit:
Und importieren Sie sie zusammen mit dem fmt-Paket in Ihre main.go-Datei:
Erstellen Sie als Nächstes eine neue printTasks-Funktion, die Ihrer Hauptfunktion folgt:
Diese printTasks-Funktion übernimmt eine Reihe von Aufgaben, durchläuft jede einzelne und druckt sie in der Standardausgabe aus. Dabei wird die grüne Farbe verwendet, um abgeschlossene Aufgaben anzuzeigen, und gelb für unvollständige Aufgaben.
Fahren Sie fort und fügen Sie die folgenden hervorgehobenen Zeilen hinzu, um einen neuen all-Befehl im Fenster Befehle zu erstellen.
Dieser Befehl druckt alle hinzugefügten Aufgaben in die Standardausgabe:
Der Befehl all ruft alle in der Datenbank vorhandenen Aufgaben ab und druckt sie in die Standardausgabe.
Wenn keine Aufgaben vorhanden sind, wird stattdessen eine Eingabeaufforderung zum Hinzufügen einer neuen Aufgabe gedruckt.
Erstellen und führen Sie Ihr Programm mit dem Befehl all aus:
Er wird alle Aufgaben, die Sie bisher hinzugefügt haben, auflisten:
Nachdem Sie nun alle Aufgaben in der Datenbank anzeigen können, können Sie im nächsten Schritt die Möglichkeit hinzufügen, eine Aufgabe als erledigt zu markieren.
Schritt 5 - Abschließen einer Aufgabe
In diesem Schritt erstellen Sie einen neuen Unterbefehl namens done, mit dem Sie eine vorhandene Aufgabe in der Datenbank als erledigt markieren können.
Um eine Aufgabe als abgeschlossen zu markieren, können Sie die Methode collection.FindOneAndUpdate () verwenden.
Sie ermöglicht es Ihnen, ein Dokument in einer Sammlung zu lokalisieren und einige oder alle seine Eigenschaften zu aktualisieren.
Diese Methode erfordert einen Filter zum Auffinden des Dokuments und ein Aktualisierungsdokument zum Beschreiben des Vorgangs.
Beide werden mit den Typen bson.D erstellt.
Beginnen Sie durch Öffnen Ihrer main.go-Datei:
Fügen Sie den folgenden Snippet nach Ihrer Funktion filterTasks ein:
Die Funktion entspricht dem ersten Dokument, in dem die Texteigenschaft dem Textparameter entspricht.
Das Dokument update gibt an, dass die Eigenschaft completed auf true gesetzt wird.
Wenn beim Vorgang FindOneAndUpdate () ein Fehler auftritt, wird dieser von completeTask () zurückgegeben.
Andernfalls wird nil zurückgegeben.
Als Nächstes fügen wir Ihrem CLI-Programm einen neuen done-Befehl hinzu, der eine Aufgabe als erledigt markiert:
Sie verwenden das an den Befehl done übergebene Argument, um das erste Dokument zu finden, dessen Texteigenschaft übereinstimmt.
Wenn es gefunden wurde, wird die Eigenschaft completed im Dokument auf true gesetzt.
Führen Sie dann Ihr Programm mit dem Befehl done aus:
Wenn Sie den Befehl all erneut verwenden, werden Sie feststellen, dass die als erledigt markierte Aufgabe jetzt grün gedruckt wird.
Screenshot der Terminalausgabe nach dem Ausführen einer Aufgabe
Manchmal möchten Sie nur Aufgaben anzeigen, die noch nicht erledigt sind.
Wir fügen diese Eigenschaft als Nächstes hinzu.
Schritt 6 - Nur ausstehende Aufgaben anzeigen
In diesem Schritt integrieren Sie einen Code zum Abrufen ausstehender Aufgaben aus der Datenbank mithilfe des MongoDB-Treibers.
Ausstehende Aufgaben sind Aufgaben, deren Eigenschaft completed auf true gesetzt ist.
Lassen Sie uns eine neue Funktion hinzufügen, die Aufgaben abruft, die noch nicht abgeschlossen sind.
Fügen Sie dann dieses Snippet nach der Funktion completeTask hinzu:
Sie erstellen einen Filter mit den Paketen bson und primitive aus dem MongoDB-Treiber, der mit Dokumenten übereinstimmt, deren Eigenschaft completed auf true gesetzt ist.
Das Segment ausstehender Aufgaben wird dann an den Anrufer zurückgegeben.
Anstatt einen neuen Befehl zum Auflisten ausstehender Aufgaben zu erstellen, sollten Sie ihn zur Standardaktion machen, wenn Sie das Programm ohne Befehle ausführen.
Sie können dies tun, indem Sie dem Programm eine Action-Eigenschaft wie folgt hinzufügen:
Die Action-Eigenschaft führt eine Standardaktion aus, wenn das Programm ohne Unterbefehle ausgeführt wird.
Hier wird eine Logik für das Auflisten ausstehender Aufgaben platziert.
Die Funktion getPending () wird aufgerufen und die resultierenden Aufgaben werden mit printTasks () in die Standardausgabe gedruckt.
Wenn keine ausstehenden Aufgaben vorhanden sind, wird stattdessen eine Eingabeaufforderung angezeigt, in der der Benutzer aufgefordert wird, mit dem Befehl add eine neue Aufgabe hinzuzufügen.
Wenn Sie das Programm jetzt ausführen, ohne Befehle hinzuzufügen, werden alle ausstehenden Aufgaben in der Datenbank aufgelistet:
Nachdem Sie unvollständige Aufgaben aufgelistet haben, fügen wir einen weiteren Befehl hinzu, mit dem Sie nur abgeschlossene Aufgaben anzeigen können.
Schritt 7 - Anzeigen von abgeschlossenen Aufgaben
In diesem Schritt fügen Sie einen neuen Unterbefehl finished hinzu, der erledigte Aufgaben aus der Datenbank abruft und auf dem Bildschirm anzeigt.
Dies beinhaltet das Filtern und Zurückgeben von Aufgaben, deren Eigenschaft completed auf true gesetzt ist.
Fügen Sie dann am Ende Ihrer Datei den folgenden Code hinzu:
Ähnlich wie bei der Funktion getPending () haben Sie eine Funktion getFinished () hinzugefügt, die einen Teil der abgeschlossenen Aufgaben zurückgibt.
In diesem Fall hat der Filter die Eigenschaft completed auf true gesetzt, sodass nur die Dokumente zurückgegeben werden, die dieser Bedingung entsprechen.
Erstellen Sie als Nächstes einen Befehl finished, der alle erledigten Aufgaben druckt:
Der Befehl finished ruft Aufgaben ab, deren Eigenschaft completed über die hier erstellte Funktion getFinished () auf true gesetzt ist.
Anschließend werden sie an die Funktion printTasks übergeben, sodass sie in der Standardausgabe gedruckt werden.
Im letzten Schritt geben Sie Benutzern die Option, Aufgaben aus der Datenbank zu löschen.
Schritt 8 - Löschen einer Aufgabe
In diesem Schritt fügen Sie einen neuen Unterbefehl delete hinzu, um Benutzern zu ermöglichen, eine Aufgabe aus der Datenbank zu löschen.
Um eine einzelne Aufgabe zu löschen, verwenden Sie die Methode collection.DeleteOne () vom MongoDB-Treiber.
Außerdem stützt er sich auf einen Filter, der dem Dokument entspricht, um das Dokument zu löschen.
Öffnen Sie Ihre main.go-Datei erneut:
Fügen Sie diese Funktion deleteTask hinzu, um Aufgaben direkt nach Ihrer Funktion getFinished aus der Datenbank zu löschen:
Diese deleteTask-Methode verwendet ein Zeichenfolgenargument, das das zu löschende Aufgabenelement darstellt.
Ein Filter wird so erstellt, dass er mit dem Aufgabenelement übereinstimmt, dessen Texteigenschaft auf das Zeichenfolgenargument festgelegt ist.
Sie übergeben den Filter an die DeleteOne () -Methode, die dem Element in der Auflistung entspricht, und löschen es.
Sie können die DeletedCount-Eigenschaft für das Ergebnis der DeleteOne-Methode überprüfen, um zu bestätigen, ob Dokumente gelöscht wurden.
Wenn der Filter nicht mit einem zu löschenden Dokument übereinstimmen kann, ist DeletedCount Null und Sie können in diesem Fall einen Fehler zurückgeben.
Fügen Sie nun einen neuen Befehl rm wie hervorgehoben hinzu:
Wie bei allen anderen zuvor hinzugefügten Unterbefehlen verwendet der Befehl rm sein erstes Argument, um eine Aufgabe in der Datenbank abzugleichen und zu löschen.
Sie können ausstehende Aufgaben auflisten, indem Sie Ihr Programm ausführen, ohne Unterbefehle zu übergeben:
Wenn Sie den Unterbefehl rm für die Aufgabe "Buch lesen" ausführen, wird er aus der Datenbank gelöscht:
Wenn Sie alle ausstehenden Aufgaben erneut auflisten, werden Sie feststellen, dass die Aufgabe "Buch lesen" nicht mehr angezeigt wird, sondern stattdessen eine Aufforderung zum Hinzufügen einer neuen Aufgabe:
In diesem Schritt haben Sie eine Funktion hinzugefügt, um Aufgaben aus der Datenbank zu löschen.
Sie haben erfolgreich ein Task-Manager-Befehlszeilenprogramm erstellt und dabei die Grundlagen der Verwendung des MongoDB Go-Treibers erlernt.
Lesen Sie unbedingt die vollständige Dokumentation zum MongoDB Go-Treiber bei GoDoc, um mehr über die Funktionen zu erfahren, die die Verwendung des Treibers bietet.
Die Dokumentation, in der die Verwendung von Aggregationen oder Transaktionen beschrieben wird, könnte für Sie von besonderem Interesse sein.
Der endgültige Code für dieses Tutorial kann in diesem GitHub repo betrachtet werden.
Hinzufügen von Unit-Tests zu Ihrem Django-Projekt
5467
Es ist fast unmöglich, Websites zu erstellen, die beim ersten Mal perfekt und fehlerfrei funktionieren.
Aus diesem Grund müssen Sie Ihre Webanwendung testen, um diese Fehler zu finden und proaktiv zu bearbeiten.
Um die Effizienz von Tests zu verbessern, ist es üblich, Tests in Einheiten zu unterteilen, die bestimmte Funktionen der Webanwendung testen.
Diese Praxis wird als Unit-Test bezeichnet.
Es erleichtert das Erkennen von Fehlern, da sich die Tests unabhängig von anderen Teilen auf kleine Teile (Units) Ihres Projekts konzentrieren.
Das Testen einer Website kann eine komplexe Aufgabe sein, da sie aus mehreren Logikebenen besteht, z. B. der Verarbeitung von HTTP-Anforderungen, der Formularvalidierung und dem Wiedergeben von Vorlagen.
Django bietet jedoch eine Reihe von Tools, mit denen Sie Ihre Webanwendung nahtlos testen können.
In Django ist die bevorzugte Methode zum Schreiben von Tests die Verwendung des Python-Moduls unittest, obwohl andere Test-Frameworks verwendet werden können.
In diesem Tutorial richten Sie eine Testsuite in Ihrem Django-Projekt ein und schreiben Unit-Tests für die Modelle und Ansichten in Ihrer Anwendung.
Sie führen diese Tests durch, analysieren ihre Ergebnisse und erfahren, wie Sie die Ursachen für fehlgeschlagene Tests ermitteln.
Django auf Ihrem Server mit einer eingerichteten Programmierumgebung installiert.
Befolgen Sie dazu eines unserer Tutorials zum Installieren des Django Web-Frameworks und zum Einrichten einer Programmierumgebung.
Ein Django-Projekt, das mit Modellen und Ansichten erstellt wurde.
In diesem Tutorial haben wir das Projekt aus unserer Django-Entwicklungs-Tutorial-Reihe nachvollzogen.
Schritt 1 - Hinzufügen einer Testsuite zu Ihrer Django-Anwendung
Eine Testsuite in Django ist eine Sammlung aller Testfälle in allen Anwendungen in Ihrem Projekt.
Damit das Django-Testdienstprogramm die vorhandenen Testfälle ermitteln kann, schreiben Sie die Testfälle in Skripts, deren Namen mit test beginnen.
In diesem Schritt erstellen Sie die Verzeichnisstruktur und die Dateien für Ihre Testsuite und erstellen darin einen leeren Testfall.
Wenn Sie der Django-Entwicklungs-Tutorial-Reihe gefolgt sind, haben Sie eine Django-App namens < ^ > blogsite < ^ >.
Erstellen wir einen Ordner für alle unsere Testskripte.
Aktivieren Sie zunächst die virtuelle Umgebung:
Navigieren Sie dann zum App-Verzeichnis < ^ > blogsite < ^ >, dem Ordner, der die Dateien models.py und views.py enthält, und erstellen Sie einen neuen Ordner mit dem Namen tests:
Als nächstes verwandeln Sie diesen Ordner in ein Python-Paket. Fügen Sie also eine _ _ init _ _ .py-Datei hinzu:
Jetzt fügen Sie eine Datei zum Testen Ihrer Modelle und eine weitere zum Testen Ihrer Ansichten hinzu:
Schließlich erstellen Sie einen leeren Testfall in test _ models.py.
Sie müssen die Django TestCase-Klasse importieren und sie zu einer Superklasse Ihrer eigenen Testfallklasse machen.
Später werden Sie diesem Testfall Methoden hinzufügen, um die Logik in Ihren Modellen zu testen.
Öffnen Sie die Datei test _ models.py:
Sie haben der blogsite-App jetzt erfolgreich eine Testsuite hinzugefügt. Als nächstes füllen Sie die Details des leeren Modell-Testfalls aus, den Sie hier erstellt haben.
Schritt 2 - Testen Ihres Python-Codes
In diesem Schritt testen Sie die Logik des in die Datei models.py geschriebenen Codes.
Insbesondere testen Sie die Methode save des Modells Post, um sicherzustellen, dass es beim Aufruf den korrekten Slug des Titels eines Posts erzeugt.
Schauen wir uns zunächst den Code an, den Sie bereits in Ihrer Datei models.py für die Methode save des Post-Modells haben:
Wir können sehen, dass überprüft wird, ob der zu speichernde Beitrag einen Slug-Wert hat, und wenn nicht, wird slugify aufgerufen, um einen Slug-Wert dafür zu erstellen. Dies ist die Art von Logik, die Sie möglicherweise testen möchten, um sicherzustellen, dass beim Speichern eines Beitrags tatsächlich Slugs erstellt werden.
Schließen Sie die Datei.
Gehen Sie zum Testen zu test _ models.py zurück:
Aktualisieren Sie sie dann wie folgt und fügen Sie die hervorgehobenen Abschnitte hinzu:
Diese neue Methode test _ post _ has _ slug erstellt einen neuen Beitrag mit dem Titel "My first post", gibt dem Beitrag einen Autor und speichert ihn.
Anschließend wird mithilfe der assertEqual-Methode aus dem Python unittest-Modul überprüft, ob der Slug für den Beitrag korrekt ist.
Die assertEqual-Methode prüft, ob die beiden an sie übergebenen Argumente gleich sind, wie durch den Operator "= =" festgelegt, und löst einen Fehler aus, wenn dies nicht der Fall ist.
Speichern und schließen Sie test _ models.py.
Das ist ein Beispiel dafür, was getestet werden kann.
Je mehr Logik Sie Ihrem Projekt hinzufügen, desto mehr gibt es zu testen.
Wenn Sie der Methode save mehr Logik hinzufügen oder neue Methoden für das Post-Modell erstellen, sollten Sie hier weitere Tests hinzufügen.
Sie können sie der Methode test _ post _ has _ slug hinzufügen oder neue Testmethoden erstellen, ihre Namen müssen jedoch mit test beginnen.
Sie haben erfolgreich einen Testfall für das Post-Modell erstellt, in dem Sie bestätigt haben, dass Slugs nach dem Speichern korrekt erstellt wurden.
Im nächsten Schritt schreiben Sie einen Testfall, der Ansichten testen soll.
Schritt 3 - Verwenden des Testclients von Django
In diesem Schritt schreiben Sie einen Testfall, der eine Ansicht mit dem Django-Testclient testet.
Der Testclient ist eine Python-Klasse, die als Dummy-Webbrowser fungiert und es Ihnen ermöglicht, Ihre Ansichten zu testen und mit Ihrer Django-Anwendung auf die gleiche Weise wie ein Benutzer zu interagieren.
Sie können auf den Testclient zugreifen, indem Sie in Ihren Testmethoden auf self.client verweisen.
Lassen Sie uns beispielsweise einen Testfall in test _ views.py erstellen.
Öffnen Sie als erstes die Datei test _ views.py:
Der ViewsTestCase enthält eine test _ index _ loads _ properly-Methode, die den Django-Testclient verwendet, um die Indexseite der Website zu besuchen (http: / / < ^ > your _ server _ ip < ^ >: 8000, wobei < ^ > your _ server _ ip < ^ > die IP-Adresse des Servers ist, den Sie benutzen).
Anschließend prüft die Testmethode, ob die Antwort den Statuscode 200 hat, was bedeutet, dass die Seite fehlerfrei geantwortet hat.
Infolgedessen können Sie sicher sein, dass die Seite auch beim Besuch des Benutzers fehlerfrei reagiert.
Abgesehen vom Statuscode können Sie sich auf der Seite Testing Response der Django-Dokumentation über weitere Eigenschaften der Testclient-Antwort informieren, die Sie testen können.
In diesem Schritt haben Sie einen Testfall erstellt, um zu testen, ob die Ansicht, in der die Indexseite wiedergegeben wird, fehlerfrei funktioniert.
In Ihrer Testsuite befinden sich jetzt zwei Testfälle.
Im nächsten Schritt führen Sie diese aus, um ihre Ergebnisse zu sehen.
Schritt 4 - Ausführen Ihrer Tests
Nachdem Sie eine Testsuite für das Projekt erstellt haben, ist es an der Zeit, diese Tests auszuführen und ihre Ergebnisse anzuzeigen.
Navigieren Sie zum Ausführen der Tests zum Ordner < ^ > blog < ^ > (der die Datei manage.py der Anwendung enthält):
Führen Sie sie dann folgendermaßen aus:
In Ihrem Terminal wird eine Ausgabe ähnlich der folgenden angezeigt:
In dieser Ausgabe gibt es zwei Punkte.., von denen jeder einen bestandenen Testfall darstellt.
Jetzt ändern Sie test _ views.py, um einen fehlgeschlagenen Test auszulösen.
Öffnen Sie zuerst die Datei mit:
Ändern Sie dann den markierten Code zu:
Hier haben Sie den Statuscode von 200 auf 404 geändert. Führen Sie nun den Test erneut aus Ihrem Verzeichnis mit manage.py aus:
Sie sehen, dass es eine beschreibende Fehlermeldung gibt, die Sie über das Skript, den Testfall und die Methode informiert, die fehlgeschlagen sind.
Außerdem wird die Fehlerursache angezeigt, wobei der Statuscode in diesem Fall nicht gleich 404 ist, mit der Meldung AssertionError: 200!
= 404. Der AssertionError wird hier in der hervorgehobenen Codezeile in der Datei test _ views.py ausgelöst:
Er zeigt Ihnen, dass die Behauptung falsch ist, d. h. der Antwortstatuscode (200) entspricht nicht den Erwartungen (404).
Vor der Fehlermeldung können Sie sehen, dass sich die beiden Punkte.. jetzt in.
F geändert haben, was Ihnen sagt, dass der erste Testfall bestanden wurde, während der zweite nicht bestanden hat.
In diesem Tutorial haben Sie in Ihrem Django-Projekt eine Testsuite erstellt, Testfälle zum Testen des Modells und zur Ansichtslogik hinzugefügt, das Ausführen von Tests gelernt und die Testausgabe analysiert.
Als nächsten Schritt können Sie neue Testskripte für Python-Code erstellen, der nicht in models.py und views.py enthalten ist.
Im Folgenden finden Sie einige Artikel, die sich beim Erstellen und Testen von Websites mit Django als hilfreich erweisen können:
Die Dokumentation zu Django Unit Tests
Die Tutorial-Reihe Skalieren von Django
Weitere Tutorials und Projekte finden Sie auf unserer Django-Themenseite.
Erstellen eines Minecraft-Servers unter Ubuntu 18.04
5474
Minecraft ist ein beliebtes Sandbox-Videospiel.
Ursprünglich im Jahr 2009 veröffentlicht, können Spieler in einer in 3D generierten Blockwelt bauen, erforschen, basteln und überleben.
Ende 2019 war es das zweitbest verkaufte Videospiel aller Zeiten.
In diesem Tutorial erstellen Sie Ihren eigenen Minecraft-Server, damit Sie und Ihre Freunde zusammen spielen können.
Insbesondere installieren Sie die erforderlichen Softwarepakete, um Minecraft auszuführen, konfigurieren den Server für die Ausführung und stellen das Spiel dann bereit.
Alternativ können Sie den One-Click Minecraft: Java Edition-Server von DigitalOcean als weiteren Installationspfad erkunden.
< $> note Dieses Tutorial verwendet die Java-Version von Minecraft.
Wenn Sie Ihre Version von Minecraft über den Microsoft App-Store gekauft haben, können Sie keine Verbindung mit diesem Server herstellen.
Die meisten Versionen von Minecraft, die auf Spielekonsolen wie PlayStation 4, Xbox One oder Nintendo Switch gekauft wurden, sind auch die Microsoft-Version von Minecraft.
Diese Konsolen können auch keine Verbindung zu dem in diesem Tutorial erstellten Server herstellen.
Hier können Sie die Java-Version von Minecraft erhalten.
Einen Server mit einer Neuinstallation von Ubuntu 18.04, einen Nicht-root-Benutzer mit sudo-Berechtigungen und aktivierter SSH.
Sie können diesem Leitfaden folgen, um Ihren Server zu initialisieren und diese Schritte zu absolvieren.
Minecraft kann ressourcenintensiv sein. Denken Sie also bei der Auswahl Ihrer Servergröße daran.
Wenn Sie DigitalOcean verwenden und mehr Ressourcen benötigen, können Sie die Größe Ihres Droplets jederzeit ändern, um mehr CPUs und RAM hinzuzufügen.
Eine Kopie von Minecraft Java Edition, die auf einem lokalen Mac-, Windows- oder Linux-Rechner installiert ist.
Schritt 1 - Installieren der erforderlichen Softwarepakete
Nachdem Ihr Server initialisiert wurde, müssen Sie zunächst Java installieren. Sie benötigen es, um Minecraft auszuführen.
Installieren Sie als nächstes die OpenJDK-Version 8 von Java, insbesondere die headless JRE.
Dies ist eine minimale Version von Java, die die Unterstützung für GUI-Anwendungen entfernt.
Das macht sie ideal für die Ausführung von Java-Anwendungen auf einem Server:
Sie müssen auch eine Software namens screen verwenden, um abtrennbare Serversitzungen zu erstellen. Mit screen können Sie eine Terminalsitzung erstellen und sich von dieser trennen, wobei der darauf gestartete Prozess weiterläuft.
Dies ist wichtig, da dies die Sitzung beenden und Ihren Server stoppen würde, wenn Sie Ihren Server starten und dann Ihr Terminal schließen würden.
Installieren Sie screen jetzt:
Nachdem Sie nun Java installiert haben, laden Sie den Minecraft-Server von der Minecraft-Website herunter.
Schritt 2 - Herunterladen der neuesten Version von Minecraft
Jetzt müssen Sie die aktuelle Version des Minecraft-Servers herunterladen.
Sie können dies tun, indem Sie zur Minecraft-Website navigieren und den Link Download minecraft _ server.
< ^ > X.X.X < ^ > .jar kopieren., wobei die X die neueste Version des Servers angeben.
Sie können jetzt wget und den kopierten Link verwenden, um den Server herunterzuladen:
Wenn Sie beabsichtigen, Ihren Minecraft-Server zu aktualisieren, oder wenn Sie verschiedene Versionen von Minecraft ausführen möchten, benennen Sie die heruntergeladene server.jar in minecraft _ server _ < ^ > 1.15.2 < ^ > .jar um und stimmen Sie die hervorgehobenen Versionsnummern mit der jeweiligen heruntergeladenen Version ab:
Wenn Sie eine ältere Version von Minecraft herunterladen möchten, finden Sie diese archiviert auf mcversions.net.
Dieses Tutorial konzentriert sich jedoch auf die aktuelle neueste Version.
Nachdem Sie Ihren Download durchgeführt haben, beginnen wir mit der Konfiguration Ihres Minecraft-Servers.
Schritt 3 - Konfigurieren und Ausführen des Minecraft-Servers
Nachdem Sie die Minecraft-JAR-Datei heruntergeladen haben, können Sie sie ausführen.
Starten Sie zunächst eine Bildschirmsitzung, indem Sie den Befehl screen ausführen:
Wenn Sie das angezeigte Banner gelesen haben, drücken Sie SPACE. screen wird wie gewohnt eine Terminalsitzung anzeigen.
Diese Sitzung ist jetzt abtrennbar. Dies bedeutet, dass Sie hier einen Befehl starten und ausführen können.
Sie können nun Ihre Erstkonfiguration ausführen.
Seien Sie nicht beunruhigt, wenn dieser nächste Befehl einen Fehler auslöst.
Minecraft hat seine Installation so konzipiert, dass Benutzer zuerst der Lizenzvereinbarung des Unternehmens zustimmen müssen.
Sie tun dies als nächstes:
Bevor wir die Ausgabe dieses Befehls untersuchen, schauen wir uns all diese Befehlszeilenargumente genauer an, die Ihren Server optimieren:
Xms1024M - hiermit wird der Server so konfiguriert, dass er mit 1024 MB oder 1 GB RAM ausgeführt wird.
Sie können dieses Limit erhöhen, wenn Ihr Server mit mehr RAM beginnen soll.
Sowohl M für Megabytes als auch G für Gigabytes werden als Optionen unterstützt.
Beispiel: Xms2G startet den Server mit 2 Gigabyte RAM.
Xmx1024M - hiermit wird der Server so konfiguriert, dass höchstens 1024 MB RAM verwendet werden.
Sie können dieses Limit erhöhen, wenn Ihr Server größer ausgeführt werden soll, mehr Spieler zugelassen werden sollen oder wenn Sie der Meinung sind, dass Ihr Server langsam läuft.
jar - dieses Flag gibt an, welche Server-JAR-Datei ausgeführt werden soll.
nogui - dies weist den Server an, keine GUI zu starten, da dies ein Server ist und Sie keine grafische Benutzeroberfläche haben.
Wenn Sie diesen Befehl zum ersten Mal ausführen, der normalerweise Ihren Server startet, wird stattdessen der folgende Fehler generiert:
Diese Fehler wurden generiert, weil der Server zwei für die Ausführung erforderliche Dateien nicht finden konnte: die EULA (Endbenutzer-Lizenzvereinbarung) in eula.txt und die Konfigurationsdatei server.properties.
Da der Server diese Dateien nicht finden konnte, wurden sie glücklicherweise in Ihrem aktuellen Arbeitsverzeichnis erstellt.
Öffnen Sie zunächst eula.txt in nano oder Ihrem bevorzugten Texteditor:
Innerhalb dieser Datei sehen Sie einen Link zur Minecraft EULA.
Kopieren Sie die URL:
Öffnen Sie die URL in Ihrem Webbrowser und lesen Sie die Vereinbarung.
Kehren Sie dann zu Ihrem Texteditor zurück und suchen Sie die letzte Zeile in eula.txt.
Ändern Sie hier eula = false in eula = < ^ > true < ^ >.
Speichern und schließen Sie jetzt die Datei.
Nachdem Sie die EULA akzeptiert haben, ist es an der Zeit, den Server gemäß Ihren Spezifikationen zu konfigurieren.
In Ihrem aktuellen Arbeitsverzeichnis finden Sie auch die neu erstellte Datei server.properties.
Diese Datei enthält alle Konfigurationsoptionen für Ihren Minecraft-Server.
Eine detaillierte Liste aller Servereigenschaften finden Sie im offiziellen Minecraft-Wiki.
Sie ändern diese Datei mit Ihren bevorzugten Einstellungen, bevor Sie Ihren Server starten.
Dieses Tutorial behandelt die grundlegenden Eigenschaften:
Ihre Datei wird wie folgt aussehen:
Sehen wir uns einige der wichtigsten Eigenschaften in dieser Liste genauer an:
difficulty ​ ​ ​ ​ ​ ​ (Standard ​ ​ ​ ​ ​ < ^ > easy < ^ > ​ ​ ​ ​ ​ ​) - hiermit wird der Schwierigkeitsgrad des Spiels festgelegt, z. B. wie viel Schaden verursacht wird und wie sich die Elemente auf Ihren Spieler auswirken.
Die Optionen sind peaceful, < ^ > easy < ^ >, normal und hard.
gamemode (Standard < ^ > survival < ^ >) - hiermit wird der Spielmodus festgelegt.
Die Optionen sind < ^ > survival < ^ >, creative, adventure und spectator.
level-name (Standard < ^ > world < ^ >) - hiermit wird der Name Ihres Servers festgelegt, der im Client angezeigt wird.
Zeichen wie das Apostroph müssen möglicherweise mit einem Rückschrägstrich versehen werden.
motd (Standard < ^ > A Minecraft-Server < ^ >) - die Meldung, die in der Serverliste des Minecraft-Clients angezeigt wird.
pvp (Standard < ^ > true < ^ >) - aktiviert den Kampf zwischen Spieler und Spieler.
Wenn der Wert auf true gesetzt ist, können die Spieler kämpfen und sich gegenseitig Schaden zufügen.
Wenn Sie die gewünschten Optionen festgelegt haben, speichern und schließen Sie die Datei.
Nachdem Sie EULA in < ^ > true < ^ > geändert und Ihre Einstellungen konfiguriert haben, können Sie Ihren Server erfolgreich starten.
Lassen Sie uns wie beim letzten Mal Ihren Server mit 1024 MB RAM starten.
Lassen Sie uns Minecraft erst jetzt auch die Möglichkeit geben, bei Bedarf bis zu 4 GB RAM zu verwenden. Denken Sie daran, dass Sie diese Nummer gerne an Ihre Servereinschränkungen oder Benutzeranforderungen anpassen können:
Geben Sie der Initialisierung einige Momente.
Bald wird Ihr neuer Minecraft-Server eine ähnliche Ausgabe produzieren:
Sobald der Server betriebsbereit ist, wird die folgende Ausgabe angezeigt:
Ihr Server wird jetzt ausgeführt und Sie wurden in das Kontrollfeld des Serveradministrators verschoben.
Geben Sie jetzt help ein:
Eine Ausgabe wie diese wird angezeigt:
Von diesem Terminal aus können Sie Administratorbefehle ausführen und Ihren Minecraft-Server steuern.
Verwenden Sie jetzt screen, um Ihren neuen Server auch nach dem Abmelden am Laufen zu halten. Dann können Sie sich mit Ihrem Minecraft-Client verbinden und ein neues Spiel starten.
Schritt 4 - Aufrechterhaltung des Serverbetriebs
Nachdem Sie nun Ihren Server eingerichtet haben, möchten Sie, dass er auch nach dem Trennen von Ihrer SSH-Sitzung ausgeführt wird.
Da Sie zuvor screen verwendet haben, können Sie sich von dieser Sitzung trennen, indem Sie Strg + A + D drücken. Jetzt befinden Sie sich wieder in Ihrer ursprünglichen Shell.
Führen Sie diesen Befehl aus, um alle Ihre screen-Sitzungen anzuzeigen:
Sie erhalten eine Ausgabe mit der ID Ihrer Sitzung, die Sie benötigen, um diese Sitzung fortzusetzen:
Um Ihre Sitzung fortzusetzen, übergeben Sie das Flag -r an den Befehl screen und geben Sie dann Ihre Sitzungs-ID ein:
Wenn Sie bereit sind, sich von Ihrem Server abzumelden, trennen Sie sich mit Strg + A + D von der Sitzung und melden Sie sich dann ab.
Schritt 5 - Herstellen einer Verbindung zu Ihrem Server über den Minecraft-Client
Nachdem Ihr Server nun betriebsbereit ist, stellen wir über den Minecraft-Client eine Verbindung zu ihm her.
Dann können Sie spielen!
Starten Sie Ihre Kopie von Minecraft Java Edition und wählen Sie im Menü Multiplayer.
Wählen Sie im Menü Multiplayer
Als nächstes müssen Sie einen Server hinzufügen, zu dem eine Verbindung hergestellt werden soll. Klicken Sie daher auf die Schaltfläche Add Server.
Klicken Sie auf die Schaltfläche "Add Server"
Geben Sie im angezeigten Bildschirm Edit Server Info Ihrem Server einen Namen und geben Sie die IP-Adresse Ihres Servers ein.
Dies ist dieselbe IP-Adresse, die Sie für die Verbindung über SSH verwendet haben.
Benennen Sie Ihren Server und geben Sie die IP-Adresse ein
Nachdem Sie Ihren Servernamen und Ihre IP-Adresse eingegeben haben, kehren Sie zum Multiplayer-Bildschirm zurück, auf dem Ihr Server nun aufgelistet wird.
Wählen Sie Ihren Server aus und klicken Sie auf "Join Server"
Ab jetzt wird Ihr Server immer in dieser Liste angezeigt.
Wählen Sie ihn aus und klicken Sie auf Join Server.
Genießen Sie das Spiel!
Sie befinden sich auf Ihrem Server und können spielen!
Sie haben jetzt einen Minecraft-Server unter Ubuntu 18.04, auf dem Sie und alle Ihre Freunde spielen können!
Viel Spaß beim Erkunden, Basteln und Überleben in einer barbarischen 3D-Welt.
Und denken Sie daran: Achten Sie auf Griefer.
Anpassen von React-Komponenten mit Props
5468
In diesem Tutorial erstellen Sie benutzerdefinierte Komponenten, indem Sie Props an Ihre Komponente übergeben.
Props sind Argumente, die Sie einem JSX-Element bereitstellen.
Sie sehen aus wie Standard-HTML-Props, sind jedoch nicht vordefiniert und können viele verschiedene JavaScript-Datentypen aufweisen, darunter Zahlen, Zeichenfolgen, Funktionen, Arrays und sogar andere React-Komponenten.
Ihre benutzerdefinierten Komponenten können mithilfe von Props Daten anzeigen oder Daten verwenden, um die Komponenten interaktiv zu gestalten.
Props sind ein wichtiger Bestandteil beim Erstellen von Komponenten, die an unterschiedliche Situationen angepasst werden können. Wenn Sie sich mit ihnen vertraut machen, erhalten Sie die Werkzeuge, um benutzerdefinierte Komponenten zu entwickeln, die mit besonderen Situationen umgehen können.
Nachdem Sie Ihrer Komponente Props hinzugefügt haben, definieren Sie mithilfe von PropTypes den Datentyp, den Sie für eine Komponente erwarten.
PropTypes sind ein einfaches Typsystem, mit dem überprüft wird, ob die Daten zur Laufzeit mit den erwarteten Typen übereinstimmen.
Sie dienen sowohl als Dokumentation als auch als Fehlerprüfung, damit Ihre Anwendung bei der Skalierung vorhersehbar bleibt.
Am Ende des Tutorials werden Sie eine Vielzahl von props verwenden, um eine kleine Anwendung zu erstellen, die ein Array von Tierdaten verwendet und die Informationen anzeigt, einschließlich Name, wissenschaftlicher Name, Größe, Ernährung und zusätzliche Informationen.
< $> note Hinweis: Im ersten Schritt wird ein leeres Projekt eingerichtet, auf dem Sie die Tutorial-Übung aufbauen.
Wenn Sie bereits ein Arbeitsprojekt haben und direkt mit Props arbeiten möchten, beginnen Sie mit Schritt 2. < $>
In diesem Tutorial verwenden Sie die Create React App.
In diesem Tutorial werden auch Kenntnisse über React-Komponenten vorausgesetzt, die Sie in unserem Tutorial Erstellen benutzerdefinierte Komponenten in React kennenlernen können.
Schritt 1 - Erstellen eines leeren Projekts
In diesem Schritt erstellen Sie unter Verwendung der Create React App ein neues Projekt.
Anschließend löschen Sie das Beispielprojekt und die zugehörigen Dateien, die beim Bootstrap des Projekts installiert werden.
Schließlich erstellen Sie eine einfache Dateistruktur, um Ihre Komponenten zu organisieren.
Führen Sie in Ihrer Befehlszeile das folgende Skript aus, um ein neues Projekt mit create-react-app zu installieren:
Falls sich das Projekt nicht in einem Browserfenster geöffnet hat, können Sie es durch Navigieren zu http: / / localhost: 3000 / öffnen.
Wenn Sie dies von einem Remote-Server aus ausführen, ist die Adresse http: / / < ^ > your _ domain < ^ >: 3000.
Ihr Browser wird mit einer einfachen React-Anwendung geladen, die Teil der Create React App ist:
Sie erstellen einen völlig neuen Satz benutzerdefinierter Komponenten.
Sie löschen zunächst einigen Standardcode, damit Sie ein leeres Projekt haben können.
Zuerst öffnen Sie src / App.js in einem Texteditor.
Weitere Informationen zu App.js finden Sie unter Einrichten eines React-Projekts mit Create React App.
Öffnen Sie src / App.js mit dem folgenden Befehl:
Löschen Sie die Zeile import logo from '. / logo.svg ';.
Ersetzen Sie dann alles in der return-Anweisung, um eine Reihe leerer Tags zurückzugeben: < > < / >.
Dadurch erhalten Sie eine Validierungsseite, die nichts zurückgibt.
Sie werden es nicht in Ihrer Anwendung verwenden und Sie sollten unbenutzte Dateien entfernen, während Sie arbeiten.
Es wird Sie in Zukunft vor Verwirrung bewahren.
Wenn Sie in Ihren Browser schauen, sehen Sie einen leeren Bildschirm.
leerer Bildschirm in Chrome
Nachdem Sie das Create React App-Beispielprojekt gelöscht haben, erstellen Sie eine einfache Dateistruktur.
Dies hilft Ihnen, Ihre Komponenten isoliert und unabhängig zu halten.
Erstellen Sie im Verzeichnis src ein Verzeichnis mit dem Namen components.
Dies enthält alle Ihre benutzerdefinierten Komponenten.
Jede Komponente verfügt über ein eigenes Verzeichnis zum Speichern der Komponentendatei zusammen mit den Stilen, gegebenenfalls Bildern und Tests.
Erstellen Sie ein Verzeichnis für App:
Verschieben Sie alle App-Dateien in dieses Verzeichnis.
Verwenden Sie den Platzhalter *, um alle mit App. beginnenden Dateien auszuwählen, unabhängig von der Dateierweiterung.
Verwenden Sie dann den Befehl mv, um sie in das neue Verzeichnis zu setzen.
Aktualisieren Sie abschließend den relativen Importpfad in index.js, der Stammkomponente, die den gesamten Prozess bootet.
Die Import-Anweisung muss auf die Datei App.js im App-Verzeichnis verweisen. Nehmen Sie daher die folgende hervorgehobene Änderung vor:
Nachdem das Projekt eingerichtet wurde, können Sie Ihre erste Komponente erstellen.
Schritt 2 - Erstellen dynamischer Komponenten mit Props
In diesem Schritt erstellen Sie eine Komponente, die sich basierend auf den als Props bezeichneten Eingabeinformationen ändert.
Props sind die Argumente, die Sie an eine Funktion oder Klasse übergeben. Da Ihre Komponenten jedoch mit JSX in HTML-ähnliche Objekte umgewandelt werden, übergeben Sie die Props wie HTML-Attribute.
Im Gegensatz zu HTML-Elementen können Sie viele verschiedene Datentypen übergeben, von Zeichenfolgen über Arrays bis hin zu Objekten und sogar Funktionen.
Hier erstellen Sie eine Komponente, die Informationen zu Tieren anzeigt.
Diese Komponente verwendet den Namen und den wissenschaftlichen Namen des Tieres als Zeichenfolgen, die Größe als Ganzzahl, die Ernährung als ein Array von Zeichenfolgen und zusätzliche Informationen als Objekt.
Sie geben die Informationen als Props an die neue Komponente weiter und verbrauchen diese Informationen in Ihrer Komponente.
Am Ende dieses Schritts haben Sie eine benutzerdefinierte Komponente, die verschiedene Props verbraucht.
Sie können die Komponente auch wiederverwenden, um ein Datenarray mit einer gemeinsamen Komponente anzuzeigen.
Hinzufügen von Daten
Zunächst benötigen Sie einige Beispieldaten. Erstellen Sie eine Datei im Verzeichnis src / App mit dem Namen data.
Öffnen Sie die neue Datei in Ihrem Texteditor:
Fügen Sie als Nächstes ein Array von Objekten hinzu, die Sie als Beispieldaten verwenden:
Das Array von Objekten enthält eine Vielzahl von Daten und bietet Ihnen die Möglichkeit, eine Vielzahl von Props auszuprobieren.
Jedes Objekt ist ein separates Tier mit dem Namen des Tieres, dem wissenschaftlichen Namen, der Größe, der Ernährung und einem optionalen Feld namens additional, das Links oder Notizen enthält.
In diesem Code haben Sie auch das Array als default exportiert.
Erstellen von Komponenten
Erstellen Sie als nächstes eine Platzhalterkomponente namens AnimalCard.
Diese Komponente nimmt schließlich Props und zeigt die Daten an.
Erstellen Sie zunächst ein Verzeichnis in src / components mit dem Namen AnimalCard und verwenden Sie touch in einer Datei mit dem Namen src / components / AnimalCard / AnimalCard.js und einer CSS-Datei mit dem Namen src / components / AnimalCard / AnimalCard.css.
Öffnen Sie AnimalCard.js in Ihrem Texteditor:
Fügen Sie eine Basiskomponente hinzu, die das CSS importiert und ein < h2 > -Tag zurückgibt.
Jetzt müssen Sie die Daten und die Komponente in Ihre Basis-App-Komponente importieren.
Öffnen Sie src / components / App / App.js:
Importieren Sie die Daten und die Komponente und bilden eine Schleife über die Daten, die die Komponente für jedes Element in dem Array zurückgeben:
Hier verwenden Sie die .map () -Array-Methode, um die Daten zu iterieren. Zusätzlich zum Hinzufügen dieser Schleife haben Sie ein wrapping-div mit einer Klasse, die Sie für das Styling verwenden, und ein < h1 > -Tag zum Beschriften Ihres Projekts.
Wenn Sie speichern, wird der Browser neu geladen und Sie sehen für jede Karte eine Beschriftung.
React-Projekt im Browser ohne Styling
Fügen Sie als nächstes ein Styling hinzu, um die Elemente auszurichten.
Öffnen Sie App.css:
Ersetzen Sie den Inhalt durch Folgendes, um die Elemente anzuordnen:
Dadurch wird Flexbox verwendet, um die Daten neu anzuordnen, sodass sie ausgerichtet werden.
Das padding bietet Platz im Browserfenster. justify-content verteilt den zusätzlichen Abstand zwischen den Elementen, und .wrapper h1 gibt der Beschriftung Animal die volle Breite.
Wenn Sie dies tun, wird der Browser aktualisiert und Sie sehen einige voneinander beabstandete Daten.
React-Projekt im Browser mit voneinander beabstandeten Daten
Hinzufügen von Props
Nachdem Sie Ihre Komponenten eingerichtet haben, können Sie Ihr eine erste Prop hinzufügen.
Als Sie eine Schleife über Ihre Daten gelegt haben, hatten Sie Zugriff auf jedes Objekt in dem Array data und die darin enthaltenen Elemente.
Sie fügen jedes Datenelement einer separaten Prop hinzu, die Sie dann in Ihrer AnimalCard-Komponente verwenden.
Fügen Sie AnimalCard eine Prop von name hinzu.
Die Prop name sieht aus wie ein Standard-HTML-Attribut, aber anstelle einer Zeichenfolge übergeben Sie die Eigenschaft name aus dem Objekt animal in geschweiften Klammern.
Nachdem Sie eine Prop an die neue Komponente übergeben haben, müssen Sie sie verwenden. Öffnen Sie AnimalCard.js:
Alle Props, die Sie an die Komponente übergeben, werden in einem Objekt gesammelt, das das erste Argument Ihrer Funktion ist.
Destrukturieren Sie das Objekt, um einzelne Props herauszuziehen:
Beachten Sie, dass Sie eine Prop nicht destrukturieren müssen, um sie zu verwenden. Dies ist jedoch eine nützliche Methode für den Umgang mit den Beispieldaten in diesem Tutorial.
Nachdem Sie das Objekt destrukturiert haben, können Sie die einzelnen Daten verwenden. In diesem Fall verwenden Sie den Titel in einem < h2 > -Tag, das den Wert mit geschweiften Klammern umgibt, damit React ihn als JavaScript auswerten kann.
Sie können auch eine Eigenschaft für das prop-Objekt in Punktnotation verwenden.
Als Beispiel könnten Sie ein < h2 > -Element wie das folgende erstellen: < h2 > {props.title} < / h2 >.
Der Vorteil der Destrukturierung besteht darin, dass Sie nicht verwendete Props Requisiten sammeln und den Object-Rest-Operator verwenden können.
Wenn Sie dies tun, wird der Browser neu geladen und Sie sehen den spezifischen Namen für jedes Tier anstelle eines Platzhalters.
React-Projekte mit gerenderten Tiernamen
Die Eigenschaft name ist eine Zeichenfolge, aber Props können beliebige Datentypen sein, die Sie an eine JavaScript-Funktion übergeben können.
Um dies zu sehen, fügen Sie den Rest der Daten hinzu.
Öffnen Sie die Datei App.js:
Fügen Sie eine Prop für jedes der folgenden Elemente hinzu: scientificName, size, diet und additional.
Diese schließen Zeichenfolgen, Ganzzahlen, Arrays und Objekte ein.
Da Sie ein Objekt erstellen, können Sie sie in beliebiger Reihenfolge hinzufügen.
Alphabetisieren erleichtert das Einrichten einer Liste von Props, besonders in einer größeren Liste.
Sie können sie auch in derselben Zeile hinzufügen, aber durch Trennen in eine pro Zeile bleiben die Dinge lesbar.
Öffnen Sie AnimalCard.js.
Destrukturieren Sie diesmal die Prop in der Funktionsparameterliste und verwenden Sie die Daten in der Komponente:
Nachdem Sie die Daten abgerufen haben, können Sie scientificName und size in Überschriften-Tags einfügen. Sie müssen das Array jedoch in eine Zeichenfolge konvertieren, damit React es auf der Seite anzeigen kann.
Sie können dies mit join (', ') tun, wodurch eine durch Kommas getrennte Liste erstellt wird.
Wenn Sie dies tun, wird der Browser aktualisiert und Sie sehen die strukturierten Daten.
React-Projekt mit Tieren mit vollen Daten
Sie können eine ähnliche Liste mit dem Objekt additional erstellen, aber stattdessen eine Funktion hinzufügen, um den Benutzer über die Daten zu informieren. Dies gibt Ihnen die Möglichkeit, Funktionen als Props zu übergeben und dann Daten innerhalb einer Komponente zu verwenden, wenn Sie eine Funktion aufrufen.
Erstellen Sie eine Funktion namens showAdditionalData, die das Objekt in eine Zeichenfolge konvertiert und als Alert anzeigt.
Die Funktion showAdditional konvertiert das Objekt in ein Array von Paaren, wobei das erste Element der Schlüssel und das zweite der Wert ist.
Anschließend bildet sie die Daten ab, indem sie das Schlüsselpaar in eine Zeichenfolge konvertiert.
Anschließend werden sie mit einem Zeilenumbruch -\ n - verbunden, bevor die vollständige Zeichenfolge an die Alert-Funktion übergeben wird.
Da JavaScript Funktionen als Argumente akzeptieren kann, kann React auch Funktionen als Props akzeptieren.
Sie können showAdditional daher als Prop mit dem Namen showAdditional an AnimalCard übergeben.
Öffnen Sie AnimalCard:
Ziehen Sie die Funktion showAdditional aus dem Props-Objekt und erstellen Sie dann einen < button > mit einem onClick-Event, das die Funktion mit dem Objekt additional aufruft:
Wenn Sie dies tun, wird der Browser aktualisiert und Sie sehen eine Schaltfläche nach jeder Karte.
Wenn Sie auf die Schaltfläche klicken, erhalten Sie einen Alert mit den zusätzlichen Daten.
Alert mit Informationen
Wenn Sie versuchen, auf More Info für den Lion zu klicken, wird eine Fehlermeldung angezeigt.
Das liegt daran, dass es keine zusätzlichen Daten für den Löwen gibt.
Wie Sie das beheben können, erfahren Sie in Schritt 3.
Fügen Sie abschließend der Musikkarte etwas Styling hinzu.
Fügen Sie dem div in AnimalCard einen className von animal-wrapper hinzu:
Öffnen Sie AnimalCard.css:
Fügen Sie CSS hinzu, um den Karten und der Schaltfläche einen kleinen Rahmen und eine Füllung zu geben:
Dieses CSS fügt der Karte einen schmalen Rand hinzu und ersetzt das Standard-Schaltflächen-Styling durch einen Rahmen und eine Füllung. cursor: pointer ändert den Cursor, wenn Sie mit der Maus über die Schaltfläche fahren.
Wenn Sie dies tun, wird der Browser aktualisiert und Sie sehen die Daten auf einzelnen Karten.
React-Projekt mit gestalteten Tierkarten
Zu diesem Zeitpunkt haben Sie zwei benutzerdefinierte Komponenten erstellt.
Sie haben Daten von der ersten Komponente mithilfe von Props an die zweite Komponente übergeben.
Die Props enthielten eine Vielzahl von Daten wie Zeichenfolgen, Ganzzahlen, Arrays, Objekte und Funktionen.
In Ihrer zweiten Komponente haben Sie die Props verwendet, um mit JSX eine dynamische Komponente zu erstellen.
Im nächsten Schritt verwenden Sie ein Typsystem namens prop-types, um die Struktur anzugeben, die von Ihrer Komponente erwartet wird. Dies schafft Vorhersagbarkeit in Ihrer App und beugt Fehlern vor.
Schritt 3 - Erstellen vorhersehbarer Props mit PropTypes und defaultProps
In diesem Schritt fügen Sie Ihren Komponenten mit PropTypes ein leichtes Typsystem hinzu.
PropTypes verhalten sich wie andere Typsysteme, indem sie explizit den Datentyp definieren, den Sie für eine bestimmte Prop erwarten.
Sie geben Ihnen auch die Möglichkeit, Standarddaten in Fällen zu definieren, in denen die Prop nicht immer benötigt wird.
Im Gegensatz zu den meisten Typsystemen ist PropTypes eine Laufzeitprüfung. Wenn die Props also nicht mit dem Typ übereinstimmen, wird der Code weiterhin kompiliert, es wird jedoch auch ein Konsolenfehler angezeigt.
Am Ende dieses Schritts fügen Sie Ihrer benutzerdefinierten Komponente Vorhersagbarkeit hinzu, indem Sie den Typ für jede Prop definieren.
Dadurch wird sichergestellt, dass die nächste Person, die an der Komponente arbeitet, eine klare Vorstellung von der Struktur der Daten hat, die die Komponente benötigt.
Das prop-types-Paket ist Teil der Installation von Create React App. Um es zu verwenden, müssen Sie es lediglich in Ihre Komponente importieren.
Öffnen Sie AnimalCard.js:
Importieren Sie dann PropTypes aus prop-types:
Fügen Sie PropTypes direkt zu der Komponentenfunktion hinzu.
In JavaScript sind Funktionen Objekte, d. h. Sie können Eigenschaften mithilfe der Punktsyntax hinzufügen.
Fügen Sie AnimalCard.js die folgenden PropTypes hinzu:
Wie Sie sehen können, gibt es viele verschiedene PropTypes.
Dies ist nur eine kleine Auswahl; in der offiziellen React-Dokumentation finden Sie die anderen, die Sie verwenden können.
Beginnen wir mit der Prop name.
Hier geben Sie an, dass name ein string sein muss.
Die Eigenschaft scientificName ist dieselbe. size ist eine Zahl, die sowohl Gleitkommazahlen wie 1,5 als auch Ganzzahlen wie 6 enthalten kann. showAdditional ist eine Funktion (func).
diet ist hingegen etwas anders.
In diesem Fall geben Sie an, dass diet ein array sein soll, aber Sie müssen auch angeben, was dieses Array enthalten soll.
In diesem Fall enthält das Array nur Zeichenfolgen.
Wenn Sie Typen mischen möchten, können Sie eine andere Prop namens oneOfType verwenden, die ein Array gültiger PropTypes verwendet.
Sie können oneOfType überall verwenden. Wenn Sie also möchten, dass size entweder eine Zahl oder eine Zeichenfolge ist, können Sie sie wie folgt ändern:
Die Prop additional ist auch etwas komplexer.
In diesem Fall geben Sie ein Objekt an, aber um etwas klarer zu sein, geben Sie an, was das Objekt enthalten soll.
Dazu verwenden Sie PropTypes.shape, das ein Objekt mit zusätzlichen Feldern verwendet, für die eigene PropTypes erforderlich sind.
In diesem Fall sind link und notes beide PropTypes.string.
Derzeit sind alle Daten wohlgeformt und stimmen mit den Props überein.
Öffnen Sie Ihre Daten, um zu sehen, was passiert, wenn die PropTypes nicht übereinstimmen:
Ändern Sie die Größe in eine Zeichenfolge in dem ersten Element:
Wenn Sie dies tun, wird der Browser aktualisiert und Sie sehen eine Fehlermeldung in der Konsole.
Browser mit Typfehler
Im Gegensatz zu anderen Typsystemen wie TypeScript gibt PropTypes beim Erstellen keine Warnung aus. Solange keine Codefehler vorliegen, wird es dennoch kompiliert.
Dies bedeutet, dass Sie versehentlich Code mit Prop-Fehlern veröffentlichen könnten.
Ändern Sie die Daten wieder auf den richtigen Typ:
Jede Prop außer additional hat die Eigenschaft isRequired.
Das bedeutet, dass sie benötigt werden.
Wenn Sie keine erforderliche Prop einfügen, wird der Code weiterhin kompiliert, es wird jedoch ein Laufzeitfehler in der Konsole angezeigt.
Wenn keine Prop benötigt wird, können Sie einen Standardwert hinzufügen.
Es wird empfohlen, immer einen Standardwert hinzuzufügen, um Laufzeitfehler zu vermeiden, wenn keine Prop erforderlich ist.
In der AnimalCard-Komponente rufen Sie beispielsweise eine Funktion mit den additional-Daten auf. Wenn sie nicht vorhanden ist, versucht die Funktion, ein nicht vorhandenes Objekt zu ändern, und die Anwendung stürzt ab.
Fügen Sie für additional eine defaultProp hinzu, um dieses Problem zu vermeiden:
Sie fügen der Funktion die defaultProps mithilfe der Punktsyntax hinzu, genau wie Sie es mit propTypes getan haben, und fügen dann einen Standardwert hinzu, den die Komponente verwenden sollte, wenn die Prop undefiniert ist.
In diesem Fall passen Sie an die Form von additional an, einschließlich einer Meldung, dass keine zusätzlichen Informationen vorhanden sind.
Wenn Sie dies tun, wird der Browser aktualisiert.
Klicken Sie nach der Aktualisierung auf die Schaltfläche More Info für Lion.
Es gibt kein Feld additional in den Daten, daher ist die Prop undefiniert.
Aber AnimalCard wird in der Standard-Prop ersetzt.
Browser mit Standardmeldung in dem Alert
Jetzt sind Ihre Props gut dokumentiert und entweder erforderlich oder haben eine Standardeinstellung, um den vorhersehbaren Code sicherzustellen.
Dies hilft zukünftigen Entwicklern (einschließlich Ihnen) zu verstehen, welche Props eine Komponente benötigt.
Dies erleichtert das Austauschen und Wiederverwenden Ihrer Komponenten, indem Sie vollständige Informationen darüber erhalten, wie die Komponente die empfangenen Daten verwendet.
In diesem Tutorial haben Sie mehrere Komponenten erstellt, die mithilfe von Props Informationen von einem übergeordneten Element anzeigen.
Props geben Ihnen die Flexibilität, größere Komponenten in kleinere, fokussiertere Teile zu zerlegen.
Da jetzt Ihre Daten nicht mehr eng mit Ihren Anzeigeinformationen verknüpft sind, können Sie entscheiden, wie Ihre Anwendung segmentiert werden soll.
Props sind ein wichtiges Werkzeug beim Erstellen komplexer Anwendungen und bieten die Möglichkeit, Komponenten zu erstellen, die sich an die empfangenen Daten anpassen können.
Mit PropTypes erstellen Sie vorhersehbare und lesbare Komponenten, mit denen ein Team die Arbeit des anderen wiederverwenden kann, um eine flexible und stabile Codebasis zu erstellen.
Installieren und Sichern von phpMyAdmin unter Ubuntu 20.04
5417
Während viele Benutzer die Funktionalität eines Datenbankverwaltungssystems wie MySQL benötigen, fühlen sie sich möglicherweise nicht wohl, wenn sie nur über die MySQL-Eingabeaufforderung mit dem System interagieren.
phpMyAdmin wurde erstellt, damit Benutzer über eine Weboberfläche mit MySQL interagieren können.
In diesem Leitfaden wird erläutert, wie Sie phpMyAdmin installieren und sichern, damit Sie Ihre Datenbanken sicher auf einem Ubuntu 20.04-System verwalten können.
Um diesen Leitfaden abzuschließen, benötigen Sie Folgendes:
Einen Ubuntu 20.04-Server.
Dieser Server sollte über einen Nicht-root-Benutzer mit Administratorberechtigungen und eine mit ufw konfigurierte Firewall verfügen.
Einen auf Ihrem Ubuntu 20.04-Server installierten LAMP (Linux, Apache, MySQL und PHP) -Stack.
Wenn dies noch nicht abgeschlossen ist, können Sie diesem Leitfaden zum Installieren eines LAMP-Stacks unter Ubuntu 20.04 folgen.
Darüber hinaus gibt es wichtige Sicherheitsaspekte bei der Verwendung von Software wie phpMyAdmin, da sie:
direkt mit Ihrer MySQL-Installation kommuniziert
die Authentifizierung mithilfe von MySQL-Anmeldeinformationen verarbeitet
Ergebnisse für beliebige SQL-Abfragen ausführt und sie zurückgibt
Aus diesen Gründen und weil es sich um eine weit verbreitete PHP-Anwendung handelt, die häufig angegriffen wird, sollten Sie phpMyAdmin niemals auf Remote-Systemen über eine einfache HTTP-Verbindung ausführen.
Wenn Sie keine vorhandene Domäne mit einem SSL- / TLS-Zertifikat konfiguriert haben, können Sie diesen Leitfaden zum Sichern von Apache mit Let "s Encrypt unter Ubuntu 20.04 befolgen.
Dazu müssen Sie einen Domänennamen registrieren, DNS-Einträge für Ihren Server erstellen und einen virtuellen Apache-Host einrichten.
Schritt 1 - Installieren von phpMyAdmin
Sie können APT verwenden, um phpMyAdmin aus den Standard-Ubuntu-Repositorys zu installieren.
Aktualisieren Sie als Nicht-root sudo-Benutzer den Paketindex Ihres Servers:
Anschließend können Sie das phpmyadmin-Paket installieren.
Zusammen mit diesem Paket wird in der offiziellen Dokumentation empfohlen, einige PHP-Erweiterungen auf Ihrem Server zu installieren, um bestimmte Funktionen zu aktivieren und die Leistung zu verbessern.
Wenn Sie das vorausgesetzte LAMP-Stack-Tutorial befolgt haben, wurden mehrere dieser Module zusammen mit dem php-Paket installiert.
Es wird jedoch empfohlen, auch diese Pakete zu installieren:
php-mbstring: Ein Modul zum Verwalten von Nicht-ASCII-Zeichenfolgen und zum Konvertieren von Zeichenfolgen in verschiedene Codierungen
php-zip: Diese Erweiterung unterstützt das Hochladen von .zip-Dateien auf phpMyAdmin
php-gd: Aktiviert die Unterstützung für die GD Graphics Library
php-json: Bietet PHP Unterstützung für die JSON-Serialisierung
php-curl: Ermöglicht PHP die Interaktion mit verschiedenen Servertypen unter Verwendung verschiedener Protokolle
Führen Sie den folgenden Befehl aus, um diese Pakete auf Ihrem System zu installieren.
Bitte beachten Sie jedoch, dass Sie für die Installation einige Auswahlmöglichkeiten zum korrekten Konfigurieren von phpMyAdmin benötigen.
Wir werden diese Optionen in Kürze durchgehen:
Hier sind die Optionen, die Sie auswählen sollten, wenn Sie dazu aufgefordert werden, um Ihre Installation korrekt zu konfigurieren:
Wählen Sie für die Serverauswahl apache2 < $> warning Warnung: Wenn die Eingabeaufforderung angezeigt wird, wird "apache2" markiert, aber nicht ausgewählt.
Wenn Sie nicht SPACE drücken, um Apache auszuwählen, verschiebt das Installationsprogramm die erforderlichen Dateien während der Installation nicht.
Drücken Sie SPACE, TAB und dann ENTER, um Apache auszuwählen.
Wählen Sie Yes, wenn Sie gefragt werden, ob dbconfig-common zum Einrichten der Datenbank verwendet werden soll.
Sie werden dann aufgefordert, ein MySQL-Anwendungspasswort für phpMyAdmin auszuwählen und zu bestätigen
< $> note Hinweis: Angenommen, Sie haben MySQL installiert, indem Sie Schritt 2 des vorausgesetzten LAMP-Stack-Tutorials ausgeführt haben, haben Sie möglicherweise beschlossen, das Plugin Validate Passwort zu aktivieren.
Zum Zeitpunkt dieses Schreibens löst das Aktivieren dieser Komponente einen Fehler aus, wenn Sie versuchen, ein Passwort für den Benutzer phpmyadmin festzulegen:
Fehler bei der Überprüfung des phpMyAdmin-Passworts
Um dies zu beheben, wählen Sie die Option abort, um den Installationsvorgang zu stoppen.
Öffnen Sie dann Ihre MySQL-Eingabeaufforderung:
Oder, wenn Sie die Passwortauthentifizierung für den MySQL-root-Benutzer aktiviert haben, führen Sie diesen Befehl aus und geben Sie dann Ihr Passwort ein, wenn Sie dazu aufgefordert werden:
Führen Sie bei der Eingabeaufforderung den folgenden Befehl aus, um die Komponente Validate Passwort zu deaktivieren.
Beachten Sie, dass dies nicht zu einer Deinstallation führt, sondern nur das Laden der Komponente auf Ihren MySQL-Server verhindert:
Danach können Sie den MySQL-Client schließen:
Versuchen Sie dann erneut, das phpmyadmin-Paket zu installieren, und es funktioniert wie erwartet:
Sobald phpMyAdmin installiert ist, können Sie die MySQL-Eingabeaufforderung erneut mit sudo mysql oder mysql -u root -p öffnen und anschließend den folgenden Befehl ausführen, um die Komponente Validate Passwort wieder zu aktivieren:
Der Installationsvorgang fügt die Apache-Konfigurationsdatei phpMyAdmin in das Verzeichnis / etc / apache2 / conf-enabled / ein, wo sie automatisch gelesen wird.
Um die Konfiguration von Apache und PHP für die Arbeit mit phpMyAdmin abzuschließen, müssen Sie in diesem Abschnitt des Tutorials nur noch explizit die mbstring-PHP-Erweiterung aktivieren. Geben Sie dazu Folgendes ein:
Starten Sie anschließend Apache neu, damit Ihre Änderungen erkannt werden:
phpMyAdmin ist jetzt installiert und konfiguriert, um mit Apache zu arbeiten.
Bevor Sie sich jedoch anmelden und mit Ihren MySQL-Datenbanken interagieren können, müssen Sie sicherstellen, dass Ihre MySQL-Benutzer über die für die Interaktion mit dem Programm erforderlichen Berechtigungen verfügen.
Schritt 2 - Anpassen der Benutzerauthentifizierung und -berechtigungen
Als Sie phpMyAdmin auf Ihrem Server installiert haben, wurde automatisch ein Datenbankbenutzer namens phpmyadmin erstellt, der bestimmte zugrunde liegende Prozesse für das Programm ausführt.
Anstatt sich als dieser Benutzer mit dem Administratorpasswort anzumelden, das Sie während der Installation festgelegt haben, wird empfohlen, dass Sie sich entweder als Ihr MySQL-Benutzer root oder als ein Benutzer anmelden, der Datenbanken über die phpMyAdmin-Schnittstelle verwaltet.
Konfigurieren des Passwortzugriffs für das MySQL root-Konto
Dadurch entsteht in vielen Fällen mehr Sicherheit und Benutzerfreundlichkeit, es können Aufgaben aber auch komplizierter werden, wenn Sie einem externen Programm - wie phpMyAdmin - den Zugriff auf den Benutzer erlauben müssen.
Um sich bei phpMyAdmin als MySQL-Benutzer root anzumelden, müssen Sie die Authentifizierungsmethode von auth _ socket auf eine umstellen, die ein Passwort verwendet, sofern Sie dies noch nicht getan haben.
Einige Versionen von PHP funktionieren jedoch nicht zuverlässig mit caching _ sha2 _ password.
PHP hat gemeldet, dass dieses Problem ab PHP 7.4 behoben wurde. Wenn Sie jedoch später beim Versuch, sich bei phpMyAdmin anzumelden, auf einen Fehler stoßen, möchten Sie möglicherweise root so einstellen, dass er sich stattdessen mit mysql _ native _ password authentifiziert:
Prüfen Sie dann die von Ihren Benutzern verwendeten Authentifizierungsmethoden erneut, um zu bestätigen, dass root nicht mehr über das auth _ socket-Plugin authentifiziert wird:
An dieser Ausgabe können Sie erkennen, dass sich der Benutzer root mit einem Passwort authentifiziert.
Sie können sich jetzt als root-Benutzer mit dem hier festgelegten Passwort bei der phpMyAdmin-Oberfläche anmelden.
Konfigurieren des Passwortzugriffs für einen dedizierten MySQL-Benutzer
Manchmal ist es hilfreicher, sich über einen dedizierten Benutzer mit phpMyAdmin zu verbinden.
Öffnen Sie dazu die MySQL-Shell erneut:
Wenn Sie die Passwortauthentifizierung für Ihren root-Benutzer aktiviert haben, wie im vorherigen Abschnitt beschrieben, müssen Sie den folgenden Befehl ausführen und Ihr Passwort eingeben, wenn Sie dazu aufgefordert werden, um eine Verbindung herzustellen:
< $> note Anmerkung: Je nachdem, welche Version von PHP Sie installiert haben, möchten Sie Ihren neuen Benutzer möglicherweise so einstellen, dass er sich mit mysql _ native _ password authentifiziert, anstatt mit caching _ sha2 _ password:
Beenden Sie anschließend die MySQL-Shell:
Sie können jetzt auf die Weboberfläche zugreifen, indem Sie den Domänennamen oder die öffentliche IP-Adresse Ihres Servers gefolgt von / phpmyadmin aufrufen:
phpMyAdmin-Anmeldebildschirm
Melden Sie sich an der Oberfläche an, entweder als root oder mit dem neuen Benutzernamen und Passwort, das Sie gerade konfiguriert haben.
Wenn Sie sich anmelden, sehen Sie die Benutzeroberfläche, die ungefähr so aussieht:
phpMyAdmin-Benutzeroberfläche
Nachdem Sie nun eine Verbindung zu phpMyAdmin herstellen und mit ihm interagieren können, müssen Sie nur noch die Sicherheit Ihres Systems erhöhen, um es vor Angreifern zu schützen.
Schritt 3 - Sichern Ihrer phpMyAdmin-Instanz
Aufgrund seiner Allgegenwart ist phpMyAdmin ein beliebtes Ziel für Angreifer, und Sie sollten besonders darauf achten, unbefugten Zugriff zu verhindern.
Dazu müssen Sie zuerst die Verwendung von .htaccess-Dateiüberschreibungen aktivieren, indem Sie die Apache-Konfigurationsdatei Ihrer phpMyAdmin-Installation bearbeiten.
Verwenden Sie Ihren bevorzugten Texteditor, um die Datei phpmyadmin.conf zu bearbeiten, die in Ihrem Apache-Konfigurationsverzeichnis abgelegt wurde.
Fügen Sie eine AllowOverride All-Anweisung im Abschnitt < Directory / usr / share / phpmyadmin > der Konfigurationsdatei wie folgt hinzu:
Wenn Sie diese Zeile hinzugefügt haben, speichern und schließen Sie die Datei.
Starten Sie Apache neu, um die vorgenommenen Änderungen zu implementieren:
Nachdem Sie die Verwendung von .htaccess-Dateien für Ihre Anwendung aktiviert haben, müssen Sie eine erstellen, um tatsächlich Sicherheit zu implementieren.
Damit dies erfolgreich ist, muss die Datei im Anwendungsverzeichnis erstellt werden.
Sie können die erforderliche Datei erstellen und in Ihrem Texteditor mit root-Berechtigungen öffnen, indem Sie Folgendes eingeben:
Geben Sie in diese Datei die folgenden Informationen ein:
Hier folgt, was jede dieser Zeilen bedeutet:
AuthType Basic: Diese Zeile gibt den Authentifizierungstyp an, den Sie implementieren.
Dieser Typ implementiert die Passwortauthentifizierung mithilfe einer Passwortdatei.
AuthName: Hiermit wird die Nachricht für das Authentifizierungsdialogfeld festgelegt.
Sie sollten dieses Generikum beibehalten, damit nicht autorisierte Benutzer keine Informationen darüber erhalten, was geschützt wird.
AuthUserFile: Hiermit wird der Speicherort der Passwortdatei festgelegt, die für die Authentifizierung verwendet wird.
Dies sollte außerhalb der Verzeichnisse liegen, die bereitgestellt werden.
Wir erstellen diese Datei in Kürze.
Require valid-user: Hiermit wird angegeben, dass nur authentifizierten Benutzern Zugriff auf diese Ressource gegeben werden soll.
Dies verhindert, dass nicht autorisierte Benutzer eintreten.
Der Speicherort, den Sie für Ihre Passwortdatei ausgewählt haben, war / etc / phpmyadmin / .htpasswd.
Sie können diese Datei jetzt erstellen und mit dem Dienstprogramm htpasswd an einen Erstbenutzer übergeben:
Sie werden aufgefordert, ein Passwort für den von Ihnen erstellten Benutzer auszuwählen und zu bestätigen.
Anschließend wird die Datei mit dem von Ihnen eingegebenen Hash-Passwort erstellt.
Wenn Sie einen zusätzlichen Benutzer eingeben möchten, müssen Sie dies wie folgt ohne das Flag -c tun:
Wenn Sie jetzt auf Ihr Unterverzeichnis phpMyAdmin zugreifen, werden Sie aufgefordert, den zusätzlichen Kontonamen und das Passwort einzugeben, die Sie gerade konfiguriert haben:
phpMyAdmin-Apache-Passwort
Nach Eingabe der Apache-Authentifizierung werden Sie zur regulären phpMyAdmin-Authentifizierungsseite weitergeleitet, auf der Sie Ihre MySQL-Anmeldeinformationen eingeben können.
Durch Hinzufügen eines zusätzlichen Satzes von Nicht-MySQL-Anmeldeinformationen bieten Sie Ihrer Datenbank eine zusätzliche Sicherheitsebene.
Dies ist wünschenswert, da phpMyAdmin in der Vergangenheit anfällig für Sicherheitsbedrohungen war.
Sie sollten jetzt phpMyAdmin konfiguriert und für die Verwendung auf Ihrem Ubuntu 20.04-Server bereit haben.
Über diese Schnittstelle können Sie Datenbanken, Benutzer und Tabellen erstellen sowie die üblichen Vorgänge wie das Löschen und Ändern von Strukturen und Daten ausführen.
Installieren und Verwenden von Composer unter Ubuntu 20.04
5462
Composer ist ein beliebtes Abhängigkeitsmanagement-Tool für PHP, das hauptsächlich zur Erleichterung der Installation und Aktualisierung von Projektabhängigkeiten entwickelt wurde.
Es prüft, von welchen anderen Paketen ein bestimmtes Projekt abhängt, und installiert sie für Sie, wobei die entsprechenden Versionen entsprechend den Projektanforderungen verwendet werden.
Composer wird auch häufig zum Booten neuer Projekte verwendet, die auf gängigen PHP-Frameworks wie Symfony und Laravel basieren.
In diesem Tutorial werden Sie Composer auf einem Ubuntu 20.04-System installieren und damit starten.
Um diesem Leitfaden folgen zu können, benötigen Sie Zugriff auf einen Ubuntu 20.04-Server als Nicht-root sudo-Benutzer und eine auf Ihrem Server aktivierte Firewall.
Schritt 1 - Installieren von PHP und zusätzlichen Abhängigkeiten
Zusätzlich zu Abhängigkeiten, die bereits in Ihrem Ubuntu 20.04-System enthalten sein sollten, wie z. B. git und curl, benötigt Composer php-cli, um PHP-Skripte in der Befehlszeile auszuführen und unzip, um komprimierte Archive zu extrahieren.
Wir installieren diese Abhängigkeiten jetzt.
Aktualisieren Sie zunächst den Paketmanager-Cache, indem Sie Folgendes ausführen:
Führen Sie als Nächstes den folgenden Befehl aus, um die erforderlichen Pakete zu installieren:
Sie werden aufgefordert, die Installation zu bestätigen, indem Sie Y und dann ENTER eingeben.
Sobald die Voraussetzungen installiert sind, können Sie Composer installieren.
Schritt 2 - Herunterladen und Installieren von Composer
Composer bietet ein in PHP geschriebenes Installationsskript.
Wir werden es herunterladen, sicherstellen, dass es nicht beschädigt ist, und es dann verwenden, um Composer zu installieren.
Stellen Sie sicher, dass Sie sich in Ihrem Startverzeichnis befinden, und rufen Sie das Installationsprogramm mit curl ab:
Als nächstes überprüfen wir, ob das heruntergeladene Installationsprogramm mit dem SHA-384-Hash für das neueste Installationsprogramm übereinstimmt, das auf der Seite Öffentliche Schlüssel / Signaturen von Composer zu finden ist.
Um den Überprüfungsschritt zu vereinfachen, können Sie den folgenden Befehl verwenden, um den neuesten Hash programmgesteuert von der Composer-Seite abzurufen und in einer Shell-Variable zu speichern:
Wenn Sie den erhaltenen Wert überprüfen möchten, können Sie Folgendes ausführen:
Führen Sie nun den folgenden PHP-Code aus, wie auf der Composer-Download-Seite angegeben, um zu überprüfen, ob das Installationsskript sicher ausgeführt werden kann:
Wenn in der Ausgabe Installer corrupt angezeigt wird, müssen Sie das Installationsskript erneut herunterladen und überprüfen, ob Sie den richtigen Hash verwenden.
Wiederholen Sie dann den Verifizierungsvorgang.
Wenn Sie ein verifiziertes Installationsprogramm haben, können Sie fortfahren.
Verwenden Sie zum globalen Installieren von Composer den folgenden Befehl, mit dem Composer als systemweiter Befehl mit dem Namen composer unter / usr / local / bin heruntergeladen und installiert wird:
Um Ihre Installation zu testen, führen Sie Folgendes aus:
Damit wird überprüft, dass Composer erfolgreich auf Ihrem System installiert und systemweit verfügbar ist.
< $> note Hinweis: Wenn Sie für jedes Projekt, das Sie auf diesem Server hosten, separate ausführbare Composer-Dateien bevorzugen, können Sie diese pro Projekt lokal installieren.
Diese Methode ist auch nützlich, wenn Ihr Systembenutzer nicht berechtigt ist, Software systemweit zu installieren.
Verwenden Sie dazu den Befehl php composer-setup.php.
Dadurch wird eine composer.phar-Datei in Ihrem aktuellen Verzeichnis generiert, die mit php composer.phar ausgeführt werden kann.
Nun schauen wir uns das Verwenden von Composer zur Verwaltung von Abhängigkeiten an.
Schritt 3 - Verwenden von Composer in einem PHP-Projekt
PHP-Projekte hängen häufig von externen Bibliotheken ab, und die Verwaltung dieser Abhängigkeiten und ihrer Versionen kann schwierig sein.
Composer löst dieses Problem, indem es die Projektversionen und -abhängigkeiten verfolgt und gleichzeitig das Auffinden, Installieren und Aktualisieren von Paketen erleichtert, die für ein Projekt erforderlich sind.
Um Composer in Ihrem Projekt verwenden zu können, benötigen Sie eine Datei composer.json.
Die Datei composer.json teilt Composer mit, welche Abhängigkeiten für Ihr Projekt heruntergeladen werden müssen und welche Versionen der einzelnen Pakete installiert werden dürfen.
Dies ist äußerst wichtig, um Ihr Projekt konsistent zu halten und die Installation instabiler Versionen zu vermeiden, die möglicherweise Abwärtskompatibilitätsprobleme verursachen können.
Sie müssen diese Datei nicht manuell erstellen. Dabei treten häufig Syntaxfehler auf.
Composer bietet eine interaktive Möglichkeit, eine neue Datei composer.json basierend auf den Eingaben des Benutzers zu erstellen. Dies ist eine gute Wahl, wenn Sie Ihr Projekt später als öffentliches Paket auf Packagist freigeben möchten.
Composer generiert außerdem automatisch eine Barebone-Datei composer.json, wenn Sie einen Befehl composer require ausführen, um eine Abhängigkeit in ein neu erstelltes Projekt aufzunehmen.
Die Verwendung von Composer zum Installieren eines Pakets als Abhängigkeit in einem Projekt umfasst die folgenden Schritte:
Identifizieren Sie, welche Art von Bibliothek die Anwendung benötigt.
Suchen Sie auf Packagist.org, dem offiziellen Paket-Repository für Composer, nach einer geeigneten Open Source-Bibliothek.
Wählen Sie das Paket, von dem Sie eine Abhängigkeit haben möchten.
Führen Sie composer require aus, um die Abhängigkeit in die Datei composer.json aufzunehmen und das Paket zu installieren.
Versuchen wir dies mit einer Demo-Anwendung.
Das Ziel dieser Anwendung ist es, einen bestimmten Satz in eine URL-freundliche Zeichenfolge umzuwandeln - einen Slug.
Dies wird häufig verwendet, um Seitentitel in URL-Pfade zu konvertieren (wie der letzte Teil der URL für dieses Lernprogramm).
Beginnen wir mit der Erstellung eines Verzeichnisses für unser Projekt.
Wir nennen es slugify:
Obwohl dies nicht erforderlich ist, können Sie jetzt einen Befehl composer init ausführen, um eine detaillierte Datei composer.json für Ihr Projekt zu erstellen.
Da das einzige Ziel unseres Projekts darin besteht, die Installation von Abhängigkeiten mit Composer zu demonstrieren, verwenden wir eine einfachere Datei composer.json, die automatisch generiert wird, wenn wir unser erstes Paket benötigen.
Jetzt ist es Zeit, Packagist.org nach einem Paket zu durchsuchen, mit dem wir Slugs generieren können.
Wenn Sie in Packagist nach dem Begriff "slug" suchen, erhalten Sie ein ähnliches Ergebnis wie dieses:
Packagist-Suchergebnisse für den Begriff "slug"
Sie sehen zwei Zahlen auf der rechten Seite jedes Pakets in der Liste.
Die Zahl oben gibt an, wie oft das Paket über Composer installiert wurde, und die Zahl unten gibt an, wie oft ein Paket auf GitHub mit einem Stern versehen wurde.
Im Allgemeinen sind Pakete mit mehr Installationen und mehr Sternen tendenziell stabiler, da sie von so vielen Menschen verwendet werden.
Es ist auch wichtig, die Paketbeschreibung auf Relevanz zu überprüfen, um sicherzustellen, dass sie Ihren Anforderungen entspricht.
Wir brauchen einen String-to-Slug-Konverter.
Aus den Suchergebnissen geht hervor, dass das Paket cocur / slugify, das als erstes Ergebnis auf dieser Seite angezeigt wird, mit einer angemessenen Anzahl von Installationen und Sternen gut übereinstimmt.
Pakete auf Packagist haben einen Anbieternamen und einen Paketnamen.
Jedes Paket hat eine eindeutige Kennung (einen Namespace) im selben Format, das GitHub für seine Repositories verwendet: < ^ > vendor < ^ > / < ^ > package < ^ >.
Die Bibliothek, die wir installieren möchten, verwendet den Namespace cocur / slugify.
Sie benötigen den Namespace eines Pakets, um ihn in Ihrem Projekt zu benötigen.
Nachdem Sie nun genau wissen, welches Paket Sie installieren möchten, können Sie composer require ausführen, um es als Abhängigkeit einzuschließen und die Datei composer.json für Ihr Projekt zu generieren.
Wenn Sie Pakete benötigen, ist es wichtig, zu beachten, dass Composer sowohl Abhängigkeiten auf Anwendungsebene als auch Abhängigkeiten auf Systemebene verfolgt.
Abhängigkeiten auf Systemebene sind wichtig, um anzugeben, auf welche PHP-Module ein Paket angewiesen ist.
Für das Paket cocur / slugify ist ein PHP-Modul erforderlich, das wir noch nicht installiert haben.
Wenn ein erforderliches Paket auf einer Systembibliothek basiert, die derzeit nicht auf Ihrem Server installiert ist, wird eine Fehlermeldung angezeigt, welche Anforderung fehlt:
Um das Systemabhängigkeitsproblem zu lösen, können wir mit apt search nach dem fehlenden Paket suchen:
Nachdem Sie den richtigen Paketnamen gefunden haben, können Sie apt erneut verwenden, um die Systemabhängigkeit zu installieren:
Nach Abschluss der Installation können Sie den Befehl composer require erneut ausführen:
Wie Sie der Ausgabe entnehmen können, hat Composer automatisch entschieden, welche Version des Pakets verwendet werden soll.
Wenn Sie jetzt das Verzeichnis Ihres Projekts überprüfen, enthält es zwei neue Dateien: composer.json und composer.lock sowie ein Verzeichnis vendor:
Die Datei composer.lock wird verwendet, um Informationen darüber zu speichern, welche Versionen jedes Pakets installiert sind, und um sicherzustellen, dass dieselben Versionen verwendet werden, wenn jemand anderes Ihr Projekt klont und seine Abhängigkeiten installiert.
Im Verzeichnis vendor befinden sich die Projektabhängigkeiten.
Der Ordner vendor sollte nicht in die Versionskontrolle übernommen werden. Sie müssen nur die Dateien composer.json und composer.lock einschließen.
< $> note Wenn Sie ein Projekt installieren, das bereits eine composer.json-Datei enthält, führen Sie composer install aus, um die Abhängigkeiten des Projekts herunterzuladen.
Lassen Sie uns einen kurzen Blick auf die Versionsbeschränkungen werfen.
Wenn Sie den Inhalt Ihrer Datei composer.json überprüfen, wird Folgendes angezeigt:
Möglicherweise bemerken Sie das Sonderzeichen ^ vor der Versionsnummer in composer.json.
Composer unterstützt verschiedene Einschränkungen und Formate zum Definieren der erforderlichen Paketversion, um Flexibilität zu bieten und gleichzeitig Ihr Projekt stabil zu halten.
Der von der automatisch generierten Datei composer.json verwendete Zirkumflex-Operator (^) ist der empfohlene Operator für maximale Interoperabilität nach der semantischen Versionierung.
In diesem Fall wird 4.0 als minimal kompatible Version definiert und Aktualisierungen auf zukünftige Versionen unter 5.0 werden zugelassen.
Im Allgemeinen müssen Sie keine Versionseinschränkungen in Ihrer composer.json-Datei vornehmen.
In einigen Situationen müssen Sie die Einschränkungen jedoch möglicherweise manuell bearbeiten, z. B. wenn eine wichtige neue Version Ihrer erforderlichen Bibliothek veröffentlicht wird und Sie ein Upgrade durchführen möchten oder wenn die Bibliothek, die Sie verwenden möchten, nicht der semantischen Versionierung folgt.
Im Folgenden finden Sie einige Beispiele, um Ihnen ein besseres Verständnis der Funktionsweise von Composer-Versionseinschränkungen zu vermitteln:
Einschränkung
Bedeutung
Erlaubte Beispielversionen
^ 1.0
> = 1.0 < 2.0
1.0, 1.2.3, 1.9.9
^ 1.1.0
> = 1.1.0 < 2.0
1.1.0, 1.5.6, 1.9.9
~ 1.0
> = 1.0 < 2.0.0
1.0, 1.4.1, 1.9.9
~ 1.0.0
> = 1.0.0 < 1.1
1.0.0, 1.0.4, 1.0.9
1.2.1
1. *
1.0.0, 1.4.5, 1.9.9
1.2.
*
> = 1.2 < 1.3
1.2.0, 1.2.3, 1.2.9
Eine ausführlichere Übersicht über die Versionseinschränkungen von Composer finden Sie in der offiziellen Dokumentation.
Schauen wir uns als nächstes an, wie Abhängigkeiten mit Composer automatisch geladen werden.
Schritt 4 - Einschließen des Autoload-Skripts
Da PHP selbst Klassen nicht automatisch lädt, bietet Composer ein Autoload-Skript, das Sie in Ihr Projekt aufnehmen können, damit das automatische Laden für Ihr Projekt funktioniert.
Diese Datei wird von Composer automatisch generiert, wenn Sie Ihre erste Abhängigkeit hinzufügen.
Das einzige, was Sie tun müssen, ist, die Datei vendor / autoload.php vor jeder Klasseninstanziierung in Ihre PHP-Skripte aufzunehmen.
Probieren wir es in unserer Demo-Anwendung aus.
Öffnen Sie eine neue Datei mit dem Namen test.php in Ihrem Texteditor:
Fügen Sie den folgenden Code hinzu, der die Datei vendor / autoload.php einbringt, die Abhängigkeit cocur / slugify lädt und daraus einen Slug erstellt:
Führen Sie das Skript jetzt aus:
Dies erzeugt die Ausgabe hello-world-this-is-a-long-sentence-and-i-need-to-make-a-slug-from-it.
Abhängigkeiten müssen aktualisiert werden, wenn neue Versionen herauskommen. Schauen wir uns also an, wie Sie damit umgehen können.
Schritt 5 - Aktualisieren von Projektabhängigkeiten
Wann immer Sie Ihre Projektabhängigkeiten auf neuere Versionen aktualisieren möchten, führen Sie den Befehl update aus:
Dadurch wird nach neueren Versionen der Bibliotheken gesucht, die Sie in Ihrem Projekt benötigt haben.
Wenn eine neuere Version gefunden wird und mit der in der Datei composer.json definierten Versionseinschränkung kompatibel ist, ersetzt Composer die zuvor installierte Version.
Die Datei composer.lock wird aktualisiert, um diese Änderungen widerzuspiegeln.
Sie können auch eine oder mehrere bestimmte Bibliotheken aktualisieren, indem Sie sie wie folgt angeben:
Stellen Sie sicher, dass Sie die Dateien composer.json und composer.lock in Ihrem Versionskontrollsystem einchecken, nachdem Sie Ihre Abhängigkeiten aktualisiert haben, damit auch andere diese neueren Versionen installieren können.
Composer ist ein leistungsstarkes Tool, das die Verwaltung von Abhängigkeiten in PHP-Projekten erheblich erleichtern kann.
Es bietet eine zuverlässige Möglichkeit, PHP-Pakete zu erkennen, zu installieren und zu aktualisieren, von denen ein Projekt abhängt.
In diesem Leitfaden haben wir erfahren, wie man Composer installiert, neue Abhängigkeiten in ein Projekt aufnimmt und diese Abhängigkeiten aktualisiert, sobald neue Versionen verfügbar sind.
Installieren von Django und Einrichten einer Entwicklungsumgebung unter Ubuntu 20.04
5478
Django ist ein kostenloses und Open-Source-basiertes Framework, das in Python geschrieben ist und dessen Grundprinzipien Skalierbarkeit, Wiederverwendbarkeit und schnelle Entwicklung sind.
Es ist auch für seine hohe Konsistenz auf Framework-Ebene und lose Kopplung bekannt, was bedeutet, dass einzelne Komponenten unabhängig voneinander sein können.
In diesem Tutorial richten wir eine Django-Umgebung für Entwicklungszwecke auf einem Ubuntu 20.04-Server ein.
Bei einer Live-Website gibt es zusätzliche Überlegungen, wie Verbinden mit einer Datenbank, Einrichten eines Domänennamens und Hinzufügen zusätzlicher Sicherheitsebenen.
Wir verfügen in unserem Django-Tag über verschiedene Tutorials zu Django, die Sie bei Ihrer Erstellung unterstützen können.
Ein Nicht-root-Benutzerkonto mit sudo-Berechtigungen, das Sie durch Befolgen und Abschließen des Tutorials Ersteinrichtung eines Servers für Ubuntu 20.04 erreichen können.
Python 3, eingerichtet mit einer virtuellen Programmierumgebung.
Sie können diese Einrichtung über den Python 3-Installationsleitfaden erreichen.
Schritt 1 - Installieren von Django
Es gibt verschiedene Möglichkeiten, Django mit dem Python-Paketmanager pip in einer virtuellen Umgebung zu installieren.
Während wir uns im Hauptverzeichnis des Servers befinden, erstellen wir das Verzeichnis, das unsere Django-Anwendung enthalten wird.
Führen Sie den folgenden Befehl aus, um ein Verzeichnis namens < ^ > django-apps < ^ > oder mit einem Namen Ihrer Wahl zu erstellen.
Navigieren Sie dann zu dem Verzeichnis.
Wenn Sie sich im Verzeichnis < ^ > django-apps < ^ > befinden, erstellen Sie Ihre virtuelle Umgebung.
Wir nennen sie die generische < ^ > env < ^ >; Sie sollten aber einen Namen verwenden, der für Sie und Ihr Projekt aussagekräftig ist.
Aktivieren Sie jetzt die virtuelle Umgebung mit dem folgenden Befehl:
Sie wissen, dass sie aktiviert ist, sobald sich das Präfix in < ^ > (env) < ^ > ändert, was in etwa wie folgt aussehen wird, je nach dem Verzeichnis, in dem Sie sich befinden:
Installieren Sie das Django-Paket mit pip in dieser Umgebung.
Durch Installieren von Django können Sie Django-Anwendungen erstellen und ausführen.
Überprüfen Sie nach der Installation Ihre Django-Installation, indem Sie eine Versionsprüfung ausführen:
So oder so ähnlich wird die resultierende Ausgabe aussehen:
Nachdem Django auf Ihrem Server installiert ist, können wir mit dem Erstellen eines Testprojekts fortfahren, um sicherzustellen, dass alles ordnungsgemäß funktioniert.
Wir erstellen eine grundlegende Webanwendung.
Wenn Sie unserer Ersteinrichtung des Servers gefolgt sind oder eine Firewall auf Ihrem Server ausgeführt wird, müssen wir den Port öffnen, den wir für die Firewall unseres Servers nutzen werden.
Für die UFW-Firewall können Sie den Port mit dem folgenden Befehl öffnen:
Wenn Sie DigitalOcean-Firewalls verwenden, können Sie HTTP aus den eingehenden Regeln auswählen.
Sie können mehr erfahren über DigitalOcean-Firewalls und die Erstellung von Regeln durch Ändern der eingehenden Regeln.
Schritt 3 - Starten des Projekts
Wir können jetzt mit django-admin, einem Befehlszeilenprogramm für Administrationsaufgaben in Python, eine Anwendung erstellen.
Dann können wir den Befehl startproject verwenden, um die Projektverzeichnisstruktur für unsere Testwebsite zu erstellen.
Führen Sie den folgenden Befehl aus, während Sie sich im Verzeichnis django-apps befinden:
< $> note Anmerkung: Durch Ausführen des Befehls django-admin startproject < ^ > < projectname > < ^ > werden sowohl das Projektverzeichnis als auch das Projektpaket < ^ > < projectname > < ^ > benannt und das Projekt im Verzeichnis, in dem der Befehl ausgeführt wurde, erstellt.
Wenn der optionale Parameter < ^ > < destination > < ^ > angegeben wird, verwendet Django das bereitgestellte Zielverzeichnis als Projektverzeichnis und erstellt manage.py sowie das Projektpaket darin. < $>
Jetzt können wir sehen, welche Projektdateien gerade erstellt wurden.
Navigieren Sie dann zum Verzeichnis testsite und listen Sie den Inhalt dieses Verzeichnisses auf, um zu sehen, welche Dateien erstellt wurden:
Sie sehen eine Ausgabe, die zeigt, dass dieses Verzeichnis eine Datei mit dem Namen manage.py und einen Ordner namens testsite enthält.
Die Datei manage.py ähnelt django-admin und legt das Paket des Projekts in sys.path ab.
Dadurch wird auch die Umgebungsvariable DJANGO _ SETTINGS _ MODULE festgelegt, sodass sie auf die Datei settings.py Ihres Projekts verweist.
Sie können das Skript manage.py in Ihrem Terminal anzeigen, indem Sie den Befehl less wie folgt ausführen:
Wenn Sie mit dem Lesen des Skripts fertig sind, drücken Sie q, um die Ansicht der Datei zu beenden.
Navigieren Sie jetzt zum Verzeichnis testsite, um die anderen Dateien, die erstellt wurden, anzuzeigen:
Führen Sie dann den folgenden Befehl aus, um den Inhalt des Verzeichnisses aufzulisten:
Sie sehen vier Dateien:
Lassen Sie uns die einzelnen Dateien genauer betrachten:
_ _ init _ _ .py dient als Einstiegspunkt für Ihr Python-Projekt.
asgi.py enthält die Konfiguration für die optionale Bereitstellung an das Asynchronous Server Gateway Interface oder ASGI, das einen Standard für Anwendungen bereitstellt, die entweder synchron und asynchron sind, und als Nachfolger von WSGI gilt (siehe unten).
settings.py beschreibt die Konfiguration Ihrer Django-Installation und lässt Django wissen, welche Einstellungen verfügbar sind.
urls.py enthält eine Liste namens urlpatterns, die URLs zu ihren views weiterleitet und abbildet.
wsgi.py enthält die Konfiguration für das Web Server Gateway Interface oder WSGI, das einen Standard für synchrone Python-Apps bietet.
< $> note Anmerkung: Obwohl Standarddateien generiert werden, können Sie die Dateien asgi.py und wsgi.py jederzeit ändern und an Ihre Bereitstellungsanforderungen anpassen.
Schritt 4 - Konfigurieren von Django
Jetzt können wir den Server starten und die Website auf einem bezeichneten Host und Port anzeigen, indem wir den Befehl runserver ausführen.
Wir müssen Ihre Server-IP-Adresse der Liste von ALLOWED _ HOSTS in der Datei settings.py im Verzeichnis ~ / test _ django _ app / testsite / testsite / hinzufügen.
Wie in der Django-Dokumentation angegeben, enthält die Variable ALLOWED _ HOSTS "eine Liste von Zeichenfolgen, die die Host- / Domänennamen repräsentieren, die diese Django-Site bereitstellen kann.
Dies ist eine Sicherheitsmaßnahme, um HTTP Host Header-Angriffe zu verhindern, was auch bei vielen scheinbar sicheren Webserverkonfigurationen möglich ist. "
Sie können Ihren bevorzugten Texteditor verwenden, um Ihre IP-Adresse hinzuzufügen.
Wenn Sie beispielsweise nano verwenden, führen Sie den folgenden Befehl aus:
Nachdem Sie den Befehl ausführen, navigieren Sie zum Abschnitt "Allowed Hosts" des Dokuments und fügen die IP-Adresse Ihres Servers in den quadratischen Klammern in einzelnen oder doppelten Anführungszeichen hinzu.
Sie können die Änderung speichern und nano beenden, indem Sie Strg + x gedrückt halten und dann die Taste y drücken.
Als Nächstes greifen wir über einen Browser auf unsere Web-App zu.
Schritt 5 - Zugreifen auf die Django-Web-App
Navigieren Sie nach Fertigstellung unserer Konfiguration zurück in das Verzeichnis, in dem sich manage.py befindet:
Führen Sie jetzt den folgenden Befehl aus, wobei Sie den Text < ^ > your-server-ip < ^ > durch die IP-Adresse Ihres Servers ersetzen:
Schließlich können Sie zum folgenden Link navigieren, um zu sehen, wie Ihre grundlegende Website aussieht, wobei Sie den hervorgehobenen Text erneut durch die tatsächliche IP-Adresse Ihres Servers ersetzen:
Sobald die Seite geladen ist, sehen Sie Folgendes:
Django-Standardseite
Dadurch wird bestätigt, dass Django richtig installiert wurde und unser Testprojekt korrekt funktioniert.
Für Zugriff auf die Verwaltungsoberfläche fügen Sie / admin / am Ende Ihrer URL hinzu:
So gelangen Sie zum Anmeldebildschirm:
Wenn Sie den Benutzernamen und das Passwort für den Administrator eingeben, die Sie gerade erstellt haben, erhalten Sie Zugriff auf den Hauptadministrationsbereich der Site:
Django-Admin-Seite
Weitere Informationen zum Arbeiten mit der Django-Administratoroberfläche finden Sie unter Aktivieren und Verbinden der Django-Administratoroberfläche.
Wenn Sie mit dem Testen Ihrer App fertig sind, können Sie STRG + C drücken, um den Befehl runserver anzuhalten.
Dadurch kehren Sie zu Ihrer Programmierumgebung zurück.
Wenn Sie bereit sind, Ihre Python-Umgebung zu verlassen, können Sie den Befehl deactivate ausführen:
Wenn Sie Ihre Programmierumgebung deaktivieren, kehren Sie zur Eingabeaufforderung im Terminal zurück.
In diesem Tutorial haben Sie Django erfolgreich installiert und eine Entwicklungsumgebung eingerichtet, um erste Schritte mit Ihrer Django-App zu machen.
Sie haben jetzt ein Fundament, um mit dem Entwickeln von Django-Webanwendungen zu beginnen.
Installieren von Nginx unter Ubuntu 20.04 Schnellstart
5466
In diesem Leitfaden erklären wir, wie Sie Nginx auf Ihrem Ubuntu 20.04-Server installieren können.
Eine ausführlichere Version dieses Tutorials finden Sie in Installieren von Nginx unter Ubuntu 20.04.
Da Nginx in Standard-Repositories von Ubuntu verfügbar ist, können Sie es mit dem Verpackungssystem apt installieren.
Installieren Sie Nginx:
Wenn Sie dem Tutorial zur Ersteinrichtung des Servers gefolgt sind, haben Sie die UFW-Firewall aktiviert.
Prüfen Sie mit dem folgenden Befehl die verfügbaren ufw-Anwendungsprofile:
Lassen Sie uns das restriktivste Profil aktivieren, das den Verkehr, den Sie konfiguriert haben, an Port 80 zulässt:
Öffnen Sie die Standardstartseite von Nginx, um zu bestätigen, dass die Software über Ihre IP-Adresse ordnungsgemäß ausgeführt wird.
Schritt 4 - Einrichten von Serverblocks (empfohlen)
Erstellen Sie das Verzeichnis für < ^ > your _ domain < ^ > mit dem Flag -p, um alle erforderlichen übergeordneten Verzeichnisse zu erstellen:
Die Berechtigungen Ihrer Webstämme sollten korrekt sein, wenn Sie Ihren unmask-Wert nicht geändert haben, aber Sie können das durch die folgende Eingabe prüfen:
Erstellen Sie einen neuen Serverblock in / etc / nginx / sites-available / < ^ > your _ domain < ^ >:
Aktivieren Sie die Datei, indem Sie einen Link von ihr zum Verzeichnis sites-enabled erstellen:
Zwei Serverblocks sind jetzt aktiviert und so konfiguriert, dass sie anhand ihrer Anweisungen listen und server _ name auf Anfragen reagieren:
Suchen Sie die Anweisung server _ names _ hash _ bucket _ size und entfernen Sie das Symbol #, um die Auskommentierung der Zeile aufzuheben:
Testen Sie auf Syntaxfehler:
Starten Sie Nginx neu, um Ihre Änderungen zu aktivieren:
Wenn Sie einen vollständigeren Anwendungsstapel erstellen möchten, lesen Sie diesen Artikel zu Konfigurieren eines LAMP-Stacks unter Ubuntu 20.04.
Installieren der Anaconda Python-Distribution unter Ubuntu 20.04
5470
Anaconda ist ein Open-Source-basierter Paketmanager, ein Umgebungsmanager und eine Distribution der Programmiersprachen Python und R.
Er wird häufig für Data Science, maschinelles Lernen, umfangreiche Datenverarbeitung, wissenschaftliches Rechnen und Vorhersageanalysen verwendet.
Mit einer Sammlung von mehr als 1.000 Data-Science-Paketen ist Anaconda in kostenlosen und gebührenpflichtigen Enterprise-Versionen verfügbar.
Die Anaconda-Distribution wird mit dem Befehlszeilenprogramm conda ausgeliefert.
Weitere Informationen über Anaconda und conda erhalten Sie, indem Sie die offizielle Anaconda-Dokumentation lesen.
In diesem Tutorial erfahren Sie, wie Sie die Python 3-Version von Anaconda auf einem Ubuntu 20.04-Server installieren.
Bevor Sie mit diesem Leitfaden beginnen, sollten Sie einen Nicht-root Benutzer mit sudo-Berechtigungen auf Ihrem Server konfiguriert haben.
Sie können diese Voraussetzung erfüllen, indem Sie unseren Leitfaden zur Ersteinrichtung des Servers unter Ubuntu 20.04 befolgen.
Installieren von Anaconda
Die beste Möglichkeit zur Installation von Anaconda besteht darin, das neueste Anaconda-Installer-Bash-Skript herunterzuladen, zu überprüfen und dann auszuführen.
Suchen Sie nach der neuesten Version von Anaconda für Python 3 auf der Anaconda Downloads-Seite.
Zum Zeitpunkt der Veröffentlichung dieses Dokuments ist die neueste Version 2020.02, aber Sie sollten eine neuere stabile Version verwenden, so verfügbar.
Wechseln Sie als Nächstes in das Verzeichnis / tmp auf Ihrem Server.
Dies ist ein gutes Verzeichnis zum Herunterladen kurzlebiger Elemente wie des Anaconda Bash-Skripts, das wir nach dem Ausführen nicht mehr benötigen.
Verwenden Sie curl, um den Link herunterzuladen, den Sie von der Anaconda-Website kopiert haben.
Wir geben dies zur schnelleren Verwendung in einer Datei namens anaconda.sh aus.
Wir können jetzt die Datenintegrität des Installationsprogramms mit kryptografischer Hash-Verifizierung durch die SHA-256-Prüfsumme überprüfen.
Wir verwenden den Befehl sha256sum zusammen mit dem Dateinamen des Skripts:
Sie erhalten eine Ausgabe, die wie folgt aussieht:
Sie sollten die Ausgabe mit den Hashes auf der Seite Anaconda mit Python 3 unter 64-Bit-Linux für Ihre entsprechende Anaconda-Version vergleichen.
Wenn Ihre Ausgabe mit dem in der Zeile sha2561 angezeigten Hash übereinstimmt, ist alles in Ordnung.
Jetzt können wir das Skript ausführen:
Drücken Sie ENTER, um fortzufahren, und drücken Sie dann erneut ENTER, um sich die Lizenz durchzulesen.
Sobald Sie mit dem Lesen der Lizenz fertig sind, werden Sie aufgefordert, die Lizenzbedingungen zu akzeptieren:
Wenn Sie zustimmen, geben Sie yes ein.
Jetzt werden Sie dazu aufgefordert, den Speicherort der Installation auszuwählen.
Sie können ENTER drücken, um den Standardspeicherort zu akzeptieren, oder zum Ändern einen anderen Ort angeben.
Der Installationsvorgang wird fortgesetzt.
Beachten Sie, dass es einige Zeit dauern kann.
Geben Sie yes ein, damit Sie Anaconda3 initialisieren können.
Sie erhalten eine Ausgabe, die bestätigt, dass Änderungen in verschiedenen Verzeichnissen vorgenommen wurden.
Eine der Zeilen, die Sie erhalten, dankt Ihnen für die Installation von Anaconda.
Jetzt können Sie die Installation aktivieren, indem Sie mit "source" die Datei ~ / .bashrc beziehen:
Sobald Sie damit fertig sind, gelangen Sie in die standardmäßige base-Programmierumgebung von Anaconda und ändert sich Ihre Eingabeaufforderung wie folgt:
Zwar wird Anaconda mit dieser standardmäßigen base-Programmierumgebung ausgeliefert, doch sollten Sie separate Umgebungen für Ihre Programme einrichten und diese isoliert voneinander speichern.
Sie können Ihre Installation weiter überprüfen, indem Sie den Befehl conda verwenden, beispielsweise mit list:
In der Ausgabe sind alle Pakete enthalten, die Teil der Anaconda-Installation sind:
Nachdem Anaconda installiert ist, können wir jetzt Anaconda-Umgebungen einrichten.
Einrichten von Anaconda-Umgebungen
Virtuelle Anaconda-Umgebungen ermöglichen es Ihnen, Projekte anhand von Python-Versionen und-Paketen zu organisieren.
Für jede von Ihnen erstellte Anaconda-Umgebung können Sie angeben, welche Version von Python verwendet wird, und alle damit verbundenen Programmierdateien in diesem Verzeichnis zusammenhalten.
Zuerst können wir überprüfen, welche Versionen von Python zur Verwendung verfügbar sind:
Sie erhalten eine Ausgabe mit den verschiedenen Versionen von Python, die Sie als Ziel nutzen können, einschließlich Python 3- und Python 2-Versionen.
Da wir in diesem Tutorial Anaconda mit Python 3 verwenden, haben Sie nur auf die Python 3-Versionen von Paketen Zugriff.
Erstellen wir nun mit der neuesten Version von Python 3 eine Umgebung. Wir können dies erreichen, indem wir Version 3 dem python-Argument anweisen.
Wir nennen diese Umgebung < ^ > my _ env < ^ >, aber Sie können einen beschreibenden Namen für Ihre Umgebung wählen, insbesondere, wenn Sie Umgebungen verwenden, um auf mehr als eine Python-Version zuzugreifen.
Wir erhalten eine Ausgabe mit Informationen dazu, was heruntergeladen wurde und welche Pakete installiert wurden, und werden dann dazu aufgefordert, mit y oder n fortzufahren. Wenn Sie zustimmen, geben Sie y ein.
Das Dienstprogramm conda holt jetzt die Pakete für die Umgebung ab und lässt Sie wissen, wann der Vorgang abgeschlossen ist.
Sie können Ihre neue Umgebung aktivieren, indem Sie Folgendes eingeben:
Nach der Aktivierung Ihrer Umgebung macht das Präfix Ihrer Eingabeaufforderung deutlich, dass Sie sich nicht mehr in der base-Umgebung befinden, sondern in der neuen Umgebung, die Sie gerade erstellt haben.
Innerhalb der Umgebung können Sie überprüfen, ob Sie die Version von Python verwenden, die Sie verwenden wollten:
Wenn Sie bereit sind, Ihre Anaconda-Umgebung zu deaktivieren, können Sie dazu Folgendes eingeben:
Beachten Sie, dass Sie das Wort source durch. ersetzen können, um dieselben Ergebnisse zu erzielen.
Um eine spezifischere Version von Python zu erhalten, können Sie eine bestimmte Version an das python-Argument übergeben, wie zum Beispiel 3.5:
Sie können alle von Ihnen eingerichteten Umgebungen mit diesem Befehl prüfen:
Das Sternchen zeigt die aktuell aktive Umgebung an.
Jede Umgebung, die Sie mit conda create erstellen, verfügt über mehrere Standardpakete:
_ libgcc _ mutex
ca-certificates
certifi
libedit
libffi
libgcc-ng
libstdcxx-ng
ncurses
openssl
pip
python
readline
setuptools
sqlite
tk
wheel
xz
zlib
Sie können mit dem folgenden Befehl zusätzliche Pakete wie z. B. numpy hinzufügen:
Wenn Sie wissen, dass Sie nach dem Erstellen eine numpy-Umgebung wünschen, können Sie diese in Ihrem Befehl conda create gezielt anlegen:
Wenn Sie nicht mehr an einem bestimmten Projekt arbeiten und die zugehörige Umgebung nicht mehr benötigen, können Sie sie entfernen. Geben Sie dazu Folgendes ein:
Wenn Sie jetzt den Befehl conda info --envs eingeben, wird die Umgebung, die Sie entfernt haben, nicht mehr aufgelistet.
Aktualisieren von Anaconda
Sie sollten regelmäßig dafür sorgen, dass Anaconda auf dem neuesten Stand ist, damit Sie mit allen aktuellen Paketversionen arbeiten.
Dazu sollten Sie zuerst das Dienstprogramm conda aktualisieren:
Wenn Sie dazu aufgefordert werden, geben Sie y ein, um mit der Aktualisierung fortzufahren.
Sobald die Aktualisierung von conda abgeschlossen ist, können Sie die Anaconda-Distribution aktualisieren:
Wenn Sie dazu aufgefordert haben, geben Sie erneut y ein.
Dadurch wird sichergestellt, dass Sie die neuesten Versionen von conda und Anaconda verwenden.
Deinstallieren von Anaconda
Wenn Sie Anaconda nicht mehr verwenden und feststellen, dass Sie die Anwendung deinstallieren möchten, sollten Sie mit dem Modul anaconda-clean beginnen. Es sorgt bei der Deinstallation von Anaconda für die Entfernung der Konfigurationsdateien.
Geben Sie y ein, wenn Sie dazu aufgefordert werden.
Nach der Installation können Sie den folgenden Befehl ausführen.
Sie werden aufgefordert, mit y zu antworten, bevor Sie die einzelnen Elemente löschen.
Wenn Sie lieber nicht dazu aufgefordert werden, geben Sie am Ende Ihres Befehls --yes ein:
Dadurch wird auch ein Backup-Ordner mit dem Namen .anaconda _ backup in Ihrem Stammverzeichnis erstellt:
Sie können jetzt Ihr gesamtes Anaconda-Verzeichnis entfernen, indem Sie den folgenden Befehl eingeben:
Schließlich können Sie die PATH-Zeile aus Ihrer .bashrc-Datei entfernen, die Anaconda hinzugefügt hat.
Öffnen Sie dazu zuerst einen Texteditor wie nano:
Scrollen Sie dann bis zum Ende der Datei (bei einer jüngeren Installation) und geben Sie STRG + W ein, um nach Anaconda zu suchen.
Löschen oder kommentieren Sie diesen Anaconda-Block aus:
Wenn Sie mit der Bearbeitung der Datei fertig sind, drücken Sie STRG + X und geben Sie y ein, um den Vorgang zu beenden und die Änderungen zu speichern.
Anaconda wurde jetzt von Ihrem Server entfernt.
Wenn Sie die base-Programmierumgebung nicht deaktiviert haben, können Sie den Server verlassen und erneut aufrufen, um sie zu entfernen.
In diesem Tutorial haben Sie die Installation von Anaconda ausgeführt und dabei mit dem Befehlszeilenprogramm conda gearbeitet, Umgebungen eingerichtet, Anaconda aktualisiert und Anaconda gelöscht, wenn Sie es nicht mehr benötigen.
Sie können Anaconda verwenden, um Workloads für Data-Science, wissenschaftliches Rechnen, Analysen und umfangreiche Datenverarbeitung zu verwalten.
Hier können Sie sich unsere Tutorials zu Datenanalyse und maschinellem Lernen ansehen, um mehr über verschiedene Tools und Projekte zu erfahren, die Sie nutzen können.
Außerdem bieten wir Ihnen ein kostenloses E-Book zum maschinellen Lernen namens Machine Learning-Projekte mit Pyhton zum Download.
Einrichten einer Firewall mit UFW unter Ubuntu 20.04
5465
UFW (oder Uncomplicated Firewall) ist eine vereinfachte Firewall-Verwaltungsschnittstelle, die die Komplexität von Paketfilterungstechnologie auf niedriger Ebene wie iptables und nftables versteckt.
Wenn Sie mit dem Sichern Ihres Netzwerks beginnen möchten und Sie nicht sicher sind, welches Tool Sie verwenden sollen, könnte UFW die richtige Wahl für Sie sein.
In diesem Tutorial erfahren Sie, wie Sie eine Firewall mit UFW unter Ubuntu 20.04 einrichten.
Einen Ubuntu 20.04-Server mit einem sudo Nicht-root User, den Sie über das Tutorial Ersteinrichtung des Servers unter Ubuntu 20.04 einrichten können.
UFW ist unter Ubuntu standardmäßig installiert.
Wenn UFW aus einem bestimmten Grund deinstalliert wurde, können Sie UFW mit sudo apt install ufw installieren.
Schritt 1 - Verwenden von IPv6 mit UFW (optional)
Dieses Tutorial wurde für IPv4 verfasst, funktioniert aber auch für IPv6, solange es aktiviert ist. Wenn auf Ihrem Ubuntu-Server IPv6 aktiviert ist, muss UFW so konfiguriert sein, dass IPv6 unterstützt wird, um Firewall-Regeln nicht für IPv4, sondern auch für IPv6 zu verwalten.
Öffnen Sie dazu die UFW-Konfiguration mit nano oder Ihrem bevorzugten Editor.
Stellen Sie dann sicher, dass der Wert von IPV6 yes lautet.
Das sollte wie folgt aussehen:
Wenn UFW aktiviert ist, wird es so konfiguriert, dass sowohl IPv4- als auch IPv6-Firewall-Regeln geschrieben werden.
Bevor wir UFW aktivieren, wollen wir jedoch überprüfen, ob Ihre Firewall so konfiguriert ist, dass Verbindungen über SSH möglich sind.
Beginnen wir mit der Einstellung der Standardrichtlinien.
Schritt 2 - Einrichten von Standardrichtlinien
Wenn Sie gerade mit der Verwendung Ihrer Firewall begonnen haben, sind Ihre Standardrichtlinien die ersten Regeln, die Sie definieren sollten.
Diese Regeln steuern die Handhabung von Daten, die nicht ausdrücklich von anderen Regeln abgedeckt werden.
Standardmäßig ist UFW so konfiguriert, dass alle eingehenden Verbindungen abgelehnt und alle ausgehenden Verbindungen zugelassen werden.
So kann niemand, der versucht, Ihren Server zu erreichen, eine Verbindung herstellen, während jede Anwendung innerhalb des Servers nach außen kommunizieren kann.
Lassen Sie uns Ihre UFW-Regeln zurück auf die Standardeinstellungen setzen, um sicherzugehen, dass Sie diesem Tutorial folgen können.
Um die von UFW verwendeten Standardeinstellungen auszuwählen, verwenden Sie diese Befehle:
Diese Befehle legen die Standardeinstellungen fest: eingehende Verbindungen werden abgelehnt und ausgehende Verbindungen zugelassen.
Die Standardeinstellungen der Firewall allein können für einen PC ausreichen, Server müssen aber normalerweise auf eingehende Anfragen von externen Benutzern reagieren.
Das sehen wir uns als Nächstes an.
Schritt 3 - Zulassen von SSH-Verbindungen
Wenn wir unsere UFW-Firewall jetzt aktivieren würden, würde sie alle eingehenden Verbindungen ablehnen.
Das bedeutet, dass wir Regeln erstellen müssen, die legitime eingehende Verbindungen (z. B. SSH- oder HTTP-Verbindungen) ausdrücklich zulassen, wenn unser Server auf diese Art von Anforderungen reagieren soll.
Wenn Sie einen Cloud-Server verwenden, werden Sie wahrscheinlich eingehende SSH-Verbindungen zulassen wollen, damit Sie sich mit Ihrem Server verbinden und den Server verwalten können.
Um Ihren Server so zu konfigurieren, dass eingehende SSH-Verbindungen zugelassen werden, können Sie diesen Befehl verwenden:
Dadurch werden Firewall-Regeln erstellt, die alle Verbindungen an Port 22 zulassen; das ist der Port, an dem der SSH-Daemon standardmäßig lauscht.
UFW weiß, was Port allow ssh bedeutet, da dies in der Datei / etc / services als Dienst aufgeführt wird.
Wir können die äquivalente Regel jedoch auch schreiben, indem wir den Port anstelle des Dienstnamens angeben.
Dieser Befehl funktioniert zum Beispiel genauso wie oben:
Wenn Sie Ihren SSH-Daemon so konfiguriert haben, dass er einen anderen Port verwendet, müssen Sie den entsprechenden Port angeben. Wenn Ihr SSH-Server beispielsweise an Port 2222 lauscht, können Sie diesen Befehl verwenden, um Verbindungen an diesem Port zuzulassen:
Nachdem Ihre Firewall nun so konfiguriert ist, dass eingehende SSH-Verbindungen zugelassen werden, können wir sie aktivieren.
Schritt 4 - Aktivieren von UFW
Um UFW zu aktivieren, verwenden Sie diesen Befehl:
Sie erhalten eine Warnung, die besagt, dass der Befehl bestehende SSH-Verbindungen stören kann.
Wir haben bereits eine Firewall-Regel eingerichtet, die SSH-Verbindungen zulässt. Daher sollte es in Ordnung sein, fortzufahren.
Beantworten Sie die Eingabeaufforderung mit y und drücken Sie ENTER.
Die Firewall ist jetzt aktiv.
Führen Sie den Befehl sudo ufw status verbose aus, um die festgelegten Regeln anzuzeigen. Im Rest des Tutorials wird die Verwendung von UFW im Detail behandelt, wie das Zulassen oder Ablehnen verschiedener Verbindungen.
Schritt 5 - Zulassen anderer Verbindungen
Jetzt sollten Sie alle anderen Verbindungen zulassen, auf die Ihr Server reagieren soll.
Die Verbindungen, die Sie zulassen sollten, sind von Ihren spezifischen Bedürfnissen abhängig.
Glücklicherweise wissen Sie bereits, wie Sie Regeln schreiben, die Verbindungen anhand eines Dienstnamens oder Ports zulassen. Das haben wir bereits für SSH an Port 22 getan. Sie können es auch tun für:
HTTP an Port 80, was nicht verschlüsselte Webserver verwenden; mit sudo ufw allow http oder sudo ufw allow 80
HTTPS an Port 443, was verschlüsselte Webserver verwenden; mit sudo ufw allow https oder sudo ufw allow 443
Es gibt weitere Möglichkeiten, um andere Verbindungen zuzulassen, abgesehen von der Angabe eines Ports oder bekannten Dienstes.
Spezifische Portbereiche
Sie können mit UFW spezifische Portbereiche angeben.
Einige Anwendungen verwenden mehrere Ports anstelle eines einzelnen Ports.
Um zum Beispiel X11-Verbindungen zuzulassen, die Ports 6000-6007 verwenden, nutzen Sie diese Befehle:
Wenn Sie mit UFW Portbereiche angeben, müssen Sie das Protokoll (tcp oder udp) angeben, für das die Regeln gelten sollen.
Wir haben das vorher nicht erwähnt, da wir ohne Angabe des Protokolls automatisch beide Protokolle zulassen, was in den meisten Fällen in Ordnung ist.
Spezifische IP-Adressen
Beim Arbeiten mit UFW können Sie auch IP-Adressen spezifizieren.
Wenn Sie zum Beispiel Verbindungen von einer bestimmten IP-Adresse zulassen möchten, wie einer Arbeits- oder privaten IP-Adresse unter 203.0.113.4, müssen Sie from und dann die IP-Adresse angeben:
Sie können auch einen bestimmten Port angeben, mit dem die IP-Adresse eine Verbindung herstellen darf, indem Sie to any port (zu jedem Port) gefolgt von der Portnummer hinzufügen.
Wenn Sie zum Beispiel 203.0.113.4 erlauben möchten, sich mit Port 22 (SSH) zu verbinden, verwenden Sie diesen Befehl:
Subnetze
Wenn Sie ein Subnetz von IP-Adressen zulassen möchten, können Sie CIDR-Notation verwenden, um eine Netzmaske anzugeben.
Wenn Sie zum Beispiel alle IP-Adressen im Bereich von 203.0.113.1 bis 203.0.113.254 zulassen möchten, können Sie diesen Befehl verwenden:
Außerdem können Sie auch den Zielport angeben, mit dem das Subnetz 203.0.113.0 / 24 eine Verbindung herstellen darf.
Auch hier verwenden wir Port 22 (SSH) als Beispiel:
Verbindungen zu einer spezifischen Netzwerkschnittstelle
Wenn Sie eine Firewall-Regel erstellen möchten, die nur für eine bestimmte Netzwerkschnittstelle gilt, können Sie dazu "allow in on" gefolgt vom Namen der Netzwerkschnittstelle angeben.
Sie möchten möglicherweise Ihre Netzwerkschnittstellen überprüfen, bevor Sie fortfahren.
Die hervorgehobene Ausgabe gibt die Namen der Netzwerkschnittstellen an.
Sie haben typischerweise Namen wie eth0 oder enp3s2.
Wenn Ihr Server eine öffentliche Netzwerkschnittstelle namens eth0 hat, könnten Sie HTTP-Verkehr (Port 80) dorthin mit diesem Befehl zulassen:
Dadurch würden Sie zulassen, dass Ihr Server HTTP-Anfragen aus dem öffentlichen Internet empfängt.
Oder wenn Sie möchten, dass Ihr MySQL-Datenbankserver (Port 3306) an der privaten Netzwerkschnittstelle eth1 nach Verbindungen lauschen soll, können Sie diesen Befehl verwenden:
Dadurch dürften andere Server in Ihrem privaten Netzwerk eine Verbindung mit Ihrer MySQL-Datenbank herstellen.
Schritt 6 - Ablehnen von Verbindungen
Wenn Sie die Standardrichtlinie für eingehende Verbindungen nicht geändert haben, ist UFW so konfiguriert, dass alle eingehenden Verbindungen abgelehnt werden.
Das vereinfacht im Allgemeinen das Erstellen einer sicheren Firewall-Richtlinie, da Sie Regeln erstellen müssen, die bestimmte Ports und IP-Adressen explizit zulassen.
Manchmal werden Sie jedoch einzelne Verbindungen auf Grundlage der Quell-IP-Adresse oder des Subnetzes ablehnen wollen, vielleicht weil Sie wissen, dass Ihr Server von dort angegriffen wird.
Wenn Sie Ihre Richtlinie für eingehenden Datenverkehr in allow ändern möchten (was nicht empfohlen wird), müssten Sie für alle Dienste oder IP-Adressen, bei denen Sie keine Verbindung zulassen wollen, deny-Regeln erstellen.
Um deny-Regeln zu schreiben, können Sie die oben beschriebenen Befehle verwenden und allow durch deny ersetzen.
Um zum Beispiel HTTP-Verbindungen abzulehnen, können Sie diesen Befehl verwenden:
Oder wenn Sie alle Verbindungen von 203.0.113.4 ablehnen möchten, können Sie diesen Befehl verwenden:
Jetzt werfen wir einen Blick auf das Löschen von Regeln.
Schritt 7 - Löschen von Regeln
Zu wissen, wie man Firewall-Regeln löscht, ist genauso wichtig wie zu wissen, wie man sie erstellt.
Es gibt zwei Wege, um anzugeben, welche Regeln gelöscht werden sollen: anhand der Regelnummer oder der tatsächlichen Regel (ähnlich wie beim Angeben der Regeln im Rahmen der Erstellung).
Wir beginnen mit der Methode Löschen anhand von Regelnummer, da sie einfacher ist.
Nach Regelnummer
Wenn Sie die Regelnummer verwenden, um Firewall-Regeln zu löschen, wird eine Liste Ihrer Firewall-Regeln angezeigt.
Der UFW-Statusbefehl hat eine Option, um neben jeder Regel eine Nummer anzuzeigen, wie hier gezeigt:
Wenn wir entscheiden, dass wir Regel 2, die Verbindungen an Port 80 (HTTP) zulässt, löschen möchten, können wir sie in einem UFW-Befehl wie diesem angeben:
Dadurch würde eine Bestätigungsaufforderung angezeigt und Regel 2, die HTTP-Verbindungen zulässt, dann gelöscht.
Beachten Sie, dass Sie bei aktiviertem IPv6 wahrscheinlich auch die entsprechende IPv6-Regel löschen möchten.
Nach tatsächlicher Regel
Die Alternative zu Regelnummern besteht darin, die tatsächlich zu löschende Regel anzugeben.
Wenn Sie zum Beispiel die Regel allow http entfernen möchten, können Sie das wie folgt schreiben:
Sie könnten die Regel auch anhand von allow 80 anstelle des Dienstnamens angeben:
Diese Methode löscht sowohl IPv4- als auch IPv6-Regeln, falls vorhanden.
Schritt 8 - Prüfen von UFW-Status und -Regeln
Sie können den Status von UFW mit diesem Befehl jederzeit überprüfen:
Wenn UFW deaktiviert ist, was standardmäßig der Fall ist, sehen Sie in etwa Folgendes:
Wenn UFW aktiv ist, was der Fall sein sollte, wenn Sie Schritt 3 ausgeführt haben, teilt die Ausgabe mit, dass UFW aktiv ist; zudem werden alle festgelegten Regeln aufgelistet. Wenn Sie die Firewall beispielsweise so einrichten, dass SSH (Port 22) -Verbindungen überall zugelassen werden, könnte die Ausgabe ungefähr wie folgt aussehen:
Verwenden Sie den Befehl status, um zu prüfen, wie UFW die Firewall konfiguriert hat.
Schritt 9 - Aktivieren oder Zurücksetzen von UFW (optional)
Wenn Sie entscheiden, dass Sie UFW nicht mehr verwenden möchten, können Sie die Firewall mit diesem Befehl deaktivieren:
Alle Regeln, die Sie mit UFW erstellt haben, sind dann nicht mehr aktiv.
Sie können später jederzeit sudo ufw enable nutzen, um sie wieder zu aktivieren.
Wenn Sie bereits UFW-Regeln konfiguriert haben, aber lieber neu anfangen möchten, können Sie den Befehl reset verwenden:
Dadurch wird UFW deaktiviert und alle Regeln, die zuvor definiert wurden, werden gelöscht.
Beachten Sie, dass die Standardrichtlinien nicht zu ihren ursprünglichen Einstellungen zurückkehren, wenn Sie sie irgendwann geändert haben.
Jetzt sollten Sie mit UFW neu anfangen können.
Ihre Firewall ist jetzt so konfiguriert, dass (mindestens) SSH-Verbindungen zugelassen werden.
Achten Sie darauf, alle anderen eingehenden Verbindungen, die Ihr Server benötigt, zuzulassen und beschränken Sie alle unnötigen Verbindungen, sodass Ihr Server gleichzeitig funktional und sicher ist.
Um mehr über gängige UFW-Konfigurationen zu erfahren, lesen Sie das Tutorial UFW-Grundlagen: Gängige Firewall-Regeln und -Befehle.
Schritt 1 - Installieren der erforderlichen Softwarepakete und Konfigurieren der Firewall
Nachdem Sie die Pakete installiert haben, müssen wir die Firewall so einstellen, dass sie Traffic gestattet, der auf unserem Minecraft-Server eingeht.
Bei der Ersteinrichtung des Servers haben Sie nur den Traffic von SSH zugelassen.
Nun müssen Sie auch den Traffic zulassen, der über Port 25565 eingeht. Das ist der Standardport, den Minecraft für die Zulassung von Verbindungen verwendet.
Fügen Sie die erforderliche Firewall-Regel hinzu, indem Sie den folgenden Befehl ausführen:
Nachdem Sie nun Java installiert und Ihre Firewall richtig konfiguriert haben, laden Sie den Minecraft-Server von der Minecraft-Website herunter.
So erstellen Sie einen Minecraft-Server unter Ubuntu 20.04
5502
Einen Server mit einer Neuinstallation von Ubuntu 20.04, einen Nicht-Root-Benutzer mit Sudo-Berechtigungen und aktiviertem SSH.
Sie haben jetzt einen Minecraft-Server unter Ubuntu 20.04, auf dem Sie und alle Ihre Freunde spielen können!
So installieren und konfigurieren Sie VNC unter Ubuntu 20.04 Quickstart
5510
In dieser Kurzanleitung erfahren Sie, wie Sie einen VNC-Server mit TightVNC auf einem Ubuntu 20.04-Server einrichten und ihn sicher über einen SSH-Tunnel verbinden.
Anschließend verwenden Sie ein VNC-Clientprogramm auf Ihrem lokalen Computer, um mit Ihrem Server über eine grafische Desktop-Umgebung zu interagieren.
Einen lokalen Computer mit einem installierten VNC-Client.
Der VNC-Client, den Sie verwenden, muss Verbindungen über SSH-Tunnel unterstützen:
Nachdem Ihr Server mit SSH verbunden ist, aktualisieren Sie Ihre Paketliste:
Installieren Sie dann Xfce zusammen mit dem xfce4-goodies-Paket, das einige Verbesserungen für die Desktop-Umgebung enthält:
Führen Sie als Nächstes den Befehl vncpasswd aus, um ein Zugangspasswort für VNC festzulegen, und erstellen Sie die Dateien für die Anfangskonfiguration:
Das Passwort muss aus sechs bis acht Zeichen bestehen; Passwörter mit mehr als 8 Zeichen werden automatisch gekürzt.
Wenn Sie das Passwort überprüfen, haben Sie die Option, ein Passwort für den reinen Lesezugriff zu erstellen. Dies ist jedoch nicht zwingend erforderlich.
Wenn Sie Ihr Passwort ändern möchten oder ein Passwort für den reinen Lesezugriff hinzufügen möchten, führen Sie den Befehl vncpasswd erneut aus.
Die Befehle, die der VNC-Server beim Starten ausführt, befinden sich in einer Konfigurationsdatei namens xstartup im Ordner .vnc unter Ihrem Stammverzeichnis.
In diesem Schritt erstellen wir ein benutzerdefiniertes xstartup-Skript, das den VNC-Server anweist, sich mit dem Xfce-Desktop zu verbinden.
Erstellen Sie eine neue xstartup-Datei und öffnen Sie sie in einem Texteditor wie nano:
Fügen Sie der neuen Datei folgende Zeilen hinzu:
Hinter dem sheband weist der erste Befehl in der Datei xrdb $HOME /.
Xresources-Datei des Serverbenutzers zu lesen.
Der zweite Befehl weist den Server an, Xfce zu starten.
Machen Sie die Datei anschließend ausführbar:
Und starten Sie den VNC-Server mit dem Befehl vncserver:
Dieser Befehl beinhaltet die Option -localhost, die den VNC-Server an die Loopback-Schnittstelle Ihres Servers bindet.
Dadurch kann VNC nur Verbindungen zulassen, die von dem Server stammen, auf dem es installiert ist.
Hier können Sie sehen, dass der Befehl eine standardmäßige Serverinstanz auf Port 5901 startet.
Schritt 3 - Sichere Verbindung mit dem VNC-Desktop
Um sich sicher mit Ihrem Server zu verbinden, richten Sie einen SSH-Tunnel ein und weisen Ihren VNC-Client an, diesen Tunnel zu verwenden, anstatt eine direkte Verbindung herzustellen.
Sie können dies mit folgendem ssh-Befehl über den Terminal auf Linux oder macOS durchführen:
Der lokale Port kann jeder Port sein, der von einem anderen Programm oder Prozess nicht bereits blockiert wurde. Wir verwenden in diesem Beispiel jedoch < ^ > 59000 < ^ >.
Stellen Sie zudem sicher, < ^ > sammy < ^ > in Ihren Ubuntu-Benutzernamen und < ^ > your _ server _ ip < ^ > der IP-Adresse Ihres Servers entsprechend zu ändern.
Wenn Sie PuTTY verwenden, um sich mit Ihrem Server zu verbinden, können Sie einen SSH-Tunnel erstellen, indem Sie mit der rechten Maustaste auf die obere Leiste des Terminalfensters und anschließend auf die Option Change Setting... (Einstellungen ändern) klicken:
Klicken Sie mit der rechten Maustaste auf die oberen Leiste, um die Option Change Settings (Einstellungen ändern) einzublenden
Suchen Sie den Verbindungszweig im Menübaum auf der linken Seite des Fensters PuTTY Reconfigure (PuTTY neu konfigurieren).
Erweitern Sie den SSH-Zweig und klicken Sie auf Tunnels.
Geben Sie auf dem Bildschirm Options controlling SSH port forwarding (Optionen für die Steuerung der SSH-Portweiterleitung) 59000 als Quell-Port und localhost: 5901 als Destination an (wie in diesem Fall):
Beispiel PuTTY SSH-Tunnelkonfiguration
Klicken Sie anschließend auf die Schaltfläche Add (Hinzufügen) und danach auf die Schaltfläche Apply (Anwenden), um den Tunnel zu implementieren.
Sobald der Tunnel läuft, verwenden Sie einen VCN Client, um sich mit localhost: 59000 zu verbinden.
VNC-Verbindung zum Ubuntu 20.04-Server mit der Xfce-Desktop-Umgebung
File Manager über VNC-Verbindung zu Ubuntu 20.04
Drücken Sie STRG + C auf Ihrem lokalen Terminal, um den SSH-Tunnel zu stoppen und zu Ihrer Aufforderung zurückzukehren.
Dadurch, dass der VNC-Server so eingerichtet wurde, dass er als systemd-Dienst ausgeführt wird, können Sie die systemd-Verwaltungsbefehle Start, Stop und Restart the server (Starten, Anhalten und Neustarten des Servers) nutzen. Zudem können Sie ihn so einstellen, dass er immer dann startet, wenn der Server hochgefahren wird.
Erstellen Sie zunächst für die systemd-Einheit eine neue Datei namens / etc / system / vncserver @ .service:
Das @ -Symbol am Ende des Namens gestattet uns, ein Argument aufzunehmen, das Sie in der Dienstkonfiguration verwenden können.
Sie verwenden dies, um den VCN-Anzeige-Port anzugeben, den Sie bei der Dienstverwaltung einsetzen möchten.
Fügen Sie der Datei folgende Zeilen hinzu und stellen Sie sicher, dass der Wert für User, Group und WorkingDirectory geändert wird und der Benutzername im Wert von PIDFILE Ihrem Benutzernamen entspricht:
Machen Sie als Nächstes das System auf die neue Unit-Datei aufmerksam:
Aktivieren Sie die Unit-Datei:
Die 1 nach dem @ -Zeichen gibt an, über welcher Anzeigenummer der Dienst angezeigt werden soll; in diesem Fall ist der Standard: 1, wie wir bereits in Schritt 2 besprochen haben.
Stoppen Sie die aktuelle Instanz des VNC-Servers, wenn er noch läuft:
Starten Sie ihn dann wie jeden anderen systemd-Dienst:
In unserem Tutorial How To Use Systemctl to Manage Systemd Services and Units (So nutzen Sie Systemctl für die Verwaltung von Systemd-Diensten und -Units) finden Sie für weitere Informationen zu systemctl.
Um Ihren SSH-Tunnel erneut zu verbinden, starten Sie ihn erneut:
Stellen Sie dann mit Ihrer VCN-Client-Software eine neue Verbindung zum localhost: 59000 her, um sich mit Ihrem Server zu verbinden.
Jetzt läuft ein sicherer VCN Server auf Ihrem Ubuntu 20.04 Server.
Jetzt können Sie Ihre Dateien, Software und Einstellungen mit einer benutzerfreundlichen grafischen Oberfläche verwalten und grafische Software wie Web-Browser per Fernzugriff ausführen.
So installieren und konfigurieren Sie VNC unter Ubuntu 20.04
5477
Um dies einzurichten, folgen Sie unserem Leitfaden für die Ersteinrichtung des Servers für Ubuntu 20.04.
Standardmäßig wird ein Ubuntu 20.04 Server nicht mit einer grafischen Desktop-Umgebung oder einem installierten VNC-Server geliefert. Daher beginnen Sie mit deren Installation.
Bei der Wahl des VNC-Servers und der Desktop-Umgebung haben Sie viele Optionen, aus denen Sie auswählen können.
In diesem Tutorial installieren Sie Pakete für die aktuellste Xfce-Desktop-Umgebung und das TightVNC-Paket, das im offiziellen Ubuntu Repository verfügbar ist.
Sowohl Xfce als auch TightVNC sind für ihre leichte und schnelle Ausführung bekannt. Dadurch wird sichergestellt, dass die VNC-Verbindung reibungslos funktioniert und auch bei langsameren Internetverbindungen stabil bleibt.
Installieren Sie nun Xfce zusammen mit dem xfce4-goodies-Paket, das einige Verbesserungen für die Desktop-Umgebung enthält:
Während der Installation werden Sie möglicherweise dazu aufgefordert, einen standardmäßigen Display-Manager für Xfce auszuwählen.
Ein Display-Manager ist ein Programm, mit dem Sie sich über eine grafische Oberfläche in einer Desktop-Umgebung einloggen und Vorgaben auswählen können.
Sie verwenden Xfce nur, wenn Sie sich mit einem VNC-Client verbinden. In diesen Xfce-Sitzungen sind Sie bereits als Nicht-Root-Benutzer für Ubuntu angemeldet.
Für die Zwecke dieses Tutorials ist Ihre Auswahl des Display-Managers nicht relevant.
Wählen Sie entweder einen aus oder drücken Sie ENTER.
Führen Sie als Nächstes den Befehl vncserver aus, um ein VNC-Zugangspasswort festzulegen, die anfänglichen Konfigurationsdateien zu erstellen und eine VNC-Serverinstanz zu starten:
Zudem startet er eine Standard-Serverinstanz auf Port 5901.
VNC kann mehrere Instanzen auf anderen Anzeige-Ports starten.: 2 bezieht sich auf Port 5902,: 3 auf 5903 und so weiter:
Wenn Sie Ihr Passwort ändern oder ein schreibgeschütztes Passwort hinzufügen möchten, beachten Sie, dass Sie dies mit dem Befehl vncpasswd tun können:
Das ist der Moment, ab dem der VNC-Server installiert ist und läuft.
Nun konfigurieren wir ihn, um Xfce zu starten und um uns Zugriff auf den Server über eine grafische Oberfläche zu erteilen.
VNC muss vor allem wissen, mit welcher grafischen Desktop-Umgebung er sich verbinden soll.
Das Start-Skript wurde erstellt, als Sie den vncserver im vorherigen Schritt ausgeführt haben, aber Sie erstellen Ihr eigenes, um den Xfce-Desktop zu starten.
Da Sie die Konfiguration des VNC-Servers ändern werden, müssen Sie zunächst die VNC-Server-Instanz stoppen, die auf Port 5901 ausgeführt wird. Dazu verwenden Sie den folgenden Befehl:
Fügen Sie der Datei anschließend folgende Zeilen hinzu:
Die erste Zeile ist ein shebang.
In ausführbaren Klartext-Dateien auf * nix-Plattformen weist ein shebang das System an, an welchen Interpreter die Datei zur Ausführung weitergegeben wird.
In diesem Fall übergeben Sie die Datei dem Bash-Interpreter.
Dies erlaubt es, jede aufeinanderfolgende Zeile als Befehl der Reihe nach auszuführen.
Immer, wenn Sie den VNC-Server starten oder neu starten, werden diese Befehle automatisch ausgeführt.
Um sicherzustellen, dass der VNC-Server diese neue Start-Datei ordnungsgemäß verwenden kann, müssen Sie sie ausführbar machen:
Starten Sie den VNC-Server anschließend neu:
Achten Sie dieses Mal darauf, dass dieser Befehl die Option -localhost beinhaltet, die den VNC-Server an die Loopback-Schnittstelle Ihres Servers bindet.
Im nächsten Schritt richten Sie einen SSH-Tunnel zwischen Ihrem lokalen Rechner und Ihrem Server ein und gaukeln VNC damit im Grunde vor, dass die Verbindung von Ihrem lokalen Rechner von Ihrem Server stammt.
Diese Strategie fügt eine zusätzliche Sicherheitsebene um VNC herum hinzu, da die einzigen Benutzer, die darauf zugreifen können, bereits SSH-Zugriff auf Ihren Server haben.
Da die Konfiguration nun betriebsbereit ist, können Sie jetzt die Verbindung von Ihrem lokalen Rechner zum VNC-Server herstellen.
Im Folgenden wird die Bedeutung dieses ssh-Befehls erklärt:
-L < ^ > 59000 < ^ >: localhost: < ^ > 5901 < ^ >: Der -L-Schalter gibt an, dass der angegebene Port auf dem lokalen Computer (59000) an den angegebenen Host und den Port auf dem Zielserver weitergeleitet werden soll (localhost: 5901, bedeutet Port 5901 auf dem Zielserver, definiert als < ^ > your _ server _ ip < ^ >).
Beachten Sie, dass der von Ihnen angegebene lokale Port etwas willkürlich ist. Solange der Port nicht bereits in einen anderen Dienst eingebunden ist, können Sie ihn als Weiterleitungsport für Ihren Tunnel verwenden.
-C: Dieses Flag ermöglicht eine Komprimierung, mit der die Ressourcennutzung minimiert und die Prozesse beschleunigt werden können.
-N: Diese Option weist ssh an, dass Sie keine Remote-Befehle ausführen möchten.
Diese Einstellung ist hilfreich, wenn Sie Ports lediglich weiterleiten möchten.
-l < ^ > sammy < ^ > < ^ > your _ server _ ip < ^ >: Mit dem -l-Schalter legen Sie den Benutzer fest, mit dem Sie sich anmelden wollen, sobald Sie sich mit dem Server verbinden.
Stellen Sie sicher, dass Sie < ^ > sammy < ^ > und < ^ > your _ server _ ip < ^ > durch den Namen Ihres Nicht-root-Benutzers und der IP-Adresse Ihres Servers ersetzen.
< $> note Hinweis: Dieser Befehl richtet einen SSH-Tunnel ein, der Informationen von Port 5901 auf Ihrem VNC-Server an Port 59000 auf Ihrem lokalen Rechner über Port 22 auf jedem Rechner (den Standardport für SSH) weiterleitet.
Wenn Sie dem Leitfaden für Ersteinrichtung eines Servers für Ubuntu 20.04 gefolgt sind, der als Voraussetzung gilt, haben Sie eine UFW-Regel hinzugefügt, die Verbindungen zu Ihrem Server über OpenSSH erlaubt.
Dies ist sicherer als einfach die Firewall Ihres Servers für Verbindungen zu Port 5901 zu öffnen. Denn dadurch würde jedermann über VNC auf Ihren Server zugreifen können.
Durch das Verbinden eines SSH-Tunnels begrenzen Sie den VNC-Zugriff auf Computer, die bereits SSH-Zugriff auf den Server haben.
Jetzt können Sie Ihren VNC-Server so konfigurieren, dass er als systemd-Dienst ausgeführt wird.
Da der VNC-Server für die Ausführung als systemd-Dienst eingerichtet wurde, können Sie ihn wie jeden anderen Dienst nach Bedarf starten, anhalten und neu starten.
Sie können auch die Verwaltungsbefehle von systemd nutzen, um sicherzustellen, dass VNC beim Hochfahren Ihres Servers startet.
Erstellen Sie zunächst eine Unit-Datei namens / etc / systemd / system / vncserver @ .service:
Beachten Sie auch, dass der Befehl ExecStart wieder die Option -localhost beinhaltet.
Ihr VNC-Server ist jetzt betriebsbereit, sobald Ihr Server hochfährt, und Sie können ihn mit systemctl-Befehlen wie jeden anderen systemd-Dienst verwalten.
Es gibt jedoch keinen Unterschied auf der Client-Seite.
Stellen Sie dann mit Ihrer VNC-Client-Software eine neue Verbindung zu localhost: 59000 her, um sich mit Ihrem Server zu verbinden.
Wir erstellen abschließend einen administrativen Benutzer, damit Sie die Django admin-Schnittstelle verwenden können.
Wir wollen dies mit dem Befehl createsuperuser tun:
Sie werden aufgefordert, einen Benutzernamen, eine E-Mail-Adresse und ein Passwort für Ihren Benutzer zu wählen.
So installieren Sie Linux, Nginx, MySQL und PHP (LEMP-Stack) unter Ubuntu 20.04 Schnellstart
5608
In diesem Schnellstartleitfaden installieren wir einen LEMP-Stack auf einem Ubuntu 20.04-Server.
Eine detailliertere Version dieses Tutorials mit weiteren Erklärungen zu einzelnen Schritten finden Sie unter Installieren des Linux-, Nginx-, MySQL-, PHP- (LEMP-) Stacks unter Ubuntu 20.04.
Aktualisieren Sie den Cache Ihres Paketmanagers und installieren Sie dann Nginx mit:
Konsultieren Sie den Schritt 6 unseres detaillierten Leitfadens für LEMP unter Ubuntu 20.04, um mehr zu erfahren.
Schritt 4 - Konfigurieren von Nginx für PHP
Schritt 5 - Testen von PHP mit Nginx
Wir erstellen jetzt ein PHP-Testskript, um zu prüfen, ob Nginx Anfragen für PHP-Dateien bearbeiten kann.
Installieren von MariaDB unter Ubuntu 20.04
5512
In diesem Tutorial erfahren Sie, wie Sie MariaDB auf einem Ubuntu 20.04-Server installieren und überprüfen können, ob das System ausgeführt wird und über eine sichere Erstkonfiguration verfügt.
Zum Zeitpunkt dieses Schreibens enthalten die Standard-APT-Repositorys von Ubuntu 20.04 die MariaDB-Version < ^ > 10.3 < ^ >.
Später behandeln wir die Frage, wie wir optional ein zusätzliches Administratorkonto für Passwortzugriff einrichten können, wenn die Socket-Authentifizierung für Ihren Anwendungsfall nicht geeignet ist.
Damit haben Sie die anfängliche Sicherheitskonfiguration von MariaDB abgeschlossen.
Der nächste Schritt ist optional. Sie sollten ihn jedoch befolgen, wenn Sie sich bei Ihrem MariaDB-Server mit einem Passwort authentifizieren möchten.
Dazu erstellen wir ein neues Konto namens admin mit den gleichen Fähigkeiten wie beim root-Konto, aber konfiguriert für die Passwortauthentifizierung.
Öffnen Sie die Eingabeaufforderung von MariaDB von Ihrem Terminal:
Dieser Befehl beispielsweise gibt an, über die Unix-Socket eine Verbindung mit MariaDB als root herzustellen und diese Version zurückzugeben:
Sie hatten die Möglichkeit, vor dem Testen der Funktionalität des MariaDB-Servers einen neuen administrativen Benutzer zu erstellen, der die Passwortauthentifizierung verwendet.
Installieren von Webmin unter Ubuntu 20.04
5506
Webmin ist ein modernes Websteuerungspanel, mit dem Sie Ihren Linux-Server über eine browserbasierte Benutzeroberfläche administrieren können.
Mit Webmin können Sie Benutzerkonten verwalten, DNS-Einstellungen konfigurieren und Einstellungen für gebräuchliche Pakete leicht und schnell ändern.
In diesem Tutorial installieren und konfigurieren Sie Webmin auf Ihrem Server und sichern den Zugriff auf die Benutzeroberfläche mit einem gültigen Zertifikat von Let 's Encrypt.
Anschließend verwenden Sie Webmin, um neue Benutzerkonten hinzuzufügen, und aktualisieren alle Pakete auf Ihrem Server über das Dashboard.
Auf diesem Server sollten ein Nicht-root Benutzers mit sudo-Berechtigungen und eine UFW-Firewall konfiguriert sein.
Apache, installiert gemäß unserem Tutorial Installieren des Apache-Webservers unter Ubuntu 20.04.
Achten Sie darauf, einen virtuellen Host zu konfigurieren, während Sie dieser Anleitung zu den Voraussetzungen folgen.
Einen vollqualifizierten Domänennamen (FQDN) mit einem DNS A-Eintrag, der auf die IP-Adresse Ihres Servers verweist.
Um dies zu konfigurieren, befolgen Sie diese Anweisungen zum DNS-Hosting auf DigitalOcean.
Schritt 1 - Installieren von Webmin
Zuerst müssen wir das Webmin-Repository hinzufügen, damit wir Webmin mithilfe unseres Paketmanagers installieren und aktualisieren können.
Dazu fügen wir das Repository der Datei / etc / apt / sources.list hinzu.
Öffnen Sie die Datei in Ihrem bevorzugten Editor.
Fügen Sie dann diese Zeile am Ende der Datei ein, um das neue Repository hinzuzufügen:
Wenn Sie nano verwendet haben, drücken Sie hierzu STRG + X, Y, und dann ENTER.
Als Nächstes fügen Sie den PGP-Schlüssel von Webmin hinzu, damit Ihr System dem neuen Repository vertraut.
Hierzu müssen Sie das Paket gnupg1 installieren, das Tool von GNU für sichere Kommunikation und Datenspeicherung.
Aktualisieren Sie den Paketindex Ihres Servers, falls Sie das nicht vor Kurzem getan haben:
Laden Sie anschließend den PGP-Schlüssel von Webmin mit wget herunter und fügen Sie ihn der Liste der Schlüssel Ihres Systems hinzu:
Aktualisieren Sie als Nächstes erneut die Liste von Paketen, um das jetzt vertrauenswürdige Webmin-Repository mit einzubeziehen:
Installieren Sie nun Webmin:
Sobald die Installation abgeschlossen ist, wird Ihnen die folgende Ausgabe angezeigt:
< $> note Anmerkung: Wenn Sie im Schritt Voraussetzungen ufw installiert und aktiviert haben, müssen Sie den folgenden Befehl ausführen, damit Webmin die Firewall passieren kann:
Für eine erhöhte Sicherheit sollten Sie Ihre Firewall so konfigurieren, dass der Zugriff auf diesen Port nur von bestimmten IP-Bereichen aus möglich ist.
Stellen wir den Zugriff auf Webmin sicher, indem wir ein gültiges Zertifikat hinzufügen.
Schritt 2 - Hinzufügen eines gültigen Zertifikats mit Let 's Encrypt
Webmin ist bereits für die Verwendung von HTTPS konfiguriert, benutzt jedoch ein selbstsigniertes, nicht vertrauenswürdiges Zertifikat.
Wir ersetzen es durch ein gültiges Zertifikat von Let 's Encrypt.
Navigieren Sie in Ihrem Webbrowser zu https: / / < ^ > your _ domain < ^ >: 10000, wobei Sie < ^ > your _ domain < ^ > durch den Domänennamen ersetzen, der auf die IP-Adresse Ihres Servers verweist.
< $> note Anmerkung: Wenn Sie sich zum ersten Mal einloggen, wird Ihnen die Warnung "Ungültige SSL" angezeigt.
Diese Warnung kann je nach Browser etwas anders lauten, sagt aber jeweils aus, dass der Server ein selbstsigniertes Zertifikat generiert hat.
Erlauben Sie die Ausnahme und gehen Sie weiter zu Ihrer Domäne, damit Sie das selbstsignierte Zertifikat durch eines von Let 's Encrypt ersetzen können.
Nun wird Ihnen ein Anmeldebildschirm angezeigt.
Melden Sie sich mit dem Nicht-root Benutzer an, den Sie beim Erfüllen der Voraussetzungen für dieses Tutorial erstellt haben.
Nach dem Einloggen ist der erste Bildschirm, den Sie sehen, das Webmin-Dashboard.
Bevor Sie ein gültiges Zertifikat anwenden können, müssen Sie den Hostnamen des Servers festlegen.
Suchen Sie nach dem Feld System hostname und klicken Sie auf den Link rechts, wie in der folgenden Abbildung gezeigt:
Abbildung mit dem Hinweis, wo sich der Link im Webmin-Dashboard befindet
Dadurch gelangen Sie zur Seite Hostname and DNS Client.
Suchen Sie das Feld Hostname und geben Sie Ihren vollqualifizierten Domänennamen ein.
Klicken Sie auf die Schaltfläche Save am Ende der Seite, um die Einstellung anzuwenden.
Nachdem Sie Ihren Hostnamen festgelegt haben, klicken Sie in der linken Navigationsleiste auf das Auswahlmenü von Webmin und anschließend auf Webmin Configuration.
Wählen Sie auf der Seite Webmin Configuration aus der Liste der Symbole SSL Encryption aus und klicken Sie anschließend auf die Registerkarte von Let 's Encrypt. Sie sehen dann einen Bildschirm wie in der folgenden Abbildung:
Abbildung der Let 's-Encrypt-Registerkarte des SSL-Encryption-Abschnitts
Auf dieser Seite legen Sie fest, wie Webmin Ihr Zertifikat erlangt und erneuert.
Let 's-Encrypt-Zertifikate erlöschen nach 3 Monaten. Sie können Webmin jedoch anweisen, jeden Monat automatisch zu versuchen, das Let' s-Encrypt-Zertifikat zu erneuern.
Let 's Encrypt sucht nach einer Verifizierungsdatei auf dem Server, also konfigurieren wir Webmin so, dass die Verifizierungsdatei im Ordner / var / www / < ^ > your _ domain < ^ > abgelegt wird. Das ist der Ordner, den der Apache-Webserver, den Sie in den Voraussetzungen konfiguriert haben, verwendet.
Folgen Sie diesen Schritten, um Ihr Zertifikat einzurichten:
Geben Sie bei Hostnames for certificate Ihre FQDN ein.
Wählen Sie bei Website root directory for validation file die Schaltfläche Other Directory aus und geben Sie das Dokumentverzeichnis Ihrer Website ein.
Wenn Sie dem Tutorial über Apache in den Voraussetzungen gefolgt sind, ist das / var / www / < ^ > your _ domain < ^ >.
Heben Sie im Bereich Months between automatic renewal die Option Only renew manually auf, indem Sie 1 in das Eingabefeld eingeben und den Auswahlknopf links vom Eingabefeld auswählen.
Klicken Sie auf die Schaltfläche Request Certificate.
Nach ein paar Sekunden sehen Sie einen Bestätigungsbildschirm.
Um das neue Zertifikat zu verwenden, klicken Sie auf die Schaltfläche Return to Webmin configuration auf dem Bestätigungsbildschirm.
Scrollen Sie auf dieser Seite nach unten und klicken Sie auf die Schaltfläche Restart Webmin.
Warten Sie etwa 30 Sekunden, laden Sie die Seite erneut und loggen Sie sich ein.
Ihr Browser sollte nun angeben, dass das Zertifikat gültig ist.
Schritt 3 - Verwenden von Webmin
Nun haben Sie eine gesicherte Arbeitsinstanz von Webmin eingerichtet.
Nun sehen wir uns an, wie wir diese verwenden können.
Webmin verfügt über viele verschiedene Module, die alles vom BIND-DNS-Server bis zum Hinzufügen von Benutzern zum System kontrollieren können.
Sehen wir uns an, wie man einen neuen Benutzer einrichtet, und erörtern wir anschließend, wie Sie die Pakete Ihres Systems mit Webmin aktualisieren.
Verwalten von Benutzern und Gruppen
Wir behandeln nun, wie Sie die Benutzer und Gruppen auf Ihrem Server verwalten.
Klicken Sie zunächst auf das Auswahlmenü von System in der linken Seitenleiste und anschließend auf den Link für Users and Groups.
Hier können Sie Benutzer und Gruppen hinzufügen und verwalten.
Wir erstellen einen neuen Benutzer namens deploy, den Sie zum Hosten von Webanwendungen verwenden können.
Wenn Sie einen Benutzer anlegen, können Sie Optionen für den Ablauf des Passworts und die Shell des Benutzers festlegen sowie bestimmen, ob ein Stammverzeichnis für ihn zugelassen wird.
Um einen Benutzer hinzuzufügen, klicken Sie auf Create a new user im oberen Bereich der Benutzertabelle.
Dadurch wird der Bildschirm Create User angezeigt, auf dem Sie den Benutzernamen, das Passwort, Gruppen und andere Optionen festlegen können.
Folgen Sie diesen Anweisungen zur Erstellung des Benutzers:
Geben Sie bei Username als Benutzernamen deploy ein.
Wählen Sie Automatic für User ID.
Geben Sie bei Real Name einen beschreibenden Namen wie Deployment user ein.
Für Home Directory wählen Sie Automatic.
Für Shell wählen Sie / bin / bash aus der Auswahlliste.
Für Password wählen Sie Normal Password und geben ein Passwort Ihrer Wahl ein.
Gehen Sie weiter nach unten zu Primary Group und wählen Sie New group with same name as user aus.
Für Secondary Group wählen Sie sudo aus der Liste All groups.
Das sollte automatisch der Liste In groups hinzugefügt werden. Falls nicht, klicken Sie zum Hinzufügen auf die Schaltfläche - >.
Nachdem die Auswahl vorgenommen wurde, klicken Sie auf Create.
Dadurch wird der Benutzer deploy in kurzer Zeit erstellt.
Als Nächstes sehen wir uns an, wie wir Aktualisierungen auf dem System installieren.
Aktualisieren von Paketen
Mit Webmin können Sie alle Pakete über die Benutzeroberfläche aktualisieren.
Um alle Pakete zu aktualisieren, klicken Sie zunächst auf die Schaltfläche Dashboard über der linken Seitenleiste und suchen dann das Feld Package updates.
Wenn Aktualisierungen verfügbar sind, sehen Sie einen Link, der die Anzahl der verfügbaren Aktualisierungen angibt.
Klicken Sie auf diesen Link und anschließend auf Update selected packages um die Aktualisierung zu starten.
Möglicherweise werden Sie aufgefordert, den Server neu zu starten. Auch das können Sie über die Benutzeroberfläche von Webmin erledigen.
Sie verfügen nun über eine gesicherte Arbeitsinstanz von Webmin und haben die Benutzeroberfläche zum Erstellen eines Benutzers und zum Aktualisieren von Paketen verwendet.
Webmin gibt Ihnen Zugriff auf viele Dinge, auf die Sie normalerweise über die Konsole zugreifen müssten, und organisiert diese auf intuitive Weise.
Wenn Sie beispielsweise Apache installiert haben, finden Sie dessen Konfigurationsregister unter Servers und dann Apache.
Erkunden Sie die Benutzeroberfläche oder lesen Sie das offizielle Webmin-Wiki, um mehr über die Verwaltung Ihres Systems mit Webmin zu erfahren.
5501
Python 3 verfügt über eine Reihe von integrierten Datenstrukturen, einschließlich Tupel, Wörterbücher und Listen.
Datenstrukturen bieten uns eine Möglichkeit, Daten zu organisieren und zu speichern. Das collections-Modul hilft uns, Datenstrukturen effizient zu füllen und zu manipulieren.
In diesem Tutorial gehen wir drei Klassen im collections-Modul durch, um Ihnen die Arbeit mit Tupeln, Wörterbüchern und Listen zu erleichtern.
Wir verwenden namedtuples, um Tupel mit benannten Feldern zu erstellen, defaultdict, um Informationen in Wörterbüchern übersichtlich zu gruppieren, und deque, um Elemente effizient zu beiden Seiten eines listenartigen Objekts hinzuzufügen.
In diesem Turorial arbeiten wir in erster Linie mit einem Bestand von Fischen, den wir modifizieren müssen, wenn Fische zu einem fiktiven Aquarium hinzugefügt oder aus diesem entfernt werden.
Um dieses Tutorial optimal zu nutzen, wird empfohlen, sich mit den Tupel-, Wörterbuch- und Listendatentypen vertraut zu machen; sowohl mit deren Syntax als auch mit der Art und Weise, Daten von ihnen abzurufen.
Sie können für die notwendigen Hintergrundinformationen diese Tutorials durchsehen:
Verstehen von Tupeln in Python 3
Verstehen von Wörterbüchern in Python 3
Verstehen von Listen in Python 3
Hinzufügen von benannten Feldern zu Tupeln
Python-Tupeln sind eine unwandelbare oder unveränderliche, geordnete Sequenz von Elementen.
Tupel werden häufig für die Darstellung von Spaltendaten verwendet, beispielsweise für Zeilen aus einer CSV-Datei oder Reihen aus einer SQL-Datenbank.
Ein Aquarium könnte seinen Bestand an Fischen als eine Reihe von Tupeln erfassen.
Ein individueller Fischtupel:
Dieses Tupel besteht aus drei Zeichenfolgenelementen.
Das Tupel ist zwar in gewisser Weise nützlich, aber es gibt nicht klar an, wofür jedes seiner Felder steht.
In Wirklichkeit ist Element 0 ein Name, Element 1 eine Spezies und Element 2 das Haltebecken.
Erläuterung der Fischtupelfelder:
Name
Spezies
Becken
Sammy
shark
tank-a
Diese Tabelle verdeutlicht, dass jedes der drei Elemente des Tupels eine klare Bedeutung hat.
Mit namedtuple aus dem collections-Modul können Sie jedem Element eines Tupels explizite Namen hinzufügen, um diese Bedeutungen in Ihrem Python-Programm klarzustellen.
Wir verwenden namedtuple zum Erstellen einer Klasse, die jedes Element des Fischtupels klar benennt:
from collections import namedtuple gibt Ihrem Python-Programm Zugriff auf die Factoryfunktion namedtuple.
Der Funktionsaufruf namedtuple () gibt eine Klasse zurück, die an den Namen Fish gebunden ist.
Die Funktion namedtuple () hat zwei Argumente: den gewünschten Namen unserer neuen Klasse "Fish" und eine Liste mit benannten Elementen ["name", "species", "tank"].
Wir können die Fish-Klasse verwenden, um das Fischtupel von vorhin zu repräsentieren:
Wenn wir diesen Code ausführen, sehen wir die folgende Ausgabe:
sammy wird mit der Fish-Klasse instanziiert. sammy ist ein Tupel mit drei klar benannten Elementen.
Auf die Felder von sammy kann über ihren Namen oder mit einem traditionellen Tupelindex zugegriffen werden:
Wenn wir diese beiden print-Aufrufe ausführen, sehen wir die folgende Ausgabe:
Der Zugriff auf .species gibt denselben Wert zurück wie der Zugriff auf das zweite Element von sammy mit [1].
Die Verwendung von namedtuple aus dem collections-Modul macht Ihr Programm lesbarer, wobei die wichtigen Eigenschaften eines Tupels (dass sie unveränderlich und geordnet sind) bewahrt bleiben.
Darüber hinaus fügt die Factoryfunktion namedtuple mehrere zusätzliche Methoden zu Instanzen von Fish hinzu.
Verwenden Sie. _ asdict (), um eine Instanz in ein Wörterbuch zu konvertieren:
Wenn wir print ausführen, sehen Sie eine Ausgabe wie die folgende:
Das Aufrufen von .asdict () auf sammy gibt ein Wörterbuch zurück, das jedem der drei Feldnamen ihre entsprechenden Werte zuordnet.
Python-Versionen, die älter als 3.8 sind, geben diese Zeile möglicherweise etwas anders aus.
Sie könnten beispielsweise ein OrderedDict anstelle des hier gezeigten, einfachen Wörterbuchs sehen.
< $> note Anmerkung: In Python werden Methoden mit vorangehenden Unterstrichen gewöhnlich als "privat" eingestuft.
Weitere Methoden, die von namedtuple bereitgestellt werden (wie _ asdict (),. _ make (),. _ replace (), usw.), sind jedoch öffentlich.
Sammeln von Daten in einem Wörterbuch
Es ist oft nützlich, Daten in Python-Wörterbüchern zu sammeln. defaultdict aus dem collections-Modul kann uns helfen, Informationen schnell und übersichtlich in Wörterbüchern zusammenzustellen.
defaultdict gibt nie einen KeyError aus.
Wenn kein Schlüssel vorhanden ist, fügt defaultdict stattdessen einfach einen Platzhalterwert ein und gibt ihn zurück:
Wenn wir diesen Code ausführen, sehen wir eine Ausgabe wie die folgende:
defaultdict fügt einen Platzhalterwert ein und gibt ihn zurück, anstatt einen KeyError auszugeben.
In diesem Fall haben wir den Platzhalterwert als Liste angegeben.
Reguläre Wörterbücher hingegen geben bei fehlenden Schlüsseln einen KeyError aus:
Das reguläre Wörterbuch my _ regular _ dict gibt einen KeyError aus, wenn wir versuchen, auf einen Schlüssel zuzugreifen, der nicht vorhanden ist.
defaultdict verhält sich anders als ein reguläres Wörterbuch.
Statt einen KeyError auf einen fehlenden Schlüssel auszugeben, ruft defaultdict den Platzhalterwert ohne Argumente auf, um ein neues Objekt zu erstellen.
In diesem Fall list (), um eine leere Liste zu erstellen.
Um mit unserem fiktiven Aquarium-Beispiel fortzufahren, nehmen wir an, wir hätten eine Liste von Fischtupeln, die den Bestand eines Aquariums repräsentieren:
Es gibt drei Fische in dem Aquarium - Name, Spezies und Haltebecken sind in diesen drei Tupeln notiert.
Unser Ziel ist es, unseren Bestand nach Becken zu organisieren. Wir wollen die Liste der in jedem Becken vorhandenen Fische kennen.
Anders ausgedrückt: Wir wollen ein Wörterbuch, das "tank-a" ["Jamie", "Mary"] und "tank-b" ["Jamie"] zuordnet.
Wir können defaultdict verwenden, um den Fisch nach Becken zu gruppieren:
Nach Ausführung dieses Codes sehen wir die folgende Ausgabe:
fish _ names _ by _ tank wird als ein defaultdict deklariert, das standardmäßig list () einfügt, anstatt einen KeyError auszugeben.
Da dies garantiert, dass jeder Schlüssel in fish _ names _ by _ tank auf eine list verweist, können wir frei .append () aufrufen, um Namen zu der Liste jedes Beckens hinzuzufügen.
defaultdict hilft Ihnen hier, weil es die Wahrscheinlichkeit unerwarteter KeyErrors reduziert.
Die Reduzierung der unerwarteten KeyErrors bedeutet, dass Ihr Programm klarer und mit weniger Zeilen geschrieben werden kann.
Konkreter gesagt: Mit dem defaultdict-Idiom können Sie manuelles Instanziieren einer leeren Liste für jedes Becken vermeiden.
Ohne defaultdict hätte der for-Schleifenkörper möglicherweise eher wie folgt ausgesehen:
Die Verwendung eines regulären Wörterbuchs (statt eines defaultdict) bedeutet, dass der for-Schleifenkörper immer das Vorhandensein des gegebenen tank in fish _ names _ by _ tank überprüfen muss.
Erst nachdem wir überprüft haben, dass tank bereits in fish _ names _ by _ tank vorhanden ist, oder gerade mit einem [] initialisiert wurde, können wir den Fischnamen ergänzen.
defaultdict kann dazu beitragen, beim Füllen der Wörterbücher den Standardcode zu reduzieren, da es nie einen KeyError ausgibt.
Verwenden von deque zum effizienten Hinzufügen von Elementen zu beiden Seiten einer Sammlung
Python-Listen sind eine wandelbare oder veränderliche, geordnete Sequenz von Elementen.
Python kann Listen in konstanter Zeit ergänzen (die Länge der Liste hat keine Auswirkungen auf die Zeit, die zum Ergänzen benötigt wird), aber das Einfügen am Anfang einer Liste kann langsamer sein - die Zeitdauer erhöht sich beim Anwachsen der Liste.
Im Sinne der Big-O-Notation ist das Ergänzen einer Liste ein O (1) -Vorgang mit konstanter Zeit.
Im Gegensatz ist das Einfügen am Anfang einer Liste langsamer mit einer O (n) ​ ​ ​ -Leistung.
< $> note Anmerkung: Softwareingenieure messen die Leistung von Vorgängen oft mit der sogenannten "Big O" -Notation.
Wenn die Größe einer Eingabe keine Auswirkungen auf die Zeit hat, die zum Ausführen eines Vorgangs benötigt wird, spricht man von einem Ablauf in konstanter Zeit oder O (1) (" Big O von 1 ").
Wie Sie oben gelernt haben, kann Python Listen mit konstanter Zeitleistung, auch als O (1) bekannt, ergänzen.
Manchmal beeinflusst die Größe einer Eingabe direkt die Zeit, die zum Ausführen eines Vorgangs benötigt wird.
Das Einfügen am Anfang einer Python-Liste zum Beispiel läuft umso langsamer ab, je mehr Elemente in der Liste vorhanden sind.
Die Big-O-Notation verwendet den Buchstaben n, um die Größe der Eingabe darzustellen.
Das bedeutet, dass das Hinzufügen von Elementen am Anfang einer Python-Liste in "linearer Zeit" oder O (n) (" Big O von n ") abläuft.
Im Allgemeinen sind O (1) -Vorgänge schneller als O (n) -Vorgänge.
Wir können am Anfang einer Python-Liste einfügen:
Wenn wir Folgendes ausführen, sehen wir eine Ausgabe wie die folgende:
Die .insert (index, object) -Methode in der Liste ermöglicht uns, "Alice" am Anfang von favorite _ fish _ list einzufügen.
Jedoch hat das Einfügen am Anfang einer Liste eine O (n) -Leistung.
Wenn die Länge der favorite _ fish _ list wächst, wird die Zeit, um einen Fisch am Anfang der Liste einzufügen, proportional anwachsen und immer länger dauern.
deque (ausgesprochen "Deck ") aus dem collections-Modul ist ein listenähnliches Objekt, das es uns ermöglicht, Elemente am Anfang oder Ende einer Sequenz mit konstanter Zeit (O (1)) -Leistung einzufügen.
Geben Sie ein Element am Anfang eines deque ein:
Wir können ein deque anhand einer bereits vorhandenen Sammlung von Elementen instanziieren, in diesem Fall einer Liste mit drei bevorzugten Fischnamen.
Das Aufrufen der appendleft-Methode von favorite _ fish _ deque ermöglicht uns, ein Element am Anfang unserer Sammlung mit O (1) -Leistung einzufügen.
O (1) -Leistung bedeutet, dass die Zeit, die zum Hinzufügen eines Elements am Anfang von favorite _ fish _ deque benötigt wird, nicht zunimmt, selbst wenn favorite _ fish _ deque Tausende oder Millionen von Elementen enthält.
< $> note Anmerkung: Obwohl deque Einträge am Anfang einer Sequenz effizienter als eine Liste hinzufügt, führt deque nicht alle seine Vorgänge effizienter als eine Liste aus.
Beispielsweise hat das Zugreifen auf ein zufälliges Element in einem deque eine O (n) -Leistung, das Zugreifen auf ein zufälliges Element in einer Liste jedoch eine O (1) -Leistung
Verwenden Sie deque, wenn es wichtig ist, Elemente schnell zu beiden Seiten Ihrer Sammlung hinzuzufügen oder zu entfernen.
Ein vollständiger Vergleich der Zeitleistung ist auf Pythons Wiki verfügbar.
Das collections-Modul ist ein leistungsfähiger Teil der Python-Standardbibliothek, mit dem Sie übersichtlich und effizient Daten bearbeiten können.
Dieses Tutorial behandelte drei der Klassen, die vom collections-Modul bereitgestellt werden, einschließlich namedtuple, defaultdict und deque.
Nun können Sie die Dokumentation des collection-Moduls nutzen, um mehr über andere verfügbare Klassen und Dienstprogramme zu erfahren.
Um im Allgemeinen mehr über Python zu erfahren, können Sie unsere Tutorialreihe Codieren in Python 3 lesen.
Verstehen von Destrukturierung, Rest-Parametern und Spread-Syntax in JavaScript
5511
Seit der Ausgabe 2015 der ECMAScript-Spezifikation wurden der JavaScript-Sprache viele neue Funktionalitäten für die Arbeit mit Arrays und Objekten zur Verfügung gestellt.
Einige der bemerkenswerten, die Sie in diesem Artikel kennenlernen, sind Destrukturierung, Rest-Parameter und Spread-Syntax.
Diese Funktionalitäten bieten direkte Zugriffsmöglichkeiten auf die Elemente eines Arrays oder Objekts und können das Arbeiten mit diesen Datenstrukturen schneller und prägnanter machen.
Viele andere Sprachen verfügen nicht über die entsprechende Syntax für Destrukturierung, Rest-Parameter und Spread. Daher können diese Funktionalitäten sowohl für neue JavaScript-Entwickler als auch für Entwickler aus einer anderen Sprache eine Lernkurve aufweisen.
In diesem Artikel erfahren Sie, wie Sie Objekte und Arrays destrukturieren, wie Sie den Spread-Operator zum Auspacken von Objekten und Arrays verwenden und wie Sie Rest-Parameter in Funktionsaufrufen nutzen.
Destrukturierung
Die Destrukturierungszuweisung ist eine Syntax, mit der Sie Objekteigenschaften oder Array-Elemente als Variablen zuweisen können.
Dadurch können die zur Manipulation von Daten in diesen Strukturen erforderlichen Codezeilen erheblich reduziert werden.
Es gibt zwei Arten von Destrukturierung: Objekt-Destrukturierung und Array-Destrukturierung.
Objekt-Destrukturierung
Mit der Objekt-Destrukturierung können Sie neue Variablen mit einer Objekteigenschaft als Wert erstellen.
Betrachten Sie dieses Beispiel eines Objekts, das eine Notiz mit einer id, einem title und einem date darstellt:
Bisher mussten Sie zur Erstellung einer neuen Variable für jede Eigenschaft jede Variable mit einer Vielzahl von Wiederholungen einzeln zuweisen:
Mit der Objekt-Destrukturierung kann das alles in einer Zeile erfolgen.
Durch Umgeben der Variablen mit geschweiften Klammern {} erstellt JavaScript aus jeder Eigenschaft neue Variablen mit dem gleichen Namen:
Protokollieren Sie nun die neuen Variablen mit console.log ():
Sie erhalten die ursprünglichen Eigenschaftswerte als Ausgabe:
< $> note Anmerkung: Das Destrukturieren eines Objekts ändert das ursprüngliche Objekt nicht.
Sie könnten immer noch die ursprüngliche note mit all ihren Einträgen aufrufen.
Die Standardzuweisung für die Objekt-Destrukturierung erstellt neue Variablen mit dem gleichen Namen wie die Objekteigenschaft.
Wenn Sie nicht möchten, dass der Name der neuen Variable und der Eigenschaftsname gleich sind, können Sie die neue Variable auch umbenennen. Verwenden Sie dazu einen Doppelpunkt (:), um einen neuen Namen festzulegen, wie im Folgenden mit noteId zu sehen ist:
Protokollieren Sie die neue Variable noteId in der Konsole:
Sie können auch geschachtelte Objektwerte destrukturieren.
Aktualisieren Sie beispielsweise das note-Objekt, um ein geschachteltes author-Objekt zu erhalten:
Sie können nun note destrukturieren und dann zur Erstellung von Variablen aus den author-Eigenschaften erneut destrukturieren:
Als Nächstes protokollieren Sie die neuen Variablen firstName und lastName unter Verwendung von Template-Buchstabensymbolen:
Beachten Sie, dass Sie in diesem Beispiel zwar Zugriff auf den Inhalt des author-Objekts haben, das author-Objekt selbst jedoch nicht zugänglich ist.
Um auf ein Objekt und seine geschachtelten Werte zuzugreifen, müssten Sie sie getrennt deklarieren:
Dieser Code gibt das author-Objekt aus:
Die Destrukturierung eines Objekts ist nicht nur nützlich, um die Menge des zu schreibenden Codes zu reduzieren, sondern ermöglicht es Ihnen auch, gezielt auf die Eigenschaften zuzugreifen, die Ihnen wichtig sind.
Zu guter Letzt kann Destrukturierung verwendet werden, um auf die Objekteigenschaften primitiver Werte zuzugreifen.
Beispielsweise ist String ein globales Objekt für Zeichenfolgen und hat eine length-Eigenschaft:
Dadurch wird die inhärente Längeneigenschaft einer Zeichenfolge gefunden und der length-Variablen gleichgesetzt.
Protokollieren Sie length, um zu sehen, ob dies funktioniert:
Die Zeichenfolge A string wurde hier implizit in ein Objekt konvertiert, um die length-Eigenschaft abzurufen.
Array-Destrukturierung
Mit der Array-Destrukturierung können Sie neue Variablen mit einem Array-Element als Wert erstellen.
Betrachten Sie dieses Beispiel eines Arrays mit den verschiedenen Teilen eines Datums:
Arrays in JavaScript behalten garantiert ihre Reihenfolge bei, sodass in diesem Fall der erste Index immer ein Jahr ist, der zweite ein Monat, und so weiter.
Mit diesem Wissen können Sie Variablen aus den Elementen im Array erstellen:
Doch dies manuell auszuführen kann viel Platz in Ihrem Code beanspruchen.
Mit der Array-Destrukturierung können Sie die Werte aus dem Array der Reihe nach entpacken und sie so ihren eigenen Variablen zuweisen:
Protokollieren Sie nun die neuen Variablen:
Werte können übersprungen werden, indem die Destrukturierungssyntax zwischen Kommas leer gelassen wird:
Das Ausführen ergibt den Wert von year und day:
Auch geschachtelte Arrays können destrukturiert werden.
Erstellen Sie zuerst ein geschachteltes Array:
Destrukturieren Sie dann dieses Array und protokollieren Sie die neuen Variablen:
Die Destrukturierungssyntax kann zur Destrukturierung der Parameter in einer Funktion angewendet werden.
Um das zu testen, destrukturieren Sie die keys und values aus Object.entries ().
Zuerst deklarieren Sie das note-Objekt:
Bei diesem Objekt könnten Sie die Schlüssel-Wert-Paare auflisten, indem Sie Argumente bei der Übergabe an die forEach () -Methode destrukturieren:
Sie können dasselbe mit einer for-Schleife erreichen:
In beiden Fällen erhalten Sie Folgendes:
Objekt-Destrukturierung und Array-Destrukturierung können in einer einzigen Destrukturierungszuweisung kombiniert werden.
Auch Standardparameter können mit Destrukturierung verwendet werden, wie in diesem Beispiel, das das Standarddatum auf new Date () festlegt.
Destrukturieren Sie dann das Objekt, wobei Sie auch eine neue date-Variable mit der Standardeinstellung von new Date () festlegen:
console.log (date) gibt dann eine ähnliche Ausgabe wie die folgende aus:
Wie in diesem Abschnitt gezeigt wird, verleiht die Syntax der Destrukturierungszuweisung JavaScript eine große Flexibilität und ermöglicht Ihnen, prägnanteren Code zu schreiben.
Im nächsten Abschnitt sehen Sie, wie die Spread-Syntax zur Erweiterung von Datenstrukturen in ihren einzelnen Dateneinträgen genutzt werden kann.
Spread
Die Spread-Syntax (...) ist eine weitere hilfreiche Ergänzung zu JavaScript zum Arbeiten mit Arrays, Objekten und Funktionsaufrufen.
Spread ermöglicht das Entpacken oder Erweitern von Objekten und Iterablen (wie beispielsweise Arrays), wodurch flache Kopien von Datenstrukturen erstellt werden können, um die Datenmanipulation zu vereinfachen.
Spread mit Arrays
Spread kann allgemeine Aufgaben mit Arrays vereinfachen.
Nehmen wir zum Beispiel an, Sie haben zwei Arrays und möchten diese kombinieren:
Ursprünglich würden Sie concat () verwenden, um die beiden Arrays zu verketten:
Sie können nun auch Spread verwenden, um die Arrays in ein neues Array zu entpacken:
Diese Ausführung würde Folgendes ergeben:
Das kann bei Unveränderlichkeit besonders hilfreich sein.
Sie könnten beispielsweise mit einer App arbeiten, die users in einem Array von Objekten gespeichert hat:
Sie könnten push verwenden, um das bestehende Array zu ändern und einen neuen Benutzer hinzuzufügen, was die veränderbare Option wäre:
Das ändert jedoch das user-Array, das wir möglicherweise erhalten möchten.
Mit Spread können Sie ein neues Array aus dem bestehenden erstellen und am Ende ein neues Element hinzufügen:
Das neue Array, updatedUsers, hat den neuen Benutzer, aber das ursprüngliche users-Array bleibt unverändert:
Das Erstellen von Datenkopien anstelle der Änderung vorhandener Daten kann helfen, unerwartete Änderungen zu verhindern.
Wenn Sie in JavaScript ein Objekt oder Array erstellen und einer anderen Variable zuweisen, erstellen Sie nicht tatsächlich ein neues Objekt - Sie übergeben eine Referenz.
Nehmen Sie dieses Beispiel, in dem ein Array erstellt und einer anderen Variable zugewiesen wird:
Das Entfernen des letzten Elements des zweiten Arrays ändert das erste:
Mit Spread können Sie eine flache Kopie eines Arrays oder Objekts erstellen. Das bedeutet, dass alle Eigenschaften der oberen Ebene geklont, geschachtelte Objekte jedoch weiterhin per Referenz übergeben werden.
Bei einfachen Arrays oder Objekten kann eine flache Kopie völlig ausreichen.
Wenn Sie den gleichen Beispielcode schreiben, aber das Array mit Spread kopieren, wird das originale Array nicht mehr geändert:
Das Folgende wird in der Konsole protokolliert:
Spread kann auch zur Konvertierung eines set oder jeder anderen iterable in ein Array verwendet werden.
Erstellen Sie ein neues Set und fügen Sie diesem einige Einträge hinzu:
Verwenden Sie als Nächstes den Spread-Operator mit set und protokollieren Sie die Ergebnisse:
Das kann auch nützlich sein, um ein Array aus einer Zeichenfolge zu erstellen:
Dadurch wird ein Array mit jedem Zeichen als Element in dem Array ausgegeben:
Spread mit Objekten
Beim Arbeiten mit Objekten kann Spread zum Kopieren und Aktualisieren von Objekten genutzt werden.
Ursprünglich wurde Object.assign () zum Kopieren eines Objekts genutzt:
Das secondObject ist nun ein Klon des originalObject.
Dies wird mit der Spread-Syntax vereinfacht - Sie können ein Objekt flach kopieren, indem Sie es in ein neues verteilen:
Genau wie bei Arrays wird auch hier nur eine flache Kopie erstellt, und verschachtelte Objekte werden weiterhin per Referenz übergeben.
Das Hinzufügen oder Ändern von Eigenschaften an einem bestehenden Objekt auf unveränderliche Weise wird mit Spread vereinfacht.
In diesem Beispiel wird die Eigenschaft isLoggedIn dem user-Objekt hinzugefügt:
Beim Aktualisieren von Objekten über Spread ist es wichtig, zu beachten, dass auch jedes geschachtelte Objekt verteilt werden muss.
Nehmen wir zum Beispiel an, dass sich im user-Objekt ein geschachteltes organization-Objekt befindet:
Wenn Sie versuchen würden, ein neues Element zu organization hinzuzufügen, würde das die bestehenden Felder überschreiben:
Dadurch würde sich Folgendes ergeben:
Wenn Veränderlichkeit kein Problem ist, könnte das Feld direkt aktualisiert werden:
Da wir aber eine unveränderliche Lösung suchen, können wir das innere Objekt verteilen, um die bestehenden Eigenschaften zu erhalten:
Spread mit Funktionsaufrufen
Spread kann auch mit Argumenten in Funktionsaufrufen genutzt werden.
Als Beispiel sehen Sie hier eine multiply-Funktion, die drei Parameter nimmt und sie multipliziert:
Normalerweise würden sie drei Werte individuell als Argumente an den Funktionsaufruf wie folgt übergeben:
Wenn jedoch alle Werte, die Sie an die Funktion übergeben möchten, bereits in einem Array vorhanden sind, können Sie mit der Spread-Syntax jedes Element in einem Array als Argument verwenden:
< $> note Anmerkung: Ohne Spread kann dies durch Verwendung von apply () erreicht werden:
Nachdem Sie nun gesehen haben, wie Spread Ihren Code verkürzen kann, können Sie sich eine andere Verwendung der Syntax... anstehen: Rest-Parameter.
Rest-Parameter
Die letzte Funktionalität, die Sie in diesem Artikel kennenlernen, ist die Rest-Parameter-Syntax.
Die Syntax entspricht der von Spread (...), hat aber den gegenteiligen Effekt.
Statt ein Array oder Objekt in einzelne Werte zu entpacken, erstellt die Rest-Syntax ein Array mit einer indefiniten Anzahl von Argumenten.
Wenn wir beispielsweise in der Funktion restTest wollten, dass args ein Array aus einer indefiniten Anzahl von Argumenten ist, könnten wir Folgendes haben:
Alle Argumente, die an die Funktion restTest übergeben werden, sind nun im args-Array verfügbar:
Die Rest-Syntax kann als einziger Parameter oder als letzter Parameter in der Liste genutzt werden.
Wenn sie als einziger Parameter genutzt wird, wird sie alle Argumente sammeln. Wenn sie am Ende einer Liste steht, wird sie jedes verbleibende Argument so wie in diesem Beispiel sammeln:
Dadurch werden die ersten beiden Argumente einzeln aufgeführt und dann die restlichen in ein Array gruppiert:
In älterem Code könnte die Variable arguments genutzt werden, um alle Argumente zu sammeln, die durch eine Funktion übergeben werden:
Dadurch ergäbe sich folgende Ausgabe:
Das hat jedoch einige Nachteile.
Erstens kann die Variable arguments nicht mit Pfeilfunktionen verwendet werden.
Das würde einen Fehler ergeben:
Außerdem ist arguments kein echtes Array und kann Methoden wie map und filter nicht verwenden, ohne zuerst in ein Array konvertiert zu werden.
Es sammelt außerdem alle übergebenen Argumente statt nur die verbleibenden Argumente, wie sie im Beispiel restTest (one, two,... args) ​ ​ zu sehen sind.
Rest kann auch bei der Destrukturierung von Arrays verwendet werden:
Rest kann auch bei der Destrukturierung von Objekten genutzt werden:
Mit der folgenden Ausgabe:
Auf diese Weise bietet die Rest-Syntax effiziente Methoden zur Erfassung einer unbestimmten Anzahl von Elementen.
In diesem Artikel haben Sie Destrukturierung, Spread-Syntax und Rest-Parameter kennengelernt.
Kurz gefasst:
Destrukturierung wird genutzt, um Variablen aus Array-Elementen oder Objekt-Eigenschaften zu erstellen.
Spread-Syntax wird genutzt, um Iterable wie Arrays, Objekte und Funktionsaufrufe zu entpacken.
Rest-Parameter-Syntax erstellt ein Array aus einer indefiniten Anzahl von Werten.
Destrukturierung, Rest-Parameter und Spread-Syntax sind nützliche Funktionalitäten in JavaScript, die dazu beitragen, Ihren Code prägnant und sauber zu halten.
Wenn Sie die Destrukturierung in Aktion sehen möchten, werfen Sie einen Blick auf Anpassen von React-Komponenten mit Props, das diese Syntax verwendet, um Daten zu destrukturieren und sie an benutzerdefinierte Frontend-Komponenten zu übergeben.
Wenn Sie mehr über JavaScript erfahren möchten, kehren Sie zu unserer Seite mit der Serie Codieren in JavaScript zurück.
Installieren und Einrichten von Laravel mit Docker Compose unter Ubuntu 20.4
5832
Auf Ihrem Server gemäß den Schritten 1 und 2 von Installieren und Verwenden von Docker unter Ubuntu 20.4 installiertes Docker.
Auf Ihrem Server gemäß Schritt 1 von Installieren und Verwenden von Docker Compose unter Ubuntu 20.04 installiertes Docker Compose
< $> note Anmerkung: Wenn Sie diese Demo auf Ihrem lokalen Rechner ausführen, verwenden Sie http: / / localhost: 8000, um von Ihrem Browser aus auf die Anwendung zuzugreifen.
Fernzugriff auf GUI-Anwendungen mit Docker und Caddy unter Ubuntu 18.04
5806
Trotz der wachsenden Beliebtheit von Cloud-Diensten besteht nach wie vor die Notwendigkeit, native Anwendungen auszuführen.
Durch die Verwendung von noVNC und TigerVNC können Sie native Anwendungen innerhalb eines Docker-Containers ausführen und über einen Webbrowser aus der Ferne auf sie zugreifen.
Darüber hinaus können Sie Ihre Anwendung auf einem Server mit mehr Systemressourcen ausführen, als Ihnen vor Ort zur Verfügung stehen, was die Flexibilität bei der Ausführung großer Anwendungen steigern kann.
In diesem Tutorial containerisieren Sie mit Docker Mozilla Thunderbird, einen E-Mail-Client.
Anschließend sichern Sie sie Ihn und bieten Fernzugriff über den Caddy Webserver.
Nach Abschluss können Sie von jedem Gerät aus mit einem Webbrowser auf Thunderbird zugreifen.
Optional können Sie auch lokal auf die Dateien zugreifen, indem Sie WebDAV verwenden.
Außerdem erhalten Sie ein völlig eigenständiges Docker-Image, das Sie überall ausführen können.
Einen Ubuntu 18.04-Server mit mindestens 2 GB RAM und 4 GB Festplattenspeicher.
einen Nicht-root-Benutzer mit sudo-Berechtigungen.
Auf Ihrem Server eingerichteten Docker.
Sie können Installieren und Verwenden von Docker unter Ubuntu 18.04 folgen.
Schritt 1 & mdash; Erstellen der supervisord-Konfiguration
Da Ihr Server nun ausgeführt wird und Docker installiert ist, können Sie mit der Konfiguration des Containers Ihrer Anwendung beginnen.
Da Ihr Container aus mehreren Komponenten besteht, müssen Sie einen Prozessmanager verwenden, um sie zu starten und zu überwachen.
In diesem Fall verwenden Sie supervisord. supervisord ist ein in Python geschriebener Prozessmanager, der häufig zur Organisation komplexer Container verwendet wird.
Erstellen und geben Sie zunächst ein Verzeichnis namens thunderbird für Ihren Container ein:
Erstellen und öffnen Sie nun eine Datei namens supervisord.conf mit nano oder Ihrem bevorzugten Editor:
Fügen Sie nun diesen ersten Code-Block in supervisord.conf ein, der die globalen Optionen für supervisord definiert:
In diesem Block konfigurieren Sie supervisord selbst.
Sie müssen nodaemon auf true setzen, da es innerhalb eines Docker-Containers als Einstiegspunkt ausgeführt wird.
Daher möchten Sie, dass es weiterhin im Vordergrund ausgeführt wird.
Außerdem setzten Sie pidfile auf einen Pfad, auf den ein Nicht-root-Benutzer Zugriff hat (mehr dazu später), und logfile, auf stdout, damit Sie die Protokolle sehen können.
Fügen Sie als Nächstes einen weiteren kleinen Code-Block zu supervisord.conf hinzu.
Dieser Block startet TigerVNC, das ein kombinierter VNC / X11-Server ist:
In diesem Block richten Sie den X11-Server ein.
X11 ist ein Display-Server-Protokoll, das die Ausführung von GUI-Anwendungen ermöglicht.
Beachten Sie, dass es in Zukunft durch Wayland ersetzt werden wird, aber der Fernzugriff befindet sich noch in der Entwicklung.
Für diesen Container verwenden Sie TigerVNC und seinen integrierten VNC-Server.
Dies hat eine Reihe von Vorteilen gegenüber der Verwendung eines separaten X11- und VNC-Servers:
Schneller Reaktionszeit, da die GUI-Zeichnung direkt auf dem VNC-Server erfolgt und nicht in einem zwischengeschalteten Frambuffer (dem Speicher, der den Bildschirminhalt speichert).
Automatische Größenanpassung des Bildschirms, wodurch die Fernanwendung die Größe automatisch an den Client (in diesem Fall Ihr Webbrowser-Fenster) anpassen kann.
Wenn Sie möchten, können Sie das Argument für die Option -desktop von Thunderbird auf etwas anderes Ihrer Wahl ändern.
Der Server zeigt Ihre Wahl als Titel der Webseite an, die für den Zugriff auf Ihre Anwendung verwendet wird.
Fügen wir nun einen dritten Code-Block zu supervisord.conf hinzu, um easy-novnc zu starten:
In diesem Block richten Sie easy-novnc ein, einen eigenständigen Server, der einen Wrapper um noVNC bereitstellt.
Dieser Server erfüllt zwei Rollen.
Erstens stellt er eine einfache Verbindungsseite bereit, auf der Sie Optionen für die Verbindung konfigurieren und Standardoptionen festlegen können.
Zweitens stellt er VNC über WebSocket als Proxy bereit, sodass der Zugriff über einen gewöhnlichen Webbrowser möglich ist.
Normalerweise wird die Größenanpassung auf der Client-Seite vorgenommen (d. h. die Bildskalierung), aber Sie verwenden die Option resize = remote, um die Vorteile der Remote-Auflösungseinstellung von TigerVNC voll zu nutzen.
Dies bietet auch eine geringere Latenz auf langsameren Geräten, wie z. B. Chromebooks niedrigerer Leistungsklassen:
< $> note Anmerkung: Dieses Tutorial verwendet easy-novnc.
Wenn Sie möchten, können Sie stattdessen websockify und einen separaten Webserver verwenden.
Der Vorteil von easy-novnc besteht darin, dass der Speicherverbrauch und die Startzeit deutlich geringer sind und dass es in sich geschlossen ist. easy-novnc bietet außerdem eine sauberere Verbindungsseite als die Standardseite von noVNC und ermöglicht die Einstellung von Standardoptionen, die für diese Einrichtung hilfreich sind (wie resize = remote).
Fügen Sie nun den folgenden Block zu Ihrer Konfiguration hinzu, um OpenBox, den Fenstermanager, zu starten:
In diesem Block richten Sie OpenBox ein, einen schlanken X11-Fenstermanager.
Sie könnten diesen Schritt überspringen, aber ohne ihn hätten Sie keine Titellisten und könnten die Fenstergröße nicht ändern.
Zum Schluss fügen wir den letzten Block zu supervisord.conf hinzu, wodurch die Hauptanwendung gestartet wird:
In diesem letzten Block setzen Sie priority auf 1, um sicherzustellen, dass Thunderbird nach TigerVNC gestartet wird. Ansonsten würde es auf eine Race-Bedingung treffen und womöglich nicht starten.
Wir setzen auch autorestart = true, um die Anwendung automatisch wieder zu öffnen, wenn sie versehentlich geschlossen wird.
Die Umgebungsvariable DISPLAY weist die Anwendung zur Anzeige auf dem zuvor erstellten VNC-Server an.
So wird Ihre fertiggestellte supervisord.conf aussehen:
Wenn Sie eine andere Anwendung containerisieren möchten, ersetzen Sie / usr / bin / thunderbird durch den Pfad zur ausführbaren Datei Ihrer Anwendung.
Andernfalls sind Sie nun bereit, das Hauptmenü Ihrer GUI zu konfigurieren.
Schritt 2 & mdash; Einrichten des OpenBox-Menüs
Nachdem Ihr Prozessmanager konfiguriert ist, richten wir nun das OpenBox-Menü ein.
Dieses Menü ermöglicht es uns, Anwendungen innerhalb des Containers zu starten.
Bei Bedarf werden wir auch einen Terminal- und Prozessmonitor für das Debugging einschließen.
Verwenden Sie innerhalb des Verzeichnisses Ihrer Anwendung nano oder Ihren bevorzugten Texteditor, um eine neue Datei namens menu.xml zu erstellen und zu öffnen:
Fügen Sie nun den folgenden Code zu menu.xml hinzu:
Diese XML-Datei enthält die Menüeinträge, die angezeigt werden, wenn Sie mit der rechten Maustaste auf den Desktop klicken.
Jedes Element besteht aus einem Label und einer Aktion.
Wenn Sie eine andere Anwendung containerisieren möchten, ersetzen Sie / usr / bin / thunderbird durch den Pfad zur ausführbaren Datei Ihrer Anwendung und ändern Sie das Label des Elements.
Schritt 3 & mdash; Erstellen der Dockerfile
Nachdem OpenBox konfiguriert ist, erstellen Sie nun die Dockerfile, die alles miteinander verbindet.
Erstellen Sie eine Dockerfile im Verzeichnis Ihres Containers:
Um zu beginnen, fügen wir etwas Code hinzu, um easy-novnc zu erstellen:
In der ersten Stufe erstellen Sie easy-novnc.
Dies wird aus Gründen der Einfachheit und Platzersparnis in einem separaten Schritt durchgeführt - Sie benötigen nicht die gesamte Go-Toolchain in Ihrem endgültigen Image.
Beachten Sie das @ v1.1.0 im Befehl "build".
Dadurch wird sichergestellt, dass das Ergebnis deterministisch ist, was wichtig ist, weil Docker das Ergebnis jedes einzelnen Schritts zwischenspeichert.
Wenn Sie keine explizite Version angegeben hätten, würde Docker zum Zeitpunkt der ersten Erstellung des Images auf die neueste Version von easy-novnc verweisen.
Darüber hinaus möchten Sie sicherstellen, dass Sie eine bestimmte Version von easy-novnc herunterladen, für den Fall, dass an der CLI-Schnittstelle gravierende Änderungen vorgenommen werden.
Erstellen wir nun die zweite Stufe, die zum endgültigen Image wird.
Hier verwenden Sie Debian 10 (buster) als Basis-Image.
Beachten Sie, dass dieses, da es in einem Container ausgeführt wird, unabhängig von der auf Ihrem Server laufenden Distribution funktioniert.
Fügen Sie als Nächstes den folgenden Block zu Ihrer Dockerfile hinzu:
In dieser Anweisung installieren Sie Debian 10 als Ihr Basis-Image und installieren dann das absolute Minimum, das erforderlich ist, um GUI-Anwendungen in Ihrem Container auszuführen.
Beachten Sie, dass Sie apt-get update als Teil der gleichen Anweisung ausführen, um Zwischenspeicherungsprobleme von Docker zu verhindern.
Um Speicherplatz zu sparen, entfernen Sie auch die danach heruntergeladenen Paketlisten (die zwischengespeicherten Pakete selbst werden standardmäßig entfernt).
Sie erstellen auch / usr / share / desktop-directories, da einige Anwendungen von dem vorhandenen Verzeichnis abhängen.
Fügen wir einen weiteren kleinen Code-Block hinzu:
In dieser Anweisung installieren Sie einige nützliche Allzweck-Dienstprogramme und -Pakete.
Von besonderem Interesse sind hier xdg-utils (das die Basisbefehle bereitstellt, die von Desktop-Anwendungen unter Linux verwendet werden) und ca-certificates (das die Stammzertifikate installiert, um uns den Zugriff auf HTTPS-Seiten zu ermöglichen).
Nun können wir die Anweisungen für die Hauptanwendung hinzufügen:
Wie zuvor installieren wir hier die Anwendung.
Wenn Sie eine andere Anwendung containerisieren möchten, können Sie diese Befehle durch die Befehle ersetzen, die zum Installieren Ihrer spezifischen Anwendung erforderlich sind. Einige Anwendungen erfordern etwas mehr Arbeit, um in Docker ausgeführt zu werden.
Wenn Sie beispielsweise eine App installieren, die Chrome, Chromium oder QtWebEngine verwendet, müssen Sie das Befehlszeilenargument --no-sandbox verwenden, da es von Docker nicht unterstützt wird.
Als Nächstes fügen wir die Anweisungen zum Hinzufügen der letzten wenigen Dateien zum Container hinzu:
Hier fügen Sie dem Image die zuvor erstellten Konfigurationsdateien hinzu, und kopieren die Binärdatei easy-novnc aus der ersten Stufe.
Dieser nächste Code-Block erstellt das Datenverzeichnis und fügt einen dedizierten Benutzer für Ihre App hinzu. Dies ist wichtig, da einige Anwendungen sich weigern als root ausgeführt zu werden.
Es ist auch eine bewährte Praxis, Anwendungen nicht als root auszuführen, auch nicht in einem Container.
Um eine konsistente UID / GID für die Dateien zu gewährleisten, setzen Sie beide ausdrücklich auf 1000.
Außerdem installieren Sie ein Volumen in das Datenverzeichnis, um sicherzustellen, dass es zwischen den Neustarts erhalten bleibt.
Zum Schluss fügen wir noch die Anweisungen zum Starten von allem hinzu:
Wenn sie den Standardbefehl auf supervisord setzen, wir der Manager die für die Ausführung Ihrer Anwendung erforderlichen Prozesse starten.
In diesem Fall verwenden Sie CMD anstatt ENTRYPOINT.
In den meisten Fällen würde es keinen Unterschied machen, aber die Verwendung von CMD ist für diesen Zweck aus einigen Gründen besser geeignet.
Erstens nimmt supervisord keine Argumente entgegen, die für uns relevant wären, und wenn Sie dem Container Argumente hinzufügen, ersetzten diese CMD und werden an ENTRYPOINT angehängt.
Zweitens ermöglicht uns die Verwendung von CMD, bei der Übergabe von Argumenten an den Container einen völlig anderen Befehl (der von / bin / sh -c ausgeführt wird), anzugeben, was das Debuggen erleichtert.
Und schließlich müssen Sie vor dem Starten von supervisord chown als root ausführen, um Berechtigungsprobleme auf dem Datenvolumen zu verhindern und den untergeordneten Prozessen das Öffnen von stdout zu ermöglichen.
Das bedeutet auch, dass sie gosu anstelle der Anweisung USER verwenden müssen, um den Benutzer zu wechseln.
So wird Ihre fertiggestellte Dockerfile aussehen:
Speichern und schließen Sie Ihre Dockerfile.
Nun sind wir bereit, unseren Container zu erstellen und auszuführen und dann auf Thunderbird & mdash; eine GUI-Anwendung & mdash; zugreifen.
Schritt 4 & mdash; Erstellen und Ausführen des Containers
Der nächste Schritt besteht darin, Ihren Container zu erstellen und so einzustellen, dass er beim Starten ausgeführt wird.
Sie werden auch ein Volumen einrichten, um die Anwendungsdaten zwischen Neustarts und Aktualisierungen zu erhalten.
Erstellen Sie zuerst Ihren Container.
Stellen Sie sicher, dass diese Befehle im Verzeichnis ~ / thunderbird ausgeführt werden:
Erstellen Sie nun ein neues Netzwerk, das von den Containern der Anwendung gemeinsam genutzt wird:
Erstellen Sie dann ein Volume zum Speichern der Anwendungsdaten:
Führen Sie es abschließen aus und stellen Sie es so ein, dass es automatisch neu startet:
Beachten Sie, dass Sie, wenn Sie möchten, die < ^ > thunderbird-app < ^ > nach der Option --name durch einen anderen Namen ersetzen können.
Was auch immer Sie gewählt haben, Ihre Anwendung ist nun containerisiert und wird ausgeführt.
Verwenden wir nun den Caddy Webserver, um sie zu sichern und eine Fernverbindung zu ihr aufzubauen.
Schritt 5 & mdash; Einrichten von Caddy
In diesem Schritt richten Sie den Caddy-Webserver so ein, dass er Authentifizierung und, optional, Fernzugriff auf Ihre Dateien über WebDAV bietet.
Der Einfachheit halber und damit Sie ihn mit Ihrem vorhandenen Reverse-Proxy verwenden können, werden Sie ihn in einem anderen Container ausführen.
Erstellen Sie ein neues Verzeichnis und gehen Sie dann in dieses:
Erstellen Sie nun mit nano oder Ihrem bevorzugten Editor eine neue Dockerfile:
Fügen Sie dann die folgenden Anweisungen hinzu:
Diese Dockerfile erstellt Caddy mit dem aktivierten WebDAV-Plugin und startet ihn dann auf Port 8080 mit der Caddyfile unter / etc / Caddyfile.
Als Nächstes konfigurieren Sie den Caddy-Webserver.
Erstellen Sie eine Datei namens Caddyfile im gerade erstellten Verzeichnis:
Fügen Sie nun den folgenden Code-Block zu Ihrer Caddyfile hinzu:
Diese Caddyfile verweist das Stammverzeichnis an den in Schritt 4 erstellten Container thunderbird-app (Docker löst ihn in die richtige IP auf).
Es wird auch einen schreibgeschützten webbasierten Dateibrowser auf / files bedienen und einen WebDAV-Server auf / webdav ausführen, den Sie lokal installieren können, um auf Ihre Dateien zugreifen zu können.
Der Benutzername und das Passwort werden aus den Umgebungsvariablen APP _ USERNAME und APP _ PASSWORD _ HASH gelesen.
Erstellen Sie nun den Container:
Caddy v.2 erfordert, dass Sie Ihr gewünschtes Passwort hashen.
Führen Sie den folgenden Befehl aus und denken Sie daran, < ^ > mypass < ^ > durch ein starkes Passwort Ihrer Wahl zu ersetzen:
Dieser Befehl gibt eine Zeichenfolge aus.
Kopieren Sie diese in die Zwischenablage, um die Ausführung des nächsten Befehls vorzubereiten.
Jetzt sind Sie bereit, den Container auszuführen.
Achten Sie darauf, < ^ > myuser < ^ > durch einen Benutzernamen Ihrer Wahl zu ersetzen und ersetzen Sie < ^ > mypass-hash < ^ > durch die Ausgabe des im vorherigen Schritt ausgeführten Befehls.
Sie können auch den Port (hier < ^ > 8080 < ^ >) ändern, um über einen anderen Port auf Ihren Server zuzugreifen:
Wir sind nun bereit, auf unsere Anwendung zugreifen und sie zu testen.
Schritt 6 & mdash; Testen und Verwalten der Anwendung
Greifen wir nun auf die Anwendung zu und stellen sicher, dass sie funktioniert.
Öffnen Sie zunächst http: / / < ^ > your _ server _ ip < ^ >: < ^ > 8080 < ^ > in einem Webbrowser, melden Sie sich mit den zuvor gewählten Anmeldeinformationen an und klicken Sie auf Connect.
Verbindungsseite von NoVNC
Sie sollten nun in der Lage sein, mit der Anwendung zu interagieren und sie sollte sich automatisch an die Größe Ihres Browserfensters anpassen.
Hauptmenü von Thunderbird
Wenn Sie mit der rechten Maustaste auf den schwarzen Desktop klicken, sollten Sie ein Menü sehen, das Ihnen den Zugriff auf ein Terminal ermöglicht.
Wenn Sie mit der mittleren Maustaste klicken, sollten Sie eine Liste von Fenstern sehen.
NoVNC Klicken mit der rechten Maustaste
Öffnen Sie nun http: / / < ^ > your _ server _ ip < ^ >: < ^ > 8080 < ^ > / files / in einem Webbrowser.
Sie sollten in der Lage sein, auf Ihre Dateien zugreifen.
NoVNC Dateizugriff webdav
Optional können Sie versuchen, http: / / < ^ > your _ server _ ip < ^ >: < ^ > 8080 < ^ > / webdav / in einem WebDAV-Client zu installieren.
Sie sollten in der Lage sein, direkt auf Ihre Dateien zuzugreifen und sie zu ändern.
Wenn Sie die Option Map network drive (Netzlaufwerk zuordnen) im Windows Explorer verwenden, müssen Sie entweder einen Reverse-Proxy verwenden, um HTTPS hinzuzufügen oder HKLM\ SYSTEM\ CurrentControlSet\ Services\ WebClient\ Parameters\ BasicAuthLevel auf DWORD: 2 setzen.
In beiden Fällen ist Ihre native GUI-Anwendung nun für die Fernverwendung bereit.
Sie haben nun erfolgreich einen Docker-Container für Thunderbird eingerichtet und dann mit Caddy den Zugriff darauf über einen Webbrowser konfiguriert.
Sollten Sie Ihre App jemals aktualisieren müssen, halten Sie die Container an, führen Sie docker rm thunderbird-app thunderbird-web aus, erstellen Sie die Images neu und führen Sie dann die Befehle docker run aus den vorherigen Schritten oben erneut aus.
Ihre Daten bleiben weiterhin erhalten, da sie in einem Volumen gespeichert sind.
Wenn Sie mehr über grundlegende Docker-Befehle erfahren möchten, können Sie dieses Tutorial oder dieses Cheatsheet lesen.
Für den längerfristigen Gebrauch sollten Sie auch in Betracht ziehen, HTTPS (hierfür ist eine Domäne erforderlich) für zusätzliche Sicherheit zu aktivieren.
Wenn Sie mehr als eine Anwendung bereitstellen, möchten Sie möglicherweise Docker Compose oder Kubernetes verwenden, anstatt jeden Container manuell zu starten.
Denken Sie daran, dass dieses Tutorial als Grundlage für die Ausführung jeder anderen Linux-Anwendung auf Ihrem Server dienen kann, einschließlich:
Wine, eine Kompatibilitätsschicht für die Ausführung von Windows-Anwendungen unter Linux.
GIMP, ein Open-Source-Bildbearbeitungsprogramm.
Cutter, eine Open-Source-Plattform für Reverse Engineering.
Diese letzte Option zeigt das große Potenzial der Containerisierung und des Fernzugriffs auf GUI-Anwendungen.
Mit dieser Einrichtung können Sie nun einen Server mit wesentlich mehr Rechenleistung, als Sie möglicherweise vor Ort haben, verwenden, um ressourcenintensive Tools wie Cutter auszuführen.
Fernzugriff auf GUI-Anwendungen mit Docker und Caddy unter Ubuntu 20.04
5835
Einen Ubuntu 20.04-Server mit mindestens 2 GB RAM und 4 GB Festplattenspeicher.
Sie können Installieren und Verwenden von Docker unter Ubuntu 20.04 folgen.
Erstellen eines redundanten Speicherpools mit GlusterFS unter Ubuntu 20.04
6028
Eine frühere Version dieses Tutorials wurde von Justin Ellingwood verfasst.
Single Points of Failure stellen bei der Speicherung kritischer Daten ein beträchtliches Risiko dar.
Während sich mit vielen Datenbanken und anderer Software Daten im Kontext einer einzelnen Anwendung verteilen lassen, arbeiten andere Systeme auf der Ebene des Dateisystems, um sicherzustellen, dass Daten beim Schreiben auf Festplatte jedes Mal auch an einen anderen Ort kopiert werden.
GlusterFS ist ein Network-Attached-Storage-Dateisystem (NAS), mit dem Sie Speicherressourcen verschiedener Geräte bündeln können.
So lassen sich mehrere Speichergeräte, die auf unterschiedliche Computer verteilt sind, als eine leistungsfähigere Einheit nutzen.
Außerdem bietet Ihnen GlusterFS die Möglichkeit, verschiedenartige Speicherkonfigurationen einzurichten, von denen viele funktionell RAID-Leveln ähneln.
Zum Beispiel können Sie Daten auf verschiedenen Knoten im Cluster stripen oder für eine höhere Datenverfügbarkeit Redundanz implementieren.
In diesem Leitfaden erstellen Sie ein redundantes geclustertes Speicherarray, auch als verteiltes Dateisystem oder (wie in der GlusterFS-Dokumentation) als Trusted Storage Pool bezeichnet.
Damit erhalten Sie Funktionen, die einer über das Netzwerk gespiegelten RAID-Konfiguration ähneln: Jeder unabhängige Server enthält eine eigene Kopie der Daten, sodass Ihre Anwendungen auf eine beliebige Kopie zugreifen können. Dadurch lässt sich die Leselast besser verteilen.
Dieser redundante GlusterFS-Cluster wird aus zwei Ubuntu 20.04-Servern bestehen.
Er wird sich ähnlich wie ein NAS-Server mit gespiegeltem RAID verhalten.
Dann werden Sie auf den Cluster über einen dritten Ubuntu 20.04-Server zugreifen, der als GlusterFS-Client konfiguriert ist.
Anmerkung zur sicheren Ausführung von GlusterFS
Wenn Sie einem GlusterFS-Volume Daten hinzufügen, werden diese Daten mit jedem Gerät im Speicherpool, in dem das Volumen gehostet wird, synchronisiert.
Dieser Datenverkehr zwischen Knoten wird standardmäßig nicht verschlüsselt, d. h. es besteht das Risiko, dass er von bösartigen Akteuren abgefangen wird.
Wenn Sie GlusterFS in der Produktion verwenden möchten, wird daher empfohlen, das Dateisystem in einem isolierten Netzwerk auszuführen.
Sie könnten GlusterFS beispielsweise so einrichten, dass es in einer Virtual Private Cloud (VPC) oder mit einem VPN zwischen den einzelnen Knoten ausgeführt wird.
< $> Hinweis: Wenn Sie GlusterFS in DigitalOcean bereitstellen möchten, können Sie es in einem isolierten Netzwerk einrichten, indem Sie Ihre Serverinfrastruktur einer DigitalOcean Virtual Private Cloud hinzufügen.
Details zur entsprechenden Einrichtung finden Sie in unserer VPC-Produktdokumentation.
Um diesem Tutorial zu folgen, benötigen Sie drei Server, auf denen Ubuntu 20.04 ausgeführt wird.
Jeder dieser Server sollte über einen Nicht-root-Benutzer mit Administratorberechtigungen und eine mit UFW konfigurierte Firewall verfügen.
< $> note Anmerkung: Wie im Abschnitt "Ziele" erwähnt, wird dieses Tutorial Sie durch die Konfiguration von zwei Ubuntu-Servern als Server in einem Speicherpool und dem dritten Server als Client begleiten; diesen werden Sie für Zugriff auf die beiden Speicherknoten verwenden.
Aus Gründen der Einfachheit wird sich das Tutorial auf diese Computer mit folgenden Hostnamen beziehen:
Rolle im Speicherpool
gluster0
Server
gluster1
gluster2
Client
Befehle, die entweder auf gluster0 oder gluster1 ausgeführt werden müssen, weisen einen blauen bzw. roten Hintergrund auf:
Befehle, die nur auf dem Client (gluster2) ausgeführt werden müssen, haben einen grünen Hintergrund:
Befehle, die auf mehr als einem Computer ausgeführt werden können oder müssen, weisen einen grauen Hintergrund auf:
Schritt 1 - Konfigurieren der DNS-Auflösung auf jedem Computer
Das Erstellen einer Auflösung von Hostnamen zwischen den einzelnen Computern kann Ihnen bei der Verwaltung Ihres Gluster-Speicherpools helfen.
Wenn Sie in diesem Tutorial später in einem gluster-Befehl auf einen Ihrer Computer verweisen, können Sie dies dann mit einem leicht zu merkenden Domänennamen oder sogar einem Spitznamen anstelle der jeweiligen IP-Adresse tun.
Wenn Sie keinen freien Domänennamen haben oder einfach nur eine schnelle Einrichtung vornehmen möchten, können Sie stattdessen die Datei / etc / hosts auf den einzelnen Computern bearbeiten.
Dies ist eine spezielle Datei auf Linux-Computern, in der Sie das System statisch konfigurieren können, um alle in der Datei enthaltenen Hostnamen in Form statischer IP-Adressen aufzulösen.
< $> note Anmerkung: Wenn Sie Ihre Server zur Authentifizierung mit einer Domäne konfigurieren möchten, die Ihnen gehört, müssen Sie sich zunächst einen Domänennamen von einer Domänenregistrierungstelle wie Namecheap oder Enom verschaffen und dann die entsprechenden DNS-Einträge konfigurieren.
Sobald Sie für jeden Server einen A-Eintrag konfiguriert haben, können Sie mit Schritt 2 fortfahren. Stellen Sie sicher, dass Sie gluster < ^ > N < ^ > .example.com und gluster < ^ > N < ^ > durch den Domänennamen ersetzen, der auf den jeweiligen im Beispielbefehl verwiesenen Server auflöst.
< $> info Wenn Sie Ihre Infrastruktur von DigitalOcean erhalten haben, könnten Sie Ihren Domänennamen DigitalOcean hinzufügen und für jeden Ihrer Server einen eindeutigen A-Eintrag erstellen.
Öffnen Sie die Datei mit root-Berechtigungen mit einem Texteditor Ihrer Wahl auf jedem Ihrer Computer.
Standardmäßig wird die Datei in etwa so aussehen (mit entfernten Kommentaren):
Fügen Sie auf einem Ihrer Ubuntu-Server unter der lokalen Hostdefinition die IP-Adresse der einzelnen Server hinzu, gefolgt von allen Namen, die Sie verwenden möchten, um auf sie in Befehlen verweisen zu können.
Im folgenden Beispiel erhält jeder Server einen langen Hostnamen, der auf gluster < ^ > N < ^ > .example.com abgestimmt ist, und einen kurzen Hostnamen, der auf gluster < ^ > N < ^ > abgestimmt ist.
Sie können die Abschnitte gluster < ^ > N < ^ > .example.com und gluster < ^ > N < ^ > jeder Zeile in einen beliebigen Namen - oder durch einzelne Leerzeichen getrennte Namen - ändern, die Sie für den Zugriff auf einzelne Server verwenden möchten.
Beachten Sie jedoch, dass in diesem Tutorial durchgehend die folgenden Beispiele verwenden werden:
< $> note Anmerkung: Wenn Ihre Server Teil eines Infrastrukturpools vom Typ Virtual Private Cloud sind, sollten Sie in der Datei / etc / hosts die privaten IP-Adressen der einzelnen Server anstelle ihrer öffentlichen IP-Adressen verwenden.
Wenn Sie fertig damit sind, der Datei / etc / hosts eines Computers diese neuen Zeilen hinzuzufügen, kopieren Sie die Zeilen und fügen Sie sie den / etc / hosts-Dateien auf Ihren anderen Computern hinzu.
Jede / etc / hosts-Datei sollte dieselben Zeilen enthalten und die IP-Adressen Ihrer Server mit den ausgewählten Namen verknüpfen.
Wenn Sie nano verwendet haben, drücken Sie STRG + X, Y und dann ENTER ​ ​ ​.
Nachdem Sie die Auflösung der Hostnamen zwischen den einzelnen Servern konfiguriert haben, können Sie Befehle leichter ausführen, wenn Sie später einen Speicherpool und ein Volume einrichten.
Als Nächstes führen Sie einen weiteren Schritt aus, der auf jedem Ihrer Server abgeschlossen werden muss.
Und zwar fügen Sie jedem Ihrer drei Ubuntu-Server das offizielle Personal Package Archive (PPA) des Gluster-Projekts hinzu, um dafür zu sorgen, dass Sie die neueste Version von GlusterFS installieren können.
Schritt 2 - Einrichten von Softwarequellen auf jedem Computer
Zwar enthalten die standardmäßigen Ubuntu 20.04-APT-Repositorys GlusterFS-Pakete, doch handelt es sich dabei zum Zeitpunkt der Verfassung dieses Dokuments nicht um die aktuellsten Versionen.
Eine Möglichkeit, die neueste stabile Version von GlusterFS (zum Zeitpunkt der Verfassung dieses Dokuments Version < ^ > 7.6 < ^ >) zu installieren, besteht darin, jedem Ihrer drei Ubuntu-Server das offizielle PPA des Gluster-Projekts hinzuzufügen.
Fügen Sie das PPA für die GlusterFS-Pakete hinzu, indem Sie auf jedem Server folgenden Befehl ausführen:
Drücken Sie ENTER, wenn Sie dazu aufgefordert werden, um zu bestätigen, dass Sie das PPA tatsächlich hinzufügen möchten.
Aktualisieren Sie nach dem Hinzufügen des PPA den lokalen Paketindex der einzelnen Server.
Dadurch wird sich jeder Server der neu verfügbaren Pakete bewusst:
Nachdem Sie das offizielle PPA des Gluster-Projekts den einzelnen Servern hinzugefügt und den lokalen Paketindex aktualisiert haben, können Sie die erforderlichen GlusterFS-Pakete installieren.
Da zwei Ihrer drei Computer als Gluster-Server und der dritte Computer als Client fungieren werden, müssen Sie jedoch zwei separate Installations- und Konfigurationsverfahren befolgen.
Zuerst installieren und richten Sie die Serverkomponenten ein.
Schritt 3 - Installieren von Serverkomponenten und Erstellen eines Trusted Storage Pool
Ein Speicherpool ist eine beliebige Menge an Speicherkapazität, die aus mehr als einer Speicherquelle aggregiert wird.
In diesem Schritt konfigurieren Sie zwei Ihrer Server - gluster0 und gluster1 - als Clusterkomponenten.
Installieren Sie sowohl auf gluster0 als auch gluster1 das Paket für GlusterFS-Server, indem Sie Folgendes eingeben:
Drücken Sie auf Aufforderung Y und dann ENTER, um die Installation zu bestätigen.
Der Installationsprozess konfiguriert GlusterFS automatisch so, dass eine Ausführung als systemd-Dienst erfolgt.
Er sorgt jedoch nicht für einen automatischen Start des Diensts oder das Aktivieren zum Ausführen zur Startzeit.
Um glusterd (den GlusterFS-Dienst) zu starten, führen Sie den Befehl systemctl start sowohl auf gluster0 als auch gluster1 aus:
Führen Sie dann folgenden Befehl auf beiden Servern aus.
Dadurch wird der Dienst jedes Mal gestartet, wenn der Server gestartet wird:
Anschließend können Sie den Status des Diensts auf einem oder beiden Servern überprüfen:
Wenn der Dienst erfolgreich ausgeführt wird, erhalten Sie eine Ausgabe, die wie folgt aussieht:
Wenn Sie dem Leitfaden zur Ersteinrichtung des Servers gefolgt sind, haben Sie auf jedem Ihrer Computer eine Firewall mit UFW eingerichtet.
Aus diesem Grund müssen Sie die Firewall für jeden Knoten öffnen, bevor Sie eine Verbindung zwischen ihnen herstellen und einen Speicherpool einrichten können.
Der Gluster-Daemon nutzt Port 24007, sodass Sie jedem Knoten über die Firewall der einzelnen Knoten in Ihrem Speicherpool Zugriff auf den Port gewähren müssen.
Führen Sie dazu folgenden Befehl auf gluster0 aus.
Denken Sie daran, < ^ > gluster1 _ ip _ address < ^ > in die IP-Adresse von gluster1 zu ändern:
Führen Sie dann folgenden Befehl auf gluster1 aus.
Vergessen Sie auch hier nicht, < ^ > gluster0 _ ip _ address < ^ > in die IP-Adresse von gluster0 zu ändern:
Außerdem müssen Sie Ihrem Clientcomputer (gluster2) Zugriff auf diesen Port gewähren. Andernfalls werden Sie später Probleme haben, wenn Sie versuchen, das Volumen bereitzustellen.
Führen Sie sowohl auf gluster0 als auch gluster1 folgenden Befehl aus, um diesen Port für Ihren Clientcomputer zu öffnen:
Um sicherzustellen, dass keine anderen Computer auf einem der Server auf den Port von Gluster zugreifen können, fügen Sie dann die folgende Rahmenregel deny sowohl gluster0 als auch gluster1 hinzu:
Sie können nun eine Verbindung zwischen gluster0 und gluster1 herstellen.
Dazu müssen Sie auf einem Ihrer Knoten den Befehl gluster peer probe ausführen.
Es spielt dabei keine Rolle, welchen Knoten Sie verwenden. Das folgende Beispiel veranschaulicht die Ausführung des Befehls auf gluster0:
Dieser Befehl weist gluster0 im Wesentlichen an, gluster1 zu vertrauen und als Teil seines Speicherpools zu registrieren.
Wenn der Test erfolgreich war, wird folgende Ausgabe zurückgegeben:
Sie können jederzeit überprüfen, ob die Knoten miteinander kommunizieren, indem Sie auf einem der Knoten den Befehl gluster peer status ausführen.
In diesem Beispiel wird er auf gluster1 ausgeführt:
Wenn Sie diesen Befehl auf gluster1 ausführen, wird eine Ausgabe angezeigt, die wie folgt aussieht:
An diesem Punkt kommunizieren Ihre beiden Server miteinander und sind bereit, gemeinsam Speichervolumes zu erstellen.
Schritt 4 - Einrichten eines Speichervolumes
Denken Sie daran, dass das primäre Ziel dieses Tutorials in der Einrichtung eines redundanten Speicherpools besteht.
Dazu richten Sie ein Volume mit Replikatfunktion ein, damit Sie mehrere Kopien Ihrer Daten speichern und verhindern können, dass Ihr Cluster einen Single Point of Failure aufweist.
Um ein Volume zu erstellen, verwenden Sie den Befehl gluster volume create mit dieser allgemeinen Syntax:
Das bedeuten die Argumente und Optionen des Befehls gluster volume create:
< ^ > volume _ name < ^ >: Das ist der Name, mit dem Sie nach der Erstellung auf das Volume verweisen.
Der folgende Beispielbefehl sorgt für die Erstellung eines Volumes namens volume1.
replica < ^ > number _ of _ servers < ^ >: Nach dem Namen des Volumes können Sie festlegen, welche Art von Volume Sie erstellen möchten.
Denken Sie daran, dass das Ziel dieses Tutorials darin besteht, einen redundanten Speicherpool einzurichten, sodass wir den Volume-Typ replica wählen.
Dies erfordert ein Argument, mit dem angegeben wird, auf wie viele Server die Daten des Volumes repliziert werden sollen (in diesem Tutorial 2).
< ^ > domain1.com: /... < ^ > und < ^ > domain2.com: /... < ^ >: Diese definieren die Computer und den Speicherort des Verzeichnisses der Bricks (eine GlusterFS-Bezeichnung für die grundlegende Speichereinheit des Systems), was alle Verzeichnisse auf allen Computern umfasst, die als Teil oder Kopie eines größeren Volumes dienen. So entsteht volume1.
Im folgenden Beispiel wird im root-Verzeichnis beider Server ein Verzeichnis namens gluster-storage erstellt.
force: Diese Option sorgt für das Überschreiben aller Warnungen oder Optionen, die sonst auftreten und die Erstellung des Volumes unterbrechen würden.
Anhand der in diesem Tutorial zuvor aufgeführten Konventionen können Sie diesen Befehl zur Erstellung eines Volumes ausführen.
Beachten Sie, dass Sie den Befehl entweder auf gluster0 oder gluster1 ausführen können:
Wenn das Volume erfolgreich erstellt wurde, erhalten Sie folgende Ausgabe:
An diesem Punkt wurde Ihr Volume bereits erstellt, ist aber noch nicht aktiv.
Sie können das Volume starten und zur Verwendung bereitstellen, indem Sie folgenden Befehl ausführen (erneut auf einem Ihrer beiden Gluster-Server):
Wenn das Volume korrekt gestartet wurde, erhalten Sie folgende Ausgabe:
Überprüfen Sie als Nächstes, ob das Volume online ist.
Führen Sie auf einem Ihrer Knoten folgenden Befehl aus:
Dadurch wird eine Ausgabe zurückgegeben, die der folgenden ähnelt:
Laut dieser Ausgabe sind die Bricks auf beiden Servern online.
Als letzter Schritt zur Konfiguration Ihres Volumes müssen Sie die Firewall auf beiden Servern öffnen, damit Ihr Clientcomputer in der Lage ist, sich mit dem Volume zu verbinden und das Volume bereitzustellen.
Gemäß der Beispielausgabe des vorherigen Befehls wird volume1 auf beiden Computern an Port 49152 ausgeführt.
Dies ist der Standardport von GlusterFS für das erste Volumen. Weitere Volumes werden also Port 49153, dann 49154 usw. verwenden.
Führen Sie sowohl auf gluster0 als auch gluster1 folgenden Befehl aus, um gluster2 über die jeweilige Firewall Zugriff auf diesen Port zu gewähren:
Fügen Sie dann für zusätzliche Sicherheit eine weitere deny-Rahmenregel für den Port des Volumes hinzu - sowohl auf gluster0 als auch gluster1.
Dadurch wird sichergestellt, dass auf beiden Servern keine anderen Computer außer Ihrem Client auf das Volume zugreifen können:
Nachdem Ihr Volume nun ausgeführt wird, können Sie Ihren Clientcomputer einrichten und remote nutzen.
Schritt 5 - Installieren und Konfigurieren von Clientkomponenten
Ihr Volume ist nun konfiguriert und zur Verwendung durch Ihren Clientcomputer verfügbar.
Bevor Sie beginnen, müssen Sie jedoch das Paket glusterfs-client aus dem PPA installieren, das Sie in Schritt 1 auf Ihrem Clientcomputer eingerichtet haben.
Die Abhängigkeiten dieses Pakets umfassen einige gemeinsame Bibliotheken und Übersetzermodule von GlusterFS sowie die für die Arbeit erforderlichen FUSE-Tools.
Führen Sie folgenden Befehl auf gluster2 aus:
Sie werden Ihr Remote-Speichervolume in Kürze auf Ihrem Clientcomputer bereitstellen.
Bevor Sie dies tun können, müssen Sie einen Bereitstellungspunkt erstellen.
Traditionell befindet sich dieser im Verzeichnis / mnt, doch kann jeder beliebige Ort verwendet werden.
Erstellen Sie aus Gründen der Einfachheit auf Ihrem Clientcomputer ein Verzeichnis namens / storage-pool als Bereitstellungspunkt.
Dieser Verzeichnisname beginnt mit einem Schrägstrich (/), der es im root-Verzeichnis platziert. Daher müssen Sie das Verzeichnis mit sudo-Berechtigungen erstellen:
Jetzt können Sie das Remotevolume bereitstellen.
Werfen Sie zuvor einen Blick auf die Syntax des Befehls mount, den Sie dazu verwenden werden:
mount ist ein Dienstprogramm in vielen Unix-ähnlichen Betriebssystemen.
Es dient dazu, Dateisysteme (ob externe Speichergeräte wie SD-Karten bzw. USB-Sticks oder NAS-Systeme wie im Fall dieses Tutorials) im vorhandenen Dateisystem des Computers in Verzeichnissen bereitzustellen.
Die von Ihnen verwendete mount-Befehlssyntax umfasst die Option -t, die drei Argumente erfordert: den Typ des Dateisystems, der bereitgestellt werden soll, das Gerät, auf dem sich das bereitzustellende Dateisystem befindet, und das Verzeichnis auf dem Client, in dem Sie das Volume bereitstellen möchten.
Beachten Sie, dass das Geräteargument in dieser Beispielsyntax auf einen Hostnamen verweist, gefolgt von einem Doppelpunkt und dann dem Namen des Volumes.
GlusterFS abstrahiert die tatsächlichen Speicherverzeichnisse auf jedem Host, was bedeutet, dass dieser Befehl nicht das Verzeichnis / gluster-storage, sondern vielmehr das Volume volume1 bereitstellt.
Beachten Sie außerdem, dass Sie nur ein Mitglied des Speicherclusters angeben müssen.
Dies kann einer der beiden Knoten sein, da der GlusterFS-Dienst sie als einen Computer behandelt.
Führen Sie auf Ihrem Clientcomputer (gluster2) folgenden Befehl aus, um das Volume im von Ihnen erstellten Verzeichnis / storage-pool bereitzustellen:
Führen Sie danach den Befehl df aus.
Dadurch wird für Dateisysteme, auf die der aufrufende Benutzer Zugriff hat, der verfügbare Speicherplatz angezeigt:
Dieser Befehl zeigt an, dass das GlusterFS-Volume am richtigen Ort bereitgestellt wurde:
Jetzt können Sie mit der Prüfung fortfahren, ob alle Daten, die Sie in das Volume auf Ihrem Client schreiben, wie erwartet auf Ihren Serverknoten repliziert werden.
Schritt 6 - Testen von Redundanzfunktionen
Nachdem Sie Ihren Client zur Verwendung des Speicherpools und Volumes eingerichtet haben, können Sie seine Funktionalität testen.
Navigieren Sie auf Ihrem Clientcomputer (gluster2) zum im vorherigen Schritt definierten Bereitstellungspunkt:
Erstellen Sie dann einige Testdateien.
Der folgende Befehl erstellt in Ihrem Speicherpool zehn separate leere Dateien:
Wenn Sie sich die zuvor auf den einzelnen Speicherhosts definierten Speicherverzeichnisse ansehen, werden Sie feststellen, dass alle diese Dateien in jedem System vorhanden sind.
Auf gluster0:
Und auch auf gluster1:
Wie diese Ausgaben zeigen, wurden auch die Testdateien, die Sie dem Client hinzugefügt haben, in beide Knoten geschrieben.
Sollte jemals einer der Knoten in Ihrem Speichercluster ausfallen, kann es vorkommen, dass er nicht mehr mit dem Speicherpool synchron ist, wenn Änderungen am Dateisystem vorgenommen werden.
Wenn der Knoten wieder online ist, können Sie durch Ausführung eines Lesevorgangs am Bereitstellungspunkt des Clients den Knoten auf fehlende Dateien aufmerksam machen:
Nachdem Sie verifiziert haben, dass Ihr Speichervolumen korrekt bereitgestellt wurde und Sie Daten an beiden Computer im Cluster replizieren können, können Sie den Zugriff auf den Speicherpool sperren.
Schritt 7 - Beschränken der Redundanzfunktionen
Gegenwärtig kann sich jeder Computer ganz ohne Einschränkungen mit Ihrem Speichervolume verbinden.
Sie können das ändern, indem Sie die Option auth.allow festlegen, um die IP-Adressen der einzelnen Clients zu definieren, die Zugriff auf das Volume haben sollen.
Wenn Sie die Konfiguration / etc / hosts verwenden, werden die Namen, die Sie für die Server festgelegt haben, nicht korrekt geroutet.
Sie müssen stattdessen eine statische IP-Adresse verwenden.
Wenn Sie jedoch DNS-Einträge verwenden, wird hier der Domänenname, den Sie konfiguriert haben, funktionieren.
Führen Sie auf einem Ihrer beiden Speicherknoten (gluster0 oder gluster1) folgenden Befehl aus:
Wenn der Befehl erfolgreich abgeschlossen wird, gibt er folgende Ausgabe zurück:
Wenn Sie die Einschränkung irgendwann entfernen möchten, können Sie Folgendes eingeben:
Dadurch werden wieder Verbindungen von beliebigen Computern aus möglich.
Dies ist nicht sicher, kann aber für die Fehlerbehebung nützlich sein.
Wenn Sie über mehrere Clients verfügen, können Sie ihre IP-Adressen oder Domänennamen gleichzeitig angeben (je nachdem, ob Sie / etc / hosts oder die Auflösung von DNS-Hostnamen verwenden), getrennt durch Kommas:
Ihr Speicherpool ist nun konfiguriert, gesichert und einsatzbereit.
Als Nächstes werden Sie einige Befehle kennen lernen, die Ihnen helfen, Informationen über den Status Ihres Speicherpools zu erhalten.
Schritt 8 - Abrufen von Informationen über den Speicherpool mit GlusterFS-Befehlen
Wenn Sie bestimmte Einstellungen für Ihren GlusterFS-Speicher ändern, können Sie den Überblick darüber verlieren, welche Optionen Sie zur Verfügung haben, welche Volumes live sind und welche Knoten mit einzelnen Volumes verknüpft sind.
Es gibt verschiedene Befehle, die auf Ihren Knoten verfügbar sind, mit denen Sie diese Daten abrufen und mit Ihrem Speicherpool interagieren können.
Wenn Sie Informationen über die einzelnen Volumes wünschen, führen Sie den Befehl gluster volume info aus:
Um Informationen über Peers zu erhalten, mit denen dieser Knoten verbunden ist, können Sie Folgendes eingeben:
Wenn Sie genaue Informationen zur Ausführung einzelner Knoten wünschen, können Sie ein Profil für ein Volume erstellen, indem Sie Folgendes eingeben:
Nach erfolgreicher Ausführung dieses Befehls können Sie die gesammelten Informationen abrufen, indem Sie Folgendes eingeben:
Führen Sie wie zuvor gezeigt den Befehl gluster volume status aus, um eine Liste aller zu GlusterFS zugehörigen Komponenten zu erhalten, die auf den einzelnen Knoten ausgeführt werden:
Wenn Sie Ihre GlusterFS-Speichervolumes verwalten möchten, kann es eine gute Idee sein, die GlusterFS-Konsole zu nutzen.
Dadurch können Sie mit Ihrer GlusterFS-Umgebung interagieren, ohne zunächst sudo gluster eingeben zu müssen:
Daraufhin wird eine Eingabeaufforderung angezeigt, in der Sie Ihre Befehle eingeben können. help (Hilfe) ist eine gute Methode, um sich einen Überblick zu verschaffen:
Führen Sie anschließend exit aus, um die Gluster-Konsole zu verlassen:
Nun können Sie damit beginnen, GlusterFS mit der nächsten Anwendung zu integrieren.
Durch Absolvieren dieses Tutorials haben Sie ein redundantes Speichersystem eingerichtet, mit dem Sie gleichzeitig auf zwei separate Server schreiben können.
Das kann für verschiedene Anwendungen nützlich sein und dafür sorgen, dass Ihre Daten verfügbar bleiben, auch wenn ein Server ausfällt.
Installieren und Sichern von Grafana unter Ubuntu 20.04
5741
Grafana ist ein Open-Source-basiertes Visualisierungs- und Überwachungstool für Daten, das sich mit komplexen Daten aus Quellen wie Prometheus, InfluxDB, Graphite und ElasticSearch integrieren lässt.
Mit Grafana können Sie Warnungen, Benachrichtigungen und Ad-hoc-Filter für Ihre Daten erstellen und durch integrierte Freigabefunktionen die Zusammenarbeit mit Teamkollegen erleichtern.
In diesem Tutorial installieren Sie Grafana und sichern die Anwendung mit einem SSL-Zertifikat und einem Nginx-Reverseproxy.
Nach der Einrichtung von Grafana haben Sie die Möglichkeit, Benutzerauthentifizierung über GitHub zu konfigurieren, um die Berechtigungen Ihres Teams besser zu organisieren.
Zwei Ubuntu 20.04-Server, die gemäß des Leitfadens zur Ersteinrichtung des Servers für Ubuntu 20.04 eingerichtet wurde, einschließlich eines non-root users, der über sudo-Berechtigungen verfügt, und einer mit ufw konfigurierten Firewall.
In diesem Tutorial wird durchgehend < ^ > your _ domain < ^ > verwendet.
Richten Sie die folgenden DNS-Einträge für Ihren Server ein.
Sie können Einrichten eines Hostnamens mit DigitalOcean konsultieren, um mehr über das Hinzufügen von Hostnamen zu erfahren.
Nginx, eingerichtet anhand des Tutorials zum Installieren von Nginx unter Ubuntu 20.04, einschließlich eines Serverblocks für Ihre Domäne.
Einen Nginx-Serverblock mit konfiguriertem Let 's Encrypt; folgen Sie zum Einrichten dieser Komponente Sichern von Nginx mit Let' s Encrypt unter Ubuntu 20.04.
Optional benötigen Sie zur Einrichtung der GitHub-Authentifizierung ein GitHub-Konto, das mit einer Organisation verknüpft ist.
Schritt 1 - Installieren von Grafana
In diesem ersten Schritt installieren Sie Grafana auf Ihrem Ubuntu 20.04-Server.
Sie können Grafana installieren, indem Sie entweder die Anwendung direkt von der offiziellen Website herunterladen oder ein APT-Repository verwenden.
Da ein APT-Repository die Installation und Verwaltung von Grafana-Updates erleichtert, wenden wir in diesem Tutorial diese Methode an.
Laden Sie mit wget den GPG-Schlüssel von Grafana herunter und übergeben Sie dann die Ausgabe an apt-key.
Dadurch wird der Schlüssel zur Liste der vertrauenswürdigen Schlüssel Ihrer APT-Installation hinzugefügt. Nun können Sie das GPG-signierte Grafana-Paket herunterladen und verifizieren:
In diesem Befehl deaktiviert die Option -q die Nachricht zur Statusaktualisierung für wget, während -O die Datei ausgibt, die Sie in das Terminal heruntergeladen haben.
Diese beiden Optionen sorgen dafür, dass nur die Inhalte der heruntergeladenen Datei an apt-key übertragen werden.
Fügen Sie als Nächstes das Grafana-Repository zu Ihren APT-Quellen hinzu:
Aktualisieren Sie dann den APT-Cache, um Ihre Paketlisten zu aktualisieren:
Sie können nun mit der Installation fortfahren:
Sobald Grafana installiert ist, verwenden Sie systemctl, um den Grafana-Server zu starten:
Überprüfen Sie als Nächstes, ob Grafana ausgeführt wird, indem Sie den Status des Diensts prüfen:
Diese Ausgabe enthält Informationen über den Prozess von Grafana, einschließlich seines Status, Main Process Identifier (PID) und mehr. < ^ > active (running) < ^ > bedeutet, dass der Prozess richtig ausgeführt wird.
Abschließend aktivieren Sie den Dienst, damit Grafana beim Booten automatisch gestartet wird:
Dadurch wird bestätigt, dass systemd die erforderlichen symbolischen Links für den Autostart von Grafana erstellt hat.
Grafana ist nun installiert und einsatzbereit.
Als Nächstes sichern Sie Ihre Verbindung zu Grafana mit einem Reverseproxy und SSL-Zertifikat.
Schritt 2 - Einrichten des Reverseproxy
Durch Verwendung eines SSL-Zertifikats wird sichergestellt, dass Ihre Daten sicher sind, indem die Verbindung zu und von Grafana verschlüsselt wird.
Um diese Verbindung nutzen zu können, müssen Sie jedoch zunächst Nginx als Reverseproxy für Grafana neu konfigurieren.
Öffnen Sie die Nginx-Konfigurationsdatei, die Sie beim Einrichten des Nginx-Serverblocks mit Let 's Encrypt in den Voraussetzungen erstellt haben.
Sie können einen beliebigen Texteditor verwenden, für dieses Tutorial nutzen wir allerdings nano:
Suchen Sie nach dem folgenden Block:
Da Sie Nginx bereits für Kommunikation über SSL konfiguriert haben und der gesamte Webverkehr auf Ihrem Server schon über Nginx läuft, müssen Sie Nginx nur noch dazu anweisen, alle Anfragen an Grafana (standardmäßig an Port 3000) weiterzuleiten.
Löschen Sie in diesem location block die vorhandene Zeile try _ files und ersetzen Sie sie durch die folgende Option proxy _ pass:
Dadurch wird der Proxy dem entsprechenden Port zugeordnet. Sobald Sie damit fertig sind, speichern und schließen Sie die Datei, indem Sie STRG + X, Y und dann die Eingabetaste drücken (wenn Sie nano verwenden).
Testen Sie nun die neuen Einstellungen, um sich zu vergewissern, dass alles korrekt konfiguriert wurde:
Aktivieren Sie schließlich die Änderungen, indem Sie Nginx neu laden:
Sie können nun das Standardanmeldefenster von Grafana aufrufen, indem Sie in Ihren Webbrowser https: / / < ^ > your _ domain < ^ > eingeben.
Wenn Grafana nicht erreichbar ist, prüfen Sie, ob Ihre Firewall so eingerichtet ist, dass Datenverkehr an Port 443 zugelassen wird, und befolgen Sie dann erneut die vorherigen Anweisungen.
Nachdem Sie die Verbindung zu Grafana verschlüsselt haben, können Sie nun zusätzliche Sicherheitsmaßnahmen implementieren, angefangen beim Ändern der standardmäßigen Anmeldedaten für den Grafana-Administrator.
Schritt 3 - Aktualisieren von Anmeldedaten
Da jede Grafana-Installation standardmäßig dieselben Administrator-Anmeldedaten nutzt, sollten Sie die Anmeldedaten so schnell wie möglich ändern.
In diesem Schritt aktualisieren Sie die Anmeldedaten zur Verbesserung der Sicherheit.
Navigieren Sie zunächst über Ihren Webbrowser zu https: / / < ^ > your _ domain < ^ >.
Dadurch wird das standardmäßige Anmeldefenster mit folgenden Elementen angezeigt: einem Formular, das Sie dazu auffordert, eine E-Mail-Adresse oder Benutzername und ein Passwort einzugeben, einer Schaltfläche zum Anmelden sowie einem Link namens Passwort vergessen?
Grafana-Anmeldung
Geben Sie admin sowohl in das Feld E-Mail-Adresse oder Benutzername und Passwort ein und klicken Sie dann auf die Schaltfläche Anmelden.
Im nächsten Fenster werden Sie aufgefordert, die Sicherheit Ihres Kontos zu erhöhen, indem Sie das Standardpasswort ändern:
Passwort ändern
Geben Sie das Passwort, das Sie verwenden möchten, in die Felder Neues Passwort und Neues Passwort bestätigen ein.
Jetzt können Sie auf Übermitteln klicken, um die neuen Daten zu speichern, oder auf Überspringen, um diesen Schritt zu überspringen.
Wenn Sie den Schritt überspringen, werden Sie dazu aufgefordert, das Passwort bei der nächsten Anmeldung zu ändern.
Um die Sicherheit Ihrer Grafana-Konfiguration zu erhöhen, klicken Sie auf Übermitteln.
Sie gelangen zum Dashboard Willkommen bei Grafana:
Start-Dashboard
Sie haben Ihr Konto sicherer gemacht, indem Sie die Standardanmeldedaten geändert haben.
Als Nächstes werden Sie Änderungen an Ihrer Grafana-Konfiguration vornehmen, damit ohne Ihre Genehmigung niemand ein neues Grafana-Konto erstellen kann.
Schritt 4 - Deaktivieren von Grafana-Registrierungen und anonymem Zugriff
Grafana bietet Optionen, mit denen Besucher für sich selbst Benutzerkonten erstellen und eine Vorschau von Dashboards anzeigen können, ohne sich registrieren zu müssen.
Wenn Grafana nicht über das Internet zugänglich ist oder mit öffentlich verfügbaren Daten wie Dienststatus arbeitet, können Sie diese Funktionen ggf. zulassen.
Wenn Sie Grafana jedoch online verwenden, um mit sensiblen Daten zu arbeiten, kann anonymer Zugriff ein Sicherheitsproblem darstellen.
Um dieses Problem zu beheben, nehmen Sie einige Änderungen an Ihrer Grafana-Konfiguration vor.
Öffnen Sie zunächst die Hauptkonfigurationsdatei von Grafana zur Bearbeitung:
Suchen Sie die folgende Direktive allow _ sign _ up unter der Überschrift [users]:
Wenn Sie diese Direktive mit true aktivieren, wird dem Anmeldefenster eine Schaltfläche zum Anmelden hinzugefügt, damit sich Benutzer registrieren und auf Grafana zugreifen können.
Durch Deaktivieren dieser Direktive mit false wird die Schaltfläche zum Anmelden entfernt, was die Sicherheit und den Datenschutz von Grafana erhöht.
Heben Sie die Kommentierung dieser Direktive auf, indem Sie das; -Zeichen am Anfang der Zeile entfernen und dann die Option auf false setzen:
Als Nächstes suchen Sie nach der folgenden aktivierten Direktive unter der Überschrift [auth.anonymous]:
Wenn Sie enabled auf true setzen, erhalten nicht registrierte Benutzer Zugriff auf Ihre Dashboards; wenn Sie die Option auf false setzen, wird der Zugriff auf das Dashboard auf registrierte Benutzer beschränkt.
Heben Sie die Kommentierung dieser Direktive auf, indem Sie das; -Zeichen am Anfang der Zeile entfernen und die Option dann auf false setzen:
Um die Änderungen zu aktivieren, starten Sie Grafana neu:
Vergewissern Sie sich, ob alles funktioniert, indem Sie den Dienststatus von Grafana überprüfen:
Wie zuvor wird die Ausgabe melden, dass Grafana active (running) ist.
Rufen Sie nun https: / / < ^ > your _ domain < ^ > in Ihrem Webbrowser auf.
Um zum Fenster zum Anmelden zurückzukehren, bewegen Sie den Cursor links unten im Bildschirm zu Ihrem Avatar und klicken Sie auf die Option Abmelden, die angezeigt wird.
Vergewissern Sie sich nach dem Abmelden, dass es keine Schaltfläche zum Anmelden gibt und Sie sich nicht anmelden können, ohne Anmeldedaten einzugeben.
Nun ist Grafana vollständig konfiguriert und einsatzbereit.
Als Nächstes können Sie den Anmeldeprozess für Ihre Organisation vereinfachen, indem Sie über GitHub authentifizieren.
(Optional) Schritt 5 - Einrichten einer GitHub OAuth-Anwendung
Für einen alternativen Ansatz zur Anmeldung können Sie Grafana für eine Authentifizierung über GitHub konfigurieren, womit allen Mitgliedern autorisierter GitHub-Organisationen Zugriff gewährt wird.
Dies kann besonders nützlich sein, wenn Sie möchten, dass verschiedene Entwickler zusammenarbeiten und Metriken aufrufen können, ohne spezielle Anmeldedaten für Grafana erstellen zu müssen.
Melden Sie sich zunächst bei einem GitHub-Konto, das mit Ihrer Organisation verknüpft ist, an und navigieren Sie dann zu Ihrer GitHub-Profilseite unter https: / / github.com / settings / profile.
Wechseln Sie den Einstellungskontext, indem Sie auf der linken Seite des Bildschirms auf Ihren Namen klicken und dann im Dropdown-Menü Ihre Organisation auswählen.
Dadurch wird der Kontext von Persönliche Einstellungen in Organisationseinstellungen geändert.
Im nächsten Bildschirm sehen Sie Ihr Organisationsprofil, in dem Sie Einstellungen wie den Anzeigenamen der Organisation, die E-Mail-Adresse der Organisation und die URL der Organisation ändern können.
Da Grafana OAuth, einen offenen Standard zum Gewähren von Zugriff auf lokale Ressourcen an andere Remotebenutzer und Authentifizieren von Benutzern über GitHub nutzt, müssen Sie innerhalb von GitHub eine neue OAuth-Anwendung erstellen.
Klicken Sie auf den Link OAuth-Anwendungen unter Entwicklereinstellungen unten links im Bildschirm.
Wenn Sie in GitHub nicht bereits über OAuth-Anwendungen verfügen, die mit Ihrer Organisation verknüpft sind, erhalten Sie den Hinweis, dass keine Anwendungen im Besitz der Organisation sind.
Andernfalls sehen Sie eine Liste der OAuth-Anwendungen, die bereits mit Ihrem Konto verknüpft sind.
Klicken Sie auf die Schaltfläche Neue OAuth-Anwendung, um fortzufahren.
Geben Sie im nächsten Bildschirm folgende Details zu Ihrer Grafana-Installation ein:
Anwendungsname: Damit können Sie Ihre verschiedenen OAuth-Anwendungen voneinander unterscheiden.
URL der Homepage: Dadurch erfährt GitHub, wo sich Grafana befindet.
Geben Sie in dieses Feld https: / / < ^ > your _ domain < ^ > ein, wobei Sie < ^ > your _ domain < ^ > durch Ihre Domäne ersetzen.
Anwendungsbeschreibung: Damit wird eine Beschreibung des Zwecks Ihrer OAuth-Anwendung angegeben.
Callback-URL für Anwendung: Das ist die Adresse, an die Benutzer nach der erfolgreichen Authentifizierung weitergeleitet werden.
Für Grafana muss dieses Feld https: / / < ^ > your _ domain < ^ > / login / github lauten.
Denken Sie daran, dass Grafana-Benutzer, die sich über GitHub anmelden, die Werte sehen, die Sie in den ersten drei vorangehenden Feldern eingegeben haben; geben Sie also sinnvolle und passende Werte ein.
Am Ende wird das Formular in etwa so aussehen:
GitHub Register OAuth-Anwendung
Klicken Sie auf die grüne Schaltfläche Anwendung registrieren.
Sie werden nun auf eine Seite weitergeleitet, die die Client-ID und das Clientgeheimnis enthält, die mit Ihrer neuen OAuth-Anwendung verknüpft sind.
Notieren Sie sich beide Werte, da Sie sie der Hauptkonfigurationsdatei von Grafana hinzufügen müssen, um die Einrichtung abzuschließen.
< $> warning Achtung: Bewahren Sie Ihre Client-ID und das Clientgeheimnis an einem sicheren, nicht öffentlichen Ort auf, da diese Daten als Grundlage für Angriffe dienen können.
Nach der Erstellung Ihrer GitHub OAuth-Anwendung können Sie Grafana nun neu konfigurieren, sodass GitHub zur Authentifizierung genutzt wird.
(Optional) Schritt 6 - Konfigurieren von Grafana als GitHub OAuth-Anwendung
Um die GitHub-Authentifizierung für Ihr Grafana-Setup zu abzuschließen, werden Sie nun einige Änderungen an Ihren Grafana-Konfigurationsdateien vornehmen.
Öffnen Sie zunächst die Grafana-Hauptkonfigurationsdatei.
Suchen Sie die Überschrift [auth.github] und heben Sie die Kommentierung dieses Abschnitts auf, indem Sie das; -Zeichen am Anfang der einzelnen Zeilen entfernen (außer bei; allowed _ domains = und; team _ ids =, die in diesem Tutorial nicht geändert werden).
Nehmen Sie als Nächstes folgende Änderungen vor:
Setzen Sie enabled und allow _ sign _ up auf true.
Dadurch wird die GitHub-Authentifizierung aktiviert und Mitglieder der zugelassenen Organisation können Konten selbst erstellen.
Beachten Sie, dass sich diese Einstellung von der Eigenschaft allow _ sign _ up unter [users] unterscheidet, die Sie in Schritt 4 geändert haben.
Setzen Sie client _ id und client _ secret auf die Werte, die Sie beim Erstellen Ihrer GitHub OAuth-Anwendung erhalten haben.
Verwenden Sie für allowed _ organizations den Namen Ihrer Organisation, damit sich nur Mitglieder Ihrer Organisation bei Grafana registrieren und anmelden können.
Die vollständige Konfiguration wird wie folgt aussehen:
Sie haben Grafana nun alles gesagt, was die Anwendung über GitHub wissen muss.
Zum Abschließen der Einrichtung müssen Sie Umleitungen hinter einem Reverseproxy aktivieren.
Dazu wird ein Wert root _ url unter der Überschrift [server] festgelegt.
Speichern Sie Ihre Konfiguration und schließen Sie die Datei.
Starten Sie Grafana anschließend neu, um die Änderungen zu aktivieren:
Vergewissern Sie sich abschließend, dass der Dienst ausgeführt wird.
Die Ausgabe wird anzeigen, dass der Dienst active (running) ist.
Testen Sie Ihr neues Authentifizierungssystem nun durch Navigieren zu https: / / < ^ > your _ domain < ^ >.
Wenn Sie bereits bei Grafana angemeldet sind, fahren Sie mit der Maus über das Avatar-Logo in der linken unteren Ecke des Bildschirms und klicken Sie im zweiten Menü, das neben Ihrem Namen erscheint, auf Abmelden.
Auf der Anmeldeseite sehen Sie unter der ursprünglichen Schaltfläche zum Anmelden einen neuen Bereich, der eine Schaltfläche zum Anmelden mit GitHub inklusive des GitHub-Logos enthält.
Grafana-Anmeldeseite mit GitHub
Klicken Sie auf die Schaltfläche Anmelden mit GitHub, um an GitHub weitergeleitet zu werden, wo Sie sich bei Ihrem GitHub-Konto anmelden und Ihre Absicht zum Autorisieren von Grafana bestätigen können.
Klicken Sie auf die grüne Schaltfläche < ^ > your\ _ github\ _ organization < ^ > autorisieren.
< $> note Anmerkung: Stellen Sie sicher, dass Ihr GitHub-Konto Mitglied Ihrer zugelassenen Organisation ist und Ihre Grafana-E-Mail-Adresse mit Ihrer GitHub-E-Mail-Adresse übereinstimmt.
Wenn Sie versuchen, sich bei einem GitHub-Konto zu authentifizieren, das kein Mitglied Ihrer zugelassenen Organisation ist, erhalten Sie eine Meldung zu Anmeldung fehlgeschlagen, in der Ihnen Folgendes mitgeteilt wird: Benutzer kein Mitglied einer der erforderlichen Organisationen.
Sie werden nun mit Ihrem vorhandenen Grafana-Konto angemeldet.
Wenn für den Benutzer, als der Sie sich angemeldet haben, noch kein Grafana-Konto vorhanden ist, wird Grafana ein neues Benutzerkonto mit Viewer-Berechtigungen erstellen, damit neue Anwender ausschließlich vorhandene Dashboards verwenden können.
Um die Standardberechtigungen für neue Benutzer zu ändern, öffnen Sie die Hauptkonfigurationsdatei von Grafana zur Bearbeitung.
Suchen Sie die Direktive auto _ assign _ org _ role unter der Überschrift [users] und heben Sie die Kommentierung der Einstellung auf, indem Sie das; -Zeichen am Anfang der Zeile entfernen.
Setzen Sie die Direktive auf einen der folgenden Werte:
Viewer - kann nur vorhandene Dashboards verwenden
Editor - kann Dashboards verwenden, ändern und hinzufügen
Admin - kann alles tun
In diesem Tutorial wird die automatische Zuordnung auf Viewer gesetzt:
Sobald Sie Ihre Änderungen gespeichert haben, schließen Sie die Datei und starten Sie Grafana neu:
Überprüfen Sie den Status des Diensts:
Wie zuvor wird der Status active (running) lauten.
Nun haben Sie Grafana so konfiguriert, dass Mitglieder Ihrer GitHub-Organisation Ihre Grafana-Installation registrieren und verwenden können.
In diesem Tutorial haben Sie Grafana installiert, konfiguriert und gesichert und außerdem erfahren, wie sich Mitglieder Ihrer Organisation über GitHub authentifizieren können.
Um Ihre aktuelle Grafana-Installation zu erweitern, konsultieren Sie die Liste der offiziellen und von der Community entwickelten Dashboards und Plugins.
Um mehr über die allgemeine Verwendung von Grafana zu erfahren, konsultieren Sie die offizielle Grafana-Dokumentation oder unsere anderen Überwachungsleitfäden.
Installieren von Docker Compose unter Ubuntu 20.04 Schnellstart
5840
In diesem Schnellstartleitfaden installieren wir Docker Compose auf einem Ubuntu 20.04-Server.
Eine ausführlichere Version dieses Tutorials mit genaueren Erklärungen zu den einzelnen Schritten finden Sie unter Installieren und Verwenden von Docker Compose unter Ubuntu 20.04.
Um diesem Tutorial folgen zu können, benötigen Sie als sudo-Benutzer Zugriff auf einen Ubuntu 20.04-Server oder einen lokalen Rechner. Zudem muss auf dem System Docker installiert sein.
Schritt 1 - Herunterladen von Docker Compose
Ermitteln Sie zunächst auf der Versionsseite die aktuellste Version von Docker Compose.
Führen Sie folgenden Befehl aus, um Docker Compose herunterzuladen und die Software in Ihrem System als docker-compose allgemein zugänglich zu machen:
Schritt 2 - Einrichten von Ausführungsberechtigungen
Installieren und Verwenden von Docker unter Ubuntu 20.04
Installieren von Git unter Ubuntu 20.04
5789
Systeme zur Versionskontrolle wie Git sind integraler Bestandteil von bewährten Methoden für moderne Softwareentwicklung.
Dateien aus verschiedenen Softwareprojekten werden in einem Git-Repository und Plattformen wie GitHub, GitLab und Bitbucket verwaltet, um das Freigeben von und gemeinsame Arbeiten an Softwareentwicklungsprojekten zu erleichtern.
In diesem Leitfaden erfahren Sie, wie Sie Git auf einem Ubuntu 20.04-Server installieren und konfigurieren können.
Wir werden die Installation der Software auf zwei verschiedene Arten behandeln: über den integrierten Paketmanager sowie per Quellcode.
Jeder der Ansätze bietet je nach Bedarf bestimmte Vorteile.
Sie benötigen einen Ubuntu 20.04-Server mit einem non-root-Superuser-Konto.
Die Installationsoption mit Standardpaketen ist besser geeignet, wenn Sie schnell mit der Nutzung von Git beginnen möchten, eine verbreitete stabile Version bevorzugen oder nicht die neuesten verfügbaren Funktionen benötigen.
Wenn Sie die aktuellste Version wünschen, sollten Sie zum Abschnitt Installieren aus Quellcode springen.
Git ist wahrscheinlich bereits auf Ihrem Ubuntu 20.04-Server installiert.
Sie können prüfen, ob dies auf Ihrem Server der Fall ist, indem Sie folgenden Befehl ausführen:
Wenn Sie eine Ausgabe erhalten, die der folgenden ähnelt, ist Git bereits installiert.
Wenn das der Fall ist, können Sie mit Einrichten von Git fortfahren oder den nächsten Abschnitt zum Installieren aus Quellcode lesen, wenn Sie eine aktuellere Version benötigen.
Wenn Sie keine Git-Versionsnummer als Ausgabe erhalten haben, können Sie Git mit APT, dem standardmäßigen Paketmanager von Ubuntu, installieren.
Sie können prüfen, ob Git korrekt installiert wurde, indem Sie folgenden Befehl ausführen und sich vergewissern, dass Sie eine relevante Ausgabe erhalten.
Wenn Sie sich eine flexiblere Methode zur Installation von Git wünschen, kompilieren Sie die Software lieber aus Quellcode. Darauf werden wir in diesem Abschnitt eingehen.
Der Vorgang dauert länger und wird nicht von Ihrem Paketmanager verwaltet; dafür können Sie aber die neueste Version herunterladen. Außerdem haben Sie mehr Kontrolle über die Optionen, sollten Sie Anpassungen vornehmen wollen.
Prüfen Sie die auf dem Server aktuell installierte Version von Git:
Wenn Git installiert ist, erhalten Sie eine Ausgabe, die der folgenden ähnelt:
Diese ist vollständig in den Standard-Repositorys verfügbar, sodass Sie Ihren lokalen Paketindex aktualisieren und dann die relevanten Pakete installieren können.
Von der Git-Projekt-Website können wir zur Tarball-Liste navigieren, die unter https: / / mirrors.edge.kernel.org / pub / software / scm / git / verfügbar ist, und die gewünschte Version herunterladen.
Zum Zeitpunkt der Verfassung dieses Artikels ist die aktuellste Version 2.26.2, sodass wir diese zu Veranschaulichungszwecken herunterladen.
Als Nächstes ersetzen Sie den Shell-Prozess, damit die gerade installierte Version von Git verwendet wird:
Wenn Sie mit Ihrer Git-Version zufrieden sind, sollten Sie Git so konfigurieren, dass die generierten Commit-Nachrichten die richtigen Daten enthalten und Sie bei der Erstellung Ihres Softwareprojekts unterstützen.
Eine Konfiguration lässt sich durch Verwendung des Befehls git config vornehmen.
Die Informationen, die Sie eingeben, werden in Ihrer Git-Konfigurationsdatei gespeichert. Diese können Sie bei Bedarf mit einem Texteditor bearbeiten (in unserem Fall nano):
Drücken Sie STRG und X, dann Y und anschließend die Eingabetaste, um den Texteditor zu verlassen.
Installieren von Jenkins unter Ubuntu 20.04
5827
Wenn es um die Bewältigung sich wiederholender technischer Aufgaben geht, ist es nicht immer einfach, gute Automatisierungslösungen zu finden.
Mit Jenkins, einem Open-Source-basierten Automatisierungsserver, können Sie Aufgaben von der Erstellung bis zur Bereitstellung von Software effizient verwalten.
Jenkins ist Java-basiert und wird aus Ubuntu-Paketen bzw. durch Herunterladen und Ausführen der entsprechenden WAR-Datei (Web Application Archive) installiert: Dabei handelt es sich um eine Sammlung von Dateien, die eine vollständige Webanwendung ergeben, die sich auf einem Server ausführen lässt.
In diesem Tutorial installieren wir Jenkins unter Ubuntu 20.04, starten den Entwicklungsserver und erstellen einen Administratorbenutzer, sodass Sie damit beginnen können, die Möglichkeiten von Jenkins zu erkunden.
Zwar verfügen Sie nach Abschluss dieses Tutorials über einen einsatzbereiten Entwicklungsserver, doch sollten Sie ihn für die Produktion noch sichern. Folgen Sie dazu dem Tutorial Konfigurieren von Jenkins mit SSL unter Verwendung eines Nginx-Reverseproxy unter Ubuntu 18.04.
Einen Ubuntu 20.04-Server, der anhand der Anleitung Ersteinrichtung des Servers unter Ubuntu 20.04 mit einem non-root user, der über sudo-Berechtigungen verfügt, und einer Firewall konfiguriert wurde.
Wir empfehlen Ihnen, mit mindestens 1 GB RAM zu beginnen.
Konsultieren Sie "Hardwareempfehlungen" für Anweisungen zur Planung der Kapazität einer Jenkins-Installation auf Produktionsebene.
Oracle JDK 11, installiert anhand unserer Richtlinien zum Installieren spezifischer Versionen von OpenJDK unter Ubuntu 20.04.
Schritt 1 - Installieren von Jenkins
Die Version von Jenkins, die in den standardmäßigen Ubuntu-Paketen enthalten ist, ist oft älter als die neueste verfügbare Version des Projekts selbst.
Um sicherzustellen, dass Sie über die neuesten Korrekturen und Funktionen verfügen, verwenden Sie die vom Projekt gepflegten Pakete zur Installation von Jenkins.
Fügen Sie zunächst den Repository-Schlüssel zum System hinzu:
Nach dem Hinzufügen des Schlüssels gibt das System OK zurück.
Als Nächstes fügen wir die Adresse für das Debian-Paket-Repository in die sources.list des Servers ein:
Nach Eingabe beider Befehle werden wir update ausführen, damit apt das neue Repository nutzt.
Schließlich installieren wir Jenkins und seine Abhängigkeiten.
Nachdem Jenkins und seine Abhängigkeiten vorhanden sind, starten wir nun den Jenkins-Server.
Schritt 2 - Starten von Jenkins
Starten wir Jenkins durch Verwendung von systemctl:
Da systemctl keine Statusausgabe anzeigt, nutzen wir den status-Befehl zum Überprüfen, ob Jenkins erfolgreich gestartet wurde:
Wenn alles geklappt hat, zeigt der Anfang der Statusausgabe an, dass der Dienst aktiv und so konfiguriert ist, dass er beim Booten gestartet wird:
Nachdem Jenkins ausgeführt wird, sollten wir nun unsere Firewall-Regeln so anpassen, dass wir den Server über einen Webbrowser erreichen können. Damit ist die Ersteinrichtung abgeschlossen.
Schritt 3 - Öffnen der Firewall
Um eine UFW-Firewall einzurichten, konsultieren Sie Ersteinrichtung des Servers mit Ubuntu 20.04, Schritt 4 - Einrichten einer einfachen Firewall.
Standardmäßig wird Jenkins an Port 8080 ausgeführt.
Wir öffnen diesen Port mit ufw:
< $> note Anmerkung: Wenn die Firewall inaktiv ist, können Sie mit folgenden Befehlen OpenSSH zulassen und die Firewall aktivieren:
Überprüfen Sie zum Bestätigen der neuen Regeln den Status von ufw:
Sie werden feststellen, dass Datenverkehr an Port 8080 von überall zugelassen ist:
Nach der Installation von Jenkins und der Konfiguration unserer Firewall können wir die Installationsphase abschließen und mit der Einrichtung von Jenkins beginnen.
Schritt 4 - Einrichten von Jenkins
Um Ihre Installation einzurichten, rufen Sie Jenkins an seinem Standardport 8080 auf, indem Sie den Domänennamen oder die IP-Adresse Ihres Servers verwenden: http: / / < ^ > your _ server _ ip _ or _ domain < ^ >: 8080
Sie sollten den Bildschirm Unlock Jenkins (Jenkins entsperren) erhalten, in dem der Speicherort des ersten Passworts angezeigt wird:
Bildschirm "Unlock Jenkins" (Jenkins entsperren)
Verwenden Sie im Terminalfenster den Befehl cat zum Anzeigen des Passworts:
Kopieren Sie das 32 Zeichen lange alphanumerische Passwort aus dem Terminal und fügen Sie es in das Feld Administrator password (Administratorkennwort) ein. Klicken Sie dann auf Continue (Weiter).
Im nächsten Bildschirm wird die Option zum Installieren empfohlener Plugins oder Auszuwählen spezifischer Plugins angezeigt:
Bildschirm "Customize Jenkins" (Jenkins anpassen)
Wir klicken auf die Option Install suggested plugins (Empfohlene Plugins installieren), woraufhin der Installationsprozess unmittelbar beginnt.
Bildschirm "Jenkins Getting Started Install Plugins" (Erste Schritte mit Jenkins - Plugins installieren)
Nach Abschluss der Installation werden Sie aufgefordert, den ersten Administratorbenutzer einzurichten.
Es ist möglich, diesen Schritt überspringen und als admin mit dem oben verwendeten ursprünglichen Passwort fortzufahren, aber wir werden uns einen Moment Zeit nehmen, um den Benutzer zu erstellen.
< $> note Anmerkung: Der standardmäßige Jenkins-Server ist NICHT verschlüsselt, sodass die mit diesem Formular übermittelten Daten nicht geschützt sind.
Siehe Konfigurieren von Jenkins mit SSL unter Verwendung eines Nginx-Reverseproxy unter Ubuntu 20.04, um Anmeldedaten von Benutzern und Informationen über Builds, die über die Weboberfläche übertragen werden, zu schützen.
Bildschirm "Jenkins Create First Admin User" (Erstellen des ersten Administratorbenutzers in Jenkins)
Geben Sie den Namen und das Passwort für Ihren Benutzer ein:
Jenkins Create User (Jenkins: Benutzer erstellen)
Sie sehen eine Seite zur Instance Configuration (Instanzkonfiguration), auf der Sie dazu aufgefordert werden, die bevorzugte URL für Ihre Jenkins-Instanz zu bestätigen.
Bestätigen Sie entweder den Domänennamen für Ihren Server bzw. die IP-Adresse Ihres Servers:
Jenkins Instance Configuration (Jenkins: Instanzkonfiguration)
Nach der Bestätigung der entsprechenden Daten klicken Sie auf Save and Finish (Speichern und Fertigstellen).
Sie erhalten eine Seite mit der Bestätigung, dass "Jenkins is Ready!" (Jenkins bereit ist):
Bildschirm "Jenkins is ready" (Jenkins ist bereit)
Klicken Sie auf Start using Jenkins (Mit Verwendung von Jenkins beginnen), um das Haupt-Dashboard von Jenkins aufzurufen:
Bildschirm "Welcome to Jenkins" (Willkommen bei Jenkins)
Jetzt haben Sie die Installation von Jenkins erfolgreich abgeschlossen.
In diesem Tutorial haben Sie Jenkins mit den vom Projekt bereitgestellten Paketen installiert, den Server gestartet, die Firewall geöffnet und einen Administratorbenutzer erstellt.
Nun können Sie mit der Erkundung von Jenkins beginnen.
Folgen Sie danach Abschluss dem Leitfaden Konfigurieren von Jenkins mit SSL unter Verwendung eines Nginx-Reverseproxy unter Ubuntu 20.04, um Ihre Passwörter sowie sensible System- oder Produktdaten, die zwischen dem Computer und dem Server in Klartext übertragen werden, zu schützen. Anschließend können Sie Jenkins weiter nutzen.
Um mehr über die Funktionen von Jenkins zu erfahren, lesen Sie weitere Tutorials zu dem Thema:
Erstellen von Android-Apps mit Jenkins
Einrichten von Pipelines zur kontinuierlichen Integration in Jenkins unter Ubuntu 16.04
Installieren von WordPress unter Ubuntu 20.04 mit einem LAMP-Stack
5469
WordPress ist eine sehr beliebte Open-Source-Technologie zum Erstellen von Websites und Blogs im Internet.
WordPress-Sites werden von 63% aller Websites genutzt, die ein Content Management System (CMS) verwenden, und stellen 36% aller Websites dar, die derzeit online sind.
Es gibt viele verschiedene Ansätze, um Zugriff auf WordPress zu erhalten. Außerdem sind manche Einrichtungsverfahren komplexer als andere.
Dieses Tutorial richtet sich an Personen, die eine WordPress-Instanz über die Befehlszeile auf einem nicht verwalteten Cloud-Server installieren und verwalten möchten.
Zwar beinhaltet dieser Ansatz mehr Schritte als eine fertige WordPress-Installation, doch bietet er Administratoren mehr Kontrolle über ihre WordPress-Umgebung.
< $> info Wenn Sie auf eine fertige WordPress-Installation zugreifen möchten, bietet DigitalOcean Marketplace eine One-Click-App, die Sie beim Starten Ihres Servers durch die Installation und ersten Schritten mit WordPress begleitet.
Je nach Bedarf und Zielen finden Sie möglicherweise andere Optionen, die besser geeignet sind.
Als Open-Source-Software kann WordPress kostenlos heruntergeladen und installiert werden. Um im Internet verfügbar zu sein, müssen Sie aber wahrscheinlich Cloud-Infrastruktur und einen Domänennamen kaufen.
Folgen Sie diesem Leitfaden weiter, wenn Sie erfahren möchten, wie die serverseitige Installation und Einrichtung einer WordPress-Site funktioniert.
Dieses Tutorial wird einen LAMP-Stack (Linux, Apache, MySQL und PHP) nutzen. Er ist eine Option für eine Serverarchitektur, die WordPress unterstützt, indem das Linux-Betriebssystem, der Apache-Webserver, die MySQL-Datenbank und die PHP-Programmiersprache bereitgestellt werden.
Wir installieren und richten WordPress via LAMP auf einem Linux 20.04-Server ein.
Um dieses Tutorial abzuschließen, benötigen Sie Zugriff auf einen Ubuntu 20.04-Server und müssen vor Beginn dieses Leitfadens folgende Schritte abgeschlossen haben:
Einrichten Ihres Servers anhand unseres Leitfadens zur Ersteinrichtung des Servers unter Ubuntu 20.04 und Sicherstellen, dass es einen non-root user mit sudo-Berechtigungen gibt.
Installieren eines LAMP-Stacks durch Befolgen unseres LAMP-Leitfadens zur Installation und Konfiguration dieser Software.
Sichern Ihrer Site: WordPress erfasst Eingaben von Benutzern und speichert Benutzerdaten. Daher ist es wichtig, dass es eine Sicherheitsschicht gibt.
TLS / SSL ist eine Technologie, die es Ihnen ermöglicht, den Datenverkehr von Ihrer Website zu verschlüsseln, damit Ihre Verbindung und die Verbindung der Benutzer sicher sind.
Hier sind zwei Optionen, mit denen sich diese Anforderung erfüllen lässt:
Wenn Sie einen Domänennamen haben..., können Sie Ihre Site mit Let 's Encrypt sichern, das kostenlose, vertrauenswürdige Zertifikate bereitstellt.
Schritt 1 - Erstellen einer MySQL-Datenbank und eines Benutzers für WordPress
Melden Sie sich zunächst im (administrativen) MySQL-Root-Konto an, indem Sie diesen Befehl ausgeben (beachten Sie, dass dies nicht der root user Ihres Servers ist):
< $> note Anmerkung: Wenn Sie nicht per root auf Ihre MySQL-Datenbank zugreifen können, können Sie als sudo-Benutzer das Passwort Ihres root user aktualisieren, indem Sie sich wie folgt in der Datenbank anmelden:
Sobald Sie die MySQL-Eingabeaufforderung sehen, können Sie das Passwort des root user aktualisieren.
Ersetzen Sie hier < ^ > new _ password < ^ > durch ein starkes Passwort Ihrer Wahl.
Sie können nun EXIT; eingeben und sich mit dem folgenden Befehl wieder per Passwort in der Datenbank anmelden.
Innerhalb der Datenbank können wir eine exklusive Datenbank erstellen, die WordPress kontrollieren kann.
Sie können diese Datenbank nennen, wie Sie möchten. In diesem Leitfaden werden wir jedoch den Namen wordpress verwenden.
Als Nächstes erstellen wir ein separates MySQL-Benutzerkonto, das wir ausschließlich für unsere neue Datenbank verwenden werden.
Die Erstellung spezifischer Datenbanken und Konten kann aus einer Management- und Sicherheitsperspektive hilfreich sein.
Wir werden in diesem Leitfaden den Namen wordpressuser verwenden. Sie können jedoch einen beliebigen Namen wählen, der für Sie relevant ist.
Denken Sie daran, bei < ^ > password < ^ > ein starkes Passwort für Ihren Datenbankbenutzer zu wählen:
Als Nächstes teilen Sie der Datenbank mit, dass unser wordpressuser kompletten Zugriff auf die von uns erstellte Datenbank haben soll:
Im nächsten Schritt schaffen wir Grundlagen für WordPress-Plugins, indem wir PHP-Erweiterungen für unseren Server herunterladen.
Dadurch wird die Grundlage für die Installation zusätzlicher Plugins in unserer WordPress-Site geschaffen.
Wir müssen Apache neu starten, um die neuen Erweiterungen zu laden. Im nächsten Abschnitt werden wir weitere Konfigurationen in Apache vornehmen. Sie können also bis dahin warten oder die PHP-Erweiterungen jetzt abschließen.
Schritt 3 - Anpassen der Apache-Konfiguration, um .htaccess-Overrides und -Rewrites zuzulassen
Als Beispiel verwenden wir in diesem Leitfaden / etc / apache2 / sites-available / < ^ > wordpress < ^ > .conf, aber Sie sollten den Pfad zu Ihrer Konfigurationsdatei ggf. ersetzen.
Außerdem verwenden wir / var / www / < ^ > wordpress < ^ > als Root-Verzeichnis unserer WordPress-Installation.
Wenn Sie unserem LAMP-Tutorial gefolgt sind, kann dies in beiden Fällen Ihr Domänenname anstelle von wordpress sein.
Das ist in Ordnung, wenn Sie auf diesem Server nur eine Website hosten wollen.
Nach Festlegung unserer Pfade können wir nun mit .htaccess arbeiten, sodass Apache Konfigurationsänderungen anhand einzelner Verzeichnisse handhaben kann.
WordPress und viele WordPress-Plugins setzen diese Dateien weitläufig für kleine In-Directory-Änderungen des Webserver-Verhaltens ein.
Öffnen Sie die Apache-Konfigurationsdatei für Ihre Website mit einem Texteditor wie nano.
In nano können Sie dazu zusammen Strg und X, dann Y und anschließend die Eingabetaste drücken.
Dadurch können Sie in Ihren Posts mehr für Menschen lesbare Permalinks verwenden, wie die folgenden zwei Beispiele zeigen:
Der Befehl a2enmod ruft ein Skript auf, das das angegebene Modul innerhalb der Apache-Konfiguration aktiviert.
Bevor wir die von uns vorgenommenen Änderungen implementieren, überprüfen wir, ob wir keine Syntaxfehler gemacht haben. Dazu führen wir folgenden Test aus.
Sie erhalten möglicherweise eine Ausgabe wie diese:
Dies ist jedoch nur eine Nachricht und beeinflusst die Funktionalität Ihrer Website nicht.
Starten Sie Apache neu, um die Änderungen zu implementieren.
Nehmen Sie den Neustart auch dann vor, wenn Sie in diesem Tutorial bereits einen Neustart ausgeführt haben.
Schritt 4 - Herunterladen von WordPress
Wechseln Sie in ein beschreibbares Verzeichnis (wir empfehlen ein temporäres Verzeichnis wie / tmp) und laden Sie die komprimierte Version herunter.
Außerdem kopieren wir die Muster-Konfigurationsdatei in den Dateinamen, den WordPress liest:
Wir können auch das upgrade-Verzeichnis erstellen, damit WordPress keine Berechtigungsprobleme bekommt, wenn es nach einem Software-Update versucht, dies selbst zu tun:
Sorgen Sie dafür, dass Sie das Verzeichnis / var / www / < ^ > wordpress < ^ > durch das Verzeichnis ersetzen, das Sie auf Ihrem Server eingerichtet haben.
Schritt 5 - Konfigurieren des WordPress-Verzeichnisses
Ein wichtiger Schritt, den wir erledigen müssen, ist die Einrichtung vernünftiger Dateiberechtigungen und Dateibesitzer.
Wir beginnen damit, das Eigentum an allen Dateien auf den Benutzer und die Gruppe www-data zu übertragen.
Das ist der Benutzer, unter dem der Apache-Webserver läuft; Apache muss WordPress-Dateien lesen und schreiben können, damit die Website bereitgestellt werden kann und sich automatisch Updates ausführen lassen.
Aktualisieren Sie das Eigentum mit dem Befehl chown, um den Dateibesitz zu ändern.
Achten Sie darauf, dass Sie auf das richtige Verzeichnis Ihres Servers verweisen.
Diese Berechtigungen sollten es Ihnen erlauben, effektiv mit WordPress zu arbeiten. Beachten Sie jedoch, dass einige Plugins und Verfahren zusätzliche kleine Änderungen benötigen.
Wenn wir die Datei öffnen, müssen wir zunächst einige geheime Schlüssel ändern, um unsere Installation besser zu schützen.
Diese werden nur intern verwendet, d. h. komplexe, sichere Werte haben keine Auswirkungen auf die Benutzer.
Sie erhalten eindeutige Werte zurück, die einer Ausgabe mit dem folgenden Block ähneln.
Kopieren Sie NICHT die folgenden Werte!
Suchen Sie nach dem Abschnitt, der die Beispielwerte für diese Einstellungen enthält.
Als Nächstes werden wir einige der Datenbank-Verbindungseinstellungen am Anfang der Datei ändern.
Sie müssen den Datenbanknamen, den Datenbankbenutzer und das zugehörige Passwort anpassen, das Sie in MySQL konfiguriert haben.
Da wir dem Webserver das Recht auf beliebiges Schreiben erteilt haben, können wir die Dateisystemmethode auf "direct" festlegen.
Navigieren Sie im Web-Browser zum Domänennamen oder zur öffentlichen IP-Adresse Ihres Servers:
Wählen Sie einen Namen für Ihre WordPress-Site aus und legen Sie einen Benutzernamen fest.
Es wird empfohlen, eindeutige Namen zu wählen und aus Sicherheitgründen allgemeine Benutzernamen wie "admin" zu vermeiden.
Jetzt können Sie mit der Gestaltung Ihrer WordPress-Website beginnen!
Herzlichen Glückwunsch! WordPress ist nun installiert und bereit für den Einsatz!
Jetzt können Sie Folgendes tun:
Ihre Permalinks-Einstellung für WordPress-Posts auswählen, die Sie unter Einstellungen > Permalinks finden.
In Appearance > Themes (Erscheinungsbild > Themen) ein neues Thema wählen.
Unter Plugins > Add New (Plugins > Neu hinzufügen) neue Plugins installieren, um die Funktionalität Ihrer Website zu erhöhen.
Wenn Sie mit anderen zusammenarbeiten möchten, können Sie jetzt auch weitere Benutzer hinzufügen unter Users > Add New (Benutzer > Neu hinzufügen).
Sie können weitere Ressourcen für alternative Methoden zur Installation von WordPress finden, erfahren, wie sich WordPress in verschiedenen Server-Distributionen installieren lässt, Ihre WordPress-Installationen automatisieren und WordPress-Websites skalieren, indem Sie sich unser WordPress Community-Tag ansehen.
Einrichten einer Node.js-Anwendung für die Produktion unter Ubuntu 20.04
6051
Node.js ist eine Open-Source-basierte JavaScript-Laufzeitumgebung für das Einrichten von Server- und Netzwerkanwendungen.
Die Plattform läuft unter Linux, MacOS, FreeBSD und Windows.
Sie können Node.js-Anwendungen zwar in der Befehlszeile ausführen können, doch werden Sie sie in diesem Tutorial als Dienst ausführen.
Das bedeutet, dass sie bei erneutem Booten oder einem Absturz neu gestartet werden und zur Verwendung in einer Produktionsumgebung sicher sind.
In diesem Tutorial werden Sie eine produktionsfähige Node.js-Umgebung auf einem einzelnen Ubuntu 20.04-Server einrichten.
Dieser Server wird eine von PM2 verwaltete Node.js-Anwendung ausführen und Benutzern sicheren Zugriff auf die Anwendung über einen Nginx-Reverseproxy bereitstellen.
Der Nginx-Server bietet HTTPS mit einem kostenlosen Zertifikat, das von Let 's Encrypt bereitgestellt wird.
Dieser Leitfaden geht davon aus, dass Sie Folgendes haben:
Eine Ubuntu 20.04-Server-Einrichtung, wie im Leitfaden zur Ersteinrichtung des Servers für Ubuntu 20.04 beschrieben.
Sie sollten einen non-root user mit sudo-Berechtigungen und eine aktive Firewall haben.
Einen Domänennamen, der auf die öffentliche IP-Adresse Ihres Servers verweist.
In diesem Tutorial wird durchgängig der Domänenname example.com verwendet.
Nginx installiert, wie in Installieren von Nginx unter Ubuntu 20.04 beschrieben.
Nginx, konfiguriert mit SSL unter Verwendung von Let 's Encrypt-Zertifikaten.
Sichern von Nginx mit Let 's Encrypt unter Ubuntu 20.04 wird Sie durch den Prozess führen.
Wenn Sie die Voraussetzungen abgeschlossen haben, verfügen Sie über einen Server, der die Standardplatzhalterseite Ihrer Domäne unter https: / / < ^ > example.com < ^ > / bereitstellt.
Schritt 1 - Installieren von Node.js
Beginnen wir zunächst mit der Installation der neuesten LTS-Version von Node.js unter Verwendung der NodeSource-Paketarchive.
Installieren Sie zunächst das NodeSource-PPA, um Zugriff auf dessen Inhalt zu erhalten.
Stellen Sie sicher, dass Sie sich in Ihrem Stammverzeichnis befinden, und verwenden Sie curl, um das Installationskript für die aktuellste LTS-Version von Node.js aus den Archiven abzurufen.
Sie können den Inhalt dieses Skripts mit nano oder Ihrem bevorzugten Texteditor prüfen:
Wenn Sie das Skript fertig geprüft haben, führen Sie es unter sudo aus:
Nach Ausführung des Einrichtungsskripts aus Nodesource können Sie nun das Node.js-Paket installieren:
< $> note Anmerkung: Bei der Installation aus dem NodeSource-PPA wird die ausführbare Node.js-Datei nodejs und nicht node genannt.
Mit installierter Node.js-Laufzeitumgebung können wir jetzt mit dem Schreiben einer Node.js-Anwendung fortfahren.
Schritt 2 - Erstellen einer Node.js-Anwendung
Lassen Sie uns eine Hello World-Anwendung schreiben, die in beliebigen HTTP-Anfragen "Hello World" zurückgibt.
Diese Beispielanwendung hilft Ihnen dabei, Node.js einzurichten.
Sie können sie durch eine eigene Anwendung ersetzen - stellen Sie sicher, dass Sie Ihre Anwendung so ändern, dass an den richtigen IP-Adressen und Ports gelauscht wird.
Erstellen wir zunächst eine Beispielanwendung namens hello.js:
Diese Node.js-Anwendung lauscht an der angegebenen Adresse (localhost) und Port (3000) und gibt "Hello World!"
mit dem HTTP-Erfolgscode 200 zurück.
Da wir an localhost lauschen, können sich Remoteclients nicht mit unserer Anwendung verbinden.
Geben Sie zum Testen Ihrer Anwendung Folgendes ein:
< $> note Anmerkung: Durch Ausführung einer Node.js-Anwendung auf diese Weise werden weitere Befehle blockiert, bis der Abbruch der Anwendung durch Drücken von Strg + C erzwungen wird.
Öffnen Sie zum Testen der Anwendung auf Ihrem Server eine weitere Terminalsitzung und stellen Sie unter Verwendung von curl mit localhost eine Verbindung her:
Wenn Sie die folgende Ausgabe erhalten, funktioniert die Anwendung ordnungsgemäß und lauscht an der richtigen Adresse und am richtigen Port:
Wenn Sie nicht die erwartete Ausgabe erhalten, stellen Sie sicher, dass Ihre Node.js-Anwendung ausgeführt wird und so konfiguriert ist, dass sie an der richtigen Adresse und am richtigen Port lauscht.
Sobald Sie sicher sind, dass es funktioniert, erzwingen Sie den Abbruch der Anwendung (wenn Sie es nicht bereits getan haben), indem Sie Strg + C drücken.
Schritt 3 - Installieren von PM2
Als Nächstes installieren wir PM2, einen Prozessmanager für Node.js-Anwendungen.
PM2 ermöglicht es, Anwendungen zu daemonisieren, damit sie im Hintergrund als Dienst ausgeführt werden.
Verwenden Sie npm zum Installieren der neuesten Version von PM2 auf Ihrem Server:
Die Option -g weist npm an, das Modul global zu installieren, damit es systemweit verfügbar ist.
Verwenden wir zunächst den Befehl pm2 start zum Ausführen Ihrer Anwendung hello.js im Hintergrund:
Dadurch wird Ihre Anwendung auch der Prozessliste von PM2 hinzugefügt, die bei jeder Ausführung einer Anwendung ausgegeben wird:
Wie oben angegeben, weist PM2 automatisch einen App-Namen (basierend auf dem Dateinamen ohne die Erweiterung .js) und eine PM2-ID zu. Außerdem pflegt PM2 weitere Informationen, wie z. B. die PID des Prozesses, den aktuellen Status und die Speicherauslastung.
Anwendungen, die unter PM2 ausgeführt werden, werden automatisch neu gestartet, wenn eine Anwendung abstürzt oder ihr Abbruch erzwungen wird. Wir können jedoch mit dem Unterbefehl startup einen zusätzlichen Schritt hinzufügen, damit die Anwendung beim Systemstart gestartet wird.
Dieser Unterbefehl generiert und konfiguriert ein Startskript zum Starten von PM2 und den verwalteten Prozessen beim Booten von Servern:
Die letzte Zeile der resultierenden Ausgabe enthält einen Befehl zum Ausführen mit Superuser-Berechtigungen, damit PM2 beim Booten gestartet wird:
Führen Sie den Befehl aus der Ausgabe aus, wobei Sie anstelle von < ^ > sammy < ^ > Ihren Benutzernamen verwenden:
Als weiteren Schritt können wir die PM2-Prozessliste und entsprechende Umgebungen speichern:
Sie haben nun eine systemd-Einheit erstellt, die beim Booten pm2 für Ihren Benutzer ausführt.
Diese pm2-Instanz gibt wiederum hello.js zurück.
Starten Sie den Dienst mit systemctl:
Wenn an diesem Punkt ein Fehler auftritt, müssen Sie möglicherweise neu starten, was Sie mit sudo reboot erreichen können.
Überprüfen Sie den Status der systemd-Einheit:
Für eine ausführliche Übersicht zu systemd lesen Sie bitte Systemd Essentials: Arbeiten mit Diensten, Einheiten und dem Journal.
Neben den von uns abgedeckten Befehlen bietet PM2 viele Unterbefehle, mit denen Sie Daten über Ihre Anwendung verwalten oder suchen können.
Stoppen Sie eine Anwendung mit diesem Befehl (geben Sie den App-Namen oder die ID von PM2 an):
Starten Sie eine Anwendung neu:
Listen Sie die aktuell von PM2 verwalteten Anwendungen auf:
Erhalten Sie Informationen zu einer bestimmten Anwendung mithilfe des App-Namens:
Der PM2-Prozessmonitor kann mit dem Unterbefehl monit aufgerufen werden.
Dadurch werden der Anwendungsstatus sowie die CPU-und Speicherauslastung angezeigt:
Beachten Sie, dass beim Ausführen von pm2 ohne Argumente auch eine Hilfeseite mit Beispielnutzung angezeigt wird.
Nachdem Ihre Node.js-Anwendung ausgeführt und von PM2 verwaltet wird, können wir nun den Reverseproxy einrichten.
Schritt 4 - Einrichten von Nginx als Reverseproxy-Server
Ihre Anwendung wird ausgeführt und lauscht an localhost. Sie müssen jedoch einen Weg finden, damit Ihre Benutzer darauf zugreifen können. Dazu werden wir den Nginx-Webserver als Reverseproxy einrichten.
Im Tutorial zu den Voraussetzungen richten Sie Ihre Nginx-Konfiguration in der Datei / etc / nginx / sites-available / < ^ > example.com < ^ > ein.
Innerhalb des server-Blocks sollten Sie einen vorhandenen location / -Block haben.
Ersetzen Sie den Inhalt dieses Blocks durch die folgende Konfiguration.
Wenn Ihre Anwendung so konfiguriert ist, dass sie an einem anderen Port lauscht, aktualisieren Sie den markierten Teil mit der richtigen Portnummer:
Dadurch wird der Server so konfiguriert, dass er auf Anfragen an sein Stammverzeichnis antwortet.
Angenommen, unser Server ist unter < ^ > example.com < ^ > verfügbar. Durch Aufrufen von https: / / < ^ > example.com < ^ > / über einen Webbrowser würde die Anfrage an hello.js gesendet werden, wobei an Port 3000 bei localhost gelauscht wird.
Sie können dem gleichen Serverblock zusätzliche location-Blöcke hinzufügen, um Zugriff auf andere Anwendungen auf dem gleichen Server zu gewähren.
Wenn Sie beispielsweise eine andere Node.js-Anwendung an Port 3001 ausführen würden, könnten Sie diesen location-Block hinzufügen, um Zugriff darauf über https: / / < ^ > example.com < ^ > / < ^ > app2 < ^ > zu gewähren:
Sobald Sie die location-Blöcke für Ihre Anwendungen hinzugefügt haben, speichern Sie die Datei und verlassen Sie den Editor.
Stellen Sie sicher, dass Sie keine Syntaxfehler gemacht haben, indem Sie Folgendes eingeben:
Starten Sie Nginx neu:
Wenn Ihre Node.js-Anwendung ausgeführt wird und Ihre Anwendungs- und Nginx-Konfigurationen korrekt sind, sollten Sie nun über den Nginx-Reverseproxy auf Ihre Anwendung zugreifen können.
Probieren Sie es aus, indem Sie die URL Ihres Servers (seine öffentliche IP-Adresse oder seinen Domänennamen) aufrufen.
Ihre Node.js-Anwendung wird nun hinter einem Nginx-Reverseproxy auf einem Ubuntu 20.04-Server ausgeführt.
Diese Reverseproxy-Einrichtung ist flexibel genug, um Ihren Benutzern Zugriff auf andere Anwendungen oder statische Webinhalte zu bieten, die Sie freigeben möchten.
Austricksen eines neuronalen Netzwerks in Python 3
6037
Der Autor hat Dev Color dazu ausgewählt, im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
Kann ein neuronales Netzwerk zur Klassifizierung von Tieren ausgetrickst werden?
Das Austricksen einer solchen Klassifizierung mag vielleicht wenig Konsequenzen haben, doch sieht das bei Gesichtsauthentifizierung schon anders aus.
Und was ist mit der Software von Prototypen selbstfahrender Autos?
Zum Glück gibt es zahlreiche Ingenieure und Forscher, die bei unseren mobilen Geräten und Autos zwischen einem Computermodell für Prototypen und Modellen mit Produktionsqualität stehen.
Dennoch haben diese Risiken erhebliche Auswirkungen und müssen von Benutzern, die maschinelles Lernen anwenden, berücksichtigt werden.
In diesem Tutorial versuchen Sie, eine Klassifizierungsanwendung für Tiere zu täuschen bzw. auszutricksen.
Wenn Sie das Tutorial durcharbeiten, verwenden Sie OpenCV, eine Computer-Vision-Bibliothek, und PyTorch, eine Deep Learning Library.
Wir werden die folgenden Themen im zugehörigen Bereich des Adversarial Machine Learning behandeln:
Erstellen eines gezielten Adversial-Beispiels.
Auswählen eines Bilds - etwa eines Hunds.
Auswählen einer Zielklasse, z. B. eine Katze.
Ihr Ziel besteht darin, das neuronale Netzwerk so zu täuschen, dass es denkt, dass der abgebildete Hund eine Katze sei.
Erstellen Sie eine Adversarial-Verteidigung.
Kurz gesagt: Schützen Sie Ihr neuronales Netzwerk vor diesen trickreichen Bildern, ohne zu wissen, was der Trick ist.
Am Ende des Tutorials verfügen Sie über ein Werkzeug zum Austricksen neuronaler Netzwerke und ein Verständnis dafür, wie man sich gegen Tricks verteidigen kann.
Es wird empfohlen, Build an Emotion-Based Dog Filter (Erstellen eines emotionsbasierten Hundefilters) zu konsultieren; dieses Tutorial wird nicht explizit verwendet, führt aber in den Begriff der Klassifizierung ein.
Sie nennen Ihren Arbeitsbereich AdversarialML:
Navigieren Sie zum Verzeichnis AdversarialML:
Erstellen Sie ein Verzeichnis, das alle Ihre Ressourcen enthält:
Installieren Sie nun vorgepackte Binaries für OpenCV und numpy; das sind Bibliotheken für Computer-Vision bzw. lineare Algebra.
Nach Installation der Abhängigkeiten führen wir eine Tierklassifizierung namens ResNet18 aus, was wir als Nächstes beschreiben werden.
Schritt 2 - Ausführen einer vordefinierten Tierklassifizierung
Die torchvision-Bibliothek, die offizielle Computer-Vision-Bibliothek für PyTorch, enthält vortrainierte Versionen von verbreiteten neuronalen Computer-Vision-Netzwerken.
Diese neuronalen Netzwerke werden alle in ImageNet 2012 trainiert, einem Datensatz mit 1,2 Millionen Trainingsbildern und 1.000 Klassen.
Diese Klassen umfassen Fahrzeuge, Orte und vor allem Tiere.
In diesem Schritt werden Sie eines dieser vortrainierten neuronalen Netzwerke (ResNet18) ausführen.
Wir werden ResNet18, trainiert in ImageNet, als "Tierklassifizierung" bezeichnen.
< $> note Was ist ResNet18?
ResNet18 ist das kleinste neuronale Netzwerk in einer Familie neuronaler Netzwerke, die Residual Neural Networks genannt werden, entwickelt von MSR (et al.).
Kurz gesagt: Er hat herausgefunden, dass ein neuronales Netzwerk (angegeben als Funktion f mit Eingabe x und Ausgabe f (x)) mit einer "residual connection" x + f (x) bessere Ergebnisse liefern würde.
Diese Residual Connection wird auch heute noch in hochmodernen neuronalen Netzwerken häufig verwendet.
Beispielsweise FBNetV2, FBNetV3.
Laden Sie dieses Bild eines Hundes mit dem folgenden Befehl herunter:
Bild von Corgi nahe Teich
Laden Sie dann eine JSON-Datei herunter, um die neuronale Netzwerkausgabe in einen menschenlesbaren Klassennamen zu konvertieren:
Erstellen Sie als Nächstes ein Skript zum Ausführen Ihres vortrainierten Modells für das Hundebild.
Erstellen Sie eine neue Datei namens step _ 2 _ pretrained.py:
Fügen Sie zunächst das Python-Boilerplate hinzu, indem Sie die erforderlichen Pakete importieren und eine main-Funktion deklarieren:
Als Nächstes laden Sie das Mapping von neuronaler Netzwerkausgabe zu menschenlesbaren Klassennamen.
Fügen Sie dies direkt nach Ihren Importanweisungen und vor Ihrer main-Funktion hinzu:
Erstellen Sie eine Funktion zur Bildtransformation, die sicherstellt, dass Ihr Eingabebild zum einen die richtigen Dimensionen aufweist und zum anderen richtig normalisiert ist.
Fügen Sie direkt nach der letzten Funktion folgende Funktion hinzu:
In get _ image _ transform definieren Sie eine Reihe verschiedener Transformationen für die Bilder, die an Ihr neuronales Netzwerk übergeben werden:
transforms.Resize (224): Ändert die kleinere Seite des Bildes auf 224. Wenn Ihr Bild beispielsweise 448 x 672 groß ist, würde diese Operation das Bild beispielsweise auf 224 x 336 reduzieren.
transforms.CenterCrop (224): Nimmt einen Zuschnitt von der Bildmitte mit der Größe 224 x 224 vor.
transforms.ToTensor (): Konvertiert das Bild in einen PyTorch-Tensor.
Alle PyTorch-Modelle erfordern PyTorch-Tensoren als Eingabe.
transforms.Normalize (mean =..., std =...): Standardisiert Ihre Eingabe, indem der Mittelwert subtrahiert und dann durch die Standardabweichung geteilt wird.
Dies wird in der torchvision-Dokumentation genauer beschrieben.
Fügen Sie ein Dienstprogramm hinzu, um die Tierklasse je nach Bild vorherzusagen.
Diese Methode verwendet die beiden vorherigen Dienstprogramme zur Durchführung der Tierklassifizierung:
Hier klassifiziert die Funktion predict das bereitgestellte Bild mit einem vortrainierten neuronalen Netzwerk:
model.resnet18 (pretrained = True): Lädt ein vortrainiertes neuronales Netzwerk namens ResNet18.
model.eval (): Ändert das vorhandene Modell so, dass es im Modus 'evaluation' ausgeführt wird.
Der einzige andere Modus ist der Modus 'training', doch ist der Trainingsmodus nicht erforderlich, da Sie das Modell in diesem Tutorial nicht trainieren (d. h. die Parameter des Modells nicht aktualisieren).
out = model (image): Führt das neuronale Netzwerk für das bereitgestellte, transformierte Bild aus.
_, pred = torch.max (out, 1): Das neuronale Netzwerk gibt eine Wahrscheinlichkeit für jede mögliche Klasse aus.
Dieser Schritt berechnet den Index der Klasse mit der höchsten Wahrscheinlichkeit.
Beispielsweise wenn out = [0.4, 0.1, 0.2], dann pred = 0.
idx _ to _ label = idx _ to _ label: Erhält ein Mapping vom Klassenindex zu menschenlesbaren Klassennamen.
Beispielsweise könnte das Mapping {0: Katze, 1: Hund, 2: Fisch} lauten.
cls = idx _ to _ label [str (int (pred))]: Konvertiert den vorhergesagten Klassenindex in einen Klassennamen.
Die in den letzten beiden Punkten genannten Beispiele würden cls = idx _ to _ label [0] = 'cat' ergeben.
Als Nächstes fügen Sie nach der letzten Funktion ein Dienstprogramm zum Laden von Bildern hinzu:
Dadurch wird ein Bild aus dem im ersten Argument angegebenen Pfad in das Skript geladen. transform (image) [None] wendet die Reihenfolge der in den vorherigen Zeilen definierten Bildtransformationen an.
Abschließend füllen Sie Ihre main-Funktion mit Folgendem, um Ihr Bild zu laden und das Tier im Bild zu klassifizieren:
Vergewissern Sie sich, dass Ihre Datei mit unserem letzten Schritt 2 bei step _ 2 _ pretrained.py in GitHub übereinstimmt.
Speichern und beenden Sie Ihr Skript und führen Sie die Tierklassifizierung aus:
Dadurch wird folgende Ausgabe erstellt, was zeigt, dass Ihre Tierklassifizierung wie erwartet funktioniert:
Das beschließt die Ausführung von Inferenz mit Ihrem vortrainierten Modell.
Als Nächstes sehen Sie ein Adversarial-Beispiel in der Praxis, bei dem ein neuronales Netzwerk mit unmerklichen Unterschieden im Bild ausgetrickst wird.
Schritt 3 - Ausprobieren eines Adversarial-Beispiels
Jetzt werden Sie ein Adversarial-Beispiel synchronisieren und das neuronale Netzwerk für dieses Beispiel testen.
Für dieses Tutorial erstellen Sie verschiedene Adversarial-Beispiele im Format x + r, wobei x das ursprüngliche Bild und r eine "Perturbation" ist.
Sie werden die Perturbation r noch selbst erstellen, doch in diesem Schritt werden Sie eine herunterladen, die wir zuvor für Sie erstellt haben.
Starten Sie mit dem Herunterladen der Perturbation r:
Verbinden Sie nun das Bild mit der Perturbation.
Erstellen Sie eine neue Datei namens step _ 3 _ adversarial.py:
In dieser Datei führen Sie den folgenden Prozess mit drei Schritten aus, um ein adversarial-Beispiel zu erstellen:
Transformieren eines Bilds
Anwenden der Perturbation r
Inverses Transformieren des pertubierten Bilds
Am Ende von Schritt 3 werden Sie über ein gegensätzliches Bild verfügen.
Importieren Sie zunächst die erforderlichen Pakete und deklarieren Sie eine Hauptfunktion:
Erstellen Sie als Nächstes eine "Bildtransformation", die die frühere Bildtransformation umkehrt.
Platzieren Sie dies nach Ihren Importen vor der main-Funktion:
Wie zuvor subtrahiert die Funktion transforms.Normalize den Mittelwert und teilt durch die Standardabweichung (d. h. für das ursprüngliche Bild x, y = transforms.Normalize (mean = u, std = o) = (x - u) / o).
Sie machen etwas Algebra und definieren eine neue Operation, die diese Normalisierungsfunktion umkehrt (transforms.Normalize (mean = -u / o, std = 1 / o) = (y - -u / o) / 1 / o = (y + u / o) o = yo + u = x).
Fügen Sie im Rahmen der inversen Transformation eine Methode hinzu, die einen PyTorch-Tensor in ein PIL-Bild umwandelt.
Fügen Sie dies nach der letzten Funktion hinzu:
tensor.data.numpy () konvertiert den PyTorch-Tensor in ein NumPy-Array. .transpose (1, 2, 0) arrangiert (Kanäle, Breite, Höhe) in (Höhe, Breite, Kanäle) um.
Dieses NumPy-Array befindet sich etwa im Bereich (0, 1).
Abschließend multiplizieren Sie mit 255, um sicherzustellen, dass das Bild nun im Bereich (0, 255) ist.
np.clip sorgt dafür, dass alle Werte im Bild zwischen (0, 255) liegen.
x.astype (np.uint8) sorgt dafür, dass alle Bildwerte integer sind.
Abschließend erstellt Image.fromarray (...) ein PIL-Bildobjekt aus dem NumPy-Array.
Verwenden Sie dann diese Dienstprogramme zum Erstellen des adversarial-Beispiels mit Folgendem:
Diese Funktion generiert das adversarial-Beispiel, wie zu Anfang des Abschnitts beschrieben:
y = x + r. Nehmen Sie Ihre Perturbation r und fügen Sie sie dem Originalbild x hinzu.
get _ inverse _ transform: Erhalten und wenden Sie die umgekehrte Bildtransformation an, die Sie in einigen Zeilen vorher definiert haben.
tensor _ to _ image: Konvertieren Sie abschließend den PyTorch-Tensor wieder in ein Bildobjekt.
Als Letztes ändern Sie Ihre main-Funktion so, dass sie das Laden des Bilds, das Laden der adversarial-Perturbation r, das Anwenden der Perturbation, das Speichern des adversarial-Beispiels auf Festplatte und das Ausführen der Vorhersage für das adversarial-Beispiel ausführt:
Ihre abgeschlossene Datei sollte mit step _ 3 _ adversarial.py in GitHub übereinstimmen.
Speichern Sie die Datei, beenden Sie den Editor und starten Sie Ihr Skript mit:
Sie haben nun ein Adversarial-Beispiel erstellt: Es täuscht das neuronale Netzwerk, sodass es denkt, ein Corgi sei ein Goldfisch.
Im nächsten Schritt erstellen Sie tatsächlich die Perturbation r, die Sie hier verwendet haben.
Schritt 4 - Verstehen eines Adversarial-Beispiels
Informationen zur Klassifizierung finden Sie in "How to Build an Emotion-Based Dog Filter" (" Erstellen eines emotionsbasierten Hundefilters ").
Sie erinnern sich bestimmt daran, dass Ihr Klassifizierungsmodell für jede Klasse eine Wahrscheinlichkeit ausgibt.
Bei der Inferenz prognostiziert das Modell die Klasse mit der höchsten Wahrscheinlichkeit.
Beim Training aktualisieren Sie die Modellparameter t so, dass die Wahrscheinlichkeit der richtigen Klasse y angesichts Ihrer Daten x maximiert wird.
Um jedoch Adversarial-Beispiele zu generieren, ändern Sie nun Ihr Ziel.
Anstatt eine Klasse zu finden, besteht Ihr Ziel nun darin, ein neues Bild (x) zu finden. Wählen Sie eine beliebige andere Klasse als die richtige Klasse.
Wir nennen diese neue Klasse w. Ihr neues Ziel besteht nun darin, die Wahrscheinlichkeit der falschen Klasse zu maximieren.
Beachten Sie, dass die neuronalen Netzwerkgewichtungen t aus dem obigen Ausdruck fehlen.
Das liegt daran, dass Sie nun die Rolle des Adversary übernehmen: Jemand anderes hat ein Modell trainiert und bereitgestellt.
Sie dürfen nur Adversarial-Eingaben erstellen und dürfen das bereitgestellte Modell nicht ändern.
Um das Adversarial-Beispiel x zu generieren, können Sie "training" ausführen. Allerdings aktualisieren Sie nicht die neuronalen Netzwerkgewichtungen, sondern das Eingabebild mit dem neuen Ziel.
Zur Erinnerung: In diesem Tutorial gehen Sie davon aus, dass das Adversarial-Beispiel eine affine Transformation von x ist. Mit anderen Worten: Ihr Adversarial-Beispiel nimmt die Form x + r für einige r an. Im nächsten Schritt schreiben Sie ein Skript zur Generierung dieses r.
Schritt 5 - Erstellen eines Adversarial-Beispiels
In diesem Schritt lernen Sie eine Perturbation r, sodass Ihr Corgi als Goldfisch falsch klassifiziert wird.
Erstellen Sie eine neue Datei namens step _ 5 _ perturb.py:
Importieren Sie zunächst die erforderlichen Pakete und deklarieren Sie eine main-Funktion:
Definieren Sie direkt nach Ihren Importen und vor der main-Funktion zwei Konstanten:
Die erste Konstante TARGET _ LABEL ist die Klasse, um den Corgi falsch zu klassifizieren.
In diesem Fall entspricht der Index 1 "Goldfisch".
Die zweite Konstante EPSILON ist die maximale Perturbation, die für jeden Bildwert erlaubt ist.
Diese Grenze wird eingeführt, damit das Bild unmerklich verändert wird.
Fügen Sie nach Ihren beiden Konstanten eine helper-Funktion hinzu, um ein neuronales Netzwerk und den Perturbationsparameter r zu definieren:
model.resnet18 (pretrained = True) lädt ein vortrainiertes neuronales Netzwerk namens ResNet18, wie zuvor.
Ebenso wie zuvor versetzen Sie das Modell mit .eval in den Evaluierungsmodus.
nn.Parameter (...) definiert eine neue Perturbation r, die Größe des Eingabebilds.
Das Eingabebild ist auch von der Größe (1, 3, 224, 224).
Das Schlüsselwortargument requires _ grad = True sorgt dafür, dass Sie diese Perturbation r in dieser Datei in späteren Zeilen aktualisieren können.
Als Nächstes beginnen Sie, Ihre main-Funktion zu ändern.
Laden Sie zunächst das Modell net sowie die Eingänge x und definieren die Bezeichnung label:
Als Nächstes definieren Sie sowohl das Kriterium als auch den Optimizer in Ihrer main-Funktion.
Das Kriterium sagt PyTorch, was das Ziel ist - das heißt, welcher Verlust minimiert werden soll.
Der Optimizer sagt PyTorch, wie Ihr Parameter r trainiert werden soll:
Fügen Sie direkt danach die Haupttrainingsschleife für Ihren Parameter r hinzu:
Bei jeder Iteration dieser Trainingsschleife tun Sie Folgendes:
r.data.clamp _ (...): Stellen Sie sicher, dass der Parameter r klein ist, innerhalb von EPSILON 0.
optimizer.zero _ grad (): Löschen Sie alle Gradiente, die Sie in der vorherigen Iteration berechnet haben.
model (x + r): Führen Sie Inferenz für das modifizierte Bild x + r aus.
Berechnen Sie den loss.
Berechnen Sie den Gradienten loss.backward.
Nehmen Sie einen Gradientenabstiegsschritt optimizer.step vor.
Berechnen Sie die Vorhersage pred.
Melden Sie abschließend den Verlust und die prognostizierten Klasse print (...).
Speichern Sie als Nächstes die letzte Perturbation r:
Speichern Sie direkt danach, noch in der main-Funktion, das perturbierte Bild:
Führen Sie abschließend die Vorhersage sowohl für das Originalbild als auch das Adversarial-Beispiel aus:
Überprüfen Sie sorgfältig Ihre Skriptübereinstimmungen step _ 5 _ perturb.py in GitHub.
Speichern, beenden und führen Sie das Skript aus:
Ihr Skript gibt Folgendes aus.
Die letzten beiden Zeilen zeigen, dass Sie die Gestaltung eines Adversarial-Beispiels von Grund auf nun abgeschlossen haben.
Ihr neuronales Netzwerk klassifiziert nun ein perfekt vernünftiges Corgi-Bild als Goldfisch.
Damit haben Sie gezeigt, dass neuronale Netzwerke leicht getäuscht werden können. Außerdem hat das Fehlen von Robustheit bei Adversarial-Beispielen erhebliche Folgen.
Die nächste natürliche Frage ist diese: Wie können Sie Adversarial-Beispiele bekämpfen?
Verschiedene Organisationen wie OpenAI haben viel Forschung betrieben.
Im nächsten Abschnitt führen Sie eine Verteidigungsmaßnahme aus, um dieses Adversarial-Beispiel zu vereiteln.
Schritt 6 - Verteidigen vor Adversarial-Beispielen
In diesem Schritt werden Sie eine Verteidigung gegen Adversarial-Beispiele implementieren.
Die Idee ist folgendermaßen: Sie sind nun Eigentümer der Tierklassifizierung, die in der Produktion bereitgestellt wird.
Sie wissen nicht, welche Adversarial-Beispiele generiert werden können. Sie können jedoch das Bild oder das Modell ändern, um sich vor Angriffen zu schützen.
Bevor Sie sich schützen, sollten Sie selbst sehen, wie unmerklich die Bildmanipulation ist.
Öffnen Sie die beiden folgenden Bilder:
assets / dog.jpg
outputs / adversarial.png
Hier sehen Sie beide nebeneinander.
Ihr Originalbild wird ein anderes Seitenverhältnis aufweisen.
Können Sie sagen, welches das Adversarial-Beispiel ist?
(links) Corgi als Goldfisch, Adversarial, (rechts) Corgi als Hund nicht Adversarial
Beachten Sie, dass das neue Bild genau wie das Original aussieht.
Beim linken Bild handelt es sich um Ihr Adversarial-Bild.
Laden Sie das Bild zur Sicherheit herunter und führen Sie Ihr Evaluierungsskript aus:
Dadurch wird die Klasse Goldfisch ausgeben, was als Beleg für das Adversarial-Bild dient:
Sie führen eine ziemlich naive, aber effektive Schutzmaßnahme aus: Komprimieren Sie das Bild durch Schreiben in ein verlustreiches JPEG-Format.
Öffnen Sie die interaktive Python-Aufforderung:
Laden Sie dann das Adversarial-Bild als PNG und speichern Sie nun als JPG.
Geben Sie Strg + D ein, um die interaktive Python-Eingabeaufforderung zu verlassen.
Führen Sie als Nächstes Inferenz mit Ihrem Modell für das komprimierte Adversarial-Beispiel aus:
Dadurch wird jetzt die Klasse Corgi ausgegeben, was die Wirksamkeit Ihrer einfachen Verteidigung beweist.
Sie haben nun Ihre erste Adversarial-Verteidigung abgeschlossen.
Dadurch wird die Verteidigung so effektiv.
Außerdem gibt es viele andere Formen der Verteidigung, von denen viele ein Neutrainieren des neuronalen Netzwerks beinhalten.
Allerdings sind diese Verfahren zum Neutrainieren ein eigenes Thema, das über den Umfang dieses Tutorials hinausgeht.
Damit schließt dieser Leitfaden zum Adversarial Machine Learning ab.
Um die Auswirkungen Ihrer Arbeit in diesem Tutorial zu verstehen, sehen Sie noch einmal die beiden Bilder nebeneinander an: das Original und das Adversarial-Beispiel.
Obwohl beide Bilder mit dem menschlichen Auge gleich aussehen, wurde das erste manipuliert, um Ihr Modell zu täuschen.
Beide Bilder zeigen eindeutig einen Corgi, doch ist das Modell sehr zuversichtlich, dass das zweite Modell einen Goldfisch enthält.
Das sollte Ihnen Sorge bereiten; denken Sie am Ende dieses Tutorial daran, dass Ihr Modell fragil ist.
Einfach durch Anwendung einer simplen Transformation können Sie es täuschen. Dies sind reale, plausible Gefahren, denen auch hochmoderne Forschung nicht gewachsen ist.
Forschung über Machine-Learning-Sicherheit ist genauso anfällig für diese Fehler; als Benutzer ist es Ihre Aufgabe, maschinelles Lernen sicher anzuwenden.
Weiteres Lesematerial finden Sie unter folgenden Links:
Adversarial Machine Learning Tutorial von der NeurIPS-Konferenz 2018.
Verwandte Blogeinträge OpenAI zu Adversarial-Beispielen und Robustheit gegenüber Adversarial-Angriffen.
Für mehr Informationen über maschinelles Lernen und Tutorials können Sie unsere Themenseite zum maschinellen Lernen besuchen.
Verwenden der Python-Filterfunktion
6038
Die in Python integrierte filter () -Funktion kann dazu dienen, aus einem vorhandenen iterable (wie einer Liste oder einem Wörterbuch) einen neuen iterator zu erstellen, der Elemente mithilfe einer von uns bereitgestellten Funktion effizient filtern kann.
Ein iterable ist ein Python-Objekt, bei dem "iterated over" möglich ist, d. h. Elemente werden in einer Reihenfolge zurückgegeben, die wir in einer for-Schleife verwenden können.
Die grundlegende Syntax für die Funktion filter () lautet:
Dadurch wird ein Filterobjekt zurückgegeben, das ein iterable ist.
Wir können eine Funktion wie list () verwenden, um eine Liste aller in einem Filterobjekt zurückgegebenen Elemente zu erstellen.
Die Funktion filter () bietet eine Möglichkeit, Werte zu filtern, die oft effizienter ist als eine Listen-Abstraktion, insbesondere wenn wir mit größeren Datensätzen arbeiten.
Beispielsweise erstellt eine Listen-Abstraktion eine neue Liste, was die Laufzeit dieser Verarbeitung erhöht.
So verfügen wir, nachdem unsere Listen-Abstraktion ihren Ausdruck abgeschlossen hat, über zwei Listen im Arbeitsspeicher.
Allerdings wird filter () ein einfaches Objekt erstellen, das einen Verweis auf die Originalliste, die bereitgestellte Funktion und einen Index enthält, wo in der Originalliste gesucht werden soll. Dafür wird weniger Arbeitsspeicher benötigt.
In diesem Tutorial werden wir uns vier verschiedene Methoden zur Verwendung von filter () ansehen: mit zwei verschiedenen iterable-Strukturen, mit einer lambda-Funktion und ohne definierte Funktion.
Verwenden von filter () mit einer Funktion
Das erste Argument für filter () ist eine Funktion, mit der wir entscheiden, ob die einzelnen Elemente enthalten sein oder herausgefiltert werden sollen.
Die Funktion wird für jedes Element im iterable, das als zweites Argument übergeben wird, einmal aufgerufen, und bei jeder Rückgabe von False wird der Wert gelöscht.
Da dieses Argument eine Funktion ist, können wir entweder eine normale Funktion übergeben oder lambda-Funktionen nutzen, insbesondere wenn der Ausdruck weniger komplex ist.
Im Folgenden wird die Syntax einer lambda-Funktion mit filter () dargestellt:
Mit einer Liste wie dieser können wir eine lambda-Funktion mit einem Ausdruck integrieren, gegen den wir die einzelnen Elemente aus der Liste bewerten möchten:
Um die Liste zu filtern und die Namen unserer Aquariumbewohner zu finden, die mit einem Vokal beginnen, können wir die folgende lambda-Funktion ausführen:
Hier deklarieren wir einen Punkt in unserer Liste als x. Dann legen wir unseren Ausdruck so fest, dass er auf das erste Zeichen der jeweiligen Zeichenfolge (oder Zeichen "zero ") zugreift, also x [0].
Durch Kleinschreibung aller Namen wird sichergestellt, dass Buchstaben mit der Zeichenfolge in unserem Ausdruck (aeiou) abgeglichen werden.
Abschließend übergeben wir das iterable creature _ names.
Wie im vorherigen Abschnitt wenden wir list () auf das Ergebnis an, um eine Liste aus den iterator filter () -Ergebnissen zu erstellen.
Die Ausgabe wird wie folgt aussehen:
Das gleiche Ergebnis kann mit einer selbst definierten Funktion erzielt werden:
Unsere Funktion names _ vowels definiert den Ausdruck, den wir implementieren werden, um creature _ names zu filtern.
Die Ausgabe würde erneut wie folgt aussehen:
Im Allgemeinen erzielen lambda-Funktionen mit filter () das gleiche Ergebnis, wie wenn wir eine reguläre Funktion verwenden würden.
Die Notwendigkeit zur Definition einer regulären Funktion wächst mit der Komplexität der Ausdrücke zum Filtern unserer Daten. Dadurch lässt sich in unserem Code wahrscheinlich auch eine bessere Lesbarkeit erzielen.
Verwenden von None mit filter ()
Wir können None als erstes Argument an filter () übergeben, damit der zurückgegebene iterator alle Werte ausgibt, die Python als "falsy" erachtet.
Im Allgemeinen betrachtet Python alles mit einer Länge von 0 (wie eine leere Liste oder eine leere Zeichenfolge) und alles, was 0 numerisch entspricht, als "false", daher die Verwendung des Begriffs "falsy".
Im folgenden Fall möchten wir unsere Liste so filtern, dass nur die Tanknummern unseres Aquariums angezeigt werden:
In diesem Code haben wir eine Liste, die Integer, leere Sequenzen und einen booleschen Wert enthält.
Wir verwenden die Funktion filter () mit None und übergeben die Liste aquarium _ tanks als unser iterable.
Da wir None als erstes Argument übergeben haben, prüfen wir, ob die Elemente in unserer Liste als false angesehen werden.
Dann schließen wir filtered _ tanks in eine list () -Funktion ein, damit sie beim Drucken eine Liste für filtered _ tanks zurückgibt.
Hier sehen wir, dass die Ausgabe nur die Integerwerte enthält.
Alle Elemente, die zu False ausgewertet wurden oder der Länge 0 entsprechen, wurden durch filter () entfernt:
< $> note Anmerkung: Wenn wir list () nicht verwenden und filtered _ tanks drucken, erhalten wir ein Filterobjekt, das etwa so aussieht: < Filter object at 0x7fafd5903240 >.
Das Filterobjekt ist ein iterable, sodass wir ein loop over mit for vornehmen können; alternativ können wir list () verwenden, um es in eine Liste umzuwandeln. Dies tun wir hier, da es eine gute Möglichkeit ist, die Ergebnisse zu prüfen.
Bei None haben wir filter () verwendet, um Elemente aus unserer Liste schnell zu entfernen, die als false betrachtet wurden.
Verwenden von filter () mit einer Liste von Wörterbüchern
Wenn wir eine komplexere Datenstruktur aufweisen, können wir filter () dennoch verwenden, um die einzelnen Elemente zu bewerten.
Wenn wir beispielsweise über eine Liste von Wörterbüchern verfügen, wollen wir nicht nur über jedes einzelne Element in der Liste - einem der Wörterbücher - iterieren, sondern wollen ggf. auch über jedes key: value-Paar in einem Wörterbuch iterieren, um alle Daten auszuwerten.
Als Beispiel gehen wir davon aus, dass wir eine Liste jedes einzelnen Tiers in unserem Aquarium sowie verschiedene Details zu ihnen haben:
Wir möchten diese Daten mit einer Suchzeichenfolge filtern, die wir der Funktion übergeben.
Damit filter () auf jedes Wörterbuch und jedes Element in den Wörterbüchern zugreift, richten wir eine geschachtelte Funktion ein, die wie folgt aussieht:
Wir definieren eine filter _ set () -Funktion, die aquarium _ creatures und search _ string als Parameter verwendet.
In filter _ set () übergeben wir unsere iterator _ func () als Funktion an filter ().
Die Funktion filter _ set () gibt den iterator zurück, der aus filter () resultiert.
Die iterator _ func () nimmt x als Argument, was ein Element in unserer Liste (d. h. einem einzelnen Wörterbuch) darstellt.
Als Nächstes greift die for-Schleife auf die Werte in den einzelnen key: value-Paaren in unseren Wörterbüchern zu und nutzt dann eine bedingte Anweisung, um zu prüfen, ob die search _ string in v ist, was einen Wert darstellt.
Wie in unseren vorherigen Beispielen: Wenn der Ausdruck zu True auswertet, fügt die Funktion das Element dem Filterobjekt hinzu.
Dieses wird zurückgegeben, sobald die Funktion filter _ set () abgeschlossen ist.
Wir positionieren return False außerhalb unserer Schleife, damit jedes Element in jedem Wörterbuch geprüft wird, anstatt nach der Überprüfung des ersten Wörterbuchs zurückzukehren.
Wir rufen filter _ set () mit unserer Liste der Wörterbücher und der Suchzeichenfolge auf, für die wir Übereinstimmungen finden möchten:
Nach Abschluss der Funktion ist unser Filterobjekt in der Variable filtered _ records gespeichert, die wir in eine Liste verwandeln und drucken:
Wir sehen die folgende Ausgabe aus diesem Programm:
Wir haben die Liste der Wörterbücher mit der Suchzeichenfolge 2 gefiltert. Wir können sehen, dass die drei Wörterbücher zurückgegeben wurden, die eine Tanknummer mit 2 enthielten.
Mit unserer eigenen geschachtelten Funktion können wir auf jedes einzelne Element zugreifen und effizient mit der Suchzeichenfolge abgleichen.
In diesem Tutorial haben wir die verschiedenen Möglichkeiten zur Verwendung der Funktion filter () in Python kennengelernt.
Jetzt können Sie filter () mit Ihrer eigenen Funktion, einer lambda-Funktion oder mit None verwenden, um in Datenstrukturen unterschiedlicher Komplexität nach Elementen zu filtern.
Zwar haben wir in diesem Tutorial die Ergebnisse von filter () sofort im Listenformat gedruckt, doch ist es wahrscheinlich, dass wir das zurückgegebene filter () -Objekt in unseren Programmen verwenden und die Daten weiter bearbeiten werden.
Wenn Sie mehr über Python erfahren möchten, lesen Sie unsere Reihe Codieren in Python 3 und unsere Python Themenseite.
Hosten einer Website mit Cloudflare und Nginx unter Ubuntu 20.04
6062
Cloudflare ist ein Dienst, der sich zwischen dem Besucher und dem Server des Website-Eigentümers befindet und als Reverse-Proxy für Websites fungiert.
Cloudflare bietet ein Content Delivery Network (CDN) sowie DDoS-Minderungs- und verteilte Domänennamen-Serverdienste.
Nginx ist ein beliebter Webserver, der für das Hosting einiger der größten und am stärksten frequentierten Websites im Internet verantwortlich ist.
Es ist üblich, dass Unternehmen Websites mit Nginx bereitstellen und Cloudflare als CDN- und DNS-Anbieter verwenden.
In diesem Tutorial sichern Sie Ihre von Nginx bereitgestellte Website mit einem Origin CA-Zertifikat von Cloudflare und konfigurieren Nginx anschließend für die Verwendung authentifizierter Pull-Anforderungen.
Die Verwendung dieses Setups bietet den Vorteil, dass Sie vom CDN und der schnellen DNS-Auflösung von Cloudflare profitieren und gleichzeitig sicherstellen, dass alle Verbindungen über Cloudflare erfolgen.
Dadurch wird verhindert, dass böswillige Anforderungen Ihren Server erreichen.
Sie können unserem Leitfaden zur Installation von Nginx unter Ubuntu 18.04 folgen.
Ein Cloudflare-Konto.
Eine registrierte Domäne, die Ihrem Cloudflare-Konto hinzugefügt wurde und auf Ihren Nginx-Server verweist.
Unser Leitfaden Abwehren von DDoS-Angriffen auf Ihre Website mit Cloudflare kann Ihnen dabei helfen, dies einzurichten.
Unsere Einführung in die DNS-Terminologie, -Komponenten und -Konzepte kann ebenfalls hilfreich sein.
Ein für Ihre Domäne konfigurierter Nginx-Serverblock. Befolgen Sie dazu Schritt 5 von Installieren von Nginx unter Ubuntu 18.04.
Schritt 1 - Generieren eines Origin CA TLS-Zertifikats
Mit der Cloudflare Origin-Zertifizierungsstelle können Sie ein kostenloses TLS-Zertifikat generieren, das von Cloudflare signiert wurde, um es auf Ihrem Nginx-Server zu installieren.
Mit dem von Cloudflare generierten TLS-Zertifikat können Sie die Verbindung zwischen den Cloudflare-Servern und Ihrem Nginx-Server sichern.
Um ein Zertifikat mit Origin CA zu generieren, melden Sie sich in einem Webbrowser bei Ihrem Cloudflare-Konto an.
Wählen Sie die Domäne aus, die Sie sichern möchten, und navigieren Sie zum Abschnitt SSL / TLS Ihres Cloudflare-Dashboards.
Navigieren Sie von dort zur Registerkarte Origin Server und klicken Sie auf die Schaltfläche Zertifikat erstellen:
Erstellen einer Zertifikatoption im Cloudflare-Dashboard
Lassen Sie die Standardoption Cloudflare einen privaten Schlüssel und einen CSR generieren lassen ausgewählt.
Origin CA GUI-Optionen
Klicken Sie auf Weiter und Sie sehen einen Dialog mit dem Origin Certificate und dem privaten Schlüssel.
Sie müssen sowohl das Ursprungszertifikat als auch den privaten Schlüssel von Cloudflare auf Ihren Server übertragen.
Aus Sicherheitsgründen werden die Informationen zum privaten Schlüssel nicht mehr angezeigt. Kopieren Sie den Schlüssel daher auf Ihren Server, bevor Sie auf OK klicken.
Dialog, der das Ursprungszertifikat und den privaten Schlüssel anzeigt
Sie verwenden das Verzeichnis / etc / ssl auf dem Server, um das Ursprungszertifikat und die privaten Schlüsseldateien zu speichern.
Der Ordner existiert bereits auf dem Server.
Kopieren Sie zunächst den Inhalt des Origin Certificate, das im Dialogfeld in Ihrem Browser angezeigt wird.
Öffnen Sie dann auf Ihrem Server / etc / ssl / cert.pem in Ihrem bevorzugten Texteditor:
Fügen Sie den Zertifikatinhalt zur Datei hinzu.
Speichern und schließen Sie den Editor danach.
Kehren Sie dann zu Ihrem Browser zurück und kopieren Sie den Inhalt des privaten Schlüssels.
Öffnen Sie die Datei / etc / ssl / key.pem zur Bearbeitung:
Fügen Sie den privaten Schlüssel in die Datei ein, speichern Sie die Datei und beenden Sie den Editor.
< $> note Hinweis: Wenn Sie das Zertifikat und den Schlüssel aus dem Cloudflare-Dashboard kopieren und in die entsprechenden Dateien auf dem Server einfügen, werden manchmal Leerzeilen eingefügt.
Nginx behandelt solche Zertifikate und Schlüssel als ungültig. Stellen Sie also sicher, dass es keine leeren Zeilen in Ihren Dateien gibt.
< $> warning Warnung: Das Origin CA-Zertifikat von Cloudflare wird nur von Cloudflare als vertrauenswürdig eingestuft und sollte daher nur von Ursprungsservern verwendet werden, die aktiv mit Cloudflare verbunden sind.
Wenn Sie Cloudflare zu irgendeinem Zeitpunkt anhalten oder deaktivieren, wird in Ihrem Origin CA-Zertifikat ein nicht vertrauenswürdiger Zertifikatfehler ausgegeben.
Nachdem Sie die Schlüssel- und Zertifikatdateien auf Ihren Server kopiert haben, müssen Sie die Nginx-Konfiguration aktualisieren, um sie verwenden zu können.
Schritt 2 - Installieren des Origin CA-Zertifikats in Nginx
Im vorherigen Abschnitt haben Sie mithilfe des Cloudflare-Dashboards ein Ursprungszertifikat und einen privaten Schlüssel generiert und die Dateien auf Ihrem Server gespeichert.
Jetzt aktualisieren Sie die Nginx-Konfiguration für Ihre Website, um mithilfe des Ursprungszertifikats und des privaten Schlüssels die Verbindung zwischen den Cloudflare-Servern und Ihrem Server zu sichern.
Stellen Sie zunächst sicher, dass UFW HTTPS-Datenverkehr zulässt.
Aktivieren Sie Nginx Full, wodurch sowohl Port 80 (HTTP) als auch Port 443 (HTTPS) geöffnet werden:
Laden Sie nun UFW neu:
Überprüfen Sie abschließend, ob Ihre neuen Regeln erlaubt sind und ob UFW aktiv ist:
Jetzt können Sie Ihren Nginx-Serverblock anpassen.
Nginx erstellt während der Installation einen Standard-Serverblock.
Entfernen Sie ihn, wenn er noch vorhanden ist, da Sie bereits einen benutzerdefinierten Serverblock für Ihre Domäne konfiguriert haben:
Öffnen Sie als Nächstes die Nginx-Konfigurationsdatei für Ihre Domäne:
Die Datei sollte so aussehen:
Wir ändern die Nginx-Konfigurationsdatei, um Folgendes zu tun:
Hören Sie auf Port 80 und leiten Sie alle Anfragen um, um https zu verwenden.
Hören Sie auf Port 443 und verwenden Sie das im vorherigen Abschnitt hinzugefügte Ursprungszertifikat und den privaten Schlüssel.
Ändern Sie die Datei, damit sie wie folgt aussieht:
Als Nächstes testen Sie, um sicherzustellen, dass es in keiner Ihrer Nginx-Konfigurationsdateien Syntaxfehler gibt:
Wenn keine Probleme gefunden wurden, starten Sie Nginx neu, um Ihre Änderungen zu aktivieren:
Wechseln Sie nun zum Abschnitt SSL / TLS des Cloudflare-Dashboards, navigieren Sie zur Registerkarte Übersicht und ändern Sie den SSL / TLS-Verschlüsselungsmodus in Voll (streng).
Dadurch wird Cloudflare informiert, die Verbindung zwischen Cloudflare und Ihrem ursprünglichen Nginx-Server immer zu verschlüsseln.
Aktivieren des vollen (strengen) SSL-Modus im Cloudflare-Dashboard
Besuchen Sie nun Ihre Website unter https: / / < ^ > your _ domain < ^ >, um zu überprüfen, ob sie richtig eingerichtet ist.
Sie sehen Ihre Startseite angezeigt und der Browser wird melden, dass die Website sicher ist.
Im nächsten Abschnitt richten Sie Authenticated Origin Pulls ein, um zu überprüfen, ob Ihr Ursprungsserver tatsächlich mit Cloudflare und nicht einem anderen Server spricht.
Auf diese Weise wird Nginx so konfiguriert, dass nur Anforderungen akzeptiert werden, die ein gültiges Client-Zertifikat von Cloudflare verwenden. Alle Anforderungen, die Cloudflare nicht durchlaufen haben, werden gelöscht.
Schritt 3 - Einrichten von Authenticated Origin Pulls
Mithilfe des Origin CA-Zertifikats kann Cloudflare überprüfen, ob es mit dem richtigen Origin-Server kommuniziert.
In diesem Schritt wird mithilfe der TLS-Client-Authentifizierung überprüft, ob Ihr ursprünglicher Nginx-Server mit Cloudflare kommuniziert.
Bei einem vom Client authentifizierten TLS-Handshake stellen beide Seiten ein zu überprüfendes Zertifikat bereit.
Der Ursprungsserver ist so konfiguriert, dass nur Anfragen akzeptiert werden, die ein gültiges Client-Zertifikat von Cloudflare verwenden.
Anforderungen, die Cloudflare nicht durchlaufen haben, werden abgegeben, da sie nicht über ein Cloudflare-Zertifikat verfügen.
Dies bedeutet, dass Angreifer die Sicherheitsmaßnahmen von Cloudflare nicht umgehen und keine direkte Verbindung zu Ihrem Nginx-Server herstellen können.
Cloudflare präsentiert Zertifikate, die von einer CA mit dem folgenden Zertifikat signiert werden:
Außerdem können Sie das Zertifikat direkt hier von Cloudflare herunterladen.
Kopieren Sie dieses Zertifikat.
Erstellen Sie dann die Datei / etc / ssl / cloudflare.crt, um das Zertifikat von Cloudflare zu speichern:
Fügen Sie das Zertifikat der Datei hinzu.
Aktualisieren Sie jetzt Ihre Nginx-Konfiguration, um TLS Authenticated Origin Pulls zu verwenden.
Öffnen Sie die Konfigurationsdatei für Ihre Domäne:
Fügen Sie die Direktiven ssl _ client _ certificate und ssl _ verify _ client ein, wie im folgenden Beispiel dargestellt:
Testen Sie als Nächstes, um sicherzustellen, dass in Ihrer Nginx-Konfiguration keine Syntaxfehler vorhanden sind:
Öffnen Sie schließlich den Abschnitt SSL / TLS im Cloudflare-Dashboard, navigieren Sie zur Registerkarte Origin Server und aktivieren Sie die Option Authenticated Origin Pulls.
Authenticated Origin Pulls aktivieren
Besuchen Sie nun Ihre Website unter https: / / < ^ > your _ domain < ^ >, um zu überprüfen, dass sie richtig eingerichtet wurde.
Wie zuvor sehen Sie Ihre Startseite angezeigt.
Um zu überprüfen, ob Ihr Server nur von der CA von Cloudflare signierte Anforderungen akzeptiert, aktivieren Sie die Option Authenticated Origin Pulls, um sie zu deaktivieren, und laden Sie dann Ihre Website neu.
Sie sollten folgende Fehlermeldung erhalten:
Fehlermeldung
Ihr Ursprungsserver gibt einen Fehler aus, wenn eine Anforderung nicht von der CA von Cloudflare signiert ist.
< $> note Hinweis: Die meisten Browser werden Anfragen zwischenspeichern. Um die obige Änderung zu sehen, können Sie Incognito / Private Browsing-Modus in Ihrem Browser verwenden.
Um zu verhindern, dass Cloudflare Anforderungen zwischenspeichert, während Sie Ihre Website einrichten, navigieren Sie im Cloudflare-Dashboard zu Übersicht und schalten Sie den Entwicklungsmodus um.
Nachdem Sie jetzt wissen, dass es ordnungsgemäß funktioniert, kehren Sie zum Abschnitt SSL / TLS im Cloudflare-Dashboard zurück, navigieren Sie zur Registerkarte Origin Server und schalten Sie die Option Authenticated Origin Pulls erneut um, um sie zu aktivieren.
In diesem Tutorial haben Sie Ihre Nginx-basierte Website durch Verschlüsselung des Datenverkehrs zwischen Cloudflare und dem Nginx-Server mithilfe eines Origin CA-Zertifikats von Cloudflare gesichert.
Anschließend haben Sie Authenticated Origin Pulls auf dem Nginx-Server eingerichtet, um sicherzustellen, dass nur die Anforderungen der Cloudflare-Server akzeptiert werden, sodass andere Personen keine direkte Verbindung zum Nginx-Server herstellen können.
6069
Einen Ubuntu 20.04-Server, der gemäß des Leitfadens zur Ersteinrichtung des Servers für Ubuntu 20.04 eingerichtet wurde, einschließlich eines sudo-Nicht-root-Benutzers und einer Firewall.
Sie können unserem Leitfaden zur Installation von Nginx unter Ubuntu 20.04 folgen.
Ein für Ihre Domäne konfigurierter Nginx-Serverblock. Befolgen Sie dazu Schritt 5 von Installieren von Nginx unter Ubuntu 20.04.
Sie ändern die Nginx-Konfigurationsdatei, um Folgendes zu tun:
Wenn Sie keine Probleme gefunden haben, starten Sie Nginx neu, um Ihre Änderungen zu aktivieren:
Testen Sie als Nächstes Nginx, um sicherzustellen, dass in Ihrer Nginx-Konfiguration keine Syntaxfehler vorhanden sind:
Ihr Ursprungsserver löst einen Fehler, wenn die CA von Cloudflare keine Anfrage signiert.
Installieren eines ERPNext-Stacks unter Ubuntu 20.04
6076
Der Autor hat Software in the Public Interest dazu ausgewählt, im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
ERPNext ist eine Suite für Enterprise Resource Planning (ERP), die die Leistung und Flexibilität von Open-Source-Technologien nutzt.
Sie eignet sich bestens zur Verwaltung von wichtigen Geschäftsprozessen wie Finanzen, Vertrieb, Personalverwaltung, Herstellung, Einkauf, Dienstleistungen, Helpdesk und vielem mehr.
Zu den Vorteilen der Implementierung eines Systems wie ERPNext gehören:
Höhere Produktivität durch Automatisieren wiederholter Geschäftsprozesse
Verbesserte IT-Effizienz durch Freigabe einer Datenbank für alle Abteilungen innerhalb des Unternehmens
Bessere Entscheidungsprozesse dank einer integrierten Übersicht darüber, wie Geschäftseinheiten miteinander verbunden sind
ERPNext basiert auf Frappe, einem Full-Stack-Webanwendungsframework, das in Python geschrieben wurde. Es nutzt umfassend die Node- / JavaScript-Laufzeitumgebung und verwendet MariaDB als Datenbank-Backend.
Einer der vielen Vorteile von Frappe-basierten Anwendungen wie ERPNext ist das Befehlszeilentool bench.
Die bench-CLI spart Administratoren Zeit, indem sie Aufgaben wie Installation, Aktualisierung, Konfiguration und Verwaltung verschiedener Frappe- / ERPNext-Sites automatisiert.
In diesem Tutorial installieren und konfigurieren Sie einen ERPNext-Stack auf einem Server, auf dem Ubuntu 20.04 ausgeführt wird.
Dadurch können Sie Ihren Stack je nach Bedarf für verschiedene Entwicklungs- oder Produktionsumgebungen konfigurieren. So erhalten Sie die Möglichkeit, eine komplexere und fehlertolerantere Architektur einzurichten.
Ein Ubuntu 20.04-Server mit mindestens 4 GB RAM und einem Nicht-root-Benutzer mit sudo-Berechtigungen.
Sie können Ihren Server und Benutzer einrichten, indem Sie unserem Leitfaden zur Ersteinrichtung des Servers unter Ubuntu 20.04 folgen.
< $> note Anmerkung: Bei der Auswahl der Spezifikationen Ihres Servers sollten Sie daran denken, dass ERP-Systeme ressourcenintensiv sind.
Dieser Leitfaden erfordert einen Server mit 4 GB RAM, was für grundlegende Anwendungsfälle ausreicht. Die spezifischen Hardwareanforderungen können jedoch je nach Anzahl der Benutzer sowie der Unternehmensgröße variieren.
Ein vollständig registrierter Domänenname mit einem A-Eintrag, der auf Ihren Server verweist.
Wenn Sie ein DigitalOcean-Droplet verwenden, können Sie diesem Leitfaden folgen, um Ihr DNS richtig einzurichten.
Schritt 1 & mdash; Konfigurieren der Firewall
Zwar ist die Konfiguration einer Firewall für Entwicklungsumgebungen optional, für die Produktion ist sie jedoch eine obligatorische Sicherheitsmaßnahme.
Sie müssen auf Ihrem ERPNext-Server die folgenden Ports öffnen:
80 / tcp und 443 / tcp für HTTP bzw. HTTPS
3306 / tcp für die MariaDB-Verbindung (nur empfohlen, wenn Sie Remotezugriff auf die Datenbank benötigen)
143 / tcp und 25 / tcp für IMAP bzw. STMP
22 / tcp für SSH (wenn Sie OpenSSH in Ihren UFW-Einstellungen nicht bereits aktiviert haben)
8000 / tcp zum Testen Ihrer Plattform vor Bereitstellung in der Produktion
Zum Öffnen verschiedener Ports auf einmal können Sie folgenden Befehl verwenden:
Alternativ können Sie Verbindungen von bestimmten IP-Adressen zu bestimmten Ports mit diesem Befehl zulassen:
Nach dem Öffnen aller erforderlichen Ports aktivieren Sie die Firewall:
Überprüfen Sie nun den Status Ihrer Firewall:
UFW gibt eine Liste Ihrer aktivierten Regeln aus.
Stellen Sie sicher, dass die erforderlichen Ports von ERPNext geöffnet sind:
Weitere Informationen zur Konfiguration von UFW finden Sie in unserem Leitfaden zum Einrichten einer Firewall mit UFW unter Ubuntu 20.04.
Das Einrichten einer ordnungsgemäß funktionierenden Firewall ist der erste von zwei Vorbereitungsschritten.
Jetzt konfigurieren Sie die Tastenbelegung und Zeichencodierung auf Ihrem Server.
Schritt 2 & mdash; Konfigurieren von Gebietsschemas
Es wird dringend empfohlen, die Tastenbelegung für die Konsole sowie die Sprache und die Zeichencodierung auf Ihrem Host zu konfigurieren.
Dies ist notwendig, um mögliche Probleme bei der ERPNext 12-Installation zu verhindern.
Beachten Sie, dass diese Konfiguration nichts mit der UI-Sprache in Ihrer eigentlichen ERPNext-Plattform zu tun hat, sondern mit der Systemkonfiguration des Gebietsschemas.
Aktualisieren Sie zunächst Ihren Server:
Konfigurieren Sie nun die Tastenbelegung, Sprache und Zeichencodierung:
Das Dienstprogramm localectl wird von Ubuntu 20.04 und anderen Linux-Distributionen verwendet, um systemweite Einstellungen für das Gebietsschema und die Tastaturbelegung zu steuern und zu ändern, bevor der Benutzer sich anmeldet. Das ist genau das, was ERPNext 12 benötigt.
Sie müssen Ihrer Datei / etc / environment außerdem die folgenden Zeilen hinzufügen.
Verwenden Sie nano oder Ihren bevorzugten Texteditor, um die Datei zu öffnen:
Fügen Sie jetzt den folgenden Inhalt hinzu.
Starten Sie Ihren Server neu, um alle Änderungen anzuwenden:
Geben Sie Ihrem Server einige Minuten zum Neustarten und verwenden Sie dann ssh, um Ihre Instanz neu einzugeben.
Sie sind nun bereit, Ihre Datenbank zu installieren.
Schritt 3 & mdash; Installieren von MariaDB
Jetzt fügen Sie MariaDB Ihrem Server-Stack hinzu.
ERPNext 12 erfordert MariaDB 10.2 oder höher für einen ordnungsgemäßen Betrieb.
Da Ubuntu 20.04 MariaDB 10.3 in seinen offiziellen Repositorys enthält, können Sie diese Version mit dem Befehl apt installieren:
Wenn Sie alternativ eine neuere MariaDB-Version bevorzugen, können Sie Schritt 3 unseres Leitfadens zum Installieren eines ERPNext-Stacks unter Ubuntu 18.04 folgen.
Dieser Schritt führt Sie durch den Online-Repository-Assistenten von MariaDB, der Ihnen beim Installieren der neuesten Version (MariaDB 10.5) hilft.
Installieren Sie nach der Installation von mariadb-server die folgenden Pakete:
ERPNext 12 ist eine Python-Anwendung und erfordert daher die Bibliothek python3-mysqldb für das Datenbankmanagement. libmysqlclient-dev ist für den Zugriff auf bestimmte MariaDB-Entwicklerfunktionen erforderlich.
Fügen Sie als Nächstes dem MariaDB-Server eine zusätzliche Sicherheitsschicht hinzu, indem Sie das Skript mysql _ secure _ installation ausführen:
Das Skript mysql _ secure _ installation wird Ihnen mehrere Fragen stellen:
Die erste Eingabeaufforderung wird Sie nach dem root-Passwort fragen. Da jedoch noch kein Passwort konfiguriert ist, drücken Sie ENTER.
Als Nächstes antworten Sie mit N, wenn Sie dazu aufgefordert werden, das root-Passwort für Maria DB zu ändern. Eine Verwendung des Standardpassworts zusammen mit Unix-Authentifizierung ist das empfohlene Verfahren für Ubuntu-basierte Systeme, da das root-Konto eng mit automatisierten Systemwartungsaufgaben verbunden ist.
Die übrigen Fragen haben mit dem Entfernen des anonymen Datenbankbenutzers zu tun, wobei das root-Konto so beschränkt wird, dass es sich remote bei localhost anmeldet, die Testdatenbank entfernt wird und Berechtigungstabellen neu geladen werden.
Sie können alle diese Fragen mit Y beantworten.
Nach Abschluss des Skripts mysql _ secure _ installation wird MariaDB mit ihrer Standardkonfiguration gestartet.
Die standardmäßige ERPNext-Installation verwendet für alle Datenbankoperationen den root-Benutzer von MariaDB.
Zwar mag dieser Ansatz für einzelne Serverkonfigurationen praktisch sein, doch gilt er nicht als besonders sicher.
Im nächsten Abschnitt erfahren Sie daher, wie Sie das Problem vermeiden können, indem Sie einen neuen Benutzer mit speziellen Berechtigungen erstellen.
Erstellen eines MariaDB Super Admin-Benutzers
ERPNext erwartet, dass zur Verwaltung von Datenbankverbindungen der root-Benutzer von MariaDB verwendet wird. Dies ist jedoch nicht immer ideal.
Um diese Einschränkung zu umgehen und MariaDB von einem Nicht-root-Benutzer verwalten zu lassen, erstellen Sie nun manuell eine Datenbank, die nach diesem Benutzer benannt ist.
Dann können Sie dem neuen Benutzer spezielle Berechtigungen zuweisen, um ihm ERPNext-Datenbankoperationen zu ermöglichen.
Öffnen Sie die Eingabeaufforderung von MariaDB:
Erstellen Sie nun eine neue Datenbank, die nach dem Benutzer benannt ist, den Sie für MariaDB-Verbindungen zuweisen möchten.
In diesem Tutorial wird < ^ > sammy < ^ > verwendet, Sie können jedoch einen anderen Namen wählen:
Prüfen Sie mit dieser SQL-Anweisung, ob die Datenbank erstellt wurde:
Erstellen Sie nun den MariaDB-Benutzer < ^ > sammy < ^ > mit Berechtigungen, die denen von root ähneln, und weisen Sie dem Benutzer ein starkes Passwort Ihrer Wahl zu.
Bewahren Sie das Passwort an einem sicheren Ort auf; Sie werden es später benötigen:
Prüfen Sie nun sowohl die Erstellung des Benutzers als auch die Berechtigungen des neuen Benutzers:
Bereinigen Sie nun Berechtigungen, um alle Änderungen anzuwenden:
Abschließend beenden Sie die Sitzung:
Nachdem Sie einen Datenbankbenutzer erstellt haben, müssen Sie nun nur noch MariaDB optimieren, um einen ordnungsgemäßen Betrieb von ERPNext 12 sicherzustellen.
Das ERPNext-Team verfügt zum Glück über eine ausgezeichnete Konfigurationsvorlage, die Sie als Ausgangspunkt für Ihre Implementierung verwenden werden.
Im nächsten Abschnitt erfahren Sie, wie Sie die MariaDB-Datenbank mit dieser Vorlage richtig konfigurieren.
Schritt 4 & mdash; Konfigurieren von MariaDB für ERPNext
Nach dem Installieren und Schützen von MariaDB ist es nun Zeit für die Optimierung der ERPNext-Verbindungen.
Halten Sie zunächst mariadb.service an:
Verwenden Sie nun nano oder Ihren bevorzugten Texteditor, um eine MariaDB-Konfigurationsdatei namens mariadb.cnf zu erstellen:
Fügen Sie nun die offizielle Konfigurationsvorlage von ERPNext hinzu:
Weitere Informationen zu diesen Konfigurationen finden Sie in dieser Vorlagendatei im Github-Repository von ERPNext.
Dies ist ein nützlicher Ausgangspunkt für die Erkundung der Optionen.
Die Konfigurationsdatei / etc / mysql / mariadb.conf.d / mariadb.cnf ergänzt und überschreibt auch einige Werte, die in der Standardkonfiguration von MariaDB unter / etc / mysql / my.cnf enthalten sind.
Diese Datei dient Ihnen als kuratierte Vorlage, die die Datenbankleistung für ERPNext erheblich verbessert.
Denken Sie daran, dass diese Vorlage zwar ein guter Ausgangspunkt ist, Sie die Leistung von MariaDB jedoch noch weiter verbessern können, indem Sie die Parameter an Ihre Bedürfnisse anpassen.
Testen der MariaDB-Verbindung
Da ERPNext bei fast allen internen Operationen auf die Datenbankverbindung angewiesen ist, ist es sinnvoll, die Verbindung vor dem Fortfahren zu testen.
Starten Sie mariadb.service:
Zum Testen der Verbindung können Sie folgenden Befehl verwenden.
Denken Sie daran, < ^ > sammy < ^ > und < ^ > mariadb _ password < ^ > durch Ihre Anmeldedaten zu ersetzen:
Sie erhalten eine Ausgabe mit dem grundlegenden Hilfeinhalt von MariaDB und mehreren Parametern.
Das bedeutet, dass Ihre Verbindung erfolgreich hergestellt wurde:
Wenn Sie Änderungen an den Einstellungen von MariaDB vornehmen oder Fehler beheben müssen, können Sie den Dienst mit dem folgenden Befehl neu laden:
Aktivieren Sie anschließend MariaDB:
Nachdem Sie die Datenbankverbindung getestet haben, können Sie nun mit der Installation Ihrer ERPNext-Anwendung fortfahren.
Schritt 5 & mdash; Einrichten von ERPNext 12
Nachdem Ihr Datenbank-Backend nun bereit ist, können Sie mit der Einrichtung Ihrer ERPNext-Webanwendung fortfahren.
In diesem Abschnitt erfahren Sie, wie Sie alle von ERPNext 12 benötigten Komponenten installieren und konfigurieren und dann die Anwendung selbst installieren.
Bereiten Sie zunächst mit allen Systempaketen, die ERPNext 12 benötigt, den Server vor. Installieren Sie systemweite Abhängigkeiten mit dem folgenden Befehl:
Die Variable DEBIAN _ FRONTEND = noninteractive wurde an den Installationsbefehl übergeben, um Postfix-Eingabeaufforderungen zu vermeiden.
Detaillierte Informationen zur Postfix-Konfiguration finden Sie in unserem Leitfaden zum Installieren und Konfigurieren von Postfix unter Ubuntu 20.04.
Aktualisieren Sie als Nächstes pip3, den standardmäßigen Paketmanager von Python, und installieren Sie dann die neuesten Versionen von drei zusätzlichen Python-Modulen:
setuptools erleichtert die Installation und Aktualisierung von Python-Paketen, cryptography fügt Ihrem Stack Verschlüsselungsfunktionen hinzu und psutil hilft Ihnen bei der Systemüberwachung.
Nachdem Sie alle erforderlichen globalen Abhängigkeiten installiert haben, installieren Sie nun sämtliche Dienste und Bibliotheken, die ERPNext 12 benötigt.
Einrichten von Node.js und Yarn
ERPNext 12 kann mit Version 8 der Node.js-Serverumgebung und höher zusammenarbeiten.
Zum Zeitpunkt der Verfassung dieses Dokuments verwendet das offizielle ERPNext easy _ install-Skript Node 8. Aus Sicherheitsgründen ist es jedoch ratsam, eine neuere Version zu installieren, da Node 8 2020 sein Lebensende (End of Life, EOL) erreicht hat und somit keine Sicherheitspatches mehr erhält.
Ubuntu 20.04 enthält zum Zeitpunkt der Verfassung dieses Dokuments Node.js-Version 10.19.
Zwar wird diese Version weiterhin unterstützt, doch aus ähnlichen Gründen (EOL in weniger als einem Jahr) ist es sehr ratsam, deren Verwendung zu vermeiden. Für diesen Leitfaden wird Node.js-Version 12 LTS zusammen mit den entsprechenden npm- und yarn-Paketmanagern installiert.
Bitte beachten Sie, dass das Frappe-Framework yarn zum Installieren von Abhängigkeiten verwendet.
Wenn Sie sich dazu entscheiden, eine alternative Installationsmethode zu nutzen, stellen Sie sicher, dass am Ende Version 1.12 von yarn oder höher in Ihrem System ausgeführt wird.
Fügen Sie das NodeSource-Repository zu Ihrem System hinzu:
Sie können nun den Inhalt des heruntergeladenen Skripts überprüfen:
Sobald Sie mit den Inhalten des Skripts zufrieden sind, können Sie das Skript ausführen:
Dieses Skript aktualisiert automatisch die Liste apt.
Sie können nodejs nun auf Ihrem Server installieren:
Installieren Sie als Nächstes yarn global mit dem npm-Paketmanager:
Nachdem Sie Node installiert haben, können Sie nun mit der Konfiguration von wkhtmltopdf für Ihre Plattform fortfahren.
ERPNext verwendet das Open-Source-Tool wkhtmltopdf, um HTML-Inhalte mit der Qt WebKit-Rendering-Engine in PDF zu konvertieren.
Diese Funktion dient hauptsächlich zum Drucken von Rechnungen, Angeboten und anderen Berichten.
Für ERPNext 12 wird eine spezifische Version von wkhtmltopdf (0.12.5) mit gepatchtem Qt benötigt.
Wechseln Sie zum Installieren von wkhtmltopdf zunächst in ein geeignetes Verzeichnis, in das Sie das Paket herunterladen möchten, in diesem Fall / tmp:
Laden Sie die entsprechende wkhtmltopdf-Version und das Paket für Ubuntu 20.04 von der Projektseite herunter:
Installieren Sie das Paket nun mit dem Tool dpkg:
Kopieren Sie als Nächstes alle relevanten ausführbaren Dateien in Ihr Verzeichnis / usr / bin:
Ändern Sie anschließend ihre Berechtigungen, um sie ausführbar zu machen:
Nachdem wkhtmltopdf richtig installiert ist, fügen wir nun Redis unserem Datenbank-Stack hinzu.
Installieren von Redis
ERPNext 12 verwendet Redis, um die Leistung von MariaDB zu verbessern.
Insbesondere hilft Redis beim Caching.
Installieren Sie zunächst Redis aus dem offiziellen Ubuntu 20.04-Repository:
Aktivieren Sie anschließend Redis beim Start:
Nachdem Sie Redis Ihrem Stack hinzugefügt haben, können wir uns nun einen Moment Zeit nehmen, um zusammenzufassen, was Sie bisher erreicht haben.
Wir haben alle wichtigen Komponenten installiert, die ERPNext 12 benötigt, darunter:
Ein MariaDB-Datenbank-Backend
Die Node.js-JavaScript-Serverumgebung
Den Yarn-Paketmanager
Einen Redis-Datenbankcache
Den wkhtmltopdf-Generator für PDF-Dokumente
Egal, ob Sie das ERP-System für die Entwicklung oder Produktion installieren, sind Sie nun bereit für den nächsten Schritt, bei dem das Frappe-Full-Stack-Framework und die eigentliche ERPNext-12-Webanwendung installiert werden.
Schritt 6 & mdash; Installieren der Frappe Bench-CLI
Nachdem Sie alle Stack-Voraussetzungen für ERPNext installiert haben, können Sie nun die Flexibilität des Befehlszeilentools bench von Frappe nutzen.
Die bench-CLI wurde mit dem Ziel entwickelt, Benutzer beim Installieren, Einrichten und Verwalten von Anwendungen wie ERPNext, die auf dem Frappe-Framework basieren, zu unterstützen.
In den kommenden Abschnitten installieren Sie die bench-CLI und verwenden diese dann zum Abschließen der Einrichtung von ERPNext 12.
Stellen Sie sicher, dass der Frappe-Benutzer (in diesem Fall < ^ > sammy < ^ >) über die richtigen Berechtigungen für das Verzeichnis home verfügt:
Klonen Sie nun das frappe / bench-Repository in Ihr Stammverzeichnis.
Denken Sie daran, < ^ > sammy < ^ > durch Ihren Systembenutzernamen zu ersetzen:
Installieren Sie die bench-CLI:
Dieser Leitfaden geht davon aus, dass Sie ERPNext 12 für Test- bzw. Produktionsszenarien installieren und daher die Verzweigung master verwenden.
Wenn Sie jedoch Anwendungen oder benutzerdefinierte ERPNext-Module entwickeln möchten, kann die Verzweigung develop eine bessere Option sein.
In jedem Fall sind Sie nun bereit, das Frappe-Framework zu installieren.
Dies ist der letzte Schritt vor der Installation von ERPNext selbst.
Einrichten der Frappe-Framework-Umgebung
In diesem Abschnitt erstellen Sie eine Frappe-Umgebung mithilfe der bench-CLI.
Bei der Installation von Frappe überschreiten Sie ggf. das File-Watch-Limit von Ubuntu, das standardmäßig auf 8192 festgelegt ist.
Legen Sie mit dem folgenden Befehl ein höheres Limit fest, um das zu verhindern:
Der Befehl tee fügt den Inhalt Ihres Befehls echo der aufgerufenen Datei an; außerdem wird die Ausgabe in Ihrer Konsole gedruckt.
Initialisieren Sie als Nächstes Frappe Framework 12. Ersetzen Sie Sammy durch Ihren Systembenutzernamen:
Bei der Ausführung werden möglicherweise ein Fehler zu Ihrem Pfad sowie mehrere Warnungen angezeigt.
Lassen Sie den Prozess bis zum Ende fortlaufen.
Nach seinem Abschluss sehen Sie eine Ausgabe, die der folgenden ähnelt; das bedeutet, dass Ihre Umgebung erfolgreich erstellt wurde:
< $> note Anmerkung: Der Prozess bench init kann angehalten werden, wenn ein Fehler vom Typ spawn ENOMEM auftritt.
Dieser Fehler wird ausgelöst, wenn Ihr System nicht mehr genügend Arbeitsspeicher hat.
Sie müssen das Problem vor dem Fortfahren beheben, entweder durch Installieren von mehr physischem Arbeitsspeicher oder durch Zuweisen eines Auslagerungsbereichs.
Sehen wir uns den Befehl genauer an, mit dem die Umgebung erstellt wird:
/ home / < ^ > sammy < ^ > / < ^ > frappe-bench < ^ > ist der Pfad, in dem das Frappe-Framework, die Websites und zugehörige Anwendungen installiert werden.
Es wird ein neues Verzeichnis (in diesem Beispiel namens < ^ > frappe-bench < ^ >) erstellt, um alle erforderlichen Dateien unterzubringen.
--frappe-path verweist auf das Frappe-Repository, das in diesem Fall das offizielle Github-Repository ist.
--frappe-branch ist die zu installierende Frappe-Version.
Da Sie ERPNext 12 installieren möchten, ist die gewählte Version Frappe 12.
--python ist die zu verwendende Python-Version.
ERPNext 12 erfordert Python 3.6 oder höher.
Frühere Versionen nutzen jedoch immer noch Python 2.7.
Weitere Informationen zu bench-CLI-Befehlen finden Sie im Spickzettel mit Bench-Befehlen.
Die Flexibilität, die das Frappe-Framework bietet, geht weit über die Verwendung isolierter Umgebungen hinaus.
Sie können auch verschiedene Websites erstellen und Anwendungen in ihnen installieren.
Schritt 7 & mdash; Installieren der ERPNext 12-Webanwendung
In diesem Abschnitt richten Sie eine auf Frappe basierende Site ein und installieren dann darin die ERPNext 12-Anwendung.
Wechseln Sie zu dem Verzeichnis, in dem Frappe initialisiert wurde.
Bevor Sie fortfahren, müssen Sie bestimmte Versionen von Python-Bibliotheken numpy und pandas in der virtuellen Frappe-Umgebung installieren.
Installieren Sie diese Pakete mit dem folgenden Befehl:
An diesem Punkt stoppt die Installation möglicherweise etwa 10 bis 20 Minuten lang, während die folgende Nachricht angezeigt wird:
Dies hat mit einem Fehler in Verbindung mit pandas und Ubuntu 20.04 zu tun, das zum Zeitpunkt der Verfassung dieses Dokuments noch relativ neu war.
Dennoch werden die Pakete erstellt. Danach sehen Sie eine Ausgabe, die wie folgt aussieht:
Laden Sie ERPNext 12 aus dem Repository mit der bench-CLI herunter:
Erstellen Sie als Nächstes die neue Site und ersetzen Sie < ^ > your _ domain < ^ > durch die Domäne, die Sie mit der IP-Adresse dieses Servers verknüpft haben:
Nehmen wir uns einen Moment Zeit, um die im obigen Befehl verwendeten Optionen zu betrachten:
bench new-site erstellt eine neue Site basierend auf dem Frappe-Framework.
< ^ > your _ domain < ^ > ist der Name für die neue Site.
Stellen Sie sicher, dass das DNS Ihrer Domäne über einen A-Eintrag verfügt, der auf die IP-Adresse Ihres Servers verweist.
Bewahren Sie dieses Passwort an einem sicheren Ort auf; Sie werden es in Kürze benötigen.
< ^ > mariadb _ password < ^ > ist das Passwort, das Sie am Anfang des Leitfadens für den MariaDB-Benutzer < ^ > sammy < ^ > erstellt haben.
Installieren Sie anschließend die ERPNext-Anwendung in der Site:
Nach Abschluss der Installation verfügen Sie über eine funktionierende ERPNext 12-Anwendung.
Testen wir dies nun mit einem bench-Befehl:
Mit dem oben genannten Schritt wird eine Konsole zur Echtzeitüberwachung gestartet, die Ihnen verschiedene Nachrichten zum Webserver und anderen Diensten anzeigt.
Öffnen Sie einen Webbrowser und navigieren Sie zu localhost: 8000 (bei lokalen Installationen) oder < ^ > your _ domain < ^ >: 8000 (wenn Sie einen Remoteserver verwenden).
Sie sehen den ERPNext-Anmeldebildschirm (wir werden in einem späteren Schritt mit der Anmeldung und Einrichtung fortfahren, wenn unsere Site produktionsfertig ist).
Kehren Sie nach dem Besuch Ihrer Testbereitstellung zu Ihrem Terminal zurück und drücken Sie Strg + C.
Dadurch wird ERPNext angehalten und die Überwachungskonsole beendet.
Wenn Ihr Hauptziel darin besteht, Module zu erstellen oder ERPNext 12 zu modifizieren, können Sie an diesem Punkt aufhören.
Für Entwicklungszwecke sind keine Komponenten mehr erforderlich.
Wenn Sie jedoch ein produktionsfähiges System benötigen, das keine manuelle Initialisierung voraussetzt, müssen Sie noch einige zusätzliche Komponenten installieren und konfigurieren.
Dies ist Ihr nächster Schritt.
Schritt 8 & mdash; Einrichten von ERPNext 12 für die Produktion
Zwar ist Ihre ERPNext 12-Anwendung fertig, doch ist das System insgesamt noch nicht bereit für die Produktion.
Um die Zuverlässigkeit und Sicherheit von ERPNext zu gewährleisten, müssen Sie einige zusätzliche Dienste aktivieren:
Fail2ban bietet eine zusätzliche Schutzschicht vor Brute-Force-Angriffen durch bösartige Benutzer und Bots.
Nginx dient hauptsächlich als Webproxy, der den gesamten Datenverkehr von Port 8000 zu Port 80 (HTTP) oder Port 443 (HTTPS) umleitet.
Supervisor sorgt dafür, dass die wichtigsten Prozesse von ERPNext kontinuierlich ausgeführt und bei Bedarf neu gestartet werden.
Bisher haben Sie ERPNext 12 manuell installiert und konfiguriert, sodass Sie den Prozess an einen bestimmten Anwendungsfall anpassen konnten.
Für den Rest der Produktionseinrichtung können Sie jedoch aus Gründen der Einfachheit die bench-CLI nutzen und die Installation und Konfiguration der verbleibenden Dienste automatisch erledigen lassen.
Stellen Sie sicher, dass Sie sich im Arbeitsverzeichnis Frappe befinden:
Verwenden Sie nun folgenden Befehl, um die Einrichtung von ERPNext 12 für die Produktion abzuschließen:
Dadurch werden Nginx, Supervisor und Fail2Ban installiert und konfiguriert und wird < ^ > sammy < ^ > als Eigentümer der Produktionsumgebung festgelegt.
Die Konfigurationsdateien, die mit dem Befehl bench erstellt werden, sind:
Zwei Nginx-Konfigurationsdateien, die sich unter / etc / nginx / nginx.conf und / etc / nginx / conf.d / < ^ > frappe-bench < ^ > .conf befinden
Ein Fail2Ban-Proxy-Jail unter / etc / fail2ban / jail.d / nginx-proxy.conf und ein Filter unter / etc / fail2ban / filter.d / nginx-proxy.conf
Diese Standardkonfigurationen reichen für dieses Tutorial aus; Sie sollten diese Dateien jedoch erkunden und an Ihre Anforderungen anpassen.
Sie können alle Dienste anhalten, indem Sie Folgendes ausführen:
Und wenn Sie bereit dazu sind, können Sie Ihre Dienste neu starten:
Sie sind nun in der Lage, Ihre Installation zu testen.
Testen Ihrer ERPNext 12-Installation
Vergewissern Sie sich zunächst, dass wichtige Produktionsdienste ausgeführt werden.
Verwenden Sie folgenden Befehl systemctl und übergeben Sie ihn dann an grep:
Nachdem Sie sich vergewissert haben, dass alles wie erwartet funktioniert, können Sie ERPNext 12 auf Ihrem Server live testen.
Öffnen Sie Ihren bevorzugten Browser und navigieren Sie zu < ^ > your _ domain < ^ > oder dort hin, wo Sie Ihre ERPNext 12-Anwendung hosten.
Nach wenigen Sekunden sollten Sie den Anmeldebildschirm von ERPNext 12 sehen.
Verwenden Sie Administrator als Benutzernamen (email) und das zuvor für das Passwort erstellte < ^ > erpnext _ admin _ password < ^ >.
Anmeldefenster von ERPNext
Im nächsten Bildschirm sehen Sie ein Dropdownmenü, in dem Sie die Sprache der Benutzeroberfläche für die Anwendung auswählen können:
Sprachauswahl
Nach der Sprachauswahl wird Sie ERPNext zur Eingabe Ihres Lands, Ihrer Zeitzone und der Währung auffordern:
Wählen Sie Ihre Region aus
Sobald Sie die Regionsinformationen festgelegt haben, können Sie Ihren ersten ERPNext-Benutzer erstellen.
Die von Ihnen angegebenen Informationen werden als Anmeldedaten des Benutzers verwendet.
Erster ERPNext-Benutzer
Im nächsten Bildschirm werden Sie nach etwas gefragt, das ERPNext Domänen nennt.
Wenn Sie sich nicht sicher sind, wie Ihre Domäne lautet, wählen Sie als Nächstes Distribution und klicken Sie auf die Schaltfläche Weiter.
Wählen Sie Ihre Domänen aus
Als Nächstes müssen Sie einen Firmennamen und eine Abkürzung angeben.
Firmenname
Im letzten Bildschirm fragt Sie ERPNext nach dem, was Ihre Firma tut, nach dem Namen ihrer Bank, nach der Art der Kontenpläne und nach der Geschäftsjahresperiode.
Sie können später weitere Banken eingeben.
Füllen Sie vorerst alle Felder wie gewünscht aus und klicken Sie dann auf die Schaltfläche Complete Setup (Einrichtung abschließen).
Finanzdaten
Als Nächstes sehen Sie eine Fortschrittsleiste.
Einrichten von ERPNext
Nach Abschluss der Einrichtung wird das Haupt-Dashboard von ERPNext 12 angezeigt.
ERPNext 12-Dashboard
Sie haben nun eine ERPNext 12-Anwendung fertig installiert und konfiguriert.
Nachdem Sie Ihre ERPNext 12-Anwendung richtig installiert haben, können Sie nun mit der Anpassung des Systems an Ihre geschäftlichen Bedürfnisse beginnen.
Ein guter Ausgangspunkt ist ein Klick auf die Schaltfläche Getting Started (Erste Schritte) im ERPNext-Dashboard.
ERPNext hilft Ihnen dann bei der Konfiguration der Plattform für alle Ihre geschäftlichen und E-Commerce-Anforderungen.
Erste Schritte
Vielleicht möchten Sie auch die Geschwindigkeit von ERPNext erhöhen.
In diesem Fall können Sie mehr über die Leistungsoptimierung bei ERPNext erfahren; hier erhalten Sie Informationen über bewährte Praktiken und die Behebung von Leistungsproblemen.
Einrichten eines Remotedesktops mit X2Go unter Ubuntu 20.04
6064
Der Autor hat Software in the Public Interest (SPI) dazu ausgewählt, im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
In der Regel verfügen Linux-basierte Server nicht über eine vorinstallierte grafische Benutzeroberfläche (GUI).
Wann immer Sie GUI-Anwendungen in Ihrer Instanz ausführen möchten, besteht die typische Lösung aus der Verwendung von Virtual Network Computing (VNC).
Leider können VNC-Lösungen jedoch langsam und unsicher sein; viele von ihnen sind außerdem mit einem hohen manuellen Konfigurationsaufwand verbunden.
Im Gegensatz dazu bietet X2Go einen funktionierenden "Cloud-Desktop" mit allen Vorteilen eines stets verfügbaren, aus der Ferne zugänglichen und leicht skalierbaren Rechensystems, das zudem über ein schnelles Netzwerk verfügt.
Es reagiert schneller und ist sicherer als viele VNC-Lösungen.
In diesem Tutorial verwenden Sie X2Go zum Erstellen einer XFCE-Desktopumgebung unter Ubuntu 20.04, auf die Sie remote zugreifen können.
Dieser Cloud-Desktop umfasst dieselben Dienstprogramme, die Sie erhalten würden, wenn Sie Ubuntu 20.04 und die XFCE-Umgebung auf Ihrem persönlichen Computer installieren (fast identisch mit einer Xubuntu-Einrichtung).
Die in diesem Tutorial beschriebene Einrichtung ist in folgenden Fällen nützlich:
Sie benötigen Zugriff auf ein Linux-basiertes Betriebssystem, das mit einer Desktopumgebung ausgestattet ist, können es aber nicht auf Ihrem persönlichen Computer installieren.
Sie verwenden mehrere Geräte an verschiedenen Orten und wünschen sich eine konsistente Arbeitsumgebung mit den gleichen Werkzeugen, Erscheinungsbildern, Dateien und Leistungswerten.
Ihr Internetdienstleister bietet Ihnen nur wenig Bandbreite, Sie benötigen aber Zugriff auf Dutzende oder Hunderte Gigabyte von Daten.
Lang laufende Aufträge führen dafür dazu, dass Ihr lokaler Computer Stunden oder Tage lang nicht verfügbar ist.
Stellen Sie sich vor, dass Sie ein großes Projekt erstellen müssen, das auf Ihrem Laptop 8 Stunden dauert.
Sie können währenddessen keine Filme anschauen oder andere ressourcenintensive Anwendungen nutzen.
Wenn Sie diesen Auftrag jedoch auf Ihrem Server ausführen, kann Ihr Computer andere Aufgaben erledigen.
Sie arbeiten mit einem Team, für das es vorteilhaft wäre, wenn es einen gemeinsamen Computer hätte, auf den es zur Zusammenarbeit an einem Projekt zugreifen kann.
Eine Ubuntu 20.04 x64-Instanz mit 2 GB RAM oder mehr.
2 GB ist das Minimum; ein Server mit 4 GB oder mehr ist jedoch ideal, wenn Sie arbeitsspeicherintensive Anwendungen ausführen möchten.
Sie können ein DigitalOcean-Droplet verwenden, wenn Sie wollen.
Einen Benutzer mit sudo-Berechtigungen und einen SSH-Schlüssel.
Folgen Sie diesem Leitfaden, um zu beginnen: Ersteinrichtung des Servers unter Ubuntu 20.04.
Stellen Sie sicher, dass Sie Schritt 4 abschließen und Ihre Firewall so konfigurieren, dass alle Verbindungen außer OpenSSH eingeschränkt werden.
Schritt 1 & mdash; Installieren der Desktopumgebung auf Ihrem Server
Nachdem Ihr Server ausgeführt wird und Ihre Firewall konfiguriert ist, können Sie nun die grafische Umgebung für den X2Go-Server installieren.
Aktualisieren Sie zunächst die Informationen des Paketmanagers zur neuesten verfügbaren Software:
In diesem Tutorial installieren Sie XFCE als Desktopumgebung.
XFCE verwendet keine grafischen Effekte wie Compositing, sodass es mit X2Go und der Optimierung von Bildschirmaktualisierungen besser kompatibel ist.
Hinweis: Die LXDE- und die MATE-Desktopumgebung (mit deaktiviertem Compositing) funktionieren ebenfalls; Sie müssen jedoch den Befehl in diesem Tutorial ändern, mit dem die Desktopumgebung installiert wird.
Statt sudo apt-get install xubuntu-desktop geben Sie zum Beispiel sudo apt-get install < ^ > lubuntu < ^ > -desktop ein, um LXDE zu installieren.
Es gibt zwei Möglichkeiten zum Installieren von XFCE: die minimale Desktopumgebung oder die vollständige Desktopumgebung.
Die optimale Option hängt von Ihren Bedürfnissen ab; auf diese gehen wir als Nächstes ein.
Wählen Sie eine von beiden aus.
Die vollständige Desktopumgebung
Für die meisten Anwendungsfälle empfohlen.
Wenn Sie nicht jede Komponente, die Sie benötigen, manuell auswählen und stattdessen einen Standardsatz von Paketen wie Textverarbeitungsprogramm, Webbrowser, E-Mail-Client und anderem zuvor installierten Zubehör nutzen möchten, können Sie xubuntu-desktop wählen.
Installieren und konfigurieren Sie die vollständige Desktopumgebung.
Die vollständige Desktopumgebung ähnelt dem, was Sie erhalten würden, wenn Sie Xubuntu von einer DVD / einem bootfähigen USB-Stick auf Ihrem lokalen PC installieren:
Wenn Sie zur Auswahl eines Displaymanagers aufgefordert werden, wählen Sie lightdm aus.
Wählen von lightdm als Displaymanager
Die minimale Desktopumgebung
Wenn Sie alternativ einen kleinen Kernsatz an Paketen installieren und dann darauf aufbauen möchten, indem Sie das manuell hinzufügen, was Sie benötigen, können Sie das Meta-Paket xubuntu-core verwenden.
Ein Meta-Paket enthält keine einzelnen Pakete; vielmehr enthält ein Meta-Paket eine ganze Paketsammlung.
Durch die Installation eines Meta-Pakets erspart sich der Benutzer die manuelle Installation zahlreicher Komponenten.
Installieren Sie xfce4 und alle zusätzlichen Abhängigkeiten, die zur Unterstützung erforderlich sind:
Sie haben eine grafische Umgebung installiert.
Jetzt schaffen Sie eine Möglichkeit, sie remote anzuzeigen.
Schritt 2 & mdash; Installieren von X2Go auf dem Server
X2Go verfügt über zwei Hauptkomponenten: den Server, der die grafische Sitzung auf dem Remotecomputer startet und verwaltet, und den Client, den Sie auf Ihrem lokalen Computer installieren, um den Remotedesktop oder die Remoteanwendung anzuzeigen und zu steuern.
In früheren Versionen von Ubuntu (vor 18.04) war x2goserver nicht in den Standard-Repositorys enthalten; in diesem Fall müssten Sie Schritte wie diese befolgen, um sich das Softwarepaket zu beschaffen.
Wir hinterlassen hier den Link zu Referenzzwecken, sollte das Paket in zukünftigen Versionen von Ubuntu weggelassen werden.
Zum Glück enthält Ubuntu 20.04, Codename Focal Fossa, das benötigte Paket in seinen Standard-Repositorys, sodass die Installation schneller verläuft.
Geben Sie zum Installieren von X2Go auf Ihrem Server folgenden Befehl ein:
Aktuell erfordert Ihr Server keine weitere Einrichtung.
Denken Sie jedoch daran, dass Sie Ihren SSH-Schlüssel auf jedem lokalen Rechner verfügbar haben müssen, den Sie verwenden möchten, wenn Sie der Empfehlung zur Einrichtung von SSH-Schlüsseln im Leitfaden zur Ersteinrichtung des Servers unter Ubuntu 20.04 gefolgt sind.
Wenn Sie keinen privaten SSH-Schlüssel eingerichtet haben, stellen Sie sicher, dass Sie ein starkes Passwort wählen.
< $> note Anmerkung: Denken Sie daran, dass der Linux-Kernel bei ausgelastetem RAM bestimmte Anwendungen abrupt beenden kann, was zu verlorener Arbeit führt.
Wenn Sie ein DigitalOcean-Droplet verwenden und feststellen, dass Ihre Programme mehr RAM benötigen, können Sie Ihr Droplet vorübergehend deaktivieren und ein Upgrade auf eines mit mehr Arbeitsspeicher vornehmen (resize).
Sie haben Ihren Server konfiguriert.
Geben Sie exit ein oder schließen Sie Ihr Terminalfenster.
In den restlichen Schritten geht es um die Konfiguration des Clients auf Ihrem lokalen Rechner.
Schritt 3 & mdash; Lokales Installieren des X2Go-Clients
X2Go ist sofort einsatzbereit.
Wenn Sie auf Ihrem lokalen Computer Windows oder Mac OS X verwenden, können Sie hier die X2Go-Clientsoftware herunterladen.
Wenn Sie Debian oder Ubuntu nutzen, können Sie den X2Go-Client mit diesem Befehl auf Ihrem lokalen Rechner installieren:
Nach dem Herunterladen der Software können Sie sie installieren. Öffnen Sie dazu das Installationsprogramm und wählen Sie Ihre bevorzugte Sprache aus.
Stimmen Sie der Lizenz zu und lassen Sie sich vom Assistenten durch die verbleibenden Schritte führen.
Normalerweise sollte es keinen Grund geben, die in diesen Schritten vorausgefüllten Standardwerte zu ändern.
X2Go funktioniert gut mit den Standardeinstellungen, ist aber genau anpassbar.
Wenn Sie weitere Informationen wünschen, besuchen Sie die offizielle Dokumentation von X2Go.
Nachdem Sie den Desktop-Client installiert haben, können Sie nun dessen Einstellungen konfigurieren und sich mit dem X2Go-Server verbinden, um Ihren Remote-XFCE-Desktop zu verwenden.
Schritt 4 & mdash; Herstellen einer Verbindung zum Remotedesktop
Wenn Sie den X2Go-Client zum ersten Mal öffnen, erscheint ein Fenster.
Wenn nicht, klicken Sie im linken oberen Menü auf Session (Sitzung) und wählen Sie dann New session... (Neue Sitzung...).
X2Go-Client-Screenshot - Erstellen einer neuen Sitzung
Geben Sie im Feld Session name (Sitzungsname) etwas ein, das Ihnen dabei hilft, zwischen Servern zu unterscheiden.
Die Verwendung eines Sitzungsnamens ist besonders nützlich, wenn Sie Verbindungen zu verschiedenen Computern planen.
Geben Sie die IP-Adresse Ihres Servers oder einen vollständig qualifizierten Domänennamen (FQDN) in das Feld Host ein, das sich unter Server befindet.
Geben Sie den Benutzernamen, den Sie für Ihre SSH-Verbindung verwendet haben, in das Feld Login (Anmelden) ein.
Da Sie in Schritt 2 XFCE installiert haben, wählen Sie XFCE als Ihren Session Type (Sitzungstyp).
Nachdem Sie die Verbindung zum Server mit SSH-Schlüsseln hergestellt haben, klicken Sie schließlich auf das Ordnersymbol neben Use RSA / DSA key for ssh connection (RSA / DSA-Schlüssel für SSH-Verbindung verwenden) und suchen Sie nach Ihrem privaten Schlüssel.
Wenn Sie sich nicht dazu entschieden haben, die sichereren SSH-Schlüssel zu verwenden, lassen Sie dieses Feld leer; der X2Go-Client fragt bei der Anmeldung jedes Mal nach einem Passwort.
Die restlichen Standardeinstellungen reichen vorerst aus; wenn Sie sich jedoch weiter mit der Software vertraut machen, können Sie den Client anhand Ihrer individuellen Präferenzen optimieren.
Nach dem Drücken der OK-Schaltfläche können Sie Ihre grafische Sitzung starten, indem Sie auf das weiße Feld klicken, das den Namen Ihrer Sitzung auf der rechten oberen Seite des Felds enthält.
X2Go-Hauptfenster - Sitzungsliste
Wenn Sie OS X auf Ihrem lokalen Rechner ausführen, kann OS X Sie dazu auffordern, XQuartz zu installieren, was zum Ausführen von X11 erforderlich ist.
Befolgen Sie in diesem Fall nun die Anweisungen zum Installieren des Servers.
In wenigen Sekunden wird Ihr Remotedesktop angezeigt und Sie können mit ihm interagieren.
Es gibt einige nützliche Tastenkombinationen, die Sie in Windows- und Linux-basierten Betriebssystemen für ein besseres Erlebnis verwenden können.
< $> note Anmerkung: Diese beiden ersten Optionen können in modernen Windows-Editionen zu fehlerhaftem Verhalten führen.
Sie können sie jetzt dennoch für den Fall testen, dass spätere Versionen von X2Go die Probleme beheben.
Wenn sie fehlschlagen, vermeiden Sie in Zukunft einfach die Verwendung derselben Tastenkombination.
Strg + Alt + F schaltet den Vollbildmodus ein und aus.
Die Arbeit im Vollbildmodus kann sich mehr wie eine lokale Desktop-Erfahrung anfühlen.
Der Vollbildmodus hilft außerdem dem Remoterechner beim Erfassen von Tastenkombinationen anstelle Ihres lokalen Computers.
Strg + Alt + M minimiert die Remoteansicht, selbst wenn Sie sich im Vollbildmodus befinden.
Strg + Alt + T trennt die Sitzung, lässt die GUI auf dem Server aber weiter laufen.
Es handelt sich um eine schnelle Methode zur Trennung der Verbindung ohne Abmelden oder Schließen von Anwendungen auf dem Server.
Das Gleiche geschieht, wenn Sie auf die Schaltfläche zum Schließen des Fensters klicken.
Schließlich gibt es zwei Möglichkeiten, um die Remotesitzung zu beenden und alle darin laufenden grafischen Programme zu schließen. Sie können sich remote aus dem Startmenü von XFCE abmelden; oder Sie können auf die mit einem Kreis und einem kleinen Strich (wie ein Ein / Aus- / Standby-Symbol) markierte Schaltfläche klicken, die sich im Hauptbereich des X2Go-Bildschirms unten rechts befindet.
Die erste Methode ist sauberer, kann aber dazu führen, dass Programme wie Software zur Sitzungsverwaltung weiter ausgeführt werden.
Die zweite Methode schließt alles und erzwingt dies auch, wenn ein Prozess nicht sauber beendet werden kann.
In jedem Fall sollten Sie Ihre Arbeit vor dem Fortfahren speichern.
X2Go-Hauptfenster - Schaltfläche "Terminate Session" (Sitzung beenden)
Sie haben Ihren Remotedesktop nun erfolgreich aufgerufen und konfiguriert.
In diesem Tutorial haben Sie X2Go verwendet, um für das Ubuntu-Betriebssystem eine robuste und Remote-basierte GUI-Umgebung einzurichten.
Da der Desktop nun ausgeführt wird, hier einige Ideen zu seiner Verwendung:
Sie können Ihre Entwicklungsarbeiten durch Erstellen eines git-Repositorys zentralisieren.
Sie können einen IDE- / Code-Editor wie NetBeans oder Eclipse installieren.
Sie können auch Visual Studio Code zur Remoteentwicklung über das Remote-SSH Plugin verwenden.
Sie können einen Webserver zum Testen von Webanwendungen konfigurieren.
Sie können Ihren Remotedesktop um ein gutes Sicherungsschema erweitern, um Ihre Arbeitsumgebung und wesentliche Daten zu bewahren, falls mal etwas schiefgeht.
Mit DigitalOcean können Sie außerdem Snapshots Ihrer Droplets erstellen, wenn Sie mit einer bestimmten Einrichtung zufrieden sind.
Auf diese Weise können Sie riskante Änderungen testen und immer wieder zu einem bekannten funktionsfähigen Status zurückkehren.
Wenn Sie mehr erfahren möchten, besuchen Sie die offizielle Dokumentations-Website von X2Go.
Erstellen eines Discord-Bots mit Node.js
6155
Discord ist eine Chat-Anwendung, die Millionen von Benutzern auf der ganzen Welt für Messaging und Voice Chat nutzen - in Communities, die Gilden oder Server genannt werden.
Außerdem bietet Discord eine umfangreiche API, die Entwickler zum Einrichten leistungsfähiger Discord-Bots verwenden können.
Bots können verschiedene Aktionen wie das Senden von Nachrichten an Server, das Ausführen von DM-Befehlen für Benutzer, das Moderieren von Servern und das Abspielen von Audio in Voice Chats erledigen.
So können Entwickler leistungsstarke Bots entwickeln, die erweiterte, anspruchsvolle Merkmale wie Moderationstools oder auch Spiele umfassen.
Der Utility-Bot Dyno zum Beispiel stellt Millionen von Gilden bereit und bietet nützliche Merkmale wie Spamschutz, einen Musikplayer und andere Dienstprogrammfunktionen.
Wenn Sie wissen, wie man Discord-Bots erstellt, können Sie viele Möglichkeiten implementieren, mit denen Tausende von Menschen täglich interagieren können.
In diesem Tutorial erstellen Sie von Grund auf einen Discord-Bot mit Node.js und der Discord.js-Bibliothek, sodass Benutzer direkt mit der Discord-API interagieren können.
Sie werden ein Profil für einen Discord-Bot einrichten, Authentifizierungstoken für den Bot erhalten und den Bot mit der Fähigkeit zur Verarbeitung von Befehlen mit Argumenten von Benutzern programmieren.
Einen Texteditor Ihrer Wahl wie Visual Studio Code, Atom, Sublime oder Nano.
Ein kostenloses Discord-Konto mit einem verifizierten E-Mail-Konto und einem kostenlosen Discord-Server, den Sie zum Testen Ihres Discord-Bots verwenden werden.
Schritt 1 - Einrichten eines Discord-Bots
In diesem Schritt verwenden Sie die Entwickler-GUI von Discord, um einen Discord-Bot einzurichten und das Token des Bots zu empfangen, das Sie an Ihr Programm übergeben werden.
Um einen Bot in der Discord-Plattform zu registrieren, verwenden Sie das Dashboard der Discord-Anwendung.
Hier können Entwickler Discord-Anwendungen einschließlich Discord-Bots erstellen.
Abbildung des Dashboards der Discord-Anwendung nach dem ersten Besuch von https: / / discord.com / developers / applications
Um zu beginnen, klicken Sie auf Neue Anwendung.
Discord wird Sie bitten, einen Namen für Ihre neue Anwendung einzugeben.
Klicken Sie dann auf Erstellen, um die Anwendung zu erstellen.
Abbildung der Eingabeaufforderung für die Erstellung einer Anwendung, mit "Test Node.js Bot" als Name der Anwendung
< $> note Anmerkung: Der Name Ihrer Anwendung ist unabhängig vom Namen des Bots und der Bot muss nicht den gleichen Namen tragen wie die Anwendung.
Öffnen Sie nun Ihr Anwendungs-Dashboard.
Um der Anwendung einen Bot hinzuzufügen, navigieren Sie links in der Navigationsleiste zur Registerkarte Bot.
Abbildung der Registerkarte Bot des Anwendungs-Dashboards
Klicken Sie auf die Schaltfläche Bot hinzufügen, um der Anwendung einen Bot hinzuzufügen.
Klicken Sie auf die Schaltfläche Ja, los!,
wenn sie Sie zur Bestätigung aufgefordert werden.
Nun befinden Sie sich in einem Dashboard, das Details wie den Namen des Bots, das Authentifizierungstoken und das Profilbild enthält.
Dashboard mit Details Ihres Bots
Sie können den Namen oder das Profilbild Ihres Bots hier im Dashboard ändern.
< $> warning Achtung: Teilen oder laden Sie Ihr Bot-Token niemals hoch, damit sich andere Personen nicht bei Ihrem Bot anmelden können. < $>
Jetzt müssen Sie eine Einladung erstellen, mit der Sie die Discord-Gilden des Bots hinzufügen, in denen Sie den Bot testen können. Navigieren Sie zunächst zum Tab OAuth2 des Anwendungs-Dashboards.
Um eine Einladung zu erstellen, scrollen Sie nach unten und wählen Sie unter Bereiche Bot aus.
Außerdem müssen Sie Berechtigungen festlegen, um zu kontrollieren, welche Aktionen Ihr Bot in Gilden ausführen darf.
Wählen Sie für die Zwecke dieses Tutorials Administrator, wodurch Ihr Bot die Berechtigung erhält, fast alle Aktionen in Gilden auszuführen.
Kopieren Sie den Link mit der Schaltfläche Kopieren.
OAuth2-Registerkarte, mit dem Bereich auf "bot" und Berechtigungen auf "administator" gesetzt
Als Nächstes fügen Sie den Bot einem Server hinzu.
Folgen Sie dem gerade erstellten Einladungs-Link.
Sie können den Bot jedem Server hinzufügen, den Sie besitzen oder für den Sie über Administratorberechtigungen verfügen (über das Dropdownmenü).
Seite nach dem Folgen des Einladungs-Links, über die Benutzer den Bot Servern hinzufügen können
Klicken Sie nun auf Weiter.
Stellen Sie sicher, dass Sie das Kontrollkästchen neben Administrator aktiviert haben - dadurch erhält der Bot Administratorberechtigungen.
Klicken Sie dann auf Autorisieren.
Discord wird Sie bitten, eine CAPTCHA zu lösen, bevor der Bot dem Server beitritt.
Sie werden den Discord-Bot nun auf der Mitgliederliste in dem Server sehen, dem Sie den Bot unter offline hinzugefügt haben.
Mitgliederliste eines Discord-Servers mit dem neu erstellten Bot unter dem Abschnitt "offline" der Mitgliederliste
Sie haben erfolgreich einen Discord-Bot erstellt und einem Server hinzugefügt.
Als Nächstes schreiben Sie ein Programm, um sich bei dem Bot anzumelden.
Schritt 2 - Erstellen Ihres Projekts
In diesem Schritt richten Sie die grundlegende Codierungsumgebung ein, in der Sie Ihren Bot erstellen und sich programmatisch beim Bot anmelden werden.
Zuerst müssen Sie einen Projektordner und die erforderlichen Projektdateien für den Bot einrichten.
Erstellen Sie Ihren Projektordner:
Wechseln Sie in den gerade erstellten Projektordner:
Als Nächstes verwenden Sie Ihren Texteditor, um eine Datei namens config.json zu erstellen und das Authentifizierungstoken Ihres Bots zu speichern:
Fügen Sie dann den folgenden Code der config-Datei hinzu und ersetzen Sie den hervorgehobenen Text durch das Authentifizierungstoken Ihres Bots:
Als Nächstes erstellen Sie eine package.json-Datei, in der Details Ihres Projekts und Informationen über die Abhängigkeiten gespeichert werden, die Sie für das Projekt verwenden werden.
Sie erstellen eine package.json-Datei, indem Sie den folgenden npm-Befehl ausführen:
npm wird Sie nach verschiedenen Details zu Ihrem Projekt fragen.
Wenn Sie eine Anleitung für diese Eingabeaufforderungen wünschen, konsultieren Sie Verwenden von Node.js-Modulen mit npm und package.json.
Sie installieren nun das discord.js-Paket, das Sie zur Interaktion mit der Discord-API verwenden werden.
Sie können discord.js über npm mit dem folgenden Befehl installieren:
Nachdem Sie die Konfigurationsdatei eingerichtet und die erforderliche Abhängigkeit installiert haben, können Sie nun mit der Einrichtung Ihres Bots beginnen. In einer realen Anwendung würde ein großer Bot auf viele Dateien verteilt, aber für die Zwecke dieses Tutorials wird sich der Code Ihres Bots in einer Datei befinden.
Erstellen Sie zunächst für den Code eine Datei mit dem Namen index.js im Ordner < ^ > discord-bot < ^ >:
Beginnen Sie mit dem Codieren des Bots, indem Sie die discord.js-Abhängigkeit und die Konfigurationsdatei mit dem Token des Bots vorschreiben:
Fügen Sie danach die folgenden zwei Codezeilen hinzu:
Die erste Zeile des Codes erstellt einen neuen Discord.Client und weist ihn der Konstanten client zu.
Dieser Client ist ein Teil davon, wie Sie mit der Discord-API interagieren werden und wie Discord Sie bei Ereignissen wie neuen Meldungen benachrichtigen wird.
Der Client ist in Wirklichkeit der Discord-Bot.
Die zweite Zeile des Codes verwendet die login-Methode für den Client, um sich bei dem von Ihnen erstellten Discord-Bot anzumelden, wobei das Token in der Datei config.json als Passwort verwendet wird.
Mit dem Token erfährt die Discord-API, an welches Programm sich der Bot richtet und dass Sie für die Nutzung des Bots authentifiziert sind.
Führen Sie nun mit Node die Datei index.js aus:
Der Status Ihres Bots wird sich auf dem Discord-Server, dem er hinzugefügt wurde, in "online" ändern.
Abbildung des Bots im Online-Zustand
Sie haben erfolgreich eine Codierungsumgebung eingerichtet und den grundlegenden Code für die Anmeldung bei einem Discord-Bot erstellt. Im nächsten Schritt werden Sie Benutzerbefehle verwalten und Ihren Bot zur Durchführung von Aktionen veranlassen, wie z. B. zum Senden von Nachrichten.
Schritt 3 - Verwendung Ihres ersten Benutzerbefehls
In diesem Schritt erstellen Sie einen Bot, der Benutzerbefehle handhaben kann.
Sie werden Ihren ersten Befehl (ping) implementieren, der mit "pong" und der Zeit antworten wird, die zum Antworten auf den Befehl benötigt wurde.
Zunächst müssen Sie alle Nachrichten erkennen und empfangen, die Benutzer senden, damit Sie Befehle verarbeiten können.
Mit der Methode on auf dem Discord-Client wird Ihnen Discord eine Benachrichtigung zu neuen Ereignissen senden.
Die Methode on hat zwei Argumente: den Namen eines Ereignisses, auf das gewartet wird, und eine Funktion, die jedes Mal ausgeführt wird, wenn das Ereignis eintritt.
Bei dieser Methode können Sie auf das Ereignis message warten - es wird jedes Mal eintreten, wenn eine Nachricht an eine Gilde gesendet wird, in der der Bot die Berechtigung zum Anzeigen von Nachrichten hat.
Lassen Sie uns daher eine Funktion erstellen, die bei jeder Übermittlung einer Nachricht ausgeführt wird, um Befehle zu verarbeiten.
Öffnen Sie zunächst Ihre Datei:
Diese Funktion, die beim Ereignis message ausgeführt wird, nutzt message als Parameter. message wird den Wert einer Discord.js message-Instanz haben, die Informationen über die gesendete Nachricht und Methoden enthält, um dem Bot beim Antworten zu helfen.
Fügen Sie nun Ihrer Befehlsverarbeitungsfunktion die folgende Codezeile hinzu:
Diese Zeile prüft, ob der Autor der Nachricht ein Bot ist; wenn ja, wird die Verarbeitung des Befehls gestoppt.
Dies ist wichtig, da Sie Nachrichten von Bots im Allgemeinen weder bearbeiten noch beantworten möchten.
Bots müssen oder wollen unseren Bot in der Regel nicht verwenden, sodass ein Ignorieren ihrer Nachrichten Rechenleistung spart und unbeabsichtigte Antworten verhindert.
Jetzt schreiben Sie einen Befehlshandler.
Um das zu erreichen, ist es hilfreich, das übliche Format eines Discord-Befehls zu verstehen.
In der Regel enthält die Struktur eines Discord-Befehls drei Teile in der folgenden Reihenfolge: ein Präfix, einen Befehlsnamen und (manchmal) Befehlsargumente.
Abbildung eines typischen Discord-Befehls, der "!
add 1 2 "lautet
Präfix: Das Präfix kann alles sein, ist aber in der Regel eine Interpunktion oder abstrakte Phrase, die normalerweise nicht am Anfang einer Nachricht stehen würde.
Das bedeutet, dass bei Eingabe des Präfix am Anfang der Nachricht der Bot weiß, dass der Befehl von einem Bot verarbeitet werden soll.
Befehlsname: Der Name des Befehls, den der Benutzer verwenden möchte.
Das bedeutet, dass der Bot mehrere Befehle mit unterschiedlicher Funktionalität unterstützen kann und Benutzer durch Angabe eines anderen Befehlsnamens zwischen ihnen wählen können.
Argumente: Wenn der Befehl ggf. zusätzliche Informationen vom Benutzer benötigt oder verwendet, kann der Benutzer nach dem Befehlsnamen Argumente angeben, wobei jedes Argument durch ein Leerzeichen getrennt wird.
< $> note Anmerkung: Es gibt keine zwingende Befehlsstruktur; Bots können Befehle verarbeiten, wie sie wollen. Die hier dargestellte Struktur ist jedoch eine effiziente Struktur, die eine überwiegende Mehrheit der Bots verwendet.
Um mit der Erstellung eines Befehlsparsers zu beginnen, der dieses Format handhabt, fügen Sie der Nachrichtenverarbeitungsfunktion folgende Codezeilen hinzu:
Sie fügen die erste Codezeile hinzu, um den Wert "!"
der Konstanten prefix zuzuweisen, die Sie als Präfix des Bots nutzen werden.
Die zweite Codezeile, die Sie hinzufügen, prüft, ob der Inhalt der Nachricht, die der Bot verarbeitet, mit dem von Ihnen festgelegten Präfix beginnt; wenn nicht, wird die Weiterverarbeitung der Nachricht gestoppt.
Jetzt müssen Sie den Rest der Nachricht in einen Befehlsnamen und jegliche Argumente konvertieren, die in der Nachricht vorhanden sind.
Fügen Sie die folgenden hervorgehobenen Zeilen hinzu:
Sie verwenden hier die erste Zeile, um das Präfix aus dem Nachrichteninhalt zu entfernen und das Ergebnis der Konstanten commandBody zuzuweisen.
Dies ist notwendig, da Sie das Präfix nicht in den analysierten Befehlsnamen einfügen möchten.
Die zweite Zeile nimmt die Nachricht mit dem entfernten Präfix und wendet die split-Methode darauf an, wobei ein Leerzeichen als Trennzeichen dient.
Dadurch wird eine Aufspaltung in eine Gruppe von untergeordneten Zeichenfolgen vorgenommen, wobei bei jedem Leerzeichen eine Trennung vorgenommen wird.
So entsteht ein Array, das den Befehlsnamen und dann (wenn in der Nachricht enthalten) Argumente beinhaltet.
Sie weisen dieses Array der Konstanten args zu.
Die dritte Zeile entfernt das erste Element aus dem Array args (was der bereitgestellte Befehlsname sein wird), konvertiert es in Kleinbuchstaben und weist es dann der Konstanten command zu.
Dadurch können Sie den Befehlsnamen isolieren und nur Argumente im Array belassen.
Außerdem verwenden Sie die Methode toLowerCase, da bei Befehlen in Discord-Bots typischerweise nicht zwischen Groß- / Kleinschreibung unterschieden wird.
Sie haben die Erstellung eines Befehlsparsers, die Implementierung eines erforderlichen Präfix und das Erhalten des Befehlsnamens und der Argumente von Nachrichten abgeschlossen.
Sie werden nun den Code für die spezifischen Befehle implementieren und erstellen.
Fügen Sie folgenden Code hinzu, um den ping-Befehl zu implementieren:
Diese if-Anweisung prüft, ob der analysierte Befehlsname (der der Konstanten command zugewiesen ist) mit "ping" übereinstimmt.
Wenn ja, heißt das, dass der Benutzer den Befehl "ping" verwenden möchte.
Sie werden den Code für den spezifischen Befehl im if-Anweisungsblock verschachteln.
Sie werden dieses Muster für andere Befehle, die Sie implementieren möchten, wiederholen.
Jetzt können Sie den Code für den Befehl "ping" implementieren:
Dadurch wird berechnet, wie lang die Verarbeitung der Nachricht und das "ping" des Bots benötigt haben.
Die zweite Zeile reagiert auf den Befehl des Benutzers mit der reply-Methode in der Konstanten message.
Die reply-Methode pingt (wodurch der Benutzer benachrichtigt und die Nachricht für den angegebenen Benutzer hervorgehoben wird) den Benutzer an, der den Befehl aufgerufen hat, gefolgt von dem Inhalt, der als erstes Argument der Methode angegeben wurde.
Sie stellen ein template literal bereit, das eine Nachricht und den errechneten Ping als Antwort enthält, die die reply-Methode verwenden wird.
Damit ist die Implementierung des Befehls "ping" abgeschlossen.
Führen Sie Ihren Bot mit dem folgenden Befehl aus (im selben Ordner wie index.js):
Sie können nun den Befehl "!
ping "in jedem Kanal nutzen, den der Bot anzeigen und in dem der Bot Nachrichten senden kann; dabei kommt es zu einer Antwort.
Abbildung von Bot, der in Discord auf "!
ping "mit" @ T0M, Pong!
This message had a latency of 1128ms. "antwortet.
Sie haben nun erfolgreich einen Bot erstellt, der Benutzerbefehle handhaben kann, und Ihren ersten Befehl implementiert.
Im nächsten Schritt werden Sie Ihren Bot weiterentwickeln, indem Sie einen sum-Befehl implementieren.
Schritt 4 - Implementieren des sum-Befehls
Jetzt werden Sie Ihr Programm durch Implementieren des "!
sum "-Befehls erweitern.
Der Befehl nimmt eine beliebige Anzahl von Argumenten an und fügt sie zusammen, bevor die Summe aller Argumente an den Benutzer zurückgegegen wird.
Wenn Ihr Discord-Bot noch ausgeführt wird, können Sie den Prozess mit Strg + C anhalten.
Öffnen Sie erneut Ihre index.js-Datei:
Um mit der Implementierung des "!
sum "-Befehls zu beginnen, werden Sie einen else-if-Block verwenden.
Nach der Prüfung des ping-Befehlsnamens wird geprüft, ob der Befehlsname gleich "sum" ist.
Wir verwenden einen else-if-Block, da nur ein Befehl auf einmal verarbeitet wird; wenn das Programm dem Befehlsnamen "ping" entspricht, muss also nicht auf den Befehl "sum" geprüft werden.
Fügen Sie in Ihrer Datei die folgenden hervorgehobenen Zeilen hinzu:
Sie können mit der Implementierung des Codes für den "sum" -Befehl beginnen.
Der Code für den Befehl "sum" wird in den gerade erstellten else-if-Block eingebunden.
Fügen Sie nun folgenden Code hinzu:
Sie verwenden die map-Methode in der Argumentenliste, um eine neue Liste zu erstellen, indem Sie die Funktion parseFloat auf jedes Element im Array args anwenden.
Dadurch ensteht ein neues Array (das der Konstanten numArgs zugewiesen ist), in dem alle Elemente Zahlen anstelle von Zeichenfolgen sind.
Das bedeutet, dass Sie durch Addieren die Summe der Zahlen ermitteln können.
Die zweite Zeile wendet die reduce-Methode auf die Konstante numArgs an; so ist eine Funktion verfügbar, die die Summe aller Elemente in der Liste errechnet.
Sie weisen die Summe aller Elemente in numArgs der Konstanten sum zu.
Dann wenden Sie die reply-Methode auf das Nachrichtenobjekt an, um auf den Befehl des Benutzers mit einem template literal zu antworten, das die Summe aller Argumente enthält, die der Benutzer an den Bot sendet.
Damit ist die Implementierung des Befehls "sum" abgeschlossen.
Führen Sie Ihren Bot nun mit dem folgenden Befehl aus (im selben Ordner wie index.js):
Sie können den Befehl "!
sum "jetzt in jedem Kanal verwenden, den der Bot anzeigen und in dem er Nachrichten senden kann.
Abbildung von Bot, der mit "Die Summe aller von Ihnen angegebenen Argumente ist 6!"
auf "!
sum 1 2 3 "und dann mit" Die Summe aller von Ihnen angegebenen Argumente ist 13! "auf"!
sum 1.5 1.5 10 "antwortet
Im Folgenden finden Sie eine fertige Version des index.js-Bot-Skripts:
In diesem Schritt haben Sie Ihren Discord-Bot durch Implementieren des sum-Befehls weiterentwickelt.
Sie haben einen Discord-Bot implementiert, der verschiedene Benutzerbefehle und Befehlsargumente handhaben kann.
Wenn Sie Ihren Bot erweitern möchten, können Sie ggf. weitere Befehle implementieren oder weitere Bestandteile der Discord-API zur Erstellung eines leistungsfähigen Discord-Bots testen. Konsultieren Sie die Discord.js-Dokumentation oder Discord-API-Dokumentation, um mehr über die Discord-API zu erfahren.
Bei der Erstellung von Discord-Bots müssen Sie stets die allgemeinen Geschäftsbedingungen der Discord-API im Auge behalten; darin wird umrissen, wie Entwickler die Discord-API verwenden müssen.
Außerdem können Sie diesen Satz an Leitfäden lesen, um einen Discord-Bot optimal zu implementieren und Tipps zur Gestaltung von Discord-Bots zu erhalten.
Wenn Sie mehr über Node.js erfahren möchten, lesen Sie unsere Serie zum Codieren in Node.js.
Installieren eines ERPNext-Stacks unter Ubuntu 18.04
6035
In diesem Tutorial installieren und konfigurieren Sie einen ERPNext-Stack auf einem Server, auf dem Ubuntu 18.04 ausgeführt wird.
Ein Ubuntu 18.04-Server mit mindestens 4 GB RAM und einem Nicht-root-Benutzer mit sudo-Berechtigungen.
Sie können Ihren Server und Benutzer einrichten, indem Sie unserem Leitfaden zur Ersteinrichtung des Servers unter Ubuntu 18.04 folgen.
22 / tcp für SSH (wenn Sie OpenSSH nicht bereits aktiviert haben)
8000 / tcp für Entwicklungstests, bevor Sie Ihre Site bereitstellen
Nach Aktivierung der Firewall prüfen Sie den Status Ihrer offenen Ports:
Weitere Informationen zum Firewall-Setup finden Sie in unserem Leitfaden Einrichten einer Firewall mit UFW unter Ubuntu 18.04.
Das Dienstprogramm localectl wird von Ubuntu 18.04 und anderen Linux-Distributionen verwendet, um systemweite Einstellungen für das Gebietsschema und die Tastaturbelegung zu steuern und zu ändern, bevor der Benutzer sich anmeldet. Das ist genau das, was ERPNext 12 benötigt.
Geben Sie Ihrem Server einige Minuten zum Neustart und stellen Sie dann erneut eine SSH-Verbindung her.
Schritt 3 & mdash; Installieren von MariaDB 10.04
ERPNext 12 erfordert MariaDB 10.2 oder höher; die im offiziellen Repository von Ubuntu 18.04 enthaltene Version lautet jedoch 10.1, was bedeutet, dass Sie eine höhere Version installieren müssen.
Für die Zwecke dieses Leitfadens verwenden Sie die neueste stabile Version von MariaDB, zum Zeitpunkt der Verfassung dieses Textes Version 10.4.
Um MariaDB 10.4 unter Ubuntu 18.04 zu installieren, müssen Sie den entsprechenden Signaturschlüssel und das Repository hinzufügen.
Diese Informationen finden Sie im Repository-Assistenten der MariaDB Foundation.
Besuchen Sie diese URL in Ihrem Webbrowser.
Klicken Sie nun unter 1. Choose a Distro (1. Distro wählen) auf Ubuntu.
Eine zweite Spalte mit dem Titel 2. Choose a Release (2. Release wählen) wird angezeigt.
Klicken Sie unter diesem Titel auf 18.04 LTS "bionic".
Dann wird eine dritte Spalte mit dem Titel 3. Choose a Version (3. Version wählen) angezeigt.
Klicken Sie darunter auf 10.4 stable.
Eine dritte Spalte mit dem Titel 4. Choose a Mirror (4. Spiegelung wählen) wird angezeigt.
Wählen Sie eine auf Ihrem Standort basierende Spiegelung aus; dann wird MariaDB die entsprechenden Befehle für Ihre benutzerdefinierte Installation eingeben.
Repository-Assistent von MariaDB
Führen Sie die drei eingegebenen Befehle aus, wodurch das MariaDB-Repository und der Schlüssel ordnungsgemäß hinzugefügt werden.
Ihre eigenen Befehle werden etwa wie folgt aussehen:
Sobald Sie das Repository hinzugefügt haben, installieren Sie MariaDB:
ERPNext 12 ist eine Python-Anwendung und benötigt daher die Bibliothek python3-mysqldb für das Datenbankmanagement.
Bezüglich libmysqlclient-dev, mariadb-client und libmariadbclient18: Über diese Pakete können Benutzer mit dem MariaDB-Dienst kommunizieren. ntpdate und libdate-manip-perl werden von ERPNext zur Synchronisierung der Serverzeit verwendet.
Fügen Sie dem MariaDB-Server als Nächstes eine grundlegende Sicherheitsschicht hinzu, indem Sie das Skript mysql _ secure _ installation ausführen:
Als Nächstes müssen Sie entscheiden, ob Sie Unix-Authentifizierung verwenden möchten oder nicht.
Antworten Sie mit Y (J), um diese Authentifizierungsmethode zu akzeptieren.
Antworten Sie mit N, wenn Sie dazu aufgefordert werden, dasroot-Passwort für MariaDB zu ändern. Eine Verwendung des Standardpassworts zusammen mit Unix-Authentifizierung ist das empfohlene Verfahren für Ubuntu-basierte Systeme, da das root-Konto eng mit automatisierten Systemwartungsaufgaben verbunden ist.
Um diese Einschränkung zu umgehen und MariaDB von einem Nicht-root-Benutzer verwalten zu lassen, müssen Sie nun manuell eine Datenbank erstellen, die nach diesem Benutzer benannt ist.
In diesem Tutorial wird < ^ > sammy < ^ > verwendet, Sie können jedoch einen Namen Ihrer Wahl nutzen:
Verwenden Sie nun nano oder Ihren bevorzugten Texteditor, um eine MariaDB-Konfigurationsdatei namens settings.cnf zu erstellen:
Fügen Sie nun die Konfigurationsvorlage von ERPNext hinzu:
Erstellen Sie als Nächstes eine weitere Datei namens erpnext.cnf:
Die erste Datei / etc / mysql / conf.d / settings.cnf ergänzt und überschreibt auch einige Werte, die in der Standardkonfiguration von MariaDB unter / etc / mysql / my.cnf enthalten sind.
Beachten Sie, dass diese Vorlage zwar ein guter Ausgangspunkt ist, Sie die Leistung von MariaDB jedoch noch weiter verbessern können, indem Sie die Parameter an Ihre Bedürfnisse anpassen.
Die zweite Datei / etc / mysql / mariadb.conf.d / erpnext.cnf überschreibt ebenfalls einige Werte, indem bestimmte Informationen zu Ihrer Datenbankverbindung hinzugefügt werden.
Denken Sie daran, < ^ > sammy < ^ > und < ^ > mariadb _ password < ^ > durch Ihre eigenen Anmeldedaten zu ersetzen:
Detaillierte Informationen zur Postfix-Konfiguration finden Sie in unserem Leitfaden zum Installieren und Konfigurieren von Postfix unter Ubuntu 18.04.
Aktualisieren Sie nun pip3 und installieren Sie dann die neuesten Versionen von drei zusätzlichen Python-Modulen, die ERPNext benötigt:
Zum Zeitpunkt der Verfassung dieses Dokuments verwendet das offizielle ERPNext easy _ install-Skript Node 8. Aus Sicherheitsgründen ist es jedoch ratsam, eine neuere Version zu installieren, da Node 8 2020 sein Lebensende (End of Life, EOL) erreicht hat und somit keine Sicherheitspatches mehr erhalten wird.
Für die Zwecke dieses Leitfadens wird Node.js-Version 12 LTS zusammen mit den entsprechenden Paketmanagern npm und yarn installiert.
Sobald Sie zufrieden sind, können Sie das Skript ausführen:
Installieren Sie als Nächstes yarn global mit dem enthaltenen npm-Paket:
Laden Sie die entsprechende wkhtmltopdf-Version und das Paket für Ubuntu 18.04 von der Projektseite herunter:
Installieren Sie zunächst Redis aus dem offiziellen Ubuntu 18.04-Repository:
Bisher haben Sie alle wichtigen Komponenten installiert, die ERPNext 12 benötigt, darunter folgende Komponenten:
Laden Sie nun mit der bench-CLI ERPNext 12 aus dem Repository herunter:
& lt; ^ & gt; erpnext _ admin _ password & lt; ^ & gt; ist das gewünschte Passwort für den ERPNext-Benutzer Administrator.
Zwar ist die ERPNext 12-Anwendung bereit, doch ist das System insgesamt noch nicht ganz fertig für die Produktion.
Nginx dient hauptsächlich als Webproxy, der den gesamten Datenverkehr von Port 8000 an Port 80 (HTTP) oder Port 443 (HTTPS) weiterleitet.
Supervisor: Dieser Dienst sorgt dafür, dass die wichtigsten Prozesse von ERPNext kontinuierlich ausgeführt und bei Bedarf neu gestartet werden.
Diese Standardkonfigurationen reichen für dieses Tutorial aus; Sie sollten diese Dateien jedoch erkunden und an Ihre eigenen Anforderungen anpassen.
Überprüfen Sie zunächst mit dem folgenden Befehl systemctl, ob die wichtigsten Produktionsdienste ausgeführt werden, und leiten Sie dann an grep weiter:
Öffnen Sie Ihren bevorzugten Browser und navigieren Sie zu der Domäne, wo Sie Ihre ERPNext 12-Anwendung hosten.
Verwenden Sie Administrator als Benutzernamen und das zuvor für das Passwort erstellte < ^ > erpnext _ admin _ password < ^ >.
Nachdem Sie die Regionsinformationen festgelegt haben, können Sie den ersten ERPNext-Benutzer erstellen.
In diesem Fall können Sie Leistungsoptimierung bei ERPNext lesen; hier erhalten Sie Informationen über bewährte Praktiken und die Behebung von Leistungsproblemen.
Installieren von TensorFlow unter Ubuntu 20.04
6034
TensorFlow ist eine Open-Source-Softwarebibliothek für maschinelles Lernen, die dem Trainieren neuronaler Netze dient.
Jeder Knoten im Graph (ausgedrückt in Form von stateful dataflow graphs) stellt die Operationen dar, die von neuronalen Netzen in multidimensionalen Arrays ausgeführt werden.
Diese multidimensionalen Arrays werden im Allgemeinen als "Tensoren" bezeichnet (daher der Name TensorFlow).
In diesem Tutorial installieren Sie TensorFlow mit virtualenv in einer virtuellen Python-Umgebung.
Dieser Ansatz sorgt für eine Isolierung der TensorFlow-Installation und schnelle Inbetriebnahme.
Nachdem Sie die Installation abgeschlossen haben, werden Sie sie durch Importieren von Tensorflow validieren, um sicherzustellen, dass keine Fehler vorliegen.
Einen Ubuntu 20.04-Server mit mindestens 4 GB RAM, der anhand des Leitfadens zur Ersteinrichtung des Servers für Ubuntu 20.04 eingerichtet wurde, einschließlich eines non-root user mit sudo-Berechtigungen und einer Firewall.
Installiertes Python 3.8 oder höher und virtualenv.
Folgen Sie Installieren von Python 3 unter Ubuntu 20.04, um Python und virtualenv zu konfigurieren.
Schritt 1 - Einrichten einer Programmierumgebung
In diesem Schritt richten wir eine virtuelle Umgebung ein, um TensorFlow darin zu installieren, ohne unsere anderen Programmierprojekte zu beeinträchtigen.
Wenn Sie bereits über eine saubere Programmierumgebung verfügen, können Sie diesen Schritt überspringen.
Erstellen Sie zunächst ein Projektverzeichnis.
Wir werden es für Demonstrationszwecke tf-demo nennen; Sie können jedoch einen anderen Verzeichnisnamen wählen, der sinnvoll für Sie ist:
Navigieren Sie zu Ihrem neu erstellten Verzeichnis namens tf-demo:
Erstellen Sie dann beispielsweise eine neue virtuelle Umgebung namens tensorflow-dev.
Führen Sie den folgenden Befehl aus, um Ihre Umgebung zu erstellen:
Dadurch wird ein neues Verzeichnis namens tensorflow-dev erstellt, das alle von Ihnen installierten Pakete enthalten wird, während diese Umgebung aktiviert wird.
Dazu gehören auch pip und eine eigenständige Version von Python.
Aktivieren Sie nun Ihre virtuelle Umgebung:
Nach der Aktivierung wird Ihre Terminal-Eingabeaufforderung widerspiegeln, dass Sie sich in der virtuellen Umgebung befinden:
Nun können Sie TensorFlow in Ihrer virtuellen Umgebung installieren.
Schritt 2 - Installieren von TensorFlow
Bei der Installation von TensorFlow wollen wir sicherstellen, dass wir die neueste in PyPi verfügbare Version installieren.
Daher verwenden wir die folgende Befehlssyntax mit pip:
Sobald Sie die Eingabetaste drücken, wird TensorFlow installiert; Sie sollten eine Ausgabe erhalten, die meldet, dass die Installation zusammen mit allen abhängigen Paketen erfolgreich war.
< $> note Anmerkung: Sie können Ihre virtuelle Umgebung jederzeit deaktivieren, indem Sie folgenden Befehl ausführen:
Um die Umgebung später zu reaktivieren, navigieren Sie zu Ihrem Projektverzeichnis und führen Sie source < ^ > tensorflow-dev < ^ > / bin / activate aus.
Nachdem Sie TensorFlow installiert haben, vergewissern wir uns nun, dass die TensorFlow-Installation funktioniert.
Schritt 3 - Validieren der Installation
Um die Installation von TensorFlow zu validieren, stellen wir sicher, dass wir das TensorFlow-Paket importieren können.
In Ihrem Terminal wird die folgende Eingabeaufforderung angezeigt:
Dies ist die Eingabeaufforderung des Python-Interpreters; sie gibt an, dass Sie mit der Eingabe von Python-Befehlen beginnen können.
Geben Sie zunächst diese Zeile ein, um das TensorFlow-Paket zu importieren und als lokale Variable tf verfügbar zu machen.
Drücken Sie nach Eingabe der Codezeile die Eingabetaste:
Wenn Sie keine Fehler erhalten, haben Sie TensorFlow erfolgreich installiert.
Wenn Sie einen Fehler erhalten, sollten Sie sicherstellen, dass Ihr Server genug Leistung für die Verwaltung von TensorFlow bietet.
Möglicherweise müssen Sie Ihren Server neu bemessen und dafür sorgen, dass er mindestens 4 GB Arbeitsspeicher hat.
In diesem Tutorial haben Sie TensorFlow in einer virtuellen Python-Umgebung installiert und durch Importieren des TensorFlow-Pakets überprüft, ob TensorFlow funktioniert.
Der Leitfaden für Programmierer von TensorFlow dient als nützliche Ressource und Referenz für die TensorFlow-Entwicklung.
Außerdem können Sie sich Kaggle, eine kompetitive Umgebung zur praktischen Anwendung von maschinellen Lernkonzepten, ansehen, in der Sie gegen andere Fans von maschinellem Lernen, Datenwissenschaft und Statistik antreten können.
6031
Python-Threads stellen eine Form von Parallelismus dar, mit der Ihr Programm verschiedene Operationen gleichzeitig ausführen kann.
Parallelismus in Python lässt sich auch durch Verwendung mehrerer Prozesse erzielen; Threads eignen sich jedoch besonders gut für die Beschleunigung von Anwendungen, die hohe I / O-Leistung benötigen.
Beispiel: I / O-gerichtete Operationen umfassen die Erstellung von Webanfragen und das Lesen von Daten aus Dateien.
Im Gegensatz zu I / O-gerichteten Operationen werden CPU-gerichtete Operationen (wie die Ausführung von Berechnungen mit der Python-Standardbibliothek) von Python-Threads nur wenig profitieren.
Python 3 enthält das Dienstprogramm ThreadPoolExecutor zur Ausführung von Code in einem Thread.
In diesem Tutorial werden wir ThreadPoolExecutor verwenden, um zügige Netzwerkanfragen zu erstellen.
Wir werden eine Funktion definieren, die für Aufrufe innerhalb von Threads geeignet ist, ThreadPoolExecutor zur Ausführung dieser Funktion nutzen und Ergebnisse aus diesen Ausführungen verarbeiten.
In diesem Tutorial werden wir Netzwerkanfragen stellen, um die Existenz von Wikipedia-Seiten zu überprüfen.
< $> note Anmerkung: Die Tatsache, dass I / O-gerichtete Operationen mehr von Threads profitieren als I / O-orientierte Operationen, hängt mit einer Eigenart von Python zusammen, die global interpreter lock genannt wird.
Wenn Sie möchten, können Sie in der offiziellen Python-Dokumentation mehr über "global interpreter lock" von Python erfahren.
Für eine optimale Nutzung des Tutorials empfiehlt sich Vertrautheit mit der Programmierung in Python und einer lokalen Python-Programmierumgebung mit installiertem requests-Paket.
Installieren von Python 3 und Einrichten einer lokalen Programmierumgebung unter Ubuntu 18.04
Um das requests-Paket in Ihrer lokalen Python-Programmierumgebung zu installieren, können Sie folgenden Befehl ausführen:
Schritt 1 - Definieren einer Funktion zur Ausführung in Threads
Definieren wir zunächst eine Funktion, die wir mithilfe von Threads ausführen möchten.
Mit nano oder Ihrem bevorzugten Texteditor / Ihrer bevorzugten Entwicklungsumgebung können Sie diese Datei öffnen:
In diesem Tutorial werden wir eine Funktion schreiben, die ermittelt, ob eine Wikipedia-Seite vorhanden ist oder nicht:
Die Funktion get _ wiki _ page _ existence akzeptiert zwei Argumente: eine URL zu einer Wikipedia-Seite (wiki _ page _ url) und eine timeout-Anzahl von Sekunden, während der auf eine Antwort von dieser URL gewartet werden soll.
get _ wiki _ page _ existence nutzt das requests-Paket, um eine Webanfrage an diese URL zu stellen.
Je nach Statuscode der HTTP-Antwort wird eine Zeichenfolge zurückgegeben, die beschreibt, ob die Seite vorhanden ist oder nicht.
Verschiedene Statuscodes stellen verschiedene Ergebnisse einer HTTP-Anfrage dar.
Hier gehen wir davon aus, dass ein 200-Statuscode (" Erfolg ") bedeutet, dass die Wikipedia-Seite existiert, und ein 404-Statuscode (" Nicht gefunden ") bedeutet, dass die Wikipedia-Seite nicht existiert.
Wie im Abschnitt zu den Voraussetzungen beschrieben, benötigen Sie das installierte requests-Paket, um diese Funktion ausführen zu können.
Versuchen wir, die Funktion auszuführen, indem wir die url und den Funktionsaufruf nach der Funktion get _ wiki _ page _ existence hinzufügen:
Nachdem Sie den Code hinzugefügt haben, speichern und schließen Sie die Datei.
Wenn wir diesen Code ausführen:
Erhalten wir eine Ausgabe wie die folgende:
Bei Aufruf der Funktion get _ wiki _ page _ existence mit einer gültigen Wikipedia-Seite wird eine Zeichenfolge zurückgegeben, die bestätigt, dass die Seite tatsächlich existiert.
< $> warning Achtung: Im Allgemeinen ist es nicht sicher, Python-Objekte oder -Status zwischen Threads zu teilen, ohne sorgfältig darauf zu achten, dass keine Parallelitätsfehler auftreten.
Wenn Sie eine Funktion definieren, die in einem Thread ausgeführt werden soll, ist es am besten, eine Funktion festzulegen, die einen einzelnen Auftrag ausführt und den Status nicht an andere Threads weitergibt oder veröffentlicht. get _ wiki _ page _ existence ist ein Beispiel für eine solche Funktion.
Schritt 2 - Verwenden von ThreadPoolExecutor zur Ausführung einer Funktion in Threads
Nachdem wir nun über eine Funktion verfügen, die sich in Threads aufrufen lässt, können wir ThreadPoolExecutor verwenden, um zügig mehrere Aufrufe dieser Funktion auszuführen.
Fügen Sie Ihrem Programm in wiki _ page _ function.py den folgenden hervorgehobenen Code hinzu:
Werfen wir einen Blick auf die Funktionsweise dieses Codes:
concurrent.futures wird importiert, um uns Zugriff auf ThreadPoolExecutor zu gewähren.
Eine with-Anweisung dient der Erstellung eines ThreadPoolExecutor-Instanz-Executors, der Threads unmittelbar nach dem Abschluss bereinigt.
Vier Aufträge werden dem Executor übergeben: einer für jede der URLs in der Liste wiki _ page _ urls.
Jeder Aufruf an submit gibt eine Future-Instanz zurück, die in der futures-Liste gespeichert ist.
Die Funktion as _ completed wartet, bis jeder Future get _ wiki _ page _ existence-Aufruf abgeschlossen ist, damit wir das Ergebnis ausgeben können.
Wenn wir dieses Programm mit dem folgenden Befehl erneut ausführen:
Diese Ausgabe ergibt Sinn: drei der URLs sind gültige Wikipedia-Seiten, eine nicht (this _ page _ does _ not _ exist).
Beachten Sie, dass Ihre Ausgabe eine andere Reihenfolge aufweisen kann als diese Ausgabe.
Die Funktion concurrent.futures.as _ completed in diesem Beispiel gibt Ergebnisse zurück, sobald sie verfügbar sind. Dabei ist es egal, in welcher Reihenfolge die Aufträge übermittelt wurden.
Schritt 3 - Vearbeiten von Ausnahmen bei Funktionsausführungen in Threads
Im vorherigen Schritt hat get _ wiki _ page _ existence bei allen unseren Aufrufen erfolgreich einen Wert zurückgegeben.
In diesem Schritt sehen wir, dass ThreadPoolExecutor auch Ausnahmen auslösen kann, die in Threaded-Funktionsaufrufen generiert werden.
Betrachten wir den folgenden beispielhaften Codeblock:
Dieser Codeblock ist fast identisch mit dem, den wir in Schritt 2 verwendet haben; er weist jedoch zwei wichtige Unterschiede auf:
Wir übergeben nun timeout = 0.00001 an get _ wiki _ page _ existence.
Da das requests-Paket seine Webanfrage an Wikipedia in 0,00001 Sekunden nicht abschließen kann, wird eine ConnectTimeout-Ausnahme ausgelöst.
Wir erfassen ConnectTimeout-Ausnahmen, die durch future.result () ausgelöst werden, und drucken dabei jedes Mal eine Zeichenfolge aus.
Wenn wir das Programm erneut ausführen, sehen wir die folgende Ausgabe:
Sie haben gesehen, dass wenn ein Funktionsaufruf an einen ThreadPoolExecutor eine Ausnahme auslöst, diese Ausnahme normalerweise durch Aufruf von Future.result ausgelöst werden kann.
Ein Aufruf von Future.result bei all Ihren übermittelten Aufrufen stellt sicher, dass Ihr Programm keine Ausnahmen verpasst, die von Ihrer Threaded-Funktion ausgelöst werden.
Schritt 4 - Vergleichen der Ausführungszeit mit und ohne Threads
Überprüfen wir nun, ob die Verwendung von ThreadPoolExecutor Ihr Programm tatsächlich schneller macht.
Lassen Sie uns zunächst die Ausführung von get _ wiki _ page _ existence ohne Threads messen:
Im Codebeispiel rufen wir unsere get _ wiki _ page _ existence-Funktion mit fünfzig verschiedenen URLs von Wikipedia-Seiten hintereinander auf.
Wir verwenden die Funktion time.time (), um die Anzahl der Sekunden auszugeben, die für die Ausführung unseres Programms benötigt wurde.
Wenn wir diesen Code wie zuvor erneut ausführen, erhalten wir eine Ausgabe wie die folgende:
Einträge 2 bis 47 in dieser Ausgabe wurden der Kürze halber ausgelassen.
Die Anzahl der Sekunden, die nach Without threads time (Zeit ohne Threads) ausgegeben wird, wird sich bei Ausführung auf Ihrem Computer unterscheiden. Das ist in Ordnung; Sie erhalten einfach eine Baseline-Zahl, die Sie mit einer Lösung vergleichen können, die ThreadPoolExecutor nutzt.
In diesem Fall waren es ~ 5,803 Sekunden.
Führen wir nun die gleichen fünfzig Wikipedia-URLs über get _ wiki _ page _ existence aus, diesmal jedoch mit ThreadPoolExecutor:
Der Code ist der gleiche Code, den wir in Schritt 2 erstellt haben; diesmal enthält er jedoch zusätzlich einige Druckanweisungen, um die Anzahl der Sekunden anzuzeigen, die zur Ausführung unseres Codes benötigt wurden.
Wenn wir das Programm erneut ausführen, erhalten wir die folgende Ausgabe:
Auch die Anzahl der Sekunden, die nach Threaded time (Zeit mit Threads) ausgegeben wird, wird sich auf Ihrem Computer unterscheiden (ebenso die Reihenfolge Ihrer Ausgabe).
Jetzt können Sie die Ausführungszeit beim Abrufen der fünfzig URLs von Wikipedia-Seiten mit und ohne Threads miteinander vergleichen.
Auf dem in diesem Tutorial verwendeten Rechner dauerte es ohne Threads ~ 5,803 Sekunden; mit Threads waren es ~ 1,220 Sekunden.
Unser Programm lief mit Threads also deutlich schneller.
In diesem Tutorial haben Sie erfahren, wie Sie das Dienstprogramm ThreadPoolExecutor in Python 3 verwenden können, um I / O-gerichteten Code effizient auszuführen.
Sie haben eine Funktion erstellt, die sich für Aufrufe innerhalb von Threads eignet, gelernt, wie man sowohl Ausgaben als auch Ausnahmen von Threaded-Ausführungen dieser Funktion abruft, und den Leistungsschub beobachten können, der durch Verwendung von Threads entsteht.
Nun können Sie mehr über andere Parallelitätsfunktionen des concurrent.futures-Moduls erfahren.
Verstehen relationaler Datenbanken
6075
Datenbankmanagementsysteme (DBMS) sind Computerprogramme, mit denen Benutzer mit einer Datenbank interagieren können.
Ein DBMS ermöglicht es Benutzern, den Zugriff auf eine Datenbank zu steuern, Daten zu schreiben, Abfragen auszuführen und andere Aufgaben im Zusammenhang mit der Datenbankverwaltung durchzuführen.
Um eine dieser Aufgaben auszuführen, muss das DBMS jedoch eine Art zugrunde liegendes Modell haben, das definiert, wie die Daten organisiert sind.
Das relationale Modell ist ein Ansatz zur Organisation von Daten, der in der Datenbanksoftware seit seiner Entwicklung in den späten 60er Jahren breite Verwendung gefunden hat, sodass zum Zeitpunkt der Erstellung dieses Artikels vier der fünf beliebtesten DBMS relational sind.
Dieser konzeptionelle Artikel skiziert die Geschichte des relationalen Modells, wie relationale Datenbanken Daten organisieren und wie sie heute verwendet werden.
Geschichte des relationalen Modells
Datenbanken sind logisch modellierte Cluster von Informationen oder Daten. Jede Sammlung von Daten ist eine Datenbank, unabhängig davon, wie oder wo sie gespeichert ist.
Sogar ein Aktenschrank mit Lohn- und Gehaltsabrechnungsinformationen ist eine Datenbank, ebenso wie ein Stapel von Krankenhauspatientenformularen oder die Sammlung von Kundeninformationen eines Unternehmens, die über mehrere Standorte verteilt sind.
Bevor die Speicherung und Verwaltung von Daten mit Computern gängige Praxis war, waren physische Datenbanken wie diese die einzigen Datenbanken, die Behörden und Unternehmen zur Speicherung von Informationen zur Verfügung standen.
Um die Mitte des 20. Jahrhunderts führten Entwicklungen in der Informatik zu Computern mit mehr Verarbeitungsleistung sowie größerer lokaler und externer Speicherkapazität.
Diese Fortschritte führten dazu, dass Computerwissenschaftler begannen, das Potenzial für die Speicherung und Verwaltung immer größerer Datenmengen zu erkennen.
Es gab jedoch keine Theorien darüber, wie Computer Daten auf sinnvolle, logische Weise organisieren können.
Es ist eine Sache, unsortierte Daten auf einem Computer zu speichern, aber es ist viel komplizierter, Systeme zu entwerfen, die es erlauben, diese Daten auf konsistente, praktische Weise hinzuzufügen, abzurufen, zu sortieren und anderweitig zu verwalten.
Der Bedarf an einem logischen Rahmen zur Speicherung und Organisation von Daten führte zu einer Reihe von Vorschlägen, wie Computer für die Datenverwaltung nutzbar gemacht werden können.
Ein frühes Datenbankmodell war das hierarchische Modell, bei dem die Daten in einer baumartigen Struktur organisiert sind, ähnlich wie bei modernen Dateisystemen.
Das folgende Beispiel zeigt, wie das Layout eines Teils einer hierarchischen Datenbank zur Kategorisierung von Tieren aussehen könnte:
Beispiel einer hierarchischen Datenbank: Kategorisierung von Tieren
Das hierarchische Modell wurde in den frühen Datenbankmanagementsystemen verbreitet eingesetzt, erwies sich aber auch als etwas unflexible.
Obwohl in diesem Modell einzelne Datensätze mehrere "untergeordnete Datensätze" haben können, kann jeder Datensatz nur einen "übergeordneten" Datensatz in der Hierarchie haben.
Aus diesem Grund waren diese früheren hierarchischen Datenbanken darauf beschränkt, nur "Eins-zu-Eins" und "Eins-zu-Viele" -Beziehungen darzustellen.
Dieser Mangel an "Viele-zu-Viele" -Beziehungen könnte zu Problemen führen, wenn Sie mit Datenpunkten arbeiten, die Sie mit mehr als einem übergeordneten Datensatz verknüpfen möchten.
In den späten 1960er Jahren entwickelte Edgar F. Codd, ein Informatiker, der bei IBM arbeitete, das relationale Modell der Datenbankverwaltung.
Das relationale Modell von Codd ermöglichte es, einzelne Datensätze mit mehr als einer Tabelle zu verknüpfen, wodurch zusätzlich zu den "Eins-zu-Viele" -Beziehungen auch "Viele-zu-Viele" -Beziehungen zwischen Datenpunkten möglich wurden.
Dies bot mehr Flexibilität als andere existierende Modelle, wenn es um die Gestaltung von Datenbankstrukturen ging, und bedeutetet, dass relationale Datenbankmanagementsysteme (RDBMS) ein wesentlich breiteres Spektrum von Geschäftsanforderungen erfüllen können.
Codd schlug eine Sprache zur Verwaltung relationaler Daten vor, bekannt als Alpha, die die Entwicklung späterer Datenbanksprachen beeinflusste.
Zwei Kollegen von Codd bei IBM, Donald Chamberlin und Raymond Boyce, schufen eine solche Sprache, die von Alpha inspiriert ist.
Sie nannten ihre Sprache SEQUEL, kurz für Structured English Query Language, aber aufgrund eines bestehenden Warenzeichens kürzten sie den Namen ihrer Sprache auf SQL (formal eher als Structured Query Language bezeichnet).
Aufgrund von Hardware-Beschränkungen waren die frühen relationalen Datenbanken noch ungemein langsam, und es dauerte einige Zeit, bis die Technologie weit verbreitet war.
Aber Mitte der 1980er Jahre war das relationale Modell von Codd bereits in einer Reihe kommerzieller Datenbankmanagementprodukte sowohl von IBM als auch seinen Konkurrenten implementiert worden.
Diese Anbieter folgten ebenfalls dem IBM-Vorbild, indem sie ihre eigenen Dialekte von SQL entwickelten und implementierten.
Bis 1987 hatten sowohl das American Standards Institute als auch die International Organization for Standardization Standards für SQL ratifiziert und veröffentlicht und damit den Status von SQL als akzeptierte Sprache für die Verwaltung von RDBMS gefestigt.
Die weite Verwendung des relationalen Modells in mehreren Branchen führte dazu, dass es als Standardmodell für das Datenmanagement anerkannt wurde.
Selbst mit dem Aufkommen verschiedener NoSQL-Datenbanken in den letzten Jahren bleiben relationale Datenbanken die dominierenden Werkzeuge zur Speicherung und Organisation von Daten.
Wie relationale Datenbanken Daten organisieren
Nachdem Sie nun ein allgemeines Verständnis für die Geschichte des relationalen Modells haben, lassen Sie uns einen genaueren Blick darauf werfen, wie das Modell Daten organisiert.
Die grundlegendsten Elemente des relationalen Modells sind Beziehungen, die von Benutzern und modernen RDBMS als Tabellen erkannt werden.
Eine Beziehung ist ein Satz von Tupeln oder Zeilen in einer Tabelle, wobei jedes Tupel einen Satz von Attributen oder Spalten gemeinsam hat:
Diagrammbeispiel, wie Beziehungen, Tupel und Attribute miteinander verknüpft sind
Eine Spalte ist die kleinste Organisationsstruktur einer relationalen Datenbank und stellt die verschiedenen Facetten dar, die die Datensätze in der Tabelle definieren.
Daher ihr formellerer Name, Attribute.
Sie können sich jedes Tupel als eine einzigartige Instanz jeder Art von Personen, Objekten, Ereignissen oder Assoziationen vorstellen, die die Tabelle enthält.
Diese Instanzen können z. B. Mitarbeiter eines Unternehmens, Verkäufe aus einem Online-Geschäft oder Labor-Testergebnisse sein.
In einer Tabelle, die beispielsweise Mitarbeiterdaten von Lehrern an einer Schule enthält, können die Tupel Attribute wie name, subjects, start _ date usw. haben.
Bei der Erstellung von Spalten geben Sie einen Datentyp an, der festlegt, welche Art von Einträge in dieser Spalte zulässig sind.
RDBMS implementieren oft ihre eigenen eindeutigen Datentypen, die möglicherweise nicht direkt mit ähnlichen Datentypen in anderen Systemen austauschbar sind.
Einige gängige Datentypen umfassen Datumsangaben, Zeichenketten, Ganzzahlen und Boolesche.
Im relationalen Modell enthält jede Tabelle mindestens eine Spalte, die zur eindeutigen Identifizierung jeder Zeile verwendet werden kann, was als Primärschlüssel bezeichnet wird.
Dies ist wichtig, da es bedeutet, dass Benutzer nicht wissen müssen, wo ihre Daten physisch auf einem Computer gespeichert sind; stattdessen können ihre DBMS jeden Datensatz verfolgen und ad hoc zurückgeben.
Dies wiederum bedeutet, dass die Datensätze keine definierte logische Reihenfolge haben und die Benutzer die Möglichkeit haben, ihre Daten in beliebiger Reihenfolge oder durch beliebige Filter zurückgeben.
Wenn Sie zwei Tabellen haben, die Sie miteinander verknüpfen möchten, können Sie dies unter anderem mit einem Fremdschlüssel tun.
Ein Fremdschlüssel ist im Wesentlichen eine Kopie des Primärschlüssels einer Tabelle (der "übergeordneten" Tabelle), der in eine Spalte einer anderen Tabelle (der "untergeordneten" Tabelle) eingefügt wird.
Das folgende Beispiel verdeutlicht die Beziehung zwischen zwei Tabellen, von denen die eine zur Aufzeichnung von Informationen über die Mitarbeiter eines Unternehmens und die andere zur Verfolgung der Verkäufe des Unternehmens verwendet wird.
In diesem Beispiel wird der Primärschlüssel der Tabelle EMPLOYEES als Fremdschlüssel der Tabelle SALES verwendet:
Diagrammbeispiel, wie der Primärschlüssel der Tabelle EMPLOYEES als Fremdschlüssel der Tabelle SALES fungiert
Wenn Sie versuchen, der untergeordneten Tabelle einen Datensatz hinzuzufügen, und der in die Fremdschlüsselspalte eingegebene Wert im Primärschlüssel der übergeordneten Tabelle nicht existiert, ist die Einfügeanweisung ungültig.
Dies hilft, die Integrität der Beziehungsebene aufrechtzuerhalten, da die Zeilen in beiden Tabellen immer korrekt zueinander in Beziehung stehen werden.
Die Strukturelemente des relationalen Modells tragen dazu bei, die Daten auf organisierte Weise zu speichern, aber die Speicherung von Daten ist nur dann sinnvoll, wenn Sie diese auch abrufen können. Um Informationen aus einem RDBMS abzurufen, können Sie eine Abfrage oder eine strukturierte Anfrage nach einem Satz von Informationen stellen.
Wie zuvor erwähnt, verwenden die meisten relationalen Datenbanken SQL zur Verwaltung und Abfrage von Daten. SQL ermöglicht es Ihnen, Abfrageergebnisse mit einer Vielzahl von Klauseln, Prädikaten und Ausdrücken zu filtern und zu manipulieren, wodurch Sie eine genaue Kontrolle darüber erhalten, welche Daten im Ergebnissatz angezeigt werden.
Vorteile und Grenzen relationaler Datenbanken
Lassen Sie uns mit Blick auf die zugrunde liegende Organisationsstruktur relationaler Datenbanken einige ihrer Vor- und Nachteile betrachten.
Heute weichen sowohl SQL als auch die Datenbanken, die es implementieren, in mehrfacher Hinsicht von Codds relationalem Modell ab.
Beispielsweise schreibt das Modell von Codd vor, dass jede Zeile in einer Tabelle eindeutig sein sollte, während die meisten modernen relationalen Datenbanken aus Gründen der Praktikabilität duplizierte Zeilen zulassen.
Es gibt einige, die SQL-Datenbanken nicht als echte relationale Datenbanken betrachten, wenn sich nicht an jede von Codds Spezifikationen für das relationale Modell halten.
In der Praxis wird jedoch jedes DBMS, das SQL verwendet und sich zumindest teilweise an das relationale Modell hält, als relationales Datenbankmanagementsystem bezeichnet.
Obwohl relationale Datenbanken schnell an Popularität gewannen, wurden einige Unzulänglichkeiten des relationalen Modells offensichtlich, als Daten immer wertvoller wurden und Unternehmen begannen, mehr davon zu speichern. Zum einen kann es schwierig sein, eine relationale Datenbank horizontal zu skalieren.
Dies steht oft im Gegensatz zur vertikalen Skalierung, bei der die Hardware eines vorhandenen Servers aufgerüstet wird, in der Regel durch Hinzufügen von mehr RAM oder CPU.
Der Grund dafür, dass die horizontale Skalierung einer relationalen Datenbank schwierig ist, hängt damit zusammen, dass das relationale Modell auf Konsistenz ausgelegt ist, d, h. Clients, die dieselbe Datenbank abfragen, werden immer dieselben Daten abrufen. Wenn Sie eine relationale Datenbank horizontal über mehrere Computer skalieren, wird es schwierig, die Konsistenz zu gewährleisten, da Clients Daten auf einen Knoten Schreiben können, aber nicht auf die anderen.
Es gäbe wahrscheinlich eine Verzögerung zwischen dem anfänglichen Schreiben und dem Zeitpunkt, zu dem die anderen Knoten aktualisiert werden, um die Änderungen widerspiegeln, was zu Inkonsistenzen zwischen ihnen führen würde.
Eine weitere Einschränkung bei RDBMS besteht darin, dass das relationale Modell für die Verwaltung strukturierter Daten oder von Daten konzipiert wurde, die mit einem vordefinierten Datentyp übereinstimmen oder zumindest auf eine vorher festgelegte Weise organisiert sind, sodass sie leicht sortierbar und durchsuchbar sind.
Mit der Verbreitung von Personal Computing und dem Aufkommen des Internets in den frühen 1990er Jahren wurden jedoch unstrukturierte Daten - wie E-Mail-Nachrichten, Fotos, Videos usw. - immer üblicher.
All dies bedeutet nicht, dass relationale Datenbanken nicht nützlich sind.
Ganz im Gegenteil, das relationale Modell ist auch nach über 40 Jahren noch immer der dominierende Rahmen für das Datenmanagement.
Ihre Verbreitung und Langlebigkeit bedeuten, das relationale Datenbanken eine ausgereifte Technologie darstellen, was wiederum einer ihrer wichtigsten Vorteile ist.
Es gibt viele Anwendungen, die für die Arbeit mit dem relationalen Modell konzipiert wurden, sowie viele Datenbankadministratoren, die in ihrer Laufbahn Experten auf dem Gebiet der relationalen Datenbanken sind.
Für diejenigen, die mit relationalen Datenbanken beginnen möchten, gibt es ein breites Angebot an Ressourcen, in gedruckter Form und online.
Ein weiterer Vorteil relationaler Datenbanken besteht darin, dass fast jedes RDBMS Transaktionen unterstützt.
Eine Transaktion besteht aus einer oder mehreren einzelnen SQL-Anweisungen, die nacheinander als eine einzige Arbeitseinheit ausgeführt werden.
Transaktionen stellen einen Alles-oder-Nichts-Ansatz dar, was bedeutet, dass jede SQL-Anweisung in der Transaktion gültig sein muss, da ansonsten die gesamte Transaktion fehlschlägt.
Dies ist sehr hilfreich, um die Datenintegrität zu gewährleisten, wenn Änderungen an mehreren Zeilen oder Tabellen vorgenommen werden.
Und schließlich sind relationale Datenbanken äußerst flexibel.
Sie wurden zum Aufbau einer Vielzahl unterschiedlicher Anwendungen verwendet und arbeiten auch bei sehr großen Datenmengen weiterhin effizient. SQL ist ebenfalls extrem leistungsfähig, sodass Sie im Handumdrehen Daten hinzufügen und ändern sowie die Struktur von Datenbankschemata und Tabellen ändern können, ohne die vorhandenen Daten zu beeinträchtigen.
Dank ihrer Flexibilität und ihres Designs für Datenintegrität sind relationale Datenbanken auch mehr als fünfzig Jahre nach ihrer ersten Konzeption immer noch die wichtigste Art und Weise, wie Daten verwaltet und gespeichert werden.
Selbst mit dem Aufkommen verschiedener NoSQL-Datenbanken in den letzten Jahren sind das Verständnis des relationalen Modells und die Arbeit mit RDBMS der Schlüssel für jeden, der Anwendungen entwickeln möchte, die die Datenleistung nutzen.
Um mehr über einige beliebte Open-Source-RDBMS zu erfahren, empfehlen wir Ihnen, sich unseren Vergleich verschiedener relationaler Open-Source-Datenbanken anzusehen.
Wenn Sie mehr über Datenbanken im Allgemeinen erfahren möchten, empfehlen wir Ihnen, einen Blick in unsere vollständige Bibliothek datenbankbezogener Inhalte zu werfen.
Entwickeln einer Drupal 9-Website auf einem lokalen Rechner mit Docker und DDEV
6214
DDEV ist ein Open-Source-Tool, das Docker verwendet, um für viele verschiedene PHP-Frameworks lokale Entwicklungsumgebungen zu erstellen.
Durch Verwendung der Vorteile von Containerisierung kann DDEV die Arbeit an mehreren Projekten, bei denen verschiedene Tech-Stacks und Cloud-Server zum Einsatz kommen, erheblich vereinfachen.
DDEV enthält Vorlagen für WordPress, Laravel, Magento, TYPO3, Drupal und mehr.
Drupal 9 wurde am 3. Juni 2020 für das Drupal CMS veröffentlicht.
Drupal ist bekannt für seine hohe Benutzerfreundlichkeit und eine enorme Bibliothek an Modulen und Themen. Als PHP-Framework ist es beliebt für die Entwicklung und Pflege verschiedener Websites und Anwendungen aller Größen.
In diesem Tutorial werden Sie mit DDEV auf Ihrem lokalen Rechner eine Drupal 9-Website erstellen.
Damit können Sie zunächst Ihre Website einrichten und Ihr Projekt dann später, wenn Sie bereit sind, auf einem Produktionsserver bereitstellen.
Einen lokalen Rechner mit Linux oder macOS
Für macOS: den Homebrew-Paketmanager, den Sie zur Installation von DDEV verwenden werden.
Um Homebrew auf Ihrem lokalen Rechner zu installieren, folgen Sie Schritt 3 - Installieren und Einrichten von Homebrew in diesem Ruby-Tutorial.
Docker und Docker Compose, auf Ihrem lokalen Rechner installiert.
Für Linux: Sie können Docker und Docker Compose anhand der folgenden Tutorials installieren: Installieren und Verwenden von Docker und Installieren von Docker Compose.
Wählen Sie Ihre Linux-Distribution aus der Liste aus und folgen Sie den enthaltenen Anweisungen.
Für macOS: Docker Compose war früher als Teil von Docker Toolbox verfügbar; Docker Toolbox ist inzwischen jedoch eine Legacy-Lösung.
Heute empfiehlt Docker offiziell die Installation von Docker Desktop, zu dem Docker Compose, Docker Engine und mehr gehören.
Folgen Sie dem offiziellen Leitfaden von Docker, um Docker Desktop unter macOS zu installieren.
Weitere Informationen finden Sie im Leitfaden zu ersten Schritten mit Docker Desktop.
Wenn Sie zuvor Docker Toolbox zum Installieren verschiedener Docker-Tools verwendet haben, können Sie diesen offiziellen Artikel über die Unterschiede zwischen Docker Toolbox und Docker Desktop und über Möglichkeiten zur Koexistenz lesen.
< $> note Anmerkung: Es ist möglich, Drupal 9 mit DDEV auf einem Remoteserver zu entwickeln; Sie benötigen jedoch eine Lösung, um in einem Webbrowser auf localhost zuzugreifen.
Zur persönlichen Verwendung können Sie auch eine GUI auf Ihrem Remoteserver installieren und über einen Webbrowser innerhalb dieser Oberfläche auf Ihre Entwicklungs-Site zugreifen.
Folgen Sie dazu unserem Leitfaden Installieren und Konfigurieren von VNC unter Ubuntu 20.04.
Für eine noch schnellere GUI-Lösung können Sie unserem Leitfaden zum Einrichten eines Remotedesktops mit X2Go unter Ubuntu 20.04 folgen.
Schritt 1 & mdash; Installieren von DDEV
In diesem Schritt installieren Sie DDEV auf Ihrem lokalen Rechner.
Option 1 enthält Anweisungen für macOS, während Option 2 Anweisungen für Linux beinhaltet.
Dieses Tutorial wurde mit DDEV Version 1.15.0 getestet.
Option 1 & mdash; Installieren von DDEV unter macOS
DDEV empfiehlt macOS-Benutzern, das Tool mit dem Homebrew-Paketmanager zu installieren.
Verwenden Sie folgenden brew-Befehl zum Installieren der neuesten stabilen Version:
Wenn Sie die absolut neueste Version bevorzugen, können Sie brew zum Installieren von ddev-edge verwenden:
Wenn Sie bereits eine Version von DDEV installiert haben oder Ihre Version aktualisieren möchten, schließen Sie DDEV und verwenden brew zum Aktualisieren Ihrer Installation:
Sobald Sie DDEV installiert oder aktualisiert haben, führen Sie ddev version aus, um Ihre Software zu überprüfen:
DDEV enthält eine leistungsstarke CLI oder Befehlszeilenschnittstelle.
Führen Sie ddev aus, um sich über einige gängige Befehle zu informieren:
Weitere Informationen zur Verwendung der DDEV-CLI finden Sie in der offiziellen DDEV-Dokumentation.
Nachdem DDEV auf Ihrem lokalen Rechner installiert wurde, sind Sie nun bereit, Drupal 9 zu installieren und mit der Entwicklung einer Website zu beginnen.
Option 2 & mdash; Installieren von DDEV unter Linux
In einem Linux-Betriebssystem können Sie DDEV mit Homebrew für Linux installieren oder das offizielle Installationsskript verwenden.
Beginnen Sie unter Ubuntu mit dem Aktualisieren Ihrer Liste mit Paketen im apt-Paketmanager (Sie können apt in Debian bzw. den äquivalenten Paketmanager, der mit Ihrer Linux-Distribution verknüpft ist, verwenden):
Installieren Sie nun einige Voraussetzungspakete aus dem offiziellen Repository von Ubuntu:
Mit diesen Paketen können Sie das DDEV-Installationsskript aus dem offiziellen GitHub-Repository herunterladen.
Laden Sie das Skript jetzt herunter:
Öffnen Sie es vor dem Ausführen des Skripts in nano oder Ihrem bevorzugten Texteditor und inspizieren Sie den Inhalt:
Sobald Sie die Inhalte des Skripts geprüft haben und damit zufrieden sind, speichern und schließen Sie die Datei.
Jetzt sind Sie bereit, das Installationsskript auszuführen.
Verwenden Sie den Befehl chmod, um das Skript ausführbar zu machen:
Im Installationsprozess werden Sie ggf. dazu aufgefordert, einige Einstellungen zu bestätigen oder Ihr sudo-Passwort einzugeben.
Nach Abschluss der Installation wird DDEV in Ihrem Linux-Betriebssystem verfügbar sein.
Führen Sie ddev version aus, um Ihre Software zu überprüfen:
DDEV ist eine leistungsstarke CLI oder Befehlszeilenschnittstelle.
Führen Sie ddev ohne etwas anderes aus, um sich über einige gängige Befehle zu informieren:
Nachdem DDEV auf Ihrem lokalen Rechner installiert ist, sind Sie nun bereit, Drupal 9 bereitzustellen und mit der Entwicklung einer Website zu beginnen.
Schritt 2 & mdash; Bereitstellen einer neuen Drupal 9-Site mit DDEV
Mit ausgeführtem DDEV werden Sie nun ein Drupal-spezifisches Dateisystem erstellen, Drupal 9 installieren und dann ein standardmäßiges Websiteprojekt initiieren.
Zuerst erstellen Sie ein root-Verzeichnis für das Projekt und öffnen es. Von hier aus führen Sie alle verbleibenden Befehle aus.
In diesem Tutorial wird < ^ > d9test < ^ > verwendet; Sie können Ihr Verzeichnis jedoch auch anders nennen.
Beachten Sie jedoch, dass DDEV nicht gut mit Bindestrichen in Namen umgehen kann.
Es gilt als bewährtes Verfahren, Verzeichnisnamen wie < ^ > my-project < ^ > oder < ^ > drupal-site-1 < ^ > zu vermeiden.
Erstellen Sie das root-Verzeichnis für Ihr Projekt und navigieren Sie dort hin:
DDEV eignet sich hervorragend zum Erstellen von Verzeichnisstrukturen, die mit bestimmten CMS-Plattformen übereinstimmen.
Verwenden Sie den Befehl ddev config zum Einrichten einer Verzeichnisstruktur, die für Drupal 9 spezifisch ist:
Da Sie --project-type = < ^ > drupal9 < ^ > an Ihren Befehl ddev config übergeben haben, hat DDEV mehrere Unterverzeichnisse und Dateien erstellt, die die Standardorganisation für eine Drupal-Website darstellen.
Die Verzeichnisstruktur Ihres Projekts wird nun wie folgt aussehen:
.ddev / wird der Hauptordner für die ddev-Konfiguration sein. web / ist der docroot für Ihr neues Projekt; er enthält mehrere spezifische Dateien mit Einstellungen (settings).
Sie verfügen nun über das Grundgerüst für Ihr neues Drupal-Projekt.
Der nächste Schritt besteht darin, Ihre Plattform zu initialisieren, wodurch die erforderlichen Container und Networking-Konfigurationen erstellt werden.
DDEV bindet sich an Ports 80 und 443. Wenn Sie also einen Webserver wie Apache auf Ihrem Rechner ausführen oder etwas anderes verwenden, das diese Ports nutzt, halten Sie diese Dienste vor dem Fortfahren an.
Verwenden Sie den Befehl ddev start, um Ihre Plattform zu initialisieren:
Dadurch werden alle Docker-basierten Container für Ihr Projekt erstellt, darunter ein Webcontainer, ein Datenbankcontainer und phpmyadmin.
Nach Abschluss der Initialisierung sehen Sie eine Ausgabe wie diese (Ihre Portnummer kann sich davon unterscheiden):
< $> note Anmerkung: Denken Sie daran, dass DDEV hier im Hintergrund Docker-Container startet.
Wenn Sie diese Container anzeigen oder überprüfen möchten, ob sie ausgeführt werden, können Sie den Befehl docker ps verwenden:
Neben anderen Containern, die Sie derzeit ausführen, finden Sie vier neue Container, die jeweils ein anderes Image ausführen: php-myadmin, ddev-webserver, ddev-router und ddev-dbserver-mariadb.
ddev start hat Ihre Container erfolgreich erstellt und eine Ausgabe mit zwei URLs geliefert.
Zwar steht in dieser Ausgabe, dass Ihr Projekt "unter http: / / < ^ > d9test < ^ > .ddev.site und http: / / 127.0.0.1: < ^ > 32773 < ^ > erreichbar ist", doch wenn Sie diese URLs besuchen, wird ein Fehler ausgelöst.
Ab Drupal 8 funktionieren der Drupal Core und die contrib-Module wie Abhängigkeiten.
Deshalb müssen Sie Drupal zunächst mit Composer, dem Paketmanager für PHP-Projekte, installieren, bevor Sie etwas in Ihrem Webbrowser laden können.
Eine der nützlichsten und elegantesten Funktionen von DDEV ist, dass Sie Composer-Befehle über die DDEV-CLI in Ihre containerisierte Umgebung übergeben können.
Das bedeutet, dass Sie die spezifische Konfiguration Ihres Computers von Ihrer Entwicklungsumgebung trennen können.
So müssen Sie die verschiedenen Probleme mit Dateipfaden, Abhängigkeiten und Versionen, die eine lokale PHP-Entwicklung im Allgemeinen begleiten, nicht mehr verwalten.
Außerdem können Sie rasch und mit minimalem Aufwand zwischen verschiedenen Frameworks und Tech-Stacks wechseln.
Verwenden Sie den Befehl ddev composer zum Herunterladen von drupal / recommended-project.
Dadurch wird Drupal core mit seinen Bibliotheken und anderen verwandten Ressourcen heruntergeladen und ein Standardprojekt erstellt:
Laden Sie nun eine abschließende Komponente namens Drush oder Drupal Shell herunter.
In diesem Tutorial werden wir nur einen drush-Befehl nutzen. Das Tutorial bietet eine Alternative, doch ist drush eine leistungsstarke CLI für die Drupal-Entwicklung, die für zusätzliche Effizienz sorgt.
Verwenden Sie ddev composer zum Installieren von drush:
Sie haben nun ein standardmäßiges Drupal 9-Projekt erstellt und drush installiert.
Jetzt werden Sie Ihr Projekt in einem Browser anzeigen und die Einstellungen Ihrer Website konfigurieren.
Schritt 3 & mdash; Konfigurieren des Drupal-9-Projekts
Nachdem Sie Drupal 9 installiert haben, können Sie Ihr neues Projekt nun in Ihrem Browser öffnen.
Dazu können Sie ddev start erneut ausführen und eine der beiden ausgegebenen URLs kopieren. Alternativ können Sie folgenden Befehl verwenden, um Ihre Website in einem neuen Browserfenster automatisch zu starten:
Der standardmäßige Drupal-Installationsassistent wird angezeigt.
Drupal 9-Installationsprogramm über Browser
Hier haben Sie zwei Optionen.
Sie können diese Benutzeroberfläche verwenden und dem Assistenten durch die Installation folgen oder zu Ihrem Terminal zurückkehren und über ddev einen drush-Befehl übergeben.
Die letztere Option automatisiert den Installationsprozess und setzt admin sowohl als Ihren Benutzernamen als auch als Passwort.
Option 1 & mdash; Verwenden des Assistenten
Kehren Sie zum Assistenten in Ihrem Browser zurück.
Wählen Sie unter Sprache auswählen eine Sprache aus dem Drop-down-Menü aus und klicken Sie auf Speichern und fortfahren.
Wählen Sie nun ein Installationsprofil aus.
Sie können zwischen Standard, Minimal und Demo wählen.
Treffen Sie Ihre Wahl und klicken Sie dann auf Speichern und fortfahren.
Drupal überprüft automatisch Ihre Anforderungen, erstellt eine Datenbank und installiert Ihre Site.
Ihr letzter Schritt besteht darin, einige Konfigurationen anzupassen.
Fügen Sie für die Site einen Namen und eine E-Mail-Adresse, die auf Ihre Domäne endet, hinzu.
Wählen Sie dann einen Benutzernamen und ein Passwort.
Wählen Sie ein starkes Passwort aus und bewahren Sie Ihre Anmeldedaten an einem sicheren Ort auf.
Fügen Sie schließlich eine private E-Mail-Adresse hinzu, die Sie regelmäßig überprüfen, füllen Sie die regionalen Einstellungen aus und klicken Sie auf Speichern und fortfahren.
Drupal 9-Willkommensnachricht mit einer Warnung zu Berechtigungen
Ihre neue Site wird mit einer Willkommensnachricht geladen.
Option 2 & mdash; Verwenden der Befehlszeile
Führen Sie im root-Verzeichnis Ihres Projekts den Befehl ddev exec aus, um mit drush eine standardmäßige Drupal-Site zu installieren:
Dadurch wird Ihre Website wie beim Assistenten erstellt, jedoch mit verschiedenen Standardkonfigurationen.
Ihr Benutzername und Passwort werden admin lauten.
Starten Sie nun die Site, um sie in Ihrem Browser anzuzeigen:
Sie sind nun bereit, mit dem Erstellen Ihrer Website zu beginnen. Es gilt jedoch als bewährte Methode zu überprüfen, ob Ihre Berechtigungen für das Verzeichnis / sites / web / default korrekt sind.
Solange Sie lokal arbeiten, ist das kein großes Problem; wenn Sie diese Berechtigungen jedoch an einen Produktionsserver übertragen, stellen sie ein Sicherheitsrisiko dar.
Schritt 4 & mdash; Überprüfen Ihrer Berechtigungen
Während der Installation mit dem Assistenten oder dem ersten Laden Ihrer Willkommensseite sehen Sie ggf. eine Warnung zu den Berechtigungseinstellungen in Ihrem Verzeichnis / sites / web / default und eine Datei in diesem Verzeichnis: settings.php.
Nach Ausführung des Installationsskripts wird Drupal versuchen, die Berechtigungen für das Verzeichnis web / sites / default festzulegen, um für alle Gruppen zu lesen und auszuführen: Dies ist eine 555-Berechtigungseinstellung.
Außerdem wird es versuchen, Berechtigungen für default / settings.php auf schreibgeschützt oder 444 festzulegen. Wenn Sie auf diese Warnung stoßen, führen Sie im root-Verzeichnis Ihres Projekts diese zwei chmod-Befehle aus.
Wenn Sie das nicht tun, besteht ein Sicherheitsrisiko:
Um zu überprüfen, ob Sie die richtigen Berechtigungen haben, führen Sie diesen ls-Befehl mit den Switches a, l, h und d aus:
Überprüfen Sie, ob Ihre Berechtigungen mit der folgenden Ausgabe übereinstimmen:
Sie sind nun bereit, auf Ihrem lokalen Rechner eine Drupal 9-Website zu erstellen.
Schritt 5 & mdash; Erstellen des ersten Beitrags in Drupal
Um einige Funktionen von Drupal zu testen, erstellen Sie nun mit der Web-Benutzeroberfläche einen Beitrag.
Klicken Sie auf der ersten Seite Ihrer Website auf die Schaltfläche Inhalt am linken Rand des oberen Menüs.
Klicken Sie nun auf die Schaltfläche Inhalt hinzufügen.
Eine neue Seite wird angezeigt.
Klicken Sie auf Artikel, und eine weitere Seite wird angezeigt.
Drupal 9-Eingabeaufforderung zum Erstellen eines Artikels
Fügen Sie einen beliebigen Titel und Content hinzu.
Sie können auch ein Bild wie eines der Hintergrundbilder von DigitalOcean hinzufügen.
Klicken Sie dann auf die blaue Schaltfläche Speichern.
Ihr erster Beitrag wird auf der Website angezeigt.
Drupal 9 - Beitrag erstellt
Sie entwickeln nun eine Drupal 9-Website auf Ihrem lokalen Rechner, ohne jemals mit einem Server zu interagieren (dank Docker und DDEV).
Im folgenden Schritt verwalten Sie den DDEV-Container, der Ihren Workflow aufnehmen wird.
Schritt 6 & mdash; Verwalten des DDEV-Containers
Wenn Sie Ihr Projekt abgeschlossen haben oder eine Pause machen möchten, können Sie Ihren DDEV-Container anhalten, ohne sich Gedanken über Datenverluste machen zu müssen.
DDEV kann zwischen vielen Projekten schnelle Kontextwechsel verwalten; dies ist eine der nützlichsten Funktionen.
Ihr Code und Ihre Daten werden stets in Ihrem Projektverzeichnis gespeichert, auch wenn Sie den DDEV-Container angehalten oder gelöscht haben.
Um Ressourcen freizugeben, können Sie DDEV jederzeit anhalten.
Führen Sie im root-Verzeichnis Ihres Projekts folgenden Befehl aus:
DDEV ist global verfügbar, sodass Sie ddev-Befehle von überall her ausführen können, solange Sie das DDEV-Projekt angeben:
Außerdem können Sie mit ddev list alle Ihre Projekte auf einmal anzeigen:
DDEV umfasst viele andere nützliche Befehle.
Sie können DDEV neu starten und jederzeit lokal weiterentwickeln.
In diesem Tutorial haben Sie Docker und die Vorteile von Containerisierung verwendet, um mit Hilfe von DDEV lokal eine Drupal-Site zu erstellen.
DDEV lässt sich in zahlreiche IDEs einbinden und bietet natives PHP-Debugging für Atom, PHPStorm und Visual Studio Code (vscode).
Nun können Sie mehr über das Erstellen von Entwicklungsumgebungen für Drupal mit DDEV oder das Entwickeln anderer PHP-Frameworks wie Wordpress erfahren.
Verwenden eines Unterprozesses zum Ausführen externer Programme in Python 3
6167
Python 3 beinhaltet das subprocess-Modul zum Ausführen externer Programme und Lesen ihrer Ausgaben in Ihrem Python-Code.
Möglicherweise finden Sie subprozess nützlich, wenn Sie in Ihrem Python-Code auf Ihrem Computer ein anderes Programm verwenden möchten.
Beispielsweise möchten Sie vielleicht git aus Ihrem Python-Code aufrufen, um Dateien in Ihrem Projekt abzurufen, die in der git-Versionskontrolle verfolgt werden.
Da sich jedes Programm, das Sie auf Ihrem Computer aufrufen können, über subprocess steuern lässt, gelten die hier angegebenen Beispiele für externe Programme, die Sie ggf. über Ihren Python-Code aufrufen möchten.
subprocess enthält verschiedene Klassen und Funktionen; wir werden in diesem Tutorial jedoch eine der nützlichsten Funktionen von subprocess abdecken: subprocess.run.
Wir werden uns die verschiedenen Einsatzmöglichkeiten und wichtigsten Schlüsselwortargumente ansehen.
Ausführen eines externen Programms
Sie können mit der Funktion subprocess.run ein externes Programm über Ihren Python-Code ausführen.
Zuerst müssen Sie jedoch die Module subprocess und sys in Ihr Programm importieren:
Wenn Sie dies ausführen, erhalten Sie eine Ausgabe wie die folgende:
Sehen wir uns dieses Beispiel an:
sys.executable ist der absolute Pfad zur ausführbaren Python-Datei, mit der Ihr Programm ursprünglich aufgerufen wurde.
Beispielsweise kann sys.executable ein Pfad wie / usr / local / bin / python sein.
An subprocess.run wird eine Liste mit Zeichenfolgen übergeben, die aus den Komponenten des Befehls besteht, den wir ausführen möchten.
Da die erste Zeichenfolge, die wir übergeben, sys.executable ist, weisen wir subprocess.run an, ein neues Python-Programm auszuführen.
Die Komponente -c ist eine python-Befehlszeilenoption, mit der Sie eine Zeichenfolge mit einem gesamten Python-Programm zur Ausführung übergeben können.
In unserem Fall übergeben wir ein Programm, das die Zeichenkette ocean ausgibt.
Sie können sich jeden Eintrag in der Liste, den wir an subprocess.run übergeben, als durch ein Leerzeichen getrennt vorstellen.
Beispielsweise wird [sys.executable, "-c", "print (' ocean ')"] in etwa zu / usr / local / bin / python -c "print (' ocean ')".
Beachten Sie, dass subprocess automatisch die Komponenten des Befehls angibt, bevor versucht wird, sie im zugrunde liegenden Betriebssystem auszuführen. So können Sie beispielsweise einen Dateinamen übergeben, der Leerzeichen enthält.
< $> warning Warnung: Übergeben Sie nie unvertrauenswürdige Eingaben an subprocess.run.
Da subprocess.run die Fähigkeit hat, beliebige Befehle auf Ihrem Computer auszuführen, können bösartige Akteure damit Ihren Computer auf unerwartete Weise manipulieren.
Erfassen von Ausgaben aus einem externen Programm
Nachdem wir mit subprocess.run ein externes Programm aufrufen können, sehen wir uns nun an, wie wir Ausgaben von diesem Programm erfassen können.
Beispielsweise kann dieser Prozess nützlich sein, wenn wir git ls-files verwenden möchten, um alle aktuell in der Versionskontrolle gespeicherten Dateien auszugeben.
< $> note Anmerkung: Die in diesem Abschnitt angegebenen Beispiele benötigen Python 3.7 oder höher.
Insbesondere wurden die Schlüsselwortargumente capture _ output und text in Python 3.7 hinzugefügt, als es im Juni 2018 veröffentlicht wurde.
Ergänzen wir unser vorheriges Beispiel:
Dieses Beispiel ist weitgehend das gleiche wie das im ersten Abschnitt eingeführte: Wir führen noch immer einen Unterprozess zum Ausdrucken von ocean aus.
Wichtig: Wir übergeben jedoch die Schlüsselwortargumente capture _ output = True und text = True an subprocess.run.
subprocess.run gibt ein subprocess.CompletedProcess-Objekt zurück, das an result gebunden ist.
Das Objekt subprocess.CompletedProcess enthält Details zum Exitcode des externen Programms und seiner Ausgabe. capture _ output = True sorgt dafür, dass result.stdout und result.stderr mit der entsprechenden Ausgabe aus dem externen Programm gefüllt werden.
Standardmäßig sind result.stdout und result.stderr als Bytes gebunden; das Schlüsselwortargument text = True weist Python an, die Bytes in Zeichenfolgen zu decodieren.
Im Ausgabebereich lautet stdout ocean (plus der nachfolgenden neuen Zeile, die print implizit hinzufügt); wir verfügen über kein stderr.
Versuchen wir es mit einem Beispiel, das für stderr einen nicht leeren Wert erstellt:
Dieser Code führt einen Python-Unterprozess aus, der sofort einen ValueError auslöst.
Wenn wir das endgültige Ergebnis prüfen, sehen wir in stdout nichts und ein Traceback unseres ValueError in stderr.
Das liegt daran, dass Python das Traceback der nicht behandelten Ausnahme in stderr schreibt.
Auslösen einer Ausnahme bei einem fehlerhaften Exitcode
Manchmal ist es nützlich, eine Ausnahme auszulösen, wenn ein ausgeführtes Programm mit einem fehlerhaften Exitcode beendet wird.
Programme, die mit einem Nullcode beendet werden, werden als erfolgreich betrachtet; Programme, die mit einem Nicht-Nullcode beendet werden, werden hingegen als fehlerhaft betrachtet.
Als Beispiel kann dieses Muster nützlich sein, wenn wir eine Ausnahme auslösen möchten für den Fall, dass wir git ls-files in einem Verzeichnis ausführen, das in Wahrheit kein git-Repository ist.
Wir können das Schlüsselwortargument check = True nutzen, damit für subprocess.run eine Ausnahme ausgelöst wird, wenn das externe Programm einen Nicht-Null-Exitcode zurückgibt:
Diese Ausgabe zeigt, dass wir einen Unterprozess ausgeführt haben, der einen Fehler ausgelöst hat, der in unserem Terminal in stderr ausgegeben wird.
Dann hat subprocess.run in unserem Namen in unserem zentralen Python-Programm ordnungsgemäß einen subprocess.CalledProcessError ausgelöst.
Alternativ enthält das subprocess-Modul auch die Methode subprocess.CompletedProcess.check _ returncode, die wir mit ähnlicher Wirkung aufrufen können:
Wenn wir diesen Code ausführen, erhalten wir:
Da wir check = True nicht an subprocess.run übergeben haben, haben wir eine subprocess.CompletedProcess-Instanz erfolgreich an result gebunden, obwohl unser Programm mit einem Nicht-Nullcode beendet wurde.
Ein Aufruf von result.check _ returncode () löst jedoch einen subprocess.CalledProcessError aus, da erkannt wird, dass der abgeschlossene Prozess mit einem fehlerhaften Code beendet wurde.
Verwenden von Timeouts zum frühzeitigen Beenden von Programmen
subprocess.run enthält das timeout-Argument, sodass Sie ein externes Programm anhalten können, wenn dessen Ausführung zu lange dauert:
Der Unterprozess, den wir ausführen wollten, verwendet die Funktion time.sleep, um für 2 Sekunden zu schlafen.
Wir haben jedoch das Schlüsselwortargument timeout = 1 an subprocess.run übergeben, um bei unserem Unterprozess nach 1 Sekunde für ein Timeout zu sorgen.
Das erklärt, warum unser Aufruf an subprocess.run letztlich eine subprocess.TimeoutExpired-Ausnahme ausgelöst hat.
Beachten Sie, dass das Schlüsselwortargument timeout für subprocess.run ungefähr ist.
Python wird sich bemühen, den Unterprozess nach der timeout-Zahl von Sekunden zu beenden; der Vorgang wird jedoch nicht unbedingt genau sein.
Übergeben von Eingaben an Programme
Manchmal erwarten Programme, dass Eingaben über stdin an sie übergeben werden.
Das Schlüsselwortargument input an subprocess.run ermöglicht Ihnen, Daten an stdin des Unterprozesses zu übergeben.
Nach Ausführung dieses Codes erhalten wir eine Ausgabe wie die folgende:
In diesem Fall haben wir die Bytes underwater an input übergeben.
Unser Zielunterprozess hat sys.stdin verwendet, um das übergebene stdin (underwater) zu lesen und in unserer Ausgabe auszugeben.
Das Schlüsselwortargument input kann nützlich sein, wenn Sie mehrere subprocess.run-Aufrufe verketten möchten, um die Ausgabe eines Programms als Eingabe an ein anderes zu übergeben.
Das subprocess-Modul ist ein leistungsfähiger Bestandeil der Python-Standardbibliothek, mit dem Sie externe Programme ausführen und deren Ausgaben bequem überprüfen können.
In diesem Tutorial haben Sie gelernt, wie Sie subprocess.run verwenden können, um externe Programme zu steuern, Eingaben an sie zu übergeben, ihre Ausgabe zu analysieren und ihre Rückgabecodes zu überprüfen.
Das subprocess-Modul macht zusätzliche Klassen und Dienstprogramme verfügbar, auf die wir in diesem Tutorial nicht eingegangen sind.
Nachdem Sie nun über Grundkenntnisse verfügen, können Sie die Dokumentation des subprocess-Moduls nutzen, um mehr über andere verfügbare Klassen und Dienstprogramme zu erfahren.
Verwenden der Python-Map-Funktion
6188
Wir können die integrierte Python-Funktion map () verwenden, um für jedes Element in einer Iterablen (wie einer Liste oder einem Wörterbuch) eine Funktion anzuwenden und einen neuen Iterator zum Abrufen der Ergebnisse zurückgeben. map () gibt ein map-Objekt (einen Iterator) zurück, das wir in anderen Teilen unseres Programms nutzen können.
Außerdem können wir das map-Objekt an die Funktion list () oder einen anderen Sequenztyp übergeben, um eine Iterable zu erstellen.
Die Syntax für die Funktion map () lautet folgendermaßen:
Anstatt eine for-Schleife zu verwenden, bietet die Funktion map () die Möglichkeit, für jedes Element in einer Iterablen eine Funktion anzugeben.
Daher kann dies oft effektiver sein, da die Funktion nur auf ein Element auf einmal angewendet wird, anstatt Kopien der Elemente in einer anderen Iterablen zu erstellen.
Dies ist besonders nützlich, wenn Sie Programme verwenden, die große Datenmengen verarbeiten. map () kann zudem mehrere Iterablen als Argumente an die Funktion übergeben, indem ein Element aus jeder Iterablen an die Funktion gesendet wird.
In diesem Tutorial werden wir uns drei verschiedene Möglichkeiten der Verwendung von map () ansehen: mit einer lambda-Funktion, mit einer benutzerdefinierten Funktion und schließlich mit einer integrierten Funktion unter Verwendung mehrerer Iterablenargumente.
Verwenden einer Lambda-Funktion
Das erste Argument für map () ist eine Funktion, die wir auf die einzelnen Elemente anwenden.
Python ruft die Funktion für jedes Element in der Iterablen, das wir an map () übergeben, einmal auf und gibt das bearbeitete Element innerhalb eines map-Objekts zurück.
Für das erste Funktionsargument können wir entweder eine benutzerdefinierte Funktion übergeben oder lambda-Funktionen nutzen (insbesondere wenn der Ausdruck nicht sehr komplex ist).
Die Syntax von map () mit einer lambda-Funktion lautet wie folgt:
Mit einer Liste wie der folgenden können wir eine lambda-Funktion mit einem Ausdruck implementieren, den wir auf jedes Element in unserer Liste anwenden möchten:
Um einen Ausdruck auf jede unserer Zahlen anzuwenden, können wir map () und lambda nutzen:
Hier deklarieren wir in unserer Liste ein Element als x. Dann fügen wir unseren Ausdruck hinzu.
Wir übergeben unsere Liste mit Zahlen als Iterable für map ().
Um die Ergebnisse davon sofort zu erhalten, drucken wir eine Liste des map-Objekts aus:
Wir haben list () verwendet, damit das map-Objekt als Liste zurückgegeben wird und nicht als schlechter menschenlesbares Objekt wie: < map object at 0x7fc250003a58 >.
Das map-Objekt ist ein Iterator über unsere Ergebnisse; daher können wir mit for eine Schleife darüber legen oder list () verwenden, um es in eine Liste umzuwandeln.
Wir tun dies hier, da es eine gute Möglichkeit zur Prüfung der Ergebnisse darstellt.
Letztlich ist map () am nützlichsten bei der Arbeit mit großen Datensätzen; daher würden wir mit dem map-Objekt wahrscheinlich weiter arbeiten und in der Regel keinen Konstruktor wie list () anwenden.
Für kleinere Datensätze eignet sich die Erfassung in Listen ggf. besser; für die Zwecke dieses Tutorials verwenden wir jedoch einen kleinen Datensatz zum Demonstrieren von map ().
Implementieren einer benutzerdefinierten Funktion
Ähnlich wie bei lambda können wir eine Funktion verwenden, die wir zur Anwendung auf eine Iterable definiert haben.
Zwar sind lambda-Funktionen nützlicher für die Implementierung, wenn Sie mit einem einzeiligen Ausdruck arbeiten, doch sind benutzerdefinierte Funktionen besser geeignet, wenn der Ausdruck komplexer wird.
Wenn wir weitere Daten an die Funktion übergeben müssen, die Sie auf Ihre Iterable anwenden möchten, können benutzerdefinierte Funktionen außerdem besser lesbar sein.
Beispielsweise ist jedes Element in der folgenden Iterable ein Wörterbuch, das verschiedene Details zu den einzelnen Aquariumgeschöpfen enthält:
Wir haben beschlossen, dass alle Aquariumgeschöpfe tatsächlich in den gleichen Tank kommen sollen.
Wir müssen unsere Datensätze aktualisieren, um widerzuspiegeln, dass alle unsere Geschöpfe in Tank 42 kommen werden. Um map () -Zugriff auf alle Wörterbücher und Schlüsselwertpaare in den Wörterbüchern zu erhalten, erstellen wir eine geschachtelte Funktion:
Wir definieren eine assign _ to _ tank () -Funktion, die aquarium _ creatures und new _ tank _ number als Parameter verwendet.
In assign _ to _ tank () übergeben wir in der letzten Zeile apply () als Funktion an map ().
Die Funktion assign _ to _ tank gibt den Iterator zurück, der sich aus map () ergibt.
apply () nimmt x als Argument, das einen Eintrag in unserer Liste darstellt (ein einzelnes Wörterbuch).
Als Nächstes definieren wir, dass x der Schlüssel "tank number" von aquarium _ creatures sein und die übergebene new _ tank _ number speichern soll.
Wir geben jedes Element nach Anwenden der neuen Tanknummer zurück.
Wir rufen assign _ to _ tank () mit unserer Liste von Wörterbüchern und der neuen Tanknummer auf, die wir für die einzelnen Geschöpfe ersetzen möchten:
Nach Abschluss der Funktion ist unser map-Objekt in der Variable assigned _ tanks gespeichert, die wir in eine Liste verwandeln und drucken:
Wir erhalten die folgende Ausgabe aus diesem Programm:
Wir haben die neue Tanknummer unserer Liste von Wörterbüchern zugeordnet.
Mithilfe einer von uns definierten Funktion können wir map () integrieren, um die Funktion effizient auf jedes Element der Liste anzuwenden.
Verwenden einer integrierten Funktion mit mehreren Iterablen
Genauso wie bei lambda-Funktionen oder unseren selbst definierten Funktionen können wir integrierte Python-Funktionen mit map () verwenden.
Um eine Funktion mit mehreren Iterablen anzuwenden, übergeben wir nach dem ersten einen weiteren Iterablennamen.
Hier sehen wir unsere Listen mit Integerzahlen, die wir mit pow () verwenden möchten:
Als Nächstes übergeben wir pow () als unsere Funktion an map () und stellen die beiden Listen als unsere Iterablen bereit:
map () wendet die Funktion pow () in jeder Liste auf das gleiche Element an, um die Potenz anzugeben.
Daher werden unsere Ergebnisse 2 * * 1, 4 * * 2, 6 * * 3 usw. anzeigen:
Wenn wir map () eine Iterable zur Verfügung stellen würden, die länger als die andere ist, würde map () die Berechnung stoppen, sobald das Ende der kürzesten Iterable erreicht ist.
Im folgenden Programm ergänzen wir base _ numbers um drei zusätzliche Zahlen:
Als Folge ändert sich innerhalb der Berechnung dieses Programms nichts und bleibt das Ergebnis somit gleich:
Wir haben die Funktion map () mit einer integrierten Python-Funktion verwendet und gesehen, dass sie mehrere Iterablen handhaben kann.
Außerdem haben wir überprüft, ob map () weitere Iterablen verarbeitet, bis das Ende der Iterablen mit den wenigsten Elementen erreicht wurde.
In diesem Tutorial haben wir die verschiedenen Möglichkeiten zur Verwendung der Funktion map () in Python kennengelernt.
Jetzt können Sie map () mit Ihrer eigenen Funktion, einer lambda-Funktion und anderen integrierten Funktionen verwenden.
Zudem können Sie map () mit Funktionen implementieren, die mehrere Iterablen erfordern.
In diesem Tutorial haben wir die Ergebnisse von map () für Demonstrationszwecke sofort in einem Listenformat ausgegeben.
In unseren Programmen würden wir normalerweise das zurückgegebene map-Objekt verwenden, um die Daten weiter zu bearbeiten.
Erstellen einer REST-API mit Prisma und PostgreSQL
6223
Prisma ist ein Open-Source-basiertes Datenbank-Toolkit.
Es besteht aus drei Haupttools:
Prisma Client: Ein automatisch generierter und typensicherer Query Builder für Node.js und TypeScript.
Prisma Migrate: Ein deklaratives Modellierungs- und Migrationssystem für Daten.
Prisma Studio: Eine GUI zum Anzeigen und Bearbeiten von Daten in Ihrer Datenbank.
Diese Tools dienen dazu, die Produktivität von Anwendungsentwicklern in ihren Datenbank-Workflows zu steigern.
Einer der größten Vorteile von Prisma ist die Ebene der Abstraktion, die möglich ist: Anstatt komplexe SQL-Abfragen oder Schemamigrationen zu erstellen, können Anwendungsentwickler bei der Arbeit mit ihrer Datenbank unter Verwendung von Prisma Daten intuitiver verwalten.
In diesem Tutorial erstellen Sie eine REST-API für eine kleine Blogging-Anwendung in TypeScript mithilfe von Prisma und eine PostgreSQL-Datenbank.
Sie werden Ihre PostgreSQL-Datenbank mit Docker lokal einrichten und die REST-API mit Express implementieren.
Am Ende des Tutorials verfügen Sie über einen Webserver, der auf Ihrem Rechner lokal ausgeführt wird und auf verschiedene HTTP-Anfragen reagieren sowie Daten in der Datenbank lesen und schreiben kann.
Dieses Tutorial setzt Folgendes voraus:
Node.js v10 oder höher, auf Ihrem Rechner installiert.
Sie können zur Einrichtung einen der Leitfäden zum Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung verwenden, die für Ihr Betriebssystem gelten.
Docker, auf Ihrem Rechner installiert (zum Ausführen der PostgreSQL-Datenbank).
Sie können die Installation unter macOS und Windows über die Docker-Website vornehmen oder Installieren und Verwenden von Docker für Linux-Distributionen folgen.
Grundlegende Vertrautheit mit TypeScript und REST-APIs ist hilfreich, für dieses Tutorial jedoch nicht erforderlich.
Schritt 1 - Erstellen Ihres TypeScript-Projekts
In diesem Schritt werden Sie mit npm ein einfaches TypeScript-Projekt einrichten.
Dieses Projekt wird als Grundlage für die REST-API dienen, die Sie im Laufe dieses Tutorials erstellen werden.
Erstellen Sie zunächst ein neues Verzeichnis für Ihr Projekt:
Navigieren Sie als Nächstes in das Verzeichnis und initialisieren Sie ein leeres npm-Projekt.
Beachten Sie, dass die Option -y hier bedeutet, dass Sie die interaktiven Eingabeaufforderungen des Befehls überspringen.
Um die Eingabeaufforderungen zu durchlaufen, entfernen Sie -y aus dem Befehl:
Weitere Details zu diesen Eingabeaufforderungen finden Sie in Schritt 1 unter Verwenden von Node.js-Modulen mit npm und package.json.
Sie erhalten eine Ausgabe, die der folgenden ähnelt und die Standardantworten umfasst:
Dieser Befehl erstellt eine minimale package.json-Datei, die Sie als Konfigurationsdatei für Ihr npm-Projekt verwenden können.
Sie sind nun bereit, TypeScript in Ihrem Projekt zu konfigurieren.
Führen Sie den folgenden Befehl für eine einfache TypeScript-Einrichtung aus:
Dadurch werden drei Pakete als Entwicklungsabhängigkeiten in Ihrem Projekt installiert:
typescript: Die TypeScript-Toolchain.
ts-node: Ein Paket zum Ausführen von TypeScript-Anwendungen ohne vorherige Kompilierung in JavaScript.
@ types / node: Die TypeScript-Typdefinitionen für Node.js.
Die letzte Aufgabe besteht darin, eine tsconfig.json-Datei hinzuzufügen, um sicherzustellen, dass TypeScript für die von Ihnen erstellte Anwendung richtig konfiguriert ist.
Führen Sie den folgenden Befehl aus, um die Datei zu erstellen:
Fügen Sie in der Datei den folgenden JSON-Code hinzu:
Dies ist eine Standard- und Minimalkonfiguration für ein TypeScript-Projekt.
Wenn Sie mehr über die einzelnen Eigenschaften der Konfigurationsdatei erfahren möchten, können Sie die TypeScript-Dokumentation konsultieren.
Sie haben Ihr einfaches TypeScript-Projekt mit npm eingerichtet.
Als Nächstes werden Sie Ihre PostgreSQL-Datenbank mit Docker einrichten und Prisma damit verbinden.
Schritt 2 - Einrichten von Prisma mit PostgreSQL
In diesem Schritt installieren Sie die Prisma-CLI, erstellen Ihre erste Prisma-Schemadatei, richten PostgreSQL mit Docker ein und verbinden Prisma damit. Das Prisma-Schema ist die wichtigste Konfigurationsdatei für Ihr Prisma-Setup und enthält das Datenbankschema.
Installieren Sie zunächst die Prisma-CLI mit dem folgenden Befehl:
Als bewährte Praxis wird empfohlen, die Prisma-CLI in Ihrem Projekt lokal zu installieren (und nicht im Rahmen einer globalen Installation).
Dadurch lassen sich Versionskonflikte vermeiden, falls Sie mehr als ein Prisma-Projekt auf Ihrem Rechner verwenden.
Als Nächstes richten Sie mit Docker Ihre PostgreSQL-Datenbank ein.
Erstellen Sie mit dem folgenden Befehl eine neue Docker Compose-Datei:
Fügen Sie der neu erstellten Datei den folgenden Code hinzu:
Diese Docker Compose-Datei konfiguriert eine PostgreSQL-Datenbank, auf die über Port 5432 des Docker-Containers zugegriffen werden kann.
Beachten Sie außerdem, dass die Anmeldedaten für die Datenbank aktuell sammy (Benutzer) und your _ password (Passwort) lauten.
Sie können diese Anmeldedaten in Ihren bevorzugten Benutzer und Ihr bevorzugtes Passwort ändern.
Fahren Sie nun fort und starten Sie den PostgreSQL-Datenbankserver mit dem folgenden Befehl:
Die Ausgabe dieses Befehls wird in etwa wie folgt aussehen:
Sie können mit folgendem Befehl überprüfen, ob der Datenbankserver ausgeführt wird:
Dadurch erhalten Sie eine Aufgabe, die in etwa wie folgt aussieht:
Nachdem der Datenbankserver ausgeführt wird, können Sie nun Ihr Prisma-Setup erstellen.
Führen Sie den folgenden Befehl über die Prisma-CLI aus:
Beachten Sie, dass Sie als bewährte Praxis allen Aufrufen der Prisma-CLI npx voranstellen sollten.
Dadurch wird sichergestellt, dass Sie Ihre lokale Installation verwenden.
Nachdem Sie den Befehl ausgeführt haben, erstellt die Prisma-CLI in Ihrem Projekt einen neuen Ordner namens prisma.
Er enthält die folgenden zwei Dateien:
schema.prisma: Die Hauptkonfigurationsdatei für Ihr Prisma-Projekt (schließt Ihr Datenmodell mit ein).
.env: Eine dotenv-Datei zum Definieren Ihrer Datenbankverbindungs-URL.
Um sicherzustellen, dass Prisma den Speicherort Ihrer Datenbank kennt, öffnen Sie die Datei .env und passen Sie die Umgebungsvariable DATABASE _ URL an.
Öffnen Sie zunächst die .env-Datei:
Jetzt können Sie die Umgebungsvariable wie folgt setzen:
Ändern Sie die Anmeldedaten für die Datenbank unbedingt auf jene, die Sie in der Docker Compose-Datei angegeben haben.
Um mehr über das Format der Verbindungs-URL zu erfahren, besuchen Sie die Prisma-Dokumentation.
In diesem Schritt haben Sie Ihre PostgreSQL-Datenbank mit Docker eingerichtet, die Prisma-CLI installiert und Prisma über eine Umgebungsvariable mit der Datenbank verbunden.
Im nächsten Abschnitt definieren Sie Ihr Datenmodell und erstellen Ihre Datenbanktabellen.
Schritt 3 - Definieren des Datenmodells und Erstellen von Datenbanktabellen
In diesem Schritt definieren Sie Ihr Datenmodell in der Prisma-Schemadatei.
Dieses Datenmodell wird dann mit Prisma Migrate der Datenbank zugeordnet; dadurch werden die SQL-Anweisungen generiert und gesendet, um die Tabellen zu erstellen, die Ihrem Datenmodell entsprechen.
Da Sie eine Blogging-Anwendung erstellen, werden die wichtigsten Entitäten der Anwendung Benutzer und Beiträge sein.
Prisma verwendet seine eigene Datenmodellierungssprache zum Definieren der Form Ihrer Anwendungsdaten.
Öffnen Sie zunächst Ihre schema.prisma-Datei mit dem folgenden Befehl:
Fügen Sie nun folgende Modelldefinitionen hinzu. Sie können die Modelle am Ende der Datei platzieren, unmittelbar nach dem generator client-Block:
Sie definieren zwei Modelle namens User und Post.
Jedes von ihnen verfügt über eine Reihe von Feldern, die die Eigenschaften des Modells darstellen.
Die Modelle werden Datenbanktabellen zugeordnet; die Felder stellen die einzelnen Spalten dar.
Beachten Sie außerdem, dass es eine one-to-many-Beziehung zwischen den beiden Modellen gibt, die von den Beziehungsfeldern posts und author in User und Post angegeben werden.
Das bedeutet, dass ein Benutzer mit verschiedenen Beiträgen verknüpft sein kann.
Nach Implementierung dieser Modelle können Sie nun unter Verwendung von Prisma Migrate die entsprechenden Tabellen in der Datenbank erstellen.
Führen Sie in Ihrem Terminal folgenden Befehl aus:
Dieser Befehl erstellt in Ihrem Dateisystem eine neue Migration.
Hier finden Sie einen kurzen Überblick über die drei Optionen, die dem Befehl bereitgestellt werden:
--experimental: Erforderlich, da Prisma Migrate derzeit in einem experimentellen Zustand ist.
--create-db: Ermöglicht Prisma Migrate die Erstellung der Datenbank mit dem Namen my-blog, die in der Verbindungs-URL angegeben ist.
--name "init ": Gibt den Namen der Migration an (wird zum Benennen des Migrationsordners verwendet, der in Ihrem Dateisystem erstellt wird).
Sie können die Migrationsdateien erkunden, die im Verzeichnis prisma / migrations erstellt wurden.
Um die Migration für Ihre Datenbank auszuführen und die Tabellen für Ihre Prisma-Modelle zu erstellen, führen Sie in Ihrem Terminal folgenden Befehl aus:
Prisma Migrate generiert nun die SQL-Anweisungen, die für die Migration erforderlich sind, und sendet sie an die Datenbank.
Im Folgenden sehen Sie die SQL-Anweisungen, die die Tabellen erstellt haben:
In diesem Schritt haben Sie mit Prisma Migrate Ihr Datenmodell im Prisma-Schema definiert und die jeweiligen Datenbanktabellen erstellt.
Im nächsten Schritt installieren Sie Prisma Client in Ihrem Projekt, damit Sie die Datenbank abfragen können.
Schritt 4 - Erkunden von Prisma Client-Abfragen in einem einfachen Skript
Prisma Client ist ein automatisch generierter und typensicherer Query Builder, mit dem Sie aus einer Node.js- oder TypeScript-Anwendung Daten in einer Datenbank programmatisch lesen und schreiben können.
Sie werden ihn für Datenbankzugriff in Ihren REST-API-Routen verwenden, indem Sie traditionelle ORMs, einfache SQL-Abfragen, benutzerdefinierte Datenzugriffsebenen oder andere Methoden zur Kommunikation mit einer Datenbank ersetzen.
In diesem Schritt installieren Sie Prisma Client und lernen die Abfragen kennen, die Sie damit senden können. Bevor Sie in den nächsten Schritten die Routen für Ihre REST-API implementieren, werden Sie zunächst einige der Prisma Client-Abfragen in einem einfachen ausführbaren Skript erkunden.
Zuerst installieren Sie Prisma Client in Ihrem Projekt, indem Sie Ihr Terminal öffnen und das Prisma Client npm-Paket installieren:
Erstellen Sie als Nächstes ein neues Verzeichnis namens src, das Ihre Quelldateien enthalten wird:
Erstellen Sie nun in dem neuen Verzeichnis eine TypeScript-Datei:
Alle Prisma Client-Abfragen geben promises (Zusagen) zurück, auf die Sie in Ihrem Code warten können.
Dazu müssen Sie die Abfragen in einer async-Funktion senden.
Fügen Sie den folgenden Codebaustein mit einer async-Funktion hinzu, die in Ihrem Skript ausgeführt wird:
Hier finden Sie einen kurzen Überblick über den Codebaustein:
Sie importieren den PrismaClient-Konstruktor aus dem zuvor installierten @ prisma / client npm-Paket.
Sie instanziieren PrismaClient, indem Sie den Konstrukteur aufrufen, und erhalten eine Instanz namens prisma.
Sie definieren eine async-Funktion namens main, wo Sie als Nächstes Ihre Prisma Client-Abfragen hinzufügen werden.
Sie rufen die main-Funktion auf, fangen dabei alle möglichen Ausnahmen ab und stellen sicher, dass Prisma Client alle offenen Datenbankverbindungen schließt, indem Sie prisma.disconnect () aufrufen.
Nach Implementierung der main-Funktion können Sie mit dem Hinzufügen von Prisma Client-Abfragen in das Skript beginnen.
Passen Sie index.ts wie folgt an:
In diesem Code verwenden Sie zwei Prisma Client-Abfragen:
create: Erstellt einen neuen User-Eintrag.
Beachten Sie, dass Sie in Wahrheit ein nested write verwenden, was bedeutet, dass Sie in derselben Abfrage sowohl einen User- als auch einen Post-Eintrag erstellen.
findMany: Liest alle vorhandenen User-Einträge aus der Datenbank.
Sie geben die include-Option an, wodurch zusätzlich auch die entsprechenden Post-Einträge für die einzelnen User-Einträge geladen werden.
Führen Sie das Skript nun mit dem folgenden Befehl aus:
Sie erhalten in Ihrem Terminal folgende Ausgabe:
Anmerkung: Wenn Sie eine Datenbank-GUI verwenden, können Sie überprüfen, ob die Daten erstellt wurden, indem Sie sich die Tabellen User und Post ansehen.
Alternativ können Sie die Daten in Prisma Studio erkunden, indem Sie npx prisma studio --experimental ausführen.
Sie haben Prisma Client nun verwendet, um Daten in Ihrer Datenbank zu lesen und zu schreiben.
In den verbleibenden Schritten wenden Sie dieses neue Wissen an, um die Routen für eine beispielhafte REST-API zu implementieren.
Schritt 5 - Implementieren Ihrer ersten REST-API-Route
In diesem Schritt installieren Sie Express in Ihrer Anwendung.
Express ist ein beliebtes Webframework für Node.js, das Sie zur Implementierung der REST-API-Routen in diesem Projekt verwenden werden.
Die erste Route, die Sie implementieren werden, erlaubt es, mithilfe einer GET-Anfrage alle Benutzer von der API abzurufen.
Die Benutzerdaten werden mit Prisma Client aus der Datenbank abgerufen.
Installieren Sie dann Express mit dem folgenden Befehl:
Da Sie TypeScript verwenden, sollten Sie die jeweiligen Typen auch als Entwicklungsabhängigkeiten installieren.
Nach Implementierung der Abhängigkeiten können Sie Ihre Express-Anwendung einrichten.
Öffnen Sie dazu erneut Ihre zentrale Quelldatei:
Löschen Sie nun den ganzen Code in index.ts und ersetzen Sie ihn durch Folgendes, um Ihre REST-API zu starten:
Hier finden Sie eine kurze Aufschlüsselung des Codes:
Sie importieren PrismaClient und express aus den jeweiligen npm-Paketen.
Sie erstellen Ihre Express-Anwendung, indem Sie express () aufrufen.
Sie fügen die Middleware express.json () hinzu, um sicherzustellen, dass Express JSON-Daten ordnungsgemäß verarbeiten kann.
Sie starten den Server unter Port 3000.
Jetzt können Sie Ihre erste Route implementieren.
Fügen Sie zwischen den Aufrufen für app.use und app.listen folgenden Code hinzu:
Speichern und schließen Sie anschließend Ihre Datei.
Starten Sie dann mit dem folgenden Befehl Ihren lokalen Webserver:
Um auf die Route / users zuzugreifen, können Sie Ihren Browser auf http: / / localhost: 3000 / users oder einen anderen HTTP-Client verweisen.
In diesem Tutorial testen Sie alle REST-API-Routen mit curl (einem Terminal-basierten HTTP-Client).
Anmerkung: Wenn Sie lieber einen GUI-basierten HTTP-Client verwenden, können Sie Alternativen wie Postwoman oder den Advanced REST Client verwenden.
Öffnen Sie zum Testen Ihrer Route ein neues Terminalfenster oder eine Registerkarte (damit Ihr lokaler Webserver weiter ausgeführt werden kann) und führen Sie folgenden Befehl aus:
Sie erhalten die im vorherigen Schritt erstellten User-Daten:
Beachten Sie, dass das posts-Array diesmal nicht enthalten ist.
Das liegt daran, dass Sie die Option include nicht an den findMany-Aufruf in der Implementierung der Route / users übergeben.
Sie haben unter / users Ihre erste REST-API-Route implementiert.
Im nächsten Schritt implementieren Sie die verbleibenden REST-API-Routen, um Ihrer API weitere Funktionen hinzuzufügen.
Schritt 6 - Implementieren der verbleibenden REST-API-Routen
In diesem Schritt implementieren Sie die verbleibenden REST-API-Routen für Ihre Blogging-Anwendung.
Am Ende wird Ihr Webserver verschiedene GET-, POST-, PUT- und DELETE-Anfragen bereitstellen.
Hier finden Sie einen Überblick über die verschiedenen Routen, die Sie implementieren werden:
HTTP-Methode
Route
GET
/ feed
Ruft alle veröffentlichten Beiträge ab.
/ post /: id
Ruft einen einzelnen Beitrag anhand seiner ID ab.
POST
/ user
Erstellt einen neuen Benutzer.
/ post
Erstellt einen neuen Beitrag (als Entwurf).
PUT
/ post / publish /: id
Setzt das published-Feld eines Beitrags auf true.
DELETE
post /: id
Löscht einen Beitrag anhand seiner ID.
Fahren Sie fort und implementieren Sie zunächst die verbleibenden GET-Routen.
Öffnen Sie die Datei index.ts mit dem folgenden Befehl:
Fügen Sie dann im Anschluss an die Implementierung der Route / users folgenden Code hinzu:
Dieser Code implementiert die API-Routen für zwei GET-Anfragen:
/ feed: Gibt eine Liste mit veröffentlichten Beiträgen zurück.
/ post /: id: Gibt einen einzelnen Beitrag anhand seiner ID zurück.
Prisma Client wird in beiden Implementierungen verwendet.
In der Implementierung der Route / feed filtert die Abfrage, die Sie mit Prisma Client senden, nach allen Post-Einträgen, bei denen die Spalte published den Wert true enthält.
Außerdem nutzt die Prisma Client-Abfrage include, um die entsprechenden author-Informationen für die einzelnen Beiträge abzurufen.
In der Implementierung der Route / post /: id übergeben Sie die ID, die aus dem URL-Pfad abgerufen wird, um einen bestimmten Post-Eintrag aus der Datenbank zu lesen.
Sie können den Server anhalten, indem Sie auf Ihrer Tastatur Strg + C drücken.
Starten Sie dann den Server neu:
Um die Route / feed zu testen, können Sie folgenden curl-Befehl verwenden:
Da noch keine Beiträge veröffentlicht wurden, ist die Antwort ein leeres Array:
Um die Route / post /: id zu testen, können Sie folgenden curl-Befehl verwenden:
Dadurch wird der von Ihnen ursprünglich erstellte Beitrag zurückgegeben:
Implementieren Sie als Nächstes die beiden POST-Routen.
Fügen Sie nach den Implementierungen der drei GET-Routen folgenden Code zu index.ts hinzu:
Dieser Code implementiert die API-Routen für zwei POST-Anfragen:
/ user: Erstellt in der Datenbank einen neuen Benutzer.
/ post: Erstellt in der Datenbank einen neuen Beitrag.
Wie zuvor wird in beiden Implementierungen Prisma Client verwendet.
In der Implementierung der Route / user übergeben Sie die Werte aus dem Haupttext der HTTP-Anfrage an die Prisma Client-Abfrage create.
Die Route / post ist etwas aufwendiger: Hier können Sie die Werte aus dem Haupttext der HTTP-Anfrage nicht direkt übergeben; stattdessen müssen Sie sie zunächst manuell extrahieren, um sie dann an die Prisma Client-Abfrage zu übergeben.
Der Grund dafür besteht darin, dass die Struktur von JSON im Haupttext der Anfrage nicht mit der Struktur übereinstimmt, die Prisma Client erwartet. Daher müssen Sie die erwartete Struktur manuell einrichten.
Sie können die neuen Routen testen, indem Sie den Server mit Strg + C anhalten.
Um über die Route / user einen neuen Benutzer zu erstellen, können Sie mit curl folgende POST-Anfrage senden:
Dadurch wird in der Datenbank ein neuer Benutzer erstellt und folgende Ausgabe ausgedruckt:
Um über die Route / post einen neuen Beitrag zu erstellen, können Sie mit curl folgende POST-Anfrage senden:
Dadurch wird in der Datenbank ein neuer Beitrag erstellt und unter Verwendung der E-Mail-Adresse bob @ prisma.io mit dem Benutzer verbunden.
Schließlich können Sie die Routen PUT und DELETE implementieren.
Fügen Sie dann nach der Implementierung der beiden POST-Routen den hervorgehobenen Code hinzu:
Dieser Code implementiert die API-Routen für eine PUT- sowie eine DELETE-Anfrage.
/ post / publish /: id (PUT): Veröffentlicht einen Beitrag anhand seiner ID.
/ post /: id (DELETE) Löscht einen Beitrag anhand seiner ID.
Erneut wird Prisma Client in beiden Implementierungen verwendet.
In der Implementierung der Route / post / publish /: id wird die ID des zu veröffentlichenden Beitrags von der URL abgerufen und an die update-Abfrage von Prisma Client übergeben.
Die Implementierung der Route / post /: id zum Löschen eines Beitrags in der Datenbank ruft außerdem die Beitrags-ID aus der URL ab und übergibt sie an die delete-Abfrage von Prisma Client.
Halten Sie den Server mit Strg + C auf Ihrer Tastatur erneut an.
Sie können die Route PUT mit dem folgenden curl-Befehl testen:
Dadurch wird der Beitrag mit einem ID-Wert von 2 veröffentlicht. Wenn Sie die Anfrage / feed neu senden, wird dieser Beitrag nun in die Antwort aufgenommen.
Abschließend können Sie die Route DELETE mit dem folgenden curl-Befehl testen:
Dadurch wird der Beitrag mit dem ID-Wert von 1 gelöscht. Um zu überprüfen, ob der Beitrag mit dieser ID gelöscht wurde, können Sie erneut eine GET-Anfrage an die Route / post / 1 senden.
In diesem Schritt haben Sie die verbleibenden REST-API-Routen für Ihre Blogging-Anwendung implementiert.
Die API reagiert nun auf verschiedene GET-, POST-, PUT- und DELETE-Anfragen und implementiert Funktionen zum Lesen und Schreiben von Daten in der Datenbank.
In diesem Artikel haben Sie eine REST-API mit einer Reihe von verschiedenen Routen erstellt, um Benutzer- und Beitragsdaten für eine beispielhafte Blogging-Anwendung zu erstellen, zu lesen, zu aktualisieren und zu löschen.
Innerhalb der API-Routen haben Sie den Prisma Client zum Senden der entsprechenden Abfragen an Ihre Datenbank verwendet.
Als Nächstes können Sie weitere API-Routen implementieren oder Ihr Datenbankschema mithilfe von Prisma Migrate erweitern.
Konsultieren Sie die Prisma-Dokumentation, um mehr über verschiedene Aspekte von Prisma zu erfahren und einige sofort einsatzbereite Beispielprojekte zu erkunden (im Repository prisma-examples). Verwenden Sie dazu Tools wie GraphQL oder grPC APIs.
Verwenden von .map () zur Iteration durch Array-Elemente in JavaScript
Eine der beliebtesten Methoden zur Iteration durch Datensätze in JavaScript ist die .map () -Methode. .map () erstellt ein Array durch den Aufruf einer bestimmten Funktion für jedes Element im übergeordneten Array. .map () ist eine nicht mutierende Methode, die ein neues Array erstellt, anstatt das Original zu ändern.
In diesem Tutorial betrachten wir vier bemerkenswerte Verwendungsmöglichkeiten von .map () in JavaScript: Aufrufen einer Funktion von Array-Elementen, Konvertieren von Zeichenfolgen in Arrays, Rendern von Listen in Bibliotheken und Neuformatieren von Array-Objekten.
3609
Von der klassischen forloop- bis zur forEach () -Methode werden verschiedene Techniken und Methoden verwendet, um durch Datensätze in JavaScript zu iterieren.
Eine der beliebtesten Methoden ist die .map () -Methode. .map () erstellt ein Array durch den Aufruf einer bestimmten Funktion für jedes Element im übergeordneten Array. .map () ist eine nicht mutierende Methode, die ein neues Array erzeugt, im Gegensatz zu mutierenden Methoden, bei denen nur Änderungen am aufrufenden Array vorgenommen werden.
Diese Methode kann bei der Arbeit mit Arrays viele Anwendungen haben.
In diesem Tutorial behandeln wir vier bemerkenswerte Verwendungsmöglichkeiten von .map () in JavaScript: Aufrufen einer Funktion von Array-Elementen, Konvertieren von Zeichenfolgen in Arrays, Rendern von Listen in JavaScript-Bibliotheken und Neuformatieren von Array-Objekten.
Dieses Tutorial erfordert keine Programmierung, aber wenn Sie die Beispiele mitverfolgen möchten, können Sie entweder das Node.js REPL oder Browser-Entwicklertools verwenden.
Um Node.js lokal zu installieren, können Sie den Schritten in Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung folgen.
Chrome DevTools sind verfügbar, indem Sie die neueste Version von Google Chrome herunterladen und installieren.
Schritt 1 - Aufrufen einer Funktion für jedes Element in einem Array
.map () akzeptiert eine Callback-Funktion als eines ihrer Argumente und ein wichtiger Parameter dieser Funktion ist der aktuelle Wert des Elements, das von der Funktion verarbeitet wird.
Dies ist ein erforderlicher Parameter.
Mit diesem Parameter können Sie jedes Element in einem Array ändern und eine neue Funktion erstellen.
Hier ist ein Beispiel:
Diese Ausgabe wird in der Konsole protokolliert:
Um sie sauberer zu gestalten, kann sie weiter vereinfacht werden mit:
Die gleiche Ausgabe wird in der Konsole protokolliert:
Durch Verwendung von Code wie sweetArray.map (makeSweeter) wird Ihr Code etwas lesbarer.
Schritt 2 - Konvertieren einer Zeichenfolge in ein Array
.map () ist dafür bekannt, zum Array-Prototyp zu gehören.
In diesem Schritt verwenden Sie es, um eine Zeichenfolge in ein Array zu konvertieren.
Sie entwickeln hier nicht die Methode, die für Zeichenfolgen funktioniert.
Vielmehr verwenden Sie die spezielle Methode .call ().
In JavaScript ist alles ein Objekt und Methoden sind Funktionen, die an diese Objekte angehängt sind. Mit .call () können Sie den Kontext eines Objekts auf ein anderes Objekt anwenden.
Sie würden also den Kontext von .map () in einem Array in eine Zeichenfolge kopieren.
.call () können Argumente des zu verwendenden Kontexts und Parameter für die Argumente der ursprünglichen Funktion übergeben werden.
Hier haben Sie den Kontext von .map () auf einer Zeichenfolge verwendet und ein Argument der Funktion übergeben, die .map () erwartet.
Dies funktioniert wie die .split () -Methode einer Zeichenfolge, nur dass jedes einzelne Zeichen einer Zeichenfolge geändert werden kann, bevor es in ein Array zurückgegeben wird.
Schritt 3 - Rendern von Listen in JavaScript-Bibliotheken
JavaScript-Bibliotheken wie React verwenden .map (), um Elemente in einer Liste zu rendern.
Dies erfordert jedoch die JSX-Syntax, da die .map () -Methode in JSX-Syntax eingehüllt ist.
Hier ist ein Beispiel für eine React-Komponente:
Dies ist eine zustandslose Komponente in React, die ein div mit einer Liste rendert.
Die einzelnen Listenelemente werden mit .map () gerendert, um über das ursprünglich erstellte Namens-Array zu iterieren.
Diese Komponente wird mit ReactDOM auf dem DOM-Element mit der Id von root gerendert.
Schritt 4 - Neuformatieren von Array-Objekten
.map () kann verwendet werden, um durch Objekte in einem Array zu iterieren und, ähnlich wie bei herkömmlichen Arrays, den Inhalt jedes einzelnen Objekts zu ändern und ein neues Array zurückzugeben.
Diese Änderung erfolgt anhand dessen, was in der Callback-Funktion zurückgegeben wird.
Hier haben Sie jedes Objekt im Array mit der Klammer- und Punktnotation geändert.
Dieser Anwendungsfall kann zur Verarbeitung oder Kondensierung empfangener Daten verwendet werden, bevor sie in einer Frontend-Anwendung gespeichert oder geparst werden.
In diesem Tutorial haben wir vier Verwendungsmöglichkeiten der Methode .map () in JavaScript betrachtet.
In Kombination mit anderen Methoden kann die Funktionalität von .map () erweitert werden.
Weitere Informationen finden Sie in unserem Artikel Verwenden von Array-Methoden in JavaScript: Iterationsmethoden.
Verwenden von EJS zur Vorlagenerstellung Ihrer Node-Anwendung
Bei der Erstellung von schnellen, spontanen Node-Anwendungen ist manchmal eine einfache und schnelle Methode zur Erstellung von Vorlagen unserer Anwendung erforderlich.
Bisher haben wir voll erstellt...
6453
Jade wird standardmäßig als Ansichts-Engine für Express bereitgestellt, doch kann die Jade-Syntax für viele Anwendungsfälle übermäßig komplex sein.
EJS ist eine Alternative, die diese Aufgabe gut erfüllt und sehr einfach einzurichten ist.
Sehen wir uns an, wie wir eine einfache Anwendung erstellen und EJS verwenden können, um wiederholbare Teile unserer Website (Teilbereiche oder Partials) einzubinden und Daten an unsere Ansichten zu übergeben.
Einrichten der Demo-Anwendung
Wir werden für unsere Anwendung zwei Seiten erstellen, wobei eine Seite eine volle Breite und die andere eine Seitenleiste hat.
Holen Sie sich den Code: Ein git-Repo des vollständigen Demo-Codes auf Github finden Sie hier.
Dateistruktur
Hier sind die Dateien, die wir für unsere Anwendung benötigen.
Wir werden unsere Vorlage im Ansichten-Ordner "views" vornehmen und der Rest ist ganz normale Node-Praxis.
package.json enthält unsere Node-Anwendungsinformationen und die Abhängigkeiten, die wir benötigen (express und EJS). server.js enthält die Einrichtung und Konfiguration unseres Express-Servers.
Hier definieren wir unsere Routen zu unseren Seiten.
Node-Einrichtung
Gehen wir in unsere Datei package.json und richten dort unser Projekt ein.
Wir benötigen lediglich Express und EJS.
Jetzt müssen wir die Abhängigkeiten installieren, die wir gerade definiert haben.
Fahren Sie fort und führen Sie aus:
Nach der Installation aller unserer Abhängigkeiten konfigurieren wir unsere Anwendung für die Verwendung von EJS und richten unsere Routen für die beiden Seiten ein, die wir benötigen: die Index-Seite (volle Breite) und die Info- oder About-Seite (Seitenleiste).
Dies führen wir alles in unserer Datei server.js aus.
Hier definieren wir unsere Anwendung und stellen sie so ein, dass sie auf Port 8080 angezeigt wird.
Außerdem müssen wir EJS als Anzeige-Engine für unsere Express-Anwendung mit app.set (' view engine ',' ejs '); festlegen.
Beachten Sie, wie wir mit res.render () eine Ansicht an den Benutzer senden.
Es ist wichtig zu beachten, dass res.render () in einem Ansichten-Ordner nach der Ansicht sucht.
Wir müssen also nur pages / index definieren, da der vollständige Pfad views / pages / index lautet.
Starten unseres Servers
Fahren Sie fort und starten Sie den Server mit:
Jetzt können wir unsere Anwendung im Browser unter http: / / localhost: 8080 und http: / / localhost: 8080 / about sehen.
Unsere Anwendung ist eingerichtet und wir müssen unsere View-Dateien definieren und sehen, wie EJS dort funktioniert.
Erstellen der EJS-Teilbereiche
Wie bei vielen von uns erstellten Anwendungen gibt es viel Code, der wiederverwendet wird.
Wir nennen diese Teilbereiche oder Partials und definieren drei Dateien, die wir auf allen Seiten verwenden: head.ejs, header.ejs und footer.ejs.
Lassen Sie uns diese Dateien jetzt erstellen.
Hinzufügen der EJS Partials zu Ansichten
Wir haben unsere Partials nun definiert.
Wir müssen sie nur in unsere Ansichten einbinden.
Gehen wir in index.ejs und about.ejs und verwenden die Syntax include zum Hinzufügen der Partials.
Syntax für das Einbinden eines EJS-Partials
Verwenden Sie <% < ^ > - < ^ > include (< ^ > 'RELATIVE / PATH / TO / FILE' < ^ >)% >, um ein EJS-Partial in eine andere Datei einzubinden.
Der Bindestrich < ^ > <% - < ^ > anstatt nur <% weist EJS an, rohes HTML zu rendern.
Der Pfad zu dem Partial ist relativ zur aktuellen Datei.
Jetzt können wir unsere definierte Ansicht im Browser unter http: / / localhost: 8080 sehen. node-ejs-templating-index
Für die Info- oder About-Seite fügen wir auch eine Bootstrap-Seitenleiste hinzu, um zu zeigen, wie Partials für die Wiederverwendung über verschiedene Vorlagen und Seiten strukturiert werden können.
Wenn wir http: / / localhost: 8080 / about besuchen, können wir unsere About-Seite mit einer Seitenleiste sehen!
node-ejs-templating-about
Jetzt können wir damit beginnen, EJS für die Datenübergabe von unserer Node-Anwendung an unserer Ansichten zu verwenden.
Übergeben von Daten an Ansichten und Partials
Definieren wir einige grundlegende Variablen und eine Liste, die an unsere Startseite übergeben werden sollen.
Gehen Sie zurück in Ihre Datei server.js und fügen Sie Folgendes in Ihrer Route app.get (' / ') ein.
Wir haben eine Liste namens mascots und eine einfache Zeichenfolge namens tagline erstellt.
Gehen wir in unsere Datei index.ejs und verwenden sie.
Rendern einer einzelnen Variable in EJS
Um eine einzelne Variable zu wiederholen, verwenden wir einfach <% = tagline% >.
Fügen wir dies unserer Datei index.ejs hinzu:
Überschleifen von Daten in EJS
Um unsere Daten überzuschleifen verwenden wir .forEach.
Fügen wir dies unserer View-Datei hinzu:
Jetzt können wir in unserem Browser die neu hinzugefügten Informationen sehen!
node-ejs-templating-rendered
Übergeben von Daten an ein Partial in EJS
Das EJS-Partial hat Zugriff auf dieselben Daten wie die übergeordnete Ansicht.
Seien Sie jedoch vorsichtig: Wenn Sie eine Variable in einem Partial referenzieren, muss < ^ > sie in jeder Ansicht definiert werden, die das Partial verwendet < ^ >, da sonst ein Fehler ausgelöst wird.
Sie können Variablen auch wie folgt in der Include-Syntax definieren und an ein EJS-Partial übergeben:
Sie müssen jedoch wieder vorsichtig sein, wenn Sie annehmen, dass eine Variable definiert wurde.
Wenn Sie in einer Partial auf eine Variable verweisen, die eventuell nicht immer definiert ist, und ihr einen Standardwert geben möchten, können Sie dies folgendermaßen tun:
In der obigen Zeile rendert der EJS-Code den von variant, wenn er definiert ist, und default, wenn er nicht definiert ist.
EJS ermöglicht uns das schnelle Erstellen von Anwendungen, wenn wir nichts allzu Komplexes benötigen.
Durch Verwendung von Partials und die Möglichkeit, Variablen einfach an unsere Ansichten zu übergeben, können wir schnell einige großartige Anwendungen erstellen.
Weitere Informationen zu EJS finden Sie in den offiziellen Dokumenten hier.
Einrichten eines Node-Projekts mit TypeScript
Das Schreiben von serverseitigem JavaScript kann schwierig sein, wenn eine Codebase wächst.
TypeScript ist eine typisierte (optionale) Obermenge von JavaScript, die bei der Erstellung und Verwaltung großer JavaScript-Projekte helfen kann.
Man kann es sich als JavaScript mit zusätzlichen Funktionen wie starker statischer Typisierung, Kompilierung und objektorientierter Programmierung vorstellen.
In diesem Tutorial werden wir erkunden, wie das Express-Framework mit TypeScript verwendet wird.
3569
Node ist eine Laufzeitumgebung, die es ermöglicht, serverseitiges JavaScript zu schreiben.
Seit ihrer Veröffentlichung im Jahr 2011 hat sie eine weite Verbreitung gefunden.
Das Schreiben von serverseitigem JavaScript kann eine Herausforderung sein, da die Codebasis aufgrund der Art der JavaScript-Sprache - dynamisch und schwach typisiert - wächst.
Entwickler, die aus anderen Sprachen zu JavaScript kommen, beklagen sich oft über den Mangel an starker statischer Typisierung, aber genau hier kommt TypeScript ins Spiel, um diese Lücke zu schließen.
Anmerkung: TypeScript ist technisch gesehen eine Obermenge von JavaScript, wobei der gesamte JavaScript-Code gültiger TypeScript-Code ist.
Hier sind einige Vorteile der Verwendung von TypeScript:
Optionale statische Typisierung.
Typinferenz.
Fähigkeit zur Verwendung von Schnittstellen.
In diesem Tutorial richten Sie ein Node-Projekt mit TypeScript ein.
Sie werden eine Express-Anwendung mit TypeScript erstellen und diese in einen sauberen und zuverlässigen JavaScript-Code transpilieren.
Bevor Sie mit diesem Leitfaden beginnen, muss Node.js auf Ihrem Computer installiert sein.
Sie können dies erreichen, indem Sie dem Leitfaden Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung für Ihr Betriebssystem folgen.
Schritt 1 - Initialisieren eines npm-Projekts
Erstellen Sie zunächst einen neuen Ordner namens < ^ > node _ project < ^ > und wechseln Sie in dieses Verzeichnis.
Initialisieren Sie es als npm-Projekt:
Nach der Ausführung von npm init müssen Sie npm Informationen über Ihr Projekt bereitstellen.
Wenn Sie es vorziehen, npm sinnvolle Standardeinstellungen annehmen zu lassen, können Sie das Flag y hinzufügen, um die Eingabeaufforderungen für zusätzliche Informationen zu überspringen:
Nachdem Ihr Projektraum nun eingerichtet ist, können Sie mit der Installation der erforderlichen Abhängigkeiten fortfahren.
Schritt 2 - Installieren der Abhängigkeiten
Wenn ein blankes npm-Projekt initialisiert wurde, besteht der nächste Schritt darin, die Abhängigkeiten zu installieren, die zur Ausführung von TypeScript erforderlich sind.
Führen Sie die folgenden Befehle aus Ihrem Projektverzeichnis aus, um die Abhängigkeiten zu installieren:
Das Flag -D ist die Kurzform für: -save-dev.
In der Dokumentation zu npmjs können Sie mehr über dieses Flag erfahren.
Jetzt ist es an der Zeit, das Express-Framework zu installieren:
Der zweite Befehl installiert die Express-Typen für die TypeScript-Unterstützung.
Typen in TypeScript sind Dateien, normalerweise mit einer Erweiterung von .d.ts.
Die Dateien werden verwendet, um Typinformationen über eine API, in diesem Fall das Express-Framework, bereitzustellen.
Dieses Paket ist erforderlich, da TypeScript und Express unabhängige Pakete sind.
Ohne das Paket @ types / express gibt es für TypeScript keine Möglichkeit, Informationen über die Typen von Express-Klassen zu erhalten.
Schritt 3 - Konfigurieren von TypeScript
In diesem Abschnitt richten Sie TypeScript ein und konfigurieren Linting für TypeScript.
TypeScript verwendet eine Datei namens tsconfig.json, um die Compiler-Optionen für ein Projekt zu konfigurieren.
Erstellen Sie eine Datei tsconfig.json im Stammverzeichnis des Projektverzeichnisses und fügen Sie das folgende Snippet ein:
Sehen wir uns einige der Schlüssel im obigen JSON-Snippet an:
module: Gibt die Methode zur Erzeugung des Modulcodes an.
Node verwendet commonjs.
target: Gibt das Ausgabe-Sprachniveau an.
moduleResolution: Dies hilft dem Compiler herauszufinden, worauf sich ein Import bezieht.
Der Wert node imitiert den Mechanismus zur Auflösung des Node-Moduls.
outDir: Dies ist der Speicherort für die Ausgabe von .js-Dateien nach der Transpilation.
In diesem Tutorial speichern Sie sie als dist.
Eine Alternative zur manuellen Erstellung und Population der Datei tsconfig.json ist die Ausführung des folgenden Befehls:
Dieser Befehl generiert eine gut kommentierte tsconfig.json-Datei.
Um mehr über die verfügbaren Schlüsselwertoptionen zu erfahren, bietet die offizielle TypeScript-Dokumentation Erklärungen zu jeder Option.
Jetzt können Sie TypeScript-Linting für das Projekt konfigurieren.
Führen Sie den folgenden Befehl in einem Terminal im Stammverzeichnis Ihres Projektverzeichnisses aus, das in diesem Tutorial als < ^ > node _ project < ^ > eingerichtet wurde, um eine tslint.json-Datei zu erzeugen:
Öffnen Sie die neu erzeugte tslint.json-Datei und fügen Sie die Regel no-console entsprechend hinzu:
Standardmäßig verhindert der TypeScript-Linter die Verwendung von Debugging mittels der Anweisungen console, weshalb der Linter explizit angewiesen werden muss, die standardmäßige Regel no-console zu widerrufen.
Schritt 4 - Aktualisieren der Datei package.json
An diesem Punkt im Tutorial können Sie entweder Funktionen im Terminal einzeln ausführen oder ein npm-Skript erstellen, um sie auszuführen.
In diesem Schritt erstellen Sie ein Skript start, das den TypeScript-Code kompiliert und transpiliert und dann die resultierende Anwendung .js ausführt.
Öffnen Sie die Datei package.json und aktualisieren Sie sie entsprechend:
Im obigen Snippet haben Sie den Pfad main aktualisiert und den Befehl start dem Abschnitt scripts hinzugefügt.
Wenn Sie sich den Befehl start ansehen, sehen Sie, dass zuerst der Befehl tsc ausgeführt wird, und dann der Befehl node.
Dadurch wird die generierte Ausgabe mit node kompiliert und dann ausgeführt.
Der Befehl tsc weist TypeScript an, die Anwendung zu kompilieren und die generierte Ausgabe .js im angegebenen Verzeichnis outDir zu platzieren, wie es in der Datei tsconfig.json festgelegt ist.
Schritt 5 - Erstellen und Ausführen eines einfachen Express-Servers
Nachdem TypeScript und dessen Linter konfiguriert sind, ist es an der Zeit, einen Node Express-Server zu erstellen.
Erstellen Sie zunächst einen Ordner src im Stammverzeichnis Ihres Projektverzeichnisses:
Erstellen Sie dann darin eine Datei namens app.ts:
An diesem Punkt sollte die Ordnerstruktur wie folgt aussehen:
Öffnen Sie die Datei app.ts mit einem Texteditor Ihrer Wahl und fügen Sie das folgende Code-Snippet ein:
Der obige Code erstellt einen Node-Server, der auf Port 3000 auf Anfragen lauscht.
Führen Sie die Anwendung mit dem folgenden Befehl aus:
Wenn sie erfolgreich ausgeführt wird, wird eine Nachricht im Terminal protokolliert:
Sie können nun http: / / localhost: 3000 in Ihrem Browser besuchen, und Sie sollten die Nachricht sehen:
Browserfenster mit der Nachricht: The sedulous hyena ate the antelope! (Die fleißige Hyäne fraß die Antilope!)
Öffnen Sie die Datei dist / app.js und Sie finden die transpilierte Version des TypeScript-Codes:
An diesem Punkt haben Sie Ihr Node-Projekt erfolgreich für die Verwendung von TypeScript eingerichtet.
In diesem Tutorial haben Sie erfahren, warum TypeScript nützlich ist, um zuverlässigen JavaScript-Code zu schreiben.
Sie haben auch einige Vorteile der Arbeit mit TypeScript kennengelernt.
Abschließend haben Sie ein Node-Projekt mit dem Express-Framework eingerichtet, das Projekt jedoch mit TypeScript kompiliert und ausgeführt.
