How To Use Routing with React Navigation in React Native
React Navigation is a popular library for routing and navigation in a React Native application.
In this tutorial, you will build a social media application to explore how to navigate mobile application screens using react-navigation.
4936
Introduction
This library helps solve the problem of navigating between multiple screens and sharing data between them.
At the end of this tutorial, you will have a rudimentary social network.
It will display the number of connections a user has and provide a way to connect with additional friends.
You will use this sample application to explore how to navigate mobile application screens using react-navigation.
Prerequisites
To complete this tutorial, you'll need:
A local development environment for Node.js.
Follow How to Install Node.js and Create a Local Development Environment.
Familiarity with setting up your environment to create a new React Native project and use the iOS or Android simulators may be beneficial.
< $> note
Note: If you worked with react-navigation in the past, you might encounter some differences.
You can consult the documentation for guides on migrating from 3.x and migrating from 4.x.
< $>
This tutorial was verified with Node v14.7.0, npm v6.14.7, react v16.13.1, react-native v0.63.2, @ react-navigation / native v5.7.3, and @ react-navigation / stack v5.9.0.
Step 1 - Creating a New React Native App
First, create a new React Native app by entering the following command in your terminal:
Then, navigate to the new directory:
And start the application for iOS:
Alternatively, for Android:
Note: If you experience any problems, you may need to consult troubleshooting issues for React Native CLI.
This will create a skeleton project for you.
It doesn't look much like a social network right now.
Let's fix that.
Open up App.js:
Replace the contents of App.js with the following code to display a welcome message:
Save the file.
Now, when you run the application, a Welcome to MySocialNetwork!
message will appear in your simulator.
In the next step, you will add more screens to your application.
Step 2 - Creating a HomeScreen and FriendsScreen
Currently, you have a single screen displaying a welcome message.
In this step, you will create the two screens for your application: HomeScreen and FriendsScreen.
HomeScreen
Your app will need a HomeScreen.
The HomeScreen will display the number of friends already in your network.
Take the code from App.js and add it to a new file called HomeScreen.js:
Open HomeScreen.js:
Modify HomeScreen.js to use HomeScreen instead of App:
This code will produce a You have (undefined) friends. placeholder message.
You will provide a value later.
FriendsScreen
Your app will also need a FriendsScreen.
On the FriendsScreen you will be able to add new friends.
Take the code from App.js and add it to a new file called FriendsScreen.js:
Open FriendsScreen.js:
Modify FriendsScreen.js to use FriendsScreen instead of App:
This code will produce an Add friends here!
message.
At this point, you have a HomeScreen and FriendsScreen.
However, there is no way to navigate between them.
You will build this functionality in the next step.
Step 3 - Using StackNavigator with React Navigation
For navigating between screens, you will use a StackNavigator.
A StackNavigator works exactly like a call stack.
Each screen you navigate to is pushed to the top of the stack.
Each time you hit the back button, the screens pop off the top of the stack.
First, install @ react-navigation / native:
Then, install @ react-navigation / stack and its peer dependencies:
Note: If you are developing for iOS, you may need to navigate to the ios directory and run pod install.
Next, revisit App.js:
Add NavigationContainer and createStackNavigator to App.js:
Then, replace the contents of render:
Nested inside < Stack.Navigator >, add the HomeScreen:
This code creates a very small stack for your navigator with only one screen: HomeScreen.
Nested inside < Stack.Navigator >, add the FriendsScreen:
This code adds the FriendsScreen to the navigator.
Note: This differs from how createStackNavigator was used in previous versions of React Navigation.
Now, the navigator is aware of your two screens.
Adding Buttons to HomeScreen and FriendsScreen
Finally, add buttons to take you between your two screens.
In HomeScreen.js, add the following code:
In FriendsScreen.js, add the following code:
Let's talk about this.props.navigation.
As long as your screen is included in the StackNavigator, it automatically inherits many useful props from the navigation object.
In this case, you used navigate to move to a different page.
HomeScreen and FriendsScreen
If you open your simulator now, you can navigate between the HomeScreen and FriendsScreen.
Step 4 - Using Context to Pass Data to Other Screens
In this step, you will create an array of possible friends - Alice, Bob, and Sammy - and an empty array of current friends.
You will also create functionality for the user to add possible friends to their current friends.
Open App.js:
Add possibleFriends and currentFriends to the component's state:
Next, add a function to move a possible friend into the list of current friends:
At this point, you have finished building the functionality to add friends.
Adding FriendsContext to App
Now you can add friends in App.js, but you will want to add them to FriendsScreen.js and have them display in HomeScreen.js.
Since this project is built with React, you can inject this functionality into your screens with context.
Note: In previous versions of React Navigation, it was possible to use screenProps to share data between screens.
In the current version of React Navigation, it is recommended to use React Context to share data between screens.
To avoid a circular reference, you will want a new FriendsContext file:
Export FriendsContext:
Add the FriendsContext:
This code establishes FriendsContext as a new Context object and wraps the NavigationContainer in a Context.Provider component so any children in the component tree can subscribe to context changes.
Since you are no longer using View or Text, it is possible to remove those imports from react-native.
You will need to provide a value to make data accessible by consumers:
This will allow the HomeScreen and FriendsScreen to reference any context changes to currentFriends and possibleFriends.
Now you can work on referencing the context in your screens.
Adding FriendsContext to HomeScreen
In this step, you will set up the application to display the current friend count.
And add the FriendsContext:
This code establishes a Class.contextType.
You can now access context in your screens.
For instance, let's make your HomeScreen display how many currentFriends you have:
If you open your app up again in the simulator and view the HomeScreen, you will see the message: You have 0 friends!.
Adding FriendsContext to FriendsScreen
In this step, you will set up the application to display the possible friends and provide buttons for adding them to the current friends.
Next, open FriendsScreen.js:
Now, create a button to add friends in FriendsScreen.js:
If you open your app up again in the simulator and view the FriendsScreen, you will see a list of friends to add.
HomeScreen with 0 currentFriends and FriendScreen with 3 possibleFriends
If you visit the FriendsScreen and click on the button to add friends, you will see the list of possibleFriends decrease.
If you visit the HomeScreen, you will see the number of friends increase.
You can now navigate between screens and share data between them.
Conclusion
In this tutorial, you have created a sample React Native application with multiple screens.
Using React Navigation, you devised a way to navigate between screens.
Using React Context, you developed a way to share data between screens.
The complete source code for this tutorial is available on GitHub.
If you want to dig deeper into React Navigation, check out their documentation.
React Navigation is not the only routing and navigation solution.
There is also React Native Navigation, React Native Router Flux, and React Router Native.
If you'd like to learn more about React, take a look at our How To Code in React.js series, or check out our React topic page for exercises and programming projects.
The author selected the COVID-19 Relief Fund to receive a donation as part of the Write for DOnations program.
A little-known feature of Gmail and Google Apps email is Google's portable SMTP server.
All you need is either (i) a free Gmail account, or (ii) a paid G Suite account.
Benefits
SMTP password: < ^ > Your Gmail or G Suite email password < ^ >
You can verify the list by going to the Accounts and Import tab on the settings screen.
This limit restricts the number of messages sent per day to 99 emails; the restriction is automatically removed 24 hours after you hit the limit.
In this article, you will be introduced to each principle individually to understand how SOLID can help make you a better developer.
Note: While these principles can apply to various programming languages, the sample code contained in this article will use PHP.
L - Liskov Substitution Principle
D - Dependency Inversion Principle
Single-Responsibility Principle
The area of a square is calculated by length squared.
Here is an example with a collection of three shapes:
a circle with a radius of 2
Now, the logic you need to output the data to the user is handled by the SumCalculatorOutputter class.
Objects or entities should be open for extension but closed for modification.
Here is the area method defined in Square:
How do you know that the object passed into the AreaCalculator is actually a shape or if the shape has a method named area?
Modify your shape classes to implement the ShapeInterface.
When you call the HTML method on the $output2 object, you will get an E _ NOTICE error informing you of an array to string conversion.
Still building from the previous ShapeInterface example, you will need to support the new three-dimensional shapes of Cuboid and Spheroid, and these shapes will need to also calculate volume.
This is a much better approach, but a pitfall to watch out for is when type-hinting these interfaces.
Instead of using a ShapeInterface or a ThreeDimensionalShapeInterface, you can create another interface, maybe ManageShapeInterface, and implement it on both the flat and three-dimensional shapes.
This principle allows for decoupling.
In this article, you were presented with the five principles of SOLID Code.
Continue your learning by reading about other practices for Agile and Adaptive software development.
In this tutorial, you will create both GET and POST requests using the Fetch API.
To complete this tutorial, you will need the following:
To install Node on macOS, follow the steps outlined in this How to Install Node.js and Create a Local Development Environment on macOS tutorial.
Read the Promises section of this article on the event loop, callbacks, promises, and async / await in JavaScript.
If this happens, the reject promise will be returned.
Create a constant variable called url which will hold the API URL that will return ten random users:
The resp parameter takes the value of the object returned from fetch (url).
Use the json () method to convert resp into JSON data:
The API offers a name for the author and a picture that goes along with the name.
The innerHTML property and string interpolation will allow you to do this:
With both then () functions completed, you can now add the catch () function.
This function will log the potential error to the console:
Step 3 - Handling POST Requests
This will be an object called data with the key name and value Sammy (or your name):
The Headers interface is a property of the Fetch API, which allows you to perform various actions on HTTP request and response headers.
With this code in place, the POST request can be made using the Fetch API.
The new Request construct takes two arguments: the API url (url) and an object.
Now, request can be used as the sole argument for fetch () since it also includes the API url:
Styling Scrollbars in Chrome, Edge, and Safari
Here is an example that uses:: -webkit-scrollbar,:: -webkit-scrollbar-track, and:: webkit-scrollbar-thumb pseudo-elements:
This code works in the latest releases of Chrome, Edge, and Safari.
Building Future-Proof Scrollbar Styles
You can write your CSS in a way to support both -webkit-scrollbar and CSS Scrollbars specifications.
In this article, you were introduced to using CSS to style scrollbars and how to ensure these styles are recognized in most modern browsers.
However, these approaches run into limitations with reproducing experiences like inertia scrolling (e.g., decaying motion when scrolling via trackpads).
With CSS and CSS3 you can do a lot of things, but setting an opacity on a CSS background is not one of them.
Both of the following methods have excellent browser support down to Internet Explorer 8.
All you have to do is put the image inside of a position: relative; container.
And here's what your CSS will look like:
Here's a live demo:
Here again we must move the z-index of content (in this cas the < h1 >) above the background pseudoelement, and we must explicitly define the position: absolute; and z-index: 1 on the: before pseudoelement.
In this article, we will discuss how to correctly and securely obtain root privileges, with a special focus on editing the / etc / sudoers file.
Log In As Root
If you have not set up SSH keys for the root user, enter the root password when prompted.
Use su to Become Root
To gain root privileges, type:
When you have finished the tasks which require root privileges, return to your normal shell by typing:
Use sudo to Execute Commands as Root
Unlike su, the sudo command will request the password of the current user, not the root password.
Check out our How To Create a New Sudo-enabled User quickstart tutorials for Ubuntu and CentOS to learn how to set up a sudo-enabled user.
What is Visudo?
The sudo command is configured through a file located at / etc / sudoers.
< $> warning
Because improper syntax in the / etc / sudoers file can leave you with a broken system where it is impossible to obtain elevated privileges, it is important to use the visudo command to edit the file.
The visudo command opens a text editor like normal, but it validates the syntax of the file upon saving.
This prevents configuration errors from blocking sudo operations, which may be your only way of obtaining root privileges.
Traditionally, visudo opens the / etc / sudoers file with the vi text editor.
On CentOS, you can change this value by adding the following line to your ~ / .bashrc:
You will be presented with the / etc / sudoers file in your selected text editor.
Let's take a look at what these lines do.
The first line, "Defaults env _ reset", resets the terminal environment to remove any user variables.
By default, this is the root account.
The third line, which begins with "Defaults secure _ path =...", specifies the PATH (the places in the filesystem the operating system will look for applications) that will be used for sudo operations.
root ALL = (ALL: < ^ > ALL < ^ >) ALL
Group Privilege Lines
As with the / etc / sudoers file itself, you should always edit files within the / etc / sudoers.d directory with visudo.
The most common operation that users want to accomplish when managing sudo permissions is to grant a new user general sudo access.
Or, using gpasswd:
We can then allow members of GROUPTWO to update the apt database by creating a rule like this:
This will allow anyone who is a member of GROUPONE to execute commands as the www-data user or the apache user.
It has a companion command called PASSWD, which is the default behavior.
You will be prompted for your password, which will be cached for later sudo uses until the sudo time frame expires.
This gives you a good idea of what you will or will not be allowed to do with sudo as any user.
For some fun, you can add the following line to your / etc / sudoers file with visudo:
You should now have a basic understanding of how to read and modify the sudoers file, and a grasp on the various methods that you can use to obtain root privileges.
Understanding Nginx Server and Location Block Selection Algorithms
In this guide, we will discuss how Nginx selects the server and location block that will handle a given client's request.
1381
In this guide, we will discuss some of the behind-the-scenes details that determine how Nginx processes client requests.
Understanding these ideas can help take the guesswork out of designing server and location blocks and can make the request handling seem less unpredictable.
Nginx logically divides the configurations meant to serve different content into blocks, which live in a hierarchical structure.
The main blocks that we will be discussing are the server block and the location block.
It is an extremely flexible model.
It matches this against the listen directive of each server to build a list of the server blocks that can possibly resolve the request.
The listen directive typically defines which IP address and port that the server block will respond to.
The listen directive can be set to:
A lone port which will listen to every interface on that port.
The last option will generally only have implications when passing requests between different servers.
When trying to determine which server block to send a request to, Nginx will first try to decide based on the specificity of the listen directive using the following rules:
A block with no listen directive uses the value 0.0.0.0: 80.
A block set to port 8888 with no IP address becomes 0.0.0.0: 8888
Parsing the "server _ name" Directive to Choose a Match
If no match is found using a leading wildcard, Nginx then looks for a server block with a server _ name that matches using a trailing wildcard (indicated by a server name ending with a * in the config).
If one is found, that block is used to serve the request.
If multiple matches are found, the longest match will be used to serve the request.
The first server _ name with a regular expression that matches the "Host" header will be used to serve the request.
The first matching regular expression will be selected to respond to the request.
Location blocks live within server blocks (or other location blocks) and are used to decide how to process the request URI (the part of the request that comes after the domain name or IP address / port).
The < ^ > location _ match < ^ > in the above defines what Nginx should check the request URI against.
~: If a tilde modifier is present, this location will be interpreted as a case-sensitive regular expression match.
~ *: If a tilde and asterisk modifier is used, the location block will be interpreted as a case-insensitive regular expression match.
Finally, this block would prevent regular expression matching from occurring if it is determined to be the best non-regular expression match.
If the longest matching prefix location has the ^ ~ modifier, then Nginx will immediately end its search and select this location to serve the request.
If no regular expression locations are found that match the request URI, the previously stored prefix location is selected to serve the request.
It is important to understand that, by default, Nginx will serve regular expression matches in preference to prefix matches.
Although this is a general rule that will allow you to design your location blocks in a predictable way, it is important to realize that there are times when a new location search is triggered by certain directives within the selected location.
In this example, the first location is matched by a request URI of / exact, but in order to handle the request, the index directive inherited by the block initiates an internal redirect to the second block:
For instance, you could set an invalid index for that block and turn on autoindex:
This is one way of preventing an index from switching contexts, but it's probably not useful for most configurations.
This will trigger another location search that will be caught by the second location block.
Another directive that can lead to a location block pass off is the rewrite directive.
However, if a request is made for / rewriteme / fallback / hello, the first block again will match.
It is famous for being scalable, powerful, reliable, and easy to use.
7257
An understanding of the differences between JSON and BSON data in MongoDB.
Once the download completes you should have a file called primer-dataset.json (12 MB size) in the current directory.
Because we didn't have a database called newdb, MongoDB created it automatically.
The result will show 25359, which is the number of imported documents.
For an even better check you can select the first document from the restaurants collection like this:
Such a detailed check could reveal problems with the documents such as their content, encoding, etc. The json format uses UTF-8 encoding and your exports and imports should be in that encoding.
Otherwise, MongoDB will automatically handle it for you.
A simple mongoexport example would be to export the restaurants collection from the newdb database which we have previously imported.
In the above command, we use --db to specify the database, -c for the collection and --out for the file in which the data will be saved.
In some cases you might need to export only a part of your collection.
To exit the MongoDB prompt, type exit:
Similarly, you have to escape any other special characters in the query.
How To Set Up an OpenVPN Server on Ubuntu 18.04
2644
A previous version of this tutorial was written by Justin Ellingwood
Want to access the Internet safely and securely from your smartphone or laptop when connected to an untrusted network such as the WiFi of a hotel or coffee shop?
A Virtual Private Network (VPN) allows you to traverse untrusted networks privately and securely as if you were on a private network.
The traffic emerges from the VPN server and continues its journey to the destination.
When combined with HTTPS connections, this setup allows you to secure your wireless logins and transactions.
You can circumvent geographical restrictions and censorship, and shield your location and any unencrypted HTTP traffic from the untrusted network.
OpenVPN is a full-featured, open-source Secure Socket Layer (SSL) VPN solution that accommodates a wide range of configurations.
In this tutorial, you will set up an OpenVPN server on an Ubuntu 18.04 server and then configure access to it from Windows, macOS, iOS and / or Android.
This tutorial will keep the installation and configuration steps as simple as possible for each of these setups.
< $> note Note: If you plan to set up an OpenVPN server on a DigitalOcean Droplet, be aware that we, like many hosting providers, charge for bandwidth overages.
For this reason, please be mindful of how much traffic your server is handling.
See this page for more info.
To complete this tutorial, you will need access to an Ubuntu 18.04 server to host your OpenVPN service.
You will need to configure a non-root user with sudo privileges before you start this guide.
You can follow our Ubuntu 18.04 initial server setup guide to set up a user with appropriate permissions.
The linked tutorial will also set up a firewall, which is assumed to be in place throughout this guide.
Additionally, you will need a separate machine to serve as your certificate authority (CA).
While it "s technically possible to use your OpenVPN server or your local machine as your CA, this is not recommended as it opens up your VPN to some security vulnerabilities.
Per the official OpenVPN documentation, you should place your CA on a standalone machine that "s dedicated to importing and signing certificate requests.
For this reason, this guide assumes that your CA is on a separate Ubuntu 18.04 server that also has a non-root user with sudo privileges and a basic firewall.
Please note that if you disable password authentication while configuring these servers, you may run into difficulties when transferring files between them later on in this guide.
To resolve this issue, you could re-enable password authentication on each server.
Alternatively, you could generate an SSH keypair for each server, then add the OpenVPN server "s public SSH key to the CA machine" s authorized _ keys file and vice versa.
See How to Set Up SSH Keys on Ubuntu 18.04 for instructions on how to perform either of these solutions.
When you have these prerequisites in place, you can move on to Step 1 of this tutorial.
Step 1 - Installing OpenVPN and EasyRSA
To start off, update your VPN server "s package index and install OpenVPN.
OpenVPN is available in Ubuntu's default repositories, so you can use apt for the installation:
OpenVPN is a TLS / SSL VPN.
This means that it utilizes certificates in order to encrypt traffic between the server and clients.
To issue trusted certificates, you will set up your own simple certificate authority (CA).
To do this, we will download the latest version of EasyRSA, which we will use to build our CA public key infrastructure (PKI), from the project "s official GitHub repository.
As mentioned in the prerequisites, we will build the CA on a standalone server.
The reason for this approach is that, if an attacker were able to infiltrate your server, they would be able to access your CA private key and use it to sign new certificates, giving them access to your VPN.
Accordingly, managing the CA from a standalone machine helps to prevent unauthorized users from accessing your VPN.
Note, as well, that it "s recommended that you keep the CA server turned off when not being used to sign keys as a further precautionary measure.
To begin building the CA and PKI infrastructure, use wget to download the latest version of EasyRSA on both your CA machine and your OpenVPN server.
To get the latest version, go to the Releases page on the official EasyRSA GitHub project, copy the download link for the file ending in .tgz, and then paste it into the following command:
Then extract the tarball:
You have successfully installed all the required software on your server and CA machine.
Continue on to configure the variables used by EasyRSA and to set up a CA directory, from which you will generate the keys and certificates needed for your server and clients to access the VPN.
Step 2 - Configuring the EasyRSA Variables and Building the CA
EasyRSA comes installed with a configuration file which you can edit to define a number of variables for your CA.
On your CA machine, navigate to the EasyRSA directory:
Inside this directory is a file named vars.example.
Make a copy of this file, and name the copy vars without a file extension:
Open this new file using your preferred text editor:
Find the settings that set field defaults for new certificates.
It will look something like this:
Uncomment these lines and update the highlighted values to whatever you'd prefer, but do not leave them blank:
When you are finished, save and close the file.
Within the EasyRSA directory is a script called easyrsa which is called to perform a variety of tasks involved with building and managing the CA.
Run this script with the init-pki option to initiate the public key infrastructure on the CA server:
After this, call the easyrsa script again, following it with the build-ca option.
This will build the CA and create two important files - ca.crt and ca.key - which make up the public and private sides of an SSL certificate.
ca.crt is the CA "s public certificate file which, in the context of OpenVPN, the server and the client use to inform one another that they are part of the same web of trust and not someone performing a man-in-the-middle attack.
For this reason, your server and all of your clients will need a copy of the ca.crt file.
ca.key is the private key which the CA machine uses to sign keys and certificates for servers and clients.
If an attacker gains access to your CA and, in turn, your ca.key file, they will be able to sign certificate requests and gain access to your VPN, impeding its security.
This is why your ca.key file should only be on your CA machine and that, ideally, your CA machine should be kept offline when not signing certificate requests as an extra security measure.
If you don "t want to be prompted for a password every time you interact with your CA, you can run the build-ca command with the nopass option, like this:
In the output, you "ll be asked to confirm the common name for your CA:
The common name is the name used to refer to this machine in the context of the certificate authority.
You can enter any string of characters for the CA "s common name but, for simplicity" s sake, press ENTER to accept the default name.
With that, your CA is in place and it "s ready to start signing certificate requests.
Step 3 - Creating the Server Certificate, Key, and Encryption Files
Now that you have a CA ready to go, you can generate a private key and certificate request from your server and then transfer the request over to your CA to be signed, creating the required certificate.
You "re also free to create some additional files used during the encryption process.
Start by navigating to the EasyRSA directory on your OpenVPN server:
From there, run the easyrsa script with the init-pki option.
Although you already ran this command on the CA machine, it "s necessary to run it here because your server and CA will have separate PKI directories:
Then call the easyrsa script again, this time with the gen-req option followed by a common name for the machine.
Again, this could be anything you like but it can be helpful to make it something descriptive.
Throughout this tutorial, the OpenVPN server "s common name will simply be" server ".
Be sure to include the nopass option as well.
Failing to do so will password-protect the request file which could lead to permissions issues later on:
< $> note Note: If you choose a name other than "server" here, you will have to adjust some of the instructions below.
For instance, when copying the generated files to the / etc / openvpn directory, you will have to substitute the correct names.
You will also have to modify the / etc / openvpn / server.conf file later to point to the correct .crt and .key files.
This will create a private key for the server and a certificate request file called server.req.
Copy the server key to the / etc / openvpn / directory:
Using a secure method (like SCP, in our example below), transfer the server.req file to your CA machine:
Next, on your CA machine, navigate to the EasyRSA directory:
Using the easyrsa script again, import the server.req file, following the file path with its common name:
Then sign the request by running the easyrsa script with the sign-req option, followed by the request type and the common name.
The request type can either be client or server, so for the OpenVPN server "s certificate request, be sure to use the server request type:
In the output, you "ll be asked to verify that the request comes from a trusted source.
Type yes then press ENTER to confirm this:
If you encrypted your CA key, you "ll be prompted for your password at this point.
Next, transfer the signed certificate back to your VPN server using a secure method:
Before logging out of your CA machine, transfer the ca.crt file to your server as well:
Next, log back into your OpenVPN server and copy the server.crt and ca.crt files into your / etc / openvpn / directory:
Then navigate to your EasyRSA directory:
From there, create a strong Diffie-Hellman key to use during key exchange by typing:
This may take a few minutes to complete.
Once it does, generate an HMAC signature to strengthen the server's TLS integrity verification capabilities:
When the command finishes, copy the two new files to your / etc / openvpn / directory:
With that, all the certificate and key files needed by your server have been generated.
You "re ready to create the corresponding certificates and keys which your client machine will use to access your OpenVPN server.
Step 4 - Generating a Client Certificate and Key Pair
Although you can generate a private key and certificate request on your client machine and then send it to the CA to be signed, this guide outlines a process for generating the certificate request on the server.
The benefit of this is that we can create a script which will automatically generate client configuration files that contain all of the required keys and certificates.
This lets you avoid having to transfer keys, certificates, and configuration files to clients and streamlines the process of joining the VPN.
We will generate a single client key and certificate pair for this guide.
If you have more than one client, you can repeat this process for each one.
Please note, though, that you will need to pass a unique name value to the script for every client.
Throughout this tutorial, the first certificate / key pair is referred to as client1.
Get started by creating a directory structure within your home directory to store the client certificate and key files:
Since you will store your clients "certificate / key pairs and configuration files in this directory, you should lock down its permissions now as a security measure:
Next, navigate back to the EasyRSA directory and run the easyrsa script with the gen-req and nopass options, along with the common name for the client:
Press ENTER to confirm the common name.
Then, copy the client1.key file to the / client-configs / keys / directory you created earlier:
Next, transfer the client1.req file to your CA machine using a secure method:
Log in to your CA machine, navigate to the EasyRSA directory, and import the certificate request:
Then sign the request as you did for the server in the previous step.
This time, though, be sure to specify the client request type:
At the prompt, enter yes to confirm that you intend to sign the certificate request and that it came from a trusted source:
Again, if you encrypted your CA key, you "ll be prompted for your password here.
This will create a client certificate file named client1.crt.
Transfer this file back to the server:
SSH back into your OpenVPN server and copy the client certificate to the / client-configs / keys / directory:
Next, copy the ca.crt and ta.key files to the / client-configs / keys / directory as well:
With that, your server and client "s certificates and keys have all been generated and are stored in the appropriate directories on your server.
There are still a few actions that need to be performed with these files, but those will come in a later step.
For now, you can move on to configuring OpenVPN on your server.
Step 5 - Configuring the OpenVPN Service
Now that both your client and server "s certificates and keys have been generated, you can begin configuring the OpenVPN service to use these credentials.
Start by copying a sample OpenVPN configuration file into the configuration directory and then extract it in order to use it as a basis for your setup:
Open the server configuration file in your preferred text editor:
Find the HMAC section by looking for the tls-auth directive.
This line should already be uncommented, but if isn "t then remove the ";" to uncomment it:
Next, find the section on cryptographic ciphers by looking for the commented out cipher lines.
The AES-256-CBC cipher offers a good level of encryption and is well supported.
Again, this line should already be uncommented, but if it isn "t then just remove the ";" preceding it:
Below this, add an auth directive to select the HMAC message digest algorithm.
For this, SHA256 is a good choice:
Next, find the line containing a dh directive which defines the Diffie-Hellman parameters.
Because of some recent changes made to EasyRSA, the filename for the Diffie-Hellman key may be different than what is listed in the example server configuration file.
If necessary, change the file name listed here by removing the 2048 so it aligns with the key you generated in the previous step:
Finally, find the user and group settings and remove the "; "at the beginning of each to uncomment these lines:
The changes you "ve made to the sample server.conf file up to this point are necessary in order for OpenVPN to function.
The changes outlined below are optional, though they too are needed for many common use cases.
(Optional) Push DNS Changes to Redirect All Traffic Through the VPN
The settings above will create the VPN connection between the two machines, but will not force any connections to use the tunnel.
If you wish to use the VPN to route all of your traffic, you will likely want to push the DNS settings to the client computers.
There are a few directives in the server.conf file which you must change in order to enable this functionality.
Find the redirect-gateway section and remove the semicolon "; "from the beginning of the redirect-gateway line to uncomment it:
Just below this, find the dhcp-option section.
Again, remove the "; "from in front of both of the lines to uncomment them:
This will assist clients in reconfiguring their DNS settings to use the VPN tunnel for as the default gateway.
(Optional) Adjust the Port and Protocol
By default, the OpenVPN server uses port 1194 and the UDP protocol to accept client connections.
If you need to use a different port because of restrictive network environments that your clients might be in, you can change the port option.
If you are not hosting web content on your OpenVPN server, port 443 is a popular choice since it is usually allowed through firewall rules.
Oftentimes, the protocol is restricted to that port as well.
If so, change proto from UDP to TCP:
If you do switch the protocol to TCP, you will need to change the explicit-exit-notify directive "s value from 1 to 0, as this directive is only used by UDP.
Failing to do so while using TCP will cause errors when you start the OpenVPN service:
If you have no need to use a different port and protocol, it is best to leave these two settings as their defaults.
(Optional) Point to Non-Default Credentials
If you selected a different name during the. / build-key-server command earlier, modify the cert and key lines that you see to point to the appropriate .crt and .key files.
If you used the default name, "server", this is already set correctly:
After going through and making whatever changes to your server "s OpenVPN configuration are required for your specific use case, you can begin making some changes to your server" s networking.
Step 6 - Adjusting the Server Networking Configuration
There are some aspects of the server "s networking configuration that need to be tweaked so that OpenVPN can correctly route traffic through the VPN.
The first of these is IP forwarding, a method for determining where IP traffic should be routed.
This is essential to the VPN functionality that your server will provide.
Adjust your server "s default IP forwarding setting by modifying the / etc / sysctl.conf file:
Inside, look for the commented line that sets net.ipv4.ip _ forward.
Remove the "#" character from the beginning of the line to uncomment this setting:
Save and close the file when you are finished.
To read the file and adjust the values for the current session, type:
If you followed the Ubuntu 18.04 initial server setup guide listed in the prerequisites, you should have a UFW firewall in place.
Regardless of whether you use the firewall to block unwanted traffic (which you almost always should do), for this guide you need a firewall to manipulate some of the traffic coming into the server.
Some of the firewall rules must be modified to enable masquerading, an iptables concept that provides on-the-fly dynamic network address translation (NAT) to correctly route client connections.
Before opening the firewall configuration file to add the masquerading rules, you must first find the public network interface of your machine.
To do this, type:
Your public interface is the string found within this command "s output that follows the word" dev ".
For example, this result shows the interface named wlp11s0, which is highlighted below:
When you have the interface associated with your default route, open the / etc / ufw / before.rules file to add the relevant configuration:
UFW rules are typically added using the ufw command.
Rules listed in the before.rules file, though, are read and put into place before the conventional UFW rules are loaded.
Towards the top of the file, add the highlighted lines below.
This will set the default policy for the POSTROUTING chain in the nat table and masquerade any traffic coming from the VPN.
Remember to replace < ^ > wlp11s0 < ^ > in the -A POSTROUTING line below with the interface you found in the above command:
Next, you need to tell UFW to allow forwarded packets by default as well.
To do this, open the / etc / default / ufw file:
Inside, find the DEFAULT _ FORWARD _ POLICY directive and change the value from DROP to ACCEPT:
Next, adjust the firewall itself to allow traffic to OpenVPN.
If you did not change the port and protocol in the / etc / openvpn / server.conf file, you will need to open up UDP traffic to port 1194.
If you modified the port and / or protocol, substitute the values you selected here.
In case you forgot to add the SSH port when following the prerequisite tutorial, add it here as well:
After adding those rules, disable and re-enable UFW to restart it and load the changes from all of the files you've modified:
Your server is now configured to correctly handle OpenVPN traffic.
Step 7 - Starting and Enabling the OpenVPN Service
You're finally ready to start the OpenVPN service on your server.
This is done using the systemd utility systemctl.
Start the OpenVPN server by specifying your configuration file name as an instance variable after the systemd unit file name.
The configuration file for your server is called / etc / openvpn / < ^ > server < ^ > .conf, so add < ^ > @ server < ^ > to end of your unit file when calling it:
Double-check that the service has started successfully by typing:
If everything went well, your output will look something like this:
You can also check that the OpenVPN tun0 interface is available by typing:
This will output a configured interface:
After starting the service, enable it so that it starts automatically at boot:
Your OpenVPN service is now up and running.
Before you can start using it, though, you must first create a configuration file for the client machine.
This tutorial already went over how to create certificate / key pairs for clients, and in the next step we will demonstrate how to create an infrastructure that will generate client configuration files easily.
Step 8 - Creating the Client Configuration Infrastructure
Creating configuration files for OpenVPN clients can be somewhat involved, as every client must have its own config and each must align with the settings outlined in the server "s configuration file.
Rather than writing a single configuration file that can only be used on one client, this step outlines a process for building a client configuration infrastructure which you can use to generate config files on-the-fly.
You will first create a "base" configuration file then build a script which will allow you to generate unique client config files, certificates, and keys as needed.
Get started by creating a new directory where you will store client configuration files within the client-configs directory you created earlier:
Next, copy an example client configuration file into the client-configs directory to use as your base configuration:
Open this new file in your text editor:
Inside, locate the remote directive.
This points the client to your OpenVPN server address - the public IP address of your OpenVPN server.
If you decided to change the port that the OpenVPN server is listening on, you will also need to change 1194 to the port you selected:
Be sure that the protocol matches the value you are using in the server configuration:
Next, uncomment the user and group directives by removing the "; "at the beginning of each line:
Find the directives that set the ca, cert, and key.
Comment out these directives since you will add the certs and keys within the file itself shortly:
Similarly, comment out the tls-auth directive, as you will add ta.key directly into the client configuration file:
Mirror the cipher and auth settings that you set in the / etc / openvpn / server.conf file:
Next, add the key-direction directive somewhere in the file.
You must set this to "1" for the VPN to function correctly on the client machine:
Finally, add a few commented out lines.
Although you can include these directives in every client configuration file, you only need to enable them for Linux clients that ship with an / etc / openvpn / update-resolv-conf file.
This script uses the resolvconf utility to update DNS information for Linux clients.
If your client is running Linux and has an / etc / openvpn / update-resolv-conf file, uncomment these lines from the client "s configuration file after it has been generated.
Next, create a simple script that will compile your base configuration with the relevant certificate, key, and encryption files and then place the generated configuration in the ~ / client-configs / files directory.
Open a new file called make _ config.sh within the ~ / client-configs directory:
Inside, add the following content:
Before moving on, be sure to mark this file as executable by typing:
This script will make a copy of the base.conf file you made, collect all the certificate and key files you "ve created for your client, extract their contents, append them to the copy of the base configuration file, and export all of this content into a new client configuration file.
This means that, rather than having to manage the client "s configuration, certificate, and key files separately, all the required information is stored in one place.
The benefit of this is that if you ever need to add a client in the future, you can just run this script to quickly create the config file and ensure that all the important information is stored in a single, easy-to-access location.
Please note that any time you add a new client, you will need to generate new keys and certificates for it before you can run this script and generate its configuration file.
You will get some practice using this script in the next step.
Step 9 - Generating Client Configurations
If you followed along with the guide, you created a client certificate and key named client1.crt and client1.key, respectively, in Step 4. You can generate a config file for these credentials by moving into your ~ / client-configs directory and running the script you made at the end of the previous step:
This will create a file named client1.ovpn in your ~ / client-configs / files directory:
You need to transfer this file to the device you plan to use as the client.
For instance, this could be your local computer or a mobile device.
While the exact applications used to accomplish this transfer will depend on your device's operating system and your personal preferences, a dependable and secure method is to use SFTP (SSH file transfer protocol) or SCP (Secure Copy) on the backend.
This will transport your client's VPN authentication files over an encrypted connection.
Here is an example SFTP command using the < ^ > client1.ovpn < ^ > example which you can run from your local computer (macOS or Linux).
It places the .ovpn file in your home directory:
Here are several tools and tutorials for securely transferring files from the server to a local computer:
WinSCP
How To Use SFTP to Securely Transfer Files with a Remote Server
How To Use Filezilla to Transfer and Manage Files Securely on your VPS
Step 10 - Installing the Client Configuration
This section covers how to install a client VPN profile on Windows, macOS, Linux, iOS, and Android.
None of these client instructions are dependent on one another, so feel free to skip to whichever is applicable to your device.
The OpenVPN connection will have the same name as whatever you called the .ovpn file.
In regards to this tutorial, this means that the connection is named client1.ovpn, aligning with the first client file you generated.
Windows
Installing
Download the OpenVPN client application for Windows from OpenVPN's Downloads page.
Choose the appropriate installer version for your version of Windows.
< $> note label Note OpenVPN needs administrative privileges to install.
After installing OpenVPN, copy the .ovpn file to:
When you launch OpenVPN, it will automatically see the profile and make it available.
You must run OpenVPN as an administrator each time it's used, even by administrative accounts.
To do this without having to right-click and select Run as administrator every time you use the VPN, you must preset this from an administrative account.
This also means that standard users will need to enter the administrator's password to use OpenVPN.
On the other hand, standard users can't properly connect to the server unless the OpenVPN application on the client has admin rights, so the elevated privileges are necessary.
To set the OpenVPN application to always run as an administrator, right-click on its shortcut icon and go to Properties.
At the bottom of the Compatibility tab, click the button to Change settings for all users.
In the new window, check Run this program as an administrator.
Connecting
Each time you launch the OpenVPN GUI, Windows will ask if you want to allow the program to make changes to your computer.
Click Yes.
Launching the OpenVPN client application only puts the applet in the system tray so that you can connect and disconnect the VPN as needed; it does not actually make the VPN connection.
Once OpenVPN is started, initiate a connection by going into the system tray applet and right-clicking on the OpenVPN applet icon.
This opens the context menu.
Select client1 at the top of the menu (that's your client1.ovpn profile) and choose Connect.
A status window will open showing the log output while the connection is established, and a message will show once the client is connected.
Disconnect from the VPN the same way: Go into the system tray applet, right-click the OpenVPN applet icon, select the client profile and click Disconnect.
macOS
Tunnelblick is a free, open source OpenVPN client for macOS.
You can download the latest disk image from the Tunnelblick Downloads page.
Double-click the downloaded .dmg file and follow the prompts to install.
Towards the end of the installation process, Tunnelblick will ask if you have any configuration files.
Answer I have configuration files and let Tunnelblick finish.
Open a Finder window and double-click client1.ovpn.
Tunnelblick will install the client profile.
Administrative privileges are required.
Launch Tunnelblick by double-clicking the Tunnelblick icon in the Applications folder.
Once Tunnelblick has been launched, there will be a Tunnelblick icon in the menu bar at the top right of the screen for controlling connections.
Click on the icon, and then the Connect client1 menu item to initiate the VPN connection.
Linux
If you are using Linux, there are a variety of tools that you can use depending on your distribution.
Your desktop environment or window manager might also include connection utilities.
The most universal way of connecting, however, is to just use the OpenVPN software.
On Ubuntu or Debian, you can install it just as you did on the server by typing:
On CentOS you can enable the EPEL repositories and then install it by typing:
Configuring
Check to see if your distribution includes an / etc / openvpn / update-resolv-conf script:
Next, edit the OpenVPN client configuration file you transfered:
If you were able to find an update-resolv-conf file, uncomment the three lines you added to adjust the DNS settings:
If you are using CentOS, change the group directive from nogroup to nobody to match the distribution's available groups:
Save and close the file.
Now, you can connect to the VPN by just pointing the openvpn command to the client configuration file:
This should connect you to your VPN.
iOS
From the iTunes App Store, search for and install OpenVPN Connect, the official iOS OpenVPN client application.
To transfer your iOS client configuration onto the device, connect it directly to a computer.
The process of completing the transfer with iTunes is outlined here.
Open iTunes on the computer and click on iPhone > apps.
Scroll down to the bottom to the File Sharing section and click the OpenVPN app. The blank window to the right, OpenVPN Documents, is for sharing files.
Drag the .ovpn file to the OpenVPN Documents window.
iTunes showing the VPN profile ready to load on the iPhone
Now launch the OpenVPN app on the iPhone.
You will receive a notification that a new profile is ready to import.
Tap the green plus sign to import it.
The OpenVPN iOS app showing new profile ready to import
OpenVPN is now ready to use with the new profile.
Start the connection by sliding the Connect button to the On position.
Disconnect by sliding the same button to Off.
< $> note label Note The VPN switch under Settings cannot be used to connect to the VPN.
If you try, you will receive a notice to only connect using the OpenVPN app. < $>
The OpenVPN iOS app connected to the VPN
Android
Open the Google Play Store.
Search for and install Android OpenVPN Connect, the official Android OpenVPN client application.
You can transfer the .ovpn profile by connecting the Android device to your computer by USB and copying the file over.
Alternatively, if you have an SD card reader, you can remove the device's SD card, copy the profile onto it and then insert the card back into the Android device.
Start the OpenVPN app and tap the menu to import the profile.
The OpenVPN Android app profile import menu selection
Then navigate to the location of the saved profile (the screenshot uses / sdcard / Download /) and select the file.
The app will make a note that the profile was imported.
The OpenVPN Android app selecting VPN profile to import
To connect, simply tap the Connect button.
You'll be asked if you trust the OpenVPN application.
Choose OK to initiate the connection.
To disconnect from the VPN, go back to the OpenVPN app and choose Disconnect.
The OpenVPN Android app ready to connect to the VPN
Step 11 - Testing Your VPN Connection (Optional)
< $> note Note: This method for testing your VPN connection will only work if you opted to route all your traffic through the VPN in Step 5. < $>
Once everything is installed, a simple check confirms everything is working properly.
Without having a VPN connection enabled, open a browser and go to DNSLeakTest.
The site will return the IP address assigned by your internet service provider and as you appear to the rest of the world.
To check your DNS settings through the same website, click on Extended Test and it will tell you which DNS servers you are using.
Now connect the OpenVPN client to your Droplet's VPN and refresh the browser.
A completely different IP address (that of your VPN server) should now appear, and this is how you appear to the world.
Again, DNSLeakTest's Extended Test will check your DNS settings and confirm you are now using the DNS resolvers pushed by your VPN.
Step 12 - Revoking Client Certificates
Occasionally, you may need to revoke a client certificate to prevent further access to the OpenVPN server.
To do so, navigate to the EasyRSA directory on your CA machine:
Next, run the easyrsa script with the revoke option, followed by the client name you wish to revoke:
This will ask you to confirm the revocation by entering yes:
After confirming the action, the CA will fully revoke the client "s certificate.
However, your OpenVPN server currently has no way to check whether any clients "certificates have been revoked and the client will still have access to the VPN.
To correct this, create a certificate revocation list (CRL) on your CA machine:
This will generate a file called crl.pem.
Securely transfer this file to your OpenVPN server:
On your OpenVPN server, copy this file into your / etc / openvpn / directory:
Next, open the OpenVPN server configuration file:
At the bottom of the file, add the crl-verify option, which will instruct the OpenVPN server to check the certificate revocation list that we've created each time a connection attempt is made:
Finally, restart OpenVPN to implement the certificate revocation:
The client should no longer be able to successfully connect to the server using the old credential.
To revoke additional clients, follow this process:
Revoke the certificate with the. / easyrsa revoke < ^ > client _ name < ^ > command
Generate a new CRL
Transfer the new crl.pem file to your OpenVPN server and copy it to the / etc / openvpn directory to overwrite the old list.
Restart the OpenVPN service.
You can use this process to revoke any certificates that you've previously issued for your server.
You are now securely traversing the internet protecting your identity, location, and traffic from snoopers and censors.
To configure more clients, you only need to follow steps 4 and 9-11 for each additional device.
To revoke access to clients, just follow step 12.
How To Configure a Galera Cluster with MySQL on Ubuntu 18.04 Servers
3324
The author selected the Free and Open Source Fund to receive a donation as part of the Write for DOnations program.
Clustering adds high availability to your database by distributing changes to different servers.
In the event that one of the instances fails, others are quickly available to continue serving.
Clusters come in two general configurations, active-passive and active-active.
In active-passive clusters, all writes are done on a single active server and then copied to one or more passive servers that are poised to take over only in the event of an active server failure.
Some active-passive clusters also allow SELECT operations on passive nodes.
In an active-active cluster, every node is read-write and a change made to one is replicated to all.
MySQL is an open source relational database management system that is a popular choice for SQL databases.
Galera is a database clustering solution that enables you to set up multi-master clusters using synchronous replication.
Galera automatically handles keeping the data on different nodes in sync while allowing you to send read and write queries to any of the nodes in the cluster.
You can learn more about Galera at the official documentation page.
In this guide, you will configure an active-active MySQL Galera cluster.
For demonstration purposes, you will configure and test three Ubuntu 18.04 Droplets that will act as nodes in the cluster.
This amount of nodes is the smallest configurable cluster.
To follow along, you will need a DigitalOcean account, in addition to the following:
Three Ubuntu 18.04 Droplets with private networking enabled, each with a non-root user with sudo privileges.
For setting up private networking on the three Droplets, follow our Private Networking Quickstart guide.
For assistance setting up a non-root user with sudo privileges, follow our Initial Server Setup with Ubuntu 18.04 tutorial.
While the steps in this tutorial have been written for and tested against DigitalOcean Droplets, much of them will also be applicable to non-DigitalOcean servers with private networking enabled.
Step 1 - Adding the MySQL Repositories to All Servers
In this step, you will add the relevant MySQL and Galera package repositories to each of your three servers so that you will be able to install the right version of MySQL and Galera used in this tutorial.
< $> note Note: Codership, the company behind Galera Cluster, maintains the Galera repository, but be aware that not all external repositories are reliable.
Be sure to install only from trusted sources.
In this tutorial, you will use MySQL version 5.7.
You'll start by adding the external Ubuntu repository maintained by the Galera project to all three of your servers.
Once the repositories are updated on all three servers, you will be ready to install MySQL along with Galera.
First, on all three of your servers, add the Galera repository key with the apt-key command, which the APT package manager will use to verify that the package is authentic:
After a few seconds, you will receive the following output:
Once you have the trusted key in each server "s database, you can add the repositories.
To do so, create a new file called galera.list within the / etc / apt / sources.list.d / directory on each server:
In the text editor, add the following lines, which will make the appropriate repositories available to the APT package manager:
Save and close the files on each server (press CTRL + X, Y, then ENTER).
The Codership repositories are now available to all three of your servers.
However, it "s important that you instruct apt to prefer Codership" s repositories over others to ensure that it installs the patched versions of the software needed to create a Galera cluster.
To do this, create another new file called galera.pref within the / etc / apt / preferences.d / directory of each server:
Add the following lines to the text editor:
Save and close that file, then run the following command on each server in order to include package manifests from the new repositories:
Now that you have successfully added the package repository on all three of your servers, you're ready to install MySQL in the next section.
Step 2 - Installing MySQL on All Servers
In this step, you will install the MySQL package on your three servers.
Run the following command on all three servers to install a version of MySQL patched to work with Galera, as well as the Galera package.
You will be asked to confirm whether you would like to proceed with the installation.
Enter Y to continue with the installation.
During the installation, you will also be asked to set a password for the MySQL administrative user.
Set a strong password and press ENTER to continue.
Once MySQL is installed, you will disable the default AppArmor profile to ensure that Galera functions properly, as per the official Galera documentation.
AppArmor is a kernel module for Linux that provides access control functionality for services through security profiles.
Disable AppArmor by executing the following on each server:
This command adds a symbolic link of the MySQL profile to the disable directory, which disables the profile on boot.
Then, run the following command to remove the MySQL definition that has already been loaded in the kernel.
Once you have installed MySQL and disabled the AppArmor profile on your first server, repeat these steps for your other two servers.
Now that you have installed MySQL successfully on each of the three servers, you can proceed to the configuration step in the next section.
Step 3 - Configuring the First Node
In this step you will configure your first node.
Each node in the cluster needs to have a nearly identical configuration.
Because of this, you will do all of the configuration on your first machine, and then copy it to the other nodes.
By default, MySQL is configured to check the / etc / mysql / conf.d directory to get additional configuration settings from files ending in .cnf.
On your first server, create a file in this directory with all of your cluster-specific directives:
Add the following configuration into the file.
The configuration specifies different cluster options, details about the current server and the other servers in the cluster, and replication-related settings.
Note that the IP addresses in the configuration are the private addresses of your respective servers; replace the highlighted lines with the appropriate IP addresses.
The first section modifies or re-asserts MySQL settings that will allow the cluster to function correctly.
For example, Galera won "t work with MyISAM or similar non-transactional storage engines, and mysqld must not be bound to the IP address for localhost.
You can learn about the settings in more detail on the Galera Cluster system configuration page.
The "Galera Provider Configuration" section configures the MySQL components that provide a WriteSet replication API.
This means Galera in your case, since Galera is a wsrep (WriteSet Replication) provider.
You specify the general parameters to configure the initial replication environment.
This doesn't require any customization, but you can learn more about Galera configuration options in the documentation.
The "Galera Cluster Configuration" section defines the cluster, identifying the cluster members by IP address or resolvable domain name and creating a name for the cluster to ensure that members join the correct group.
You can change the wsrep _ cluster _ name to something more meaningful than test _ cluster or leave it as-is, but you must update wsrep _ cluster _ address with the private IP addresses of your three servers.
The Galera Synchronization Configuration section defines how the cluster will communicate and synchronize data between members.
This is used only for the state transfer that happens when a node comes online.
For your initial setup, you are using rsync, because it's commonly available and does what you'll need for now.
The Galera Node Configuration section clarifies the IP address and the name of the current server.
This is helpful when trying to diagnose problems in logs and for referencing each server in multiple ways.
The wsrep _ node _ address must match the address of the machine you're on, but you can choose any name you want in order to help you identify the node in log files.
When you are satisfied with your cluster configuration file, copy the contents into your clipboard, then save and close the file.
Now that you have configured your first node successfully, you can move on to configuring the remaining nodes in the next section.
Step 4 - Configuring the Remaining Nodes
In this step, you will configure the remaining two nodes.
On your second node, open the configuration file:
Paste in the configuration you copied from the first node, then update the Galera Node Configuration to use the IP address or resolvable domain name for the specific node you're setting up.
Finally, update its name, which you can set to whatever helps you identify the node in your log files:
Save and exit the file.
Once you have completed these steps, repeat them on the third node.
You're almost ready to bring up the cluster, but before you do, make sure that the appropriate ports are open in your firewall.
Step 5 - Opening the Firewall on Every Server
In this step, you will configure your firewall so that the ports required for inter-node communication are open.
On every server, check the status of the firewall by running:
In this case, only SSH is allowed through:
Since only SSH traffic is permitted in this case, you "ll need to add rules for MySQL and Galera traffic.
If you tried to start the cluster, it would fail because of these firewall rules.
Galera can make use of four ports:
3306 For MySQL client connections and State Snapshot Transfer that use the mysqldump method.
4567 For Galera Cluster replication traffic.
Multicast replication uses both UDP transport and TCP on this port.
4568 For Incremental State Transfer.
4444 For all other State Snapshot Transfer.
In this example, you "ll open all four ports while you do your setup.
Once you've confirmed that replication is working, you'd want to close any ports you're not actually using and restrict traffic to just servers in the cluster.
Open the ports with the following commands:
< $> note Note: Depending on what else is running on your servers, you might want to restrict access right away.
The UFW Essentials: Common Firewall Rules and Commands guide can help with this.
After you have configured your firewall on the first node, create the same firewall settings on the second and third node.
Now that you have configured the firewalls successfully, you're ready to start the cluster in the next step.
Step 6 - Starting the Cluster
In this step, you will start your MySQL Galera cluster.
But first, you will enable the MySQL systemd service, so that MySQL will start automatically whenever the server is rebooted.
Enable MySQL to Start on Boot on All Three Servers
Use the following command on all three servers to enable the MySQL systemd service:
You will see the following output, which shows that the service has been linked successfully to the startup services list:
Now that you've enabled mysql to start on boot on all of the servers, you're ready to proceed to bring the cluster up.
Bring Up the First Node
To bring up the first node, you'll need to use a special startup script.
The way you've configured your cluster, each node that comes online will try to connect to at least one other node specified in its galera.cnf file to get its initial state.
Without using the mysqld _ bootstrap script that allows systemd to pass the --wsrep-new-cluster parameter, a normal systemctl start mysql would fail because there are no nodes running for the first node to connect with.
Run the following on your first server:
This command will not display any output on successful execution.
When this script succeeds, the node is registered as part of the cluster, and you can see it with the following command:
After entering your password, you will see the following output, indicating that there is one node in the cluster:
On the remaining nodes, you can start mysql normally.
They will search for any member of the cluster list that is online, and when they find one, they will join the cluster.
Bring Up the Second Node
Now you can bring up the second node.
Start mysql:
No output will be displayed on successful execution.
You will see your cluster size increase as each node comes online:
You will see the following output indicating that the second node has joined the cluster and that there are two nodes in total.
Bring Up the Third Node
It's now time to bring up the third node.
Run the following command to find the cluster size:
You will see the following output, which indicates that the third node has joined the cluster and that the total number of nodes in the cluster is three.
At this point, the entire cluster is online and communicating successfully.
Next, you can ensure the working setup by testing replication in the following section.
Step 7 - Testing Replication
You've gone through the steps up to this point so that your cluster can perform replication from any node to any other node, known as active-active replication.
In this step, you will test and see if the replication is working as expected.
Write to the First Node
You'll start by making database changes on your first node.
The following commands will create a database called playground and a table inside of this database called equipment.
In the previous command, the CREATE DATABASE statement creates a database named playground.
The CREATE statement creates a table named equipment inside the playground database having an auto-incrementing identifier column called id and other columns.
The type column, quant column, and color column are defined to store the type, quantity, and color of the equipment respectively.
The INSERT statement inserts an entry of type slide, quantity 2 and color blue.
You now have one value in your table.
Read and Write on the Second Node
Next, look at the second node to verify that replication is working:
The data you entered on the first node will be visible here on the second, proving that replication is working:
From this same node, write data to the cluster:
Read and Write on the Third Node
From the third node, you can read all of this data by querying the table again:
You will see the following output showing the two rows:
Again, you can add another value from this node:
Read on the First Node
Back on the first node, you can verify that your data is available everywhere:
You will see the following output, which indicates that the rows are available on the first node.
You've now verified successfully that you can write to all of the nodes and that replication is being performed properly.
At this point, you have a working three-node Galera test cluster configured.
If you plan on using a Galera cluster in a production situation, it "s recommended that you begin with no fewer than five nodes.
Before production use, you may want to take a look at some of the other state snapshot transfer (sst) agents like xtrabackup, which allows you to set up new nodes quickly and without large interruptions to your active nodes.
This does not affect the actual replication, but is a concern when nodes are being initialized.
You might also be interested in other clustering solutions for MySQL, in which case you can check out our How To Create a Multi-Node MySQL Cluster on Ubuntu 18.04 tutorial.
If you are looking to try out a managed database solution, see our DigitalOcean Managed Databases documentation.
How To Containerize a Laravel Application for Development with Docker Compose on Ubuntu 18.04
3403
To containerize an application refers to the process of adapting an application and its components in order to be able to run it in lightweight environments known as containers.
Such environments are isolated and disposable, and can be leveraged for developing, testing, and deploying applications to production.
In this guide, we'll use Docker Compose to containerize a Laravel application for development.
When you're finished, you'll have a demo Laravel application running on three separate service containers:
An app service running PHP7.4-FPM;
A db service running MySQL 5.7;
An nginx service that uses the app service to parse PHP code before serving the Laravel application to the final user.
To allow for a streamlined development process and facilitate application debugging, we'll keep application files in sync by using shared volumes.
We'll also see how to use docker-compose exec commands to run Composer and Artisan on the app container.
Access to an Ubuntu 18.04 local machine or development server as a non-root user with sudo privileges.
If you're using a remote server, it's advisable to have an active firewall installed.
To set these up, please refer to our Initial Server Setup Guide for Ubuntu 18.04.
Docker installed on your server, following Steps 1 and 2 of How To Install and Use Docker on Ubuntu 18.04.
Docker Compose installed on your server, following Step 1 of How To Install Docker Compose on Ubuntu 18.04.
Step 1 - Obtaining the Demo Application
To get started, we'll fetch the demo Laravel application from its Github repository.
We're interested in the tutorial-01 branch, which contains the basic Laravel application we've created in the first guide of this series.
To obtain the application code that is compatible with this tutorial, download release tutorial-1.0.1 to your home directory with:
We'll need the unzip command to unpack the application code.
In case you haven't installed this package before, do so now with:
Now, unzip the contents of the application and rename the unpacked directory for easier access:
Navigate to the travellist-demo directory:
In the next step, we'll create a .env configuration file to set up the application.
Step 2 - Setting Up the Application's .env File
The Laravel configuration files are located in a directory called config, inside the application's root directory.
Additionally, a .env file is used to set up environment-dependent configuration, such as credentials and any information that might vary between deploys.
This file is not included in revision control.
< $> warning Warning: The environment configuration file contains sensitive information about your server, including database credentials and security keys.
For that reason, you should never share this file publicly.
The values contained in the .env file will take precedence over the values set in regular configuration files located at the config directory.
Each installation on a new environment requires a tailored environment file to define things such as database connection settings, debug options, application URL, among other items that may vary depending on which environment the application is running.
We'll now create a new .env file to customize the configuration options for the development environment we're setting up.
Laravel comes with an example.env file that we can copy to create our own:
Open this file using nano or your text editor of choice:
The current .env file from the travellist demo application contains settings to use a local MySQL database, with 127.0.0.1 as database host.
We need to update the DB _ HOST variable so that it points to the database service we will create in our Docker environment.
In this guide, we "ll call our database service db.
Go ahead and replace the listed value of DB _ HOST with the database service name:
Feel free to also change the database name, username, and password, if you wish.
These variables will be leveraged in a later step where we'll set up the docker-compose.yml file to configure our services.
Save the file when you're done editing.
If you used nano, you can do that by pressing Ctrl + x, then Y and Enter to confirm.
Step 3 - Setting Up the Application's Dockerfile
Although both our MySQL and Nginx services will be based on default images obtained from the Docker Hub, we still need to build a custom image for the application container.
We'll create a new Dockerfile for that.
Our travellist image will be based on the php: 7.4-fpm official PHP image from Docker Hub.
On top of that basic PHP-FPM environment, we'll install a few extra PHP modules and the Composer dependency management tool.
We'll also create a new system user; this is necessary to execute artisan and composer commands while developing the application.
The uid setting ensures that the user inside the container has the same uid as your system user on your host machine, where you're running Docker.
This way, any files created by these commands are replicated in the host with the correct permissions.
This also means that you'll be able to use your code editor of choice in the host machine to develop the application that is running inside containers.
Create a new Dockerfile with:
Copy the following contents to your Dockerfile:
Don't forget to save the file when you're done.
Our Dockerfile starts by defining the base image we're using: php: 7.4-fpm.
After installing system packages and PHP extensions, we install Composer by copying the composer executable from its latest official image to our own application image.
A new system user is then created and set up using the user and uid arguments that were declared at the beginning of the Dockerfile.
These values will be injected by Docker Compose at build time.
Finally, we set the default working dir as / var / www and change to the newly created user.
This will make sure you're connecting as a regular user, and that you're on the right directory, when running composer and artisan commands on the application container.
Step 4 - Setting Up Nginx Configuration and Database Dump Files
When creating development environments with Docker Compose, it is often necessary to share configuration or initialization files with service containers, in order to set up or bootstrap those services.
This practice facilitates making changes to configuration files to fine-tune your environment while you're developing the application.
We'll now set up a folder with files that will be used to configure and initialize our service containers.
To set up Nginx, we'll share a travellist.conf file that will configure how the application is served.
Create the docker-compose / nginx folder with:
Open a new file named travellist.conf within that directory:
Copy the following Nginx configuration to that file:
This file will configure Nginx to listen on port 80 and use index.php as default index page.
It will set the document root to / var / www / public, and then configure Nginx to use the app service on port 9000 to process * .php files.
Save and close the file when you're done editing.
To set up the MySQL database, we'll share a database dump that will be imported when the container is initialized.
This is a feature provided by the MySQL 5.7 image we'll be using on that container.
Create a new folder for your MySQL initialization files inside the docker-compose folder:
Open a new .sql file:
The following MySQL dump is based on the database we've set up in our Laravel on LEMP guide.
It will create a new table named places.
Then, it will populate the table with a set of sample places.
Add the following code to the file:
The places table contains three fields: id, name, and visited.
The visited field is a flag used to identify the places that are still to go.
Feel free to change the sample places or include new ones.
Save and close the file when you're done.
We've finished setting up the application's Dockerfile and the service configuration files.
Next, we'll set up Docker Compose to use these files when creating our services.
Step 5 - Creating a Multi-Container Environment with Docker Compose
Docker Compose enables you to create multi-container environments for applications running on Docker.
It uses service definitions to build fully customizable environments with multiple containers that can share networks and data volumes.
This allows for a seamless integration between application components.
To set up our service definitions, we'll create a new file called docker-compose.yml.
Typically, this file is located at the root of the application folder, and it defines your containerized environment, including the base images you will use to build your containers, and how your services will interact.
We'll define three different services in our docker-compose.yml file: app, db, and nginx.
The app service will build an image called travellist, based on the Dockerfile we've previously created.
The container defined by this service will run a php-fpm server to parse PHP code and send the results back to the nginx service, which will be running on a separate container.
The mysql service defines a container running a MySQL 5.7 server.
Our services will share a bridge network named travellist.
The application files will be synchronized on both the app and the nginx services via bind mounts.
Bind mounts are useful in development environments because they allow for a performant two-way sync between host machine and containers.
Create a new docker-compose.yml file at the root of the application folder:
A typical docker-compose.yml file starts with a version definition, followed by a services node, under which all services are defined.
Shared networks are usually defined at the bottom of that file.
To get started, copy this boilerplate code into your docker-compose.yml file:
We'll now edit the services node to include the app, db and nginx services.
The app Service
The app service will set up a container named travellist-app.
It builds a new Docker image based on a Dockerfile located in the same path as the docker-compose.yml file.
The new image will be saved locally under the name travellist.
Even though the document root being served as the application is located in the nginx container, we need the application files somewhere inside the app container as well, so we're able to execute command line tasks with the Laravel Artisan tool.
Copy the following service definition under your services node, inside the docker-compose.yml file:
These settings do the following:
build: This configuration tells Docker Compose to build a local image for the app service, using the specified path (context) and Dockerfile for instructions.
The arguments user and uid are injected into the Dockerfile to customize user creation commands at build time.
image: The name that will be used for the image being built.
container _ name: Sets up the container name for this service.
restart: Always restart, unless the service is stopped.
working _ dir: Sets the default directory for this service as / var / www.
volumes: Creates a shared volume that will synchronize contents from the current directory to / var / www inside the container.
Notice that this is not your document root, since that will live in the nginx container.
networks: Sets up this service to use a network named travellist.
The db Service
The db service uses a pre-built MySQL 5.7 image from Docker Hub.
Because Docker Compose automatically loads .env variable files located in the same directory as the docker-compose.yml file, we can obtain our database settings from the Laravel .env file we created in a previous step.
Include the following service definition in your services node, right after the app service:
image: Defines the Docker image that should be used for this container.
In this case, we're using a MySQL 5.7 image from Docker Hub.
container _ name: Sets up the container name for this service: travellist-db.
restart: Always restart this service, unless it is explicitly stopped.
environment: Defines environment variables in the new container.
We're using values obtained from the Laravel .env file to set up our MySQL service, which will automatically create a new database and user based on the provided environment variables.
volumes: Creates a volume to share a .sql database dump that will be used to initialize the application database.
The MySQL image will automatically import .sql files placed in the / docker-entrypoint-initdb.d directory inside the container.
The nginx Service
The nginx service uses a pre-built Nginx image on top of Alpine, a lightweight Linux distribution.
It creates a container named travellist-nginx, and it uses the ports definition to create a redirection from port 8000 on the host system to port 80 inside the container.
Include the following service definition in your services node, right after the db service:
In this case, we're using the Alpine Nginx 1.17 image.
container _ name: Sets up the container name for this service: travellist-nginx.
ports: Sets up a port redirection that will allow external access via port 8000 to the web server running on port 80 inside the container.
volumes: Creates two shared volumes.
The first one will synchronize contents from the current directory to / var / www inside the container.
This way, when you make local changes to the application files, they will be quickly reflected in the application being served by Nginx inside the container.
The second volume will make sure our Nginx configuration file, located at docker-compose / nginx / travellist.conf, is copied to the container's Nginx configuration folder.
Finished docker-compose.yml File
This is how our finished docker-compose.yml file looks like:
Make sure you save the file when you're done.
Step 6 - Running the Application with Docker Compose
We'll now use docker-compose commands to build the application image and run the services we specified in our setup.
Build the app image with the following command:
This command might take a few minutes to complete.
You'll see output similar to this:
When the build is finished, you can run the environment in background mode with:
This will run your containers in the background.
To show information about the state of your active services, run:
You'll see output like this:
Your environment is now up and running, but we still need to execute a couple commands to finish setting up the application.
You can use the docker-compose exec command to execute commands in the service containers, such as an ls -l to show detailed information about files in the application directory:
We'll now run composer install to install the application dependencies:
The last thing we need to do before testing the application is to generate a unique application key with the artisan Laravel command-line tool.
This key is used to encrypt user sessions and other sensitive data:
Now go to your browser and access your server's domain name or IP address on port 8000:
You'll see a page like this:
Demo Laravel Application
You can use the logs command to check the logs generated by your services:
If you want to pause your Docker Compose environment while keeping the state of all its services, run:
You can then resume your services with:
To shut down your Docker Compose environment and remove all of its containers, networks, and volumes, run:
For an overview of all Docker Compose commands, please check the Docker Compose command-line reference.
In this guide, we've set up a Docker environment with three containers using Docker Compose to define our infrastructure in a YAML file.
From this point on, you can work on your Laravel application without needing to install and set up a local web server for development and testing.
Moreover, you'll be working with a disposable environment that can be easily replicated and distributed, which can be helpful while developing your application and also when moving towards a production environment.
How To Install Apache Kafka on Debian 10
3330
Apache Kafka is a popular distributed message broker designed to handle large volumes of real-time data. A Kafka cluster is highly scalable and fault-tolerant, and also has a much higher throughput compared to other message brokers such as ActiveMQ and RabbitMQ.
Though it is generally used as a publish / subscribe messaging system, a lot of organizations also use it for log aggregation because it offers persistent storage for published messages.
A publish / subscribe messaging system allows one or more producers to publish messages without considering the number of consumers or how they will process the messages.
Subscribed clients are notified automatically about updates and the creation of new messages.
This system is more efficient and scalable than systems where clients poll periodically to determine if new messages are available.
In this tutorial, you will install and configure Apache Kafka 2.1.1 securely on a Debian 10 server, then test your setup by producing and consuming a Hello World message.
You will then optionally install KafkaT to monitor Kafka and set up a Kafka multi-node cluster.
To follow along, you will need:
One Debian 10 server with at least 4GB of RAM and a non-root user with sudo privileges.
Follow the steps specified in our Initial Server Setup guide for Debian 10 if you do not have a non-root user set up.
OpenJDK 11 installed on your server.
To install this version, follow the instructions in How To Install Java with Apt on Debian 10 on installing specific versions of OpenJDK.
Kafka is written in Java, so it requires a JVM.
< $> note Note: Installations without 4GB of RAM may cause the Kafka service to fail, with the Java virtual machine (JVM) throwing an Out Of Memory exception during startup.
Step 1 - Creating a User for Kafka
Since Kafka can handle requests over a network, it is a best practice to create a dedicated user for it. This minimizes damage to your Debian machine should the Kafka server be compromised.
You will create the dedicated user kafka in this step.
Logged in as your non-root sudo user, create a user called kafka with the useradd command:
The -m flag ensures that a home directory will be created for the user.
This home directory, / home / kafka, will act as your workspace directory for executing commands later on.
Set the password using passwd:
Enter the password you wish to use for this user.
Next, add the kafka user to the sudo group with the adduser command, so that it has the privileges required to install Kafka's dependencies:
Your kafka user is now ready.
Log into this account using su:
Now that you've created the Kafka-specific user, you can move on to downloading and extracting the Kafka binaries.
Step 2 - Downloading and Extracting the Kafka Binaries
In this step, you will download and extract the Kafka binaries into dedicated folders in your kafka user's home directory.
To start, create a directory in / home / kafka called Downloads to store your downloads:
Next, install curl using apt-get so that you'll be able to download remote files:
When prompted, type Y to confirm the curl download.
Once curl is installed, use it to download the Kafka binaries:
Create a directory called kafka and change to this directory.
This will be the base directory of the Kafka installation:
Extract the archive you downloaded using the tar command:
You specified the --strip 1 flag to ensure that the archive's contents are extracted in ~ / kafka / itself and not in another directory inside of it, such as ~ / kafka / kafka _ < ^ > 2.12-2.1.1 < ^ > /.
Now that you've downloaded and extracted the binaries successfully, you can move on to configuring Kafka to allow for topic deletion.
Step 3 - Configuring the Kafka Server
Kafka's default behavior will not allow us to delete a topic, the category, group, or feed name to which messages can be published.
To modify this, you will edit the configuration file.
Kafka's configuration options are specified in server.properties.
Open this file with nano or your favorite editor:
Let's add a setting that will allow us to delete Kafka topics.
Add the following highlighted line to the bottom of the file:
Save the file, and exit nano.
Now that you've configured Kafka, you can create systemd unit files for running and enabling Kafka on startup.
Step 4 - Creating Systemd Unit Files and Starting the Kafka Server
In this section, you will create systemd unit files for the Kafka service.
This will help you perform common service actions such as starting, stopping, and restarting Kafka in a manner consistent with other Linux services.
ZooKeeper is a service that Kafka uses to manage its cluster state and configurations.
It is commonly used in distributed systems as an integral component.
In this tutorial, you will use Zookeeper to manage these aspects of Kafka.
If you would like to know more about it, visit the official ZooKeeper docs.
First, create the unit file for zookeeper:
Enter the following unit definition into the file:
The [Unit] section specifies that ZooKeeper requires networking and for the filesystem to be ready before it can start.
The [Service] section specifies that systemd should use the zookeeper-server-start.sh and zookeeper-server-stop.sh shell files for starting and stopping the service.
It also specifies that ZooKeeper should be restarted automatically if it exits abnormally.
Next, create the systemd service file for kafka:
The [Unit] section specifies that this unit file depends on zookeeper.service.
This will ensure that zookeeper gets started automatically when the kafka service starts.
The [Service] section specifies that systemd should use the kafka-server-start.sh and kafka-server-stop.sh shell files for starting and stopping the service.
It also specifies that Kafka should be restarted automatically if it exits abnormally.
Now that the units have been defined, start Kafka with the following command:
To ensure that the server has started successfully, check the journal logs for the kafka unit:
You will see output similar to the following:
You now have a Kafka server listening on port 9092, which is the default port for Kafka.
You have started the kafka service, but if you were to reboot your server, it would not yet be started automatically.
To enable kafka on server boot, run:
Now that you've started and enabled the services, it's time to check the installation.
Step 5 - Testing the Installation
Let's publish and consume a Hello World message to make sure the Kafka server is behaving correctly.
Publishing messages in Kafka requires:
A producer, which enables the publication of records and data to topics.
A consumer, which reads messages and data from topics.
First, create a topic named TutorialTopic by typing:
You can create a producer from the command line using the kafka-console-producer.sh script.
It expects the Kafka server's hostname, port, and a topic name as arguments.
Publish the string Hello, World to the TutorialTopic topic by typing:
The --broker-list flag determines the list of message brokers to send the message to, in this case localhost: 9092. --topic designates the topic as TutorialTopic.
Next, you can create a Kafka consumer using the kafka-console-consumer.sh script.
It expects the ZooKeeper server's hostname and port and a topic name as arguments.
The following command consumes messages from TutorialTopic.
Note the use of the --from-beginning flag, which allows the consumption of messages that were published before the consumer was started:
--bootstrap-server provides a list of ingresses into the Kafka cluster.
In this case, you are using localhost: 9092.
You will see Hello, World in your terminal:
The script will continue to run, waiting for more messages to be published to the topic.
Feel free to open a new terminal and start a producer to publish a few more messages.
You should be able to see them all in the consumer's output.
If you'd like to learn more about how to use Kafka, see the official Kafka documentation.
When you are done testing, press CTRL + C to stop the consumer script.
Now that you have tested the installation, you can move on to installing KafkaT in order to better administer your Kafka cluster.
Step 6 - Installing KafkaT (Optional)
KafkaT is a tool from Airbnb that makes it easier for you to view details about your Kafka cluster and perform certain administrative tasks from the command line.
Because it is a Ruby gem, you will need Ruby to use it. You will also need the build-essential package to be able to build the other gems it depends on.
Install them using apt:
You can now install KafkaT using the gem command:
The CFLAGS = -Wno-error = format-overflow option disables format overflow warnings and is required for the ZooKeeper gem, which is a dependency of KafkaT.
KafkaT uses .kafkatcfg as the configuration file to determine the installation and log directories of your Kafka server.
It should also have an entry pointing KafkaT to your ZooKeeper instance.
Create a new file called .kafkatcfg:
Add the following lines to specify the required information about your Kafka server and Zookeeper instance:
You are now ready to use KafkaT.
For a start, here's how you would use it to view details about all Kafka partitions:
You will see the following output:
This output shows TutorialTopic, as well as _ _ consumer _ offsets, an internal topic used by Kafka for storing client-related information.
You can safely ignore lines starting with _ _ consumer _ offsets.
To learn more about KafkaT, refer to its GitHub repository.
Now that you have installed KafkaT, you can optionally set up Kafka on a cluster of Debian 10 servers to make a multi-node cluster.
Step 7 - Setting Up a Multi-Node Cluster (Optional)
If you want to create a multi-broker cluster using more Debian 10 servers, repeat Step 1, Step 4, and Step 5 on each of the new machines.
Additionally, make the following changes in the ~ / kafka / config / server.properties file for each:
Change the value of the broker.id property such that it is unique throughout the cluster.
This property uniquely identifies each server in the cluster and can have any string as its value.
For example, "server1", "server2", etc., would be useful as identifiers.
Change the value of the zookeeper.connect property such that all nodes point to the same ZooKeeper instance.
This property specifies the ZooKeeper instance's address and follows the < HOSTNAME / IP _ ADDRESS >: < PORT > format.
For this tutorial, you would use < ^ > your _ first _ server _ IP < ^ >: 2181, replacing < ^ > your _ first _ server _ IP < ^ > with the IP address of the Debian 10 server you already set up.
If you want to have multiple ZooKeeper instances for your cluster, the value of the zookeeper.connect property on each node should be an identical, comma-separated string listing the IP addresses and port numbers of all the ZooKeeper instances.
< $> note Note: If you have a firewall activated on the Debian 10 server with Zookeeper installed, make sure to open up port 2181 to allow for incoming requests from the other nodes in the cluster.
Step 8 - Restricting the Kafka User
Now that all of the installations are done, you can remove the kafka user's admin privileges.
Before you do so, log out and log back in as any other non-root sudo user.
If you are still running the same shell session you started this tutorial with, simply type exit.
Remove the kafka user from the sudo group:
To further improve your Kafka server's security, lock the kafka user's password using the passwd command.
This makes sure that nobody can directly log into the server using this account:
At this point, only root or a sudo user can log in as kafka by typing in the following command:
In the future, if you want to unlock it, use passwd with the -u option:
You have now successfully restricted the kafka user's admin privileges.
You now have Apache Kafka running securely on your Debian server.
You can make use of it in your projects by creating Kafka producers and consumers using Kafka clients, which are available for most programming languages.
To learn more about Kafka, you can also consult the Apache Kafka documentation.
How To Package and Publish a Snap Application on Ubuntu 18.04
3857
The author selected the Electronic Frontier Foundation to receive a donation as part of the Write for DOnations program.
One of the largest challenges in application development is the final step of distributing the finished product to your users or customers.
Many existing application deployment methodologies lack user-friendliness and security, or do not provide methods for automatically updating an application once it has been installed.
Snap is a modern application packaging format with powerful sandboxing and security features, including file system isolation, automatic updates, and integrated dependency management.
Snap applications, known as Snaps, can be downloaded and installed using a command-line program, much like apt or yum.
Ubuntu comes with Snap pre-installed, meaning that there is a wide audience for Snap applications.
In this tutorial, you will create a Snap application and publish it on the Snap Store.
To complete this tutorial, you will need:
One Ubuntu 18.04 server set up by following the Initial Server Setup with Ubuntu 18.04, including a sudo non-root user.
An application that you wish to package and release as a Snap.
This may be a complex application that you created, a common open-source project, or a simple "Hello, world!"
program.
If you don't already have an application, Step 1 of this tutorial will cover how you can create a Hello World program in Go.
An account on the Snapcraft Developer Dashboard.
Once you have these ready, log in to your server as your non-root user to begin.
Step 1 - Getting Your Application Ready for Packaging
Firstly, you'll prepare your application for packaging as a Snap application by ensuring that everything required is present in a single directory.
Start by creating a new directory for your Snap and moving into it:
Next, if you already have an application, put a complete copy of the source code for your application into the directory that you just created.
The process here will vary significantly depending on the exact application that you're packaging, however in the case that the source code is stored in a Git repository, you can git init a repository in the directory and pull down all of the relevant code.
If you don't yet have an application that you'd like to package, you may create a "Hello World" program to use instead.
If you would like more context on writing this program with Go, check out the How to Write Your First Program in Go tutorial.
You can do this by first creating a new Go file and opening it using your preferred text editor:
Next, add the following code to the file:
Then save and exit the file.
If you don't have Go installed, you can install it using the following command:
Once Go is installed, you can run your new program to check that it is working:
You'll see the following output:
You've prepared your application for packaging as a Snap.
Next, you will install the software required to begin the packaging process.
Step 2 - Installing Snapcraft
In this step, you'll download and install Snapcraft, which is the name of the official Snap application packaging tool.
Snapcraft is available from the Snap Store, which is built into Ubuntu by default.
This means that you can install Snapcraft from the command-line using the snap command.
The snap command is equivalent to the apt command, but you can use it to install software from the Snap Store, rather than packages from the Apt repositories.
In order to install Snapcraft, run the following command:
You use the --classic command argument so that Snapcraft installs without the strict sandboxing features that Snaps normally use.
Snapcraft requires this argument as it needs more privileged access to your system to reliably package applications.
Once you've installed Snapcraft, you'll see the following:
Finally, you can double-check the Snapcraft installation by running:
This will display something similar to:
Now that you've installed Snapcraft, you can begin to define the configuration and metadata for your Snap application.
Step 3 - Defining the Configuration and Metadata for Your Snap
In this step, you will begin to define the configuration, structure, and metadata for your Snap application.
Begin by ensuring that you are still working in your Snap application directory:
Next, create and edit the snapcraft.yaml file using your preferred text editor:
You'll use the snapcraft.yaml file to store all of the configuration for your Snap application, including the name, description, and version, as well as settings related to dependency management and sandboxing.
Begin by defining the name, summary, description, and version number for your application:
The name of your Snap needs to be unique if you wish to publish it on the Snap Store - search for other applications with the same name to make sure that it isn't already taken.
Next, you can define the command (s) that you wish to associate with your application.
This will allow your Snap to be used directly from the Bash command-line as a normal command.
Add the following to your snapcraft.yaml file:
your-snap-command is the name of the command that you want to define.
For example, you may wish to use the command helloworld to run your Hello World program.
You use command: < ^ > your-snap < ^ > to tell Snapcraft what to do when the application command is run.
In the case of the Hello World program, you would use the value helloworld to reference the helloworld.go file, which will allow Snapcraft to run your program successfully.
This results in the following example configuration:
If the command name matches the Snap name exactly, you'll be able to run it directly from the command-line.
If the command does not match the Snap name, the command will be automatically prefixed with the name of the Snap.
For example, helloworld.command1.
Finally, you can define the parts that make up your Snap application.
Snap applications are made up of multiple parts, which are all of the components that make up your application.
In many cases, there is only one part, which is the application itself.
Each part has an associated plugin.
For example, for components of your application written in Ruby, the ruby plugin is used, and for components written in Go, the go plugin is used.
You can use the Snapcraft list-plugins command to identify the correct plugin (s) for your application:
This will output a list similar to the following:
The most common plugins are those for common programming languages, such as Go, Rust, Ruby, or Python.
Once you have identified the correct plugins for your application, you can begin to add the parts configuration to your snapcraft.yaml file:
You use the source configuration parameter to specify the relative path to the source code for your application.
Usually this will be the same directory as the snapcraft.yaml file itself, so the source value is a single dot (.).
< $> note Note: If your application component has any dependencies that are required for either building or running it, you can specify these using the build-packages and stage-packages attributes.
The specified dependency names will then be automatically fetched from the default package manager for your system.
For example:
Some Snapcraft plugins have their own specific options that may be required for your application, so it's worthwhile reviewing the relevant manual pages for your plugin:
In the case of Go applications, you would also specify the go-importpath.
For the Hello World configuration, this results in the following example configuration:
You can leave your snapcraft.yaml file open to add further configuration in the next step.
You've defined the base configuration for your Snap application.
Next, you'll configure the security and sandboxing aspects of your application.
Step 4 - Securing Your Snap Application
Snap applications are designed to run within a sandboxed environment, so in this step you'll configure sandboxing for your Snap.
Firstly, you'll need to enable sandboxing for your application, known within Snapcraft as confinement.
This will enable sandboxing for your application, preventing it from accessing the internet, other running Snaps, or the host system itself.
However, in most cases, applications do need to be able to communicate outside of their sandbox like when they need to access the internet or read / write to the file system.
These permissions, known within Snapcraft as interfaces, can be granted to your Snap application using Plugs.
Using Plugs, you can have fine-grain control over the sandboxing for your application, to give it the access that it requires and nothing more (principle of least privilege).
The exact interfaces that are required will vary depending on your application.
Some of the most common interfaces are:
audio-playback - Allows audio output / playing sounds.
audio-record - Allows audio input / recording.
camera - Allows access to connected webcams.
home - Allows access to non-hidden files within your home directory.
network - Allows access to the network / internet.
network-bind - Allows binding to ports to operate as a network service.
system-files - Allows access to the entire file system of the host machine.
The full list of available interfaces can be found within the Snapcraft documentation under Supported Interfaces.
Once you've identified all of the required interfaces for your application, you can begin to assign these to plugs within your snapcraft.yaml file.
The following example configuration will allow the application to access the network and users' home area:
Save and exit your file.
The name of the Plug should be a descriptive name to help users identify the purpose of the Plug.
You've enabled sandboxing for your Snap and configured some Plugs to grant limited access to system resources.
Next you'll finish building your Snap app.
Step 5 - Building and Testing Your Snap Application
Now that you've written all of the required configuration for your Snap, you can proceed with building it and testing the Snap package locally.
If you've been following along using a Hello World program as your application, your complete snapcraft.yaml file will now look similar to the following:
In order to build your Snap application, run the snapcraft command from within the directory for your Snap:
Snapcraft will then automatically launch a virtual machine (VM) and begin building your Snap.
Once complete, Snapcraft will exit and you will see something similar to the following:
You can now install your Snap locally in order to check that it is working:
The --dangerous command argument is required as you are installing a local Snap that has not been signed.
Once the installation process is complete, you can then run your Snap using its associated command.
In the case of the example Hello World program, the following output would be:
You can also view the sandboxing policy for your Snap to make sure that the assigned permissions have been properly granted:
This will output a list of Plugs and interfaces, similar to the following:
In this step, you built your Snap and installed it locally to test that it is working.
Next, you'll publish your Snap on the Snap Store.
Step 6 - Publishing Your Snap
Now that you've built and tested your Snap application, it's time to release it on the Snap Store.
Begin by logging in to your Snap Developer account using the Snapcraft command-line application:
Follow the prompts to enter your email address and password.
Next, you need to register the name of the application on the Snap Store:
Once you've registered the Snap name, you can push the built Snap package to the store:
You'll see output similar to the following:
Each time you push to the Snap store, the revision number is incremented, starting at one.
This is useful to help identify the various different builds of your Snap.
Finally, you can release your Snap to the public:
If this is the first time you've pushed to the Snap Store, the revision number will be 1. You can also choose between releasing to the stable, candidate, beta, and edge channels, if you have multiple versions of your application at different stages of development.
For example, the following command will release revision 1 of the Hello World Snap to the stable channel:
You can now search for your application on the Snap Store and install it on any of your devices.
Snapcraft store with HelloWorld App displayed from search results
In this final step, you uploaded your built Snap package to the Snap Store and released it to the public.
In this article you configured and built a Snap application, and then released it to the public via the Snap Store.
You now have the foundational knowledge required to maintain your application and building new ones.
If you wish to explore Snaps further, you may wish to browse the full Snap Store.
You may also wish to review the Snapcraft YAML Reference to understand more about it and identify additional attributes for your Snap configuration.
Finally, if you'd like to investigate Snap development further, you may enjoy reading about and implementing Snap Hooks, which allow Snaps to dynamically react to system changes such as upgrades or security policy adjustments.
How To Set Up Password Authentication with Apache on Ubuntu 18.04 Quickstart
3693
This tutorial will walk you through password-protecting assets on an Apache web server running on Ubuntu 18.04.
Completing these steps will provide your server with additional security so that unauthorized users cannot access certain parts of your page.
For a more detailed version of this tutorial, with more explanations of each step, please refer to How To Set Up Password Authentication with Apache on Ubuntu 18.04.
In order to complete this tutorial, you will need access to the following on an Ubuntu 18.04 server:
A sudo user on your server
An Apache2 web server
A site secured with SSL
Step 1 - Install the Apache Utilities Package
We "ll install a utility called htpasswd, part of the apache2-utils package to manage usernames and passwords with access to restricted content.
Step 2 - Create the Password File
We "ll create the first user as follows (replace '< ^ > first _ username < ^ > with username of your choice):
You will be asked to supply and confirm a password for the user.
Leave out the -c argument for any additional users you wish to add so you don "t overwrite the file:
Step 3 - Configure Apache Password Authentication
In this step, we need to configure Apache to check this file before serving our protected content.
We will do this by using the site "s virtual host file, but there is another option detailed in the longer tutorial if you don" t have access or prefer to use .htaccess files instead.
Open up the virtual host file that you wish to add a restriction to with a text editor such as nano:
Authentication is done on a per-directory basis.
In our example, we "ll restrict the entire document root, but you can modify this listing to only target a specific directory within the web space.
In this step, add the following highlighted lines in your file:
Check the configuration with the following command:
You can restart the server to implement your password policy, and then check the status of your server.
Step 4 - Confirm Password Authentication
To confirm that your content is protected, try to access your restricted content in a web browser.
You should be presented with a username and password prompt:
Apache2 password prompt
Related Tutorials
Here are links to more detailed guides related to this tutorial:
How To Set Up Password Authentication with Apache on Ubuntu 18.04
Getting Familiar with Important Apache Files and Directories in our Apache installation guide.
How To Set Up Apache Virtual Hosts on Ubuntu 16.04
How To Use the .htaccess File.
How To Set Up SSH Keys on CentOS 8
3697
SSH, or secure shell, is an encrypted protocol used to administer and communicate with servers.
When working with a CentOS server, chances are you will spend most of your time in a terminal session connected to your server through SSH.
In this guide, we'll focus on setting up SSH keys for a CentOS 8 server.
SSH keys provide a straightforward, secure method of logging into your server and are recommended for all users.
Step 1 - Creating the RSA Key Pair
The first step is to create a key pair on the client machine (usually your local computer):
By default, ssh-keygen will create a 2048-bit RSA key pair, which is secure enough for most use cases (you may optionally pass in the -b 4096 flag to create a larger 4096-bit key).
After entering the command, you should see the following prompt:
Press ENTER to save the key pair into the .ssh / subdirectory in your home directory, or specify an alternate path.
If you had previously generated an SSH key pair, you may see the following prompt:
If you choose to overwrite the key on disk, you will not be able to authenticate using the previous key anymore.
Be very careful when selecting yes, as this is a destructive process that cannot be reversed.
You should then see the following prompt:
Here you may optionally enter a secure passphrase, which is highly recommended.
A passphrase adds an additional layer of security to your key, to prevent unauthorized users from logging in.
You should then see the following output:
You now have a public and private key that you can use to authenticate.
The next step is to get the public key onto your server so that you can use SSH-key-based authentication to log in.
Step 2 - Copying the Public Key to Your CentOS Server
The quickest way to copy your public key to the CentOS host is to use a utility called ssh-copy-id.
This method is highly recommended if available.
If you do not have ssh-copy-id available to you on your client machine, you may use one of the two alternate methods that follow (copying via password-based SSH, or manually copying the key).
Copying your Public Key Using ssh-copy-id
The ssh-copy-id tool is included by default in many operating systems, so you may have it available on your local system.
For this method to work, you must already have password-based SSH access to your server.
To use the utility, you need only specify the remote host that you would like to connect to and the user account that you have password SSH access to.
This is the account to which your public SSH key will be copied:
You may see the following message:
This means that your local computer does not recognize the remote host.
This will happen the first time you connect to a new host.
Type yes and press ENTER to continue.
Next, the utility will scan your local account for the id _ rsa.pub key that we created earlier.
When it finds the key, it will prompt you for the password of the remote user's account:
Type in the password (your typing will not be displayed for security purposes) and press ENTER.
The utility will connect to the account on the remote host using the password you provided.
It will then copy the contents of your ~ / .ssh / id _ rsa.pub key into the remote account's ~ / .ssh / authorized _ keys file.
You should see the following output:
At this point, your id _ rsa.pub key has been uploaded to the remote account.
You can continue on to Step 3.
Copying Public Key Using SSH
If you do not have ssh-copy-id available, but you have password-based SSH access to an account on your server, you can upload your keys using a more conventional SSH method.
We can do this by using the cat command to read the contents of the public SSH key on our local computer and piping that through an SSH connection to the remote server.
On the other side, we can make sure that the ~ / .ssh directory exists and has the correct permissions under the account we "re using.
We can then output the content we piped over into a file called authorized _ keys within this directory.
We "ll use the > > redirect symbol to append the content instead of overwriting it. This will let us add keys without destroying any previously added keys.
The full command looks like this:
Afterwards, you should be prompted to enter the remote user account password:
After entering your password, the content of your id _ rsa.pub key will be copied to the end of the authorized _ keys file of the remote user's account.
Continue on to Step 3 if this was successful.
Copying Public Key Manually
If you do not have password-based SSH access to your server available, you will have to complete the above process manually.
We will manually append the content of your id _ rsa.pub file to the ~ / .ssh / authorized _ keys file on your remote machine.
To display the content of your id _ rsa.pub key, type this into your local computer:
You will see the key's content, which should look something like this:
Log in to your remote host using whichever method you have available.
Once you have access to your account on the remote server, you should make sure the ~ / .ssh directory exists.
This command will create the directory if necessary, or do nothing if it already exists:
Now, you can create or modify the authorized _ keys file within this directory.
You can add the contents of your id _ rsa.pub file to the end of the authorized _ keys file, creating it if necessary, using this command:
In the above command, substitute the < ^ > public _ key _ string < ^ > with the output from the cat ~ / .ssh / id _ rsa.pub command that you executed on your local system.
It should start with ssh-rsa AAAA....
Finally, we "ll ensure that the ~ / .ssh directory and authorized _ keys file have the appropriate permissions set:
This recursively removes all "group" and "other" permissions for the ~ / .ssh / directory.
If you "re using the root account to set up keys for a user account, it" s also important that the ~ / .ssh directory belongs to the user and not to root:
In this tutorial our user is named < ^ > sammy < ^ > but you should substitute the appropriate username into the above command.
We can now attempt key-based authentication with our CentOS server.
Step 3 - Logging In to Your CentOS Server Using SSH Keys
If you have successfully completed one of the procedures above, you should now be able to log into the remote host without the remote account's password.
The initial process is the same as with password-based authentication:
If this is your first time connecting to this host (if you used the last method above), you may see something like this:
Type yes and then press ENTER to continue.
If you did not supply a passphrase when creating your key pair in step 1, you will be logged in immediately.
If you supplied a passphrase you will be prompted to enter it now.
After authenticating, a new shell session should open for you with the configured account on the CentOS server.
If key-based authentication was successful, continue on to learn how to further secure your system by disabling your SSH server's password-based authentication.
Step 4 - Disabling Password Authentication on your Server
If you were able to login to your account using SSH without a password, you have successfully configured SSH-key-based authentication to your account.
However, your password-based authentication mechanism is still active, meaning that your server is still exposed to brute-force attacks.
Before completing the steps in this section, make sure that you either have SSH-key-based authentication configured for the root account on this server, or preferably, that you have SSH-key-based authentication configured for a non-root account on this server with sudo privileges.
This step will lock down password-based logins, so ensuring that you will still be able to get administrative access is crucial.
Once you've confirmed that your remote account has administrative privileges, log into your remote server with SSH keys, either as root or with an account with sudo privileges.
Then, open up the SSH daemon's configuration file:
Inside the file, search for a directive called PasswordAuthentication.
This may be commented out with a # hash.
Press i to put vi into insertion mode, and then uncomment the line and set the value to no. This will disable your ability to log in via SSH using account passwords:
When you are finished making changes, press ESC and then: wq to write the changes to the file and quit.
To actually implement these changes, we need to restart the sshd service:
As a precaution, open up a new terminal window and test that the SSH service is functioning correctly before closing your current session:
Once you have verified your SSH service is still working properly, you can safely close all current server sessions.
The SSH daemon on your CentOS server now only responds to SSH keys.
Password-based authentication has successfully been disabled.
You should now have SSH-key-based authentication configured on your server, allowing you to sign in without providing an account password.
If you'd like to learn more about working with SSH, take a look at our SSH Essentials Guide.
How To Set Up the code-server Cloud IDE Platform on Ubuntu 18.04 Quickstart
3341
code-server is Microsoft Visual Studio Code running on a remote server and accessible directly from your browser.
This means that you can use various devices, running different operating systems, and always have a consistent development environment on hand.
In this tutorial, you will set up the code-server cloud IDE platform on your Ubuntu 18.04 machine and expose it at your domain, secured with Let's Encrypt.
For a more detailed version of this tutorial, please refer to How To Set Up the code-server Cloud IDE Platform on Ubuntu 18.04.
A server running Ubuntu 18.04 with at least 2GB of RAM, root access, and a sudo, non-root account.
You can set this up by following the Initial Server Setup Guide for Ubuntu 18.04.
Nginx installed on your server.
For a guide on how to do this, complete Steps 1 to 4 of How To Install Nginx on Ubuntu 18.04.
A fully registered domain name to host code-server, pointed to your server.
This tutorial will use < ^ > code-server.your-domain < ^ > throughout.
You can purchase a domain name on Namecheap, get one for free on Freenom, or use the domain registrar of your choice.
Both of the following DNS records set up for your server.
You can follow this introduction to DigitalOcean DNS for details on how to add them.
An A record with < ^ > your-domain < ^ > pointing to your server's public IP address.
An A record with www. < ^ > your-domain < ^ > pointing to your server's public IP address.
Step 1 - Installing code-server
Create the directory to store all data for code-server:
Navigate to it:
Visit the Github releases page of code-server and pick the latest Linux build.
Download it using:
Unpack the archive:
Navigate to the directory containing the code-server executable:
To access the code-server executable across your system, copy it with:
Create a folder for code-server to store user data:
Create a systemd service, code-server.service, in the / lib / systemd / system directory:
Add the following lines:
--host 127.0.0.1 binds it to localhost.
--user-data-dir / var / lib / code-server sets its user data directory.
--auth password specifies that it should authenticate visitors with a password.
Remember to replace < ^ > your _ password < ^ > with your desired password.
Start the code-server service:
Check that it's started correctly:
You'll see output similar to:
Enable the code-server service to start automatically after a server reboot:
Step 2 - Exposing code-server
Now you will configure Nginx as a reverse proxy for code-server.
Create code-server.conf to store the configuration for exposing code-server at your domain:
Add the following lines to set up your server block with the necessary directives:
Replace < ^ > code-server.your _ domain < ^ > with your desired domain, then save and close the file.
To make this site configuration active, create a symlink of it:
Test the validity of the configuration:
For the configuration to take effect, restart Nginx:
Step 3 - Securing Your Domain
Now you'll secure your domain using a Let's Encrypt TLS certificate.
Add the Certbot package repository to your server:
Install Certbot and its Nginx plugin:
Configure ufw to accept encrypted traffic:
The output will be:
Reload it for the configuration to take effect:
The output will show:
Navigate to your code-server domain.
code-server login prompt
Enter your code-server password.
You'll see the interface exposed at your domain.
code-server GUI
To secure it, install a Let's Encrypt TLS certificate using Certbot.
Request a certificate for your domain with:
Provide an email address for urgent notices, accept the EFF's Terms of Service, and decide whether to redirect all HTTP traffic to HTTPS.
The output will be similar to this:
Certbot has successfully generated TLS certificates and applied them to the Nginx configuration for your domain.
You now have code-server, a versatile cloud IDE, installed on your Ubuntu 18.04 server, exposed at your domain and secured using Let's Encrypt certificates.
For further information, see the Visual Studio Code documentation for additional features and detailed instructions on other components of code-server.
How To Use Cron to Automate Tasks on CentOS 8
3753
A previous version of this tutorial was written by Shaun Lewis.
Cron is a time-based job scheduling daemon found in Unix-like operating systems, including Linux distributions.
Cron runs in the background and tasks scheduled with cron, referred to as "cron jobs," are executed automatically, making cron useful for automating maintenance-related tasks.
This guide provides an overview of how to schedule tasks using cron's special syntax.
It also goes over a few shortcuts one can use to make job schedules easier to write and understand.
To complete this guide, you'll need access to a computer running CentOS 8. This could be your local machine, a virtual machine, or a virtual private server.
Regardless of what kind of computer you use to follow this guide, it should have a non-root user with administrative privileges configured.
To set this up, follow our Initial Server Setup guide for CentOS 8.
Installing Cron
Almost every Linux distribution has some form of cron installed by default.
However, if you "re using a CentOS machine on which cron isn't installed, you can install it using dnf.
Before installing cron on a CentOS machine, update the computer's local package index:
Then install the cron daemon with the following command:
This command will prompt you to confirm that you want to install the crontabs package and its dependencies.
Do so by pressing y then ENTER.
This will install cron on your system, but you'll need to start the daemon manually.
You "ll also need to make sure it's set to run whenever the server boots.
You can perform both of these actions with the systemctl command.
To start the cron daemon, run the following command:
To set cron to run whenever the server starts up, type:
Following that, cron will be installed on your system and ready for you to start scheduling jobs.
Understanding How Cron Works
Cron jobs are recorded and managed in a special file known as a crontab.
Each user profile on the system can have their own crontab where they can schedule jobs, which is stored under / var / spool / cron /.
To schedule a job, you just need to open up your crontab for editing and add a task written in the form of a cron expression.
The syntax for cron expressions can be broken down into two elements: the schedule and the command to run.
The command can be virtually any command you would normally run on the command line.
The schedule component of the syntax is broken down into 5 different fields, which are written in the following order:
Field
Allowed Values
minute
0-59
hour
0-23
Day of the month
1-31
month
1-12 or JAN-DEC
Day of the week
0-6 or SUN-SAT
Together, tasks scheduled in a crontab are structured like this:
Here's a functional example of a cron expression.
This expression runs the command curl http: / / www.google.com every Tuesday at 5: 30 PM:
There are also a few special characters you can include in the schedule component of a cron expression to make scheduling easier:
*: In cron expressions, an asterisk is a wildcard variable that represents "all."
Thus, a task scheduled with * * * * *... will run every minute of every hour of every day of every month.
,: Commas break up scheduling values to form a list.
If you want to have a task run at the beginning and middle of every hour, rather than writing out two separate tasks (e.g., 0 * * * *... and 30 * * * *...), you could achieve the same functionality with one (0,30 * * * *...).
-: A hyphen represents a range of values in the schedule field.
Instead of having 30 separate scheduled tasks for a command you want to run for the first 30 minutes of every hour (as in 0 * * * *..., 1 * * * *..., 2 * * * *..., and so on), you could just schedule it as 0-29 * * * *....
/: You can use a forward slash with an asterisk to express a step value.
For example, instead of writing out eight separate separate cron tasks to run a command every three hours (as in, 0 0 * * *..., 0 3 * * *..., 0 6 * * *..., and so on), you could schedule it to run like this: 0 * / 3 * * *....
< $> note Note: You cannot express step values arbitrarily; you can only use integers that divide evenly into the range allowed by the field in question.
For instance, in the "hours" field you could only follow a forward slash with 1, 2, 3, 4, 6, 8, or 12. < $>
Here are some more examples of how to use cron's scheduling component:
* * * * * - Run the command every minute.
12 * * * * - Run the command 12 minutes after every hour.
0,15,30,45 * * * * - Run the command every 15 minutes.
* / 15 * * * * - Run the command every 15 minutes.
0 4 * * * - Run the command every day at 4: 00 AM.
0 4 * * 2-4 - Run the command every Tuesday, Wednesday, and Thursday at 4: 00 AM.
20,40 * / 8 * 7-12 * - Run the command on the 20th and 40th minute of every 8th hour every day of the last 6 months of the year.
If you find any of this confusing or if you'd like help writing schedules for your own cron tasks, Cronitor provides a handy cron schedule expression editor named "Crontab Guru" which you can use to check whether your cron schedules are valid.
Managing Crontabs
Once you "ve settled on a schedule and you know the job you want to run, you" ll need to put it somewhere your daemon will be able to read it.
As mentioned previously, a crontab is a special file that holds the schedule of jobs cron will run.
However, these are not intended to be edited directly.
Instead, it's recommended that you use the crontab command.
This allows you to edit your user profile's crontab without changing your privileges with sudo.
The crontab command will also let you know if you have syntax errors in the crontab, while editing it directly will not.
You can edit your crontab with the following command:
This will open up your crontab in your user profile's default text editor.
< $> note Note: On new CentOS 8 servers, the crontab -e command will open up your user's crontab with vi by default. vi is an extremely powerful and flexible text editor, but it can feel somewhat obtuse for users who lack experience with it.
If you'd like to use a more approachable text editor as your default crontab editor, you could install and configure nano as such.
To do so, install nano with dnf:
When prompted, press y and then ENTER to confirm that you want to install nano.
To set nano as your user profile's default visual editor, open up the .bash _ profile file for editing.
Now that you've installed it, you can do so with nano:
At the bottom of the file, add the following line:
This sets the VISUAL environment variable to nano.
VISUAL is a Unix environment variable that many programs - including crontab - invoke to edit a file.
After adding this line, save and close the file by pressing CTRL + X, Y, then ENTER.
Then reload .bash _ profile so the shell picks up the new change:
Once in the editor, you can input your schedule with each job on a new line.
Otherwise, you can save and close the crontab for now.
If you opened your crontab with vi, the default CentOS 8 text editor, you can do so by pressing ESC to make sure you're in vi's command mode, then type: x and press ENTER.
Please note that, on Linux systems, there is another crontab stored under the / etc / directory.
This is a system-wide crontab that has an additional field for which user profile each cron job should be run under.
This tutorial focuses on user-specific crontabs, but if you wanted to edit the system-wide crontab, you could do so with the following command:
If you "d like to view the contents of your crontab, but not edit it, you can use the following command:
You can erase your crontab with the following command:
< $> warning Warning: The following command will not ask you to confirm that you want to erase your crontab.
Only run it if you are certain that you want to erase it. < $>
This command will delete the user's crontab immediately.
However, you can include the -i flag to have the command prompt you to confirm that you actually want to delete the user's crontab:
When prompted, you must enter y to delete the crontab or n to cancel the deletion.
Managing Cron Job Output
Because cron jobs are executed in the background, it isn't always apparent that they've run successfully.
Now that you know how to use the crontab command and how to schedule a cron job, you can start experimenting with some different ways of redirecting the output of cron jobs to help you track that they've been executed successfully.
If you have a mail transfer agent - such as Sendmail - installed and properly configured on your server, you can send the output of cron tasks to the email address associated with your Linux user profile.
You can also manually specify an email address by providing a MAILTO setting at the top of the crontab.
For example, you could add the following lines to a crontab.
These include a MAILTO statement followed by an example email address, a SHELL directive that indicates the shell to run (bash in this example), a HOME directive pointing to the path in which to search for the cron binary, and a single cron task:
This particular job will return "Run this command every minute," and that output will get emailed every minute to the email address specified after the MAILTO directive.
You can also redirect a cron task's output into a log file or into an empty location to prevent getting an email with the output.
To append a scheduled command's output to a log file, add > > to the end of the command followed by the name and location of a log file of your choosing, like this:
Let's say you want to use cron to run a script but keep it running in the background.
To do so, you could redirect the script's output to an empty location, like / dev / null which immediately deletes any data written to it. For example, the following cron job executes a PHP script and runs it in the background:
This cron job also redirects standard error - represented by 2 - to standard output (> & 1).
Because standard output is already being redirected to / dev / null, this essentially allows the script to run silently.
Even if the crontab contains a MAILTO statement, the command's output won't be sent to the specified email address.
Restricting Access
You can manage which users are allowed to use the crontab command with the cron.allow and cron.deny files, both of which are stored in the / etc / directory.
If the cron.deny file exists, any user listed in it will be barred from editing their crontab.
If cron.allow exists, only users listed in it will be able to edit their crontabs.
If both files exist and the same user is listed in each, the cron.allow file will override cron.deny and the user will be able to edit their crontab.
For example, to deny access to all users and then give access to the user ishmael, you could use the following command sequence:
First, we lock out all users by appending ALL to the cron.deny file.
Then, by appending the username to the cron.allow file, we give the ishmael user profile access to execute cron jobs.
Note that if a user has sudo privileges, they can edit another user's crontab with the following command:
However, if cron.deny exists and < ^ > user < ^ > is listed in it and they aren't listed in cron.allow, you'll receive the following error after running the previous command:
By default, most cron daemons will assume all users have access to cron unless either cron.allow or cron.deny exists.
Special Syntax
There are also several shorthand commands you can use in your crontab file to help streamline job scheduling.
They are essentially shortcuts for the equivalent numeric schedule specified:
Shortcut
Shorthand for
@ hourly
0 * * * *
@ daily
0 0 * * *
@ weekly
0 0 * * 0
@ monthly
0 0 1 * *
@ yearly
0 0 1 1 *
< $> note Note: Not all cron daemons can parse this syntax (particularly older versions), so double-check it works before you rely on it. < $>
Additionally, the @ reboot shorthand will run whatever command follows it any time the server starts up:
Using these shortcuts whenever possible can help make it easier to interpret the schedule of tasks in your crontab.
Cron is a flexible and powerful utility that can reduce the burden of many tasks associated with system administration.
When combined with shell scripts, you can automate tasks that are normally tedious or complicated.
Recommended Steps To Harden Apache HTTP on FreeBSD 12.0
3757
Although the default installation of an Apache HTTP server is already safe to use, its configuration can be substantially improved with a few modifications.
You can complement already present security mechanisms, for example, by setting protections around cookies and headers, so connections can't be tampered with at the user's client level.
By doing this you can dramatically reduce the possibilities of several attack methods, like Cross-Site Scripting attacks (also known as XSS).
You can also prevent other types of attacks, such as Cross-Site Request Forgery, or session hijacking, as well as Denial of Service attacks.
In this tutorial you'll implement some recommended steps to reduce how much information on your server is exposed.
You will verify the directory listings and disable indexing to check the access to resources.
You'll also change the default value of the timeout directive to help mitigate Denial of Service type of attacks.
Furthermore you'll disable the TRACE method so sessions can't be reversed and hijacked.
Finally you'll secure headers and cookies.
Most of the configuration settings will be applied to the Apache HTTP main configuration file found at / usr / local / etc / apache24 / httpd.conf.
Before you begin this guide you'll need the following:
A FreeBSD 12 server set up by following this tutorial on How To Get Started with FreeBSD.
A firewall set up following the Configuring a Firewall Section in the Recommended Steps for New FreeBSD 12.0 Servers article.
A complete FAMP stack installed by following How To Install an Apache, MySQL, and PHP (FAMP) Stack on FreeBSD 12.0 tutorial.
A Let's Encrypt certificate installed by following the tutorial How To Secure Apache with Let's Encrypt on FreeBSD.
With the prerequisites in place you have a FreeBSD system with a stack on top able to serve web content using anything written in PHP, such as major CMS software.
Furthermore, you've encrypted safe connections through Let's Encrypt.
Reducing Server Information
The operating system banner is a method used by computers, servers, and devices of all kinds to present themselves into networks.
Malicious actors can use this information to gain exploits into the relevant systems.
In this section you'll reduce the amount of information published by this banner.
Sets of directives control how this information is displayed.
For this purpose the ServerTokens directive is important; by default it displays all details about the operating system and compiled modules to the client that's connecting to it.
You'll use a tool for network scanning to check what information is currently revealed prior to applying any changes.
To install nmap run the following command:
To get your server's IP address, you can run the following command:
You can check the web server response by using the following command:
You invoke nmap to make a scan (hence the -s flag), to display the version (the -V flag) on port 80 (the -p flag) on the given IP or domain.
You'll receive information about your web server, similar to the following:
This output shows that information such as the operating system, the Apache HTTP version, and OpenSSL are visible.
This can be useful for attackers to gain information about the server and choose the right tools to exploit, for example, a vulnerability in the software running on the server.
You'll place the ServerTokens directive in the main configuration file since it doesn't come configured by default.
The lack of this configuration makes Apache HTTP display the full information about the server as the documentation states.
To limit the information that is revealed about your server and configuration, you'll place the ServerTokens directive inside the main configuration file.
You'll place this directive following the ServerName entry in the configuration file.
Run the following command to find the directive
You'll find the line number that you can then search with vi:
Run the following command:
Add the following highlighted line:
Save and exit the file with: wq and ENTER.
Setting the ServerTokens directive to Prod will make it only display that this is an Apache web server.
For this to take effect restart the Apache HTTP server:
To test the changes, run the following command:
You'll see similar output to the following with more minimal information on your Apache web server:
You've seen what information the server was announcing prior to the change and you've now reduced this to the minimum.
With this you're providing fewer clues about your server to an external actor.
In the next step you'll manage the directory listings for your web server.
Managing Directory Listings
In this step you'll ensure the directory listing is correctly configured, so the right parts of the system are publicly available as intended, while the remainder are protected.
< $> note Note: When an argument is declared it is active, but the + can visually reinforce it is in fact enabled.
When a minus sign - is placed the argument is denied, for example, Options -Indexes.
Arguments with + and / or - can not be mixed, it is considered bad syntax in Apache HTTP and it may be rejected at the start up.
Adding the statement Options -Indexes will set the content inside the data path / usr / local / www / apache24 / data to not index (read listed) automatically if an .html file doesn't exist, and not show if a URL maps this directory.
This will also apply when using virtual host configurations such as the one used for the prerequisite tutorial for the Let's Encrypt certificate.
You will set the Options directive with the -Indexes argument and with the + FollowSymLinks directive, which will allow symbolic links to be followed.
You'll use the + symbol in order to comply with Apache's HTTP conventions.
Run the following command to find the line to edit in the configuration file:
Run this command to directly access the line for editing:
Now edit the line as per the configuration:
Restart Apache HTTP to implement these changes:
At your domain in the browser, you'll see a forbidden access message, also known as the 403 error.
This is due to the changes you've applied.
Placing -Indexes into the Options directive has disabled the auto-index capability of Apache HTTP and therefore there's no index.html file inside the data path.
You can solve this by placing an index.html file inside the VirtualHost you enabled in the prerequisite tutorial for the Let's Encrypt certificate.
You'll use the default block within Apache HTTP and place it in the same folder as the DocumentRootthat you declared in the virtual host.
Use the following command to do this:
Now you'll see an It works!
message when visiting your domain.
In this section you've placed restrictions to the Indexes directive to not automatically enlist and display content other than what you intend.
Now if there is not an index.html file inside the data path Apache HTTP will not automatically create an index of contents.
In the next step you'll move beyond obscuring information and customize different directives.
Reducing the Timeout Directive Value
The Timeout directive sets the limit of time Apache HTTP will wait for new input / output before failing the connection request.
This failure can occur due to different circumstances such as packets not arriving to the server or data not being confirmed as received by the client.
By default the timeout is set to 60 seconds.
In environments where the internet service is slow this default value may be sensible, but one minute is quite a long time particularly if the server is covering a target of users with faster internet service.
Furthermore the time during which the server is not closing the connection can be abused to perform Denial of Service attacks (DoS).
If a flood of these malicious connections occurs the server will stumble and possibly become saturated and irresponsive.
To change the value you'll find the Timeout entries in the httpd-default.conf file:
You'll see similar output to:
In the output line 10 sets the Timeout directive value.
To directly access this line run the following command:
You'll change it to 30 seconds, for example, like the following:
The value of the Timeout directive has to balance a time range large enough for those events to allow a legitimate and successful connection to happen, but short enough to prevent undesired connection attempts.
< $> note Note: Denial of Service attacks can drain the server's resources quite effectively.
A complementary and very capable counter measure is using a threaded MPM to get the best performance out of how Apache HTTP handles connections and processes.
In this tutorial How To Configure Apache HTTP with MPM Event and PHP-FPM on FreeBSD 12.0 there are steps on enabling this capability.
For this change to take effect restart the Apache HTTP server:
You've changed the default value of the Timeout directive in order to partially mitigate DoS attacks.
Disabling the TRACE method
The Hypertext Transport Protocol was developed following a client-server model and as such, the protocol has request methods to retrieve or place information from / to the server.
The server needs to understand these sets of methods and the interaction between them.
In this step you'll configure the minimum necessary methods.
TheTRACE method, which was considered harmless, was leveraged to perform Cross Site Tracing attacks.
These types of attacks allow malicious actors to steal user sessions through that method.
The method was designed for debugging purposes by the server returning the same request originally sent by the client.
Because the cookie from the browser's session is sent to the server it will be sent back again.
However, this could potentially be intercepted by a malicious actor, who can then redirect a browser's connection to a site of their control and not to the original server.
Because of the possibility of the misuse of the TRACE method it is recommended to only use it for debugging and not in production.
In this section you'll disable this method.
Edit the httpd.conf file with the following command and then press G to reach the end of the file:
Add the following entry path at the end of the file:
A good practice is to only specify the methods you'll use in your Apache HTTP web server.
This will help limit potential entry points for malicious actors.
LimitExcept can be useful for this purpose since it will not allow any other methods than those declared in it. For example a configuration can be established like this one:
As declared within the LimitExcept directive only the GET, POST, and HEAD methods are allowed in the configuration.
The GET method is part of the HTTP protocol and it is used to retrieve data.
The POST method is also part of the HTTP protocol and is used to send data to the server.
The HEAD method is similar to GET, however this has no response body.
You'll use the following command and place the LimitExcept block inside the file:
To set this configuration you'll place the following block into the DocumentRoot directive entry where the content will be read from, more specifically inside the Directory entry:
To apply the changes restart Apache HTTP:
The newer directive AllowedMethods provides similar functionality, although its status is still experimental.
You've seen what HTTP methods are, their use, and the protection they offer from malicious activity leveraging the TRACE method as well as how to declare what methods to use.
Next you'll work with further protections dedicated to HTTP headers and cookies.
Securing Headers and Cookies
In this step you'll set specific directives to protect the sessions that the client machines will open when visiting your Apache HTTP web server.
This way your server will not load unwanted content, encryption will not be downgraded, and you'll avoid content sniffing.
Headers are components of the requests methods.
There are headers to adjust authentication, communication between server and client, caching, content negotiation, and so on.
Cookies are bits of information sent by the server to the browser.
These bits allow the server to recognize the client browser from one computer to another.
They also allow servers to recognize user sessions.
For example, they can track a shopping cart of a logged-in user, payment information, history, and so on.
Cookies are used and retained in the client's web browser since HTTP is a stateless protocol, meaning once the connection closes the server does not remember the request sent by one client, or another one.
It is important to protect headers as well as cookies because they provide communication between the web browser client and the web server.
The headers module comes activated by default.
To check if it's loaded you'll use the following command:
If you don't see any output, check if the module is activated inside Apache's httpd.conf file:
As output you'll see an uncommented line referring to the specific module for headers:
Remove the hashtag at the beginning of the mod _ headers.so line, if present, to activate the directive.
By making use of the following Apache HTTP directives you'll protect headers and cookies from malicious activity to reduce the risk for clients and servers.
Now you'll set the header's protection.
You'll place all these header values in one block.
You can choose to apply these values as you wish, but all are recommended.
Place the following block at the end of the file:
Header set Strict-Transport-Security "max-age = 31536000; includeSubDomains ": HTTP Strict Transport Security (HTSTS) is a mechanism for web servers and clients (mainly browsers) to establish communications using only HTTPS.
By implementing this you're avoiding man-in-the-middle attacks, where a third party in between the communication could potentially access the bits, but also tamper with them.
Header always edit Set-Cookie (.
*) "$1; HttpOnly; Secure ": The HttpOnly and Secure flags on headers help prevent cross-site scripting attacks, also known as XSS.
Cookies can be misused by attackers to pose as legitimate visitors presenting themselves as someone else (identity theft), or be tampered.
Header set Referrer-Policy "strict-origin ": The Referrer-Policy header sets what information is included as the referrer information in the header field.
Header set Content-Security-Policy "default-src 'self '; upgrade-insecure-requests; ": The Content-Security-Policy header (CSP) will completely prevent loading content not specified in the parameters, which is helpful to prevent cross-site scripting (XSS) attacks.
There are many possible parameters to configure the policy for this header.
The bottom line is configuring it to load content from the same site and upgrade any content with an HTTP origin.
Header set X-XSS-Protection "1; mode = block ": This supports older browsers that do not cope with Content-Security-Policy headers.
The 'X-XSS-Protection' header provides protection against Cross-Site Scripting attacks.
You do not need to set this header unless you need to support old browser versions, which is rare.
Header set X-Frame-Options: "deny ": This prevents clickjacking attacks.
The 'X-Frame-Options' header tells a browser if a page can be rendered in a < frame >, < iframe >, < embed >, or < object >.
This way content from other sites cannot be embedded into others, preventing clickjacking attacks.
Here you're denying all frame render so the web page can't be embedded anywhere else, not even inside the same web site.
You can adapt this to your needs, if, for example, you must authorize rendering some pages because they are advertisements or collaborations with specific websites.
Header set X-Content-Type-Options "nosniff ": The 'X-Content-Type-Options' header controls MIME types so they're not changed and followed.
MIME types are file format standards; they work for text, audio, video, image, and so on.
This header blocks malicious actors from content sniffing those files and trying to alter the file types.
Now restart Apache for the changes to take effect:
To check the security levels of your configuration settings, visit the security headers website.
Having followed the steps in this tutorial, your domain will score an A grade.
< $> note Note: If you make your headers check by visiting https: / / securityheaders.com / and get an F grade it could be because there is no index.html inside the DocumentRoot of your site as instructed at the end of Step 2. If checking your headers you get a different grade than an A or an F, check each Header set line looking for any misspelling that may have caused the downgrade.
In this step you have worked with up to seven settings to improve the security of your headers and cookies.
These will help prevent cross-site scripting, clickjacking, and other types of attacks.
In this tutorial you've addressed several security aspects, from information disclosure, to protecting sessions, through setting alternative configuration settings for important functionality.
For further resources on hardening Apache, here are some other references:
Apache's HTTP security tips
Mozilla's security guidelines
Center for Internet Security audit recommendations for Apache HTTP
For extra tools to protect Apache HTTP:
mod _ evasive is a useful tool to help mitigate DoS attacks.
You can find more detail in the How To Protect Against DoS and DDoS with mod _ evasive for Apache tutorial.
fail2ban is an intrusion prevention software useful to block repeated login attempts from non-authorized users.
You can read more in the How To Protect an Apache Server with Fail2Ban tutorial.
ModSecurity is a Web Application Firewall (or WAF) and as such it provides a wide range of possibilities based on predefined rules written by SpyderLabs and community members.
You can read more about this in the How To Set Up ModSecurity with Apache tutorial.
Understanding Map and Set Objects in JavaScript
3752
The author selected the Open Internet / Free Speech Fund to receive a donation as part of the Write for DOnations program.
In JavaScript, developers often spend a lot of time deciding the correct data structure to use.
This is because choosing the correct data structure can make it easier to manipulate that data later on, saving time and making code easier to comprehend.
The two predominant data structures for storing collections of data are Objects and Arrays (a type of object).
Developers use Objects to store key / value pairs and Arrays to store indexed lists.
However, to give developers more flexibility, the ECMAScript 2015 specification introduced two new types of iterable objects: Maps, which are ordered collections of key / value pairs, and Sets, which are collections of unique values.
In this article, you will go over the Map and Set objects, what makes them similar or different to Objects and Arrays, the properties and methods available to them, and examples of some practical uses.
Maps
A Map is a collection of key / value pairs that can use any data type as a key and can maintain the order of its entries.
Maps have elements of both Objects (a unique key / value pair collection) and Arrays (an ordered collection), but are more similar to Objects conceptually.
This is because, although the size and order of entries is preserved like an Array, the entries themselves are key / value pairs like Objects.
Maps can be initialized with the new Map () syntax:
This gives us an empty Map:
Adding Values to a Map
You can add values to a map with the set () method.
The first argument will be the key, and the second argument will be the value.
The following adds three key / value pairs to map:
Here we begin to see how Maps have elements of both Objects and Arrays.
Like an Array, we have a zero-indexed collection, and we can also see how many items are in the Map by default.
Maps use the = > syntax to signify key / value pairs as key = > value:
This example looks similar to a regular object with string-based keys, but we can use any data type as a key with Maps.
In addition to manually setting values on a Map, we can also initialize a Map with values already.
We do this using an Array of Arrays containing two elements that are each key / value pairs, which looks like this:
Using the following syntax, we can recreate the same Map:
< $> note Note: This example uses trailing commas, also referred to as dangling commas.
This is a JavaScript formatting practice in which the final item in a series when declaring a collection of data has a comma at the end.
Though this formatting choice can be used for cleaner diffs and easier code manipulation, whether to use it or not is a matter of preference.
For more information on trailing commas, see this Trailing Comma article from the MDN web docs.
Incidentally, this syntax is the same as the result of calling Object.entries () on an Object.
This provides a ready-made way to convert an Object to a Map, as shown in the following code block:
Alternatively, you can turn a Map back into an Object or an Array with a single line of code.
The following converts a Map to an Object:
This will result in the following value of obj:
Now, let's convert a Map to an Array:
This will result in the following Array for arr:
Map Keys
Maps accept any data type as a key, and do not allow duplicate key values.
We can demonstrate this by creating a map and using non-string values as keys, as well as setting two values to the same key.
First, let's initialize a map with non-string keys:
This example will override the first key of 1 with the subsequent one, and it will treat '1' the string and 1 the number as unique keys:
Although it is a common belief that a regular JavaScript Object can already handle Numbers, booleans, and other primitive data types as keys, this is actually not the case, because Objects change all keys to strings.
As an example, initialize an object with a numerical key and compare the value for a numerical 1 key and a stringified "1" key:
This is why if you attempt to use an Object as a key, it will print out the string object Object instead.
As an example, create an Object and then use it as the key of another Object:
This will yield the following:
This is not the case with Map.
Try creating an Object and setting it as the key of a Map:
The key of the Map element is now the object we created.
There is one important thing to note about using an Object or Array as a key: the Map is using the reference to the Object to compare equality, not the literal value of the Object.
In JavaScript {} = = = {} returns false, because the two Objects are not the same two Objects, despite having the same (empty) value.
That means that adding two unique Objects with the same value will create a Map with two entries:
But using the same Object reference twice will create a Map with one entry.
Which will result in the following:
The second set () is updating the same exact key as the first, so we end up with a Map that only has one value.
Getting and Deleting Items from a Map
One of the disadvantages of working with Objects is that it can be difficult to enumerate them, or work with all the keys or values.
The Map structure, by contrast, has a lot of built-in properties that make working with their elements more direct.
We can initialize a new Map to demonstrate the following methods and properties: delete (), has (), get (), and size.
Use the has () method to check for the existence of an item in a map. has () will return a Boolean.
Use the get () method to retrieve a value by key.
One particular benefit Maps have over Objects is that you can find the size of a Map at any time, like you can with an Array.
You can get the count of items in a Map with the size property.
This involves fewer steps than converting an Object to an Array to find the length.
Use the delete () method to remove an item from a Map by key.
The method will return a Boolean - true if an item existed and was deleted, and false if it did not match any item.
This will result in the following Map:
Finally, a Map can be cleared of all values with map.clear ().
This will yield:
Keys, Values, and Entries for Maps
Objects can retrieve keys, values, and entries by using the properties of the Object constructor.
Maps, on the other hand, have prototype methods that allow us to get the keys, values, and entries of the Map instance directly.
The keys (), values (), and entries () methods all return a MapIterator, which is similar to an Array in that you can use for... of to loop through the values.
Here is another example of a Map, which we can use to demonstrate these methods:
The keys () method returns the keys:
The values () method returns the values:
The entries () method returns an array of key / value pairs:
Iteration with Map
Map has a built-in forEach method, similar to an Array, for built-in iteration.
However, there is a bit of a difference in what they iterate over.
The callback of a Map's forEach iterates through the value, key, and map itself, while the Array version iterates through the item, index, and array itself.
This is a big advantage for Maps over Objects, as Objects need to be converted with keys (), values (), or entries (), and there is not a simple way to retrieve the properties of an Object without converting it.
To demonstrate this, let's iterate through our Map and log the key / value pairs to the console:
This will give:
Since a for... of loop iterates over iterables like Map and Array, we can get the exact same result by destructuring the array of Map items:
Map Properties and Methods
The following table shows a list of Map properties and methods for quick reference:
Properties / Methods
Description
Returns
set (key, value)
Appends a key / value pair to a Map
Map Object
delete (key)
Removes a key / value pair from a Map by key
Boolean
get (key)
Returns a value by key
value
has (key)
Checks for the presence of an element in a Map by key
clear ()
Removes all items from a Map
N / A
keys ()
Returns all keys in a Map
MapIterator object
values ()
Returns all values in a Map
entries ()
Returns all keys and values in a Map as [key, value]
forEach ()
Iterates through the Map in insertion order
size
Returns the number of items in a Map
Number
When to Use Map
Summing up, Maps are similar to Objects in that they hold key / value pairs, but Maps have several advantages over objects:
Size - Maps have a size property, whereas Objects do not have a built-in way to retrieve their size.
Iteration - Maps are directly iterable, whereas Objects are not.
Flexibility - Maps can have any data type (primitive or Object) as the key to a value, while Objects can only have strings.
Ordered - Maps retain their insertion order, whereas objects do not have a guaranteed order.
Due to these factors, Maps are a powerful data structure to consider.
However, Objects haves some important advantages as well:
JSON - Objects work flawlessly with JSON.parse () and JSON.stringify (), two essential functions for working with JSON, a common data format that many REST APIs deal with.
Working with a single element - Working with a known value in an Object, you can access it directly with the key without the need to use a method, such as Map's get ().
This list will help you decide if a Map or Object is the right data structure for your use case.
Set
A Set is a collection of unique values.
Unlike a Map, a Set is conceptually more similar to an Array than an Object, since it is a list of values and not key / value pairs.
However, Set is not a replacement for Arrays, but rather a supplement for providing additional support for working with duplicated data.
You can initialize Sets with the new Set () syntax.
This gives us an empty Set:
Items can be added to a Set with the add () method.
(This is not to be confused with the set () method available to Map, although they are similar.)
Since Sets can only contain unique values, any attempt to add a value that already exists will be ignored.
< $> note Note: The same equality comparison that applies to Map keys applies to Set items.
Two objects that have the same value but do not share the same reference will not be considered equal.
You can also initialize Sets with an Array of values.
If there are duplicate values in the array, they will be removed from the Set.
Conversely, a Set can be converted into an Array with one line of code:
Set has many of the same methods and properties as Map, including delete (), has (), clear (), and size.
Note that Set does not have a way to access a value by a key or index, like Map.get (key) or arr [index].
Keys, Values, and Entries for Sets
Map and Set both have keys (), values (), and entries () methods that return an Iterator.
However, while each one of these methods have a distinct purpose in Map, Sets do not have keys, and therefore keys are an alias for values.
This means that keys () and values () will both return the same Iterator, and entries () will return the value twice.
It makes the most sense to only use values () with Set, as the other two methods exist for consistency and cross-compatibility with Map.
Iteration with Set
Like Map, Set has a built-in forEach () method.
Since Sets don't have keys, the first and second parameter of the forEach () callback return the same value, so there is no use case for it outside of compatibility with Map.
The parameters of forEach () are (value, key, set).
Both forEach () and for... of can be used on Set.
First, let's look at forEach () iteration:
Then we can write the for... of version:
Both of these strategies will yield the following:
Set Properties and Methods
The following table shows a list of Set properties and methods for quick reference:
add (value)
Appends a new item to a Set
Set Object
delete (value)
Removes the specified item from a Set
has ()
Checks for the presence of an item in a Set
Removes all items from a Set
Returns all values in a Set (same as values ())
SetIterator object
Returns all values in a Set (same as keys ())
Returns all values in a Set as [value, value]
Iterates through the Set in insertion order
Returns the number of items in a Set
When to Use Set
Set is a useful addition to your JavaScript toolkit, particularly for working with duplicate values in data.
In a single line, we can create a new Array without duplicate values from an Array that has duplicate values.
Set can be used for finding the union, intersection, and difference between two sets of data. However, Arrays have a significant advantage over Sets for additional manipulation of the data due to the sort (), map (), filter (), and reduce () methods, as well as direct compatibility with JSON methods.
In this article, you learned that a Map is a collection of ordered key / value pairs, and that a Set is a collection of unique values.
Both of these data structures add additional capabilities to JavaScript and simplify common tasks such as finding the length of a key / value pair collection and removing duplicate items from a data set, respectively.
On the other hand, Objects and Arrays have been traditionally used for data storage and manipulation in JavaScript, and have direct compatibility with JSON, which continues to make them the most essential data structures, especially for working with REST APIs.
Maps and Sets are primarily useful as supporting data structures for Objects and Arrays.
If you would like to learn more about JavaScript, check out the homepage for our How To Code in JavaScript series, or browse our How to Code in Node.js series for articles on back-end development.
Recommended Steps to Secure a DigitalOcean Kubernetes Cluster
3870
The author selected Open Sourcing Mental Illness to receive a donation as part of the Write for DOnations program.
Kubernetes, the open-source container orchestration platform, is steadily becoming the preferred solution for automating, scaling, and managing high-availability clusters.
As a result of its increasing popularity, Kubernetes security has become more and more relevant.
Considering the moving parts involved in Kubernetes and the variety of deployment scenarios, securing Kubernetes can sometimes be complex.
Because of this, the objective of this article is to provide a solid security foundation for a DigitalOcean Kubernetes (DOKS) cluster.
Note that this tutorial covers basic security measures for Kubernetes, and is meant to be a starting point rather than an exhaustive guide.
For additional steps, see the official Kubernetes documentation.
In this guide, you will take basic steps to secure your DigitalOcean Kubernetes cluster.
You will configure secure local authentication with TLS / SSL certificates, grant permissions to local users with Role-based access controls (RBAC), grant permissions to Kubernetes applications and deployments with service accounts, and set up resource limits with the ResourceQuota and LimitRange admission controllers.
In order to complete this tutorial you will need:
A DigitalOcean Kubernetes (DOKS) managed cluster with 3 Standard nodes configured with at least 2 GB RAM and 1 vCPU each.
For detailed instructions on how to create a DOKS cluster, read our Kubernetes Quickstart guide.
This tutorial uses DOKS version 1.16.2-do.1.
A local client configured to manage the DOKS cluster, with a cluster configuration file downloaded from the DigitalOcean Control Panel and saved as ~ / .kube / config.
For detailed instructions on how to configure remote DOKS management, read our guide How to Connect to a DigitalOcean Kubernetes Cluster.
In particular, you will need:
The kubectl command-line interface installed on your local machine.
You can read more about installing and configuring kubectl in its official documentation.
This tutorial will use kubectl version 1.17.0-00.
The official DigitalOcean command-line tool, doctl.
For instructions on how to install this, see the doctl GitHub page.
This tutorial will use doctl version 1.36.0.
Step 1 - Enabling Remote User Authentication
After completing the prerequisites, you will end up with one Kubernetes superuser that authenticates through a predefined DigitalOcean bearer token.
However, sharing those credentials is not a good security practice, since this account can cause large-scale and possibly destructive changes to your cluster.
To mitigate this possibility, you can set up additional users to be authenticated from their respective local clients.
In this section, you will authenticate new users to the remote DOKS cluster from local clients using secure SSL / TLS certificates.
This will be a three-step process: First, you will create Certificate Signing Requests (CSR) for each user, then you will approve those certificates directly in the cluster through kubectl.
Finally, you will build each user a kubeconfig file with the appropriate certificates.
For more information regarding additional authentication methods supported by Kubernetes, refer to the Kubernetes authentication documentation.
Creating Certificate Signing Requests for New Users
Before starting, check the DOKS cluster connection from the local machine configured during the prerequisites:
Depending on your configuration, the output will be similar to this one:
This means that you are connected to the DOKS cluster.
Next, create a local folder for client's certificates.
For the purpose of this guide, ~ / certs will be used to store all certificates:
In this tutorial, we will authorize a new user called sammy to access the cluster.
Feel free to change this to a user of your choice.
Using the SSL and TLS library OpenSSL, generate a new private key for your user using the following command:
The -out flag will make the output file ~ / < ^ > certs < ^ > / < ^ > sammy < ^ > .key, and 4096 sets the key as 4096-bit.
For more information on OpenSSL, see our OpenSSL Essentials guide.
Now, create a certificate signing request configuration file.
Open the following file with a text editor (for this tutorial, we will use nano):
Add the following content into the < ^ > sammy.csr.cnf < ^ > file to specify in the subject the desired username as common name (CN), and the group as organization (O):
The certificate signing request configuration file contains all necessary information, user identity, and proper usage parameters for the user.
The last argument extendedKeyUsage = serverAuth, clientAuth will allow users to authenticate their local clients with the DOKS cluster using the certificate once it's signed.
Next, create the sammy certificate signing request:
The -config lets you specify the configuration file for the CSR, and -new signals that you are creating a new CSR for the key specified by -key.
You can check your certificate signing request by running the following command:
Here you pass in the CSR with -in and use -text to print out the certificate request in text.
The output will show the certificate request, the beginning of which will look like this:
Repeat the same procedure to create CSRs for any additional users.
Once you have all certificate signing requests saved in the administrator's ~ / certs folder, proceed with the next step to approve them.
Managing Certificate Signing Requests with the Kubernetes API
You can either approve or deny TLS certificates issued to the Kubernetes API by using kubectl command-line tool.
This gives you the ability to ensure that the requested access is appropriate for the given user.
In this section, you will send the certificate request for sammy and aprove it.
To send a CSR to the DOKS cluster use the following command:
Using a Bash here document, this command uses cat to pass the certificate request to kubectl apply.
Let's take a closer look at the certificate request:
name: < ^ > sammy-authentication < ^ > creates a metadata identifier, in this case called sammy-authentication.
request: $(cat ~ / < ^ > certs < ^ > / < ^ > sammy < ^ > .csr | base64 | tr -d '\ n ') sends the sammy.csr certificate signing request to the cluster encoded as Base64.
server auth and client auth specify the intended usage of the certificate.
In this case, the purpose is user authentication.
The output will look similar to this:
You can check certificate signing request status using the command:
Depending on your cluster configuration, the output will be similar to this:
Next, approve the CSR by using the command:
You will get a message confirming the operation:
< $> note Note: As an administrator you can also deny a CSR by using the command kubectl certificate deny < ^ > sammy-authentication < ^ >.
For more information about managing TLS certificates, please read Kubernetes official documentation.
Now that the CSR is approved, you can download it to the local machine by running:
This command decodes the Base64 certificate for proper usage by kubectl, then saves it as ~ / < ^ > certs < ^ > / < ^ > sammy < ^ > .crt.
With the sammy signed certificate in hand, you can now build the user's kubeconfig file.
Building Remote Users Kubeconfig
Next, you will create a specific kubeconfig file for the sammy user.
This will give you more control over the user's access to your cluster.
The first step in building a new kubeconfig is making a copy of the current kubeconfig file.
For the purpose of this guide, the new kubeconfig file will be called < ^ > config-sammy < ^ >:
Next, edit the new file:
Keep the first eight lines of this file, as they contain the necessary information for the SSL / TLS connection with the cluster.
Then starting from the user parameter, replace the text with the following highlighted lines so that the file looks similar to the following:
< $> note Note: For both client-certificate and client-key, use the absolute path to their corresponding certificate location.
Otherwise, kubectl will produce an error.
You can test the new user connection using kubectl cluster-info:
You will see an error similar to this:
This error is expected because the user sammy has no authorization to list any resource on the cluster yet.
Granting authorization to users will be covered in the next step.
For now, the output is confirming that the SSL / TLS connection was successful and the sammy authentication credentials were accepted by the Kubernetes API.
Step 2 - Authorizing Users Through Role Based Access Control (RBAC)
Once a user is authenticated, the API determines its permissions using Kubernetes built-in Role Based Access Control (RBAC) model.
RBAC is an effective method of restricting user rights based on the role assigned to it. From a security point of view, RBAC allows setting fine-grained permissions to limit users from accessing sensitive data or executing superuser-level commands.
For more detailed information regarding user roles refer to Kubernetes RBAC documentation.
In this step, you will use kubectl to assign the predefined role edit to the user sammy in the default namespace.
In a production environment, you may want to use custom roles and / or custom role bindings.
Granting Permissions
In Kubernetes, granting permissions means assigning the desired role to a user.
Assign edit permissions to the user sammy in the default namespace using the following command:
This will give output similar to the following:
Let's analyze this command in more detail:
create rolebinding < ^ > sammy-edit-role < ^ > creates a new role binding, in this case called < ^ > sammy-edit-role < ^ >.
--clusterrole = edit assigns the predefined role edit at a global scope (cluster role).
--user = < ^ > sammy < ^ > specifies what user to bind the role to.
--namespace = default grants the user role permissions within the specified namespace, in this case default.
Next, verify user permissions by listing pods in the default namespace.
You can tell if RBAC authorization is working as expected if no errors are shown.
You will get the following output:
Now that you have assigned permissions to sammy, you can now practice revoking those permissions in the next section.
Revoking Permissions
Revoking permissions in Kubernetes is done by removing the user role binding.
For this tutorial, delete the edit role from the user sammy by running the following command:
Verify if user permissions were revoked as expected by listing the default namespace pods:
You will receive the following error:
This shows that the authorization has been revoked.
From a security standpoint, the Kubernetes authorization model gives cluster administrators the flexibility to change users rights on-demand as required.
Moreover, role-based access control is not limited to a physical user; you can also grant and remove permissions to cluster services, as you will learn in the next section.
For more information about RBAC authorization and how to create custom roles, please read the official documentation.
Step 3 - Managing Application Permissions with Service Accounts
As mentioned in the previous section, RBAC authorization mechanisms extend beyond human users.
Non-human cluster users, such as applications, services, and processes running inside pods, authenticate with the API server using what Kubernetes calls service accounts.
When a pod is created within a namespace, you can either let it use the default service account or you can define a service account of your choice.
The ability to assign individual SAs to applications and processes gives administrators the freedom of granting or revoking permissions as required.
Moreover, assigning specific SAs to production-critical applications is considered a best security practice.
Since service accounts are used for authentication, and thus for RBAC authorization checks, cluster administrators could contain security threats by changing service account access rights and isolating the offending process.
To demonstrate service accounts, this tutorial will use an Nginx web server as a sample application.
Before assigning a particular SA to your application, you need to create the SA.
Create a new service account called < ^ > nginx-sa < ^ > in the default namespace:
You will get:
Verify that the service account was created by running the following:
This will give you a list of your service accounts:
Now you will assign a role to the < ^ > nginx-sa < ^ > service account.
For this example, grant < ^ > nginx-sa < ^ > the same permissions as the sammy user:
Running this will yield the following:
This command uses the same format as for the user sammy, except for the --serviceaccount = default: nginx-sa flag, where you assign the < ^ > nginx-sa < ^ > service account in the default namespace.
Check that the role binding was successful using this command:
This will give the following output:
Once you've confirmed that the role binding for the service account was successfully configured, you can assign the service account to an application.
Assigning a particular service account to an application will allow you to manage its access rights in real-time and therefore enhance cluster security.
For the purpose of this tutorial, an nginx pod will serve as the sample application.
Create the new pod and specify the < ^ > nginx-sa < ^ > service account with the following command:
The first portion of the command creates a new pod running an nginx web server on port: 80, and the last portion --serviceaccount = "< ^ > nginx-sa < ^ >" indicates that this pod should use the < ^ > nginx-sa < ^ > service account and not the default SA.
This will give you output similar to the following:
Verify that the new application is using the service account by using kubectl describe:
This will output a lengthy description of the deployment parameters.
Under the Pod Template section, you will see output similar to this:
In this section, you created the < ^ > nginx-sa < ^ > service account in the default namespace and assigned it to the nginx webserver.
Now you can control nginx permissions in real-time by changing its role as needed.
You can also group applications by assigning the same service account to each one and then make bulk changes to permissions.
Finally, you could isolate critical applications by assigning them a unique SA.
Summing up, the idea behind assigning roles to your applications / deployments is to fine-tune permissions.
In real-world production environments, you may have several deployments requiring different permissions ranging from read-only to full administrative privileges.
Using RBAC brings you the flexibility to restrict the access to the cluster as needed.
Next, you will set up admission controllers to control resources and safeguard against resource starvation attacks.
Step 4 - Setting Up Admission Controllers
Kubernetes admission controllers are optional plug-ins that are compiled into the kube-apiserver binary to broaden security options.
Admission controllers intercept requests after they pass the authentication and authorization phase.
Once the request is intercepted, admission controllers execute the specified code just before the request is applied.
While the outcome of either an authentication or authorization check is a boolean that allows or denies the request, admission controllers can be much more diverse.
Admission controllers can validate requests in the same manner as authentication, but can also mutate or change the requests and modify objects before they are admitted.
In this step, you will use the ResourceQuota and LimitRange admission controllers to protect your cluster by mutating requests that could contribute to a resource starvation or Denial-of-Service attack.
The ResourceQuota admission controller allows administrators to restrict computing resources, storage resources, and the quantity of any object within a namespace, while the LimitRange admission controller will limit the number of resources used by containers.
Using these two admission controllers together will protect your cluster from attacks that render your resources unavailable.
To demonstrate how ResourceQuota works, you will implement a few restrictions in the default namespace.
Start by creating a new ResourceQuota object file:
Add in the following object definition to set constraints for resource consumption in the default namespace.
You can adjust the values as needed depending on your nodes' physical resources:
This definition uses the hard keyword to set hard constraints, such as the maximum number of pods, configmaps, PersistentVolumeClaims, ReplicationControllers, secrets, services, and loadbalancers.
This also set contraints on compute resources, like:
requests.cpu, which sets the maximum CPU value of requests in milliCPU, or one thousandth of a CPU core.
requests.memory, which sets the maximum memory value of requests in bytes.
limits.cpu, which sets the maximum CPU value of limits in milliCPUs.
limits.memory, which sets the maximum memory value of limits in bytes.
Now, create the object in the namespace running the following command:
Notice that you are using the -f flag to indicate to Kubernetes the location of the ResourceQuota file and the --namespace flag to specify which namespace will be updated.
Once the object has been created, your ResourceQuota will be active.
You can check the default namespace quotas with describe quota:
The output will look similar to this, with the hard limits you set in the < ^ > resource-quota-default < ^ > .yaml file:
ResourceQuotas are expressed in absolute units, so adding additional nodes will not automatically increase the values defined here.
If more nodes are added, you will need to manually edit the values here to proportionate the resources.
ResourceQuotas can be modified as often as you need, but they cannot be removed unless the entire namespace is removed.
If you need to modify a particular ResourceQuota, update the corresponding .yaml file and apply the changes using the following command:
For more information regarding the ResourceQuota Admission Controller, refer to the official documentation.
Now that your ResourceQuota is set up, you will move on to configuring the LimitRange Admission Controller.
Similar to how the ResourceQuota enforces limits on namespaces, the LimitRange enforces the limitations declared by validating and mutating containers.
In a similar way to before, start by creating the object file:
Now, you can use the LimitRange object to restrict resource usage as needed.
Add the following content as an example of a typical use case:
The sample values used in < ^ > limit-ranges-default < ^ > .yaml restrict container memory to a maximum of 1Gi and limits CPU usage to a maximum of 400m, which is a Kubernetes metric equivalent to 400 milliCPU, meaning the container is limited to use almost half its core.
Next, deploy the object to the API server using the following command:
Now you can check the new limits with following command:
Your output will look similar to this:
To see LimitRanger in action, deploy a standard nginx container with the following command:
Check how the admission controller mutated the container by running the following command:
This will give many lines of output.
Look in the container specification section to find the resource limits specified in the LimitRange Admission Controller:
This would be the same as if you manually declared the resources and requests in the container specification.
In this step, you used the ResourceQuota and LimitRange admission controllers to protect against malicious attacks toward your cluster's resources.
For more information about LimitRange admission controller, read the official documentation.
Throughout this guide, you configured a basic Kubernetes security template.
This established user authentication and authorization, applications privileges, and cluster resource protection.
Combining all the suggestions covered in this article, you will have a solid foundation for a production Kubernetes cluster deployment.
From there, you can start hardening individual aspects of your cluster depending on your scenario.
If you would like to learn more about Kubernetes, check out our Kubernetes resource page, or follow our Kubernetes for Full-Stack Developers self-guided course.
How To Display Data from the DigitalOcean API with Django
3190
The author selected the Mozilla Foundation to receive a donation as part of the Write for DOnations program.
As demand for full-stack development continues to grow, web frameworks are making development workflows less cumbersome and more efficient; Django is one of those frameworks.
Django has been used in major websites such as Mozilla, Pinterest, and Instagram.
Unlike Flask, which is a neutral micro-framework, the Django PyPI package includes everything you would need for full-stack development; no need to set up a database or control panel for development.
One common use-case for Django is to use it to display information from APIs (such as Instagram posts or GitHub repositories) in your own websites and web apps.
While this is possible with other frameworks, Django's "batteries included" philosphy means there will be less hassle and fewer packages required to create the same result.
In this tutorial you will build a Django project that will display your DigitalOcean account's Droplet information using the DigitalOcean v2 API.
Specifically, you will be creating a website that will display a table of Droplets listing each of their IP addresses, IDs, hosting regions, and resources.
Your website will use BulmaCSS to style the page so you can focus on development while also having something nice to look at in the end.
Once you complete this tutorial, you will have a Django project that can produce a webpage that looks like this:
Template with Table of Droplet Data
A DigitalOcean account with at least one Droplet, and a personal access token.
Make sure to record the token in a safe place; you'll need it later on in this tutorial.
Familiarity in making requests to APIs.
For a comprehensive tutorial on working with APIs, take a look at How to Use Web APIs in Python3.
A local virtual environment for Python for maintaining dependencies.
In this tutorial we'll use the name < ^ > do _ django _ api < ^ > for our project directory and < ^ > env < ^ > for our virtual environment.
Familiarity with Django's template logic for rendering pages with API data.
Familiarity with Django's view logic for handling data recieved from the API and giving it to a template for rendering.
Step 1 - Making a Basic Django Project
From within the virtual environment < ^ > env < ^ >, install Django:
Now you can start a Django project and run some initial setup commands.
Use django-admin startproject < ^ > < name > < ^ > to create a subdirectory in the project folder named after your Django project, then switch to that directory.
Once it's created, inside this subdirectory, you will find manage.py, which is the usual way to interact with Django and run your project.
Use migrate to update Django's development database:
You'll see output that looks like this as the database updates:
Next, use the runserver command to run the project so you can test it out:
The output will look like this as the server starts:
You now have a basic Django project and a development server running.
To view your running development server, visit 127.0.0.1: 8000 in a browser.
It will display the Django startup page:
Generic Django Start-Page
Next you'll create a Django app and configure your project to run a view from that app so you'll see something more interesting than the default page.
Step 2 - Making a Basic Django App
In this step, you'll create the skeleton of the app that will hold your Droplet results.
You'll come back to this app later once you've set up the API call to populate it with data.
Make sure you're in the do _ django _ project directory, and create a Django app using the following command:
Now you need to add the new app to INSTALLED _ APPS in the settings.py file, so Django will recognize it. settings.py is a Django configuration file that's located inside another subdirectory in the Django project and has the same name as the project folder (do _ django _ project).
Django created both folders for you.
Switch to the do _ django _ project directory:
Edit settings.py in the editor of your choice:
Add your new app to the INSTALLED _ APPS section of the file:
GetDroplets View Function
Next you'll create a function, GetDroplets, inside the display _ droplets app's views.py file.
This function will render the template you'll use to display Droplet data, as context, from the API. context is a dictionary that is used to take data from Python code and send it to an HTML template so it can be displayed in a web page.
Switch to the display _ droplets directory:
Open views.py for editing:
Later you will populate this function and create the droplets.html file, but first let's configure urls.py to call this function when you visit the development server root directory (127.0.0.1: 8000).
Switch back to the do _ django _ project directory:
Open urls.py for editing:
Add an import statement for GetDroplets, then add an additional path to urlpatterns that will point to the new view.
If you want to make your own custom paths, the first parameter is the URL (such as example.com / * * admin * *), the second parameter is the function to call to produce the web page, and the third is just a name for the path.
Droplets Template
Next you'll be working with templates.
Templates are HTML files that Django uses to create web pages.
In this case, you'll use a template to construct an HTML page that displays the API data.
Switch back to the display _ droplets directory:
Inside this directory, create a template folder and switch to that directory:
Create droplets.html and open it for editing:
To avoid having to write any sort of CSS for this project, we'll use Bulma CSS because it's a free and lightweight CSS framework that allows you to create clean-looking web pages just by adding a few class attributes to the HTML.
Now let's create a template with a basic navigation bar.
Add the following code to the droplets.html file.
This code imports Bulma into boilerplate HTML and creates a nav bar displaying "Droplets."
Refresh your browser tab to view the changes you made to the template.
Template with Basic Header
So far you haven't touched anything related to APIs; you've created a foundation for the project.
Next you'll put this page to good use by making an API call and presenting the Droplet data.
Step 3 - Making The API Call
In this step, you'll set up an API call and send the Droplet data as context to the template to display in a table.
Getting Droplet Data
Navigate back to the display _ droplets app directory:
Install the requests library so you can talk to the API:
The requests library enables your code to request data from APIs and add headers (additional data sent along with our request).
Next, you'll create a services.py file, which is where you'll make the API call.
This function will use requests to talk to https: / / api.digitalocean.com / v2 / droplets and append each Droplet in the JSON file returned to a list.
Open services.py for editing:
Inside the get _ droplets function, two things occur: a request is made and data is parsed. url contains the URL requesting Droplet data from the DigitalOcean API. r stores the requested data.
requests takes two parameters in this case: url and headers.
If you want data from a different API, you'd replace the url value with the appropriate URL. headers sends DigitalOcean your access token, so they know you're allowed to make the request and for what account the request is being made.
droplets contains the information from the r variable, but now it has been converted from JSON, the format the API sends information in, into a dictionary which is easy to use in a for loop.
The next three lines create an array, droplet _ list [].
Then a for loop iterates over the information in droplets, and adds each item to the list.
All of the information taken from the API and stored in droplets can be found in DigitalOcean's Developer Docs.
< $> note Note: Don't forget to replace access _ token with your access token.
Also, keep it safe and never publish that token online.
Protecting Your Access Token
You should always hide your access token, but if someone ever wanted to run your project, you should have an easy way for them to add their own access token without having to edit Python code.
DotENV is the solution as variables are kept in a .env file that can be conveniently edited.
Navigate back to the do _ django _ project directory:
To start working with environment variables, install python-dotenv:
Once it's installed, you need to configure Django to handle environment variables, so you can reference them in code.
To do that, you need to add a few lines of code to manage.py and wsgi.py.
Open manage.py for editing:
Add the following code:
Adding this in manage.py means that when you issue commands to Django in development it will handle environment variables from your .env file.
If you ever need to handle environment variables in your production projects, you can do that from the wsgi.py file.
Change to the do _ django _ project directory:
And open wsgi.py for editing:
Add the following code to wsgi.py:
This code snippet has an additional os.path.dirname () because wsgi.py needs to look two directories back to find the .env file.
This snippet is not the same as the one used for manage.py.
Now you can use an environment variable in services.py instead of your access token.
Now replace your access token with an environment variable:
The next step is to create a .env file.
Create a .env file and pen the file for editing:
In .env, add your token as the variable DO _ ACCESS _ TOKEN:
< $> note Note: Add .env to your .gitignore file so it is never included in your commits.
The API connection is now set up and configured, and you've protected your access token as well.
It's time to present the information you retrieved to the user.
Step 4 - Handling Droplet Data in Views and Templates
Now that you can make API calls, you need to send the Droplet data to the template for rendering.
Let's return to the stub of the function, GetDroplets you created earlier in views.py.
In the function you'll send droplet _ list as context to the droplets.html template.
Add the following code to views.py:
Information sent to the droplets.html template is handled via the context dictionary.
This is why droplets acts as a key and the array returned from get _ droplets () acts as a value.
Presenting the Data in the Template
Inside the droplets.html template you'll create a table and populate it with the droplet data.
Switch to the templates directory:
Open droplets.html for editing:
Add the following code after the nav element in droplets.html:
{% for droplet in droplets%}... {% endfor%} is a loop that iterates through the array of Droplets retrieved from views.py.
Each Droplet is inserted in a table row.
The various {{droplet. < attribute >}} lines retrieve that attribute for each Droplet in the loop, and inserts it in a table cell.
Refresh your browser and you will see a list of Droplets.
You can now handle the DigitalOcean API inside your Django projects.
You've taken the data retrieved from the API and plugged it into the template you created earlier, to display the information in a readable and flexible manner.
In this article you built a Django project that can display Droplet information from the DigitalOcean API with Bulma CSS styling.
You've learned three important skills by following this tutorial:
How to handle API requests in Python using the requests and json modules.
How to display API data in a Django project using view and template logic.
How to safely handle your API tokens using dotenv in Django.
Now that you've gotten an introduction to handling APIs in Django, you can create a project of your own using either another feature from the DigitalOcean API or another API altogether.
You can also check out other Django tutorials or a similar tutorial with the React framework.
How To Manage and Use MySQL Database Triggers on Ubuntu 18.04
3270
The author selected the the Apache Software Foundation to receive a donation as part of the Write for DOnations program.
In MySQL a trigger is a user-defined SQL command that is invoked automatically during an INSERT, DELETE, or UPDATE operation.
The trigger code is associated with a table and is destroyed once a table is dropped.
You can specify a trigger action time and set whether it will be activated before or after the defined database event.
Triggers have several advantages.
For instance, you can use them to generate the value of a derived column during an INSERT statement.
Another use case is enforcing referential integrity where you can use a trigger to save a record to multiple related tables.
Other benefits include logging user actions to audit tables as well as live-copying data across different database schemas for redundancy purposes to prevent a single point of failure.
You can also use triggers to keep validation rules at the database level.
This helps in sharing the data source across multiple applications without breaking the business logic.
This greatly reduces round-trips to the database server, which therefore improves the response time of your applications.
Since the database server executes triggers, they can take advantage of improved server resources such as RAM and CPU.
In this tutorial, you'll create, use, and delete different types of triggers on your MySQL database.
Before you begin, make sure you have the following:
A MySQL database running on your server by following: How To Install MySQL on Ubuntu 18.04
Root user account credentials for your MySQL database.
Step 1 - Creating a Sample Database
In this step, you'll create a sample customer database with multiple tables for demonstrating how MySQL triggers work.
To understand more about MySQL queries read our Introduction to Queries in MySQL.
First, log in to your MySQL server as root:
Enter your MySQL root password when prompted and hit ENTER to continue.
When you see the mysql > prompt, run the following command to create a < ^ > test _ db < ^ > database:
Next, switch to the test _ db with:
You'll start by creating a customers table.
This table will hold the customers' records including the customer _ id, customer _ name, and level.
There will be two customer levels: BASIC and VIP.
Now, add a few records to the customers table.
To do this, run the following commands one by one:
You'll see the following output after running each of the INSERT commands:
To make sure that the sample records were inserted successfully, run the SELECT command:
You'll also create another table for holding related information about the customers account.
The table will have a customer _ id and status _ notes fields.
Next, you'll create a sales table.
This table will hold sales data related to the different customers through the customer _ id column:
You'll add sample data to the sales data in the coming steps while testing the triggers.
Next, create an audit _ log table to log updates made to the sales table when you implement the AFTER UPDATE trigger in Step 5:
With the < ^ > test _ db < ^ > database and the four tables in place, you'll now move on to work with the different MySQL triggers in your database.
Step 2 - Creating a Before Insert Trigger
In this step, you'll examine the syntax of a MySQL trigger before applying this logic to create a BEFORE INSERT trigger that validates the sales _ amount field when data is inserted into the sales table.
The general syntax for creating a MySQL trigger is shown in the following example:
The structure of the trigger includes:
DELIMITER / /: The default MySQL delimiter is; - it's necessary to change it to something else in order for MySQL to treat the following lines as one command until it hits your custom delimiter.
In this example, the delimiter is changed to / / and then the; delimiter is redefined at the end.
[TRIGGER _ NAME]: A trigger must have a name and this is where you include the value.
[TRIGGER TIME]: A trigger can be invoked during different timings.
MySQL allows you to define if the trigger will initiate before or after a database operation.
[TRIGGER EVENT]: Triggers are only invoked by INSERT, UPDATE, and DELETE operations.
You can use any value here depending on what you want to achieve.
[TABLE]: Any trigger that you create on your MySQL database must be associated with a table.
FOR EACH ROW: This statement tells MySQL to execute the trigger code for every row that the trigger affects.
[TRIGGER BODY]: The code that is executed when the trigger is invoked is called a trigger body.
This can be a single SQL statement or multiple commands.
Note that if you are executing multiple SQL statements on the trigger body, you must wrap them between a BEGIN...
END block.
< $> note Note: When creating the trigger body, you can use the OLD and NEW keywords to access the old and new column values entered during an INSERT, UPDATE, and DELETE operation.
In a DELETE trigger, only the OLD keyword can be used (which you'll use in Step 4).
Now you'll create your first BEFORE INSERT trigger.
This trigger will be associated with the sales table and it will be invoked before a record is inserted to validate the sales _ amount.
The function of the trigger is to check if the sales _ amount being inserted to the sales table is greater than 10000 and raise an error if this evaluates to true.
Make sure you're logged in to the MySQL server.
Then, enter the following MySQL commands one by one:
You're using the IF...
THEN...
END IF statement to evaluate if the amount being supplied during the INSERT statement is within your range.
The trigger is able to extract the new sales _ amount value being supplied by using the NEW keyword.
To raise a generic error message, you use the following lines to inform the user about the error:
Next, insert a record with a sales _ amount of 11000 to the sales table to check if the trigger will stop the operation:
This error shows that the trigger code is working as expected.
Now try a new record with a value of 7500 to check if the command will be successful:
Since the value is within the recommended range, you'll see the following output:
To confirm that the data was inserted run the following command:
The output confirms that the data is in the table:
In this step you've tested triggers to validate data before insertion into a database.
Next, you'll work with the AFTER INSERT trigger to save related information into different tables.
Step 3 - Creating an After Insert Trigger
AFTER INSERT triggers are executed when records are successfully inserted into a table.
This functionality can be used to run other business-related logics automatically.
For instance, in a bank application, an AFTER INSERT trigger can close a loan account when a customer finishes paying off the loan.
The trigger can monitor all payments inserted to a transaction table and close the loan automatically once the loan balance is zero.
In this step, you'll work with your customer _ status table by using an AFTER INSERT trigger to enter related customer records.
To create the AFTER INSERT trigger, enter the following commands:
Here you instruct MySQL to save another record to the customer _ status table once a new customer record is inserted to the customers table.
Now, insert a new record in the customers table to confirm your trigger code will be invoked:
Since the record was inserted successfully, check that a new status record was inserted into the customer _ status table:
The output confirms that the trigger ran successfully.
The AFTER INSERT trigger is useful in monitoring the lifecycle of a customer.
In a production environment, customers' accounts may undergo different stages such as account opening, suspension, and closing.
In the following steps you'll work with UPDATE triggers.
Step 4 - Creating a Before Update Trigger
A BEFORE UPDATE trigger is similar to the BEFORE INSERT trigger - the difference is when they are invoked.
You can use the BEFORE UPDATE trigger to check a business logic before a record is updated.
To test this, you'll use the customers table in which you've inserted some data already.
You have two levels for your customers in the database.
In this example, once a customer account is upgraded to the VIP level, the account can not be downgraded to the BASIC level.
To enforce such a rule, you will create a BEFORE UPDATE trigger that will execute before the UPDATE statement as shown following.
If a database user tries to downgrade a customer to the BASIC level from the VIP level, a user-defined exception will be triggered.
Enter the following SQL commands one by one to create the BEFORE UPDATE trigger:
You use the OLD keyword to capture the level that the user is supplying when running the UPDATE command.
Again, you use the IF...
END IF statement to signal a generic error statement to the user.
Next, run the following SQL command that tries to downgrade a customer account associated with the customer _ id of 3:
You'll see the following output providing the SET MESSAGE _ TEXT:
If you run the same command to a BASIC level customer, and try to upgrade the account to the VIP level, the command will execute successfully:
You've used the BEFORE UPDATE trigger to enforce a business rule.
Now you'll move on to use an AFTER UPDATE trigger for audit logging.
Step 5 - Creating an After Update Trigger
An AFTER UPDATE trigger is invoked once a database record is updated successfully.
This behavior makes the trigger suitable for audit logging.
In a multi-user environment, the administrator may want to view a history of users updating records in a particular table for audit purposes.
You'll create a trigger that logs the update activity of the sales table.
Our audit _ log table will contain information about the MySQL users updating the sales table, the date of the update, and the new and old sales _ amount values.
To create the trigger, run the following SQL commands:
You insert a new record to the audit _ log table.
You use the NEW keyword to retrieve the value of the sales _ id and the new sales _ amount.
Also, you use the OLD keyword to retrieve the previous sales _ amount since you want to log both amounts for audit purposes.
The command SELECT USER () retrieves the current user performing the operation and the NOW () statement retrieves the value of the current date and time from the MySQL server.
Now if a user tries to update the value of any record in the sales table, the log _ sales _ updates trigger will insert a new record to the audit _ log table.
Let's create a new sales record with a random sales _ id of 5 and try to update it. First, insert the sales record with:
Next, update the record:
Now run the following command to verify if the AFTER UPDATE trigger was able to register a new record into the audit _ log table:
The trigger logged the update.
Your output shows the previous sales _ amount and new amount registered with the user that updated the records:
You also have the date and time the update was performed, which are valuable for audit purposes.
Next you'll use the DELETE trigger to enforce referencing integrity at the database level.
Step 6 - Creating a Before Delete Trigger
BEFORE DELETE triggers invoke before a DELETE statement executes on a table.
These kinds of triggers are normally used to enforce referential integrity on different related tables.
For example, each record on the sales table relates to a customer _ id from the customers table.
If a database user deleted a record from the customers table that has a related record in the sales table, you would have no way of knowing the customer associated with that record.
To avoid this, you can create a BEFORE DELETE trigger to enforce your logic.
Run the following SQL commands one by one:
Now, try to delete a customer that has a related sales record:
As a result you'll receive the following output:
The BEFORE DELETE trigger can prevent accidental deletion of related information in a database.
However, in some situations, you may want to delete all the records associated with a particular record from the different related tables.
In this situation you would use the AFTER DELETE trigger, which you'll test in the next step.
Step 7 - Creating an After Delete Trigger
AFTER DELETE triggers are activated once a record has been deleted successfully.
An example of how you can use an AFTER DELETE trigger is a situation in which the discount level a particular customer receives is determined by the number of sales made during a defined period.
If some of the customer's records are deleted from the sales table, the customer discount level would need to be downgraded.
Another use of the AFTER DELETE trigger is deleting related information from another table once a record from a base table is deleted.
For instance, you'll set a trigger that deletes the customer record if the sales records with the related customer _ id are deleted from the sales table.
Run the following command to create your trigger:
Next, run the following to delete all sales records associated with a customer _ id of 2:
Now check if there are records for the customer from the sales table:
You will receive an Empty Set output since the customer record associated with the customer _ id of 2 was deleted by the trigger:
You've now used each of the different forms of triggers to perform specific functions.
Next you will see how you can remove a trigger from the database if you no longer need it.
Step 8 - Deleting Triggers
Similarly to any other database object, you can delete triggers using the DROP command.
The following is the syntax for deleting a trigger:
For instance, to delete the last AFTER DELETE trigger that you created, run the following command:
The need to delete triggers arises when you want to recreate its structure.
In such a case, you can drop the trigger and redefine a new one with the different trigger commands.
In this tutorial you've created, used, and deleted the different kinds of triggers from a MySQL database.
Using an example customer-related database you've implemented triggers for different use cases such as data validation, business-logic application, audit logging, and enforcing referential integrity.
For further information on using your MySQL database, check out the following:
How To Optimize MySQL with Query Cache on Ubuntu 18.04
How To Implement Pagination in MySQL with PHP on Ubuntu 18.04
How To Troubleshoot Issues in MySQL
How To Create a Node.js Module
3214
In Node.js, a module is a collection of JavaScript functions and objects that can be used by external applications.
Describing a piece of code as a module refers less to what the code is and more to what it does - any Node.js file or collection of files can be considered a module if its functions and data are made usable to external programs.
Because modules provide units of functionality that can be reused in many larger programs, they enable you to create loosely coupled applications that scale with complexity, and open the door for you to share your code with other developers.
Being able to write modules that export useful functions and data will allow you to contribute to the wider Node.js community - in fact, all packages that you use on npm were bundled and shared as modules.
This makes creating modules an essential skill for a Node.js developer.
In this tutorial, you will create a Node.js module that suggests what color web developers should use in their designs.
You will develop the module by storing the colors as an array, and providing a function to retrieve one randomly.
Afterwards, you will run through various ways of importing a module into a Node.js application.
You will need Node.js and npm installed on your development environment.
This tutorial uses version 10.17.0.
To install this on macOS or Ubuntu 18.04, follow the steps in How To Install Node.js and Create a Local Development Environment on macOS or the Installing Using a PPA section of How To Install Node.js on Ubuntu 18.04.
By having Node.js installed you will also have npm installed; this tutorial uses version 6.11.3.
You should also be familiar with the package.json file, and experience with npm commands would be useful as well.
To gain this experience, follow How To Use Node.js Modules with npm and package.json, particularly the Step 1 - Creating a package.json File.
It will also help to be comfortable with the Node.js REPL (Read-Evaluate-Print-Loop).
You will use this to test your module.
If you need more information on this, read our guide on How To Use the Node.js REPL.
Step 1 - Creating a Module
This step will guide you through creating your first Node.js module.
Your module will contain a collection of colors in an array and provide a function to get one at random.
You will use the Node.js built-in exports property to make the function and array available to external programs.
First, you'll begin by deciding what data about colors you will store in your module.
Every color will be an object that contains a name property that humans can easily identify, and a code property that is a string containing an HTML color code.
HTML color codes are six-digit hexadecimal numbers that allow you to change the color of elements on a web page.
You can learn more about HTML color codes by reading this HTML Color Codes and Names article.
You will then decide what colors you want to support in your module.
Your module will contain an array called allColors that will contain six colors.
Your module will also include a function called getRandomColor () that will randomly select a color from your array and return it.
In your terminal, make a new folder called colors and move into it:
Initialize npm so other programs can import this module later in the tutorial:
You used the -y flag to skip the usual prompts to customize your package.json.
If this were a module you wished to publish to npm, you would answer all these prompts with relevant data, as explained in How To Use Node.js Modules with npm and package.json.
In this case, your output will be:
Now, open up a command-line text editor such as nano and create a new file to serve as the entry point for your module:
Your module will do a few things.
First, you'll define a Color class.
Your Color class will be instantiated with its name and HTML code.
Add the following lines to create the class:
Now that you have your data structure for Color, add some instances into your module.
Write the following highlighted array to your file:
Finally, enter a function that randomly selects an item from the allColors array you just created:
The exports keyword references a global object available in every Node.js module.
All functions and objects stored in a module's exports object are exposed when other Node.js modules import it. The getRandomColor () function was created directly on the exports object, for example.
You then added an allColors property to the exports object that references the local constant allColors array created earlier in the script.
When other modules import this module, both allColors and getRandomColor () will be exposed and available for usage.
So far, you have created a module that contains an array of colors and a function that returns one randomly.
You have also exported the array and function, so that external programs can use them.
In the next step, you will use your module in other applications to demonstrate the effects of export.
Step 2 - Testing your Module with the REPL
Before you build a complete application, take a moment to confirm that your module is working.
In this step, you will use the REPL to load the colors module.
While in the REPL, you will call the getRandomColor () function to see if it behaves as you expect it to.
Start the Node.js REPL in the same folder as the index.js file:
When the REPL has started, you will see the > prompt.
This means you can enter JavaScript code that will be immediately evaluated.
If you would like to read more about this, follow our guide on using the REPL.
First, enter the following:
In this command, require () loads the colors module at its entry point.
When you press ENTER you will get:
The REPL shows us the value of colors, which are all the functions and objects imported from the index.js file.
When you use the require keyword, Node.js returns all the contents within the exports object of a module.
Recall that you added getRandomColor () and allColors to exports in the colors module.
For that reason, you see them both in the REPL when they are imported.
At the prompt, test the getRandomColor () function:
You'll be prompted with a random color:
As the index is random, your output may vary.
Now that you confirmed that the colors module is working, exit the Node.js REPL:
This will return you to your terminal command line.
You have just confirmed that your module works as expected using the REPL.
Next, you will apply these same concepts and load your module into an application, as you would do in a real project.
Step 3 - Saving your Local Module as a Dependency
While testing your module in the REPL, you imported it with a relative path.
This means you used the location of the index.js file in relation to the working directory to get its contents.
While this works, it is usually a better programming experience to import modules by their names so that the import is not broken when the context is changed.
In this step, you will install the colors module with npm's local module install feature.
Set up a new Node.js module outside the colors folder.
First, go to the previous directory and create a new folder:
Now move into your new project:
Like with the colors module, initialize your folder with npm:
The following package.json will be generated:
Now, install your colors module and use the --save flag so it will be recorded in your package.json file:
You just installed your colors module in the new project.
Open the package.json file to see the new local dependency:
You will find that the following highlighted lines have been added:
Exit the file.
The colors module was copied to your node _ modules directory.
Verify it's there with the following command:
Use your installed local module in this new program.
Re-open your text editor and create another JavaScript file:
Your program will first import the colors module.
It will then choose a color at random using the getRandomColor () function provided by the module.
Finally, it will print a message to the console that tells the user what color to use.
Enter the following code in index.js:
Save and exit this file.
Your application will now tell the user a random color option for a website component.
Run this script with:
Your output will be similar to:
You've now successfully installed the colors module and can manage it like any other npm package used in your project.
However, if you added more colors and functions to your local colors module, you would have to run npm update in your applications to be able to use the new options.
In the next step, you will use the local module colors in another way and get automatic updates when the module code changes.
Step 4 - Linking a Local Module
If your local module is in heavy development, continually updating packages can be tedious.
An alternative would be to link the modules.
Linking a module ensures that any updates to the module are immediately reflected in the applications using it.
In this step, you will link the colors module to your application.
You will also modify the colors module and confirm that its most recent changes work in the application without having to reinstall or upgrade.
First, uninstall your local module:
npm links modules by using symbolic links (or symlinks), which are references that point to files or directories in your computer.
Linking a module is done in two steps:
Creating a global link to the module. npm creates a symlink between your global node _ modules directory and the directory of your module.
The global node _ modules directory is the location in which all your system-wide npm packages are installed (any package you install with the -g flag).
Create a local link. npm creates a symlink between your local project that's using the module and the global link of the module.
First, create the global link by returning to the colors folder and using the link command:
Once complete, your shell will output:
You just created a symlink in your node _ modules folder to your colors directory.
Return to the really-large-application folder and link the package:
You will receive output similar to the following:
< $> note Note: If you would like to type a bit less, you can use ln instead of link.
For example, npm ln colors would have worked the exact same way.
As the output shows, you just created a symlink from your really-large-application's local node _ modules directory to the colors symlink in your global node _ modules, which points to the actual directory with the colors module.
The linking process is complete.
Run your file to ensure it still works:
Your program functionality is intact.
Next, test that updates are immediately applied.
In your text editor, re-open the index.js file in the colors module:
Now add a function that selects the very best shade of blue that exists.
It takes no arguments, and always returns the third item of the allColors array.
Add these lines to the end of the file:
Save and exit the file, then re-open the index.js file in the really-large-application folder:
Make a call to the newly created getBlue () function, and print a sentence with the color's properties.
Add these statements to the end of the file:
The code now uses the newly create getBlue () function.
Execute the file as before:
You will get output like:
Your script was able to use the latest function in your colors module, without having to run npm update.
This will make it easier to make changes to this application in development.
As you write larger and more complex applications, think about how related code can be grouped into modules, and how you want these modules to be set up.
If your module is only going to be used by one program, it can stay within the same project and be referenced by a relative path.
If your module will later be shared separately or exists in a very different location from the project you are working on now, installing or linking might be more viable.
Modules in active development also benefit from the automatic updates of linking.
If the module is not under active development, using npm install may be the easier option.
In this tutorial, you learned that a Node.js module is a JavaScript file with functions and objects that can be used by other programs.
You then created a module and attached your functions and objects to the global exports object to make them available to external programs.
Finally, you imported that module into a program, demonstrating how modules come together into larger applications.
Now that you know how to create modules, think about the type of program you want to write and break it down into various components, keeping each unique set of activities and data in their own modules.
The more practice you get writing modules, the better your ability to write quality Node.js programs on your learning journey.
To work through an example of a Node.js application that uses modules, see our How To Set Up a Node.js Application for Production on Ubuntu 18.04 tutorial.
How To Install the OpenLiteSpeed Web Server on Ubuntu 18.04
3208
OpenLiteSpeed is an optimized open source web server that can be used to manage and serve sites.
OpenLiteSpeed has some useful features that make it a solid choice for many installations: it features Apache-compatible rewrite rules, a built-in web-based administration interface, and customized PHP processing optimized for the server.
In this guide, we'll demonstrate how to install and configure OpenLiteSpeed on an Ubuntu 18.04 server.
To complete this tutorial you will need an Ubuntu 18.04 server with a sudo-enabled, non-root user and the ufw firewall enabled.
Please refer to our Initial Server Setup with Ubuntu 18.04 tutorial for instructions on fulfilling these requirements.
Step 1 - Installing OpenLiteSpeed
OpenLiteSpeed provides a software repository we can use to download and install the server with Ubuntu's standard apt command.
To enable this repository for your Ubuntu system, first download and add the developer's software signing key:
This key is used to cryptographically verify that nobody has tampered with the software we're about to download.
Next, we add the repository information to our system:
After the repository has been added, the add-apt-repository command will refresh our package cache and the new software will be available to install.
Install the OpenLiteSpeed server and its PHP processor using apt install:
Finally, create a soft link to the PHP processor we just installed.
This directs the OpenLiteSpeed server to use the correct version:
Now that the OpenLiteSpeed server is installed, we'll secure it by updating the default admin account.
Step 2 - Setting the Administrative Password
Before we test the server, we should set a new administrative password for OpenLiteSpeed.
By default, the password is set to 123456, so we should change this immediately.
We can do this by running a script provided by OpenLiteSpeed:
You will be asked to provide a username for the administrative user.
If you press ENTER without choosing a new username, the default of admin will be used.
Then, you will be prompted to create and confirm a new password for the account.
Do so, then hit ENTER one last time.
The script will confirm a successful update:
Now that we've secured the admin account, let's test out the server and make sure it's running properly.
Step 3 - Starting and Connecting to the Server
OpenLiteSpeed should have started automatically after it was installed.
We can verify this using the lswsctrl command:
If you don't see a similar message, you can start the server using lswsctrl:
The server should now be running.
Before we can visit it in our browser, we need to open up some ports on our firewall.
We'll do this with the ufw command:
The first port, 8088, is the default port for OpenLiteSpeed's example site.
It should now be accessible to the public.
In your web browser, navigate to your server's domain name or IP address, followed by: 8088 to specify the port:
Your browser should load the default OpenLiteSpeed web page, which looks like this:
screenshot of the default OpenLiteSpeed demo page
The links towards the bottom of the page are designed to demonstrate various features of the server.
If you click through them you will notice that these features are already installed and properly configured.
For instance, an example CGI script is available, a customized PHP instance is up and running, and custom error pages and authentication gates are configured.
Click around to explore a little.
When you are satisfied with the default site, you can move on to the administrative interface.
In your web browser, using HTTPS, navigate to your server's domain name or IP address followed by: 7080 to specify the port:
You will likely see a page warning you that the SSL certificate from the server cannot be validated.
Since this is a self-signed certificate, this is expected.
Click through the available options to proceed to the site.
In Chrome, you must click "Advanced" and then "Proceed to...".
You will be prompted to enter the administrative username and password that you selected with the admpass.sh script in the previous step:
screenshot of the OpenLiteSpeed admin login page
Once you correctly authenticate, you will be presented with the OpenLiteSpeed administration interface:
screenshot of the OpenLiteSpeed admin dashboard
This is where the majority of your configuration for the web server will take place.
Next, we'll explore this interface by walking through a common configuration task: updating the port used by the default page.
Step 4 - Changing the Port for the Default Page
To demonstrate how to configure options through the web interface, we will change the port that the default site uses from 8088 to the conventional HTTP port 80.
To accomplish this, first click Listeners in the list of options on the left side of the interface.
A list of all available listeners will load.
In the list of listeners, click the "View / Edit" button for the Default listener:
screenshot of OpenLiteSpeed's listeners summary page
This will load a page with more details about the Default listener.
Click the edit button in the top-right corner of the "Address Settings" table to modify its values:
screenshot of OpenLiteSpeed's listener detail page
On the next screen, change port 8088 to port 80, then click the floppy disk icon, Save:
screenshot of OpenLiteSpeed's listener update interface
After the modification, you will need to restart the server.
Click the "reload" arrow icon to restart OpenLiteSpeed:
graceful restart button
Additionally, you'll need to now open up port 80 on your firewall:
The default web page should now be accessible in your browser on port 80 instead of port 8088.
Visiting your server's domain name or IP address without providing any port number will now display the site.
OpenLiteSpeed is a fully-featured web server that is primarily managed through the administrative web interface.
A full run through of how to configure your site through this interface is outside of the scope of this guide.
However, to get you started, we'll touch on a few important points below:
Everything associated with OpenLiteSpeed will be found under the / usr / local / lsws directory.
The document root (where your files will be served from) for the default virtual host is located at / usr / local / lsws / DEFAULT / html.
The configuration and logs for this virtual host can be found under the / usr / local / lsws / DEFAULT directory.
You can create new virtual hosts for different sites using the admin interface.
However, all of the directories that you will reference when setting up your configuration must be created ahead of time on your server.
OpenLiteSpeed is not able to create the directories
You can set up virtual host templates for virtual hosts that share the same general format.
Often, it is easiest to copy the default virtual host's directory structure and configuration to use as a starting point for new configurations.
The admin interface has a built-in tooltip help system for almost all fields.
There is also a Help menu option in the left-hand menu that links to the server documentation.
Consult these sources of information during configuration if you need more help.
To secure your OpenLiteSpeed installation with HTTPS, see the officall documentation "s section on SSL Setup
At this point, you should have OpenLiteSpeed and PHP installed and running on an Ubuntu 18.04 server.
OpenLiteSpeed offers great performance, a web-based configuration interface, and pre-configured options for script handling.
How To Install MySQL on CentOS 8
3941
MySQL is an open-source database management system, commonly installed as part of the popular LEMP (Linux, Nginx, MySQL / MariaDB, PHP / Python / Perl) stack.
It implements the relational model and Structured Query Language (SQL) to manage and query data.
This tutorial explains how to install MySQL version 8 on a CentOS 8 server.
To complete this tutorial, you will need a server running CentOS 8. This server should have a non-root user with administrative privileges and a firewall configured with firewalld.
To set this up, see our Initial Server Setup guide for CentOS 8.
Step 1 - Installing MySQL
On CentOS 8, MySQL version 8 is available from the default repositories.
Run the following command to install the mysql-server package and a number of its dependencies:
When prompted, press y and then ENTER to confirm that you want to proceed:
With that, MySQL is installed on your server but it isn't yet operational.
The package you just installed configures MySQL to run as a systemd service named mysqld.service.
In order to use MySQL, you will need to start it with the systemctl command:
To check that the service is running correctly, run the following command.
Note that for many systemctl commands - including start and, as shown here, status - you don't need to include .service after the service name:
If MySQL was successfully started, the output will show that the MySQL service is active:
Next, set MySQL to start whenever the server boots up with the following command:
< $> note Note: If you ever want to change this behavior and disable MySQL from starting up at boot, you can do so by running:
MySQL is now installed, running, and enabled on your server.
Next, we'll go over how to harden your database's security using a shell script that came preinstalled with your MySQL instance.
Step 2 - Securing MySQL
MySQL includes a security script that allows you to change some default configuration options in order to improve MySQL's security.
To use the security script, run the following command:
This will take you through a series of prompts asking if you want to make certain changes to your MySQL installation "s security options.
The first prompt will ask whether you "d like to set up the Validate Password Plugin, which you can use to test the strength of your MySQL password.
If you elect to set up the Validate Password Plugin, the script will ask you to choose a password validation level.
The strongest level - which you select by entering 2 - will require your password to be at least eight characters long and include a mix of uppercase, lowercase, numeric, and special characters:
Regardless of whether you choose to set up the Validate Password Plugin, the next prompt will be to set a password for the MySQL root user.
Enter and then confirm a secure password of your choice:
If you used the Validate Password Plugin, you'll receive feedback on the strength of your new password.
Then the script will ask if you want to continue with the password you just entered or if you want to enter a new one.
Assuming you're satisfied with the strength of the password you just entered, enter Y to continue the script:
Following that, you can press Y and then ENTER to accept the defaults for all the subsequent questions.
This will remove some anonymous users and the test database, disable remote root logins, and load these new rules so that MySQL immediately respects the changes you have made.
With that, you've installed and secured MySQL on your CentOS 8 server.
As a final step, we will test that the database is accessible and working as expected.
Step 3 - Testing MySQL
You can verify your installation and get information about it by connecting with the mysqladmin tool, a client that lets you run administrative commands.
Use the following command to connect to MySQL as root (-u root), prompt for a password (-p), and return the installation's version:
You will see output similar to this:
This indicates your installation was successful.
If you'd like to connect to MySQL and begin adding data to it, run the following:
Like the previous mysqladmin command, this command includes the -u option, which allows you to specify the user you'd like to connect as (root in this case), and the -p option, which tells the command to prompt you for the user password you set in the previous step.
After you enter your root MySQL user's password, you will see the MySQL prompt:
From there, you can begin using your MySQL installation to create and load databases and start running queries.
By following this tutorial, you've installed and secured MySQL on a CentOS 8 server.
From here, you could install Nginx and PHP to have a fully operational LEMP stack on your server.
To learn more about using MySQL, we encourage you to review the official documentation.
How To Configure Packet Filter (PF) on FreeBSD 12.1
4005
The firewall is arguably one of the most important lines of defense against cyber attacks.
The ability to configure a firewall from scratch is an empowering skill that enables the administrator to take control of their networks.
Packet Filter (PF) is a renown firewall application that is maintained upstream by the security-driven OpenBSD project.
It is more accurately expressed as a packet filtering tool, hence the name, and it is known for its simple syntax, user-friendliness, and extensive features.
PF is a stateful firewall by default, storing information about connections in a state table that can be accessed for analytical purposes.
PF is part of the FreeBSD base system and is supported by a strong community of developers.
Although there are differences between the FreeBSD and OpenBSD versions of PF related to kernel architectures, in general their syntax is similar.
Depending on their complexity, common rulesets can be modified to work on either distribution with relatively little effort.
In this tutorial you'll build a firewall from the ground up on a FreeBSD 12.1 server with PF.
You'll design a base ruleset that can be used as a template for future projects.
You'll also explore some of PF's advanced features such as packet hygiene, brute force prevention, monitoring and logging, and other third-party tools.
Before you start this tutorial, you'll need the following:
A 1G FreeBSD 12.1 server (either ZFS or UFS).
You can use our How To Get Started with FreeBSD tutorial to set your server up to your preferred configuration.
FreeBSD has no firewall enabled by default - customization is a hallmark of the FreeBSD ethos.
Therefore when you first launch your server, you need temporary protection while PF is being configured.
If you're using DigitalOcean, you can enable your cloud firewall immediately after spinning up the server.
Refer to DigitalOcean's Firewall Quickstart for instructions on configuring a cloud firewall.
If you're using another cloud provider, determine the fastest route to immediate protection before you begin.
Whichever method you choose, your temporary firewall must permit only inbound SSH traffic, and can allow all types of outbound traffic.
Step 1 - Building Your Preliminary Ruleset
You'll begin this tutorial by drafting a preliminary ruleset that provides basic protection and access to critical services from the internet.
At this point you have a running FreeBSD 12.1 server with an active cloud firewall.
There are two approaches to building a firewall: default deny and default permit.
The default deny approach blocks all traffic, and only permits what is specified in a rule.
The default permit approach does the exact opposite: it passes all traffic, and only blocks what is specified in a rule.
You'll use the default deny approach.
PF rulesets are written in a configuration file named / etc / pf.conf, which is also its default location.
It is OK to store this file somewhere else as long as it is specified in the / etc / rc.conf configuration file.
In this tutorial you'll use the default location.
Log in to your server with your non-root user:
Next create your / etc / pf.conf file:
< $> note Note: If you would like to see the complete base ruleset at any point in the tutorial, you can refer to the examples in Step 4 or Step 8. < $>
PF filters packets according to three core actions: block, pass, and match.
When combined with other options they form rules.
An action is taken when a packet meets the criteria specified in a rule.
As you may expect, pass and block rules will pass and block traffic.
A match rule performs an action on a packet when it finds a matching criteria, but doesn't pass or block it. For example, you can perform network address translation (NAT) on a matching packet without passing or blocking it, and it will sit there until you tell it to do something in another rule, such as route it to another machine or gateway.
Next add the first rule to your / etc / pf.conf file:
This rule blocks all forms of traffic in every direction.
Since it does not specify a direction, it defaults to both in and out. This rule is legitimate for a local workstation that needs to be insulated from the world, but it is largely impractical, and will not work on a remote server because it does not permit SSH traffic.
In fact, had you enabled PF, you would have locked yourself out of the server.
Revise your / etc / pf.conf file to allow SSH traffic with the following highlighted line:
< $> note Note: Alternatively, you can use the name of the protocol:
For the sake of consistency we will use port numbers, unless there is a valid reason not to.
There is a detailed list of protocols and their respective port numbers in the / etc / services file, which you are encouraged to view.
PF processes rules sequentially from top-to-bottom, therefore your current ruleset initially blocks all traffic, but then passes it if the criteria on the next line is matched, which in this case is SSH traffic.
You can now SSH in to your server, but you're still blocking all forms of outbound traffic.
This is problematic because you can't access critical services from the internet to install packages, update your time settings, and so on.
To address this, append the following highlighted rule to the end of your / etc / pf.conf file:
Your ruleset now permits outbound SSH, DNS, HTTP, NTP, and HTTPS traffic, as well as blocking all inward traffic, (with the exception of SSH).
You place the port numbers and protocols inside curly brackets, which forms a list in PF syntax, allowing you to add more port numbers if needed.
You also add a pass out rule for the UDP protocol on ports 53 and 123 because DNS and NTP often toggle between both the TCP and UDP protocols.
You're almost finished with the preliminary ruleset, and only need to add a couple of rules to achieve basic functionality.
Complete the preliminary ruleset with the highlighted rules:
You create a set skip rule for the loopback device because it does not need to filter traffic and would likely bring your server to a crawl.
You add a pass out inet rule for the ICMP protocol, which allows you to use the ping (8) utility for troubleshooting.
The inet option represents the IPv4 address family.
ICMP is a multi-purpose messaging protocol used by networking devices for various types of communication.
The ping utility for example uses a type of message known as an echo request, which you've added to your icmp _ type list.
As a precaution, you only permit the message types that you need to prevent unwelcome devices from contacting your server.
As your needs increase you can add more message types to your list.
You now have a working ruleset that provides basic functionality to most machines.
In the next section, let's confirm that everything is working correctly by enabling PF and testing your preliminary ruleset.
Step 2 - Testing Your Preliminary Ruleset
In this step you'll test your preliminary ruleset and make the transition from your cloud firewall to your PF firewall, allowing PF to completely take over.
You'll activate your ruleset with the pfctl utility, which is PF's built-in command-line tool, and the primary method of interfacing with PF.
PF rulesets are nothing more than text files, which means there are no delicate procedures involved with loading new rulesets.
You can load a new ruleset, and the old one is gone.
There is rarely, if ever, a need to flush an existing ruleset.
FreeBSD uses a web of shell scripts known as the rc system to manage how services are started at boot-time; we specify those services in various rc configuration files.
For global services such as PF, you use the / etc / rc.conf file.
Since rc files are critical to the well being of a FreeBSD system, they should not be edited directly.
Instead FreeBSD provides a command-line utility known as sysrc designed to help you edit these files safely.
Let's enable PF using the sysrc command-line utility:
Verify these changes by printing the contents of your / etc / rc.conf file:
You also enable the pflog service, which in turn, enables the pflogd daemon for logging in PF. (
You'll work with logging in a later step.
You specify two global services in your / etc / rc.conf file, but they won't initialize until you reboot the server or start them manually.
Reboot the server so that you can also test your SSH access.
Start PF by rebooting the server:
The connection will be dropped.
Give it a few minutes to update.
Now SSH back in to the server:
Although you've initialized your PF services, you haven't actually loaded your / etc / pf.conf ruleset, which means your firewall is not yet active.
Load the ruleset with pfctl:
If there are no errors or messages, it means your ruleset has no errors and the firewall is active.
Now that PF is running, you can detach your server from your cloud firewall.
This can be accomplished at the control panel in your DigitalOcean account by removing your Droplet from your cloud firewall's portal.
If you're using another cloud provider, ensure that whatever you are using for temporary protection is disabled.
Running two different firewalls on a server will almost certainly cause problems.
For good measure, reboot your server again:
After a few minutes, SSH back in to your server:
PF is now your acting firewall.
You can ensure that it is running by accessing some data with the pfctl utility.
Let's view some statistics and counters with pfctl -si:
You pass the -si flags, which stand for show info.
This is one of the many filter parameter combinations you can use with pfctl to parse data about your firewall activity.
You will see the following tabular data (the values will vary from machine-to-machine):
Since you just activated your ruleset, you won't see a lot of information yet.
However this output shows that PF already recorded 23 matched rules, meaning that the criteria of your ruleset was matched 23 times.
The output also confirms that your firewall is working.
Your ruleset also permits outbound traffic to access some critical services from the internet, including the ping utility.
Let's check for internet connectivity and DNS service with ping against google.com:
Since you ran the count flag -c 3, you'll see three successful connection responses:
Ensure that you can access the the pkgs repository with the following command:
If there are any packages to upgrade, go ahead and upgrade them.
If both of these services are working, it means your firewall is working and you can now proceed.
Although your preliminary ruleset provides protection and functionality, it is still an elementary ruleset, and could use some enhancements.
In the remaining sections you'll complete your base ruleset, and use some of PF's advanced features.
Step 3 - Completing Your Base Ruleset
In this step you'll build off of the preliminary ruleset to complete your base ruleset.
You'll reorganize some of your rules and work with more advanced concepts.
Incorporating Macros and Tables
In your preliminary ruleset you hard coded all of your parameters into each rule, that is, the port numbers that make up the lists.
This may become unmanageable in the future, depending on the nature of your networks.
For organizational purposes PF includes macros, lists, and tables.
You've already included lists directly in your rules, but you can also separate them from your rules and assign them to a variable using macros.
Open your file to transfer some of your parameters into macros:
Now add the following content to the very top of the ruleset:
Modify your previous SSH and ICMP rules with your new variables:
Your previous SSH and ICMP rules now use macros.
The variable names are denoted by PF's dollar sign syntax.
You assign your vtnet0 interface to a variable with the same name just as a formality, which gives you the option to rename it in the future if needed.
Other common variable names for public facing interfaces include $pub _ if or $ext _ if.
Next you'll implement a table, which is similar to a macro, but designed to hold groups of IP addresses.
Let's create a table for non-routable IP addresses, which often play a role in denial of service attacks (DOS).
You can use the IP addresses specified in RFC6890, which defines special-purpose IP address registries.
Your server should not send or receive packets to or from these addresses via the public facing interface.
Create this table by adding the following content directly under the icmp _ types macro:
Now add your rules for the < rfc6890 > table underneath the set skip on lo0 rule:
Here you introduce the return option, which complements your block out rule.
This will drop the packets and also send an RST message to the host that tried to make those connections, which is useful for analyzing host activity.
Then, you add the egress keyword, which automatically finds the default route (s) on any given interface (s).
This is typically a cleaner method of finding default routes, especially with complex networks.
The quick keyword executes rules immediately without considering the rest of the ruleset.
For example, if a packet with an illogical IP addresses tries to connect to the server, you want to drop the connection immediately, and have no reason to run that packet through the remainder of the ruleset.
Protecting Your SSH Ports
Since your SSH port is open to the public, it is subject to exploitation.
One of the more obvious warning signs of an attacker is mass quantities of log-in attempts.
For example if the same IP address tries to log in to your server ten times in one second, you can assume that it was not done with human hands, but with computer software that was trying to crack your login password.
These types of systematic exploits are often referred to as brute force attacks, and usually succeed if the server has weak passwords.
< $> warning Warning: We strongly recommend using public-key authentication on all servers.
Refer to DigitalOcean's tutorial on key-based authentication.
PF has built-in features for handling brute force and other similar attacks.
With PF you can limit the number of simultaneous connection attempts allowed by a single host.
If a host exceeds those limits, the connection will be dropped, and they will be banned from the server.
To accomplish this you'll use PF's overload mechanism, which maintains a table of banned IP addresses.
Modify your previous SSH rule to limit the number of simultaneous connections from a single host as per the following:
You add the keep state option that allows you to define the state criteria for the overload table.
You pass the max-src-conn parameter to specify the number of simultaneous connections allowed from a single host per second, and the max-src-conn-rate parameter to specify the number of new connections allowed from a single host per second.
You specify 15 connections for max-src-conn, and 3 connections for max-src-conn-rate.
If these limits are exceeded by a host, the overload mechanism adds the source IP to the < bruteforce > table, which bans them from the server.
Finally, the flush global option immediately drops the connection.
You've defined an overload table in your SSH rule, but haven't declared that table in your ruleset.
Add the < bruteforce > table underneath the icmp _ types macro:
The persist keyword allows an empty table to exist in the ruleset.
Without it, PF will complain that there are no IP addresses in the table.
These measures ensure that your SSH port is protected by a powerful security mechanism.
PF allows you to configure quick solutions to protect from disastrous forms of exploitation.
In the next sections you'll take steps to clean up packets as they arrive at your server.
Sanitizing Your Traffic
< $> note Note: The following sections describe basic fundamentals of the TCP / IP protocol suite.
If you plan on building web applications or networks, it is in your best interest to master these concepts.
Have a look at DigitalOcean's Introduction to Networking Terminology, Interfaces, and Protocols tutorial.
Due to the complexity of the TCP / IP protocol suite, and the perserverance of malicious actors, packets often arrive with discrepancies and ambiguities such as overlapping IP fragments, phony IP addresses, and more.
It is imperative that you sanitize your traffic before it enters the system.
The technical term for this process is normalization.
When data travels through the internet, it is typically broken up into smaller fragments at its source to accommodate for the transmission parameters of the target host, where it is reassembled into complete packets.
Unfortunately an intruder can hijack this process in a number of ways that span beyond the scope of this tutorial.
However, with PF you can manage fragmentation with one rule.
PF includes a scrub keyword that normalizes packets.
Add the scrub keyword directly preceding your block all rule:
This rule applies scrubbing to all incoming traffic.
You include the fragment reassemble option that prevents fragments from entering the system.
Instead they are cached in memory until they are reassembled into complete packets, which means your filter rules will only have to contend with uniform packets.
You also include the max-mss 1440 option, which represents the maximum segment size of reassembled TCP packets, also known as the payload.
You specify a value of 1440 bytes, which strikes a balance between size and performance, leaving plenty of room for the headers.
Another important aspect of fragmentation is a term known as the maximum transmission unit (MTU).
The TCP / IP protocols enable devices to negotiate packet sizes for making connections.
The target host uses ICMP messages to inform the source IP of its MTU, a process known as MTU path discovery.
The specific ICMP message type is the destination unreachable.
You'll enable MTU path discovery by adding the unreach message type to your icmp _ types list.
You'll use your server's default MTU of 1500 bytes, which can be determined with the ifconfig command:
You will see the following output that includes your current MTU:
Update the icmp _ types list to include the destination unreachable message type:
Now that you have policies in place to handle fragmentation, the packets that enter your system will be uniform and consistent.
This is desirable because there are so many devices exchanging data over the internet.
You'll now work to prevent another security concern known as IP spoofing.
Attackers often change their source IPs to make it appear as if they reside on a trusted node within an organization.
PF includes an antispoofing directive for handling spoofed source IPs.
When applied to a specific interface (s), antispoofing blocks all traffic from the network of that interface (unless it originates from that interface).
For example, if you apply antispoofing to an interface (s) that resides at 5.5.5.1 / 24, all traffic from the 5.5.5.0 / 24 network cannot communicate with the system unless it originates from that interface (s).
Add the following highlighted content to apply antispoofing to your vtnet0 interface:
This antispoofing rule says that all traffic from vtnet0's network (s) can only pass through the vtnet0 interface, or it will be dropped immediately with the quick keyword.
Bad actors will not be able to hide in vtnet0's network and communicate with other nodes.
To demonstrate your antispoofing rule, you'll print your ruleset to the screen in its verbose form. Rules in PF are typically written in a shortened form, but they can also be written in a verbose form. It is generally impractical to write rules this way, but for testing purposes it can be useful.
Print the contents of / etc / pf.conf using pfctl with the following command:
This pfctl command takes the -nvf flags, which print the ruleset and test it without actually loading anything, also known as a dry run.
You will now see the entire contents of / etc / pf.conf in its verbose form.
You'll see something similar to the following output within the antispoofing portion:
Your antispoofing rule discovered that it is part of the < ^ > your _ server _ ip < ^ > / 20 network.
It also detected that (for this tutorial's example) the server is part of a < ^ > network _ address < ^ > / 16 network, and has an additional IPv6 address.
Antispoofing blocks all of these networks from communicating with the system, unless their traffic passes through the vtnet0 interface.
Your antispoofing rule is the last addition to your base ruleset.
In the next step you'll initiate these changes and perform some testing.
Step 4 - Testing Your Base Ruleset
In this step you'll review and test your base ruleset to ensure that everything is functioning properly.
It's best to avoid implementing too many rules at once without testing them.
Best practice is to start with the essentials, expand incrementally, and back work up while making configuration changes.
Here is your complete base ruleset:
Be sure that your / etc / pf.conf file is identical to the complete base ruleset here before continuing.
Your complete base ruleset provides you with:
A collection of macros that can define key services and devices.
Network hygiene policies to address packet fragmentation and illogical IP addresses.
A default deny filtering structure that blocks everything and permits only what you specify.
Inbound SSH access with limits on the number of simultaneous connections that can be made by a host.
Outbound traffic policies that give you access to some critical services from the internet.
ICMP policies that provide access to the ping utility and MTU path discovery.
Run the following pfctl command to take a dry run:
You pass the -nf flags that tell pfctl to run the ruleset without loading it, which will throw errors if anything is wrong.
Now, with no encountered errors, load the ruleset:
If there are no errors, it means your base ruleset is active and functioning properly.
As earlier in the tutorial, you'll perform a few tests on your ruleset.
First test for internet connectivity and DNS service:
Then, check that you reach the pkgs repository:
Once again, upgrade packages if it's needed.
Finally, reboot your server:
Give your server a few minutes to reboot.
You've completed and implemented your base ruleset, which is a significant step in terms of your progress.
You're now ready to explore some of PF's advanced features.
In the next step you will continue to prevent brute force attacks.
Step 5 - Managing Your Overload Table
Over time the < bruteforce > overload table will become full of malicious IP addresses and will need to be cleared periodically.
It is unlikely that an attacker will continue using the same IP address, so it is counterintuitive to store them in the overload table for long periods of time.
You'll use pfctl to manually clear IP addresses that have been stored in the overload table for 48 hours or more with the following command:
You will see output similar to:
You pass the -t bruteforce flag, which stands for table bruteforce, and the -T flag, which lets you run a handful of built-in commands.
In this case you run the expire command to clear all entries from -t bruteforce with a time value represented in seconds.
Since you're working on a fresh server, there are probably no IP addresses in the overload table yet.
This rule works for quick fixes, but a more robust solution would be to automate the process with cron, FreeBSD's job scheduler.
Let's create a shell script that runs this command sequence instead.
Create a shell script file in the / usr / local / bin directory:
Add the following content to the shell script:
Make the file executable with the following command:
Next you'll create a cron job.
These are jobs that will run repetitively according to a time that you specify.
They are commonly used for backups, or any process that needs to run at the same time every day.
You create cron jobs with crontab files.
Please refer to the man pages to learn more about cron (8) and crontab (5).
Create a root user crontab file with the following command:
Now add the following contents to the crontab file:
< $> note Note: Please align every value to its corresponding table entry for readability if things do not align properly when you add the content.
This cron job runs the clear _ overload.sh script every day at midnight, removing IP addresses that are 48 hours old from the overload table < bruteforce >.
Next you'll add anchors to your ruleset.
Step 6 - Introducing Anchors to Your Rulesets
In this step you'll introduce anchors, which are used for sourcing rules into the main ruleset, either manually or from an external text file.
Anchors can contain rule snippets, tables, and even other anchors, known as nested anchors.
Let's demonstrate how anchors work by adding a table to an external file, and sourcing it into your base ruleset.
Your table will include a group of internal hosts that you want to prevent from connecting to the outside world.
Create a file named / etc / blocked-hosts-anchor:
Add the following contents to the file:
These rules declare and define the < blocked-hosts > table, and then prevent every IP address in the < blocked-hosts > table from accessing services from the outside world.
You use the egress keyword as a preferred method of finding the default route, or way out, to the internet.
You still need to declare the anchor in your / etc / pf.conf file:
Now add the following anchor rules after the block all rule:
These rules declare the blocked _ hosts and load the anchor rules into your main ruleset from the / etc / blocked-hosts-anchor file.
Now initiate these changes by reloading your ruleset with pfctl:
If there are no errors, it means that there are no errors in your ruleset and your changes are active.
Use pfctl to verify that your anchor is running:
The -s Anchors flag stands for "show anchors".
The pfctl utility can also parse the specific rules of your anchor with the -a and -s flags:
Another feature of anchors is that they allow you to add rules on-demand without having to reload the ruleset.
This can be useful for testing, quick-fixes, emergencies, and so on.
For example if an internal host is acting peculiar and you want to block it from making outward connections, you can have an anchor in place that allows you to intervene quickly from the command line.
Let's open / etc / pf.conf and add another anchor:
You'll name the anchor rogue _ hosts, and place it in the block all rule:
To initiate these changes, reload the ruleset with pfctl:
Once again, use pfctl to verify that the anchor is running:
This will generate the following output:
Now that the anchor is running, you can add rules to it at anytime.
Test this by adding the following rule:
This invokes the echo command and its string content, which is then piped into the pfctl utility with the | symbol, where it is processed into an anchor rule.
You open another shell session with the sh -c command.
This is because you establish a pipe between two processes, but need sudo privileges to persist throughout the entire command sequence.
There are multiple ways of resolving this; here you open an additional shell process with sudo privileges using sudo sh -c.
Now, use pfctl again to verify that these rules are active:
The use of anchors is completely situational and often subjective.
Like any other feature there are pros and cons to using anchors.
Some applications such as blacklistd interface with anchors by design.
Next you'll focus on logging with PF, which is a critical aspect of network security.
Your firewall is not useful if you can't see what it is doing.
Step 7 - Logging Your Firewall's Activity
In this step you'll work with PF logging, which is managed by a pseudo-interface named pflog.
Logging is enabled at boot-time by adding pflog _ enabled = YES to the / etc / rc.conf file, which you did in Step 2. This enables the pflogd daemon that brings up an interface named pflog0 and writes logs in binary format to a file named / var / log / pflog.
Logs can be parsed in realtime from the interface, or read from the / var / log / pflog file with the tcpdump (8) utility.
First access some logs from the / var / log / pflog file:
You pass the -ner flags that format the output for readability, and also specify a file to read from, which in your case is / var / log / pflog.
In these early stages there may not be any data in the / var / log / pflog file.
In a short period of time the log file will begin to grow.
You can also view logs in realtime from the pflog0 interface by using the following command:
You pass the -nei flags, which also format the output for readability, but this time specify an interface, which in your case is pflog0.
You will now see connections in realtime.
If possible, ping your server from a remote machine and you will see the connections occurring.
The server will remain in this state until you exit out of it.
To exit out of this state and return to the command line hit CTRL + Z.
There is a wealth of information on the internet about tcpdump (8), including the official website.
Accessing Log Files with pftop
The pftop utility is a tool for quickly viewing firewall activity in realtime.
Its name is influenced by the well-known Unix top utility.
To use it, you need to install the pftop package:
Now run the pftop binary:
This will generate the following output (your IPs will differ):
Creating Additional Log Interfaces
Like any other interface, multiple log interfaces can be created and named with a / etc / hostname file.
You may find this useful for organizational purposes, for example if you want to log certain types of activity separately.
Create an additional logging interface named pflog1:
Add the following contents to the / etc / hostname.pflog1 file:
Now enable the device at boot-time in your / etc / rc.conf file:
You can now monitor and log your firewall activity.
This allows you to see who is making connections to your server and the types of connections being made.
Throughout this tutorial you've incorporated some advanced concepts into your PF ruleset.
It's only necessary to implement advanced features as you need them.
That said, in the next step you'll revert back to the base ruleset.
Step 8 - Reverting Back to Your Base Ruleset
In this final section you'll revert back to your base ruleset.
This is a quick step that will bring you back to a minimalist state of functionality.
Open the base ruleset with the following command:
Delete the current ruleset in your file and replace it with the following base ruleset:
Reload the ruleset:
If there are no errors from the command, then there are no errors in your ruleset and your firewall is functioning properly.
You also need to disable the pflog1 interface that you created.
Since you might not know if you need it yet, you can disable pflog1 with the sysrc utility:
Now remove the / etc / hostname.pflog1 file from the / etc directory:
Before signing off, reboot the server once more to ensure that all of your changes are active and persistent:
Wait a few minutes before logging in to your server.
Optionally, if you would like to implement PF with a webserver, the following is a ruleset for this scenario.
This ruleset is a sufficient starting point for most web applications.
This creates an overload table named < webcrawlers >, which has a more liberal overload policy than your SSH port based on the values of max-src-conn 45 and max-src-conn-rate.
This is because not all overloads are from bad actors.
They also can originate from non-malicious netbots, so you avoid excessive security measures on ports 80 and 443. If you decide to implement the webserver ruleset, you need to add the < webcrawlers > table to / etc / pf.conf, and clear the IPs from the table periodically.
Refer to Step 5 for this.
In this tutorial, you configured PF on FreeBSD 12.1.
You now have a base ruleset that can serve as a starting point for all of your FreeBSD projects.
For further information on PF take a look at the pf.conf (5) man pages.
Visit our FreeBSD topic page for more tutorials and Q & A.
How To Create a New Sudo-enabled User on Ubuntu 18.04 Quickstart
3991
The sudo command provides a mechanism for granting administrator privileges - ordinarily only available to the root user - to normal users.
This guide will show you how to create a new user with sudo access on Ubuntu 18.04, without having to modify your server's / etc / sudoers file.
If you want to configure sudo for an existing user, skip to step 3.
Step 1 - Logging Into Your Server
SSH in to your server as the root user:
Step 2 - Adding a New User to the System
Use the adduser command to add a new user to your system:
Be sure to replace < ^ > sammy < ^ > with the user name that you want to create.
You will be prompted to create and verify a password for the user:
Next you'll be asked to fill in some information about the new user.
It is fine to accept the defaults and leave all of this information blank:
Step 3 - Adding the User to the sudo Group
Use the usermod command to add the user to the sudo group:
Again, be sure to replace < ^ > sammy < ^ > with the username you just added.
By default, on Ubuntu, all members of the sudo group have full sudo privileges.
Step 4 - Testing sudo Access
To test that the new sudo permissions are working, first use the su command to switch to the new user account:
As the new user, verify that you can use sudo by prepending sudo to the command that you want to run with superuser privileges:
For example, you can list the contents of the / root directory, which is normally only accessible to the root user:
The first time you use sudo in a session, you will be prompted for the password of that users account.
Enter the password to proceed:
< $> note Note: This is not asking for the root password!
Enter the password of the sudo-enabled user, not a root password.
If your user is in the proper group and you entered the password correctly, the command that you issued with sudo will run with root privileges.
In this quickstart tutorial we created a new user account and added it to the sudo group to enable sudo access.
For more detailed information on setting up an Ubuntu 18.04 server, please read our Initial Server Setup with Ubuntu 18.04 tutorial.
How To Install and Configure SimpleSAMLphp for SAML Authentication on Ubuntu 18.04
4008
SimpleSAMLphp is an open-source PHP authentication application that provides support for SAML 2.0 as a Service Provider (SP) or Identity Provider (IdP).
SAML (Security Assertion Markup Language) is a secure XML-based communication mechanism for exchanging authentication and authorization data between organizations and applications.
It "s often used to implement Web SSO (Single Sign On).
This eliminates the need to maintain multiple authentication credentials across multiple organizations.
Simply put, you can use one identity, like a username and password, to access multiple applications.
An instance of SimpleSAMLphp connects to an authentication source, which is an identity provider like LDAP or a database of users.
It authenticates users against this authentication source before granting access to resources made available from linked Service Providers.
In this tutorial you'll install SimpleSamlPHP and configure it to use a MySQL database as an authentication source.
You'll store users and encrypted passwords in the MySQL database and test that you can use those users to log in.
One Ubuntu 18.04 server set up by following the Ubuntu 18.04 initial server setup guide, including a sudo non-root user and a firewall.
Apache, MySQL, and PHP installed on the server by following How To Install Linux, Apache, MySQL, PHP (LAMP stack) on Ubuntu 18.04.
A domain name configured to point to your server.
You can learn how to point domains to DigitalOcean Droplets by following the How To Point to DigitalOcean Nameservers From Common Domain Registrars tutorial.
A Virtual Host configured for the domain using the ServerName directive.
Follow How To Set Up Apache Virtual Hosts on Ubuntu 18.04 to set one up for your domain name.
A Let's Encrypt certificate set up for the domain you've configured by following the How To Secure Apache with Let's Encrypt on Ubuntu 18.04 guide.
Step 1 - Downloading and Installing SimpleSAMLphp
Installing SimpleSAMLphp involves a couple of steps.
We have to download the software itself as well as a few additional components and prerequisites.
We'll also need to make some changes to our Virtual Host configuration.
Log in to your server if you're not logged in already.
Download SimpleSAMLphp from the project "s website.
SimpleSAMLphp always links the latest stable version of their software to the same URL.
This means we can get the latest version by typing this:
This will download a compressed file called download? latest which contains SimpleSAMLphp.
Extract the contents with the tar command:
The files will be extracted to a new directory labeled simplesamlphp-1. < ^ > x.y < ^ >, where < ^ > x.y < ^ > is the current version number.
Use the ls command to identify the file:
You'll see the filename displayed:
Now, copy the contents of the directory to / var / simplesamlphp using the cp command.
Be sure to replace the version number with the version you have:
The -a switch ensures that the file permissions are copied along with the files and folders.
The dot at the end of the source file ensures everything in the source directory including hidden files gets copied to the destination directory.
< $> note Note: If you need to install the files in a different location, you'll need to update several files.
Refer to SimpleSAMLphp's official installation documentation for specifics.
There are a few additional software packages SimpleSAMLphp needs, including PHP extensions to work with XML, multi-byte strings, curl, and LDAP.
It also requires memcached.
Install these using your package manager.
First, update your package list:
Then install the packages:
Once the installation completes, restart Apache to activate the new PHP extensions:
Now that SimpleSAMLphp is installed, let's configure Apache to serve the files.
Step 2 - Configuring Apache to Serve SimpleSAMLphp
You've already configured a domain and pointed at this server, and you've set up a Virtual Host to work with HTTPS by securing Apache with Let's Encrypt.
Let's use that to serve SimpleSAMLphp.
The only SimpleSAMLphp directory that needs to be visible to the web is / var / simplesamlphp / www.
To expose it to the web, edit the Virtual Host SSL Apache configuration file for your domain.
If your Virtual Host config file is named < ^ > your _ domain < ^ > .conf, Let's Encrypt created a new config file called < ^ > your _ domain < ^ > -le-ssl.conf that handles HTTPS requests for your domain.
Open the SSL config file with the following command to edit the file.
Be sure to replace < ^ > your _ domain < ^ > with the actual name of the file:
The file should look like the following, although the actual file may have more descriptive comments:
The ServerName directive here defines the base domain that should match for this virtual host definition.
This should be the domain name you set up an SSL certificate for in the Prerequisites section.
Let's add an Alias directive that gives control to SimpleSAMLphp for all URLs matching https: / / < ^ > your _ domain < ^ > / simplesaml / *.
Do that by adding the following line to the config file:
This means all URLs matching < ^ > domain _ name < ^ > / simplesaml / * will be directed to the / var / simplesamlphp / www directory giving SimpleSAMLphp control.
Next, we'll grant access to the / var / simplesamlphp / www directory by specifying a Require all granted access control for it. This will make the SimpleSAMLphp service accessible over the Web.
Do that by adding the following to the config file:
Restart Apache for the changes to take effect:
Now that Apache is configured to serve the application files, let's configure SimpleSAMLphp.
Step 3 - Configuring SimpleSAMLphp
Next, we need to make several changes to the core SimpleSAMLphp configuration located at / var / simplesamlphp / config / config.php.
Open the file in your editor:
Set the administrator password by locating the 'auth.adminpassword' line and replacing the default value of < ^ > 123 < ^ > with a more secure password.
This password lets you access some of the pages in your SimpleSAMLphp installation web interface:
Next, set a secret salt, which should be a randomly-generated string of characters.
Some parts of SimpleSAMLphp use this salt to create cryptographically secure hashes.
You'll get errors if the salt isn't changed from the default value.
You can use the OpenSSL rand function to generate a random string to use as your secret salt string.
Open a new terminal, connect to your server again, and run the following command to generate this string:
The -base64 32 option ensures a Base64 encoded string that is 32 characters long.
Then, in the configuration file, locate the 'secretsalt' entry and replace < ^ > defaultsecretsalt < ^ > with the string you generated:
Then set the technical contact information.
This information will be available in the generated metadata, and SimpleSAMLphp will send automatically-generated error reports to the email address you specify.
Locate the following section:
Replace < ^ > Administrator < ^ > and < ^ > na @ example.org < ^ > with appropriate values.
Then set the timezone you would like to use.
Locate this section:
Replace < ^ > null < ^ > with a preferred time zone from this list of timezones for PHP.
Be sure to enclose the value in quotes:
You should now be able to access the site in your browser by visiting https: / / < ^ > your _ domain < ^ > / simplesaml.
You'll see the following screen in your browser:
simplesaml web interface
To make sure your PHP installation meets all requirements for SimpleSAMLphp to run smoothly, select the Configuration tab and click on the Login as administrator link.
Then use the administrator password you set in the configuration file in Step 3.
Once logged in, you'll see a list of required and optional PHP extensions used by SimpleSAMLphp.
Check that you have installed every extension except predis / predis:
All extensions installed
If there are any required components missing, review this tutorial and install the missing components before you move on.
You'll also see a link that says Sanity check of your SimpleSAMLphp setup.
Click this link to get a list of checks applied to your setup to see whether they are successful.
Let's move on to configure an authentication source for for SimpleSAMLphp.
Step 4 - Configuring the Authentication Source
Now that we have SimpleSAMLphp installed and set up, let's configure an authentication source so we can authenticate users.
We will use a MySQL database to store a list of usernames and passwords to authenticate against.
To get started, log in to the MySQL root account:
You will be prompted for the MySQL root account password.
Provide it to proceed.
Next, create a database that will act as the authentication source.
We'll call it auth.
Feel free to name yours differently:
Now let's create a separate MySQL user to exclusively operate on our auth database.
From a management and security standpoint, it is a good practice to create one-function databases and accounts.
We will name our user authuser.
Execute the following command to create the user, set a password, and grant it access to our auth database.
Remember to provide a strong password here for your new database user.
Now create a users table, which will be made up of two fields: username and password.
For some additional security, we are going to use the MySQL AES _ ENCRYPT () function to encrypt the password string so we don't store the passwords in plain text.
This function encrypts a string and returns a binary string.
Then insert three users into the newly created table.
This is where we'll use the AES _ ENCRYPT () function to encrypt the values for the password field.
You need to provide a string that's used as an encryption key.
Make sure to replace this with your own string, which can be any string you'd like, as long as it's complex.
Use the same key for each user, and be sure to remember the key so you can use it again to create additional users in the future.
You'll also use this secret key in the SimpleSAMLphp configuration so you can decrypt the passwords and compare them with the ones users enter.
We need to flush the privileges so that the current instance of MySQL knows about the recent privilege changes we've made:
Exit out of the MySQL prompt by typing:
To enable the identity provider functionality in SimpleSAMLphp, we need to edit the / var / simplesamlphp / config / config.php file.
There are several options available but since this guide focuses on SAML 2.0 support, we want to enable the enable.saml20-idp option.
To do that, open the / var / simplesamlphp / config / config.phpand enable SAML 2.0 support:
Locate this section of the file and replace false with true.:
Then save the file and exit the editor.
Now that we have the identity provider functionality enabled, we need to indicate the authentication module to be used.
Since we have a users table on a MySQL database, we are going to use the SQL Authentication Module.
Open the authsources configuration file:
Locate the following block, which is commented out:
This code defines a database connection and a query that SimpleSAMLphp can use to look up a user in a database table called users.
We need to uncomment it and change the query to look up a user from our table using MySQL's AES _ DECRYPT () function.
We'll need to provide the AES _ DECRYPT () function the same key we used to encrypt the passwords in the query.
Modify the section of the file to specify the database connection details and the query:
Be sure to place the secret key you specified in place of < ^ > your _ secret _ key < ^ >.
Let's test our identity provider.
Step 5 - Testing the Identity Provider with the SAML 2.0 SP Demo
You can test the MySQL authentication source you just set up by navigating to the Authentication tab and clicking on the Test configured authentication sources link.
You will be presented with a list of authentication sources already configured.
The list of configured authentication sources
Click example-sql, as this is the provider you configured in the previous step.
A prompt to enter a username and password will appear.
Enter any of the three test user and password combinations you inserted in the MySQL users table.
Try user1 with the password user1pass.
With a successful attempt, you will be presented with the SAML 2.0 SP Demo Example page:
The successful Demo page
If you're unable to log in and you know the password is correct, ensure that you used the same key with both the AES _ ENCRYPT () function when you created the user, and the AES _ DECRYPT () function when you looked up the user.
You can now integrate SimpleSAMLphp with your own applications by following the SimpleSAMLphp API documentation.
You now have the SimpleSAMLphp application appropriately installed and configured on your Ubuntu 18.04 VPS.
SimpleSAMLphp also allows for extensive user interface customization through theming.
You can refer to their theming docs for more on that.
How To Install MariaDB on Ubuntu 18.04
3996
MariaDB is an open-source database management system, commonly used as an alternative for the MySQL portion of the popular LAMP (Linux, Apache, MySQL, PHP / Python / Perl) stack.
It is intended to be a drop-in replacement for MySQL.
The short version of this installation guide consists of these three steps:
Update your package index using apt
Install the mariadb-server package using apt.
The package also pulls in related tools to interact with MariaDB
Run the included mysql _ secure _ installation security script to restrict access to the server
This tutorial will explain how to install MariaDB on an Ubuntu 18.04 server, and verify that it is running and has a safe initial configuration.
To follow this tutorial, you will need:
One Ubuntu 18.04 server set up by following this initial server setup guide, including a non-root user with sudo privileges and a firewall.
Step 1 - Installing MariaDB
On Ubuntu 18.04, MariaDB version 10.1 is included in the APT package repositories by default.
To install it, update the package index on your server with apt:
Then install the package:
These commands will install MariaDB, but will not prompt you to set a password or make any other configuration changes.
Because the default configuration leaves your installation of MariaDB insecure, we will use a script that the mariadb-server package provides to restrict access to the server and remove unused accounts.
Step 2 - Configuring MariaDB
For new MariaDB installations, the next step is to run the included security script.
This script changes some of the less secure default options.
We will use it to block remote root logins and to remove unused database users.
Run the security script:
This will take you through a series of prompts where you can make some changes to your MariaDB installation "s security options.
The first prompt will ask you to enter the current database root password.
Since we have not set one up yet, press ENTER to indicate "none".
The next prompt asks you whether you'd like to set up a database root password.
Type N and then press ENTER.
On Ubuntu, the root account for MariaDB is tied closely to automated system maintenance, so we should not change the configured authentication methods for that account.
Doing so would make it possible for a package update to break the database system by removing access to the administrative account.
Later, we will cover how to optionally set up an additional administrative account for password access if socket authentication is not appropriate for your use case.
From there, you can press Y and then ENTER to accept the defaults for all the subsequent questions.
This will remove some anonymous users and the test database, disable remote root logins, and load these new rules so that MariaDB immediately implements the changes you have made.
Step 3 - (Optional) Adjusting User Authentication and Privileges
On Ubuntu systems running MariaDB 10.1, the root MariaDB user is set to authenticate using the unix _ socket plugin by default rather than with a password.
This allows for some greater security and usability in many cases, but it can also complicate things when you need to allow an external program (e.g., phpMyAdmin) administrative rights.
Because the server uses the root account for tasks like log rotation and starting and stopping the server, it is best not to change the root account's authentication details.
Changing credentials in the / etc / mysql / debian.cnf configuration file may work initially, but package updates could potentially overwrite those changes.
Instead of modifying the root account, the package maintainers recommend creating a separate administrative account for password-based access.
To do so, we will create a new account called admin with the same capabilities as the root account, but configured for password authentication.
To do this, open up the MariaDB prompt from your terminal:
Now, we will create a new user with root privileges and password-based access.
Change the username and password to match your preferences:
Flush the privileges to ensure that they are saved and available in the current session:
Following this, exit the MariaDB shell:
Finally, let's test the MariaDB installation.
Step 4 - Testing MariaDB
When installed from the default repositories, MariaDB should start running automatically.
To test this, check its status.
You'll receive output that is similar to the following:
If MariaDB isn't running, you can start it with the command sudo systemctl start mariadb.
For an additional check, you can try connecting to the database using the mysqladmin tool, which is a client that lets you run administrative commands.
For example, this command says to connect to MariaDB as root and return the version using the Unix socket:
You should receive output similar to this:
If you configured a separate administrative user with password authentication, you could perform the same operation by typing:
This means that MariaDB is up and running and that your user is able to authenticate successfully.
In this guide you installed MariaDB to act as an SQL server.
During the installation process you also secured the server.
Optionally, you also created a separate password-authenticated administrative user.
Now that you have a running and secure MariaDB server, here some examples of next steps that you can take to work with the server:
Import and export databases
You can also incorporate MariaDB into a larger application stack:
How To Install Linux, Nginx, MariaDB, PHP (LEMP stack) on Ubuntu 18.04
How To Install Software on Kubernetes Clusters with the Helm 3 Package Manager
4012
Helm is a package manager for Kubernetes that allows developers and operators to more easily configure and deploy applications on Kubernetes clusters.
In this tutorial, you will set up Helm 3 and use it to install, reconfigure, rollback, and delete an instance of the Kubernetes Dashboard application.
The dashboard is an official web-based Kubernetes GUI.
For a conceptual overview of Helm and its packaging ecosystem, please read our article, An Introduction to Helm.
For this tutorial you will need:
A Kubernetes cluster with role-based access control (RBAC) enabled.
Helm 3.1 supports clusters from versions 1.14 to 1.17.
For further information check the Helm releases page.
The kubectl command-line tool installed on your local machine, configured to connect to your cluster.
You can read more about installing kubectl in the official documentation.
You can test your connectivity with the following command:
If you see no errors, you're connected to the cluster.
If you access multiple clusters with kubectl, be sure to verify that you've selected the correct cluster context:
In this example the asterisk (< ^ > * < ^ >) indicates that we are connected to the do-fra1-helm3-example cluster.
To switch clusters run:
When you are connected to the correct cluster, continue to Step 1 to begin installing Helm.
Step 1 - Installing Helm
First, you'll install the helm command-line utility on your local machine.
Helm provides a script that handles the installation process on MacOS, Windows, or Linux.
Change to a writable directory and download the script from Helm's GitHub repository:
Make the script executable with chmod:
You can use your favorite text editor to open the script and inspect it to make sure it "s safe.
When you are satisfied, run it:
You may be prompted for your password.
Provide it and press ENTER to continue.
The output will look like this:
Now that you've got Helm installed, you're ready to use Helm to install your first chart.
Step 2 - Installing a Helm Chart
Helm software packages are called charts.
There is a curated chart repository called stable, mostly consisting of common charts, which you can see in their GitHub repo.
Helm does not come preconfigured for it, so you'll need to manually add it. Then, as an example, you are going to install the Kubernetes Dashboard.
Add the stable repo by running:
Then, use helm to install the kubernetes-dashboard package from the stable repo:
The --set parameter lets you to customize chart variables, which the chart exposes to allow you to customize its configuration.
Here, you set the rbac.clusterAdminRole variable to true to grant the Kubernetes Dashboard access to your whole cluster.
The output will look like:
Notice the NAME line, highlighted in the above example output.
In this case, you specified the name dashboard-demo.
This is the name of the release.
A Helm release is a single deployment of one chart with a specific configuration.
You can deploy multiple releases of the same chart, each with its own configuration.
You can list all the releases in the cluster:
You can now use kubectl to verify that a new service has been deployed on the cluster:
Notice that by default, the service name corresponding to the release is a combination of the Helm release name and the chart name.
Now that you've deployed the application, you'll use Helm to change its configuration and update the deployment.
Step 3 - Updating a Release
The helm upgrade command can be used to upgrade a release with a new or updated chart, or update its configuration options (variables).
You're going to make a simple change to the dashboard-demo release to demonstrate the update and rollback process: you'll update the name of the dashboard service to just kubernetes-dashboard, instead of dashboard-demo-kubernetes-dashboard.
The kubernetes-dashboard chart provides a fullnameOverride configuration option to control the service name.
To rename the release, run helm upgrade with this option set:
By passing in the --reuse-values argument, you make sure that chart variables you've previously set do not get reset by the upgrade process.
You'll see output similar to the initial helm install step.
Check if your Kubernetes services reflect the updated values:
The output will look like the following:
Notice that the service name has been updated to the new value.
< $> note Note: At this point you may want to actually load the Kubernetes Dashboard in your browser and check it out. To do so, first run the following command:
This creates a proxy that lets you access remote cluster resources from your local computer.
Based on the previous instructions, your dashboard service is named kubernetes-dashboard and it's running in the default namespace.
You may now access the dashboard at the following URL:
Instructions for actually using the dashboard are out of scope for this tutorial, but you can read the official Kubernetes Dashboard docs for more information.
Next, you'll have a look at Helm's ability to roll back and delete releases.
Step 4 - Rolling Back and Deleting a Release
When you updated the < ^ > dashboard-demo < ^ > release in the previous step, you created a second revision of the release.
Helm retains all the details of previous releases in case you need to roll back to a prior configuration or chart.
Use helm list to inspect the release again:
The REVISION column tells you that this is now the second revision.
Use helm rollback to roll back to the first revision:
You should see the following output, indicating that the rollback succeeded:
At this point, if you run kubectl get services again, you will notice that the service name has changed back to its previous value.
Helm has re-deployed the application with revision 1's configuration.
Helm releases can be deleted with the helm delete command:
You can try listing Helm releases:
You'll see that there are none:
Now the release has been truly deleted, and you can reuse the release name.
In this tutorial, you installed the helm command-line tool and explored installing, upgrading, rolling back, and deleting Helm charts and releases by managing the kubernetes-dashboard chart.
For more information about Helm and Helm charts, please see the official Helm documentation.
How To Run Multiple PHP Versions on One Server Using Apache and PHP-FPM on Ubuntu 18.04
4003
The Apache web server uses virtual hosts to manage multiple domains on a single instance.
Similarly, PHP-FPM uses a daemon to manage multiple PHP versions on a single instance.
Together, you can use Apache and PHP-FPM to host multiple PHP web-applications, each using a different version of PHP, all on the same server, and all at the same time.
This is useful because different applications may require different versions of PHP, but some server stacks, like a regularly configured LAMP stack, can only manage one.
Combining Apache with PHP-FPM is also a more cost-efficient solution than hosting each application on its own instance.
PHP-FPM also offers configuration options for stderr and stdout logging, emergency restarts, and adaptive process spawning, which is useful for heavy-loaded sites.
In fact, using Apache with PHP-FPM is one of the best stacks for hosting PHP applications, especially when it comes to performance.
In this tutorial you will set up two PHP sites on a single instance.
Each site will use its own domain, and each domain will deploy its own version of PHP.
The first, < ^ > site1.your _ domain < ^ >, will deploy PHP 7.0.
The second, < ^ > site2.your _ domain < ^ >, will deploy PHP 7.2.
One Ubuntu 18.04 server with at least 1GB of RAM set up by following the Initial Server Setup with Ubuntu 18.04, including a sudo non-root user and a firewall.
An Apache web server set up and configured by following How to Install the Apache Web Server on Ubuntu 18.04.
A domain name configured to point to your Ubuntu 18.04 server.
You can learn how to point domains to DigitalOcean Droplets by following How To Point to DigitalOcean Nameservers From Common Domain Registrars.
For the purposes of this tutorial, we will use two subdomains, each specified with an A record in our DNS settings: < ^ > site1.your _ domain < ^ > and < ^ > site2.your _ domain < ^ >.
Step 1 - Installing PHP Versions 7.0 and 7.2 with PHP-FPM
With the prerequisites completed, you will now install PHP versions 7.0 and 7.2, as well as PHP-FPM and several additional extensions.
But to accomplish this, you will first need to add the Ondrej PHP repository to your system.
Execute the apt-get command to install software-properties-common:
The software-properties-common package provides the apt-add-repository command-line utility, which you will use to add the ondrej / php PPA (Personal Package Archive) repository.
Now add the ondrej / php repository to your system.
The ondrej / php PPA will have more up-to-date versions of PHP than the official Ubuntu repositories, and it will also allow you to install multiple versions of PHP in the same system:
Update the repository:
Next, install php7.0, php7.0-fpm, php7.0-mysql, libapache2-mod-php7.0, and libapache2-mod-fcgid with the following commands:
php7.0 is a metapackage used to run PHP applications.
php7.0-fpm provides the Fast Process Manager interpreter that runs as a daemon and receives Fast / CGI requests.
php7.0-mysql connects PHP to the MySQL database.
libapahce2-mod-php7.0 provides the PHP module for the Apache webserver.
libapache2-mod-fcgid contains a mod _ fcgid that starts a number of CGI program instances to handle concurrent requests.
Now repeat the process for PHP version 7.2.
Install php7.2, php7.2-fpm, php7.2-mysql, and libapache2-mod-php7.2:
After installing both PHP versions, start the php7.0-fpm service:
Next, verify the status of the php7.0-fpm service:
Repeating this process, now start the php7.2-fpm service:
And verify the status of the php7.2-fpm service:
Lastly, you must enable several modules so that your Apache2 service can work with multiple PHP versions:
actions is used for executing CGI scripts based on media type or request method.
fcgid is a high performance alternative to mod _ cgi that starts a sufficient number instances of the CGI program to handle concurrent requests.
alias provides for the mapping of different parts of the host filesystem in the document tree, and for URL redirection.
proxy _ fcgi allows Apache to forward requests to PHP-FPM.
Now restart the Apache service to apply your changes:
At this point you have installed two PHP versions on your server.
Next, you will create a directory structure for each website you want to deploy.
Step 2 - Creating Directory Structures for Both Websites
In this section, you will create a document root directory and an index page for each of your two websites.
First, create document root directories for both < ^ > site1.your _ domain < ^ > and < ^ > site2.your _ domain < ^ >:
By default, the Apache webserver runs as a www-data user and www-data group.
To ensure that you have the correct ownership and permissions of your website root directories, execute the following commands:
Next you will create an info.php file inside each website root directory.
This will display each website's PHP version information.
Begin with < ^ > site1 < ^ >:
Add the following line:
Now copy the info.php file you created to < ^ > site2 < ^ >:
Your web server should now have the document root directories that each site requires to serve data to visitors.
Next, you will configure your Apache web server to work with two different PHP versions.
Step 3 - Configuring Apache for Both Websites
In this section, you will create two virtual host configuration files.
This will enable your two websites to work simultaneously with two different PHP versions.
In order for Apache to serve this content, it is necessary to create a virtual host file with the correct directives.
Instead of modifying the default configuration file located at / etc / apache2 / sites-available / 000-default.conf, you'll create two new ones inside the directory / etc / apache2 / sites-available /.
First create a new virtual host configuration file for the website < ^ > site1.your _ domain < ^ >.
Here you will direct Apache to render content using php7.0:
Add the following content.
Make sure the website directory path, server name, and PHP version match your setup:
In this file you updated the DocumentRoot to your new directory and ServerAdmin to an email that the your _ domain site administrator can access.
You "ve also updated ServerName, which establishes the base domain for this virtual host configuration, and you've added a SetHandler directive to run PHP as a fastCGI process server.
Next, create a new virtual host configuration file for the website < ^ > site2.your _ domain < ^ >.
You will specify this subdomain to deploy php7.2:
Again, make sure the website directory path, server name, and PHP version match your unique information:
Then check the Apache configuration file for any syntax errors:
Next, enable both virtual host configuration files:
Now disable the default site, since you won't need it.:
Finally, restart the Apache service to implement your changes:
Now that you have configured Apache to serve each site, you will test them to make sure the proper PHP versions are running.
Step 4 - Testing Both Websites
At this point, you have configured two websites to run two different versions of PHP.
Now test the results.
Open your web browser and visit both sites http: / / < ^ > site1.your _ domain < ^ > and http: / / < ^ > site2.your _ domain < ^ >.
You will see two pages that look like this:
PHP 7.0 info page PHP 7.2 info page
Note the titles.
The first page indicates that < ^ > site1.your _ domain < ^ > deployed PHP version 7.0.
The second indicates that < ^ > site2.your _ domain < ^ > deployed PHP version 7.2.
Now that you've tested your sites, remove the info.php files.
Because they contain sensitive information about your server and are accessible to unauthorized users, they pose a security threat.
To remove both files, run the following commands:
You now have a single Ubuntu 18.04 server handling two websites with two different PHP versions.
PHP-FPM, however, is not limited to this one application.
You have now combined virtual hosts and PHP-FPM to serve multiple websites and multiple versions of PHP on a single server.
The only practical limit on the number of PHP sites and PHP versions that your Apache service can handle is the processing power of your instance.
From here you might consider exploring PHP-FPM's more advanced features, like its adaptive spawning process or how it can log sdtout and stderr.
Alternatively, you could now secure your websites.
To accomplish this, you can follow our tutorial on how to secure your sites with free TLS / SSL certificates from Let's Encrypt.
How To Set Up a Firewall Using firewalld on CentOS 8
4018
firewalld is firewall management software available for many Linux distributions, which acts as a frontend for Linux's in-kernel nftables or iptables packet filtering systems.
In this guide, we will show you how to set up a firewalld firewall for your CentOS 8 server, and cover the basics of managing the firewall with the firewall-cmd administrative tool.
To complete this tutorial, you will need a server running CentOS 8. We will assume you are logged into this server as a non-root, sudo-enabled user.
To set this up, see our Initial Server Setup for CentOS 8 guide.
Basic Concepts in firewalld
Before we begin talking about how to actually use the firewall-cmd utility to manage your firewall configuration, we should get familiar with a few concepts that the tool introduces.
Zones
The firewalld daemon manages groups of rules using entities called zones.
Zones are sets of rules that dictate what traffic should be allowed depending on the level of trust you have in the network.
Network interfaces are assigned to a zone to dictate the behavior that the firewall should allow.
For computers that might move between networks frequently (like laptops), this kind of flexibility provides a good method of changing your rules depending on your environment.
You may have strict rules in place prohibiting most traffic when operating on a public WiFi network, while allowing more relaxed restrictions when connected to your home network.
For a server, these zones are often not as important because the network environment rarely, if ever, changes.
Regardless of how dynamic your network environment may be, it is still useful to be familiar with the general idea behind each of the predefined zones for firewalld.
The predefined zones within firewalld are, in order from least trusted to most trusted:
drop: The lowest level of trust.
All incoming connections are dropped without reply and only outgoing connections are possible.
block: Similar to the above, but instead of simply dropping connections, incoming requests are rejected with an icmp-host-prohibited or icmp6-adm-prohibited message.
public: Represents public, untrusted networks.
You don't trust other computers but may allow selected incoming connections on a case-by-case basis.
external: External networks in the event that you are using the firewall as your gateway.
It is configured for NAT masquerading so that your internal network remains private but reachable.
internal: The other side of the external zone, used for the internal portion of a gateway.
The computers are fairly trustworthy and some additional services are available.
dmz: Used for computers located in a DMZ (isolated computers that will not have access to the rest of your network).
Only certain incoming connections are allowed.
work: Used for work machines.
Trust most of the computers in the network.
A few more services might be allowed.
home: A home environment.
It generally implies that you trust most of the other computers and that a few more services will be accepted.
trusted: Trust all of the machines in the network.
The most open of the available options and should be used sparingly.
To use the firewall, we can create rules and alter the properties of our zones and then assign our network interfaces to whichever zones are most appropriate.
Rule Permanence
In firewalld, rules can be applied to the current runtime ruleset, or be made permanent.
When a rule is added or modified, by default, only the currently running firewall is modified.
After the next reboot - or reload of the firewalld service - only the permanent rules will remain.
Most firewall-cmd operations can take a --permanent flag to indicate that the changes should be applied to the permenent configuration.
Additionally, the currently running firewall can be saved to the permanent configuration with the firewall-cmd --runtime-to-permanent command.
This separation of runtime vs permanent configuration means that you can safely test rules in your active firewall, then reload to start over if there are problems.
Installing and Enabling firewalld
firewalld is installed by default on some Linux distributions, including many images of CentOS 8. However, it may be necessary for you to install firewalld yourself:
After you install firewalld, you can enable the service and reboot your server.
Keep in mind that enabling firewalld will cause the service to start up at boot.
It is best practice to create your firewall rules and take the opportunity to test them before configuring this behavior in order to avoid potential issues.
When the server restarts, your firewall should be brought up, your network interfaces should be put into the zones you configured (or fall back to the configured default zone), and any rules associated with the zone (s) will be applied to the associated interfaces.
We can verify that the service is running and reachable by typing:
This indicates that our firewall is up and running with the default configuration.
Getting Familiar with the Current Firewall Rules
Before we begin to make modifications, we should familiarize ourselves with the default environment and rules provided by firewalld.
Exploring the Defaults
We can see which zone is currently selected as the default by typing:
Since we haven't given firewalld any commands to deviate from the default zone, and none of our interfaces are configured to bind to another zone, that zone will also be the only active zone (the zone that is controlling the traffic for our interfaces).
We can verify that by typing:
Here, we can see that our example server has two network interfaces being controlled by the firewall (eth0 and eth1).
They are both currently being managed according to the rules defined for the public zone.
How do we know what rules are associated with the public zone though?
We can print out the default zone's configuration by typing:
We can tell from the output that this zone is both the default and active, and that the eth0 and eth1 interfaces are associated with this zone (we already knew all of this from our previous inquiries).
However, we can also see that this zone allows traffic for a DHCP client (for IP address assignment), SSH (for remote administration), and Cockpit (a web-based console).
Exploring Alternative Zones
Now we have a good idea about the configuration for the default and active zone.
We can find out information about other zones as well.
To get a list of the available zones, type:
We can see the specific configuration associated with a zone by including the --zone = parameter in our --list-all command:
You can output all of the zone definitions by using the --list-all-zones option.
You will probably want to pipe the output into a pager for easier viewing:
Next we will learn about assiging zones to network interfaces.
Selecting Zones for your Interfaces
Unless you have configured your network interfaces otherwise, each interface will be put in the default zone when the firewall is started.
Changing the Zone of an Interface
You can move an interface between zones during a session by using the --zone = parameter in combination with the --change-interface = parameter.
As with all commands that modify the firewall, you will need to use sudo.
For instance, we can move our eth0 interface to the home zone by typing this:
< $> note Note: Whenever you are moving an interface to a new zone, be aware that you are probably modifying which services will be operational.
For instance, here we are moving to the home zone, which has SSH available.
This means that our connection shouldn't drop.
Some other zones do not have SSH enabled by default, and switching to one of these zones could cause your connection to drop, preventing you from logging back into your server.
We can verify that this was successful by asking for the active zones again:
Adjusting the Default Zone
If all of your interfaces can be handled well by a single zone, it's probably easiest to just designate the best zone as default and then use that for your configuration.
You can change the default zone with the --set-default-zone = parameter.
This will immediately change any interface using the default zone:
Setting Rules for your Applications
Let's run through the basic way of defining firewall exceptions for the services you wish to make available.
Adding a Service to your Zones
The most straighforward method is to add the services or ports you need to the zones you are using.
You can get a list of the available service definitions with the --get-services option:
< $> note Note: You can get more details about each of these services by looking at their associated .xml file within the / usr / lib / firewalld / services directory.
For instance, the SSH service is defined like this:
You can enable a service for a zone using the --add-service = parameter.
The operation will target the default zone or whatever zone is specified by the --zone = parameter.
By default, this will only adjust the current firewall session.
You can adjust the permanent firewall configuration by including the --permanent flag.
For instance, if we are running a web server serving conventional HTTP traffic, we can temporarily allow this traffic for interfaces in our public zone by typing:
You can leave out the --zone = flag if you wish to modify the default zone.
We can verify the operation was successful by using the --list-all or --list-services operations:
Once you have tested that everything is working as it should, you will probably want to modify the permanent firewall rules so that your service will still be available after a reboot.
We can make our previous change permanent by retyping it and adding the --permanent flag:
Alternately, you could use the --runtime-to-permanent flag to save the currently running firewall configuration to the permanant config:
Be careful with this, as all changes made to the running firewall will be commited permenantly.
Whichever method you chose, you can verify that it was successful by adding the --permanent flag to the --list-services operation.
You need to use sudo for any --permanent operations:
Your public zone will now allow HTTP web traffic on port 80. If your web server is configured to use SSL / TLS, you'll also want to add the https service.
We can add that to the current session and the permanent rule-set by typing:
What If No Appropriate Service Is Available?
The services that are included with the firewalld installation represent many of the most common applications that you may wish to allow access to.
However, there will likely be scenarios where these services do not fit your requirements.
In this situation, you have two options.
Opening a Port for your Zones
The easiest way to add support for your specific application is to open up the ports that it uses in the appropriate zone (s).
This is done by specifying the port or port range, and the associated protocol (TCP or UDP) for the ports.
For instance, if our application runs on port 5000 and uses TCP, we could temporarily add this to the public zone using the --add-port = parameter.
Protocols can be designated as either tcp or udp:
We can verify that this was successful using the --list-ports operation:
It is also possible to specify a sequential range of ports by separating the beginning and ending port in the range with a dash.
For instance, if our application uses UDP ports 4990 to 4999, we could open these up on public by typing:
After testing, we would likely want to add these to the permanent firewall.
Use sudo firewall-cmd --runtime-to-permanent to do that, or rerun the commands with the --permanent flag:
Defining a Service
Opening ports for your zones is a straightforward solution, but it can be difficult to keep track of what each one is for.
If you ever decommission a service on your server, you may have a hard time remembering which ports that have been opened are still required.
To avoid this situation, it is possible to define a new service.
Services are collections of ports with an associated name and description.
Using services is easier to administer than ports, but requires a bit of up-front work.
The easiest way to start is to copy an existing script (found in / usr / lib / firewalld / services) to the / etc / firewalld / services directory where the firewall looks for non-standard definitions.
For instance, we could copy the SSH service definition to use for our example service definition like this.
The filename minus the .xml suffix will dictate the name of the service within the firewall services list:
Now, you can adjust the definition found in the file you copied.
First open it in your favorite text editor.
We'll use vi here:
To start, the file will contain the SSH definition that you copied:
The majority of this definition is actually metadata.
You will want to change the short name for the service within the < short > tags.
This is a human-readable name for your service.
You should also add a description so that you have more information if you ever need to audit the service.
The only configuration you need to make that actually affects the functionality of the service will likely be the port definition where you identify the port number and protocol you wish to open.
Multiple < port / > tags can be specified.
For our example service, imagine that we need to open up port 7777 for TCP and 8888 for UDP.
We can modify the existing definition with something like this:
Reload your firewall to get access to your new service:
You can see that it is now among the list of available services:
You can now use this service in your zones as you normally would.
Creating Your Own Zones
While the predefined zones will probably be more than enough for most users, it can be helpful to define your own zones that are more descriptive of their function.
For instance, you might want to create a zone for your web server, called publicweb.
However, you might want to have another zone configured for the DNS service you provide on your private network.
You might want a zone called "privateDNS" for that.
When adding a zone, you must add it to the permanent firewall configuration.
You can then reload to bring the configuration into your running session.
For instance, we could create the two zones we discussed above by typing:
You can verify that these are present in your permanent configuration by typing:
As stated before, these won't be available in the runtime firewall yet:
Reload the firewall to bring these new zones into the active runtime configuration:
Now, you can begin assigning the appropriate services and ports to your zones.
It's usually a good idea to adjust the runtime firewall and then save those changes to the permanent configuration after testing.
For instance, for the publicweb zone, you might want to add the SSH, HTTP, and HTTPS services:
Likewise, we can add the DNS service to our privateDNS zone:
We could then change our interfaces over to these new zones to test them out:
At this point, you have the opportunity to test your configuration.
If these values work for you, you will want to add these rules to the permanent configuration.
You could do that by running all the commands again with the --permanent flag appended, but in this case we'll use the --runtime-to-permanent flag to save our entire runtime configuration permanently:
After permanently applying these rules, reload the firewall to test that the changes remain:
Validate that the correct zones were assigned:
And validate that the appropriate services are available for both of the zones:
You have successfully set up your own zones!
If you want to make one of these zones the default for other interfaces, remember to configure that behavior with the --set-default-zone = parameter:
You should now have a fairly thorough understanding of how to administer the firewalld service on your CentOS system for day-to-day use.
The firewalld service allows you to configure maintainable rules and rule-sets that take into consideration your network environment.
It allows you to seamlessly transition between different firewall policies through the use of zones and gives administrators the ability to abstract the port management into more friendly service definitions.
Acquiring a working knowledge of this system will allow you to take advantage of the flexibility and power that this tool provides.
For more information on firewalld, please see the official firewalld documentation.
How To Use the PDO PHP Extension to Perform MySQL Transactions in PHP on Ubuntu 18.04
3992
A MySQL transaction is a group of logically related SQL commands that are executed in the database as a single unit.
Transactions are used to enforce ACID (Atomicity, Consistency, Isolation, and Durability) compliance in an application.
This is a set of standards that govern the reliability of processing operations in a database.
Atomicity ensures the success of related transactions or a complete failure if an error occurs.
Consistency guarantees the validity of the data submitted to the database according to defined business logic.
Isolation is the correct execution of concurrent transactions ensuring the effects of different clients connecting to a database do not affect each other.
Durability ensures that logically related transactions remain in the database permanently.
SQL statements issued via a transaction should either succeed or fail altogether.
If any of the queries fails, MySQL rolls back the changes and they are never committed to the database.
A good example to understand how MySQL transactions work is an e-commerce website.
When a customer makes an order, the application inserts records into several tables, such as: orders and orders _ products, depending on the business logic.
Multi-table records related to a single order must be atomically sent to the database as a single logical unit.
Another use-case is in a bank application.
When a client is transferring money, a couple of transactions are sent to the database.
The sender's account is debited and the receiver's party account is credited.
The two transactions must be committed simultaneously.
If one of them fails, the database will revert to its original state and no changes should be saved to disk.
In this tutorial, you will use the PDO PHP Extension, which provides an interface for working with databases in PHP, to perform MySQL transactions on an Ubuntu 18.04 server.
Before you begin, you will need the following:
Apache, MySQL, and PHP installed on your system.
You can follow the guide on How To Install Linux, Apache, MySQL, PHP (LAMP) stack on Ubuntu 18.04.
You can skip Step 4 (setting up virtual hosts) and work directly with the default Apache settings.
Step 1 - Creating a Sample Database and Tables
You'll first create a sample database and add some tables before you start working with MySQL transactions.
When prompted, enter your MySQL root password and hit ENTER to proceed.
Then, create a database, for the purposes of this tutorial we'll call the database sample _ store:
Create a user called sample _ user for your database.
Remember to replace PASSWORD with a strong value:
Issue full privileges for your user to the sample _ store database:
Finally, reload the MySQL privileges:
You'll see the following output once you've created your user:
With the database and user in place, you can now create several tables for demonstrating how MySQL transactions work.
Log out from the MySQL server:
Once the system logs you out, you will see the following output:
Then, log in with the credentials of the sample _ user you just created:
Enter the password for the sample _ user and hit ENTER to proceed.
Switch to the sample _ store to make it the currently selected database:
You'll see the following output once it is selected:
Next, create a products table:
This command creates a products table with a field named product _ id.
You use a BIGINT data type that can accommodate a large value of up to 2 ^ 63-1.
You use this same field as a PRIMARY KEY to uniquely identify products.
The AUTO _ INCREMENT keyword instructs MySQL to generate the next numeric value as new products are inserted.
The product _ name field is of type VARCHAR that can hold up to a maximum of 50 letters or numbers.
For the product price, you use a DOUBLE data type to cater for floating point formats in prices with decimal numbers.
Lastly, you use the InnoDB as the ENGINE because it comfortably supports MySQL transactions as opposed to other storage engines such as MyISAM.
Once you've created your products table, you'll get the following output:
Next, add some items to the products table by running the following commands:
You'll see output similar to the following after each INSERT operation:
Then, verify that the data was added to the products table:
You will see a list of the four products that you have inserted:
Next, you'll create a customers table for holding basic information about customers:
As in the products table, you use the BIGINT data type for the customer _ id and this will ensure the table can support a lot of customers up to 2 ^ 63-1 records.
The keyword AUTO _ INCREMENT increments the value of the columns once you insert a new customer.
Since the customer _ name column accepts alphanumeric values, you use VARCHAR data type with a limit of 50 characters.
Again, you use the InnoDB storage ENGINE to support transactions.
After running the previous command to create the customers table, you will see the following output:
You'll add three sample customers to the table.
Run the following commands:
Once the customers have been added, you will see an output similar to the following:
Then, verify the data in the customers table:
You'll see a list of the three customers:
Next, you'll create an orders table for recording orders placed by different customers.
To create the orders table, execute the following command:
You use the column order _ id as the PRIMARY KEY.
The BIGINT data type allows you to accommodate up to 2 ^ 63-1 orders and will auto-increment after each order insertion.
The order _ date field will hold the actual date and time the order is placed and hence, you use the DATETIME data type.
The customer _ id relates to the customers table that you created previously.
Since a single customer's order may contain multiple items, you need to create an orders _ products table to hold this information.
To create the orders _ products table, run the following command:
You use the ref _ id as the PRIMARY KEY and this will auto-increment after each record insertion.
The order _ id and product _ id relate to the orders and the products tables respectively.
The price column is of data type DOUBLE in order to accommodate floating values.
The storage engine InnoDB must match the other tables created previously since a single customer's order will affect multiple tables simultaneously using transactions.
Your output will confirm the table's creation:
You won't be adding any data to the orders and orders _ products tables for now but you'll do this later using a PHP script that implements MySQL transactions.
Your database schema is now complete and you've populated it with some records.
You'll now create a PHP class for handling database connections and MySQL transactions.
Step 2 - Designing a PHP Class to Handle MySQL Transactions
In this step, you will create a PHP class that will use PDO (PHP Data Objects) to handle MySQL transactions.
The class will connect to your MySQL database and insert data atomically to the database.
Save the class file in the root directory of your Apache web server.
To do this, create a DBTransaction.php file using your text editor:
Then, add the following code to the file.
Replace PASSWORD with the value you created in Step 1:
Toward the beginning of the DBTransaction class, the PDO will use the constants (DB _ HOST, DB _ NAME, DB _ USER, and DB _ PASSWORD) to initialize and connect to the database that you created in step 1.
< $> note Note: Since we are demonstrating MySQL transactions in a small scale here, we have declared the database variables in the DBTransaction class.
In a large production project, you would normally create a separate configuration file and load the database constants from that file using a PHP require _ once statement.
Next, you set two attributes for the PDO class:
ATTR _ ERRMODE, PDO:: ERRMODE _ EXCEPTION: This attribute instructs PDO to throw an exception if an error is encountered.
Such errors can be logged for debugging.
ATTR _ EMULATE _ PREPARES, false: This option disables emulation of prepared statements and allows the MySQL database engine to prepare the statements itself.
Now add the following code to your file to create the methods for your class:
Save and close the file by pressing CTRL + X, Y, then ENTER.
To work with MySQL transactions, you create three main methods in the DBTransaction class; startTransaction, insertTransaction, and submitTransaction.
startTransaction: This method instructs PDO to start a transaction and turns auto-commit off until a commit command is issued.
insertTransaction: This method takes two arguments.
The $sql variable holds the SQL statement to be executed while the $data variable is an array of the data to be bound to the SQL statement since you're using prepared statements.
The data is passed as an array to the insertTransaction method.
submitTransaction: This method commits the changes to the database permanently by issuing a commit () command.
However, if there is an error and the transactions have a problem, the method calls the rollBack () method to revert the database to its original state in case a PDO exception is raised.
Your DBTransaction class initializes a transaction, prepares the different SQL commands to be executed, and finally commits the changes to the database atomically if there are no issues, otherwise, the transaction is rolled back.
In addition, the class allows you to retrieve the record order _ id you just created by accessing the public property last _ insert _ id.
The DBTransaction class is now ready to be called and used by any PHP code, which you'll create next.
Step 3 - Creating a PHP Script to Use the DBTransaction Class
You'll create a PHP script that will implement the DBTransaction class and send a group of SQL commands to the MySQL database.
You'll mimic the workflow of a customer's order in an online shopping cart.
These SQL queries will affect the orders and the orders _ products tables.
Your DBTransaction class should only allow changes to the database if all of the queries are executed without any errors.
Otherwise, you'll get an error back and any attempted changes will roll back.
You are creating a single order for the customer JOHN DOE identified with customer _ id 1. The customer's order has three different items with differing quantities from the products table.
Your PHP script takes the customer's order data and submits it into the DBTransaction class.
Create the orders.php file:
Then, add the following code to the file:
You've created a PHP script that initializes an instance of the DBTransaction class that you created in Step 2.
In this script, you include the DBTransaction.php file and you initialize the DBTransaction class.
Next, you prepare a multi-dimensional array of all the products the customer is ordering from the store.
You also invoke the startTransaction () method to start a transaction.
Next add the following code to finish your orders.php script:
You prepare the command to be inserted to the orders table via the insertTransaction method.
After this, you retrieve the value of the public property last _ insert _ id from the DBTransaction class and use it as the $order _ id.
Once you have an $order _ id, you use the unique ID to insert the customer's order items to the orders _ products table.
Finally, you call the method submitTransaction to commit the entire customer's order details to the database if there are no problems.
Otherwise, the method submitTransaction will rollback the attempted changes.
Now you'll run the orders.php script in your browser.
Run the following and replace your-server-IP with the public IP address of your server:
http: / / < ^ > your-server-IP < ^ > / orders.php
You will see confirmation that the records were successfully submitted:
PHP Output from MySQL Transactions Class
Your PHP script is working as expected and the order together with the associated order products were submitted to the database atomically.
You've run the orders.php file on a browser window.
The script invoked the DBTransaction class which in turn submitted the orders details to the database.
In the next step, you will verify if the records saved to the related database tables.
Step 4 - Confirming the Entries in Your Database
In this step, you'll check if the transaction initiated from the browser window for the customer's order was posted to the database tables as expected.
To do this, log in to your MySQL database again:
Enter the password for the sample _ user and hit ENTER to continue.
Switch to the sample _ store database:
Ensure the database is changed before proceeding by confirming the following output:
Then, issue the following command to retrieve records from the orders table:
This will display the following output detailing the customer's order:
Next, retrieve the records from the orders _ products table:
You'll see output similar to the following with a list of products from the customer's order:
The output confirms that the transaction was saved to the database and your helper DBTransaction class is working as expected.
In this guide, you used the PHP PDO to work with MySQL transactions.
Although this is not a conclusive article on designing an e-commerce software, it has provided an example for using MySQL transactions in your applications.
To learn more about the MySQL ACID model, consider visiting the InnoDB and the ACID Model guide from the official MySQL website.
Visit our MySQL content page for more related tutorials, articles, and Q & A.
How To Add and Delete Users on CentOS 8
4020
When you first start using a fresh Linux server, adding and removing users is often one of first things you'll need to do.
In this guide, we will cover how to create user accounts, assign sudo privileges, and delete users on a CentOS 8 server.
This tutorial assumes you are logged into a CentOS 8 server with a non-root sudo-enabled user.
If you are logged in as root instead, you can drop the sudo portion of all the following commands, but they will work either way.
Adding Users
Throughout this tutorial we will be working with the user sammy.
Please susbtitute with the username of your choice.
You can add a new user by typing:
Next, you'll need to give your user a password so that they can log in.
To do so, use the passwd command:
You will be prompted to type in the password twice to confirm it. Now your new user is set up and ready for use!
< $> note Note: if your SSH server disallows password-based authentication, you will not yet be able to connect with your new username.
Details on setting up key-based SSH authentication for the new user can be found in step 5 of Initial Server Setup with CentOS 8. < $>
Granting Sudo Privileges to a User
If your new user should have the ability to execute commands with root (administrative) privileges, you will need to give them access to sudo.
We can do this by adding the user to the wheel group (which gives sudo access to all of its members by default).
Use the usermod command to add your user to the wheel group:
Now your new user is able to execute commands with administrative privileges.
To do so, append sudo ahead of the command that you want to execute as an administrator:
You will be prompted to enter the password of the your user account (not the root password).
Once the correct password has been submitted, the command you entered will be executed with root privileges.
Managing Users with Sudo Privileges
While you can add and remove users from a group with usermod, the command doesn't have a way to show which users are members of a group.
To see which users are part of the wheel group (and thus have sudo privileges), you can use the lid command. lid is normally used to show which groups a user belongs to, but with the -g flag, you can reverse it and show which users belong in a group:
The output will show you the usernames and UIDs that are associated with the group.
This is a good way of confirming that your previous commands were successful, and that the user has the privileges that they need.
Deleting Users
If you have a user account that you no longer need, it's best to delete it.
To delete the user without deleting any of their files, use the userdel command:
If you want to delete the user's home directory along with their account, add the -r flag to userdel:
With either command, the user will automatically be removed from any groups that they were added to, including the wheel group if applicable.
If you later add another user with the same name, they will have to be added to the wheel group again to gain sudo access.
You should now have a good grasp on how to add and remove users from your CentOS 8 server.
Effective user management will allow you to separate users and give them only the access that is needed for them to do their job.
You can now move on to configuring your CentOS 8 server for whatever software you need, such as a LAMP or LEMP web stack.
How To Install Linux, Apache, MariaDB, PHP (LAMP) Stack on CentOS 8
4022
A "LAMP" stack is a group of open source software that is typically installed together to enable a server to host dynamic websites and web apps written in PHP.
This term is an acronym which represents the Linux operating system, with the Apache web server.
The backend data is stored in a MariaDB database and the dynamic processing is handled by PHP.
The database layer in a LAMP stack is typically a MySQL database server, but prior to the release of CentOS 8, MySQL wasn't available from the default CentOS repositories.
Because of this, MariaDB, a community fork of MySQL, became a widely accepted alternative to MySQL as the default database system for LAMP stacks on CentOS machines.
MariaDB works as a drop-in replacement for the original MySQL server, which in practice means you can switch to MariaDB without having to make any configuration or code changes in your application.
In this guide, you'll install a LAMP stack on a CentOS 8 server, using MariaDB as the database management system.
To follow this guide, you'll need access to a CentOS 8 server as a non-root user with sudo privileges, and an active firewall installed on your server.
To set this up, you can follow our Initial Server Setup Guide for CentOS 8.
Step 1 - Installing the Apache Web Server
In order to display web pages to our site visitors, we are going to employ Apache, a popular open source web server that can be configured to serve PHP pages.
We'll use the dnf package manager, which is the new default package manager on CentOS 8, to install this software.
Install the httpd package with:
When prompted, enter y to confirm that you want to install Apache.
After the installation is finished, run the following command to enable and start the server:
In case you have enabled the firewalld firewall as per our initial server setup guide, you will need to allow connections to Apache.
The following command will permanently enable HTTP connections, which run on port 80 by default:
To verify that the change was applied, you can run:
You'll need to reload the firewall configuration so the changes take effect:
With the new firewall rule added, you can test if the server is up and running by accessing your server "s public IP address or domain name from your web browser.
< $> note Note: In case you are using DigitalOcean as DNS hosting provider, you can check our product docs for detailed instructions on how to set up a new domain name and point it to your server.
If you do not have a domain name pointed at your server and you do not know your server "s public IP address, you can find it by running the following command:
This will print out a few IP addresses.
You can try each of them in turn in your web browser.
As an alternative, you can check which IP address is accessible, as viewed from other locations on the internet:
Type the address that you receive in your web browser and it will take you to Apache's default landing page:
Default Apache Page CentOS 8
If you see this page, then your web server is now correctly installed.
Step 2 - Installing MariaDB
Now that you have a web server up and running, you need to install a database system to be able to store and manage data for your site.
We'll install MariaDB, a community-developed fork of the original MySQL server by Oracle.
To install this software, run:
When the installation is finished, you can enable and start the MariaDB server with:
To improve the security of your database server, it's recommended that you run a security script that comes pre-installed with MariaDB.
This script will remove some insecure default settings and lock down access to your database system.
Start the interactive script by running:
This script will take you through a series of prompts where you can make some changes to your MariaDB setup.
This is not to be confused with the system root user.
The database root user is an administrative user with full privileges over the database system.
Because you just installed MariaDB and haven "t made any configuration changes yet, this password will be blank, so just press ENTER at the prompt.
Because MariaDB uses a special authentication method for the root user that is typically safer than using a password, you don't need to set this now.
This will remove anonymous users and the test database, disable remote root login, and load these new rules so that the server immediately respects the changes you have made.
When you're finished, log in to the MariaDB console by typing:
This will connect to the MariaDB server as the administrative database user root, which is inferred by the use of sudo when running this command.
You should see output like this:
Notice that you didn't need to provide a password to connect as the root user.
That works because the default authentication method for the administrative MariaDB user is unix _ socket instead of password.
Even though this might look like a security concern at first, it makes the database server more secure because the only users allowed to log in as the root MariaDB user are the system users with sudo privileges connecting from the console or through an application running with the same privileges.
In practical terms, that means you won't be able to use the administrative database root user to connect from your PHP application.
For increased security, it's best to have dedicated user accounts with less expansive privileges set up for every database, especially if you plan on having multiple databases hosted on your server.
To demonstrate such a setup, we'll create a database named example\ _ database and a user named example\ _ user, but you can replace these names with different values.
To create a new database, run the following command from your MariaDB console:
Now you can create a new user and grant them full privileges on the custom database you've just created.
The following command defines this user's password as < ^ > password < ^ >, but you should replace this value with a secure password of your own choosing:
This will give the example\ _ user user full privileges over the example\ _ database database, while preventing this user from creating or modifying other databases on your server.
You can test if the new user has the proper permissions by logging in to the MariaDB console again, this time using the custom user credentials:
Note the -p flag in this command, which will prompt you for the password you chose when creating the example\ _ user user.
After logging in to the MariaDB console, confirm that you have access to the example\ _ database database:
This will give you the following output:
To exit the MariaDB shell, type:
At this point, your database system is set up and you can move on to installing PHP, the final component of the LAMP stack.
Step 3 - Installing PHP
You have Apache installed to serve your content and MariaDB installed to store and manage your data. PHP is the component of our setup that will process code to display dynamic content to the final user.
In addition to the php package, you "ll need php-mysqlnd, a PHP module that allows PHP to communicate with MySQL-based databases.
Core PHP packages will automatically be installed as dependencies.
To install the php and php-mysqlnd packages using the dnf package manager, run:
After the installation is finished, you'll need to restart the Apache web server in order to enable the PHP module:
Your web server is now fully set up.
In the next step, we'll create a PHP testing script to make sure everything works as expected.
Step 4 - Testing PHP with Apache
The default Apache installation on CentOS 8 will create a document root located at / var / www / html.
You don't need to make any changes to Apache's default settings in order for PHP to work correctly within your web server.
The only adjustment we'll make is to change the default permission settings on your Apache document root folder.
This way, you'll be able to create and modify files in that directory with your regular system user, without the need to prefix each command with sudo.
The following command will change the ownership of the default Apache document root to a user and group called < ^ > sammy < ^ >, so be sure to replace the highlighted username and group in this command to reflect your system's username and group.
We'll now create a test PHP page to make sure the web server works as expected.
The default text editor that comes with CentOS 8 is vi. vi is an extremely powerful text editor, but it can be somewhat obtuse for users who lack experience with it. You might want to install a more user-friendly editor such as nano to facilitate editing files on your CentOS 8 server:
Type y when prompted to confirm the installation.
Now, create a new PHP file called info.php at the / var / www / html directory:
The following PHP code will display information about the current PHP environment running on the server:
If you are using nano, you can do that by typing CTRL + X, then Y and ENTER to confirm.
Now we can test whether our web server can correctly display content generated by a PHP script.
Go to your browser and access your server hostname or IP address, followed by / info.php:
You'll see a page similar to this:
CentOS 8 default PHP info Apache
After checking the relevant information about your PHP server through that page, it "s best to remove the file you created as it contains sensitive information about your PHP environment and your CentOS server.
You can use rm to remove that file:
You can always regenerate this file if you need it later.
Next, we "ll test the database connection from the PHP side.
Step 5 - Testing Database Connection from PHP (Optional)
If you want to test if PHP is able to connect to MariaDB and execute database queries, you can create a test table with dummy data and query for its contents from a PHP script.
First, connect to the MariaDB console with the database user you created in Step 2 of this guide:
Create a table named todo _ list.
From the MariaDB console, run the following statement:
Now, insert a few rows of content in the test table.
You might want to repeat the next command a few times, using different values:
To confirm that the data was successfully saved to your table, run:
After confirming that you have valid data in your test table, you can exit the MariaDB console:
Now you can create the PHP script that will connect to MariaDB and query for your content.
Create a new PHP file in your custom web root directory using your preferred editor.
We'll use nano for that:
Add the following content to your PHP script:
You can now access this page in your web browser by visiting your server's host name or public IP address, followed by / todo _ list.php:
You should see a page like this, showing the content you've inserted in your test table:
Example PHP todo list
That means your PHP environment is ready to connect and interact with your MariaDB server.
In this guide, you've built a flexible foundation for serving PHP websites and applications to your visitors, using Apache as web server.
You've set up Apache to handle PHP requests, and you've also set up a MariaDB database to store your website's data.
How To Install MariaDB on CentOS 8
4019
In this tutorial, we will explain how to install the latest version of MariaDB on a CentOS 8 server.
If you're wondering about MySQL vs. MariaDB, MariaDB is the preferred package and should work seamlessly in place of MySQL.
If you specifically need MySQL, see the How to Install MySQL on CentOS 8 guide.
To follow this tutorial, you will need a CentOS 8 server with a non-root sudo-enabled user.
You can learn more about how to set up a user with these privileges in the Initial Server Setup with CentOS 8 guide.
First, use dnf to install the MariaDB package:
You will be asked to confirm the action.
Press y then ENTER to proceed.
Once the installation is complete, start the service with systemctl:
Then check the status of the service:
If MariaDB has successfully started, the output should show active (running) and the final line should look something like:
Next, let's take a moment to ensure that MariaDB starts at boot, using the systemctl enable command:
We now have MariaDB running and configured to run at startup.
Next, we'll turn our attention to securing our installation.
Step 2 - Securing the MariaDB Server
MariaDB includes a security script to change some of the less secure default options for things like remote root logins and sample users.
Use this command to run the security script:
The script provides a detailed explanation for every step.
The first step asks for the root password, which hasn't been set so we'll press ENTER as it recommends.
Next, we'll be prompted to set that root password.
Keep in mind that this is for the root database user, not the root user for your CentOS server itself.
Type Y then ENTER to enter a password for the root database user, then follow the prompts.
After updating the password, we will accept all the security suggestions that follow by pressing y and then ENTER.
This will remove anonymous users, disallow remote root login, remove the test database, and reload the privilege tables.
Now that we've secured the installation, we'll verify it's working by connecting to the database.
Step 3 - Testing the Installation
We can verify our installation and get information about it by connecting with the mysqladmin tool, a client that lets you run administrative commands.
Use the following command to connect to MariaDB as root (-u root), prompt for a password (-p), and return the version.
You should see output similar to this:
This indicates the installation has been successful.
You may want to import and export databases
You could incorporate MariaDB into a larger software stack, such as the LAMP stack: How To Install Linux, Apache, MariaDB, PHP (LAMP stack) on CentOS 7
You may need to update your firewalld firewall to allow for external database traffic
How To Install the Ampache Music Streaming Server on Ubuntu 18.04
4023
The author selected Open Internet / Free Speech Fund to receive a donation as part of the Write for DOnations program.
Ampache is an open-source music streaming server that allows you to host and manage your digital music collection on your own server.
Ampache can stream your music to your computer, smartphone, tablet, or smart TV.
This means that you don't have to maintain multiple copies of your music on the device you want to use to listen to it. With Ampache you can manage your collection on your server using Ampache's web interface and listen to it anywhere.
In this tutorial, you will install and configure the Apache webserver and PHP that will serve your Ampache instance.
You will then create a MySQL database that Ampache will use to store all of its operational information.
Finally you will upload your music collection so you can start streaming your music.
One Ubuntu 18.04 server set up by following the Initial Server Setup with Ubuntu 18.04 tutorial, including a non-root, sudo user.
Apache, MySQL, and PHP installed on your server, as shown in How To Install Linux, Apache, MySQL, PHP (LAMP) stack on Ubuntu 18.04.
You can learn how to point domains to DigitalOcean Droplets by following the How To Set Up a Host Name with DigitalOcean tutorial.
Step 1 - Installing Ampache
In this step, you will download the Ampache ZIP archive to your server, unpack it to its installation home directory, and make some necessary file system changes.
First, log in to your server as the non-root user.
Then create Ampache's home directory with the following command:
Next, install the zip utility that you will need to unpack the Ampache archive:
Next, download the ZIP archive of the latest release.
You can find the link to the latest release on Ampache's GitHub page.
In these examples, version < ^ > 4.1.1 < ^ > is used:
Next, unpack the ZIP archive into the / var / www / ampache / directory using the -d option:
Next, set the user and group identity of Ampache's files so that Apache is able to read, write, and execute the Ampache instance files:
The --recursive option makes chown change the ownership and group identity of all the files and subdirectory under / var / www / ampache / to Apache's user and group www-data.
Next, rename the .htaccess files that are included in the ZIP archive. .htaccess files contain security and other operation information for Apache but they will only work with the file extension name .htaccess.
Rename the supplied .htaccess.dist files to .htaccess with the following commands:
Now you'll create the directory that will contain your music files.
For security reasons, this directory is best created outside of Ampache's installation directory.
That way, it will not be readable or changeable by malicious web requests as it resides outside of Ampache's DocumentRoot.
Create the directory, / data / Music, with the following command:
Then change its owner and group identity so that Apache can read and write to it:
To finish setting up, you'll install FFmpeg, which is a utility that converts audio and video from one format to another.
For example, you could use it to convert an MP3 music file to an OPUS music file.
Ampache uses FFmpeg to convert audio on the fly from the format in which it was uploaded to a format that the listening device can play.
This is a process known as transcoding.
This is useful because not all devices are able to play all formats of music.
Ampache can detect what formats are supported on the playing device and automatically supply your music in the supported format.
Install FFmpeg with the following command:
You've now unpacked and prepared your Ampache instance for the web installer and installed the FFmpeg utility.
Next you'll configure Apache and PHP to serve your Ampache instance.
Step 2 - Configuring Apache and PHP
In this section, you will configure Apache by creating a new VirtualHost file, which will provide the configuration that Apache needs to serve your Ampache server domain.
You will also install some additional PHP modules that extend the abilities of PHP to cover the requirements of Ampache
First, install some additional PHP modules that were not included with the default PHP installation:
These modules provide the following additional functionality:
php-mysql - Enables PHP to communicate with a MySQL database.
php-curl - Enables PHP to use the curl utility to download files, such as album cover art, from remote servers.
php-json - Enables PHP to read and manipulate JSON formatted text files.
php-gd - Enables PHP to manipulate and create image files.
php7.2-xml - Enables PHP to read and manipulate XML formatted text files.
Next, enable a couple of Apache modules using the a2enmod utility:
These Apache modules allow Apache to do the following:
rewrite - Modify or rewrite URLs following rules supplied by Ampache.
expires - Set the cache expiry times for objects such as images so that they are more efficiently stored by browsers.
Now, you will create the VirtualHost file that tells Apache how and where to load the Ampache instance.
< $> note Note: If you created a test VirtualHost file that uses your Ampache domain name when you followed the "How To Install Linux, Apache, MySQL, PHP (LAMP) stack on Ubuntu 18.04" guide you must disable it. Use the a2dissite utility to disable it:
Now, create and open the VirtualHost file with your text editor at / etc / apache2 / sites-available / ampache.conf:
Add the following VirtualHost template into your file:
You must change < ^ > your _ domain < ^ > to the domain name that you redirected to your server.
When you have finished editing this file save and exit the editor.
The directives in this VirtualHost file are:
ServerName - The domain name that Apache will use to serve your Ampache instance.
DocumentRoot - The file system location on your server where the Ampache instance is located.
This is the same place that you unpacked the ZIP archive to in Step 1.
Directory - This section passes configuration to Apache that applies to the files and directories contained in the path.
RewriteEngine - Enables the rewrite Apache module.
CustomLog - Creates a log file that Apache will use to record all access logs for your Ampache server.
ErrorLog - Creates a log file that Apache will use to record all error logs generated by your Ampache server.
Next, check that the VirtualHost file you created does not have any errors with the apachectl utility:
If your configuration does not contain any errors, you will see the following output after the command:
If your configuration does contain errors, the output will print the file name and line number where the error was encountered.
< $> note Note: If you see the error:
Then you will need to edit Apache's main configuration file at / etc / apache2 / apache2.conf and add the following line:
Next, enable the new VirtualHost configuration using the a2ensite utility:
The final configuration is optional, but recommended.
The default setting for file uploads in PHP is that no file greater than 2MB can be uploaded.
Music files tend to be larger than this so increasing this size will allow you to use the Ampache interface to upload larger files in your music collection.
Open / etc / php / 7.2 / apache2 / php.ini with the following command:
And change the following lines:
To:
You will now be able to upload music files up to 100MB.
Use a larger value if you intend to upload files larger than this size.
Finally, reload your updated Apache configuration:
You've now configured Apache to serve Ampache over HTTP.
Next you'll obtain a TLS certificate and configure Apache to use it so you can securely access Ampache over HTTPS.
Step 3 - Enabling HTTPS
In this step, you will get a free Let's Encrypt TLS certificate using the Certbot utility, which enables HTTPS browsing.
Certbot will create the certificate, automatically generate the required Apache configuration, and manage the automatic renewal of the certificate.
This is important because every time you log in to Ampache you will send your username and password across the internet.
If you are not using HTTPS then your password will be sent in plain text that can be read as it travels across the internet.
Ubuntu's LTS releases tend not to have the latest packages and this is true of the Certbot program.
The Certbot developers maintain a dedicated Ubuntu repository, known as a PPA, so that Ubuntu users can keep an up-to-date copy of the Certbot.
Install the Certbot repository with the following command:
Now, install the certbot utility:
Next, use certbot to get the TLS certificate:
The --apache option uses the Apache plugin that enables Certbot to read and configure Apache automatically. -d < ^ > your _ domain < ^ > specifies the domain name for which you want Certbot to create the certificate.
When you run the certbot command you will be asked a series of questions.
You'll be prompted to enter an email address and agree to the terms of service.
If certbot successfully confirms that you control your domain, it will ask you to configure your HTTPS settings:
No redirect: Apache will serve Ampache via HTTP and HTTPS.
Redirect: Apache will redirect any HTTP connections to HTTPS automatically.
This means that your Ampache server will only be available via HTTPS.
This option is the more secure and will not affect how your Ampache instance behaves.
This is the recommended choice.
Finally, test that the auto-renewal of the certificate will take place successfully by running the following command:
The --dry-run option means that certbot will test a renewal attempt without making any permanent changes to your server.
If the test was successful the output will include the following line:
Apache and PHP are now ready to serve your Ampache instance.
In the next step, you will create and configure Ampache's database.
Step 4 - Creating a MySQL Database
Ampache uses a MySQL database to store information such as playlists, user preferences, and so on.
In this step, you will create a database and MySQL user that Ampache will use to access it.
You will need to choose three pieces of information to complete the following instructions to create Ampache's database:
< ^ > ampache _ database < ^ >: The name of the Ampache database.
< ^ > database _ user < ^ >: The MySQL user that Ampache will use to access the database.
This is not a system user and can only access the database.
< ^ > database _ password < ^ >: The database user's password.
Be sure to choose a secure password.
Make a note of these details as you will need them later.
First, open the interactive MySQL shell with the mysql command:
--user = root opens the MySQL shell as the MySQL root user and --password prompts for the root user's password.
The following command will create an empty database:
Next, create the MySQL user:
Now, give the new user full access to the database:
Finally, check that the new database exists by running the following command:
Exit the MySQL shell by entering exit;.
Finally, test the database, username, and password by trying to log in to the MySQL shell with the < ^ > database _ user < ^ >.
Enter the following command to log in to the MySQL shell as the new user:
You've now created the database that Ampache will use.
You've finished your server configuration and are ready to complete the installation with the web installer.
Step 5 - Using the Web Installer
In this step, you will use Ampache's web installer to finish the installation by giving Ampache the information it will need to run, such as a web interface admin user, the database details, and other settings.
Start the web installation by entering https: / / < ^ > your _ domain < ^ > into your browser.
Choose Installation Language
Select Ampache's interface language and click the Start Configuration button to continue.
Requirements
This page is where Ampache checks that the server meets its requirements.
Each line on this page represents a test that the installer performs to ensure that, for example, all the required PHP modules are present and working.
You'll see every test has a green checkmark to indicate that your server is ready for Ampache.
Click the Continue button to move on to the next page.
Insert Ampache Database
This page creates Ampache's database if it does not exist and formats it. Fill in the fields as follows:
Desired Database Name: < ^ > ampache _ database < ^ >
MySQL Hostname: localhost
MySQL Port (optional):
MySQL Administrative Username: < ^ > database _ user < ^ >
MySQL Administrative Password: < ^ > database _ password < ^ >
Create Database:
Create Tables (ampache.sql):
Create Database User:
Image showing the completed form
Click the Insert Database button to continue.
Generate Configuration File
This page creates the configuration file that Ampache will use to run.
Fill in the fields as follows:
Web Path:
Database Name: < ^ > ampache _ database < ^ >
MySQL Username: < ^ > database _ user < ^ >
MySQL Password: < ^ > database _ password < ^ >
Installation Type Leave this at the default setting.
Allow Transcoding Select ffmpeg from the drop-down list.
Players Leave these at the default settings.
Click the Create Config button to continue.
Create Admin Account
This page creates the first web interface user.
This user is created with full administrative, privileges and is the one that you will use to log in and configure Ampache for the first time.
Choose a username and a secure password and enter it into the Password and Confirm Password fields.
Click the Create Account button to continue.
Ampache Update
This page will perform any administrative changes to Ampache's database that need making.
These changes are made during version upgrades but as this is a new installation the installer will not make any changes.
Click the Update Now!
button to continue.
This page prints and explains any updates that the installer made in the previous step.
You should not see any updates listed.
Click the Return to main page link to continue to the login page.
Enter your username and password you set to log in to your Ampache server.
Ampache is not quite fully set up and ready for use.
You'll now complete the setup by adding your music so you can start using your new Ampache server.
Step 6 - Adding Your Music to Ampache
A music server is no use without some music to play.
In this step, you will configure a music catalog and upload some music.
"Catalog" is the name that Ampache gives to a collection of music.
Ampache is able to read music from many sources both on and off the server but in this tutorial, you will upload and store your music on the server in a local catalog, as Ampache refers to it.
First, click the add a Catalog link in the following line on the first page that you see when you log in to Ampache:
No Catalog configured yet.
To start streaming your media, you now need to add a Catalog.
This will take you to the Add Catalog page.
Catalog Name: Give this catalog a short, memorable name.
Catalog Type: local
Filename Pattern:
Folder Pattern: Leave this with the defaults.
Gather Art:
Build Playlists from Playlist Files. (m3u, m3u8, asx, pls, xspf):
Path: / data / Music
Image showing the completed add catalog form
Click the Add Catalog button to complete this page.
On the page that follows click the Continue button.
This will take you to the Show Catalogs page that will print the details for the catalog you created.
You can change the catalog settings at any time in the future.
Now, enable Ampache's web upload facility by clicking on the third navigation icon to open the extended settings:
Image showing the third settings icon
Scroll down to the Server Config section and click the System link to open the system settings page.
Find the Allow user uploads line and select Enable from the drop-down menu in the Value column.
You can also choose what level of user is able to upload music files.
The default level is Catalog Manager, this allows the Catalog Manager and all users with greater privileges to upload music.
In this case, that is the administrator.
You also need to set the catalog to which the music will get added.
Set this using the Destination catalog line.
Select the catalog you created in the Value drop-down.
Image showing the allow upload and destination catalog lines
Click the Update Preferences button at the bottom of the page to complete the configuration.
You are now ready to upload some music.
First, click the first settings icon:
Icon showing the first settings icon
Then click the Upload link in the Music section.
Image showing upload link
On the Upload page, click the Browse button and locate your music files on your computer and upload them.
If you leave the Artist and Album labels blank then Ampache will read the ID3 tags of the music files to discover the artist and album automatically.
After you have uploaded some music you will now be able to find it by clicking on the Songs, Albums, or Artists links in the Music section in the navigation panel on the left.
Your Ampache music server is now ready to start streaming your music.
In this article, you installed and configured an Ampache music streaming server and uploaded some of your music.
You can now listen to your music wherever you go on any of your devices.
Ampache's Documentation will help you use and extend your streaming server.
These Android apps and this iOS app will stream your music to your phone.
Ampache organizes your music on the server using the ID3 tags in the music files.
The MusicMrainz program will help you manage the ID3 tags of your music files.
How To Create Custom Components in React
4101
The author selected Creative Commons to receive a donation as part of the Write for DOnations program.
In this tutorial, you'll learn to create custom components in React.
Components are independent pieces of functionality that you can reuse in your application, and are the building blocks of all React applications.
Often, they can be simple JavaScript functions and classes, but you use them as if they were customized HTML elements.
Buttons, menus, and any other front-end page content can all be created as components.
Components can also contain state information and display markdown.
After learning how to create components in React, you'll be able to split complex applications into small pieces that are easier to build and maintain.
In this tutorial, you'll create a list of emojis that will display their names on click.
The emojis will be built using a custom component and will be called from inside another custom component.
By the end of this tutorial, you'll have made custom components using both JavaScript classes and JavaScript functions, and you'll understand how to separate existing code into reusable pieces and how to store the components in a readable file structure.
You will need a development environment running Node.js; this tutorial was tested on Node.js version 10.20.1 and npm version 6.14.4.
To install this on macOS or Ubuntu 18.04, follow the steps in How to Install Node.js and Create a Local Development Environment on macOS or the Installing Using a PPA section of How To Install Node.js on Ubuntu 18.04.
You will need to be able to create apps with Create React App.
You can find instructions for installing an application with Create React App at How To Set Up a React Project with Create React App.
You will be using JSX syntax, which you can learn about in our How To Create Elements with JSX tutorial.
You will also need a basic knowledge of JavaScript, which you can find in How To Code in JavaScript, along with a basic knowledge of HTML and CSS.
A good resource for HTML and CSS is the Mozilla Developer Network.
Step 1 - Setting Up the React Project
In this step, you'll create a base for your project using Create React App.
You will also modify the default project to create your base project by mapping over a list of emojis and adding a small amount of styling.
First, create a new project.
Open a terminal, then run the following command:
Once this is finished, change into the project directory:
Open the App.js code in a text editor:
Next, take out the template code created by Create React App, then replace the contents with new React code that displays a list of emojis:
This code uses JSX syntax to map () over the emojis array and list them as < li > list items.
It also attaches onClick events to display emoji data in the browser.
To explore the code in more detail, check out How to Create React Elements with JSX, which contains a detailed explanation of the JSX.
You can now delete the logo.svg file, since it was part of the template and you are not referencing it anymore:
Now, update the styling.
Open src / App.css:
Replace the contents with the following CSS to center the elements and adjust the font:
This uses flex to center the main < h1 > and list elements.
It also removes default button styles and < li > styles so the emojis line up in a row.
More details can be found at How to Create React Elements with JSX.
Open another terminal window in the root of your project.
Start the project with the following command:
After the command runs, you'll see the project running in your web browser at http: / / localhost: 3000.
Leave this running the entire time you work on your project.
Every time you save the project, the browser will auto-refresh and show the most up-to-date code.
You will see your project page with Hello, World and the three emojis that you listed in your App.js file:
Browser with emoji
Now that you've set up your code, you can now start putting together components in React.
Step 2 - Creating an Independent Component with React Classes
Now that you have your project running, you can start making your custom component.
In this step, you'll create an independent React component by extending the base React Component class.
You'll create a new class, add methods, and use the render function to show data.
React components are self-contained elements that you can reuse throughout a page.
By making small, focused pieces of code, you can move and reuse pieces as your application grows.
The key here is that they are self-contained and focused, allowing you to separate out code into logical pieces.
In fact, you have already been working with logically separated components: The App.js file is a functional component, one that you will see more of in Step 3.
There are two types of custom component: class-based and functional.
The first component you are going to make is a class-based component.
You will make a new component called Instructions that explains the instructions for the emoji viewer.
< $> note Note: Class-based components used to be the most popular way of creating React components.
But with the introduction of React Hooks, many developers and libraries are shifting to using functional components.
Though functional components are now the norm, you will often find class components in legacy code.
You don't need to use them, but you do need to know how to recognize them.
They also give a clear introduction to many future concepts, such as state management.
In this tutorial, you'll learn to make both class and functional components.
To start, create a new file.
By convention, component files are capitalized:
Then open the file in your text editor:
First, import React and the Component class and export Instructions with the following lines:
Importing React will convert the JSX.
Component is a base class that you'll extend to create your component.
To extend that, you created a class that has the name of your component (Instructions) and extended the base Component with the export line.
You're also exporting this class as the default with export default keywords at the start of the class declaration.
The class name should be capitalized and should match the name of the file.
This is important when using debugging tools, which will display the name of the component.
If the name matches the file structure, it will be easier to locate the relevant component.
The base Component class has several methods you can use in your custom class.
The most important method, and the only one you'll use in this tutorial, is the render () method.
The render () method returns the JSX code that you want to display in the browser.
To start, add a little explanation of the app in a < p > tag:
At this point, there's still no change to your browser.
That's because you haven't used the new component yet.
To use the component, you'll have to add it into another component that connects to the root component.
In this project, < App > is the root component in index.js.
To make it appear in your application, you'll need to add to the < App > component.
Open src / App.js in a text editor:
First, you'll need to import the component:
Since it's the default import, you could import to any name you wanted.
It's best to keep the names consistent for readability - the import should match the component name, which should match the file name - but the only firm rule is that the component must start with a capital letter.
That's how React knows it's a React component.
Now that you've imported the component, add it to the rest of your code as if it were a custom HTML element:
In this code, you wrapped the component with angle brackets.
Since this component doesn't have any children, it can be self closing by ending with / >.
When you do, the page will refresh and you'll see the new component.
Browser with instruction text
Now that you have some text, you can add an image.
Download an emoji image from wikimedia and save it in the src directory as emoji.svg with the following command:
curl makes the request to the URL, and the -o flag allows you to save the file as src / emoji.svg.
Next, open your component file:
Import the emoji and add it to your custom component with a dynamic link:
Notice that you need to include the file extension .svg when importing.
When you import, you are importing a dynamic path that is created by webpack when the code compiles.
For more information, refer to How To Set Up a React Project with Create React App.
You also need to wrap the < img > and < p > tags with empty tags to ensure that you are returning a single element.
When you reload, the image will be very large compared to the rest of the content:
Browser window with large emoji image
To make the image smaller, you'll need to add some CSS and a className to your custom component.
First, in Instructions.js, change the empty tags to a div and give it a className of instructions:
Next open App.css:
Create rules for the .instructions class selector:
When you add a display of flex styling, you make the img and the p centered with flexbox.
You changed the direction so that everything lines up vertically with flex-direction: column;.
The line align-items: center; will center the elements on the screen.
Now that your elements are lined up, you need to change the image size.
Give the img inside the div a width and height of 100px.
The browser will reload and you'll see the image is much smaller:
Browser window with smaller image
At this point, you've created an independent and reusable custom component.
To see how it's reusable, add a second instance to App.js.
In App.js, add a second instance of the component:
When the browser reloads, you'll see the component twice.
Browser with two instances of the Instructions component
In this case, you wouldn't want two instances of Instructions, but you can see that the component can be efficiently reused.
When you create custom buttons or tables, you will likely use them multiple times on one page, making them perfect for custom components.
For now, you can delete the extra image tag.
In your text editor, delete the second < Instructions / > and save the file:
Now you have a reusable, independent component that you can add to a parent component multiple times.
The structure you have now works for a small number of components, but there is a slight problem.
All of the files are mixed together.
The image for < Instructions > is in the same directory as the assets for < App >.
You also are mixing the CSS code for < App > with the CSS for < Instructions >.
In the next step, you'll create a file structure that will give each component independence by grouping their functionality, styles, and dependencies together, giving you the ability to move them around as you need.
Step 3 - Creating a Readable File Structure
In this step, you'll create a file structure to organize your components and their assets, such as images, CSS, and other JavaScript files.
You'll be grouping code by component, not by asset type.
In other words, you won't have a separate directory for CSS, images, and JavaScript.
Instead you'll have a separate directory for each component that will contain the relevant CSS, JavaScript, and images.
In both cases, you are separating concerns.
Since you have an independent component, you need a file structure that groups the relevant code.
Currently, everything is in the same directory.
List out the items in your src directory:
The output will show that things are getting pretty cluttered:
You have code for the < App > component (App.css, App.js, and App.test.js) sitting alongside your root component (index.css and index.js) and your custom component Instructions.js.
React is intentionally agnostic about file structure.
It does not recommend a particular structure, and the project can work with a variety of different file hierarchies.
But we recommend to add some order to avoid overloading your root directory with components, CSS files, and images that will be difficult to navigate.
Also, explicit naming can make it easier to see which pieces of your project are related.
For example, an image file named Logo.svg may not clearly be part of a component called Header.js.
One of the simplest structures is to create a components directory with a separate directory for each component.
This will allow you to group your components separately from your configuration code, such as serviceWorker, while grouping the assets with the components.
Creating a Components Directory
To start, create a directory called components:
Next, move the following components and code into the directory: App.css, App.js, App.test.js, Instructions.js, and emoji.svg:
Here, you are using a wildcard (*) to select all files that start with App..
After you move the code, you'll see an error in your terminal running npm start.
Remember, all of the code is importing using relative paths.
If you change the path for some files, you'll need to update the code.
To do that, open index.js.
Then change the path of the App import to import from the components / directory.
Your script will detect the changes and the error will disappear.
Now you have components in a separate directory.
As your applications become more complex, you may have directories for API services, data stores, and utility functions.
Separating component code is the first step, but you still have CSS code for Instructions mixed in the App.css file.
To create this logical separation, you will first move the components into separate directories.
Moving Components to Individual Directories
First, make a directory specifically for the < App > component:
Then move the related files into the new directory:
When you do you'll get a similar error to the last section:
In this case, you'll need to update two things.
First, you'll need to update the path in index.js.
Open the index.js file:
Then update the import path for App to point to the App component in the App directory.
The application still won't run.
You'll see an error like this:
Since < Instructions > is not on the same directory level as the < App > component, you'll need to change the import path.
Before that, create a directory for Instructions.
Make a directory called Instructions in the src / components directory:
Then move Instructions.js and emoji.svg into that directory:
Now that the Instructions component directory has been created, you can finish updating the file paths to connect your component to your app.
Updating import Paths
Now that components are in individual directories, you can adjust the import path in App.js.
Since the path is relative, you'll need to move up one directory - src / components - then into the Instructions directory for Instructions.js, but since this is a JavaScript file, you don't need the final import.
Now that your imports are all using the correct path, you're browser will update and show the application.
< $> note Note: You can also call the root file in each directory index.js.
For example, instead of src / components / App / App.js you could create src / components / App / index.js.
The advantage to this is that your imports are slightly smaller.
If the path points to a directory, the import will look for an index.js file.
The import for src / components / App / index.js in the src / index.js file would be import. / components / App.
The disadvantage of this approach is that you have a lot of files with the same name, which can make it difficult to read in some text editors.
Ultimately, it's a personal and team decision, but it's best to be consistent.
Separating Code in Shared Files
Now each component has its own directory, but not everything is fully independent.
The last step is to extract the CSS for Instructions to a separate file.
First, create a CSS file in src / components / Instructions:
Next, open the CSS file in your text editor:
Add in the instructions CSS that you created in an earlier section:
Next, remove the instructions CSS from src / components / App / App.css.
Remove the lines about .instructions.
The final file will look like this:
Finally, import the CSS in Instructions.js:
Import the CSS using the relative path:
Your browser window will look as it did before, except now all the file assets are grouped in the same directory.
Now, take a final look at the structure.
First, the src / directory:
You have the root component index.js and the related CSS index.css next to the components / directory and utility files such as serviceWorker.js and setupTests.js:
Next, look inside components:
You'll see a directory for each component:
If you look inside each component, you'll see the component code, CSS, test, and image files if they exist.
At this point, you've created a solid structure for your project.
You moved a lot of code around, but now that you have a structure, it will scale easier.
This is not the only way to compose your structure.
Some file structures can take advantage of code splitting by specifying a directory that will be split into different packages.
Other file structures split by route and use a common directory for components that are used across routes.
For now, stick with a less complex approach.
As a need for another structure emerges, it's always easier to move from simple to complex.
Starting with a complex structure before you need it will make refactoring difficult.
Now that you have created and organized a class-based component, in the next step you'll create a functional component.
Step 4 - Building a Functional Component
In this step, you'll create a functional component.
Functional components are the most common component in contemporary React code.
These components tend to be shorter, and unlike class-based components, they can use React hooks, a new form of state and event management.
A functional component is a JavaScript function that returns some JSX.
It doesn't need to extend anything and there are no special methods to memorize.
To refactor < Instructions > as a functional component, you need to change the class to a function and remove the render method so that you are left with only the return statement.
To do that, first open Instructions.js in a text editor.
Change the class declaration to a function declaration:
Next, remove the import of {Component}:
Finally, remove the render () method.
At that point, you are only returning JSX.
The browser will refresh and you'll see your page as it was before.
You could also rewrite the function as an arrow function using the implicit return.
The main difference is that you lose the function body.
You will also need to first assign the function to a variable and then export the variable:
Simple functional components and class-based components are very similar.
When you have a simple component that doesn't store state, it's best to use a functional component.
The real difference between the two is how you store a component's state and use properties.
Class-based components use methods and properties to set state and tend to be a little longer.
Functional components use hooks to store state or manage changes and tend to be a little shorter.
Now you have a small application with independent pieces.
You created two major types of components: functional and class.
You separated out parts of the components into directories so that you could keep similar pieces of code grouped together.
You also imported and reused the components.
With an understanding of components, you can start to look at your applications as pieces that you can take apart and put back together.
Projects become modular and interchangable.
The ability to see whole applications as a series of components is an important step in thinking in React.
If you would like to look at more React tutorials, take a look at our React Topic page, or return to the How To Code in React.js series page.
How To Install and Configure Postfix as a Send-Only SMTP Server on Ubuntu 18.04
4103
Postfix is a mail transfer agent (MTA), an application used to send and receive email.
It can be configured so that it can be used to send emails by local application only.
This is useful in situations when you need to regularly send email notifications from your apps or simply have a lot of outbound traffic that a third-party email service provider won't allow.
It's also a lighter alternative to running a full-blown SMTP server, while retaining the required functionality.
In this tutorial, you'll install and configure Postfix as a send-only SMTP server.
You'll also request free TLS certificates from Let's Encrypt for your domain and encrypt the outbound emails using them.
One Ubuntu 18.04 server set up with the Initial Server Setup with Ubuntu 18.04, including creating a sudo non-root user.
A fully registered domain name.
This tutorial will use < ^ > your _ domain < ^ > throughout.
An A DNS record with < ^ > your _ domain < ^ > pointing to your server's public IP address.
< $> note Note: Your server's hostname and your Droplet's name must match < ^ > your _ domain < ^ >, because DigitalOcean automatically sets PTR records for the Droplet's IP address according to its name.
You can verify the server's hostname by typing hostname at the command prompt.
The output should match the name you gave the Droplet when it was being created.
Step 1 - Installing Postfix
In this step, you'll install Postfix.
The fastest way is to install the mailutils package, which bundles Postfix with a few supplementary programs that you'll use to test sending email.
First, update the package database:
Then, install Postfix by running the following command:
Near the end of the installation process, you will be presented with the Postfix configuration window:
Select Internet Site from the menu, then press TAB to select
, then ENTER
The default option is Internet Site.
That's the recommended option for your use case, so press TAB, and then ENTER.
If you only see the description text, press TAB to select OK, then ENTER.
If it does not show up automatically, run the following command to start it:
After that, you'll get another configuration prompt regarding the System mail name:
Enter your domain name, then press TAB to select
, ENTER
The System mail name must be the same as the name you assigned to your server when you were creating it. When you've finished, press TAB, followed by ENTER.
You have now installed Postfix and are ready to start configuring it.
Step 2 - Configuring Postfix
In this step, you'll configure Postfix to send and receive emails only from the server on which it is running on - that is, from localhost.
For that to happen, Postfix needs to be configured to listen only on the loopback interface, the virtual network interface that the server uses to communicate internally.
To make the changes, you'll need to edit the main Postfix configuration file called main.cf, stored under etc / postfix.
Open it for editing using your favorite text editor:
Find the following lines:
Set the value of the inet _ interfaces setting to loopback-only:
Another directive you'll need to modify is mydestination, which is used to specify the list of domains that are delivered via the local _ transport mail delivery transport.
By default, the values are similar to these:
Change the line to look like this:
If your domain is actually a subdomain, and you'd want the email messages to look as if they were sent from the main domain, you can add the following line to the end of main.cf:
The optional masquerade _ domains setting specifies for which domains the subdomain part will be stripped off in the email address.
When you are done, save and close the file.
< $> note Note: If you're hosting multiple domains on a single server, the other domains can also be passed to Postfix using the mydestination directive.
Then, restart Postfix by running the following command:
You've configured Postfix to only send emails from your server.
You'll now test it by sending an example message to an email address.
Step 3 - Testing the SMTP Server
In this step, you'll test whether Postfix can send emails to an external email account using the mail command, which is part of the mailutils package that you installed in the first step.
To send a test email, run the following command:
You can change the body and the subject of the email to your liking.
Remember to replace < ^ > your _ email _ address < ^ > with a valid email address that you can access.
Now, check the email address to which you sent this message.
You should see the message in your inbox.
If it's not there, check your spam folder.
At this point, all emails you send are unencrypted, which makes service providers think it's likely spam.
You'll set up encryption later, in step 5.
If you receive an error from the mail command, or you haven't received a message after prolonged periods of time, check that the Postfix configuration you edited is valid and that your server's name and hostname are set to your domain.
Note that with this configuration, the address in the From field for the test emails you send will be in the form of < ^ > your _ user _ name < ^ > @ < ^ > your _ domain < ^ >, where < ^ > your _ user _ name < ^ > is the username of the server user you ran the command as.
You have now sent an email from your server and verified that it's successfully received.
In the next step, you'll set up email forwarding for root.
Step 4 - Forwarding System Mail
In this step, you'll set up email forwarding for user root, so that system-generated messages sent to it on your server get forwarded to an external email address.
The / etc / aliases file contains a list of alternate names for email recipients.
Open it for editing:
In its default state, it looks like this:
The only directive present specifies that system generated emails are sent to root.
Add the following line to the end of the file:
With this line, you specify that emails sent to root end up being forwarded to an email address.
Remember to replace < ^ > your _ email _ address < ^ > with your personal email address.
For the change to take effect, run the following command:
Running newaliases will build up a database of aliases that the mail command uses, which are taken from the config file you just edited.
Test that sending emails to root works by running:
You should receive the email at your email address.
In this step, you have set up forwarding system-generated messages to your email address.
You'll now enable message encryption, so that all emails your server sends are immune to tampering in transit and will be viewed as more legitimate.
Step 5 - Enabling SMTP Encryption
You'll now enable SMTP encryption by requesting a free TLS certificate from Let's Encrypt for your domain (using Certbot) and configuring Postfix to use it when sending messages.
Ubuntu includes Certbot in their default package repository, but it may happen that it's out of date.
Instead, you'll add the official repository by running the following command:
Press ENTER when prompted to accept.
Then, update your server's package manager cache:
Finally, install the latest version of Certbot:
As part of the initial server setup in the prerequisites, you installed ufw, the uncomplicated firewall.
You'll need to configure it to allow the HTTP port 80, so that domain verification can be completed.
Run the following command to enable it:
Now that the port is open, run Certbot to get a certificate:
This command orders Certbot to issue certificates with an RSA key size of 4096 bits, to run a temporary standalone web server (--standalone) for verification, and to check via port 80 (--preferred-challenges http).
Remember to replace < ^ > your _ domain < ^ > with your domain before running the command, and enter your email address when prompted.
As written in the notes, your certificate and private key file were saved under / etc / letsencrypt / live / < ^ > your _ domain < ^ >.
Now that you have your certificate, open main.cf for editing:
Find the following section:
Modify it to look like this, replacing < ^ > your _ domain < ^ > with your domain where necessary, which will update your TLS settings for Postfix:
Once you're done, save and close the file.
Apply the changes by restarting Postfix:
Now, try sending an email again:
Then, check the email address you provided.
It's possible that you'll see the message in your inbox immediately, because email providers are much more likely to mark unencrypted messages as spam.
You can check the technical info about the email message in your client to see that the message is indeed encrypted.
You now have a send-only email server, powered by Postfix.
Encrypting all outgoing messages is a good first step to email providers not marking your messages as spam outright.
If you are doing this in a development scenario, then this measure should be enough.
However, if your use case is to send emails to potential site users (such as confirmation emails for a message board sign-up), you should look into setting up SPF records, so that your server's emails are even more likely to be seen as legitimate.
How To Install and Use Linkerd with Kubernetes
5249
The author selected the Tech Education Fund to receive a donation as part of the Write for DOnations program.
A service mesh is a dedicated infrastructure layer that helps administrators handle service-to-service communication.
Offering many powerful tools, these service meshes can make your system safer, more reliable, and more visible too.
A service mesh like Linkerd, for example, can automatically encrypt connections, handle request retries and timeouts, provide telemetry information like success rates and latencies, and more.
In this tutorial, you will install the Linkerd service mesh in your Kubernetes cluster, deploy an example application, and then explore Linkerd's dashboard.
After familiarizing yourself with some of this dashboard information, you will configure Linkerd to enforce timeout and retry policies for a particular Kubernetes pod.
Alternately, consider exploring DigitalOcean's one-click Linkerd / Kubernetes installation option.
A Kubernetes 1.12 + cluster.
In this tutorial, the setup will use a DigitalOcean Kubernetes cluster with three nodes, but you are free to create a cluster using another method.
The kubectl command-line tool installed on a development server and configured to connect to your cluster.
You can read more about installing kubectl in its official documentation.
Step 1 - Deploying the Application
To see Linkerd in action, you need to have an application running in your cluster.
In this step you will deploy an application called emojivoto, which the Linkerd team created for this purpose.
In this repository, you can see the code for the four services that compose the application, as well as the manifest file that you will use to deploy these services in your Kubernetes cluster.
First, save this manifest file locally:
You are using curl to fetch the file, then passing the --output option to tell it where you want the file saved.
In this case, you are creating a file called manifest.yaml.
To better understand what this file will accomplish, inspect its contents with cat or open it with your favorite editor:
Press SPACE to page through the directives.
You will see that manifest.yaml is creating a Kubernetes namespace called emojivoto where everything related to this application will run, and a couple of Kubernetes Deployments and Services.
Next, apply this manifest in your Kubernetes cluster:
Again, you're using kubectl apply with the -f flag to assign a file that you want to apply.
This command will output a list of all the resources that were created:
Now check that the services are running:
You are using kubectl to list all the pods you have running in your cluster, and then passing the -n flag to indicate which namespaces you want to use.
You're passing the emojivoto namespace because that is where you are running all these services.
When you see all the pods in the Running state, you are good to go:
Finally, to see the application running in your browser, you will use the kubectl built-in feature of forwarding local requests to your remote cluster:
< $> note Note: If you are not running this from your local machine, you will need to add the --address 0.0.0.0 flag to listen on all addresses and not only localhost.
Here you are again using kubectl in the emojivoto namespaces, but now calling the port-forward sub-command and directing it to forward all local requests on port 8080 to the Kubernetes service web-svc, on port 80. This is just a convenient way for you to access your application without needing to have a proper load balancer in place.
Now visit http: / / localhost: 8080 and you will see the emojivoto application.
Emojivoto sample application
Press CTRL + C in your terminal.
With an application running in your cluster, you are now ready to install Linkerd and see how it works.
Step 2 - Installing Linkerd
Now that you have an application running, let's install Linkerd.
To install it in your Kubernetes cluster, you first need the Linkerd CLI.
You will use this command line interface to interact with Linkerd from your local machine.
After that, you can install Linkerd in your cluster.
First, let's install the CLI with the script provided by the Linkerd team:
Here you are using curl to download the installation script, and then you are piping the output to sh, which automatically executes the script.
Alternatively, you can download the CLI directly from Linkerd's release page.
If you use the script then it will install Linkerd at ~ / .linkerd2 / bin.
Now confirm that the CLI is working correctly:
The command will output something like this:
Then, to make it easier to run the CLI, add this directory to your $PATH:
Now you can more directly run the commands, like the previous one:
Finally, let's install Linkerd in your Kubernetes cluster.
The linkerd install command is used to generate all the necessary yaml manifests needed to run Linkerd, but it will not apply these manifests to your cluster.
Run this command to inspect its output:
You will see a long output, listing all the yaml manifests for resources Linkerd needs to run.
To apply these manifests to your cluster, run:
Running linkerd install will output all the manifests you saw previously. | then pipes this output directly to kubectl apply, which will apply them.
After you run this command, kubectl apply will output a list of all the resources that were created.
To confirm that everything is running in your cluster, run linkerd check:
This will run several checks against your cluster to confirm that all the necessary components are running:
Finally, run this command to open the built-in Linkerd dashboard in your browser (remember you will need to provide the --address 0.0.0.0 flag if you are not running this from your local machine):
Linkerd Dashboard
Most of the information you see in the dashboard you can obtain using the Linkerd CLI.
For example, run this command to see high-level stats deployments:
Here you are saying that you want the stats for deployments running in the linkerd namespace.
These are Linkerd's own components, and, interestingly, you can use Linkerd itself to monitor them.
You can see stats like requests per second (RPS), success rate, latency, and more.
You can also see a Meshed column, which indicates how many pods Linkerd has injected:
Now try this command in your emojivoto namespace:
Even though you can see your four services, none of the stats you saw before are available for these deployments, and in the "Meshed" column you can see that it says 0 / 1:
The output here means that you still have not injected Linkerd into the application.
This will be your next step.
Step 3 - Injecting Linkerd into Your Application
Now that you have Linkerd running in your cluster, you are ready to inject it into your emojivoto application.
Linkerd works by running a sidecar container in your Kubernetes pods.
That is, you will inject a linkerd proxy container into every pod you have running.
Every request that your pods then send or receive will go through this very lightweight proxy that can gather metrics (like success rate, requests per second, and latency) and enforce policies (like timeouts and retries).
You can manually inject Linkerd's proxy with this command:
In this command, you are first using kubectl get to get all the Kubernetes deployments you have running in the emojivoto namespace, and then specifying that you want the output in the yaml format.
Then you are sending that output to the linkerd inject command.
This command will read the yaml file with the current manifests you have running and modify it to include the linkerd proxy alongside every deployment.
Lastly, you are receiving this modified manifest and applying it to your cluster with kubectl apply.
After running this command you will see a message saying that all four emojivoto services (emoji, vote-bot, voting, and web) were successfully injected.
If you now retrieve stats for emojivoto, you will see that all your deployments are now meshed, and after a few seconds you will start to see the same stats that you saw for the linkerd namespace:
Here you can see the stats for all the four services that compose the emojivoto application, with their respective success rate, requests per second and latency, without having to write or change any application code.
The vote-bot service doesn't show any stats because it is just a bot that sends requests to other services and therefore is not receiving any traffic, which is in itself valuable information.
Now let's see how you can provide some extra information to Linkerd about your services to customize their behavior.
Step 4 - Defining a Service Profile
Now that you have injected Linkerd into your application, you can begin to retrieve valuable information about how each of your services is behaving.
Moreover, you have accomplished this without needing to write any custom configurations or change your application's code.
However, if you do provide Linkerd with some additional information, it can then enforce numerous policies, like timeouts and retries.
It can also then provide per-route metrics.
This information is provided through a Service Profile, which is a custom Linkerd resource where you can describe the routes in your applications and how each one will behave.
Here's an example of what a Service Profile manifest looks like:
The Service Profile describes a list of routes, and then defines how requests that match the specified condition will behave.
In this example, you are saying that every GET request sent to / my / route / path will timeout after 100ms, and, if they fail, they can be retried.
Let's now create a Service Profile for one of your services.
Taking voting-svc as an example, first use the Linkerd CLI to check the routes you have defined for this service:
Here you are using the linkerd routes command to list all the routes for the service voting-svc, in the emojiovoto namespace:
You will find only one route, [DEFAULT].
This is where all requests are grouped until you define your Service Profile.
Now open nano or your favorite editor to create a service-profile.yaml file:
Add the following Service Profile definition into this file:
Now save the file and close your editor.
Here you are declaring a Service Profile for the voting-svc service, in the emojivoto namespace.
You have defined one route, called VoteDoughnut, which will match any POST request to the / emojivoto.v1.
VotingService / VoteDoughnut path.
If a request matching these criteria takes more than 100ms, Linkerd will cancel it and the client will receive a 504 response back.
You are also telling Linkerd that if this request fails it can be retried.
Now apply this file to your cluster:
After a few seconds, recheck the routes for this service:
You will now see your newly defined VoteDoughnut route:
You can see several custom metrics, like success rate, requests per second, and latency for this specific route.
Note that the VoteDoughnut endpoint is intentionally configured to always return an error, and that it is outputting a success rate of 0%, while the [DEFAULT] route is outputting 100%.
So now, after giving Linkerd a little bit of information about your service, you have custom metrics per route, as well as two policies enforced: timeouts and retries.
In this article, you installed Linkerd in your Kubernetes cluster and used it to monitor a sample application.
You extracted useful telemetry information like success rate, throughput, and latency.
You also configured a Linkerd Service Profile to collect per-route metrics and enforce two policies in the emojivoto application.
If you are interested in learning more about Linkerd, you can browse their great documentation page, where they show how to secure your services, configure distributed tracing, automate canary releases, and much more.
From here you might also consider checking out Istio, which is another Service Mesh with a different set of features and trade-offs.
How To Install and Use PostgreSQL on Ubuntu 20.04
5245
Relational database management systems are a key component of many web sites and applications.
They provide a structured way to store, organize, and access information.
PostgreSQL, or Postgres, is a relational database management system that provides an implementation of the SQL querying language.
It's standards-compliant and has many advanced features like reliable transactions and concurrency without read locks.
This guide demonstrates how to install Postgres on an Ubuntu 20.04 server.
It also provides some instructions for general database administration.
To follow along with this tutorial, you will need one Ubuntu 20.04 server that has been configured by following our Initial Server Setup for Ubuntu 20.04 guide.
After completing this prerequisite tutorial, your server should have a non-root user with sudo permissions and a basic firewall.
Step 1 - Installing PostgreSQL
Ubuntu's default repositories contain Postgres packages, so you can install these using the apt packaging system.
If you've not done so recently, refresh your server's local package index:
Then, install the Postgres package along with a -contrib package that adds some additional utilities and functionality:
Now that the software is installed, we can go over how it works and how it may be different from other relational database management systems you may have used.
Step 2 - Using PostgreSQL Roles and Databases
By default, Postgres uses a concept called "roles" to handle authentication and authorization.
These are, in some ways, similar to regular Unix-style accounts, but Postgres does not distinguish between users and groups and instead prefers the more flexible term "role".
Upon installation, Postgres is set up to use ident authentication, meaning that it associates Postgres roles with a matching Unix / Linux system account.
If a role exists within Postgres, a Unix / Linux username with the same name is able to sign in as that role.
The installation procedure created a user account called postgres that is associated with the default Postgres role.
In order to use Postgres, you can log into that account.
There are a few ways to utilize this account to access Postgres.
Switching Over to the postgres Account
Switch over to the postgres account on your server by typing:
You can now access the PostgreSQL prompt immediately by typing:
From there you are free to interact with the database management system as necessary.
Exit out of the PostgreSQL prompt by typing:
This will bring you back to the postgres Linux command prompt.
Accessing a Postgres Prompt Without Switching Accounts
You can also run the command you'd like with the postgres account directly with sudo.
For instance, in the last example, you were instructed to get to the Postgres prompt by first switching to the postgres user and then running psql to open the Postgres prompt.
You could do this in one step by running the single command psql as the postgres user with sudo, like this:
This will log you directly into Postgres without the intermediary bash shell in between.
Again, you can exit the interactive Postgres session by typing:
Many use cases require more than one Postgres role.
Read on to learn how to configure these.
Step 3 - Creating a New Role
Currently, you just have the postgres role configured within the database.
You can create new roles from the command line with the createrole command.
The --interactive flag will prompt you for the name of the new role and also ask whether it should have superuser permissions.
If you are logged in as the postgres account, you can create a new user by typing:
If, instead, you prefer to use sudo for each command without switching from your normal account, type:
The script will prompt you with some choices and, based on your responses, execute the correct Postgres commands to create a user to your specifications.
You can get more control by passing some additional flags.
Check out the options by looking at the man page:
Your installation of Postgres now has a new user, but you have not yet added any databases.
The next section describes this process.
Step 4 - Creating a New Database
Another assumption that the Postgres authentication system makes by default is that for any role used to log in, that role will have a database with the same name which it can access.
This means that if the user you created in the last section is called sammy, that role will attempt to connect to a database which is also called "sammy" by default.
You can create the appropriate database with the createdb command.
If you are logged in as the postgres account, you would type something like:
If, instead, you prefer to use sudo for each command without switching from your normal account, you would type:
This flexibility provides multiple paths for creating databases as needed.
Step 5 - Opening a Postgres Prompt with the New Role
To log in with ident based authentication, you'll need a Linux user with the same name as your Postgres role and database.
If you don't have a matching Linux user available, you can create one with the adduser command.
You will have to do this from your non-root account with sudo privileges (meaning, not logged in as the postgres user):
Once this new account is available, you can either switch over and connect to the database by typing:
Or, you can do this inline:
This command will log you in automatically, assuming that all of the components have been properly configured.
If you want your user to connect to a different database, you can do so by specifying the database like this:
Once logged in, you can get check your current connection information by typing:
This is useful if you are connecting to non-default databases or with non-default users.
Step 6 - Creating and Deleting Tables
Now that you know how to connect to the PostgreSQL database system, you can learn some basic Postgres management tasks.
The basic syntax for creating tables is as follows:
As you can see, these commands give the table a name, and then define the columns as well as the column type and the max length of the field data. You can also optionally add table constraints for each column.
You can learn more about how to create and manage tables in Postgres here.
For demonstration purposes, create the following table:
This command will create a table that inventories playground equipment.
The first column in the table will hold equipment ID numbers of the serial type, which is an auto-incrementing integer.
This column also has the constraint of PRIMARY KEY which means that the values within it must be unique and not null.
The next two lines create columns for the equipment type and color respectively, neither of which can be empty.
The line after these creates a location column as well as a constraint that requires the value to be one of eight possible values.
The last line creates a date column that records the date on which you installed the equipment.
For two of the columns (equip _ id and install _ date), the command doesn't specify a field length.
The reason for this is that some data types don't require a set length because the length or format is implied.
You can see your new table by typing:
Your playground table is here, but there "s also something called playground _ equip _ id _ seq that is of the type sequence.
This is a representation of the serial type which you gave your equip _ id column.
This keeps track of the next number in the sequence and is created automatically for columns of this type.
If you want to see just the table without the sequence, you can type:
With a table at the ready, let's use it to practice managing data.
Step 7 - Adding, Querying, and Deleting Data in a Table
Now that you have a table, you can insert some data into it. As an example, add a slide and a swing by calling the table you want to add to, naming the columns and then providing data for each column, like this:
You should take care when entering the data to avoid a few common hangups.
For one, do not wrap the column names in quotation marks, but the column values that you enter do need quotes.
Another thing to keep in mind is that you do not enter a value for the equip _ id column.
This is because this is automatically generated whenever you add a new row to the table.
Retrieve the information you've added by typing:
Here, you can see that your equip _ id has been filled in successfully and that all of your other data has been organized correctly.
If the slide on the playground breaks and you have to remove it, you can also remove the row from your table by typing:
Query the table again:
Notice that the slide row is no longer a part of the table.
Step 8 - Adding and Deleting Columns from a Table
After creating a table, you can modify it by adding or removing columns.
Add a column to show the last maintenance visit for each piece of equipment by typing:
If you view your table information again, you will see the new column has been added but no data has been entered:
If you find that your work crew uses a separate tool to keep track of maintenance history, you can delete of the column by typing:
This deletes the last _ maint column and any values found within it, but leaves all the other data intact.
Step 9 - Updating Data in a Table
So far, you "ve learned how to add records to a table and how to delete them, but this tutorial hasn" t yet covered how to modify existing entries.
You can update the values of an existing entry by querying for the record you want and setting the column to the value you wish to use.
You can query for the swing record (this will match every swing in your table) and change its color to red. This could be useful if you gave the swing set a paint job:
You can verify that the operation was successful by querying the data again:
As you can see, the slide is now registered as being red.
You are now set up with PostgreSQL on your Ubuntu 20.04 server.
If you'd like to learn more about Postgres and how to use it, we encourage you to check out the following guides:
A comparison of relational database management systems
Practice running queries with SQL
How To Install Jitsi Meet on Ubuntu 18.04
4104
Jitsi Meet is an open-source video-conferencing application based on WebRTC.
A Jitsi Meet server provides multi-person video conference rooms that you can access using nothing more than your browser and provides comparable functionality to a Zoom or Skype conference call.
The benefit of a Jitsi conference is that all your data only passes through your server, and the end-to-end TLS encryption ensures that no one can snoop on the call.
With Jitsi you can be sure that your private information stays that way.
In this tutorial, you will install and configure a Jitsi Meet server on Ubuntu 18.04.
The default configuration allows anyone to create a new conference room.
This is not ideal for a server that is publicly available on the internet so you will also configure Jitsi Meet so that only registered users can create new conference rooms.
After you have created the conference room, any users can join, as long as they have the unique address and the optional password.
One Ubuntu 18.04 server set up by following the Initial Server Setup with Ubuntu 18.04 tutorial, including a non-root sudo-enabled user.
The size of the server you will need mostly depends on the available bandwidth and the number of participants you expect to be using the server.
The following table will give you some idea of what is needed.
Throughout this guide, the example domain name < ^ > jitsi.your-domain < ^ > is used.
When you are choosing a server to run your Jitsi Meet instance you will need to consider the system resources needed to host conference rooms.
The following benchmark information was collected from a single-core virtual machine using high-quality video settings:
CPU
Server Bandwidth
Two Participants
3%
30Kbps Up, 100Kbps Down
Three Participants
15%
7Mbps Up, 6.5Mbps Down
The jump in resource use between two and three participants is because Jitsi will route the call data directly between the clients when there are two of them.
When more than two clients are present then call data is routed through the Jitsi Meet server.
Step 1 - Setting the System Hostname
In this step, you will change the system's hostname to match the domain name that you intend to use for your Jitsi Meet instance and resolve that hostname to the localhost IP, 127.0.0.1.
Jitsi Meet uses both of these settings when it installs and generates its configuration files.
First, set the system's hostname to the domain name that you will use for your Jitsi instance.
The following command will set the current hostname and modify the / etc / hostname that holds the system's hostname between reboots:
The command that you ran breaks down as follows:
hostnamectl is a utility from the systemd tool suite to manage the system hostname.
set-hostname sets the system hostname.
Check that this was successful by running the following:
This will return the hostname you set with the hostnamectl command:
Next, you will set a local mapping of the server's hostname to the loopback IP address, 127.0.0.1.
Do this by opening the / etc / hosts file with a text editor:
Then, add the following line:
Mapping your Jitsi Meet server "s domain name to 127.0.0.1 allows your Jitsi Meet server to use several networked processes that accept local connections from each other on the 127.0.0.1 IP address.
These connections are authenticated and encrypted with a TLS certificate, which is registered to your domain name.
Locally mapping the domain name to 127.0.0.1 makes it possible to use the TLS certificate for these local network connections.
Your server now has the hostname that Jitsi requires for installation.
In the next step, you will open the firewall ports that are needed by Jitsi and the TLS certificate installer.
Step 2 - Configuring the Firewall
When you followed the Initial Server Setup with Ubuntu 18.04 guide you enabled the UFW firewall and opened the SSH port. The Jitsi server needs some ports opened so that it can communicate with the call clients.
Also, the TLS installation process needs to have a port open so that it can authenticate the certificate request.
The ports that you will open are the following:
80 / tcp used in the TLS certificate request.
443 / tcp used for the conference room creation web page.
4443 / tcp, 10000 / udp used to transmit and receive the encrypted call traffic.
Run the following ufw commands to open these ports:
Check that they were all added with the ufw status command:
You will see the following output if these ports are open:
The server is now ready for the Jitsi installation, which you will complete in the next step.
Step 3 - Installing Jitsi Meet
In this step, you will add the Jitsi stable repository to your server and then install the Jitsi Meet package from that repository.
This will ensure that you are always running the latest stable Jitsi Meet package.
First, download the Jitsi GPG key with the wget downloading utility:
The apt package manager will use this GPG key to validate the packages that you will download from the Jitsi repository.
Next, add the GPG key you downloaded to apt's keyring using the apt-key utility:
You can now delete the GPG key file as it is no longer needed:
Now, you will add the Jitsi repository to your server by creating a new source file that contains the Jitsi repository.
Open and create the new file with your editor:
Add this line to the file for the Jitsi repository:
Save and exit your editor.
Finally, perform a system update to collect the package list from the Jitsi repository and then install the jitsi-meet package:
During the installation of jitsi-meet you will be prompted to enter the domain name (for example, jitsi.your-domain) that you want to use for your Jitsi Meet instance.
Image showing the jitsi-meet installation hostname dialog
< $> note Note: You move the cursor from the hostname field to highlight the
button with the TAB key.
Press ENTER when
is highlighted to submit the hostname.
You will then be shown a new dialog box that asks if you want Jitsi to create and use a self-signed TLS certificate or use an existing one you already have:
Image showing the jitsi-meet installation certificate dialog
If you do not have a TLS certificate for your Jitsi domain select the first, Generate a new self-signed certificate, option.
Your Jitsi Meet instance is now installed using a self-signed TLS certificate.
This will cause browser warnings, so you will get a signed TLS certificate in the next step.
Step 4 - Obtaining a Signed TLS Certificate
Jitsi Meet uses TLS certificates to encrypt the call traffic so that no one can listen to your call as it travels over the internet.
TLS certificates are the same certificates that are used by websites to enable HTTPS URLs.
Jitsi Meet supplies a program to automatically download a TLS certificate for your domain name that uses the Certbot utility.
You will need to install this program before you run the certificate installation script.
First, add the Certbot repository to your system to ensure that you have the latest version of Certbot.
Run the following command to add the new repository and update your system:
Next, install the certbot package:
Your server is now ready to run the TLS certificate installation program provided by Jitsi Meet:
When you run the script you will be shown the following prompt for an email address:
This email address will be submitted to the certificate issuer https: / / letsencrypt.org and will be used to notify you about security and other matters related to the TLS certificate.
You must enter an email address here to proceed with the installation.
The installation will then complete without any further prompts.
When it finishes, your Jitsi Meet instance will be configured to use a signed TLS certificate for your domain name.
Certificate renewals will also happen automatically because the installer placed a renewal script at / etc / cron.weekly / letsencrypt-renew that will run each week.
The TLS installer used port 80 to verify you had control of your domain name.
Now that you have obtained the certificate your server no longer needs to have port 80 open because port 80 is used for regular, non-encrypted HTTP traffic.
Jitsi Meet only serves its website via HTTPS on port 443.
Close this port in your firewall with the following ufw command:
Your Jitsi Meet server is now up and running and available for testing.
Open a browser and point it to your domain name.
You will be able to create a new conference room and invite others to join you.
The default configuration for Jitsi Meet is that anyone visiting your Jitsi Meet server homepage can create a new conference room.
This will use your server's system resources to run the conference room and is not desirable for unauthorized users.
In the next step, you will configure your Jitsi Meet instance to only allow registered users to create conference rooms.
Step 5 - Locking Conference Creation
In this step, you will configure your Jitsi Meet server to only allow registered users to create conference rooms.
The files that you will edit were generated by the installer and are configured with your domain name.
The variable < ^ > your _ domain < ^ > will be used in place of a domain name in the following examples.
First, open sudo nano / etc / prosody / conf.avail / your _ domain.cfg.lua with a text editor:
Edit this line:
To the following:
This configuration tells Jitsi Meet to force username and password authentication before allowing conference room creation by a new visitor.
Then, in the same file, add the following section to the end of the file:
This configuration allows anonymous users to join conference rooms that were created by an authenticated user.
However, the guest must have a unique address and an optional password for the room to enter it.
Here, you added guest. to the front of your domain name.
For example, for jitsi.your-domain you would put guest.jitsi.your-domain.
The guest. hostname is only used internally by Jitsi Meet.
You will never enter it into a browser or need to create a DNS record for it.
Open another configuration file at / etc / jitsi / meet / your _ domain-config.js with a text editor:
Again, by using the guest. < ^ > your _ domain < ^ > hostname that you used earlier this configuration tells Jitsi Meet what internal hostname to use for the un-authenticated guests.
Next, open / etc / jitsi / jicofo / sip-communicator.properties:
And add the following line to complete the configuration changes:
This configuration points one of the Jitsi Meet processes to the local server that performs the user authentication that is now required.
Your Jitsi Meet instance is now configured so that only registered users can create conference rooms.
After a conference room is created, anyone can join it without needing to be a registered user.
All they will need is the unique conference room address and an optional password set by the room's creator.
Now that Jitsi Meet is configured to require authenticated users for room creation you need to register these users and their passwords.
You will use the prosodyctl utility to do this.
Run the following command to add a user to your server:
The user that you add here is not a system user.
They will only be able to create a conference room and are not able to log in to your server via SSH.
Finally, restart the Jitsi Meet processes to load the new configuration:
The Jitsi Meet instance will now request a username and password with a dialog box when a conference room is created.
Image showing the Jitsi username and password box
Your Jitsi Meet server is now set up and securely configured.
In this article, you deployed a Jitsi Meet server that you can use to host secure and private video conference rooms.
You can extend your Jitsi Meet instance with instructions from the Jitsi Meet Wiki.
How To Install Linux, Apache, MySQL, PHP (LAMP) stack on Ubuntu 20.04
5239
A "LAMP" stack is a group of open-source software that is typically installed together in order to enable a server to host dynamic websites and web apps written in PHP.
The site data is stored in a MySQL database, and dynamic content is processed by PHP.
In this guide, we'll install a LAMP stack on an Ubuntu 20.04 server.
In order to complete this tutorial, you will need to have an Ubuntu 20.04 server with a non-root sudo-enabled user account and a basic firewall.
This can be configured using our initial server setup guide for Ubuntu 20.04.
Step 1 - Installing Apache and Updating the Firewall
The Apache web server is among the most popular web servers in the world.
It's well documented, has an active community of users, and has been in wide use for much of the history of the web, which makes it a great default choice for hosting a website.
Install Apache using Ubuntu's package manager, apt:
If this is the first time you're using sudo within this session, you'll be prompted to provide your user's password to confirm you have the right privileges to manage system packages with apt.
You'll also be prompted to confirm Apache's installation by pressing Y, then ENTER.
Once the installation is finished, you'll need to adjust your firewall settings to allow HTTP and HTTPS traffic.
UFW has different application profiles that you can leverage for accomplishing that.
To list all currently available UFW application profiles, you can run:
Here's what each of these profiles mean:
Apache: This profile opens only port 80 (normal, unencrypted web traffic).
Apache Full: This profile opens both port 80 (normal, unencrypted web traffic) and port 443 (TLS / SSL encrypted traffic).
Apache Secure: This profile opens only port 443 (TLS / SSL encrypted traffic).
For now, it's best to allow only connections on port 80, since this is a fresh Apache installation and you still don't have a TLS / SSL certificate configured to allow for HTTPS traffic on your server.
To only allow traffic on port 80, use the Apache profile:
You can verify the change with:
Traffic on port 80 is now allowed through the firewall.
You can do a spot check right away to verify that everything went as planned by visiting your server's public IP address in your web browser (see the note under the next heading to find out what your public IP address is if you do not have this information already):
You'll see the default Ubuntu 20.04 Apache web page, which is there for informational and testing purposes.
It should look something like this:
Ubuntu 20.04 Apache default
If you see this page, then your web server is now correctly installed and accessible through your firewall.
How To Find your Server's Public IP Address
If you do not know what your server's public IP address is, there are a number of ways you can find it. Usually, this is the address you use to connect to your server through SSH.
There are a few different ways to do this from the command line.
First, you could use the iproute2 tools to get your IP address by typing this:
This will give you two or three lines back.
They are all correct addresses, but your computer may only be able to use one of them, so feel free to try each one.
An alternative method is to use the curl utility to contact an outside party to tell you how it sees your server.
This is done by asking a specific server what your IP address is:
Regardless of the method you use to get your IP address, type it into your web browser's address bar to view the default Apache page.
Step 2 - Installing MySQL
Now that you have a web server up and running, you need to install the database system to be able to store and manage data for your site.
MySQL is a popular database management system used within PHP environments.
Again, use apt to acquire and install this software:
When prompted, confirm installation by typing Y, and then ENTER.
When the installation is finished, it's recommended that you run a security script that comes pre-installed with MySQL.
This will ask if you want to configure the VALIDATE PASSWORD PLUGIN.
< $> note Note: Enabling this feature is something of a judgment call.
If enabled, passwords which don't match the specified criteria will be rejected by MySQL with an error.
It is safe to leave validation disabled, but you should always use strong, unique passwords for database credentials.
Answer Y for yes, or anything else to continue without enabling.
If you answer "yes", you'll be asked to select a level of password validation.
Keep in mind that if you enter 2 for the strongest level, you will receive errors when attempting to set any password which does not contain numbers, upper and lowercase letters, and special characters, or which is based on common dictionary words.
Regardless of whether you chose to set up the VALIDATE PASSWORD PLUGIN, your server will next ask you to select and confirm a password for the MySQL root user.
This is not to be confused with the system root.
Even though the default authentication method for the MySQL root user dispenses the use of a password, even when one is set, you should define a strong password here as an additional safety measure.
We'll talk about this in a moment.
If you enabled password validation, you'll be shown the password strength for the root password you just entered and your server will ask if you want to continue with that password.
If you are happy with your current password, enter Y for "yes" at the prompt:
For the rest of the questions, press Y and hit the ENTER key at each prompt.
When you're finished, test if you're able to log in to the MySQL console by typing:
This will connect to the MySQL server as the administrative database user root, which is inferred by the use of sudo when running this command.
To exit the MySQL console, type:
Notice that you didn't need to provide a password to connect as the root user, even though you have defined one when running the mysql _ secure _ installation script.
That is because the default authentication method for the administrative MySQL user is unix _ socket instead of password.
Even though this might look like a security concern at first, it makes the database server more secure because the only users allowed to log in as the root MySQL user are the system users with sudo privileges connecting from the console or through an application running with the same privileges.
Setting a password for the root MySQL account works as a safeguard, in case the default authentication method is changed from unix _ socket to password.
< $> note Note: At the time of this writing, the native MySQL PHP library mysqlnd doesn't support caching _ sha2 _ authentication, the default authentication method for MySQL 8. For that reason, when creating database users for PHP applications on MySQL 8, you'll need to make sure they're configured to use mysql _ native _ password instead.
We'll demonstrate how to do that in Step 6. < $>
Your MySQL server is now installed and secured.
Next, we'll install PHP, the final component in the LAMP stack.
You have Apache installed to serve your content and MySQL installed to store and manage your data. PHP is the component of our setup that will process code to display dynamic content to the final user.
In addition to the php package, you "ll need php-mysql, a PHP module that allows PHP to communicate with MySQL-based databases.
You'll also need libapache2-mod-php to enable Apache to handle PHP files.
To install these packages, run:
Once the installation is finished, you can run the following command to confirm your PHP version:
At this point, your LAMP stack is fully operational, but before you can test your setup with a PHP script, it's best to set up a proper Apache Virtual Host to hold your website's files and folders.
We'll do that in the next step.
Step 4 - Creating a Virtual Host for your Website
When using the Apache web server, you can create virtual hosts (similar to server blocks in Nginx) to encapsulate configuration details and host more than one domain from a single server.
In this guide, we'll set up a domain called your _ domain, but you should replace this with your own domain name.
Apache on Ubuntu 20.04 has one server block enabled by default that is configured to serve documents from the / var / www / html directory.
While this works well for a single site, it can become unwieldy if you are hosting multiple sites.
Instead of modifying / var / www / html, we'll create a directory structure within / var / www for the your _ domain site, leaving / var / www / html in place as the default directory to be served if a client request doesn't match any other sites.
Create the directory for your _ domain as follows:
Next, assign ownership of the directory with the $USER environment variable, which will reference your current system user:
Then, open a new configuration file in Apache's sites-available directory using your preferred command-line editor.
Here, we'll use nano:
This will create a new blank file.
Paste in the following bare-bones configuration:
With this VirtualHost configuration, we're telling Apache to serve < ^ > your _ domain < ^ > using / var / www / < ^ > your _ domain < ^ > as the web root directory.
If you'd like to test Apache without a domain name, you can remove or comment out the options ServerName and ServerAlias by adding a # character in the beginning of each option's lines.
You can now use a2ensite to enable the new virtual host:
You might want to disable the default website that comes installed with Apache.
This is required if you're not using a custom domain name, because in this case Apache's default configuration would overwrite your virtual host.
To disable Apache's default website, type:
To make sure your configuration file doesn't contain syntax errors, run:
Finally, reload Apache so these changes take effect:
Your new website is now active, but the web root / var / www / < ^ > your _ domain < ^ > is still empty.
Create an index.html file in that location so that we can test that the virtual host works as expected:
Include the following content in this file:
Now go to your browser and access your server's domain name or IP address once again:
Apache virtual host test
If you see this page, it means your Apache virtual host is working as expected.
You can leave this file in place as a temporary landing page for your application until you set up an index.php file to replace it. Once you do that, remember to remove or rename the index.html file from your document root, as it would take precedence over an index.php file by default.
A Note About DirectoryIndex on Apache
With the default DirectoryIndex settings on Apache, a file named index.html will always take precedence over an index.php file.
This is useful for setting up maintenance pages in PHP applications, by creating a temporary index.html file containing an informative message to visitors.
Because this page will take precedence over the index.php page, it will then become the landing page for the application.
Once maintenance is over, the index.html is renamed or removed from the document root, bringing back the regular application page.
In case you want to change this behavior, you'll need to edit the / etc / apache2 / mods-enabled / dir.conf file and modify the order in which the index.php file is listed within the DirectoryIndex directive:
After saving and closing the file, you'll need to reload Apache so the changes take effect:
In the next step, we'll create a PHP script to test that PHP is correctly installed and configured on your server.
Step 5 - Testing PHP Processing on your Web Server
Now that you have a custom location to host your website's files and folders, we'll create a PHP test script to confirm that Apache is able to handle and process requests for PHP files.
Create a new file named info.php inside your custom web root folder:
This will open a blank file.
Add the following text, which is valid PHP code, inside the file:
To test this script, go to your web browser and access your server's domain name or IP address, followed by the script name, which in this case is info.php:
Ubuntu 20.04 PHP info
This page provides information about your server from the perspective of PHP.
It is useful for debugging and to ensure that your settings are being applied correctly.
If you can see this page in your browser, then your PHP installation is working as expected.
After checking the relevant information about your PHP server through that page, it's best to remove the file you created as it contains sensitive information about your PHP environment -and your Ubuntu server.
You can use rm to do so:
You can always recreate this page if you need to access the information again later.
Step 6 - Testing Database Connection from PHP (Optional)
If you want to test whether PHP is able to connect to MySQL and execute database queries, you can create a test table with dummy data and query for its contents from a PHP script.
Before we can do that, we need to create a test database and a new MySQL user properly configured to access it.
At the time of this writing, the native MySQL PHP library mysqlnd doesn't support caching _ sha2 _ authentication, the default authentication method for MySQL 8. We'll need to create a new user with the mysql _ native _ password authentication method in order to be able to connect to the MySQL database from PHP.
We'll create a database named example\ _ database and a user named example\ _ user, but you can replace these names with different values.
First, connect to the MySQL console using the root account:
To create a new database, run the following command from your MySQL console:
The following command creates a new user named < ^ > example _ user < ^ >, using mysql _ native _ password as default authentication method.
We're defining this user's password as < ^ > password < ^ >, but you should replace this value with a secure password of your own choosing.
Now we need to give this user permission over the example _ database database:
Now exit the MySQL shell with:
You can test if the new user has the proper permissions by logging in to the MySQL console again, this time using the custom user credentials:
Notice the -p flag in this command, which will prompt you for the password used when creating the example\ _ user user.
After logging in to the MySQL console, confirm that you have access to the example\ _ database database:
Next, we'll create a test table named todo _ list.
From the MySQL console, run the following statement:
Insert a few rows of content in the test table.
After confirming that you have valid data in your test table, you can exit the MySQL console:
Now you can create the PHP script that will connect to MySQL and query for your content.
The following PHP script connects to the MySQL database and queries for the content of the todo _ list table, exhibiting the results in a list.
If there's a problem with the database connection, it will throw an exception.
Copy this content into your todo _ list.php script:
You can now access this page in your web browser by visiting the domain name or public IP address for your website, followed by / todo _ list.php:
That means your PHP environment is ready to connect and interact with your MySQL server.
In this guide, we've built a flexible foundation for serving PHP websites and applications to your visitors, using Apache as web server and MySQL as database system.
How To Install Python 3 and Set Up a Programming Environment on an Ubuntu 20.04 Server
5300
The Python programming language is an increasingly popular choice for both beginners and experienced developers.
Flexible and versatile, Python has strengths in scripting, automation, data analysis, machine learning, and back-end development.
First published in 1991 with a name inspired by the British comedy group Monty Python, the development team wanted to make Python a language that was fun to use.
This tutorial will get your Ubuntu 20.04 server set up with a Python 3 programming environment.
Programming on a server has many advantages and supports collaboration across development projects.
The general principles of this tutorial will apply to any distribution of Debian Linux.
In order to complete this tutorial, you should have a non-root user with sudo privileges on an Ubuntu 20.04 server.
To learn how to achieve this setup, follow our initial server setup guide.
If you "re not already familiar with a terminal environment, you may find the article" An Introduction to the Linux Terminal (https: / / www.digitalocean.com / community / tutorials / an-introduction-to-the-linux-terminal) "useful for becoming better oriented with the terminal.
With your server and user set up, you are ready to begin.
Step 1 - Setting Up Python 3
Ubuntu 20.04 and other versions of Debian Linux ship with Python 3 pre-installed.
To make sure that our versions are up-to-date, let "s update and upgrade the system with the apt command to work with Ubuntu" s Advanced Packaging Tool:
The -y flag will confirm that we are agreeing for all items to be installed, but depending on your version of Linux, you may need to confirm additional prompts as your system updates and upgrades.
Once the process is complete, we can check the version of Python 3 that is installed in the system by typing:
You "ll receive output in the terminal window that will let you know the version number.
While this number may vary, the output will be similar to this:
To manage software packages for Python, let "s install pip, a tool that will install and manage programming packages we may want to use in our development projects.
You can learn more about modules or packages that you can install with pip by reading "How To Import Modules in Python 3."
Python packages can be installed by typing:
Here, < ^ > package _ name < ^ > can refer to any Python package or library, such as Django for web development or NumPy for scientific computing.
So if you would like to install NumPy, you can do so with the command pip3 install numpy.
There are a few more packages and development tools to install to ensure that we have a robust setup for our programming environment:
Once Python is set up, and pip and other tools are installed, we can set up a virtual environment for our development projects.
Step 2 - Setting Up a Virtual Environment
Virtual environments enable you to have an isolated space on your server for Python projects, ensuring that each of your projects can have its own set of dependencies that won "t disrupt any of your other projects.
Setting up a programming environment provides greater control over Python projects and over how different versions of packages are handled.
This is especially important when working with third-party packages.
You can set up as many Python programming environments as you would like.
Each environment is basically a directory or folder on your server that has a few scripts in it to make it act as an environment.
While there are a few ways to achieve a programming environment in Python, we "ll be using the venv module here, which is part of the standard Python 3 library.
Let "s install venv by typing:
With this installed, we are ready to create environments.
Let "s either choose which directory we would like to put our Python programming environments in, or create a new directory with mkdir, as in:
Once you are in the directory where you would like the environments to live, you can create an environment by running the following command:
Essentially, pyvenv sets up a new directory that contains a few items which we can view with the ls command:
Together, these files work to make sure that your projects are isolated from the broader context of your server, so that system files and project files don "t mix.
This is good practice for version control and to ensure that each of your projects has access to the particular packages that it needs.
Python Wheels, a built-package format for Python that can speed up your software production by reducing the number of times you need to compile, will be in the Ubuntu 20.04 share directory.
To use this environment, you need to activate it, which you can achieve by typing the following command that calls the activate script:
Your command prompt will now be prefixed with the name of your environment, in this case it is called < ^ > my _ env < ^ >.
Depending on what version of Debian Linux you are running, your prefix may appear somewhat differently, but the name of your environment in parentheses should be the first thing you see on your line:
This prefix lets us know that the environment < ^ > my _ env < ^ > is currently active, meaning that when we create programs here they will use only this particular environment "s settings and packages.
< $> note Note: Within the virtual environment, you can use the command python instead of python3, and pip instead of pip3 if you would prefer.
If you use Python 3 on your machine outside of an environment, you will need to use the python3 and pip3 commands exclusively.
After following these steps, your virtual environment is ready to use.
Step 3 - Creating a "Hello, World" Program
Now that we have our virtual environment set up, let "s create a traditional" Hello, World! "
This will let us test our environment and provides us with the opportunity to become more familiar with Python if we aren "t already.
To do this, we "ll open up a command-line text editor such as nano and create a new file:
Once the text file opens up in the terminal window we "ll type out our program:
Exit nano by typing the CTRL and X keys, and when prompted to save the file press y.
Once you exit out of nano and return to your shell, let "s run the program:
The hello.py program that you just created should cause your terminal to produce the following output:
To leave the environment, type the command deactivate and you will return to your original directory.
Congratulations!
At this point you have a Python 3 programming environment set up on your Ubuntu Linux server and you can now begin a coding project!
If you are using a local machine rather than a server, refer to the tutorial that is relevant to your operating system in our "How To Install and Set Up a Local Programming Environment for Python 3" series.
With your server ready for software development, you can continue to learn more about coding in Python by reading our free How To Code in Python 3 eBook, or consulting our Python tutorials.
How to Set Up SSH Keys on Ubuntu 20.04
5247
When working with an Ubuntu server, chances are you will spend most of your time in a terminal session connected to your server through SSH.
In this guide, we'll focus on setting up SSH keys for an Ubuntu 20.04 installation.
SSH keys provide an easy, secure way of logging into your server and are recommended for all users.
Step 1 - Creating the Key Pair
The first step is to create a key pair on the client machine (usually your computer):
By default recent versions of ssh-keygen will create a 3072-bit RSA key pair, which is secure enough for most use cases (you may optionally pass in the -b 4096 flag to create a larger 4096-bit key).
After entering the command, you should see the following output:
Press enter to save the key pair into the .ssh / subdirectory in your home directory, or specify an alternate path.
Here you optionally may enter a secure passphrase, which is highly recommended.
A passphrase adds an additional layer of security to prevent unauthorized users from logging in.
To learn more about security, consult our tutorial on How To Configure SSH Key-Based Authentication on a Linux Server.
You should then see the output similar to the following:
The next step is to place the public key on your server so that you can use SSH-key-based authentication to log in.
Step 2 - Copying the Public Key to Your Ubuntu Server
The quickest way to copy your public key to the Ubuntu host is to use a utility called ssh-copy-id.
Due to its simplicity, this method is highly recommended if available.
If you do not have ssh-copy-id available to you on your client machine, you may use one of the two alternate methods provided in this section (copying via password-based SSH, or manually copying the key).
Copying the Public Key Using ssh-copy-id
To use the utility, you specify the remote host that you would like to connect to, and the user account that you have password-based SSH access to.
This is the account to which your public SSH key will be copied.
The syntax is:
Type "yes" and press ENTER to continue.
Type in the password (your typing will not be displayed, for security purposes) and press ENTER.
It will then copy the contents of your ~ / .ssh / id _ rsa.pub key into a file in the remote account's home ~ / .ssh directory called authorized _ keys.
Copying the Public Key Using SSH
If you do not have ssh-copy-id available, but you have password-based SSH access to an account on your server, you can upload your keys using a conventional SSH method.
We "ll use the > > redirect symbol to append the content instead of overwriting it. This will let us add keys without destroying previously added keys.
Copying the Public Key Manually
Access your remote host using whichever method you have available.
We can now attempt passwordless authentication with our Ubuntu server.
Step 3 - Authenticating to Your Ubuntu Server Using SSH Keys
If you have successfully completed one of the procedures above, you should be able to log into the remote host without providing the remote account's password.
The basic process is the same:
Type "yes" and then press ENTER to continue.
If you did not supply a passphrase for your private key, you will be logged in immediately.
If you supplied a passphrase for the private key when you created the key, you will be prompted to enter it now (note that your keystrokes will not display in the terminal session for security).
After authenticating, a new shell session should open for you with the configured account on the Ubuntu server.
If key-based authentication was successful, continue on to learn how to further secure your system by disabling password authentication.
Step 4 - Disabling Password Authentication on Your Server
If you were able to log into your account using SSH without a password, you have successfully configured SSH-key-based authentication to your account.
This line may be commented out with a # at the beginning of the line.
Uncomment the line by removing the #, and set the value to no. This will disable your ability to log in via SSH using account passwords:
Save and close the file when you are finished by pressing CTRL + X, then Y to confirm saving the file, and finally ENTER to exit nano.
To actually activate these changes, we need to restart the sshd service:
Once you have verified your SSH service is functioning properly, you can safely close all current server sessions.
The SSH daemon on your Ubuntu server now only responds to SSH-key-based authentication.
Password-based logins have been disabled.
How To Install and Configure Elasticsearch on Ubuntu 20.04
5416
Elasticsearch is a platform for distributed search and analysis of data in real time.
It is a popular choice due to its usability, powerful features, and scalability.
This article will guide you through installing Elasticsearch, configuring it for your use case, securing your installation, and beginning to work with your Elasticsearch server.
Before following this tutorial, you will need:
An Ubuntu 20.04 server with 4GB RAM and 2 CPUs set up with a non-root sudo user.
You can achieve this by following the Initial Server Setup with Ubuntu 20.04
OpenJDK 11 installed
For this tutorial, we will work with the minimum amount of CPU and RAM required to run Elasticsearch.
Note that the amount of CPU, RAM, and storage that your Elasticsearch server will require depends on the volume of logs that you expect.
Step 1 - Installing and Configuring Elasticsearch
The Elasticsearch components are not available in Ubuntu's default package repositories.
They can, however, be installed with APT after adding Elastic's package source list.
All of the packages are signed with the Elasticsearch signing key in order to protect your system from package spoofing.
Packages which have been authenticated using the key will be considered trusted by your package manager.
In this step, you will import the Elasticsearch public GPG key and add the Elastic package source list in order to install Elasticsearch.
To begin, use cURL, the command line tool for transferring data with URLs, to import the Elasticsearch public GPG key into APT.
Note that we are using the arguments -fsSL to silence all progress and possible errors (except for a server failure) and to allow cURL to make a request on a new location if redirected.
Pipe the output of the cURL command into the apt-key program, which adds the public GPG key to APT.
Next, add the Elastic source list to the sources.list.d directory, where APT will search for new sources:
Next, update your package lists so APT will read the new Elastic source:
Then install Elasticsearch with this command:
Elasticsearch is now installed and ready to be configured.
Step 2 - Configuring Elasticsearch
To configure Elasticsearch, we will edit its main configuration file elasticsearch.yml where most of its configuration options are stored.
This file is located in the / etc / elasticsearch directory.
Use your preferred text editor to edit Elasticsearch's configuration file.
< $> note Note: Elasticsearch's configuration file is in YAML format, which means that we need to maintain the indentation format.
Be sure that you do not add any extra spaces as you edit this file.
The elasticsearch.yml file provides configuration options for your cluster, node, paths, memory, network, discovery, and gateway.
Most of these options are preconfigured in the file but you can change them according to your needs.
For the purposes of our demonstration of a single-server configuration, we will only adjust the settings for the network host.
Elasticsearch listens for traffic from everywhere on port 9200.
You will want to restrict outside access to your Elasticsearch instance to prevent outsiders from reading your data or shutting down your Elasticsearch cluster through its REST API (https: / / en.wikipedia.org / wiki / Representational _ state _ transfer).
To restrict access and therefore increase security, find the line that specifies network.host, uncomment it, and replace its value with localhost so it reads like this:
We have specified localhost so that Elasticsearch listens on all interfaces and bound IPs.
If you want it to listen only on a specific interface, you can specify its IP in place of localhost.
Save and close elasticsearch.yml.
If you're using nano, you can do so by pressing CTRL + X, followed by Y and then ENTER.
These are the minimum settings you can start with in order to use Elasticsearch.
Now you can start Elasticsearch for the first time.
Start the Elasticsearch service with systemctl.
Give Elasticsearch a few moments to start up.
Otherwise, you may get errors about not being able to connect.
Next, run the following command to enable Elasticsearch to start up every time your server boots:
With Elasticsearch enabled upon startup, let "s move on to the next step to discuss security.
Step 3 - Securing Elasticsearch
By default, Elasticsearch can be controlled by anyone who can access the HTTP API.
This is not always a security risk because Elasticsearch listens only on the loopback interface (that is, 127.0.0.1), which can only be accessed locally.
Thus, no public access is possible and as long as all server users are trusted, security may not be a major concern.
If you need to allow remote access to the HTTP API, you can limit the network exposure with Ubuntu's default firewall, UFW.
This firewall should already be enabled if you followed the steps in the prerequisite Initial Server Setup with Ubuntu 20.04 tutorial.
We will now configure the firewall to allow access to the default Elasticsearch HTTP API port (TCP 9200) for the trusted remote host, generally the server you are using in a single-server setup, such as < ^ > 198.51.100.0 < ^ >.
To allow access, type the following command:
Once that is complete, you can enable UFW with the command:
Finally, check the status of UFW with the following command:
If you have specified the rules correctly, you should receive output like this:
The UFW should now be enabled and set up to protect Elasticsearch port 9200.
If you want to invest in additional protection, Elasticsearch offers the commercial Shield plugin for purchase.
Step 4 - Testing Elasticsearch
By now, Elasticsearch should be running on port 9200.
You can test it with cURL and a GET request.
You should receive the following response:
If you receive a response similar to the one above, Elasticsearch is working properly.
If not, make sure that you have followed the installation instructions correctly and you have allowed some time for Elasticsearch to fully start.
To perform a more thorough check of Elasticsearch execute the following command:
In the output from the above command you can verify all the current settings for the node, cluster, application paths, modules, and more.
Step 5 - Using Elasticsearch
To start using Elasticsearch, let's first add some data. Elasticsearch uses a RESTful API, which responds to the usual CRUD commands: create, read, update, and delete.
To work with it, we'll use the cURL command again.
You can add your first entry like so:
With cURL, we have sent an HTTP POST request to the Elasticsearch server.
The URI of the request was / tutorial / helloworld / 1 with several parameters:
tutorial is the index of the data in Elasticsearch.
helloworld is the type.
1 is the ID of our entry under the above index and type.
You can retrieve this first entry with an HTTP GET request.
This should be the resulting output:
To modify an existing entry, you can use an HTTP PUT request.
Elasticsearch should acknowledge successful modification like this:
In the above example we have modified the message of the first entry to "Hello, People!".
With that, the version number has been automatically increased to 2.
You may have noticed the extra argument pretty in the above request.
It enables human-readable format so that you can write each data field on a new row.
You can also "prettify" your results when retrieving data to get a more readable output by entering the following command:
Now the response will be formatted for a human to parse:
We have now added and queried data in Elasticsearch.
To learn about the other operations please check the API documentation.
You have now installed, configured, and begun to use Elasticsearch.
To further explore Elasticsearch "s functionality, please refer to the official Elasticsearch documentation.
How To Install the Apache Web Server on CentOS 8
5243
The Apache HTTP server is the most widely-used web server in the world.
It provides many powerful features including dynamically loadable modules, robust media support, and extensive integration with other popular software.
In this guide, you will install an Apache web server with virtual hosts on your CentOS 8 server.
You will need the following to complete this guide:
A non-root user with sudo privileges configured on your server, set up by following the initial server setup guide for CentOS 8.
Ensure that a basic firewall is configured by following Step 4 of the Initial Server Setup with CentOS 8 (recommended) in the above guide.
Step 1 - Installing Apache
Apache is available within CentOS's default software repositories, which means you can install it with the dnf package manager.
As the non-root sudo user configured in the prerequisites, install the Apache package:
After confirming the installation, dnf will install Apache and all required dependencies.
By completing Step 4 of the Initial Server Setup with CentOS 8 guide mentioned in the prerequisites section, you will have already installed firewalld on your server to serve requests over HTTP.
If you also plan to configure Apache to serve content over HTTPS, you will also want to open up port 443 by enabling the https service:
Next, reload the firewall to put these new rules into effect:
After the firewall reloads, you are ready to start the service and check the web server.
Step 2 - Checking your Web Server
Apache does not automatically start on CentOS once the installation completes, so you will need to start the Apache process manually:
Verify that the service is running with the following command:
You will receive an active status when the service is running:
As this output indicates, the service has started successfully.
However, the best way to test this is to request a page from Apache.
You can access the default Apache landing page to confirm that the software is running properly through your IP address.
If you do not know your server's IP address, you can get it a few different ways from the command line.
Type q to return to the command prompt and then type:
This command will display all of the host's network addresses, so you will get back a few IP addresses separated by spaces.
You can try each in your web browser to determine whether they work.
Alternatively, you can use curl to request your IP from icanhazip.com, which will give you your public IPv4 address as read from another location on the internet:
When you have your server's IP address, enter it into your browser's address bar:
You'll see the default CentOS 8 Apache web page:
Default Apache page for CentOS 8
This page indicates that Apache is working correctly.
It also includes some basic information about important Apache files and directory locations.
Step 3 - Managing the Apache Process
Now that the service is installed and running, you can now use different systemctl commands to manage the service.
To stop your web server, type:
To start the web server when it is stopped, type:
To stop and then start the service again, type:
If you are simply making configuration changes, Apache can often reload without dropping connections.
To do this, use this command:
By default, Apache is configured to start automatically when the server boots.
If this is not what you want, disable this behavior by typing:
To re-enable the service to start up at boot, type:
Apache will now start automatically when the server boots again.
The default configuration for Apache will allow your server to host a single website.
If you plan on hosting multiple domains on your server, you will need to configure virtual hosts on your Apache web server.
Step 4 - Setting Up Virtual Hosts (Recommended)
When using the Apache web server, you can use virtual hosts (if you are more familiar with Nginx, these are similar to server blocks) to encapsulate configuration details and host more than one domain from a single server.
In this step, you will set up a domain called example.com, but you should replace this with your own domain name.
If you are setting up a domain name with DigitalOcean, please refer to our Networking Documentation.
Apache on CentOS 8 has one virtual host enabled by default that is configured to serve documents from the / var / www / html directory.
Instead of modifying / var / www / html, you will create a directory structure within / var / www for the example.com site, leaving / var / www / html in place as the default directory to be served if a client request doesn't match any other sites.
Create the html directory for example.com as follows, using the -p flag to create any necessary parent directories:
Create an additional directory to store log files for the site:
Next, assign ownership of the html directory with the $USER environmental variable:
Make sure that your web root has the default permissions set:
Next, create a sample index.html page using vi or your favorite editor:
Press i to switch to INSERT mode and add the following sample HTML to the file:
Save and close the file by pressing ESC, typing: wq, and pressing ENTER.
With your site directory and sample index file in place, you are almost ready to create the virtual host files.
Virtual host files specify the configuration of your separate sites and tell the Apache web server how to respond to various domain requests.
Before you create your virtual hosts, you will need to create a sites-available directory to store them in.
You will also create the sites-enabled directory that tells Apache that a virtual host is ready to serve to visitors.
The sites-enabled directory will hold symbolic links to virtual hosts that we want to publish.
Create both directories with the following command:
Next, you will tell Apache to look for virtual hosts in the sites-enabled directory.
To accomplish this, edit Apache's main configuration file using vi or your favorite text editor and add a line declaring an optional directory for additional configuration files:
Press capital G to navigate towards the end of the file.
Then press i to switch to INSERT mode and add the following line to the very end of the file:
Save and close the file when you are done adding that line.
Now that you have your virtual host directories in place, you will create your virtual host file.
Start by creating a new file in the sites-available directory:
Add in the following configuration block, and change the example.com domain to your domain name:
This will tell Apache where to find the root directly that holds the publicly accessible web documents.
It also tells Apache where to store error and request logs for this particular site.
Now that you have created the virtual host files, you will enable them so that Apache knows to serve them to visitors.
To do this, create a symbolic link for each virtual host in the sites-enabled directory:
Your virtual host is now configured and ready to serve content.
Before restarting the Apache service, let's make sure that SELinux has the correct policies in place for your virtual hosts.
Step 5 - Adjusting SELinux Permissions for Virtual Hosts (Recommended)
SELinux is a Linux kernel security module that brings heightened security for Linux systems.
CentOS 8 comes equipped with SELinux configured to work with the default Apache configuration.
Since you changed the default configuration by setting up a custom log directory in the virtual hosts configuration file, you will receive an error if you attempt to start the Apache service.
To resolve this, you need to update the SELinux policies to allow Apache to write to the necessary files.
There are different ways to set policies based on your environment's needs as SELinux allows you to customize your security level.
This step will cover two methods of adjusting Apache policies: universally and on a specific directory.
Adjusting policies on directories is more secure, and is therefore the recommended approach.
Adjusting Apache Policies Universally
Setting the Apache policy universally will tell SELinux to treat all Apache processes identically by using the httpd _ unified Boolean.
While this approach is more convenient, it will not give you the same level of control as an approach that focuses on a file or directory policy.
Run the following command to set a universal Apache policy:
The setsebool command changes SELinux Boolean values.
The -P flag will update the boot-time value, making this change persist across reboots. httpd _ unified is the Boolean that will tell SELinux to treat all Apache processes as the same type, so you enabled it with a value of 1.
Adjusting Apache Policies on a Directory
Individually setting SELinux permissions for the / var / www / < ^ > example.com < ^ > / log directory will give you more control over your Apache policies, but may also require more maintenance.
Since this option is not universally setting policies, you will need to manually set the context type for any new log directories specified in your virtual host configurations.
First, check the context type that SELinux gave the / var / www / < ^ > example.com < ^ > / log directory:
This command lists and prints the SELinux context of the directory.
The current context is httpd _ sys _ content _ t, which tells SELinux that the Apache process can only read files created in this directory.
In this tutorial, you will change the context type of the / var / www / < ^ > example.com < ^ > / log directory to httpd _ log _ t.
This type will allow Apache to generate and append to web application log files:
Next, use the restorecon command to apply these changes and have them persist across reboots:
The -R flag runs this command recursively, meaning it will update any existing files to use the new context.
The -v flag will print the context changes the command made.
You will receive the following output confirming the changes:
You can list the contexts once more to see the changes:
The output reflects the updated context type:
Now that the / var / www / < ^ > example.com < ^ > / log directory is using the httpd _ log _ t type, you are ready to test your virtual host configuration.
Step 6 - Testing the Virtual Host (Recommended)
Once the SELinux context has been updated with either method, Apache will be able to write to the / var / www / < ^ > example.com < ^ > / log directory.
You can now successfully restart the Apache service:
List the contents of the / var / www / < ^ > example.com < ^ > / log directory to see if Apache created the log files:
You'll receive confirmation that Apache was able to create the error.log and requests.log files specified in the virtual host configuration:
Now that you have your virtual host set up and SELinux permissions updated, Apache will now serve your domain name.
You can test this by navigating to http: / / < ^ > example.com < ^ >, where you should see something like this:
Success!
The example.com virtual host is working!
This confirms that your virtual host is successfully configured and serving content.
Repeat Steps 4 and 5 to create new virtual hosts with SELinux permissions for additional domains.
In this tutorial, you installed and managed the Apache web server.
Now that you have your web server installed, you have many options for the type of content you can serve and the technologies you can use to create a richer experience.
If you'd like to build out a more complete application stack, you can look at this article on how to configure a LAMP stack on CentOS 8.
How To Install the Apache Web Server on Ubuntu 20.04 Quickstart
5388
It provides many powerful features, including dynamically loadable modules, robust media support, and extensive integration with other popular software.
In this guide, we'll explain how to install an Apache web server on your Ubuntu 20.04 server.
For a more detailed version of this tutorial, please refer to How To Install the Apache Web Server on Ubuntu 20.04.
Before you begin this guide, you should have the following:
An Ubuntu 20.04 server and a regular, non-root user with sudo privileges.
Additionally, you will need to enable a basic firewall to block non-essential ports.
You can learn how to configure a regular user account and set up a firewall for your server by following our Initial Server Setup for Ubuntu 20.04 guide.
When you have an account available, log in as your non-root user to begin.
Apache is available within Ubuntu's default software repositories, so you can install it using conventional package management tools.
Update your local package index:
Install the apache2 package:
Step 2 - Adjusting the Firewall
Check the available ufw application profiles:
Let's enable the most restrictive profile that will still allow the traffic you've configured, permitting traffic on port 80 (normal, unencrypted web traffic):
Verify the change:
Step 3 - Checking your Web Server
Check with the systemd init system to make sure the service is running by typing:
Access the default Apache landing page to confirm that the software is running properly through your IP address:
You should receive the default Ubuntu 20.04 Apache web page:
Apache default page
When using the Apache web server, you can use virtual hosts (similar to server blocks in Nginx) to encapsulate configuration details and host more than one domain from a single server.
We will set up a domain called your _ domain, but you should replace this with your own domain name.
To learn more about setting up a domain name with DigitalOcean, please refer to our our Introduction to DigitalOcean DNS.
Create the directory for < ^ > your _ domain < ^ >:
Assign ownership of the directory:
The permissions of your web roots should be correct if you haven't modified your unmask value, but you can make sure by typing:
Create a sample index.html page using nano or your favorite editor:
Inside, add the following sample HTML:
Make a new virtual host file at / etc / apache2 / sites-available / < ^ > your _ domain < ^ > .conf:
Paste in the following configuration block, updated for our new directory and domain name:
Enable the file with a2ensite:
Disable the default site defined in 000-default.conf:
Test for configuration errors:
You should receive the following output:
Restart Apache to implement your changes:
Apache should now be serving your domain name.
You can test this by navigating to http: / / < ^ > your _ domain < ^ >, where you should receive something like this:
Apache virtual host example
Now that you have your web server installed, you have many options for the type of content to serve and the technologies you want to use to create a richer experience.
If you'd like to build out a more complete application stack, check out this article on How to configure a LAMP stack on Ubuntu 20.04.
Using Buffers in Node.js
5391
A buffer is a space in memory (typically RAM) that stores binary data. In Node.js, we can access these spaces of memory with the built-in Buffer class.
Buffers store a sequence of integers, similar to an array in JavaScript.
Unlike arrays, you cannot change the size of a buffer once it is created.
You may have used buffers implicitly if you wrote Node.js code already.
For example, when you read from a file with fs.readFile (), the data returned to the callback or Promise is a buffer object.
Additionally, when HTTP requests are made in Node.js, they return data streams that are temporarily stored in an internal buffer when the client cannot process the stream all at once.
Buffers are useful when you're interacting with binary data, usually at lower networking levels.
They also equip you with the ability to do fine-grained data manipulation in Node.js.
In this tutorial, you will use the Node.js REPL to run through various examples of buffers, such as creating buffers, reading from buffers, writing to and copying from buffers, and using buffers to convert between binary and encoded data. By the end of the tutorial, you'll have learned how to use the Buffer class to work with binary data.
You will need Node.js installed on your development machine.
This tutorial uses version 10.19.0.
In this tutorial, you will interact with buffers in the Node.js REPL (Read-Evaluate-Print-Loop).
If you want a refresher on how to use the Node.js REPL effectively, you can read our guide on How To Use the Node.js REPL.
For this article we expect the user to be comfortable with basic JavaScript and its data types.
You can learn those fundamentals with our How To Code in JavaScript series.
Step 1 - Creating a Buffer
This first step will show you the two primary ways to create a buffer object in Node.js.
To decide what method to use, you need to answer this question: Do you want to create a new buffer or extract a buffer from existing data?
If you are going to store data in memory that you have yet to receive, you'll want to create a new buffer.
In Node.js we use the alloc () function of the Buffer class to do this.
Let's open the Node.js REPL to see for ourselves.
In your terminal, enter the node command:
You will see the prompt begin with >.
The alloc () function takes the size of the buffer as its first and only required argument.
The size is an integer representing how many bytes of memory the buffer object will use.
For example, if we wanted to create a buffer that was 1KB (kilobyte) large, equivalent to 1024 bytes, we would enter this in the console:
To create a new buffer, we used the globally available Buffer class, which has the alloc () method.
By providing 1024 as the argument for alloc (), we created a buffer that's 1KB large.
By default, when you initialize a buffer with alloc (), the buffer is filled with binary zeroes as a placeholder for later data. However, we can change the default value if we'd like to.
If we wanted to create a new buffer with 1s instead of 0s, we would set the alloc () function's second parameter - fill.
In your terminal, create a new buffer at the REPL prompt that's filled with 1s:
We just created a new buffer object that references a space in memory that stores 1KB of 1s.
Although we entered an integer, all data stored in a buffer is binary data.
Binary data can come in many different formats.
For example, let's consider a binary sequence representing a byte of data: 01110110.
If this binary sequence represented a string in English using the ASCII encoding standard, it would be the letter v. However, if our computer was processing an image, that binary sequence could contain information about the color of a pixel.
The computer knows to process them differently because the bytes are encoded differently.
Byte encoding is the format of the byte.
A buffer in Node.js uses the UTF-8 encoding scheme by default if it's initialized with string data. A byte in UTF-8 represents a number, a letter (in English and in other languages), or a symbol.
UTF-8 is a superset of ASCII, the American Standard Code for Information Interchange.
ASCII can encode bytes with uppercase and lowercase English letters, the numbers 0-9, and a few other symbols like the exclamation mark (!)
or the ampersand sign (&).
If we were writing a program that could only work with ASCII characters, we could change the encoding used by our buffer with the alloc () function's third argument - encoding.
Let's create a new buffer that's five bytes long and stores only ASCII characters:
The buffer is initialized with five bytes of the character a, using the ASCII representation.
< $> note Note: By default, Node.js supports the following character encodings:
ASCII, represented as ascii
UTF-8, represented as utf-8 or utf8
UTF-16, represented as utf-16le or utf16le
UCS-2, represented as ucs-2 or ucs2
Base64, represented as base64
Hexadecimal, represented as hex
ISO / IEC 8859-1, represented as latin1 or binary
All of these values can be used in Buffer class functions that accept an encoding parameter.
Therefore, these values are all valid for the alloc () method.
So far we've been creating new buffers with the alloc () function.
But sometimes we may want to create a buffer from data that already exists, like a string or array.
To create a buffer from pre-existing data, we use the from () method.
We can use that function to create buffers from:
An array of integers: The integer values can be between 0 and 255.
An ArrayBuffer: This is a JavaScript object that stores a fixed length of bytes.
A string.
Another buffer.
Other JavaScript objects that have a Symbol.toPrimitive property.
That property tells JavaScript how to convert the object to a primitive data type: boolean, null, undefined, number, string, or symbol.
You can read more about Symbols at Mozilla's JavaScript documentation.
Let's see how we can create a buffer from a string.
In the Node.js prompt, enter this:
We now have a buffer object created from the string My name is Paul.
Let's create a new buffer from another buffer we made earlier:
We've now created a new buffer asciiCopy that contains the same data as asciiBuf.
Now that we have experienced creating buffers, we can dive into examples of reading their data.
Step 2 - Reading from a Buffer
There are many ways to access data in a Buffer.
We can access an individual byte in a buffer or we can extract the entire contents.
To access one byte of a buffer, we pass the index or location of the byte we want.
Buffers store data sequentially like arrays.
They also index their data like arrays, starting at 0. We can use array notation on the buffer object to get an individual byte.
Let's see how this looks by creating a buffer from a string in the REPL:
Now let's read the first byte of the buffer:
As you press ENTER, the REPL will display:
The integer 72 corresponds the UTF-8 representation for the letter H.
< $> note Note: The values for bytes can be numbers between 0 and 255. A byte is a sequence of 8 bits.
A bit is binary, and therefore can only have one of two values: 0 or 1. If we have a sequence of 8 bits and two possible values per bit, then we have a maximum of 2 ⁸ possible values for a byte.
That works out to a maximum of 256 values.
Since we start counting from zero, that means our highest number is 255. < $>
Let's do the same for the second byte.
Enter the following in the REPL:
The REPL returns 105, which represents the lowercase i.
Finally, let's get the third character:
You will see 33 displayed in the REPL, which corresponds to!.
Let's try to retrieve a byte from an invalid index:
The REPL will return:
This is just like if we tried to access an element in an array with an incorrect index.
Now that we've seen how to read individual bytes of a buffer, let's see our options for retrieving all the data stored in a buffer at once.
The buffer object comes with the toString () and the toJSON () methods, which return the entire contents of a buffer in two different formats.
As its name suggests, the toString () method converts the bytes of the buffer into a string and returns it to the user.
If we use this method on hiBuf, we will get the string Hi!.
Let's try it!
In the prompt, enter:
That buffer was created from a string.
Let's see what happens if we use the toString () on a buffer that was not made from string data.
Let's create a new, empty buffer that's 10 bytes large:
Now, let's use the toString () method:
We will see the following result:
The string\ u0000 is the Unicode character for NULL.
It corresponds to the number 0. When the buffer's data is not encoded as a string, the toString () method returns the UTF-8 encoding of the bytes.
The toString () has an optional parameter, encoding.
We can use this parameter to change the encoding of the buffer data that's returned.
For example, if you wanted the hexadecimal encoding for hiBuf you would enter the following at the prompt:
That statement will evaluate to:
486921 is the hexadecimal representation for the bytes that represent the string Hi!.
In Node.js, when users want to convert the encoding of data from one form to another, they usually put the string in a buffer and call toString () with their desired encoding.
The toJSON () method behaves differently.
Regardless of whether the buffer was made from a string or not, it always returns the data as the integer representation of the byte.
Let's re-use the hiBuf and tenZeroes buffers to practice using toJSON ().
At the prompt, enter:
The JSON object has a type property that will always be Buffer.
That's so programs can distinguish these JSON object from other JSON objects.
The data property contains an array of the integer representation of the bytes.
You may have noticed that 72, 105, and 33 correspond to the values we received when we individually pulled the bytes.
Let's try the toJSON () method with tenZeroes:
In the REPL you will see the following:
The type is the same as noted before.
However, the data is now an array with ten zeroes.
Now that we've covered the main ways to read from a buffer, let's look at how we modify a buffer's contents.
Step 3 - Modifying a Buffer
There are many ways we can modify an existing buffer object.
Similar to reading, we can modify buffer bytes individually using the array syntax.
We can also write new contents to a buffer, replacing the existing data.
Let's begin by looking at how we can change individual bytes of a buffer.
Recall our buffer variable hiBuf, which contains the string Hi!.
Let's change each byte so that it contains Hey instead.
In the REPL, let's first try setting the second element of hiBuf to e:
Now, let's see this buffer as a string to confirm it's storing the right data. Follow up by calling the toString () method:
It will be evaluated as:
We received that strange output because the buffer can only accept an integer value.
We can't assign it to the letter e; rather, we have to assign it the number whose binary equivalent represents e:
Now when we call the toString () method:
We get this output in the REPL:
To change the last character in the buffer, we need to set the third element to the integer that corresponds to the byte for y:
Let's confirm by using the toString () method once again:
Your REPL will display:
If we try to write a byte that's outside the range of the buffer, it will be ignored and the contents of the buffer won't change.
For example, let's try to set the non-existent fourth element of the buffer to o:
We can confirm that the buffer is unchanged with the toString () method:
The output is still:
If we wanted to change the contents of the entire buffer, we can use the write () method.
The write () method accepts a string that will replace the contents of a buffer.
Let's use the write () method to change the contents of hiBuf back to Hi!.
In your Node.js shell, type the following command at the prompt:
The write () method returned 3 in the REPL.
This is because it wrote three bytes of data. Each letter has one byte in size, since this buffer uses UTF-8 encoding, which uses a byte for each character.
If the buffer used UTF-16 encoding, which has a minimum of two bytes per character, then the write () function would have returned 6.
Now verify the contents of the buffer by using toString ():
The REPL will produce:
This is quicker than having to change each element byte-by-byte.
If you try to write more bytes than a buffer's size, the buffer object will only accept what bytes fit.
To illustrate, let's create a buffer that stores three bytes:
Now let's attempt to write Cats to it:
When the write () call is evaluated, the REPL returns 3 indicating only three bytes were written to the buffer.
Now confirm that the buffer contains the first three bytes:
The REPL returns:
The write () function adds the bytes in sequential order, so only the first three bytes were placed in the buffer.
By contrast, let's make a Buffer that stores four bytes:
Write the same contents to it:
Then add some new content that occupies less space than the original content:
Since buffers write sequentially, starting from 0, if we print the buffer's contents:
We'd be greeted with:
The first two characters are overwritten, but the rest of the buffer is untouched.
Sometimes the data we want in our pre-existing buffer is not in a string but resides in another buffer object.
In these cases, we can use the copy () function to modify what our buffer is storing.
Let's create two new buffers:
The wordsBuf and catchphraseBuf buffers both contain string data. We want to modify catchphraseBuf so that it stores Nananana Turtle!
instead of Not sure Turtle!.
We'll use copy () to get Nananana from wordsBuf to catchphraseBuf.
To copy data from one buffer to the other, we'll use the copy () method on the buffer that's the source of the information.
Therefore, as wordsBuf has the string data we want to copy, we need to copy like this:
The target parameter in this case is the catchphraseBuf buffer.
When we enter that into the REPL, it returns 15 indicating that 15 bytes were written.
The string Nananana only uses 8 bytes of data, so we immediately know that our copy did not go as intended.
Use the toString () method to see the contents of catchphraseBuf:
By default, copy () took the entire contents of wordsBuf and placed it into catchphraseBuf.
We need to be more selective for our goal and only copy Nananana.
Let's re-write the original contents of catchphraseBuf before continuing:
The copy () function has a few more parameters that allow us to customize what data is copied to the other buffer.
Here's a list of all the parameters of this function:
target - This is the only required parameter of copy ().
As we've seen from our previous usage, it is the buffer we want to copy to.
targetStart - This is the index of the bytes in the target buffer where we should begin copying to.
By default it's 0, meaning it copies data starting at the beginning of the buffer.
sourceStart - This is the index of the bytes in the source buffer where we should copy from.
sourceEnd - This is the index of the bytes in the source buffer where we should stop copying.
By default, it's the length of the buffer.
So, to copy Nananana from wordsBuf into catchphraseBuf, our target should be catchphraseBuf like before.
The targetStart would be 0 as we want Nananana to appear at the beginning of catchphraseBuf.
The sourceStart should be 7 as that's the index where Nananana begins in wordsBuf.
The sourceEnd would continue to be the length of the buffers.
At the REPL prompt, copy the contents of wordsBuf like this:
The REPL confirms that 8 bytes have been written.
Note how wordsBuf.length is used as the value for the sourceEnd parameter.
Like arrays, the length property gives us the size of the buffer.
Now let's see the contents of catchphraseBuf:
We were able to modify the data of catchphraseBuf by copying the contents of wordsBuf.
You can exit the Node.js REPL if you would like to do so.
Note that all the variables that were created will no longer be available when you do:
In this tutorial, you learned that buffers are fixed-length allocations in memory that store binary data. You first created buffers by defining their size in memory and by initializing them with pre-existing data. You then read data from a buffer by examining their individual bytes and by using the toString () and toJSON () methods.
Finally, you modified the data stored by a buffer by changing its individual bytes and by using the write () and copy () methods.
Buffers give you great insight into how binary data is manipulated by Node.js.
Now that you can interact with buffers, you can observe the different ways character encoding affect how data is stored.
For example, you can create buffers from string data that are not UTF-8 or ASCII encoding and observe their difference in size.
You can also take a buffer with UTF-8 and use toString () to convert it to other encoding schemes.
To learn about buffers in Node.js, you can read the Node.js documentation on the Buffer object.
If you "d like to continue learning Node.js, you can return to the How To Code in Node.js series, or browse programming projects and setups on our Node topic page.
How To Add Swap Space on Ubuntu 20.04
5463
One way to guard against out-of-memory errors in applications is to add some swap space to your server.
In this guide, we will cover how to add a swap file to an Ubuntu 20.04 server.
< $> warning Warning: Although swap is generally recommended for systems using traditional spinning hard drives, placing swap on SSDs can cause issues with hardware degradation over time.
Due to this, we do not recommend enabling swap on DigitalOcean or any other provider that uses SSD storage.
What is Swap?
Swap is a portion of hard drive storage that has been set aside for the operating system to temporarily store data that it can no longer hold in RAM.
This lets you increase the amount of information that your server can keep in its working memory, with some caveats.
The swap space on the hard drive will be used mainly when there is no longer sufficient space in RAM to hold in-use application data.
The information written to disk will be significantly slower than information kept in RAM, but the operating system will prefer to keep running application data in memory and use swap for the older data. Overall, having swap space as a fallback for when your system's RAM is depleted can be a good safety net against out-of-memory exceptions on systems with non-SSD storage available.
Step 1 - Checking the System for Swap Information
Before we begin, we can check if the system already has some swap space available.
It is possible to have multiple swap files or swap partitions, but generally one should be enough.
We can see if the system has any configured swap by typing:
If you don't get back any output, this means your system does not have swap space available currently.
You can verify that there is no active swap using the free utility:
As you can see in the Swap row of the output, no swap is active on the system.
Step 2 - Checking Available Space on the Hard Drive Partition
Before we create our swap file, we'll check our current disk usage to make sure we have enough space.
Do this by entering:
The device with / in the Mounted on column is our disk in this case.
We have plenty of space available in this example (only 1.4G used).
Your usage will probably be different.
Although there are many opinions about the appropriate size of a swap space, it really depends on your personal preferences and your application requirements.
Generally, an amount equal to or double the amount of RAM on your system is a good starting point.
Another good rule of thumb is that anything over 4G of swap is probably unnecessary if you are just using it as a RAM fallback.
Step 3 - Creating a Swap File
Now that we know our available hard drive space, we can create a swap file on our filesystem.
We will allocate a file of the size that we want called swapfile in our root (/) directory.
The best way of creating a swap file is with the fallocate program.
This command instantly creates a file of the specified size.
Since the server in our example has 1G of RAM, we will create a 1G file in this guide.
Adjust this to meet the needs of your own server:
We can verify that the correct amount of space was reserved by typing:
Our file has been created with the correct amount of space set aside.
Step 4 - Enabling the Swap File
Now that we have a file of the correct size available, we need to actually turn this into swap space.
First, we need to lock down the permissions of the file so that only users with root privileges can read the contents.
This prevents normal users from being able to access the file, which would have significant security implications.
Make the file only accessible to root by typing:
Verify the permissions change by typing:
As you can see, only the root user has the read and write flags enabled.
We can now mark the file as swap space by typing:
After marking the file, we can enable the swap file, allowing our system to start using it:
Verify that the swap is available by typing:
We can check the output of the free utility again to corroborate our findings:
Our swap has been set up successfully and our operating system will begin to use it as necessary.
Step 5 - Making the Swap File Permanent
Our recent changes have enabled the swap file for the current session.
However, if we reboot, the server will not retain the swap settings automatically.
We can change this by adding the swap file to our / etc / fstab file.
Back up the / etc / fstab file in case anything goes wrong:
Add the swap file information to the end of your / etc / fstab file by typing:
Next we'll review some settings we can update to tune our swap space.
Step 6 - Tuning your Swap Settings
There are a few options that you can configure that will have an impact on your system's performance when dealing with swap.
Adjusting the Swappiness Property
The swappiness parameter configures how often your system swaps data out of RAM to the swap space.
This is a value between 0 and 100 that represents a percentage.
With values close to zero, the kernel will not swap data to the disk unless absolutely necessary.
Remember, interactions with the swap file are "expensive" in that they take a lot longer than interactions with RAM and they can cause a significant reduction in performance.
Telling the system not to rely on the swap much will generally make your system faster.
Values that are closer to 100 will try to put more data into swap in an effort to keep more RAM space free.
Depending on your applications' memory profile or what you are using your server for, this might be better in some cases.
We can see the current swappiness value by typing:
For a Desktop, a swappiness setting of 60 is not a bad value.
For a server, you might want to move it closer to 0.
We can set the swappiness to a different value by using the sysctl command.
For instance, to set the swappiness to 10, we could type:
This setting will persist until the next reboot.
We can set this value automatically at restart by adding the line to our / etc / sysctl.conf file:
At the bottom, you can add:
Adjusting the Cache Pressure Setting
Another related value that you might want to modify is the vfs _ cache _ pressure.
This setting configures how much the system will choose to cache inode and dentry information over other data.
Basically, this is access data about the filesystem.
This is generally very costly to look up and very frequently requested, so it's an excellent thing for your system to cache.
You can see the current value by querying the proc filesystem again:
As it is currently configured, our system removes inode information from the cache too quickly.
We can set this to a more conservative setting like 50 by typing:
Again, this is only valid for our current session.
We can change that by adding it to our configuration file like we did with our swappiness setting:
At the bottom, add the line that specifies your new value:
Following the steps in this guide will give you some breathing room in cases that would otherwise lead to out-of-memory exceptions.
Swap space can be incredibly useful in avoiding some of these common problems.
If you are running into OOM (out of memory) errors, or if you find that your system is unable to use the applications you need, the best solution is to optimize your application configurations or upgrade your server.
How To Set Up and Configure an OpenVPN Server on Ubuntu 20.04
5390
A Virtual Private Network (VPN) allows you to traverse untrusted networks as if you were on a private network.
It gives you the freedom to access the internet safely and securely from your smartphone or laptop when connected to an untrusted network, like the WiFi at a hotel or coffee shop.
You can circumvent geographical restrictions and censorship, and shield your location and any unencrypted HTTP traffic from untrusted networks.
OpenVPN is a full featured, open-source Transport Layer Security (TLS) VPN solution that accommodates a wide range of configurations.
In this tutorial, you will set up OpenVPN on an Ubuntu 20.04 server, and then configure it to be accessible from a client machine.
< $> note Note: If you plan to set up an OpenVPN Server on a DigitalOcean Droplet, be aware that we, like many hosting providers, charge for bandwidth overages.
One Ubuntu 20.04 server with a sudo non-root user and a firewall enabled.
To set this up, you can follow our Initial Server Setup with Ubuntu 20.04 tutorial.
We will refer to this as the OpenVPN Server throughout this guide.
A separate Ubuntu 20.04 server set up as a private Certificate Authority (CA), which we will refer to as the CA Server throughout this guide.
After executing the steps from the Initial Server Setup Guide on this server, you can follow steps 1 to 3 of our guide on How To Set Up and Configure a Certificate Authority (CA) on Ubuntu 20.04 to accomplish that.
< $> note Note: While it is technically possible to use your OpenVPN Server or your local machine as your CA, this is not recommended as it opens up your VPN to some security vulnerabilities.
Per the official OpenVPN documentation, you should place your CA on a standalone machine that's dedicated to importing and signing certificate requests.
For this reason, this guide assumes that your CA is on a separate Ubuntu 20.04 server that also has a non-root user with sudo privileges and a basic firewall enabled.
In addition to that, you'll need a client machine which you will use to connect to your OpenVPN Server.
In this guide, we'll call this the OpenVPN Client.
For the purposes of this tutorial, it's recommended that you use your local machine as the OpenVPN client.
With these prerequisites in place, you are ready to begin setting up and configuring an OpenVPN Server on Ubuntu 20.04.
< $> note Note: Please note that if you disable password authentication while configuring these servers, you may run into difficulties when transferring files between them later on in this guide.
Alternatively, you could generate an SSH keypair for each server, then add the OpenVPN Server "s public SSH key to the CA machine" s authorized _ keys file and vice versa.
See How to Set Up SSH Keys on Ubuntu 20.04 for instructions on how to perform either of these solutions.
Step 1 - Installing OpenVPN and Easy-RSA
The first step in this tutorial is to install OpenVPN and Easy-RSA.
Easy-RSA is a public key infrastructure (PKI) management tool that you will use on the OpenVPN Server to generate a certificate request that you will then verify and sign on the CA Server.
To start off, update your OpenVPN Server "s package index and install OpenVPN and Easy-RSA.
Both packages are available in Ubuntu "s default repositories, so you can use apt for the installation:
Next you will need to create a new directory on the OpenVPN Server as your non-root user called ~ / easy-rsa:
Now you will need to create a symlink from the easyrsa script that the package installed into the ~ / easy-rsa directory that you just created:
< $> note Note: While other guides might instruct you to copy the easy-rsa package files into your PKI directory, this tutorial adopts a symlink approach.
As a result, any updates to the easy-rsa package will be automatically reflected in your PKI's scripts.
Finally, ensure the directory's owner is your non-root sudo user and restrict access to that user using chmod:
Once these programs are installed and have been moved to the right locations on your system, the next step is to create a Public Key Infrastructure (PKI) on the OpenVPN server so that you can request and manage TLS certificates for clients and other servers that will connect to your VPN.
Step 2 - Creating a PKI for OpenVPN
Before you can create your OpenVPN server's private key and certificate, you need to create a local Public Key Infrastructure directory on your OpenVPN server.
You will use this directory to manage the server and clients' certificate requests instead of making them directly on your CA server.
To build a PKI directory on your OpenVPN server, you'll need to populate a file called vars with some default values.
First you will cd into the easy-rsa directory, then you will create and edit the vars file using nano or your preferred text editor.
Once the file is opened, paste in the following two lines:
These are the only two lines that you need in this vars file on your OpenVPN server since it will not be used as a Certificate Authority.
They will ensure that your private keys and certificate requests are configured to use modern Elliptic Curve Cryptography (ECC) to generate keys and secure signatures for your clients and OpenVPN server.
Configuring your OpenVPN & CA servers to use ECC means when a client and server attempt to establish a shared symmetric key, they can use Elliptic Curve algorithms to do their exchange.
Using ECC for a key exchange is significantly faster than using plain Diffie-Hellman with the classic RSA algorithm since the numbers are much smaller and the computations are faster.
< $> note Background: When clients connect to OpenVPN, they use asymmetric encryption (also known as public / private key) to perform a TLS handshake.
However, when transmitting encrypted VPN traffic, the server and clients use symmetric encryption, which is also known as shared key encryption.
There is much less computational overhead with symmetric encryption compared to asymmetric: the numbers that are used are much smaller, and modern CPUs integrate instructions to perform optimized symmetric encryption operations.
To make the switch from asymmetric to symmetric encryption, the OpenVPN server and client will use the Elliptic Curve Diffie-Hellman (ECDH) algorithm to agree on a shared secret key as quickly as possible.
Once you have populated the vars file you can proceed with creating the PKI directory.
To do so, run the easyrsa script with the init-pki option.
Although you already ran this command on the CA server as part of the prerequisites, it's necessary to run it here because your OpenVPN server and CA server have separate PKI directories:
Note that on your OpenVPN server there is no need to create a Certificate Authority.
Your CA server is solely responsible for validating and signing certificates.
The PKI on your VPN server is only used as a convenient and centralized place to store certificate requests and public certificates.
After you've initialized your PKI on the OpenVPN server, you are ready to move on to the next step, which is creating an OpenVPN server certificate request and private key.
Step 3 - Creating an OpenVPN Server Certificate Request and Private Key
Now that your OpenVPN server has all the prerequisites installed, the next step is to generate a private key and Certificate Signing Request (CSR) on your OpenVPN server.
After that you'll transfer the request over to your CA to be signed, creating the required certificate.
Once you have a signed certificate, you'll transfer it back to the OpenVPN server and install it for the server to use.
To start, navigate to the ~ / easy-rsa directory on your OpenVPN Server as your non-root user:
Now you'll call the easyrsa with the gen-req option followed by a Common Name (CN) for the machine.
The CN can be anything you like but it can be helpful to make it something descriptive.
Throughout this tutorial, the OpenVPN Server's CN will be server.
Failing to do so will password-protect the request file which could lead to permissions issues later on.
< $> note Note: If you choose a name other than server here, you will have to adjust some of the instructions below.
Copy the server key to the / etc / openvpn / server directory:
After completing these steps, you have successfully created a private key for your OpenVPN server.
You have also generated a Certificate Signing Request for the OpenVPN server.
The CSR is now ready for signing by your CA.
In the next section of this tutorial you will learn how to sign a CSR with your CA server's private key.
Step 4 - Signing the OpenVPN Server's Certificate Request
In the previous step you created a Certificate Signing Request (CSR) and private key for the OpenVPN server.
Now the CA server needs to know about the server certificate and validate it. Once the CA validates and relays the certificate back to the OpenVPN server, clients that trust your CA will be able to trust the OpenVPN server as well.
On the OpenVPN server, as your non-root user, use SCP or another transfer method to copy the server.req certificate request to the CA server for signing:
If you followed the prerequisite How To Set Up and Configure a Certificate Authority (CA) on Ubuntu 20.04 tutorial, the next step is to log in to the CA server as the non-root user that you created to manage your CA.
You "ll cd to the ~ / easy-rsa directory where you created your PK and then import the certificate request using the easyrsa script:
Next, sign the request by running the easyrsa script with the sign-req option, followed by the request type and the Common Name.
The request type can either be client or server.
Since we're working with the OpenVPN server's certificate request, be sure to use the server request type:
In the output, you'll be prompted to verify that the request comes from a trusted source.
Type yes then press ENTER to confirm:
Note that if you encrypted your CA private key, you'll be prompted for your password at this point.
With those steps complete, you have signed the OpenVPN server's certificate request using the CA server's private key.
The resulting server.crt file contains the OpenVPN server's public encryption key, as well as a signature from the CA server.
The point of the signature is to tell anyone who trusts the CA server that they can also trust the OpenVPN server when they connect to it.
To finish configuring the certificates, copy the server.crt and ca.crt files from the CA server to the OpenVPN server:
Now back on your OpenVPN server, copy the files from / tmp to / etc / openvpn / server:
Now your OpenVPN server is nearly ready to accept connections.
In the next step you'll perform some additional steps to increase the security of the server.
Step 5 - Configuring OpenVPN Cryptographic Material
For an additional layer of security, we'll add an extra shared secret key that the server and all clients will use with OpenVPN's tls-crypt directive.
This option is used to obfuscate the TLS certificate that is used when a server and client connect to each other initially.
It is also used by the OpenVPN server to perform quick checks on incoming packets: if a packet is signed using the pre-shared key, then the server processes it; if it is not signed, then the server knows it is from an untrusted source and can discard it without having to perform additional decryption work.
This option will help ensure that your OpenVPN server is able to cope with unauthenticated traffic, port scans, and Denial of Service attacks, which can tie up server resources.
It also makes it harder to identify OpenVPN network traffic.
To generate the tls-crypt pre-shared key, run the following on the OpenVPN server in the ~ / easy-rsa directory:
The result will be a file called ta.key.
Copy it to the / etc / openvpn / server / directory:
With these files in place on the OpenVPN server you are ready to create client certificates and key files for your users, which you will use to connect to the VPN.
Step 6 - Generating a Client Certificate and Key Pair
Although you can generate a private key and certificate request on your client machine and then send it to the CA to be signed, this guide outlines a process for generating the certificate request on the OpenVPN server.
The benefit of this approach is that we can create a script that will automatically generate client configuration files that contain all of the required keys and certificates.
Then, copy the client1.key file to the ~ / client-configs / keys / directory you created earlier:
Next, transfer the client1.req file to your CA Server using a secure method:
Now log in to your CA Server.
Then, navigate to the EasyRSA directory, and import the certificate request:
Next, sign the request the same way as you did for the server in the previous step.
When prompted, enter yes to confirm that you intend to sign the certificate request and that it came from a trusted source:
Back on your OpenVPN server, copy the client certificate to the ~ / client-configs / keys / directory:
Next, copy the ca.crt and ta.key files to the ~ / client-configs / keys / directory as well, and set the appropriate permissions for your sudo user:
With that, your server and client "s certificates and keys have all been generated and are stored in the appropriate directories on your OpenVPN server.
For now, you can move on to configuring OpenVPN.
Step 7 - Configuring OpenVPN
Like many other widely used open-source tools, OpenVPN has numerous configuration options available to customize your server for your specific needs.
In this section, we will provide instructions on how to set up an OpenVPN server configuration based on one of the sample configuration files that is included within this software's documentation.
First, copy the sample server.conf file as a starting point for your own configuration file:
Open the new file for editing with the text editor of your choice.
We'll use nano in our example:
We'll need to change a few lines in this file.
First, find the HMAC section of the configuration by searching for the tls-auth directive.
This line should be uncommented.
Comment it out by adding a; to the beginning of the line.
Then add a new line after it containing the value tls-crypt ta.key only:
Next, find the section on cryptographic ciphers by looking for the cipher lines.
The default value is set to AES-256-CBC, however, the AES-256-GCM cipher offers a better level of encryption, performance, and is well supported in up-to-date OpenVPN clients.
We'll comment out the default value by adding a; sign to the beginning of this line, and then we'll add another line after it containing the updated value of AES-256-GCM:
Right after this line, add an auth directive to select the HMAC message digest algorithm.
Next, find the line containing a dh directive, which defines Diffie-Hellman parameters.
Since we've configured all the certificates to use Elliptic Curve Cryptography, there is no need for a Diffie-Hellman seed file.
Comment out the existing line that looks like dh dh2048.pem or dh dh.pem.
The filename for the Diffie-Hellman key may be different than what is listed in the example server configuration file.
Then add a line after it with the contents dh none:
Next, we want OpenVPN to run with no privileges once it has started, so we need to tell it to run with a user nobody and group nogroup.
To enable this, find and uncomment the user nobody and group nogroup lines by removing the; sign from the beginning of each line:
The settings above will create the VPN connection between your client and server, but will not force any connections to use the tunnel.
If you wish to use the VPN to route all of your client traffic over the VPN, you will likely want to push some extra settings to the client computers.
To get started, find and uncomment the line containing push "redirect-gateway def1 bypass-dhcp".
Doing this will tell your client to redirect all of its traffic through your OpenVPN Server.
Be aware that enabling this functionality can cause connectivity issues with other network services, like SSH:
Just below this line, find the dhcp-option section.
Again, remove the; from the beginning of both of the lines to uncomment them:
These lines will tell your client to use the free OpenDNS resolvers at the listed IP addresses.
If you prefer other DNS resolvers you can substitute them in place of the highlighted IPs.
This will assist clients in reconfiguring their DNS settings to use the VPN tunnel as the default gateway.
To change OpenVPN to listen on port 443, open the server.conf file and find the line that looks like this:
Edit it so that the port is 443:
If so, find the proto line below the port line and change the protocol from udp to tcp:
Failing to do so while using TCP will cause errors when you start the OpenVPN service.
Find the explicit-exit-notify line at the end of the file and change the value to 0:
If you have no need to use a different port and protocol, it is best to leave these settings unchanged.
If you selected a different name during the. / easyrsa gen-req server command earlier, modify the cert and key lines in the server.conf configuration file so that they point to the appropriate .crt and .key files.
If you used the default name, server, this is already set correctly:
You have now finished configuring your OpenVPN general settings.
In the next step, we'll customize the server's networking options.
Step 8 - Adjusting the OpenVPN Server Networking Configuration
There are some aspects of the server's networking configuration that need to be tweaked so that OpenVPN can correctly route traffic through the VPN.
To adjust your OpenVPN server's default IP forwarding setting, open the / etc / sysctl.conf file using nano or your preferred editor:
Then add the following line at the bottom of the file:
To read the file and load the new values for the current session, type:
Now your OpenVPN server will be able to forward incoming traffic from one ethernet device to another.
This setting makes sure the server can direct traffic from clients that connect on the virtual VPN interface out over its other physical ethernet devices.
This configuration will route all web traffic from your client via your server's IP address, and your client's public IP address will effectively be hidden.
In the next step you will need to configure some firewall rules to ensure that traffic to and from your OpenVPN server flows properly.
Step 9 - Firewall Configuration
So far, you "ve installed OpenVPN on your server, configured it, and generated the keys and certificates needed for your client to access the VPN.
However, you have not yet provided OpenVPN with any instructions on where to send incoming web traffic from clients.
You can stipulate how the server should handle client traffic by establishing some firewall rules and routing configurations.
Assuming you followed the prerequisites at the start of this tutorial, you should already have ufw installed and running on your server.
To allow OpenVPN through the firewall, you "ll need to enable masquerading, an iptables concept that provides on-the-fly dynamic network address translation (NAT) to correctly route client connections.
Your public interface is the string found within this command "s output that follows the word" dev ".
For example, this result shows the interface named eth0, which is highlighted below:
Remember to replace < ^ > eth0 < ^ > in the -A POSTROUTING line below with the interface you found in the above command:
With the firewall rules in place, we can start the OpenVPN service on the server.
Step 10 - Starting OpenVPN
OpenVPN runs as a systemd service, so we can use systemctl to manage it. We will configure OpenVPN to start up at boot so you can connect to your VPN at any time as long as your server is running.
To do this, enable the OpenVPN service by adding it to systemctl:
Then start the OpenVPN service:
Double check that the OpenVPN service is active with the following command.
You should see active (running) in the output:
We "ve now completed the server-side configuration for OpenVPN.
Next, you will configure your client machine and connect to the OpenVPN Server.
Step 11 - Creating the Client Configuration Infrastructure
Open this new file using nano or your preferred text editor:
Next, uncomment the user and group directives by removing the; sign at the beginning of each line:
Similarly, comment out the tls-auth directive, as you will add ta.key directly into the client configuration file (and the server is set up to use tls-crypt):
Mirror the cipher and auth settings that you set in the / etc / openvpn / server / server.conf file:
Finally, add a few commented out lines to handle various methods that Linux based VPN clients will use for DNS resolution.
You "ll add two similar, but separate sets of commented out lines.
The first set is for clients that do not use systemd-resolved to manage DNS.
These clients rely on the resolvconf utility to update DNS information for Linux clients.
Now add another set of lines for clients that use systemd-resolved for DNS resolution:
Later in Step 13 - Installing the Client Configuration step of this tutorial you will learn how to determine how DNS resolution works on Linux clients and which section to uncomment.
Next, we'll create a script that will compile your base configuration with the relevant certificate, key, and encryption files and then place the generated configuration in the ~ / client-configs / files directory.
The benefit of using this method is that if you ever need to add a client in the future, you can run this script to quickly create a new config file and ensure that all the important information is stored in a single, easy-to-access location.
Step 12 - Generating Client Configurations
If you followed along with the guide, you created a client certificate and key named client1.crt and client1.key, respectively, in Step 6. You can generate a config file for these credentials by moving into your ~ / client-configs directory and running the script you made at the end of the previous step:
Here is an example SFTP command which you can run from your local computer (macOS or Linux).
This will copy the < ^ > client1.ovpn < ^ > file we've created in the last step to your home directory:
Here are several tools and tutorials for securely transferring files from the OpenVPN server to a local computer:
Step 13 - Installing the Client Configuration
< $> note Note: OpenVPN needs administrative privileges to install.
When you launch OpenVPN, it will automatically locate the profile and make it available.
Configuring Clients that use systemd-resolved
First determine if your system is using systemd-resolved to handle DNS resolution by checking the / etc / resolv.conf file:
If your system is configured to use systemd-resolved for DNS resolution, the IP address after the nameserver option will be 127.0.0.53.
There should also be comments in the file like the output that is shown that explain how systemd-resolved is managing the file.
If you have a different IP address than 127.0.0.53 then chances are your system is not using systemd-resolved and you can go to the next section on configuring Linux clients that have an update-resolv-conf script instead.
To support these clients, first install the openvpn-systemd-resolved package.
It provides scripts that will force systemd-resolved to use the VPN server for DNS resolution.
One that package is installed, configure the client to use it, and to send all DNS queries over the VPN interface.
Open the client "s VPN file:
Now uncomment the following lines that you added earlier:
Configuring Clients that use update-resolv-conf
If your system is not using systemd-resolved to manage DNS, check to see if your distribution includes an / etc / openvpn / update-resolv-conf script instead:
If your client includes the update-resolv-conf file, then edit the OpenVPN client configuration file that you transferred earlier:
Uncomment the three lines you added to adjust the DNS settings:
* * Connecting * *
< $> note Note: If your client uses systemd-resolved to manage DNS, check the settings are applied correctly by running the systemd-resolve --status command like this:
You should see output like the following:
If you see the IP addresses of the DNS servers that you configured on the OpenVPN server, along with the ~. setting for DNS Domain in the output, then you have correctly configured your client to use the VPN server "s DNS resolver.
You can also check that you are sending DNS queries over the VPN by using a site like DNS leak test.com.
Drag the .ovpn file to the OpenVPN Documents window. iTunes showing the VPN profile ready to load on the iPhone
The OpenVPN iOS app showing new profile ready to import Connecting
< $> note Note: The VPN switch under Settings cannot be used to connect to the VPN.
Start the OpenVPN app and tap the FILE menu to import the profile.
Then navigate to the location of the saved profile (the screenshot uses / storage / emulated / 0 / openvpn) and select your .ovpn file.
Tap the IMPORT button to finish importing this profile.
Connecting Once the profile is added, you will see a screen like this:
The OpenVPN Android app with new profile added
To connect, tap the toggle button close to the profile you want to use.
You'll see real time stats of your connection and traffic being routed through your OpenVPN server: The OpenVPN Android app connected to the VPN
To disconnect, just tap the toggle button on the top left once again.
You will be prompted to confirm that you want to disconnect from your VPN.
Step 14 - Testing Your VPN Connection (Optional)
< $> note Note: This method for testing your VPN connection will only work if you opted to route all your traffic through the VPN in Step 7 when you edited the server.conf file for OpenVPN.
Step 15 - Revoking Client Certificates
To do so, follow the example in the prerequisite tutorial on How to Set Up and Configure a Certificate Authority on Ubuntu 20.04 under the Revoking a Certificate section.
Once you have revoked a certificate for a client using those instructions, you'll need to copy the generated crl.pem file to your OpenVPN server in the / etc / openvpn / server directory:
Transfer the new crl.pem file to your OpenVPN server and copy it to the / etc / openvpn / server / directory to overwrite the old list.
You should now have a fully operational virtual private network running on your OpenVPN Server.
You can browse the web and download content without worrying about malicious actors tracking your activity.
There are several steps you could take to customize your OpenVPN installation even further, such as configuring your client to connect to the VPN automatically or configuring client-specific rules and access policies.
For these and other OpenVPN customizations, you should consult the official OpenVPN documentation.
To configure more clients, you only need to follow steps 6 and 11-13 for each additional device.
To revoke access to clients, follow step 15.
How To Use the MySQL BLOB Data Type to Store Images with PHP on Ubuntu 18.04
5464
The author selected Girls Who Code to receive a donation as part of the Write for DOnations program.
A Binary Large Object (BLOB) is a MySQL data type that can store binary data such as images, multimedia, and PDF files.
When creating applications that require a tightly-coupled database where images should be in sync with related data (for example, an employee portal, a student database, or a financial application), you might find it convenient to store images such as students' passport photos and signatures in a MySQL database alongside other related information.
This is where the MySQL BLOB data type comes in.
This programming approach eliminates the need for creating a separate file system for storing images.
The scheme also centralizes the database, making it more portable and secure because the data is isolated from the file system.
Creating backups is also more seamless since you can create a single MySQL dump file that contains all your data.
Retrieving data is faster, and when creating records you can be sure that data validation rules and referential integrity are maintained especially when using MySQL transactions.
In this tutorial, you will use the MySQL BLOB data type to store images with PHP on Ubuntu 18.04.
To follow along with this guide, you will need the following:
An Ubuntu 18.04 server configured using the Initial Server Setup with Ubuntu 18.04 and a non-root user with sudo privileges.
Apache, MySQL, and PHP set up by following the guide on How To Install Linux, Apache, MySQL, PHP (LAMP) stack on Ubuntu 18.04.
For this tutorial, it isn't necessary to create virtual hosts, so you can skip Step 4.
Step 1 - Creating a Database
You'll start off by creating a sample database for your project.
To do this, SSH in to your server and then run the following command to log in to your MySQL server as root:
Enter the root password of your MySQL database and hit ENTER to continue.
Then, run the following command to create a database.
In this tutorial we'll name it test _ company:
Once the database is created, you will see the following output:
Next, create a test _ user account on the MySQL server and remember to replace PASSWORD with a strong password:
To grant test _ user full privileges on the test _ company database, run:
Make sure you get the following output:
Finally, flush the privileges table in order for MySQL to reload the permissions:
Ensure you see the following output:
Now that the test _ company database and test _ user are ready, you'll move on to creating a products table for storing sample products.
You'll use this table later to insert and retrieve records to demonstrate how MySQL BLOB works.
Then, log back in again with the credentials of the test _ user that you created:
When prompted, enter the password for the test _ user and hit ENTER to continue.
Next, switch to the test _ company database by typing the following:
Once the test _ company database is selected, MySQL will display:
Next, create a products table by running:
This command creates a table named products.
The table has four columns:
product _ id: This column uses a BIGINT data type in order to accommodate a large list of products up to a maximum of 2 ⁶ ³ -1 items.
You've marked the column as PRIMARY KEY to uniquely identify products.
In order for MySQL to handle the generation of new identifiers for inserted columns, you have used the keyword AUTO _ INCREMENT.
product _ name: This column holds the names of the products.
You've used the VARCHAR data type since this field will generally handle alphanumerics up to a maximum of 50 characters - the limit of 50 is just a hypothetical value used for the purpose of this tutorial.
price: For demonstration purposes, your products table contains the price column to store the retail price of products.
Since some products may have floating values (for example, 23.69, 45.36, 102.99), you've used the DOUBLE data type.
product _ image: This column uses a BLOB data type to store the actual binary data of the products' images.
You've used the InnoDB storage ENGINE for the table to support a wide range of features including MySQL transactions.
After executing this for creating the products table, you'll see the following output:
Log out from your MySQL server:
You will get the following output
The products table is now ready to store some records including products' images and you'll populate it with some products in the next step.
Step 2 - Creating PHP Scripts for Connecting and Populating the Database
In this step, you'll create a PHP script that will connect to the MySQL database that you created in Step 1. The script will prepare three sample products and insert them into the products table.
To create the PHP code, open a new file with your text editor:
Then, enter the following information into the file and replace < ^ > PASSWORD < ^ > with the test _ user password that you created in Step 1:
In this file, you've used four PHP constants to connect to the MySQL database that you created in Step 1:
DB _ NAME: This constant holds the name of the test _ company database.
DB _ USER: This variable holds the test _ user username.
DB _ PASSWORD: This constant stores the MySQL PASSWORD of the test _ user account.
DB _ HOST: This represents the server where the database resides.
In this case, you are using the localhost server.
The following line in your file initiates a PHP Data Object (PDO) and connects to the MySQL database:
Toward the end of the file, you've set a couple of PDO attributes:
ATTR _ ERRMODE, PDO:: ERRMODE _ EXCEPTION: This attribute instructs PDO to throw an exception that can be logged for debugging purposes.
ATTR _ EMULATE _ PREPARES, false: This option increases security by telling the MySQL database engine to do the prepare instead of PDO.
You'll include the / var / www / html / config.php file in two PHP scripts that you will create next for inserting and retrieving records respectively.
First, create the / var / www / html / insert _ products.php PHP script for inserting records to the products table:
Then, add the following information into the / var / www / html / insert _ products.php file:
In the file, you've included the config.php file at the top.
This is the first file you created for defining the database variables and connecting to the database.
The file also initiates a PDO object and stores it in a $pdo variable.
Next, you've created an array of the products' data to be inserted into the database.
Apart from the product _ name and price, which are prepared as strings and numeric values respectively, the script uses PHP's in-built file _ get _ contents function to read images from an external source and pass them as strings to the product _ image column.
Next, you have prepared an SQL statement and used the PHP foreach {...} statement to insert each product into the database.
To execute the / var / www / html / insert _ products.php file, run it in your browser window using the following URL.
Remember to replace < ^ > your-server-IP < ^ > with the public IP address of your server:
After executing the file, you'll see a success message in your browser confirming records were inserted into the database.
A success message showing that records were inserted to database
You have successfully inserted three records containing product images into the products table.
In the next step, you'll create a PHP script for retrieving these records and displaying them in your browser.
Step 3 - Displaying Products' Information From the MySQL Database
With the products' information and images in the database, you're now going to code another PHP script that queries and displays the products' information in an HTML table on your browser.
To create the file, type the following:
Then, enter the following information into the file:
Save the changes to the file and close it.
Here you've again included the config.php file in order to connect to the database.
Then, you have prepared and executed an SQL statement using PDO to retrieve all items from the products table using the SELECT * FROM products command.
Next, you have created an HTML table and populated it with the products' data using the PHP while () {...} statement.
The line $row = $stmt- > fetch (PDO:: FETCH _ ASSOC) queries the database and stores the result in the $row variable as a multi-dimensional array, which you have then displayed in an HTML table column using the $row ['column _ name'] syntax.
The images from the product _ image column are enclosed inside the < img src = "" > tags.
You've used the width and height attributes to resize the images to a smaller size that can fit in the HTML table column.
In order to convert the data held by the BLOB data type back to images, you've used the in-built PHP base64 _ encode function and the following syntax for the Data URI scheme:
In this case, the image / png is the media _ type and the Base64 encoded string from the product _ image column is the base _ 64 _ encoded _ data.
Next, execute the display _ products.php file in a web browser by typing the following address:
After running the display _ products.php file in your browser, you will see an HTML table with a list of products and associated images.
List of products from MySQL database
This confirms that the PHP script for retrieving images from MySQL is working as expected.
In this guide, you used the MySQL BLOB data type to store and display images with PHP on Ubuntu 18.04.
You've also seen the basic advantages of storing images in a database as opposed to storing them in a file system.
These include portability, security, and ease of backup.
If you are building an application such as a students' portal or employees' database that requires information and related images to be stored together, then this technology can be of great use to you.
For more information about the supported data types in MySQL follow the MySQL Data Types guide.
If you're interested in further content relating to MySQL and PHP, check out the following tutorials:
From Containers to Kubernetes with Node.js eBook
5504
< $> note label Download the Complete eBook!
From Containers to Kubernetes with Node.js eBook in EPUB format
From Containers to Kubernetes with Node.js eBook in PDF format < $>
Introduction to the eBook
This book is designed to introduce you to using containers and Kubernetes for full-stack development.
You'll learn how to develop a full-stack application using Node.js and MongoDB and how to manage them - first with Docker, then with Docker Compose, and finally with Kubernetes.
This book is based on the From Containers to Kubernetes with Node.js tutorial series found on DigitalOcean Community.
The topics that it covers include how to:
Build a Node.js application using Docker for development
Integrate a NoSQL database into your Node.js application using MongoDB
Manage your development environment with Docker Compose
Migrate your Docker Compose workflow to Kubernetes
Scale your Node.js and MongoDB application using Helm and Kubernetes
Secure your containerized Node.js application using Nginx, Let's Encrypt, and Docker Compose
Each chapter is is designed to build progressively from the first.
However, if you're familiar with a topic, or are more interested in a particular section, feel free to jump to the chapter that best suits your purpose.
Download the eBook
You can download the eBook in either the EPUB or PDF format by following the links below.
If you'd like to learn more about app development using Node.js visit the DigitalOcean Community's Node.js section (https: / / www.digitalocean.com / community / tags / node-js).
Or if you want to continue learning about containers, Docker, and Kubernetes, you might be interested in the Kubernetes for Full-Stack Developers self-guided course.
How To Install Java with Apt on Ubuntu 20.04
5503
Java and the JVM (Java's virtual machine) are required for many kinds of software, including Tomcat, Jetty, Glassfish, Cassandra and Jenkins.
In this guide, you will install various versions of the Java Runtime Environment (JRE) and the Java Developer Kit (JDK) using apt.
You'll install OpenJDK as well as the official JDK from Oracle.
You'll then select the version you wish to use for your projects.
When you're finished, you'll be able to use the JDK to develop software or use the Java Runtime to run software.
One Ubuntu 20.04 server set up by following the the Ubuntu 20.04 initial server setup guide tutorial, including a sudo non-root user and a firewall.
Installing the Default JRE / JDK
The easiest option for installing Java is to use the version packaged with Ubuntu.
By default, Ubuntu 20.04 includes Open JDK 11, which is an open-source variant of the JRE and JDK.
To install this version, first update the package index:
Next, check if Java is already installed:
If Java is not currently installed, you'll see the following output:
Execute the following command to install the default Java Runtime Environment (JRE), which will install the JRE from OpenJDK 11:
The JRE will allow you to run almost all Java software.
Verify the installation with:
You may need the Java Development Kit (JDK) in addition to the JRE in order to compile and run some specific Java-based software.
To install the JDK, execute the following command, which will also install the JRE:
Verify that the JDK is installed by checking the version of javac, the Java compiler:
Next, let's look at how to install Oracle's official JDK and JRE.
Installing Oracle JDK 11
Oracle's licensing agreement for Java doesn't allow automatic installation through package managers.
To install the Oracle JDK, which is the official version distributed by Oracle, you must create an Oracle account and manually download the JDK to add a new package repository for the version you'd like to use.
Then you can use apt to install it with help from a third party installation script.
The version of Oracle's JDK you'll need to download must match version of the installer script.
To find out which version you need, visit the oracle-java11-installer page.
Locate the package for Focal, as shown in the following figure:
Installer package for Ubuntu 2.04
In this image, the version of the script is 11.0.7.
In this case, you'll need Oracle JDK 11.0.7.
You don't need to download anything from this page; you'll download the installation script through apt shortly.
Then visit the Downloads page and locate the version that matches the one you need.
Oracle Java 11
Click the JDK Download button and you'll be taken to a screen that shows the versions available.
Click the .tar.gz package for Linux.
Linux download
You'll be presented with a screen asking you to accept the Oracle license agreement.
Select the checkbox to accept the license agreement and press the Download button.
Your download will begin.
You may need to log in to your Oracle account one more time before the download starts.
Once the file has downloaded, you'll need to transfer it to your server.
On your local machine, upload the file to your server.
On macOS, Linux, or Windows using the Windows Subsystem for Linux, use the scp command to transfer the file to the home directory of your < ^ > sammy < ^ > user.
The following command assumes you've saved the Oracle JDK file to your local machine's Downloads folder:
Once the file upload has completed, return to your server and add the third-party repository that will help you install Oracle's Java.
Install the software-properties-common package, which adds the add-apt-repository command to your system:
Next, import the signing key used to verify the software you're about to install:
You'll see this output:
Then use the add-apt-repository command to add the repo to your list of package sources:
You'll see this message:
Press ENTER to continue the installation.
You may see a message about no valid OpenPGP data found, but you can safely ignore this.
Update your package list to make the new software available for installation:
The installer will look for the Oracle JDK you downloaded in / var / cache / oracle-jdk11-installer-local.
Create this directory and move the Oracle JDK archive there:
Finally, install the package:
The installer will first ask you to accept the Oracle license agreement.
Accept the agreement, then the installer will extract the Java package and install it.
Now let's look at how to select which version of Java you want to use.
Managing Java
You can have multiple Java installations on one server.
You can configure which version is the default for use on the command line by using the update-alternatives command.
This is what the output would look like if you've installed both versions of Java in this tutorial:
Choose the number associated with the Java version to use it as the default, or press ENTER to leave the current settings in place.
You can do this for other Java commands, such as the compiler (javac):
Other commands for which this command can be run include, but are not limited to: keytool, javadoc and jarsigner.
Setting the JAVA _ HOME Environment Variable
Many programs written using Java use the JAVA _ HOME environment variable to determine the Java installation location.
To set this environment variable, first determine where Java is installed.
Use the update-alternatives command:
This command shows each installation of Java along with its installation path:
In this case the installation paths are as follows:
OpenJDK 11 is located at / usr / lib / jvm / java-11-openjdk-amd64 / bin / java.
Oracle Java is located at / usr / lib / jvm / java-11-oracle / jre / bin / java.
Copy the path from your preferred installation.
Then open / etc / environment using nano or your favorite text editor:
At the end of this file, add the following line, making sure to replace the highlighted path with your own copied path, but do not include the bin / portion of the path:
Modifying this file will set the JAVA _ HOME path for all users on your system.
Save the file and exit the editor.
Now reload this file to apply the changes to your current session:
Verify that the environment variable is set:
You'll see the path you just set:
Other users will need to execute the command source / etc / environment or log out and log back in to apply this setting.
In this tutorial you installed multiple versions of Java and learned how to manage them.
You can now install software which runs on Java, such as Tomcat, Jetty, Glassfish, Cassandra or Jenkins.
How To Install MariaDB on Ubuntu 20.04 Quickstart
5568
A previous version of this tutorial was written by Brian Boucheron
MariaDB is an open-source relational database management system, commonly used as an alternative for MySQL as the database portion of the popular LAMP (Linux, Apache, MySQL, PHP / Python / Perl) stack.
This quickstart tutorial describes how to install MariaDB on an Ubuntu 20.04 server and set it up with a safe initial configuration.
It will also cover how to set up an additional administrative account for password access.
To follow this tutorial, you will need a server running Ubuntu 20.04.
This server should have a non-root administrative user and a firewall configured with UFW.
Set this up by following our initial server setup guide for Ubuntu 20.04.
Before you install MariaDB, update the package index on your server with apt:
When installed from the default repositories, MariaDB will start running automatically.
Run the security script that came installed with MariaDB.
This will take you through a series of prompts where you can make some changes to your MariaDB installation "s security options:
Since you have not set one up yet, press ENTER to indicate "none".
This will remove some anonymous users and the test database, disable remote root logins, and then load these new rules.
Step 3 - (Optional) Creating an Administrative User that Employs Password Authentication
On Ubuntu systems running MariaDB 10.3, the root MariaDB user is set to authenticate using the unix _ socket plugin by default rather than with a password.
Instead, the package maintainers recommend creating a separate administrative account for password-based access.
To this end, open up the MariaDB prompt from your terminal:
Then create a new user with root privileges and password-based access.
Be sure to change the username and password to match your preferences:
You can test out this new user with the mysqladmin tool, a client that lets you run administrative commands.
The following mysqladmin command connects to MariaDB as the admin user and returns the version number after prompting for the user's password:
You will receive output similar to this:
In this guide you installed the MariaDB relational database management system, and secured it using the mysql _ secure _ installation script that it came installed with.
You also had the option to create a new administrative user that uses password authentication.
Learn how to import and export databases
Practice running SQL queries
Incorporate MariaDB into a larger application stack
How To Install Node.js on Ubuntu 20.04
5508
Node.js is a JavaScript runtime for server-side programming.
It allows developers to create scalable backend functionality using JavaScript, a language many are already familiar with from browser-based web development.
In this guide, we will show you three different ways of getting Node.js installed on an Ubuntu 20.04 server:
using apt to install the nodejs package from Ubuntu's default software repository
using apt with an alternate PPA software repository to install specific versions of the nodejs package
installing nvm, the Node Version Manager, and using it to install and manage multiple versions of Node.js
For many users, using apt with the default repo will be sufficient.
If you need specific newer (or legacy) versions of Node, you should use the PPA repository.
If you are actively developing Node applications and need to switch between node versions frequently, choose the nvm method.
This guide assumes that you are using Ubuntu 20.04.
Before you begin, you should have a non-root user account with sudo privileges set up on your system.
You can learn how to do this by following the initial server setup tutorial for Ubuntu 20.04.
Option 1 - Installing Node.js with Apt from the Default Repositories
Ubuntu 20.04 contains a version of Node.js in its default repositories that can be used to provide a consistent experience across multiple systems.
At the time of writing, the version in the repositories is 10.19.
This will not be the latest version, but it should be stable and sufficient for quick experimentation with the language.
To get this version, you can use the apt package manager.
Refresh your local package index first by typing:
Then install Node.js:
Check that the install was successful by querying node for its version number:
If the package in the repositories suits your needs, this is all you need to do to get set up with Node.js.
In most cases, you'll also want to also install npm, the Node.js package manager.
You can do this by installing the npm package with apt:
This will allow you to install modules and packages to use with Node.js.
At this point you have successfully installed Node.js and npm using apt and the default Ubuntu software repositories.
The next section will show how to use an alternate repository to install different versions of Node.js.
Option 2 - Installing Node.js with Apt Using a NodeSource PPA
To install a different version of Node.js, you can use a PPA (personal package archive) maintained by NodeSource.
These PPAs have more versions of Node.js available than the official Ubuntu repositories.
Node.js v10, v12, v13, and v14 are available as of the time of writing.
First, we will install the PPA in order to get access to its packages.
From your home directory, use curl to retrieve the installation script for your preferred version, making sure to replace < ^ > 14.x < ^ > with your preferred version string (if different).
Refer to the NodeSource documentation for more information on the available versions.
Inspect the contents of the downloaded script with nano (or your preferred text editor):
When you are satisfied that the script is safe to run, exit your editor, then run the script with sudo:
The PPA will be added to your configuration and your local package cache will be updated automatically.
You can now install the Node.js package in the same way you did in the previous section:
Verify that you've installed the new version by running node with the -v version flag:
The NodeSource nodejs package contains both the node binary and npm, so you don't need to install npm separately.
At this point you have successfully installed Node.js and npm using apt and the NodeSource PPA.
The next section will show how to use the Node Version Manager to install and manage multiple versions of Node.js.
Option 3 - Installing Node Using the Node Version Manager
Another way of installing Node.js that is particularly flexible is to use nvm, the Node Version Manager.
This piece of software allows you to install and maintain many different independent versions of Node.js, and their associated Node packages, at the same time.
To install NVM on your Ubuntu 20.04 machine, visit the project's GitHub page.
Copy the curl command from the README file that displays on the main page.
This will get you the most recent version of the installation script.
Before piping the command through to bash, it is always a good idea to audit the script to make sure it isn't doing anything you don't agree with.
You can do that by removing the | bash segment at the end of the curl command:
Take a look and make sure you are comfortable with the changes it is making.
When you are satisfied, run the command again with | bash appended at the end.
The URL you use will change depending on the latest version of nvm, but as of right now, the script can be downloaded and executed by typing:
This will install the nvm script to your user account.
To use it, you must first source your .bashrc file:
Now, you can ask NVM which versions of Node are available:
It's a very long list!
You can install a version of Node by typing any of the release versions you see.
For instance, to get version v13.6.0, you can type:
You can see the different versions you have installed by typing:
This shows the currently active version on the first line (- > v13.6.0), followed by some named aliases and the versions that those aliases point to.
< $> note Note: if you also have a version of Node.js installed through apt, you may see a system entry here.
You can always activate the system-installed version of Node using nvm use system.
Additionally, you'll see aliases for the various long-term support (or LTS) releases of Node:
We can install a release based on these aliases as well.
For instance, to install the latest long-term support version, erbium, run the following:
You can switch between installed versions with nvm use:
You can verify that the install was successful using the same technique from the other sections, by typing:
The correct version of Node is installed on our machine as we expected.
A compatible version of npm is also available.
There are a quite a few ways to get up and running with Node.js on your Ubuntu 20.04 server.
Your circumstances will dictate which of the above methods is best for your needs.
While using the packaged version in Ubuntu's repository is the easiest method, using nvm or a NodeSource PPA offers additional flexibility.
For more information on programming with Node.js, please refer to our tutorial series How To Code in Node.js.
How To Upgrade to Ubuntu 20.04 Focal Fossa
5569
The Ubuntu operating system's latest Long Term Support (LTS) release, Ubuntu 20.04 (Focal Fossa), was released on April 23, 2020.
This guide will explain how to upgrade an Ubuntu system of version 18.04 or later to Ubuntu 20.04.
< $> warning Warning: As with almost any upgrade between major releases of an operating system, this process carries an inherent risk of failure, data loss, or broken software configuration.
Comprehensive backups and extensive testing are strongly advised.
To avoid these problems, we recommend migrating to a fresh Ubuntu 20.04 server rather than upgrading in-place.
You may still need to review differences in software configuration when upgrading, but the core system will likely have greater stability.
You can follow our series on how to migrate to a new Linux server to learn how to migrate between servers.
This guide assumes that you have an Ubuntu 18.04 or later system configured with a sudo-enabled non-root user.
Potential Pitfalls
Although many systems can be upgraded in place without incident, it is often safer and more predictable to migrate to a major new release by installing the distribution from scratch, configuring services with careful testing along the way, and migrating application or user data as a separate step.
You should never upgrade a production system without first testing all of your deployed software and services against the upgrade in a staging environment.
Keep in mind that libraries, languages, and system services may have changed substantially.
Before upgrading, consider reading the Focal Fossa Release Notes.
Step 1 - Backing Up Your System
Before attempting a major upgrade on any system, you should make sure you won't lose data if the upgrade goes awry.
The best way to accomplish this is to make a backup of your entire filesystem.
Failing that, ensure that you have copies of user home directories, any custom configuration files, and data stored by services such as relational databases.
On a DigitalOcean Droplet, one approach is to power down the system and take a snapshot (powering down ensures that the filesystem will be more consistent).
See How to Create Snapshots of Droplets for more details on the snapshot process.
After you have verified that the Ubuntu update was successful, you can delete the snapshot so that you will no longer be charged for its storage.
For backup methods which will work on most Ubuntu systems, see How To Choose an Effective Backup Strategy for your VPS.
Step 2 - Updating Currently Installed Packages
Before beginning the release upgrade, it's safest to update to the latest versions of all packages for the current release.
Begin by updating the package list:
Next, upgrade installed packages to their latest available versions:
You will be shown a list of upgrades, and prompted to continue.
Answer y for yes and press Enter.
This process may take some time.
Once it finishes, use the dist-upgrade command with apt-get, which will perform any additional upgrades that involve changing dependencies, adding or removing new packages as necessary.
This will handle a set of upgrades which may have been held back by the previous apt upgrade step:
Again, answer y when prompted to continue, and wait for upgrades to finish.
Now that you have an up-to-date installation of Ubuntu, you can use do-release-upgrade to upgrade to the 20.04 release.
Step 3 - Upgrading with Ubuntu's do-release-upgrade Tool
Traditionally, Ubuntu releases have been upgradeable by changing Apt's / etc / apt / sources.list - which specifies package repositories - and using apt-get dist-upgrade to perform the upgrade itself.
Though this process is still likely to work, Ubuntu provides a tool called do-release-upgrade to make the upgrade safer and easier.
do-release-upgrade handles checking for a new release, updating sources.list, and a range of other tasks, and is the officially recommended upgrade path for server upgrades which must be performed over a remote connection.
Start by running do-release-upgrade with no options:
If the new Ubuntu version has not been officially released yet, you may get the following output:
Note that on Ubuntu Server, the new LTS release isn't made available to do-release-upgrade until its first point release, in this case 20.04.1.
This usually comes a few months after the initial release date.
If you don't see an available release, add the -d option to upgrade to the development release:
If you're connected to your system over SSH, you'll be asked whether you wish to continue.
For virtual machines or managed servers you should keep in mind that losing SSH connectivity is a risk, particularly if you don't have another means of remotely connecting to the system's console (such as a web-based console feature, for example).
For other systems under your control, remember that it's safest to perform major operating system upgrades only when you have direct physical access to the machine.
At the prompt, type y and press Enter to continue:
Next, you'll be informed that do-release-upgrade is starting a new instance of sshd on port 1022:
Press Enter.
Next, you may be warned that a mirror entry was not found.
On DigitalOcean systems, it is safe to ignore this warning and proceed with the upgrade, since a local mirror for 20.04 is in fact available.
Enter y:
Once the new package lists have been downloaded and changes calculated, you'll be asked if you want to start the upgrade.
Again, enter y to continue:
New packages will now be retrieved, unpacked, and installed.
Even if your system is on a fast connection, this will take a while.
During the installation, you may be presented with interactive dialogs for various questions.
For example, you may be asked if you want to automatically restart services when required:
Service Restart Dialog
In this case, it is safe to answer Yes.
In other cases, you may be asked if you wish to replace a configuration file that you have modified.
This is often a judgment call, and is likely to require knowledge about specific software that is outside the scope of this tutorial.
Once new packages have finished installing, you'll be asked whether you're ready to remove obsolete packages.
On a stock system with no custom configuration, it should be safe to enter y here.
On a system you have modified heavily, you may wish to enter d and inspect the list of packages to be removed, in case it includes anything you'll need to reinstall later.
Finally, assuming all has gone well, you'll be informed that the upgrade is complete and a restart is required.
Enter y to continue:
On an SSH session, you'll likely see something like the following:
You may need to press a key here to exit to your local prompt, since your SSH session will have terminated on the server end.
Wait a moment for your server to reboot, then reconnect.
On login, you should be greeted by a message confirming that you're now on Focal Fossa:
You should now have a working Ubuntu 20.04 installation.
From here, you likely need to investigate necessary configuration changes to services and deployed applications.
You can find more 20.04 tutorials and questions on our Ubuntu 20.04 Tutorials tag page.
How To Code in Go eBook
5839
How To Code in Go eBook in EPUB format
How To Code in Go eBook in PDF format < $>
This book is designed to introduce you to writing programs with the Go programming language.
You'll learn how to write useful tools and applications that can run on remote servers, or local Windows, macOS, and Linux systems for development.
This book is based on the How To Code in Go tutorial series found on DigitalOcean Community.
Install and set up a local Go development environment on Windows, macOS, and Linux systems
Design your programs with conditional logic, including switch statements to control program flow
Define your own data structures and create interfaces to them for reusable code
Write custom error handling functions
Building and installing your Go programs so that they can run on different operating systems and different CPU architectures
Using flags to pass arguments to your programs, to override default options
Each chapter can be read on its own or used as a reference, or you can follow the chapters from beginning to end.
Feel free to jump to the chapter or chapters that best suits your purpose as you are learning Go with this book.
After you're finished this book, if you'd like to learn more about how to build tools and applications with Go, visit the DigitalOcean Community's Go section (https: / / www.digitalocean.com / community / tags / go).
How To Deploy Laravel 7 and MySQL on Kubernetes using Helm
6029
The author selected the Diversity in Tech Fund to receive a donation as part of the Write for DOnations program.
Laravel is one of the most popular open-source PHP application frameworks today.
It is commonly deployed with a MySQL database but can be configured to use a variety of backend data storage options.
Laravel prides itself on taking advantage of many of PHP's modern features and extensive package ecosystem.
Kubernetes is a container orchestration platform that can be hosted on DigitalOcean Kubernetes clusters to take much of the administration work out of setting up and running containers in production.
Helm is a Kubernetes package manager that makes configuring and installing services and pods on Kubernetes easier.
In this guide, you will create a Laravel PHP application, build your app into a Docker image, and deploy that image to a DigitalOcean Kubernetes cluster using the LAMP Helm chart.
Next, you'll set up an Ingress Controller to add SSL and a custom domain name to your app. When completed, you will have a working Laravel application connected to a MySQL database that is running on a Kubernetes cluster.
Docker installed on the machine that you "ll access your cluster from.
You can find detailed instructions on installing Docker for most Linux distributions here or on Docker "s website for other operating systems.
An account at Docker Hub for storing Docker images you "ll create during this tutorial.
A DigitalOcean Kubernetes 1.17 + cluster with your connection configuration set as the kubectl default.
To learn how to create a Kubernetes cluster on DigitalOcean, see Kubernetes Quickstart.
To learn how to connect to the cluster, see How to Connect to a DigitalOcean Kubernetes Cluster.
The Helm 3 package manager installed on your local machine.
Complete the first step and add the stable repo from the second step of the How To Install Software on Kubernetes Clusters with the Helm 3 Package Manager tutorial.
A fully registered domain name with an available A record.
Don't worry about associating your domain's A record with an IP at this time.
Once you reach Step 5 and your Ingress controller is in place, you will connect < ^ > your _ domain < ^ > to the proper IP.
Step 1 & mdash; Creating a New Laravel Application
In this step, you'll use Docker to create a new Laravel 7 application, but you should be able to go through the same process with an existing Laravel application that uses MySQL as the backing database.
The new application you build will verify that Laravel is connected to the database and display the name of the database.
First, move to your home directory and then create a new Laravel application using a composer Docker container:
Once the container is done and all the Composer packages have been installed, you should see a fresh installation of Laravel in your current directory called laravel-kubernetes /.
Navigate to that folder:
You'll execute the rest of this tutorial's commands from here.
The purpose of this application is to test your database connection and display its name in your browser.
In order to test the database connection, open up the. / resources / views / welcome.blade.php file in a text editor:
Find the section < div class = "links" >... < / div > and replace its contents with the following:
That's all the customization you'll need to make to the default Laravel app for this tutorial.
Once completed, this brief snippet of PHP will test your database connection and display the database's name on the Laravel splash screen in your web browser.
In the next step, you'll use Docker to build an image containing this Laravel application and Docker Compose to test that it runs locally and connects to a MySQL database.
Step 2 & mdash; Containerizing Your Laravel Application
Now that you have created a new Laravel application, you'll need to build your code into a Docker image and then test the image with Docker Compose.
While the goal of this tutorial is to deploy your application to a Kubernetes cluster, Docker Compose is a convenient way to test your Docker image and configuration locally before running it in the cloud.
This fast feedback loop can be useful for making and testing small changes.
First, using nano or your preferred text editor, create a file in the root of your Laravel application called Dockerfile:
Docker will use this file to build your code into an image:
This Dockerfile starts with the PHP 7.4 Apache Docker Image found on Docker Hub, then installs several Linux packages that are commonly required by Laravel applications.
Next, it creates Apache configuration files and enables header rewrites.
The Dockerfile installs several common PHP extensions and adds an environment variable to ensure that Laravel's logs are streamed to the container via stderr.
This will allow you to see Laravel logs by tailing your Docker Compose or Kubernetes logs.
Finally, the Dockerfile copies all the code in your Laravel application to / var / www / tmp and installs the Composer dependencies.
It then sets an ENTRYPOINT, but you'll need to create that file, which we will do next.
At the root directory of your project, create a new file called docker-entrypoint.sh.
This file will run when your container is run locally or in the Kubernetes cluster, and it will copy your Laravel application code from the / var / www / tmp directory to / var / www / html where Apache will be able to serve it.
Now add the following script:
The final line, exec "$@" instructs the shell to run whatever command was passed in as an input argument next.
This is important because you want Docker to continue running the Apache run command (apache2-foreground) after this script executes.
Next, create a .dockerignore file in your app's root directory.
This file will ensure that when you build your Docker image it won't become polluted with packages or environment files that shouldn't be copied into it:
The last file that you need to create before you can run your app locally using Docker Compose is a docker-compose.yml file.
But during the configuration of this YAML file, you will need to enter the APP _ KEY that Laravel generated during installation.
Find this by opening and searching the.
/ .env file, or by running the following cat and grep commands:
You will see an output like this:
Copy your key to your clipboard.
Be sure to include the base64: prefix.
Now create the docker-compose.yml file in your app's root directory:
Here we will include your Laravel application's PHP image as well as a MySQL container to run your database.
Add the following content:
Use the APP _ KEY variable that you copied to your clipboard for the < ^ > your _ laravel _ app _ key < ^ > variable, and use your Docker Hub username for the < ^ > your _ docker _ hub _ username < ^ > variable.
You'll create the first image locally using docker build.
The second image is the official MySQL Docker image available on Docker Hub.
Both require several environment variables, which you'll include when you run the containers.
In order to build the Docker image containing your Laravel application, run the following command.
Make sure to replace < ^ > your _ docker _ hub _ username < ^ > with your username or your team's username at Docker Hub where this image will be stored:
Next, you can run the two containers with Docker Compose with the required database credentials:
The four environment variables used here (DB _ ROOT _ PASSWORD, DB _ DATABASE, DB _ USERNAME, DB _ PASSWORD) can be modified if you'd like, but since you are only testing your application locally, you don't have to worry about securing them yet.
It may take up to 30 seconds for your MySQL database to initialize and your containers to be ready.
Once they are, you can view your Laravel application on your machine at localhost: 8000.
The Laravel application running locally using Docker Compose
Your PHP application will connect to your MySQL database.
After a successful connection, the text "Database Connected: local _ db" will appear beneath the Laravel logo.
Now that you've tested your Docker image locally using Docker Compose, you can bring the containers down by running docker-compose down:
In the next section, you'll push your Docker image to Docker Hub so that your Helm chart can use it to deploy your application to your Kubernetes cluster.
Step 3 & mdash; Pushing Your Docker Image to Docker Hub
The LAMP Helm Chart that you'll use to deploy your code to Kubernetes requires that your code be available in a container registry.
While you can push your image to a private or self-hosted registry, for the purposes of this tutorial you'll use a publicly available and free Docker registry on Docker Hub.
Access your account on Docker Hub using your web browser and then create a new repository called laravel-kubernetes.
Creating a new repository on Docker Hub
Next, if you haven't connected to Docker Hub from your local machine, you'll need to log into Docker Hub.
You can do this through the command line:
Enter your login credentials when prompted.
This typically only needs to be done once per machine as Docker will save your credentials to the ~ / .docker / config.json in your home directory.
Finally, push your image to Docker Hub:
It may take a few minutes to upload your app depending on your connecti on speed, but once Docker is done, you'll see a final digest hash and the size of your image in the terminal.
Now that your Laravel application is containerized and you've pushed an image to Docker Hub, you can use the image in a Helm Chart or Kubernetes deployment.
In the next step, you'll set custom values based on the LAMP Helm Chart and deploy it to your DigitalOcean Kubernetes cluster.
Step 4 & mdash; Configuring and Deploying the Application with the LAMP Helm Chart
Helm provides a number of Charts to help you set up Kubernetes applications using preset combinations of tools.
While you could write your own Kubernetes service files to accomplish a similar deployment, you'll see in this section that using a Helm Chart will require much less configuration.
First, you'll need a directory to store all your Helm configuration files.
Create a new directory in the root of your Laravel project called helm /:
Inside the helm / directory, you will create two new files: values.yml and secrets.yml.
First create and open values.yml:
The values.yml file will include non-secret configuration options that will override the default values in the LAMP Helm chart.
Add the following configurations, making sure to replace < ^ > your _ docker _ hub _ username < ^ > with your own username:
Now create a secrets.yml file:
secrets.yml will not be checked into version control.
It will contain sensitive configuration information like your database password and Laravel app key.
Add the following configurations, adjusting as needed to fit your credentials:
Be sure to use strong username and password combinations for your production database, and use the same < ^ > your _ laravel _ app _ key < ^ > as above, or open a new terminal window and generate a new one by running the following command.
You can then copy the new value Laravel sets in your .env file:
Save and close secrets.yml.
Next, in order to prevent your secrets.yml file from being built into the Docker image or saved to version control, make sure to add the following line to both your .dockerignore and .gitignore files.
Open and append / helm / secrets.yml to each file, or run the following command to add both:
Now that you've created Helm configuration files for your application and the Docker image, you can install this Helm chart as a new release on your Kubernetes cluster.
Install your chart from your application's root directory:
Your application will take a minute or two to become available, but you can run this command to monitor the Kubernetes services in your cluster:
Look for the name of your application:
When your new laravel-kubernetes-lamp service displays an IP address under EXTERNAL-IP, you can visit < ^ > your _ external _ ip < ^ > to see the application running on your Kubernetes cluster.
Your app will connect to your database and you will see the name of the database below the Laravel logo, like you did when running your app locally on Docker Compose.
The Laravel application running on Kubernetes using the LAMP Helm chart
Running a web application on an unsecured IP address might be okay for a proof of concept, but your website isn't production-ready without an SSL certificate and a custom domain name.
Before you set that up in the next step, uninstall your release via the command line:
In the next step you'll expand on this first Helm configuration to add an Ingress controller, SSL certificate, and custom domain to your Laravel application.
Step 5 & mdash; Adding an Ingress Controller and SSL to Your Kubernetes Cluster
In Kubernetes, an Ingress Controller is responsible for exposing your application's services to the internet.
In the previous step, the LAMP Helm chart created a DigitalOcean Load Balancer and exposed your application directly via the load balancer's IP address.
You could terminate SSL and your domain name directly on the load balancer, but because you're working in Kubernetes, it might be more convenient to manage it all in the same place.
For much more about Ingress Controllers and details about the following steps, read How To Set Up an Nginx Ingress on DigitalOcean Kubernetes Using Helm.
The LAMP Helm chart includes a configuration option for supporting Ingress.
Open up your helm / values.yml file:
Now add the following lines:
This instructs your deployment not to install a load balancer and instead to expose the application to the Kubernetes cluster's port 80 where the Ingress Controller will expose it to the internet.
Save and close values.yml.
Now run the helm install command you ran previously to get your Laravel application running again.
Make sure to run the command from your app's root directory:
Next, install the nginx-ingress controller on your Kubernetes cluster using the Kubernetes-maintained Nginx Ingress Controller:
After installation, you will see an output like this:
You also need an Ingress Resource to expose your Laravel app's deployment.
Create a new file in your app's root directory called ingress.yml:
This file defines your application's host, SSL certificate manager, and backend service and port name.
Add the following configurations, replaceing < ^ > your _ domain < ^ > with the domain of your choice:
Next, you should install Cert-Manager and create an issuer that will allow you to create production SSL certificates using Let's Encrypt.
Cert-Manager requires Custom Resource Definitions that you can apply from the Cert-Manager repository using the command line:
This will create a number of Kubernetes resources that will be displayed in the command line:
Cert-Manager also requires a namespace to isolate it in your Kubernetes cluster:
You will see this output:
Because Jetstack's Cert-Manager is not one of the Kubernetes-maintained charts, you will need to add the Jetstack Helm repository as well.
Run the following command to make it available in Helm:
A successful addition will output the following:
Now you're ready to install Cert-Manager into the cert-manager namespace on your Kubernetes cluster:
When complete, you'll see a summary of the deployment like this:
The last file you'll need to add to your Laravel application's root directory is a production _ issuer.yml Kubernetes configuration file.
Create the file:
Now add the following:
Let's Encrypt will send < ^ > your _ email _ address < ^ > any important notices and expiration warnings, so be sure to add an address that you'll check regularly.
Save this file and create a new resource for both your Ingress resource and production issuer in your Kubernetes cluster:
Finally, update your domain name's DNS records to point an A record to your load balancer's IP address.
To find the IP address for your Ingress Controller enter:
Use the < ^ > your _ external _ ip < ^ > address as the IP address for your DNS A Record.
The process for updating your DNS records varies depending on where you manage your domain names and DNS hosting, but if you're using DigitalOcean you can reference our guide on How to Manage DNS Records.
Once your DNS records update and your SSL certificate is generated, your application will be available at < ^ > your _ domain < ^ > and SSL will be enabled.
The Laravel application with SSL termination and a custom domain name
While your PHP application and database are already connected, you will still need to run database migrations.
In the last step, you'll see how to run Artisan commands on your Kubernetes pod to perform database migrations and other common maintenance tasks.
Step 6 & mdash; Running Remote Commands
While your Laravel application is running and connected to the MySQL database in Kubernetes, there are several common operations that you should run on a new Laravel installation.
One common task that you should perform is database migrations.
Before you can run an Artisan command on your Laravel application, you need to know the name of the pod that is running your Laravel application container.
Using the command line, you can view all the pods in your Kubernetes cluster:
Select the pod for your laravel-kubernetes-lamp-... deployment.
Make sure to use the name in your output and not the one listed above.
Now you can run kubectl exec on it. For example, run a database migration using the artisan migrate command.
You will add the --force flag because you're running the pod in production:
This command will produce an output:
You have now successfully deployed Laravel 7 and MySQL to Kubernetes and performed an essential database maintenance task.
In this tutorial, you learned how to containerize a Laravel PHP application, connect it to a MySQL database, push a Docker image containing your code to Docker Hub, and then use a Helm chart to deploy that image to a DigitalOcean Kubernetes cluster.
Finally, you added SSL and a custom domain name and learned how to run command line tools on your running pods.
Kubernetes and Helm offer you a number of advantages over traditional LAMP stack hosting: scalability, the ability to swap out services without logging into your server directly, tools to perform rolling upgrades, and control over your hosting environment.
That said, the complexity of initially containerizing and configuring your application makes the barrier to getting started quite high.
With this guide as a starting point, deploying Laravel to Kubernetes becomes more attainable.
From here you might consider learning more about the power of Laravel or adding monitoring tools to Kubernetes like Linkerd, which you can install manually with our guide or with a DigitalOcean 1-Click.
How To Build a Slackbot in Python on Ubuntu 20.04
6041
Slack is a communication platform designed for workplace productivity.
It includes features such as direct messaging, public and private channels, voice and video calls, and bot integrations.
A Slackbot is an automated program that can perform a variety of functions in Slack, from sending messages to triggering tasks to alerting on certain events.
In this tutorial you will build a Slackbot in the Python programming language.
Python is a popular language that prides itself on simplicity and readability.
Slack provides a rich Python Slack API for integrating with Slack to perform common tasks such as sending messages, adding emojis to messages, and much more.
Slack also provides a Python Slack Events API for integrating with events in Slack, allowing you to perform actions on events such as messages and mentions.
As a fun proof-of-concept that will demonstrate the power of Python and its Slack APIs, you will build a CoinBot & mdash; a Slackbot that monitors a channel and, when triggered, will flip a coin for you.
You can then modify your CoinBot to fulfill any number of slightly more practical applications.
Note that this tutorial uses Python 3 and is not compatible with Python 2.
In order to follow this guide, you'll need:
A Slack Workspace that you have the ability to install applications into.
If you created the workspace you have this ability.
If you don't already have one, you can create one on the Slack website.
(Optional) A server or computer with a public IP address for development.
We recommend a fresh installation of Ubuntu 20.04, a non-root user with sudo privileges, and SSH enabled.
< $> note You may want to test this tutorial on a server that has a public IP address.
Slack will need to be able to send events such as messages to your bot. If you are testing on a local machine you will need to port forward traffic through your firewall to your local system.
If you are looking for a way to develop on a cloud server, check out this tutorial on How To Use Visual Studio Code for Remote Development via the Remote-SSH Plugin.
Step 1 & mdash; Creating the Slackbot in the Slack UI
First create your Slack app in the Slack API Control Panel.
Log in to your workspace in Slack via a web browser and navigate to the API Control Panel.
Now click on the Create an App button.
Create Your Slack App
Next you'll be prompted for the name of your app and to select a development Slack workspace.
For this tutorial, name your app < ^ > CoinBot < ^ > and select a workspace you have admin access to.
Once you have done this click on the Create App button.
Name Your Slack App and Select a Workspace
Once your app is created you'll be presented with the following default app dashboard.
This dashboard is where you manage your app by setting permissions, subscribing to events, installing the app into workspaces, and more.
Default Slack App Panel
In order for your app to be able to post messages to a channel you need to grant the app permissions to send messages.
To do this, click the Permissions button in the control panel.
Select the Permissions Button in the Control Panel
When you arrive at the OAuth & Permissions page, scroll down until you find the Scopes section of the page.
Then find the Bot Token Scopes subsection in the scope and click on Add an OAuth Scope button.
Select the Add an OAuth Scope Button
Click on that button and then type chat: write.
Select that permission to add it to your bot. This will allow the app to post messages to channels that it can access.
For more information on the available permissions refer to Slack's Documentation.
Add the chat: write Permission
Now that you've added the appropriate permission it is time to install your app into your Slack workspace.
Scroll back up on the OAuth & Permissions page and click the Install App to Workspace button at the top.
Install App to Workspace
Click this button and review the actions that the app can perform in the channel.
Once you are satisfied, click the Allow button to finish the installation.
Once the bot is installed you'll be presented with a Bot User OAuth Access Token for your app to use when attempting to perform actions in the workspace.
Go ahead and copy this token; you'll need it later.
Save the Access Token
Finally, add your newly installed bot into a channel within your workspace.
If you haven't created a channel yet you can use the # general channel that is created by default in your Slack workspace.
Locate the app in the Apps section of the navigation bar in your Slack client and click on it. Once you've done that open the Details menu in the top right hand side.
If your Slack client isn't full-screened it will look like an i in a circle.
Click on the App Details Icon
To finish adding your app to a channel, click on the More button represented by three dots in the details page and select Add this app to a channel....
Type your channel into the modal that appears and click Add.
Add App to a Channel
You've now successfully created your app and added it to a channel within your Slack workspace.
After you write the code for your app it will be able to post messages in that channel.
In the next section you'll start writing the Python code that will power CoinBot.
Step 2 & mdash; Setting Up Your Python Developer Environment
First let's set up your Python environment so you can develop the Slackbot.
Open a terminal and install python3 and the relevant tools onto your system:
Next you will create a virtual environment to isolate your Python packages from the system installation of Python.
To do this, first create a directory into which you will create your virtual environment.
Make a new directory at ~ / .venvs:
Now create your Python virtual environment:
Next, activate your virtual environment so you can use its Python installation and install packages:
Your shell prompt will now show the virtual environment in parenthesis.
Now use pip to install the necessary Python packages into your virtual environment:
slackclient and slackeventsapi facilitate Python's interaction with Slack's APIs.
Flask is a popular micro web framework that you will use to deploy your app:
Now that you have your developer environment set up, you can start writing your Python Slackbot:
Step 3 & mdash; Creating the Slackbot Message Class in Python
Messages in Slack are sent via a specifically formatted JSON payload.
This is an example of the JSON that your Slackbot will craft and send as a message:
You could manually craft this JSON and send it, but instead let's build a Python class that not only crafts this payload, but also simulates a coin flip.
First use the touch command to create a file named coinbot.py:
Next, open this file with nano or your favorite text editor:
Now add the following lines of code to import the relevant libraries for your app. The only library you need for this class is the random library from the Python Standard Library.
This library will allow us to simulate a coin flip.
Add the following lines to coinbot.py to import all of the necessary libraries:
Next, create your CoinBot class and an instance of this class to craft the message payload.
Add the following lines to coinbot.py to create the CoinBot class:
Now indent by one and create the constants, constructors, and methods necessary for your class.
First let's create the constant that will hold the base of your message payload.
This section specifies that this constant is of the section type and that the text is formatted via markdown.
It also specifies what text you wish to display.
You can read more about the different payload options in the official Slack message payload documentation.
Append the following lines to coinbot.py to create the base template for the payload:
Next create a constructor for your class so that you can create a separate instance of your bot for every request.
Don't worry about memory overhead here; the Python garbage collector will clean up these instances once they are no longer needed.
This code sets the recipient channel based on a parameter passed to the constructor.
Append the following lines to coinbot.py to create the constructor:
Now write the code that simulates to flip a coin.
We'll randomly generate a one or zero, representing heads or tails respectively.
Append the following lines to coinbot.py to simulate the coin flip and return the crafted payload:
Finally, create a method that crafts and returns the entire message payload, including the data from your constructor, by calling your _ flip _ coin method.
Append the following lines to coinbot.py to create the method that will generate the finished payload:
You are now finished with the CoinBot class and it is ready for testing.
Before continuing, verify that your finished file, coinbot.py, contains the following:
Now that you have a Python class ready to do the work for your Slackbot, let's ensure that this class produces a useful message payload and that you can send it to your workspace.
Step 4 & mdash; Testing Your Message
Now let's test that this class produces a proper payload.
Create a file named coinbot _ test.py:
Now add the following code.
Be sure to change the channel name in the instantiation of the coinbot class coin _ bot = coinbot (" # < ^ > YOUR _ CHANNEL _ HERE < ^ > ").
This code will create a Slack client in Python that will send a message to the channel you specify that you have already installed the app into:
Before you can run this file you will need to export the Slack token that you saved in Step 1 as an environment variable:
Now test this file and verify that the payload is produced and sent by running the following script in your terminal.
Make sure that your virtual environment is activated.
You can verify this by seeing the (slackbot) text at the front of your bash prompt.
Run this command you will receive a message from your Slackbot with the results of a coin flip:
Check the channel that you installed your app into and verify that your bot did indeed send the coin flip message.
Your result will be heads or tails.
Coin Flip Test
Now that you've verified that your Slackbot can flip a coin, create a message, and deliver the message, let's create a Flask to perpetually run this app and make it simulate a coin flip and share the results whenever it sees certain text in messages sent in the channel.
Step 5 & mdash; Creating a Flask Application to Run Your Slackbot
Now that you have a functioning application that can send messages to your Slack workspace, you need to create a long running process so your bot can listen to messages sent in the channel and reply to them if the text meets certain criteria.
You're going to use the Python web framework Flask to run this process and listen for events in your channel.
< $> note In this section you will be running your Flask application from a server with a public IP address so that the Slack API can send you events.
If you are running this locally on your personal workstation you will need to forward the port from your personal firewall to the port that will be running on your workstation.
These ports can be the same, and this tutorial will be set up to use port 3000.
First adjust your firewall settings to allow traffic through port 3000:
Now check the status of ufw:
Now create the file for your Flask app. Name this file app.py:
Next, open this file in your favorite text editor:
Now add the following import statements.
You'll import the following libraries for the following reasons:
import os - To access environment variables
import logging - To log the events of the app
from flask import Flask - To create a Flask app
from slack import WebClient - To send messages via Slack
from slackeventsapi import SlackEventAdapter - To receive events from Slack and process them
from coinbot import CoinBot - To create an instance of your CoinBot and generate the message payload.
Append the following lines to app.py to import all of the necessary libraries:
Now create your Flask app and register a Slack Event Adapter to your Slack app at the / slack / events endpoint.
This will create a route in your Slack app where Slack events will be sent and ingested.
To do this you will need to get another token from your Slack app, which you will do later in the tutorial.
Once you get this variable you will export it as an environment variable named SLACK _ EVENTS _ TOKEN.
Go ahead and write your code to read it in when creating the SlackEventAdapter, even though you haven't set the token yet.
Append the following lines to app.py to create the Flask app and register the events adapter into this app:
Next create a web client object that will allow your app to perform actions in the workspace, specifically to send messages.
This is similar to what you did when you tested your coinbot.py file previously.
Append the following line to app.py to create this slack _ web _ client:
Now create a function that can be called that will create an instance of CoinBot, and then use this instance to create a message payload and pass the message payload to the Slack web client for delivery.
Append the following lines to app.py to create this function:
Now that you have created a function to handle the messaging aspects of your app, create one that monitors Slack events for a certain action and then executes your bot. You're going to configure your app to respond with the results of a simulated coin flip when it sees the phrase "Hey Sammy, Flip a coin".
You're going to accept any version of this & mdash; case won't prevent the app from responding.
First decorate your function with the @ slack _ events _ adapter.on syntax that allows your function to receive events.
Specify that you only want the message events and have your function accept a payload parameter containing all of the necessary Slack information.
Once you have this payload you will parse out the text and analyze it. Then, if it receives the activation phrase, your app will send the results of a simulated coin flip.
Append the following code to app.py to receive, analyze, and act on incoming messages:
Finally, create a main section that will create a logger so you can see the internals of your application as well as launch the app on your external IP address on port 3000.
In order to ingest the events from Slack, such as when a new message is sent, you must test your application on a public-facing IP address.
Append the following lines to app.py to set up your main section:
You are now finished with the Flask app and it is ready for testing.
Before you move on verify that your finished file, app.py contains the following:
Now that your Flask app is ready to serve your application let's test it out.
Step 6 & mdash; Running Your Flask App
Finally, bring everything together and execute your app.
First, add your running application as an authorized handler for your Slackbot.
Navigate to the Basic Information section of your app in the Slack UI.
Scroll down until you find the App Credentials section.
Slack Signing Secret
Copy the Signing Secret and export it as the environment variable SLACK _ EVENTS _ TOKEN:
With this you have all the necessary API tokens to run your app. Refer to Step 1 if you need a refresher on how to export your SLACK _ TOKEN.
Now you can start your app and verify that it is indeed running.
Ensure that your virtual environment is activated and run the following command to start your Flask app:
To verify that your app is up, open a new terminal window and curl the IP address of your server with the correct port at / slack / events:
curl will return the following:
Receiving the message These are not the slackbots you're looking for., indicates that your app is up and running.
Now leave this Flask application running while you finish configuring your app in the Slack UI.
First grant your app the appropriate permissions so that it can listen to messages and respond accordingly.
Click on Event Subscriptions in the UI sidebar and toggle the Enable Events radio button.
Enable Events Button
Once you've done that, type in your IP address, port, and / slack / events endpoint into the Request URL field.
Don't forget the HTTP protocol prefix.
Slack will make an attempt to connect to your endpoint.
Once it has successfully done so you'll see a green check mark with the word Verified next to it.
Event Subscriptions Request URL
Next, expand the Subscribe to bot events and add the message.channels permission to your app. This will allow your app to receive messages from your channel and process them.
Subscribe to bot events permissions
Once you've done this you will see the event listed in your Subscribe to bot events section.
Next click the green Save Changes button in the bottom right hand corner.
Confirm and Save changes
Once you do this you'll see a yellow banner across the top of the screen informing you that you need to reinstall your app for the following changes to apply.
Every time you change permissions you'll need to reinstall your app. Click on the reinstall your app link in this banner to reinstall your app.
Reinstall your app banner
You'll be presented with a confirmation screen summarizing the permissions your bot will have and asking if you want to allow its installation.
Click on the green Allow button to finish the installation process.
Reinstall confirmation
Now that you've done this your app should be ready.
Go back to the channel that you installed CoinBot into and send a message containing the phrase Hey Sammy, Flip a coin in it. Your bot will flip a coin and reply with the results.
Congrats!
You've created a Slackbot!
Hey Sammy, Flip a coin
Once you are done developing your application and you are ready to move it to production, you'll need to deploy it to a server.
This is necessary because the Flask development server is not a secure production environment.
You'll be better served if you deploy your app using a WSGI and maybe even securing a domain name and giving your server a DNS record.
There are many options for deploying Flask applications, some of which are listed below:
Deploy your Flask application to Ubuntu 20.04 using Gunicorn and Nginx
Deploy your Flask application to Ubuntu 20.04 using uWSGI and Nginx
Deploy your Flask Application Using Docker on Ubuntu 18.04
There are many more ways to deploy your application than just these.
As always, when it comes to deployments and infrastucture, do what works best for you.
In any case, you now have a Slackbot that you can use to flip a coin to help you make decisions, like what to eat for lunch.
You can also take this base code and modify it to fit your needs, whether it be automated support, resource management, pictures of cats, or whatever you can think of.
You can view the complete Python Slack API docs here.
How To Set Up Django with Postgres, Nginx, and Gunicorn on Ubuntu 20.04
6043
Django is a powerful web framework that can help you get your Python application or website off the ground.
Django includes a simplified development server for testing your code locally, but for anything even slightly production related, a more secure and powerful web server is required.
In this guide, we will demonstrate how to install and configure some components on Ubuntu 20.04 to support and serve Django applications.
We will be setting up a PostgreSQL database instead of using the default SQLite database.
We will configure the Gunicorn application server to interface with our applications.
We will then set up Nginx to reverse proxy to Gunicorn, giving us access to its security and performance features to serve our apps.
Prerequisites and Goals
In order to complete this guide, you should have a fresh Ubuntu 20.04 server instance with a basic firewall and a non-root user with sudo privileges configured.
You can learn how to set this up by running through our initial server setup guide.
We will be installing Django within a virtual environment.
Installing Django into an environment specific to your project will allow your projects and their requirements to be handled separately.
Once we have our database and application up and running, we will install and configure the Gunicorn application server.
This will serve as an interface to our application, translating client requests from HTTP to Python calls that our application can process.
We will then set up Nginx in front of Gunicorn to take advantage of its high performance connection handling mechanisms and its easy-to-implement security features.
Let's get started.
Installing the Packages from the Ubuntu Repositories
To begin the process, we'll download and install all of the items we need from the Ubuntu repositories.
We will use the Python package manager pip to install additional components a bit later.
We need to update the local apt package index and then download and install the packages.
The packages we install depend on which version of Python your project will use.
If you are using Django with Python 3, type:
Django 1.11 is the last release of Django that will support Python 2. If you are starting new projects, it is strongly recommended that you choose Python 3. If you still need to use Python 2, type:
This will install pip, the Python development files needed to build Gunicorn later, the Postgres database system and the libraries needed to interact with it, and the Nginx web server.
Creating the PostgreSQL Database and User
We're going to jump right in and create a database and database user for our Django application.
By default, Postgres uses an authentication scheme called "peer authentication" for local connections.
Basically, this means that if the user's operating system username matches a valid Postgres username, that user can login with no further authentication.
During the Postgres installation, an operating system user named postgres was created to correspond to the postgres PostgreSQL administrative user.
We need to use this user to perform administrative tasks.
We can use sudo and pass in the username with the -u option.
Log into an interactive Postgres session by typing:
You will be given a PostgreSQL prompt where we can set up our requirements.
First, create a database for your project:
< $> note Note: Every Postgres statement must end with a semi-colon, so make sure that your command ends with one if you are experiencing issues.
Next, create a database user for our project.
Make sure to select a secure password:
Afterwards, we'll modify a few of the connection parameters for the user we just created.
This will speed up database operations so that the correct values do not have to be queried and set each time a connection is established.
We are setting the default encoding to UTF-8, which Django expects.
We are also setting the default transaction isolation scheme to "read committed", which blocks reads from uncommitted transactions.
Lastly, we are setting the timezone.
By default, our Django projects will be set to use UTC.
These are all recommendations from the Django project itself:
Now, we can give our new user access to administer our new database:
When you are finished, exit out of the PostgreSQL prompt by typing:
Postgres is now set up so that Django can connect to and manage its database information.
Creating a Python Virtual Environment for your Project
Now that we have our database, we can begin getting the rest of our project requirements ready.
We will be installing our Python requirements within a virtual environment for easier management.
To do this, we first need access to the virtualenv command.
We can install this with pip.
If you are using Python 3, upgrade pip and install the package by typing:
If you are using Python 2, upgrade pip and install the package by typing:
With virtualenv installed, we can start forming our project.
Create and move into a directory where we can keep our project files:
Within the project directory, create a Python virtual environment by typing:
This will create a directory called < ^ > myprojectenv < ^ > within your < ^ > myprojectdir < ^ > directory.
Inside, it will install a local version of Python and a local version of pip.
We can use this to install and configure an isolated Python environment for our project.
Before we install our project's Python requirements, we need to activate the virtual environment.
You can do that by typing:
Your prompt should change to indicate that you are now operating within a Python virtual environment.
It will look something like this: (< ^ > myprojectenv < ^ >) < ^ > user < ^ > @ < ^ > host < ^ >: ~ / < ^ > myprojectdir < ^ > $.
With your virtual environment active, install Django, Gunicorn, and the psycopg2 PostgreSQL adaptor with the local instance of pip:
< $> note Note: When the virtual environment is activated (when your prompt has (myprojectenv) preceding it), use pip instead of pip3, even if you are using Python 3. The virtual environment's copy of the tool is always named pip, regardless of the Python version.
You should now have all of the software needed to start a Django project.
Creating and Configuring a New Django Project
With our Python components installed, we can create the actual Django project files.
Creating the Django Project
Since we already have a project directory, we will tell Django to install the files here.
It will create a second level directory with the actual code, which is normal, and place a management script in this directory.
The key to this is that we are defining the directory explicitly instead of allowing Django to make decisions relative to our current directory:
At this point, your project directory (~ / < ^ > myprojectdir < ^ > in our case) should have the following content:
~ / myprojectdir / manage.py: A Django project management script.
~ / myprojectdir / myproject /: The Django project package.
This should contain the _ _ init _ _ .py, settings.py, urls.py, asgi.py, and wsgi.py files.
~ / myprojectdir / myprojectenv /: The virtual environment directory we created earlier.
Adjusting the Project Settings
The first thing we should do with our newly created project files is adjust the settings.
Open the settings file in your text editor:
Start by locating the ALLOWED _ HOSTS directive.
This defines a list of the server's addresses or domain names may be used to connect to the Django instance.
Any incoming requests with a Host header that is not in this list will raise an exception.
Django requires that you set this to prevent a certain class of security vulnerability.
In the square brackets, list the IP addresses or domain names that are associated with your Django server.
Each item should be listed in quotations with entries separated by a comma.
If you wish requests for an entire domain and any subdomains, prepend a period to the beginning of the entry.
In the snippet below, there are a few commented out examples used to demonstrate:
< $> note Note: Be sure to include localhost as one of the options since we will be proxying connections through a local Nginx instance.
Next, find the section that configures database access.
It will start with DATABASES.
The configuration in the file is for a SQLite database.
We already created a PostgreSQL database for our project, so we need to adjust the settings.
Change the settings with your PostgreSQL database information.
We tell Django to use the psycopg2 adaptor we installed with pip.
We need to give the database name, the database username, the database user's password, and then specify that the database is located on the local computer.
You can leave the PORT setting as an empty string:
Next, move down to the bottom of the file and add a setting indicating where the static files should be placed.
This is necessary so that Nginx can handle requests for these items.
The following line tells Django to place them in a directory called static in the base project directory:
Completing Initial Project Setup
Now, we can migrate the initial database schema to our PostgreSQL database using the management script:
Create an administrative user for the project by typing:
You will have to select a username, provide an email address, and choose and confirm a password.
We can collect all of the static content into the directory location we configured by typing:
You will have to confirm the operation.
The static files will then be placed in a directory called static within your project directory.
If you followed the initial server setup guide, you should have a UFW firewall protecting your server.
In order to test the development server, we'll have to allow access to the port we'll be using.
Create an exception for port 8000 by typing:
Finally, you can test our your project by starting up the Django development server with this command:
In your web browser, visit your server's domain name or IP address followed by: 8000:
You should receive the default Django index page:
Django index page
If you append / admin to the end of the URL in the address bar, you will be prompted for the administrative username and password you created with the createsuperuser command:
Django admin login
After authenticating, you can access the default Django admin interface:
Django admin interface
When you are finished exploring, hit CTRL-C in the terminal window to shut down the development server.
Testing Gunicorn's Ability to Serve the Project
The last thing we want to do before leaving our virtual environment is test Gunicorn to make sure that it can serve the application.
We can do this by entering our project directory and using gunicorn to load the project's WSGI module:
This will start Gunicorn on the same interface that the Django development server was running on.
You can go back and test the app again.
< $> note Note: The admin interface will not have any of the styling applied since Gunicorn does not know how to find the static CSS content responsible for this.
We passed Gunicorn a module by specifying the relative directory path to Django's wsgi.py file, which is the entry point to our application, using Python's module syntax.
Inside of this file, a function called application is defined, which is used to communicate with the application.
To learn more about the WSGI specification, click here.
When you are finished testing, hit CTRL-C in the terminal window to stop Gunicorn.
We're now finished configuring our Django application.
We can back out of our virtual environment by typing:
The virtual environment indicator in your prompt will be removed.
Creating systemd Socket and Service Files for Gunicorn
We have tested that Gunicorn can interact with our Django application, but we should implement a more robust way of starting and stopping the application server.
To accomplish this, we'll make systemd service and socket files.
The Gunicorn socket will be created at boot and will listen for connections.
When a connection occurs, systemd will automatically start the Gunicorn process to handle the connection.
Start by creating and opening a systemd socket file for Gunicorn with sudo privileges:
Inside, we will create a [Unit] section to describe the socket, a [Socket] section to define the socket location, and an [Install] section to make sure the socket is created at the right time:
Next, create and open a systemd service file for Gunicorn with sudo privileges in your text editor.
The service filename should match the socket filename with the exception of the extension:
Start with the [Unit] section, which is used to specify metadata and dependencies.
We'll put a description of our service here and tell the init system to only start this after the networking target has been reached.
Because our service relies on the socket from the socket file, we need to include a Requires directive to indicate that relationship:
Next, we'll open up the [Service] section.
We'll specify the user and group that we want to process to run under.
We will give our regular user account ownership of the process since it owns all of the relevant files.
We'll give group ownership to the www-data group so that Nginx can communicate easily with Gunicorn.
We'll then map out the working directory and specify the command to use to start the service.
In this case, we'll have to specify the full path to the Gunicorn executable, which is installed within our virtual environment.
We will bind the process to the Unix socket we created within the / run directory so that the process can communicate with Nginx.
We log all data to standard output so that the journald process can collect the Gunicorn logs.
We can also specify any optional Gunicorn tweaks here.
For example, we specified 3 worker processes in this case:
Finally, we'll add an [Install] section.
This will tell systemd what to link this service to if we enable it to start at boot.
We want this service to start when the regular multi-user system is up and running:
With that, our systemd service file is complete.
Save and close it now.
We can now start and enable the Gunicorn socket.
This will create the socket file at / run / gunicorn.sock now and at boot.
When a connection is made to that socket, systemd will automatically start the gunicorn.service to handle it:
We can confirm that the operation was successful by checking for the socket file.
Checking for the Gunicorn Socket File
Check the status of the process to find out whether it was able to start:
You should receive an output like this:
Next, check for the existence of the gunicorn.sock file within the / run directory:
If the systemctl status command indicated that an error occurred or if you do not find the gunicorn.sock file in the directory, it's an indication that the Gunicorn socket was not able to be created correctly.
Check the Gunicorn socket's logs by typing:
Take another look at your / etc / systemd / system / gunicorn.socket file to fix any problems before continuing.
Testing Socket Activation
Currently, if you've only started the gunicorn.socket unit, the gunicorn.service will not be active yet since the socket has not yet received any connections.
You can check this by typing:
To test the socket activation mechanism, we can send a connection to the socket through curl by typing:
You should receive the HTML output from your application in the terminal.
This indicates that Gunicorn was started and was able to serve your Django application.
You can verify that the Gunicorn service is running by typing:
If the output from curl or the output of systemctl status indicates that a problem occurred, check the logs for additional details:
Check your / etc / systemd / system / gunicorn.service file for problems.
If you make changes to the / etc / systemd / system / gunicorn.service file, reload the daemon to reread the service definition and restart the Gunicorn process by typing:
Make sure you troubleshoot the above issues before continuing.
Configure Nginx to Proxy Pass to Gunicorn
Now that Gunicorn is set up, we need to configure Nginx to pass traffic to the process.
Start by creating and opening a new server block in Nginx's sites-available directory:
Inside, open up a new server block.
We will start by specifying that this block should listen on the normal port 80 and that it should respond to our server's domain name or IP address:
Next, we will tell Nginx to ignore any problems with finding a favicon.
We will also tell it where to find the static assets that we collected in our ~ / < ^ > myprojectdir < ^ > / static directory.
All of these files have a standard URI prefix of "/ static", so we can create a location block to match those requests:
Finally, we'll create a location / {} block to match all other requests.
Inside of this location, we'll include the standard proxy _ params file included with the Nginx installation and then we will pass the traffic directly to the Gunicorn socket:
Now, we can enable the file by linking it to the sites-enabled directory:
Test your Nginx configuration for syntax errors by typing:
If no errors are reported, go ahead and restart Nginx by typing:
Finally, we need to open up our firewall to normal traffic on port 80. Since we no longer need access to the development server, we can remove the rule to open port 8000 as well:
You should now be able to go to your server's domain or IP address to view your application.
< $> note Note: After configuring Nginx, the next step should be securing traffic to the server using SSL / TLS.
This is important because without it, all information, including passwords are sent over the network in plain text.
If you have a domain name, the easiest way to get an SSL certificate to secure your traffic is using Let's Encrypt.
Follow this guide to set up Let's Encrypt with Nginx on Ubuntu 20.04.
Follow the procedure using the Nginx server block we created in this guide.
Troubleshooting Nginx and Gunicorn
If this last step does not show your application, you will need to troubleshoot your installation.
Nginx Is Showing the Default Page Instead of the Django Application
If Nginx displays the default page instead of proxying to your application, it usually means that you need to adjust the server _ name within the / etc / nginx / sites-available / < ^ > myproject < ^ > file to point to your server's IP address or domain name.
Nginx uses the server _ name to determine which server block to use to respond to requests.
If you receive the default Nginx page, it is a sign that Nginx wasn't able to match the request to a sever block explicitly, so it's falling back on the default block defined in / etc / nginx / sites-available / default.
The server _ name in your project's server block must be more specific than the one in the default server block to be selected.
Nginx Is Displaying a 502 Bad Gateway Error Instead of the Django Application
A 502 error indicates that Nginx is unable to successfully proxy the request.
A wide range of configuration problems express themselves with a 502 error, so more information is required to troubleshoot properly.
The primary place to look for more information is in Nginx's error logs.
Generally, this will tell you what conditions caused problems during the proxying event.
Follow the Nginx error logs by typing:
Now, make another request in your browser to generate a fresh error (try refreshing the page).
You should receive a fresh error message written to the log. If you look at the message, it should help you narrow down the problem.
You might receive the following message:
connect () to unix: / run / gunicorn.sock failed (2: No such file or directory)
This indicates that Nginx was unable to find the gunicorn.sock file at the given location.
You should compare the proxy _ pass location defined within / etc / nginx / sites-available / myproject file to the actual location of the gunicorn.sock file generated by the gunicorn.socket systemd unit.
If you cannot find a gunicorn.sock file within the / run directory, it generally means that the systemd socket file was unable to create it. Go back to the section on checking for the Gunicorn socket file to step through the troubleshooting steps for Gunicorn.
connect () to unix: / run / gunicorn.sock failed (13: Permission denied)
This indicates that Nginx was unable to connect to the Gunicorn socket because of permissions problems.
This can happen when the procedure is followed using the root user instead of a sudo user.
While systemd is able to create the Gunicorn socket file, Nginx is unable to access it.
This can happen if there are limited permissions at any point between the root directory (/) the gunicorn.sock file.
We can review the permissions and ownership values of the socket file and each of its parent directories by passing the absolute path to our socket file to the namei command:
The output displays the permissions of each of the directory components.
By looking at the permissions (first column), owner (second column) and group owner (third column), we can figure out what type of access is allowed to the socket file.
In the above example, the socket file and each of the directories leading up to the socket file have world read and execute permissions (the permissions column for the directories end with r-x instead of ---).
The Nginx process should be able to access the socket successfully.
If any of the directories leading up to the socket do not have world read and execute permission, Nginx will not be able to access the socket without allowing world read and execute permissions or making sure group ownership is given to a group that Nginx is a part of.
Django Is Displaying: "could not connect to server: Connection refused"
One message that you may receive from Django when attempting to access parts of the application in the web browser is:
This indicates that Django is unable to connect to the Postgres database.
Make sure that the Postgres instance is running by typing:
If it is not, you can start it and enable it to start automatically at boot (if it is not already configured to do so) by typing:
If you are still having issues, make sure the database settings defined in the ~ / myprojectdir / myproject / settings.py file are correct.
Further Troubleshooting
For additional troubleshooting, the logs can help narrow down root causes.
Check each of them in turn and look for messages indicating problem areas.
The following logs may be helpful:
Check the Nginx process logs by typing: sudo journalctl -u nginx
Check the Nginx access logs by typing: sudo less / var / log / nginx / access.log
Check the Nginx error logs by typing: sudo less / var / log / nginx / error.log
Check the Gunicorn application logs by typing: sudo journalctl -u gunicorn
Check the Gunicorn socket logs by typing: sudo journalctl -u gunicorn.socket
As you update your configuration or application, you will likely need to restart the processes to adjust to your changes.
If you update your Django application, you can restart the Gunicorn process to pick up the changes by typing:
If you change Gunicorn socket or service files, reload the daemon and restart the process by typing:
If you change the Nginx server block configuration, test the configuration and then Nginx by typing:
These commands are helpful for picking up changes as you adjust your configuration.
In this guide, we've set up a Django project in its own virtual environment.
We've configured Gunicorn to translate client requests so that Django can handle them.
Afterwards, we set up Nginx to act as a reverse proxy to handle client connections and serve the correct project depending on the client request.
Django makes creating projects and applications simple by providing many of the common pieces, allowing you to focus on the unique elements.
By leveraging the general tool chain described in this article, you can easily serve the applications you create from a single server.
How To Centralize Logs With Journald on Ubuntu 20.04
6061
System logs are an extremely important component of managing Linux systems.
They provide an invaluable insight into how the systems are working and also how they are being used because, in addition to errors, they record operational information such as security events.
The standard configuration for Linux systems is to store their logs locally on the same system where they occurred.
This works for standalone systems but quickly becomes a problem as the number of systems increases.
The solution to managing all these logs is to create a centralized logging server where each Linux host sends its logs, in real-time, to a dedicated log management server.
A centralized logging solution offers several benefits compared with storing logs on each host:
Reduces the amount of disk space needed on each host to store log files.
Logs can be retained for longer as the dedicated log server can be configured with more storage capacity.
Advanced log analysis can be carried out that requires logs from multiple systems and also more compute resources than may be available on the hosts.
Systems administrators can access the logs for all their systems that they may not be able to log in to directly for security reasons.
In this guide, you will configure a component of the systemd suite of tools to relay log messages from client systems to a centralized log collection server.
You will configure the server and client to use TLS certificates to encrypt the log messages as they are transmitted across insecure networks such as the internet and also to authenticate each other.
Two Ubuntu 20.04 servers.
A non-root user with sudo privileges on both servers.
Follow the Initial Server Setup with Ubuntu 20.04 guide for instructions on how to do this.
You should also configure the UFW firewall on both servers as explained in the guide.
Two hostnames that point to your servers.
One hostname for the client system that generates the logs and another one for the log collection server.
Learn how to point hostnames to DigitalOcean Droplets by consulting the Domains and DNS documentation.
This guide will use the following two example hostnames:
< ^ > client.your _ domain < ^ >: The client system that generates the logs.
< ^ > server.your _ domain < ^ >: The log collection server.
Log in to both the client and server in separate terminals via SSH as the non-root sudo user to begin this tutorial.
< $> note Note: Throughout the tutorial command blocks are labeled with the server name (client or server) that the command should be run on.
Step 1 - Installing systemd-journal-remote
In this step, you will install the systemd-journal-remote package on the client and the server.
This package contains the components that the client and server use to relay log messages.
First, on both the client and server, run a system update to ensure that the package database and the system is current:
Next, install the systemd-journal-remote package:
On the server, enable and start the two systemd components that it needs to receive log messages with the following command:
The --now option in the first command starts the services immediately.
You did not use it in the second command because this service will not start until it has TLS certificates, which you will create in the next step.
On the client, enable the component that systemd uses to send the log messages to the server:
Next, on the server, open ports 19532 and 80 in the UFW firewall.
This will allow the server to receive the log messages from the client.
Port 80 is the port that certbot will use to generate the TLS certificate.
The following commands will open these ports:
On the client, you only need to open port 80 with this command:
You have now installed the required components and completed the base system configuration on the client and server.
Before you can configure these components to start relaying log messages you will register the Let's Encrypt TLS certificates for the client and server using the certbot utility.
Step 2 - Installing Certbot and Registering Certificates
Let's Encrypt is a Certificate Authority that issues free TLS certificates.
These certificates allow computers to both encrypt the data that they send between them and also verify each other's identity.
These certificates are what allow you to secure your internet browsing with HTTPS.
The same certificates can be used by any other application that wants the same level of security.
The process of registering the certificate is the same no matter what you use them for.
In this step, you will install the certbot utility and use it to register the certificates.
It will also automatically take care of renewing the certificates when they expire.
The registration process here is the same on the client and server.
You only need to change the hostname to match the host where you are running the registration command.
First, enable Ubuntu's universe repository as the certbot utility resides in the universe repository.
If you already have the universe repository enabled, running these commands will not do anything to your system and are safe to run:
Next, install certbot on both hosts:
Now you've installed certbot, run the following command to register the certificates on the client and server:
The options in this command mean as follows:
certonly: Register the certificate and make no other changes on the system.
--standalone: Use certbot's built-in web server to verify the certificate request.
--agree-tos: Automatically agree to the Let's Encrypt Terms of Service.
--email < ^ > your _ email < ^ >: This is the email address that Let's Encrypt will use to notify you about certificate expiry and other important information.
-d < ^ > your _ domain < ^ >: The hostname that the certificate will be registered for.
This must match the system where you run it.
When you run this command you will be asked if you want to share the email address with Let's Encrypt so they can email you news and other information about their work.
Doing this is optional, if you do not share your email address the certificate registration will still complete normally.
When the certificate registration process completes it will place the certificate and key files in / etc / letsencrypt / live / < ^ > your _ domain < ^ > / where your _ domain is the hostname that you registered the certificate for.
Finally you need to download a copy of Let's Encrypt's CA and intermediate certificates and put them into the same file. journald will use this file to verify the authenticity of the certificates on the client and server when they communicate with each other.
The following command will download the two certificates from Let's Encrypt's website and put them into a single file called letsencrypt-combined-certs.pem in your user's home directory.
Run this command on the client and server to download the certificates and create the combined file:
Next, move this file into the Let's Encrypt directory containing the certificates and keys:
You've now registered the certificates and keys.
In the next step, you will configure the log collection server to start listening for and storing log messages from the client.
Step 3 - Configuring the Server
In this step, you will configure the server to use the certificate and key files that you generated in the last step so that it can start accepting log messages from the client.
systemd-journal-remote is the component that listens for log messages.
Open its configuration file at / etc / systemd / journal-remote.conf with a text editor to start configuring it on the server:
Next, uncomment all the lines under the [Remote] section and set the paths to point to the TLS files you just created:
Here are the options you've used here:
Seal = false: Sign the log data in the journal.
Enable this if you need maximum security; otherwise, you can leave it as false.
If you would prefer all the logs to be added to a single file set this to SplitMode = false.
ServerKeyFile: The server's private key file.
ServerCertificateFile: The server's certificate file.
TrustedCertificateFile: The file containing the Let's Encrypt CA certificates.
Now, you need to change the permissions on the Let's Encrypt directories that contain the certificates and key so that the systemd-journal-remote can read and use them.
First, change the permissions so that the certificate and private key are readable:
You can now start systemd-journal-remote:
Your log collection server is now running and ready to start accepting log messages from a client.
In the next step, you will configure the client to relay the logs to your collection server.
Step 4 - Configuring the Client
In this step, you will configure the component that relays the log messages to the log collection server.
This component is called systemd-journal-upload.
The default configuration for systemd-journal-upload is that it uses a temporary user that only exists while the process is running.
This makes allowing systemd-journal-upload to read the TLS certificates and keys more complicated.
To resolve this you will create a new system user with the same name as the temporary user that will get used in its place.
First, create the new user called systemd-journal-upload on the client with the following adduser command:
These options to the command are:
--system: Create the new user as a system user.
This gives the user a UID (User Identifier) number under 1000.
UID's over 1000 are usually given to user accounts that a human will use to log in with.
--home / run / systemd: Set / run / systemd as the home directory for this user.
--no-create-home: Don't create the home directory set, as it already exists.
--disabled-login: The user cannot log in to the server via, for example, SSH.
--group: Create a group with the same name as the user.
Next, set the permissions and ownership of the Let's Encrypt certificate files:
Now, edit the configuration for systemd-journal-upload, which is at / etc / systemd / journal-upload.conf.
Open this file with a text editor:
Edit this file so that it looks like the following:
Your client is now set up and running and is sending its log messages to the log collection server.
In the next step, you will check that the logs are being sent and recorded correctly.
Step 5 - Testing the Client and Server
In this step, you will test that the client is relaying log messages to the server and that the server is storing them correctly.
The log collection server stores the logs from the clients in a directory at / var / log / journal / remote /.
When you restarted the client at the end of the last step it began sending log messages so there is now a log file in / var / log / journal / remote /.
The file will be named after the hostname you used for the TLS certificate.
Use the ls command to check that the client's log file is present on the server:
This will print the directory contents showing the log file:
Next, write a log message on the client to check that the server is receiving the client's messages as you expect.
You will use the logger utility to create a custom log message on the client.
If everything is working systemd-journal-upload will relay this message to the server.
On the client run the following logger command:
The -p syslog.debug in this command sets the facility and severity of the message.
Setting this to syslog.debug will make clear it's a test message.
Next, read the client's journal file on the server to check that the log messages are arriving from the client.
This file is a binary log file so you will not be able to read it with tools like less.
Instead, read the file using journalctl with the --file = option that allows you to specify a custom journal file:
The log message will appear as follows:
Your log centralization server is now successfully collecting logs from your client system.
In this article, you set up a log central collection server and configured a client to relay a copy of its system logs to the server.
You can configure as many clients as you need to relay messages to the log collection server using the client configuration steps you used here.
How To Install Elasticsearch, Logstash, and Kibana (Elastic Stack) on Ubuntu 20.04
5823
The Elastic Stack - formerly known as the ELK Stack - is a collection of open-source software produced by Elastic which allows you to search, analyze, and visualize logs generated from any source in any format, a practice known as centralized logging.
Centralized logging can be useful when attempting to identify problems with your servers or applications as it allows you to search through all of your logs in a single place.
It's also useful because it allows you to identify issues that span multiple servers by correlating their logs during a specific time frame.
The Elastic Stack has four main components:
Elasticsearch: a distributed RESTful search engine which stores all of the collected data.
Logstash: the data processing component of the Elastic Stack which sends incoming data to Elasticsearch.
Kibana: a web interface for searching and visualizing logs.
Beats: lightweight, single-purpose data shippers that can send data from hundreds or thousands of machines to either Logstash or Elasticsearch.
In this tutorial, you will install the Elastic Stack on an Ubuntu 20.04 server.
You will learn how to install all of the components of the Elastic Stack - including Filebeat, a Beat used for forwarding and centralizing logs and files - and configure them to gather and visualize system logs.
Additionally, because Kibana is normally only available on the localhost, we will use Nginx to proxy it so it will be accessible over a web browser.
We will install all of these components on a single server, which we will refer to as our Elastic Stack server.
< $> note Note: When installing the Elastic Stack, you must use the same version across the entire stack.
In this tutorial we will install the latest versions of the entire stack which are, at the time of this writing, Elasticsearch 7.7.1, Kibana 7.7.1, Logstash 7.7.1, and Filebeat 7.7.1.
You can achieve this by following the Initial Server Setup with Ubuntu 20.04.
OpenJDK 11 installed.
See the section Installing the Default JRE / JDK (https: / / www.digitalocean.com / community / tutorials / how-to-install-java-with-apt-on-ubuntu-20-04 # installing-the-default-jrejdk in our guide) How To Install Java with Apt on Ubuntu 20.04 to set this up.
Nginx installed on your server, which we will configure later in this guide as a reverse proxy for Kibana.
Follow our guide on How to Install Nginx on Ubuntu 20.04 to set this up.
Additionally, because the Elastic Stack is used to access valuable information about your server that you would not want unauthorized users to access, it's important that you keep your server secure by installing a TLS / SSL certificate.
This is optional but strongly encouraged.
However, because you will ultimately make changes to your Nginx server block over the course of this guide, it would likely make more sense for you to complete the Let's Encrypt on Ubuntu 20.04 guide at the end of this tutorial's second step.
With that in mind, if you plan to configure Let's Encrypt on your server, you will need the following in place before doing so:
A fully qualified domain name (FQDN).
An A record with < ^ > your _ domain < ^ > pointing to your server's public IP address.
An A record with www. < ^ > your _ domain < ^ > pointing to your server's public IP address.
Use your preferred text editor to edit Elasticsearch's main configuration file, elasticsearch.yml.
To restrict access and therefore increase security, find the line that specifies network.host, uncomment it, and replace its value with localhost like this:
You can test whether your Elasticsearch service is running by sending an HTTP request:
You will see a response showing some basic information about your local node, similar to this:
Now that Elasticsearch is up and running, let's install Kibana, the next component of the Elastic Stack.
Step 2 - Installing and Configuring the Kibana Dashboard
According to the official documentation, you should install Kibana only after installing Elasticsearch.
Installing in this order ensures that the components each product depends on are correctly in place.
Because you've already added the Elastic package source in the previous step, you can just install the remaining components of the Elastic Stack using apt:
Then enable and start the Kibana service:
Because Kibana is configured to only listen on localhost, we must set up a reverse proxy to allow external access to it. We will use Nginx for this purpose, which should already be installed on your server.
First, use the openssl command to create an administrative Kibana user which you'll use to access the Kibana web interface.
As an example we will name this account < ^ > kibanaadmin < ^ >, but to ensure greater security we recommend that you choose a non-standard name for your user that would be difficult to guess.
The following command will create the administrative Kibana user and password, and store them in the htpasswd.users file.
You will configure Nginx to require this username and password and read this file momentarily:
Enter and confirm a password at the prompt.
Remember or take note of this login, as you will need it to access the Kibana web interface.
Next, we will create an Nginx server block file.
As an example, we will refer to this file as < ^ > your _ domain < ^ >, although you may find it helpful to give yours a more descriptive name.
For instance, if you have a FQDN and DNS records set up for this server, you could name this file after your FQDN.
Using nano or your preferred text editor, create the Nginx server block file:
Add the following code block into the file, being sure to update < ^ > your _ domain < ^ > to match your server's FQDN or public IP address.
This code configures Nginx to direct your server's HTTP traffic to the Kibana application, which is listening on localhost: 5601.
Additionally, it configures Nginx to read the htpasswd.users file and require basic authentication.
Note that if you followed the prerequisite Nginx tutorial through to the end, you may have already created this file and populated it with some content.
In that case, delete all the existing content in the file before adding the following:
When you're finished, save and close the file.
Next, enable the new configuration by creating a symbolic link to the sites-enabled directory.
If you already created a server block file with the same name in the Nginx prerequisite, you do not need to run this command:
Then check the configuration for syntax errors:
If any errors are reported in your output, go back and double check that the content you placed in your configuration file was added correctly.
Once you see syntax is ok in the output, go ahead and restart the Nginx service:
If you followed the initial server setup guide, you should have a UFW firewall enabled.
To allow connections to Nginx, we can adjust the rules by typing:
< $> note Note: If you followed the prerequisite Nginx tutorial, you may have created a UFW rule allowing the Nginx HTTP profile through the firewall.
Because the Nginx Full profile allows both HTTP and HTTPS traffic through the firewall, you can safely delete the rule you created in the prerequisite tutorial.
Do so with the following command:
Kibana is now accessible via your FQDN or the public IP address of your Elastic Stack server.
You can check the Kibana server's status page by navigating to the following address and entering your login credentials when prompted:
This status page displays information about the server "s resource usage and lists the installed plugins.
| Kibana status page
< $> note Note: As mentioned in the Prerequisites section, it is recommended that you enable SSL / TLS on your server.
You can follow the Let "s Encrypt guide now to obtain a free SSL certificate for Nginx on Ubuntu 20.04.
After obtaining your SSL / TLS certificates, you can come back and complete this tutorial.
Now that the Kibana dashboard is configured, let's install the next component: Logstash.
Step 3 - Installing and Configuring Logstash
Although it's possible for Beats to send data directly to the Elasticsearch database, it is common to use Logstash to process the data. This will allow you more flexibility to collect data from different sources, transform it into a common format, and export it to another database.
Install Logstash with this command:
After installing Logstash, you can move on to configuring it. Logstash's configuration files reside in the / etc / logstash / conf.d directory.
For more information on the configuration syntax, you can check out the configuration reference that Elastic provides.
As you configure the file, it's helpful to think of Logstash as a pipeline which takes in data at one end, processes it in one way or another, and sends it out to its destination (in this case, the destination being Elasticsearch).
A Logstash pipeline has two required elements, input and output, and one optional element, filter.
The input plugins consume data from a source, the filter plugins process the data, and the output plugins write the data to a destination.
Logstash pipeline
Create a configuration file called 02-beats-input.conf where you will set up your Filebeat input:
Insert the following input configuration.
This specifies a beats input that will listen on TCP port 5044.
Next, create a configuration file called 30-elasticsearch-output.conf:
Insert the following output configuration.
Essentially, this output configures Logstash to store the Beats data in Elasticsearch, which is running at localhost: 9200, in an index named after the Beat used.
The Beat used in this tutorial is Filebeat:
Test your Logstash configuration with this command:
If there are no syntax errors, your output will display Config Validation Result: OK.
Exiting Logstash after a few seconds.
If you don't see this in your output, check for any errors noted in your output and update your configuration to correct them.
Note that you will receive warnings from OpenJDK, but they should not cause any problems and can be ignored.
If your configuration test is successful, start and enable Logstash to put the configuration changes into effect:
Now that Logstash is running correctly and is fully configured, let's install Filebeat.
Step 4 - Installing and Configuring Filebeat
The Elastic Stack uses several lightweight data shippers called Beats to collect data from various sources and transport them to Logstash or Elasticsearch.
Here are the Beats that are currently available from Elastic:
Filebeat: collects and ships log files.
Metricbeat: collects metrics from your systems and services.
Packetbeat: collects and analyzes network data.
Winlogbeat: collects Windows event logs.
Auditbeat: collects Linux audit framework data and monitors file integrity.
Heartbeat: monitors services for their availability with active probing.
In this tutorial we will use Filebeat to forward local logs to our Elastic Stack.
Install Filebeat using apt:
Next, configure Filebeat to connect to Logstash.
Here, we will modify the example configuration file that comes with Filebeat.
Open the Filebeat configuration file:
< $> note Note: As with Elasticsearch, Filebeat's configuration file is in YAML format.
This means that proper indentation is crucial, so be sure to use the same number of spaces that are indicated in these instructions.
Filebeat supports numerous outputs, but you "ll usually only send events directly to Elasticsearch or to Logstash for additional processing.
In this tutorial, we'll use Logstash to perform additional processing on the data collected by Filebeat.
Filebeat will not need to send any data directly to Elasticsearch, so let's disable that output.
To do so, find the output.elasticsearch section and comment out the following lines by preceding them with a #:
Then, configure the output.logstash section.
Uncomment the lines output.logstash: and hosts: ["localhost: 5044"] by removing the #.
This will configure Filebeat to connect to Logstash on your Elastic Stack server at port 5044, the port for which we specified a Logstash input earlier:
The functionality of Filebeat can be extended with Filebeat modules.
In this tutorial we will use the system module, which collects and parses logs created by the system logging service of common Linux distributions.
Let's enable it:
You can see a list of enabled and disabled modules by running:
You will see a list similar to the following:
By default, Filebeat is configured to use default paths for the syslog and authorization logs.
In the case of this tutorial, you do not need to change anything in the configuration.
You can see the parameters of the module in the / etc / filebeat / modules.d / system.yml configuration file.
Next, we need to set up the Filebeat ingest pipelines, which parse the log data before sending it through logstash to Elasticsearch.
To load the ingest pipeline for the system module, enter the following command:
Next, load the index template into Elasticsearch.
An Elasticsearch index is a collection of documents that have similar characteristics.
Indexes are identified with a name, which is used to refer to the index when performing various operations within it. The index template will be automatically applied when a new index is created.
To load the template, use the following command:
Filebeat comes packaged with sample Kibana dashboards that allow you to visualize Filebeat data in Kibana.
Before you can use the dashboards, you need to create the index pattern and load the dashboards into Kibana.
As the dashboards load, Filebeat connects to Elasticsearch to check version information.
To load dashboards when Logstash is enabled, you need to disable the Logstash output and enable Elasticsearch output:
Now you can start and enable Filebeat:
If you've set up your Elastic Stack correctly, Filebeat will begin shipping your syslog and authorization logs to Logstash, which will then load that data into Elasticsearch.
To verify that Elasticsearch is indeed receiving this data, query the Filebeat index with this command:
If your output shows 0 total hits, Elasticsearch is not loading any logs under the index you searched for, and you will need to review your setup for errors.
If you received the expected output, continue to the next step, in which we will see how to navigate through some of Kibana's dashboards.
Step 5 - Exploring Kibana Dashboards
Let's return to the Kibana web interface that we installed earlier.
In a web browser, go to the FQDN or public IP address of your Elastic Stack server.
If your session has been interrupted, you will need to re-enter entering the credentials you defined in Step 2. Once you have logged in, you should receive the Kibana homepage:
Kibana Homepage
Click the Discover link in the left-hand navigation bar (you may have to click the the Expand icon at the very bottom left to see the navigation menu items).
On the Discover page, select the predefined filebeat- * index pattern to see Filebeat data. By default, this will show you all of the log data over the last 15 minutes.
You will see a histogram with log events, and some log messages below:
Discover page
Here, you can search and browse through your logs and also customize your dashboard.
At this point, though, there won't be much in there because you are only gathering syslogs from your Elastic Stack server.
Use the left-hand panel to navigate to the Dashboard page and search for the Filebeat System dashboards.
Once there, you can select the sample dashboards that come with Filebeat's system module.
For example, you can view detailed stats based on your syslog messages:
Syslog Dashboard
You can also view which users have used the sudo command and when:
Sudo Dashboard
Kibana has many other features, such as graphing and filtering, so feel free to explore.
In this tutorial, you've learned how to install and configure the Elastic Stack to collect and analyze system logs.
Remember that you can send just about any type of log or indexed data to Logstash using Beats, but the data becomes even more useful if it is parsed and structured with a Logstash filter, as this transforms the data into a consistent format that can be read easily by Elasticsearch.
How to Set Up an IKEv2 VPN Server with StrongSwan on Ubuntu 20.04
5742
A previous version of this tutorial was written by Justin Ellingwood and Namo
A virtual private network, or VPN, allows you to securely encrypt traffic as it travels through untrusted networks, such as those at the coffee shop, a conference, or an airport.
Internet Key Exchange v2, or IKEv2, is a protocol that allows for direct IPSec tunneling between the server and client.
In IKEv2 VPN implementations, IPSec provides encryption for the network traffic.
IKEv2 is natively supported on some platforms (OS X 10.11 +, iOS 9.1 +, and Windows 10) with no additional applications necessary, and it handles client hiccups quite smoothly.
In this tutorial, you'll set up an IKEv2 VPN server using StrongSwan on an Ubuntu 20.04 server.
You'll then learn how to connect to it with Windows, macOS, Ubuntu, iOS, and Android clients.
One Ubuntu 20.04 server configured by following the Ubuntu 20.04 initial server setup guide, including a sudo non-root user and a firewall.
Step 1 - Installing StrongSwan
First, we'll install StrongSwan, an open-source IPSec daemon which we'll configure as our VPN server.
We'll also install the public key infrastructure (PKI) component so that we can create a Certificate Authority (CA) to provide credentials for our infrastructure.
Start by updating the local package cache:
Then install the software by typing:
The additional libcharon-extauth-plugins package is used to ensure that various clients can authenticate to your server using a shared username and passphrase.
Now that everything's installed, let's move on to creating our certificates.
Step 2 - Creating a Certificate Authority
An IKEv2 server requires a certificate to identify itself to clients.
To help create the required certificate, the strongswan-pki package comes with a utility called pki to generate a Certificate Authority and server certificates.
To begin, let's create a few directories to store all the assets we'll be working on.
The directory structure matches some of the directories in / etc / ipsec.d, where we will eventually move all of the items we create:
Then lock down the permissions so that our private files can't be seen by other users:
Now that we have a directory structure to store everything, we can generate a root key.
This will be a 4096-bit RSA key that will be used to sign our root certificate authority.
Execute these commands to generate the key:
Following that we can move on to creating our root certificate authority, using the key that we just generated to sign the root certificate:
The --lifetime 3650 flag is used to ensure that the certificate authority "s root certificate will be valid for 10 years.
The root certificate for an authority does not change typically, since it would have to be redistributed to every server and client that rely on it, so 10 years is a safe default expiry value.
You can change the distinguished name (DN) value to something else if you would like.
The common name (CN field) here is just the indicator, so it doesn't have to match anything in your infrastructure.
Now that we've got our root certificate authority up and running, we can create a certificate that the VPN server will use.
Step 3 - Generating a Certificate for the VPN Server
We'll now create a certificate and key for the VPN server.
This certificate will allow the client to verify the server's authenticity using the CA certificate we just generated.
First, create a private key for the VPN server with the following command:
Now, create and sign the VPN server certificate with the certificate authority's key you created in the previous step.
Execute the following command, but change the Common Name (CN) and the Subject Alternate Name (SAN) field to your VPN server's DNS name or IP address:
< $> note Note: If you are using an IP address instead of a DNS name, you will need to specify multiple --san entries.
The line in the previous command block where you specify the distinguished name (--dn...) will need to be modified with the extra entry like the following excerpted line:
The reason for this extra --san @ < ^ > IP _ address < ^ > entry is that some clients will check whether the TLS certificate has both an DNS entry and an IP Address entry for a server when they verify its identity.
The --flag serverAuth option is used to indicate that the certificate will be used explicitly for server authentication, before the encrypted tunnel is established.
The --flag ikeIntermediate option is used to support older macOS clients.
Now that we've generated all of the TLS / SSL files StrongSwan needs, we can move the files into place in the / etc / ipsec.d directory by typing:
In this step, we've created a certificate pair that will be used to secure communications between the client and the server.
We've also signed the certificates with the CA key, so the client will be able to verify the authenticity of the VPN server using the CA certificate.
With all of these certificates ready, we'll move on to configuring the software.
Step 4 - Configuring StrongSwan
StrongSwan has a default configuration file with some examples, but we will have to do most of the configuration ourselves.
Let's back up the file for reference before starting from scratch:
Create and open a new blank configuration file using your preferred text editor.
< $> note Note: As you work through this section to configure the server portion of your VPN, you will encounter settings that refer to left and right sides of a connection.
When working with IPSec VPNs, the left side by convention refers to the local system that you are configuring, in this case the server.
The right side directives in these settings will refer to remote clients, like phones and other computers.
When you move on to configuring clients later in this tutorial, the client configuration files will refer to themselves using various left directives, and the server will be referred to using right side terminology.
First, we'll tell StrongSwan to log daemon statuses for debugging and allow duplicate connections.
Add these lines to the file:
Then, we'll create a configuration section for our VPN.
We'll also tell StrongSwan to create IKEv2 VPN Tunnels and to automatically load this configuration section when it starts up.
Append the following lines to the file:
We'll also configure dead-peer detection to clear any "dangling" connections in case the client unexpectedly disconnects.
Add these lines:
Next, we'll configure the server "s" left "side IPSec parameters.
Each of the following parameters ensures that the server is configured to accept connections from clients and to identify itself correctly.
You "ll add each of these settings to the / etc / ipsec.conf file once you are familiar with what they are and why they are used:
left =% any The% any value ensures that the server will use the network interface where it receives incoming connections for subsequent communication with clients.
For example, if you are connecting a client over a private network, the server will use the private IP address where it receives traffic for the rest of the connection.
leftid = < ^ > @ server _ domain _ or _ IP < ^ > This option controls the name that the server presents to clients.
When combined with the next option leftcert, the leftid option ensures that the server "s configured name and the Distinguished Name (DN) that is contained in the public certificate match.
leftcert = server-cert.pem This option is the path to the public certificate for the server that you configured in Step 3. Without it, the server will not be able to authenticate itself with clients, or finish negotiating the IKEv2 set up.
leftsendcert = always The always value ensures that any client that connects to the server will always receive a copy of the server "s public certificate as part of the initial connection set up.
leftsubnet = 0.0.0.0 / 0 The last "left" side option that you will add tells clients about the subnets that are reachable behind the server.
In this case, 0.0.0.0 / 0 is used to represent the entire set of IPv4 addresses, meaning that the server will tell clients to send all their traffic over the VPN by default.
Now that you are familiar with each of the relevant "left" side options, add them all to the file like this:
< $> note Note: When configuring the server ID (leftid), only include the @ character if your VPN server will be identified by a domain name:
If the server will be identified by its IP address, just put the IP address in:
Next, we can configure the client "s" right "side IPSec parameters.
Each of the following parameters tells the server how to accept connections from clients, how clients should authenticate to the server, and the private IP address ranges and DNS servers that clients will use.
Add each of these settings to the / etc / ipsec.conf file once you are familiar with what they are and why they are used:
right =% any The% any option for the right side of the connection instructs the server to accept incoming connections from any remote client.
rightid =% any This option ensures that the server will not reject connections from clients that provide an identity before the encrypted tunnel is established.
rightauth = eap-mschapv2 This option configures the authentication method that clients will use to authenticate to the server. eap-mschapv2 is used here for broad compatibility to support clients like Windows, macOS, and Android devices.
rightsourceip = 10.10.10.0 / 24 This option instructs the server to assign private IP addresses to clients from the specified 10.10.10.0 / 24 pool of IPs.
rightdns = 8.8.8.8,8.8.4.4 These IP addresses are Google "s public DNS resolvers.
They can be changed to use other public resolvers, the VPN server "s resolvers, or any other resolver that clients can reach.
rightsendcert = never This option instructs the server that clients do not need to send a certificate to authenticate themselves.
Now that you are familiar with the required "right" side options for the VPN, add the following lines to / etc / ipsec.conf:
Now we'll tell StrongSwan to ask the client for user credentials when they connect:
Finally, add the following lines to support Linux, Windows, macOS, iOS, and Android clients.
These lines specify the various key exchange, hashing, authentication, and encryption algorithms (commonly referred to as Cipher Suites) that StrongSwan will allow different clients to use:
Each supported cipher suite is delineated from the others by a comma.
For example chacha20poly1305-sha512-curve25519-prfsha512 is one suite, and aes256gcm16-sha384-prfsha384-ecp384 is another.
The cipher suites that are listed here are selected to ensure the widest range of compatibility across Windows, macOS, iOS, Android, and Linux clients.
The complete configuration file should look like this:
Save and close the file once you've verified that you've added each line correctly.
If you used nano, do so by pressing CTRL + X, Y, then ENTER.
Now that we've configured the VPN parameters, let's move on to creating an account so our users can connect to the server.
Step 5 - Configuring VPN Authentication
Our VPN server is now configured to accept client connections, but we don't have any credentials configured yet.
We'll need to configure a couple things in a special configuration file called ipsec.secrets:
We need to tell StrongSwan where to find the private key for our server certificate, so the server will be able to authenticate to clients.
We also need to set up a list of users that will be allowed to connect to the VPN.
Let's open the secrets file for editing:
First, we'll tell StrongSwan where to find our private key:
Then, we'll define the user credentials.
You can make up any username or password combination that you like:
Now that we've finished working with the VPN parameters, we'll restart the VPN service so that our configuration is applied:
Now that the VPN server has been fully configured with both server options and user credentials, it's time to move on to configuring the most important part: the firewall.
Step 6 - Configuring the Firewall & Kernel IP Forwarding
With the StrongSwan configuration complete, we need to configure the firewall to allow VPN traffic through and forward it.
If you followed the prerequisite initial server setup tutorial, you should have a UFW firewall enabled.
If you don't yet have UFW configured, you should start by adding a rule to allow SSH connections through the firewall so your current session doesn't close when you enable UFW:
Then enable the firewall by typing:
Then, add a rule to allow UDP traffic to the standard IPSec ports, 500 and 4500:
Next, we will open up one of UFW's configuration files to add a few low-level policies for routing and forwarding IPSec packets.
Before we we can do this, though, we need to find which network interface on our server is used for internet access.
Find this interface by querying for the device associated with the default route:
Your public interface should follow the word "dev".
For example, this result shows the interface named eth0, which is highlighted in the following example:
When you have your public network interface, open the / etc / ufw / before.rules file in your text editor.
The rules in this file are added to the firewall before the rest of the usual input and output rules.
They are used to configure network address translation (NAT) so that the server can correctly route connections to and from clients and the Internet.
Near the top of the file (before the * filter line), add the following configuration block.
Change each instance of eth0 in the above configuration to match the interface name you found with ip route.
The * nat lines create rules so that the firewall can correctly route and manipulate traffic between the VPN clients and the internet.
The * mangle line adjusts the maximum packet segment size to prevent potential issues with certain VPN clients:
Next, after the * filter and chain definition lines, add one more block of configuration:
These lines tell the firewall to forward ESP (Encapsulating Security Payload) traffic so the VPN clients will be able to connect.
ESP provides additional security for our VPN packets as they're traversing untrusted networks.
When you're finished, ave and close the file once you've verified that you've added each line correctly.
Before restarting the firewall, we'll change some network kernel parameters to allow routing from one interface to another.
The file that controls these settings is called / etc / ufw / sysctl.conf.
We'll need to configure a few things in the file including.
First IPv4 packet forwarding needs to be turned on so that traffic can move between the VPN and public facing network interfaces on the server.
Next we'll disable Path MTU discovery to prevent packet fragmentation problems.
Finally we will not accept ICMP redirects nor send ICMP redirects to prevent man-in-the-middle attacks.
Open UFW's kernel parameters configuration file using nano or your preferred text editor:
Now add the following net / ipv4 / ip _ forward = 1 setting at the end of the file to enable forwarding packets between interfaces:
Next block sending and receiving ICMP redirect packets by adding the following lines to the end of the file:
Finally, turn off Path MTU discovery by adding this line to the end of the file:
Save the file when you are finished.
Now we can enable all of our changes by disabling and re-enabling the firewall, since UFW applies these settings any time that it restarts:
You'll be prompted to confirm the process.
Type Y to enable UFW again with the new settings.
Step 7 - Testing the VPN Connection on Windows, macOS, Ubuntu, iOS, and Android
Now that you have everything set up, it's time to try it out. First, you'll need to copy the CA certificate you created and install it on your client device (s) that will connect to the VPN.
The easiest way to do this is to log into your server and output the contents of the certificate file:
Copy this output to your computer, including the -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- lines, and save it to a file with a recognizable name, such as ca-cert.pem.
Ensure the file you create has the .pem extension.
Alternatively, use SFTP to transfer the file to your computer.
Once you have the ca-cert.pem file downloaded to your computer, you can set up the connection to the VPN.
Connecting from Windows
There are multiple ways to import the root certificate and configure Windows to connect to a VPN.
The first method uses graphical tools for each step.
The second method uses PowerShell commands, which can be scripted and modified to suit your VPN configuration.
< $> note Note: These instructions have been tested on Windows 10 installations running versions 1903 and 1909.
Configuring Windows with Graphical Tools
First, import the root certificate by following these steps:
Press WINDOWS + R to bring up the Run dialog, and enter mmc.exe to launch the Windows Management Console.
From the File menu, navigate to Add or Remove Snap-in, select Certificates from the list of available snap-ins, and click Add.
We want the VPN to work with any user, so select Computer Account and click Next.
We're configuring things on the local computer, so select Local Computer, then click Finish.
Under the Console Root node, expand the Certificates (Local Computer) entry, expand Trusted Root Certification Authorities, and then select the Certificates entry: Certificates view
From the Action menu, select All Tasks and click Import to display the Certificate Import Wizard.
Click Next to move past the introduction.
On the File to Import screen, press the Browse button, ensure that you change the file type from "X.509 Certificate (.cer; .crt)" to "All Files (.)", and select the ca-cert.pem file that you've saved.
Then click Next.
Ensure that the Certificate Store is set to Trusted Root Certification Authorities, and click Next.
Click Finish to import the certificate.
Then configure the VPN with these steps:
Launch Control Panel, then navigate to the Network and Sharing Center.
Click on Set up a new connection or network, then select Connect to a workplace.
Select Use my Internet connection (VPN).
Enter the VPN server details.
Enter the server's domain name or IP address in the Internet address field, then fill in Destination name with something that describes your VPN connection.
Then click Done.
Configuring Windows using PowerShell
To import the root CA certificate using PowerShell, first open a PowerShell prompt with administrator privileges.
To do so, right click the Start menu icon and select Windows PowerShell (Admin).
You can also open a command prompt as administrator and type powershell.
Next we'll import the certificate using the Import-Certificate PowerShell cmdlet.
In the following command, the first -CertStoreLocation argument will ensure that the certificate is imported into the computer's Trusted Root Certification Authorities store so that all programs and users will be able to verify the VPN server's certificate.
The -FilePath argument should point to the location where you copied the certificate.
In the following example the path is C:\ Users\ sammy\ Documents\ ca-cert.pem.
Ensure that you edit the command to match the location that you used.
The command will output something like the following:
Now to configure the VPN using PowerShell, run the following command.
Substitute your server's DNS name or IP address on the -ServerAddress line.
The various flags will ensure that Windows is correctly configured with the appropriate security parameters that match the options that you set in / etc / ipsec.conf.
If the command is successful there will not be any output.
To confirm the VPN is configured correctly, use the Get-VPNConnection cmdlet:
You will receive output like the following:
By default Windows chooses older and slower algorithms.
Run the Set-VpnConnectionIPsecConfiguration cmdlet to upgrade the encryption parameters that Windows will use for the IKEv2 key exchange, and to encrypt packets:
< $> note Note: If you would like to delete the VPN connection and reconfigure it with different options, you can run the Remove-VpnConnection cmdlet.
The -Force flag will skip prompting you to confirm the removal.
You must be disconnected from the VPN if you attempt to remove it using this command.
Connecting to the VPN
Once you have the certificate imported and the VPN configured using either method, your new VPN connection will be visible under the list of networks.
Select the VPN and click Connect.
You'll be prompted for your username and password.
Type them in, click OK, and you'll be connected.
Connecting from macOS
Follow these steps to import the certificate:
Double-click the certificate file.
Keychain Access will pop up with a dialog that says "Keychain Access is trying to modify the system keychain.
Enter your password to allow this. "
Enter your password, then click on Modify Keychain
Double-click the newly imported VPN certificate.
This brings up a small properties window where you can specify the trust levels.
Set IP Security (IPSec) to Always Trust and you'll be prompted for your password again.
This setting saves automatically after entering the password.
Now that the certificate is imported and trusted, configure the VPN connection with these steps:
Go to System Preferences and choose Network.
Click on the small "plus" button on the lower-left of the list of networks.
In the popup that appears, set Interface to VPN, set the VPN Type to IKEv2, and give the connection a name.
In the Server and Remote ID field, enter the server's domain name or IP address.
Leave the Local ID blank.
Click on Authentication Settings, select Username, and enter your username and password you configured for your VPN user.
Then click OK.
Finally, click on Connect to connect to the VPN.
You should now be connected to the VPN.
Connecting from Ubuntu
To connect from an Ubuntu machine, you can set up and manage StrongSwan as a service or use a one-off command every time you wish to connect.
Instructions are provided for both.
Managing StrongSwan as a Service
To manage StrongSwan as a service, you will need to perform the following configuration steps.
First, update your local package cache using apt
Next, install StrongSwan and the required plugins for authentication:
Now you "ll need a copy of the CA certificate in the / etc / ipsec.d / cacerts directory so that your client can verify the server" s identity.
Run the following command to copy the ca-cert.pem file into place:
To ensure the VPN only runs on demand, use systemctl to disable StrongSwan from running automatically:
Next configure the username and password that you will use to authenticate to the VPN server.
Edit / etc / ipsec.secrets using nano or your preferred editor:
Add the following line, editing the highlighted username and password values to match the ones that you configured on the server:
Finally, edit the / etc / ipsec.conf file to configure your client to match the server "s configuration:
To connect to the VPN, type:
To disconnect again, type:
Using the charon-cmd Client for One-Off Connections
At this point you can connect to the VPN server with charon-cmd using the server's CA certificate, the VPN server's IP address, and the username you configured.
Run the following command whenever you want to connect to the VPN:
When prompted, provide the VPN user's password and you will be connected to the VPN.
To disconnect, press CTRL + C in the terminal and wait for the connection to close.
Connecting from iOS
To configure the VPN connection on an iOS device, follow these steps:
Send yourself an email with the root certificate attached.
Open the email on your iOS device and tap on the attached certificate file, then tap Install and enter your passcode.
Once it installs, tap Done.
Go to Settings, General, VPN and tap Add VPN Configuration.
This will bring up the VPN connection configuration screen.
Tap on Type and select IKEv2.
In the Description field, enter a short name for the VPN connection.
This could be anything you like.
The Local ID field can be left blank.
Enter your username and password in the Authentication section, then tap Done.
Select the VPN connection that you just created, tap the switch on the top of the page, and you'll be connected.
Connecting from Android
Send yourself an email with the CA certificate attached.
Save the CA certificate to your downloads folder.
Download the StrongSwan VPN client from the Play Store.
Open the app. Tap the "more" icon (...) in the upper-right corner and select CA certificates.
Tap the "more" icon (...) in the upper-right corner again.
Select Import certificate.
Browse to the CA certificate file in your downloads folder and select it to import it into the app.
Now that the certificate is imported into the StrongSwan app, you can configure the VPN connection with these steps:
In the app, tap ADD VPN PROFILE at the top.
Fill out the Server with your VPN server's domain name or public IP address.
Make sure IKEv2 EAP (Username / Password) is selected as the VPN Type.
Fill out the Username and Password with the credentials you defined on the server.
Deselect Select automatically in the CA certificate section and click Select CA certificate.
Tap the IMPORTED tab at the top of the screen and choose the CA you imported (it will be named "VPN root CA" if you didn't change the "DN" earlier).
If you'd like, fill out Profile name (optional) with a more descriptive name.
When you wish to connect to the VPN, click on the profile you just created in the StrongSwan application.
Troubleshooting Connections
If you are unable to import the certificate, ensure the file has the .pem extension, and not .pem.txt.
If you're unable to connect to the VPN, check the server name or IP address you used.
The server's domain name or IP address must match what you've configured as the common name (CN) while creating the certificate.
If they don't match, the VPN connection won't work.
For example, if you set up a certificate with the CN of vpn.example.com, you must use vpn.example.com when you enter the VPN server details.
Double-check the command you used to generate the certificate, and the values you used when creating your VPN connection.
Finally, double-check the VPN configuration to ensure the leftid value is configured with the @ symbol if you're using a domain name:
If you're using an IP address, ensure that the @ symbol is omitted.
Also make sure that when you generated the server-cert.pem file that you included both --san @ < ^ > IP _ address < ^ > and --san < ^ > IP _ address < ^ > flags.
In this tutorial, you've built a VPN server that uses the IKEv2 protocol.
You learned about the directives that control the left and right sides of a connection on both server and clients.
You also configured a Windows, macOS, iOS, Android, or Linux client to connect to the VPN.
To add or remove users, skip to Step 5 again.
Each line in / etc / ipsec.secrets is for one user, so adding or removing users, or changing passwords just requires editing the file.
Now you can be assured that your online activities will remain secure wherever you go and with any device that you use to access the internet.
