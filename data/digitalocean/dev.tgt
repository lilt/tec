Erstellen einer benutzerdefinierten Paginierung mit React
Eine Methode zur Handhabung großer Datensätze in der Ansicht ist die Verwendung des unendlichen Bildlaufs, bei dem mehr Daten in Blöcken geladen werden, wenn der Benutzer sehr nahe an das Seitenende scrollt.
Dies ist die Technik, die bei der Anzeige der Suchergebnisse in Google Images verwendet wird.
In diesem Tutorial sehen wir, wie eine benutzerdefinierte Paginierungskomponente mit React zur Paginierung großer Datensätze erstellt wird.
Wir werden eine paginierte Ansicht von Ländern der Welt erstellen.
3642
Wir sind oft an der Erstellung von Webanwendungen beteiligt, bei denen wir große Mengen von Datensätzen von einem Remote-Server, einer API oder einer Datenbank abrufen müssen.
Wenn Sie beispielsweise ein Zahlungssystem entwickeln, könnten Tausende von Transaktionen abgerufen werden.
Wenn es sich um eine Social Media-Anwendung handelt, könnten viele Benutzerkommentare, -profile oder -aktivitäten abgerufen werden.
Was auch immer der Fall sein mag, es gibt verschiedene Lösungen, um Daten so darzustellen, dass der Endbenutzer, der mit der Anwendung interagiert, nicht überfordert wird.
Eine Methode zur Handhabung großer Datensätze ist die Verwendung der Paginierung.
Die Paginierung funktioniert effektiv, wenn Sie die Größe des Datensatzes (die Gesamtzahl der Datensätze im Datensatz) bereits kennen.
Zweitens laden Sie nur den erforderlichen Datenblock aus dem Gesamtdatensatz basierend auf der Interaktion des Endbenutzers mit der Paginierungssteuerung.
Diese Technik wird bei der Anzeige der Suchergebnisse in der Google-Suche verwendet.
In diesem Tutorial lernen Sie, wie Sie mit React eine benutzerdefinierte Paginierungskomponente für die Paginierung großer Datensätze erstellen.
Sie werden eine paginierte Ansicht der Länder der Welt erstellen - einen Datensatz mit einer bekannten Größe.
Hier ist eine Demo davon, was Sie in diesem Tutorial erstellen werden:
Screenshot der Demo-Anwendung - zeigt die Länder der Welt
Auf Ihrem Rechner installiertes Node.
Die Schritte hierfür finden Sie unter Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung.
Das Befehlszeilen-Paket create-react-app zur Erstellung des Boilerplate-Codes für Ihre React-Anwendung. Wenn Sie npm < 5.2 verwenden, müssen Sie create-react-app möglicherweise als globale Abhängigkeit installieren.
Schließlich geht dieses Tutorial davon aus, dass Sie bereits mit React vertraut sind.
Sollte dies nicht der Fall sein, können Sie in der Reihe Codieren in React.js nachlesen, um mehr über React zu erfahren.
Dieses Tutorial wurde mit Node v14.2.0, nmp v6.14.4, react v16.13.1 und react-scripts v3.4.1 verifiziert.
Schritt 1 - Einrichten des Projekts
Starten Sie eine neue React-Anwendung unter Verwendung des Befehls create-react-app.
Sie können diese Anwendung beliebig benennen. In diesem Tutorial wird die Anwendung react-pagination genannt:
Als Nächstes installieren Sie die für Ihre Anwendung erforderlichen Abhängigkeiten.
Verwenden Sie zunächst das Terminalfenster zur Navigation zum Projektverzeichnis:
Führen Sie den folgenden Befehl aus, um die erforderlichen Abhängigkeiten zu installieren:
Dadurch werden bootstrap, prop-types, react-flags, countries-api und node-sass installiert.
Sie haben das Paket bootstrap als Abhängigkeit für Ihre Anwendung installiert, da Sie etwas Standardgestaltung benötigen.
Sie verwenden auch Stile aus der pagination-Komponente von Bootstrap verwenden.
Um Bootstrap in die Anwendung einzubinden, bearbeiten Sie die Datei src / index.js:
Und fügen Sie die folgende Zeile vor den anderen import-Anweisungen hinzu:
Jetzt ist Bootstrap-Styling in Ihrer gesamten Anwendung verfügbar.
Außerdem haben Sie react-flags als Abhängigkeit für Ihre Anwendung installiert.
Um Zugriff auf die Flaggen-Symbole aus Ihrer Anwendung zu erhalten, müssen Sie die Symbolbilder in das Verzeichnis public Ihrer Anwendung kopieren.
Erstellen Sie in Ihrem Verzeichnis public ein Verzeichnis img:
Kopieren Sie die Bilddateien in flags zu img:
Damit stellen Sie Ihrer Anwendung eine Kopie aller Bilder von react-flag bereit.
Nachdem Sie nun einige Abhängigkeiten eingebunden haben, starten Sie die Anwendung, indem Sie den folgenden Befehl mit npm aus dem Projektverzeichnis react-pagination ausführen:
Nachdem Sie die Anwendung gestartet haben, kann die Entwicklung beginnen.
Beachten Sie, dass eine Browser-Registerkarte mit einer Live Neuladefunktionalität für Sie geöffnet wurde, um während der Entwicklung mit der Anwendung synchron zu bleiben.
Zu diesem Zeitpunkt sollte die Ansicht der Anwendung wie im folgenden Screenshot dargestellt aussehen:
Anfangsansicht - Willkommen bei React-Bildschirm
Sie sind nun bereit, mit der Erstellung von Komponenten zu beginnen.
Schritt 2 - Erstellen der Komponente CountryCard
In diesem Schritt erstellen Sie die Komponente CountryCard.
Die Komponente CountryCard gibt den Namen, die Region und die Flagge eines bestimmten Landes wieder.
Zuerst erstellen wir ein Verzeichnis components im Verzeichnis src:
Anschließend erstellen wir eine neue Datei CountryCard.js im Verzeichnis src / works:
Und fügen den folgenden Code-Ausschnitt hinzu:
Die Komponente CountryCard erfordert eine country-Prop, die die Daten über das wiederzugebende Land enthält.
Wie in den propTypes für die Komponente CountryCard zu sehen ist, muss das Prop-Objekt country die folgenden Daten enthalten:
cca2 - 2-stelliger Ländercode
region - die Länderregion (z. B. "Afrika ")
name.common - der gebräuchliche Name des Landes (z. B. "Nigeria ")
Hier ist ein Beispiel für ein Länderobjekt:
Beachten Sie auch, wie Sie die Länderflagge mit dem Paket react-flags wiedergeben können.
In der Dokumentation zu react-flags erfahren Sie mehr über die benötigten Props und die Verwendung des Pakets.
Sie haben nun eine einzelne Komponente CountryCard fertiggestellt.
Letztendlich werden Sie CountryCards mehrfach verwenden, um verschiedene Flaggen und Länderinformationen in Ihrer Anwendung anzuzeigen.
Schritt 3 - Erstellen der Komponente Pagination
In diesem Schritt erstellen Sie die Komponente Pagination.
Die Komponente Pagination enthält die Logik für das Erstellen, Rendern und Wechseln der Seiten auf der Paginierungssteuerung.
Erstellen Sie eine neue Datei Pagination.js im Verzeichnis src / components:
Die Komponente Pagination kann vier spezielle Props aufnehmen, wie im Objekt propTypes angegeben.
onPageChanged ist eine Funktion, die nur dann mit Daten des aktuellen Paginierungsstatus aufgerufen wird, wenn sich die aktuelle Seite ändert.
totalRecords gibt die Gesamtzahl der zu paginierenden Datensätze an.
Es ist erforderlich.
pageLimit gibt die Anzahl der Datensätze an, die pro Seite angezeigt werden sollen.
Wenn sie nicht angegeben wird, ist sie gemäß der Definition in der constructor () auf 30 voreingestellt.
pageNeighbours gibt die Anzahl der zusätzlichen Seitennummern an, die auf jeder Seite der aktuellen Seite angezeigt werden sollen.
Der Mindestwert ist 0, und der maximale Wert ist 2. Wir hier nichts angegeben, wird der Standardwert 0 gemäß der Definition in der constructor () verwendet.
Das folgende Bild veranschaulicht die Wirkung verschiedener Werte der Prop pageNeighbours:
Darstellung der PageNeighbours
In der Funktion constructor () berechnen Sie die Gesamtseiten wie folgt:
Beachten Sie, dass Sie hier Math.ceil () verwenden, um sicherzustellen, dass Sie einen ganzzahligen Wert für die Gesamtzahl der Seiten erhalten.
Dadurch wird auch sichergestellt, dass die überschüssigen Datensätze auf der letzten Seite erfasst werden, insbesondere in Fällen, in denen die Anzahl der überschüssigen Datensätze geringer ist als die Anzahl der pro Seite anzuzeigenden Datensätze.
Schließlich haben Sie den Status mit der Eigenschaft currentPage auf 1 initialisiert. Sie benötigen diese Statuseigenschaft, um intern den Überblick über die aktuell aktive Seite zu behalten.
Als Nächstes erstellen Sie die Methode zur Erzeugung der Seitennummern.
Fügen Sie nach import aber vor der Klasse Pagination die folgenden Konstanten und die Funktion range hinzu:
Fügen Sie in der Klasse Pagination nach dem constructor die folgende Methode fetchPageNumbers hinzu:
Hier definieren Sie zunächst zwei Konstanten: LEFT _ PAGE und RIGHT _ PAGE.
Diese Konstanten werden verwendet, um Punkte anzugeben, an denen Sie Seitenkontrollen für das Verschieben nach links bzw. rechts haben.
Außerdem haben Sie eine Hilfsfunktion range () definiert, die Ihnen bei der Erstellung von Nummernbereichen helfen kann.
Der folgende Code-Ausschnitt zeigt den Unterschied zwischen der gerade definierten Funktion range () und der von Lodash:
Als Nächstes haben Sie die Methode fetchPageNumbers () in der Klasse Pagination definiert.
Diese Methode handhabt die Kernlogik für die Erstellung der Seitennummern, die auf der Paginierungssteuerung angezeigt werden sollen.
Sie möchten, dass die erste Seite und die letzte Seite immer sichtbar sind.
Zuerst haben Sie einige Variablen definiert. totalNumbers stellt die Gesamtzahl der Seitennummern dar, die auf der Steuerung angezeigt werden. totalBlocks steht für die Gesamtseitennummern, die angezeigt werden sollen, plus zwei zusätzliche Blöcke für die Indikatoren der linken und der rechten Seite.
Wenn totalPages nicht größer als totalBlocks ist, geben Sie einen Zahlenbereich von 1 bis totalPages zurück.
Andernfalls geben Sie den Bereich der Seitennummern mit LEFT _ PAGE und RIGHT _ PAGE an Punkten zurück, an denen die Seiten nach links bzw. rechts verschoben werden.
Beachten Sie jedoch, dass Ihre Paginierungssteuerung sicherstellt, dass die erste Seite und die letzte Seite immer sichtbar sind.
Die Steuerungen für die linke und rechte Seite erscheinen nach innen.
Jetzt fügen Sie die Methode render () hinzu, mit der Sie die Paginierungssteuerung rendern können.
Fügen Sie in der Klasse Pagination nach der Methode constructor und fetchPageNumbers die folgende Methode render hinzu:
Hier generieren Sie das Array der Seitennummern durch Aufruf der zuvor erstellten Methode fetchPageNumbers ().
Dann rendern Sie jede Seitennummer mit Array.prototype.map ().
Beachten Sie, dass Sie Click-Event-Handler für jede erstellte Seitennummer registrieren, um Klicks zu verarbeiten.
Beachten Sie auch, dass die Paginierungssteuerung nicht gerendert wird, wenn das Prop totalRecords nicht korrekt an die Komponente Pagination übergeben wurde oder in Fällen, in denen nur 1 Seite vorhanden ist.
Schließlich definieren Sie die Methoden für die Ereignishandler.
Fügen Sie in der Klasse Pagination nach der Methode constructor und fetchPageNumbers und der Methode render Folgendes hinzu:
Sie definieren die Methode gotoPage (), die den Status ändert und die currentPage auf die angegebene Seite setzt.
Sie stellt sicher, dass das Argument page einen Mindestwert von 1 und einen maximalen Wert der Gesamtzahl der Seiten hat.
Schließlich ruft sie die Funktion onPageChanged () auf, die als Prop übergeben wurde, wobei die Daten den neuen Paginierungsstatus angeben.
Beachten Sie, wie Sie (this.pageNeighbours * 2) in handleMoveLeft () und handleMoveRight () verwenden, um die Seitennummern basierend auf der aktuellen Seitennummer nach links bzw. rechts zu verschieben.
Hier ist eine Demonstration der Interaktion der Bewegung von links nach rechts.
Links-rechts-Bewegung der Interaktion
Sie haben nun die Komponente Pagination abgeschlossen.
Benutzer können mit der Navigationssteuerung in dieser Komponente interagieren, um verschiedene Seiten von Flaggen anzuzeigen.
Schritt 4 - Erstellen der Komponente App
Nachdem Sie nun eine Komponente CountryCard und Pagination haben, können Sie sie in Ihrer Komponente App verwenden.
Ändern Sie die Datei App.js im Verzeichnis src:
Ersetzen Sie den Inhalt von App.js durch die folgenden Codezeilen:
Hier initialisieren Sie den Status der Komponente App mit den folgenden Attributen:
allCountries - dies ist ein Array aller Länder in Ihrer Anwendung. Initialisiert auf ein leeres Array ([]).
currentCountries - dies ist ein Array aller Länder, die auf der aktuell aktiven Seite angezeigt werden sollen.
Initialisiert auf ein leeres Array ([]).
currentPage - die Seitennummer der aktuell aktiven Seite.
Initialisiert auf null.
totalPages - die Gesamtzahl der Seiten für alle Ländereinträge.
Als Nächstes rufen Sie in der Lebenszyklusmethode componentDidMount () alle Länder der Welt mit dem Paket countries-api ab, indem Sie Countries.findAll () abrufen.
Dann aktualisieren Sie den Status der Anwendung, indem Sie allCountries so einstellen, dass es alle Länder der Welt enthält.
Um mehr über das Paket zu erfahren, können Sie die Dokumentation countries-api einsehen.
Schließlich haben Sie die Methode onPageChanged () definiert, die jedes Mal aufgerufen wird, wenn Sie über die Paginierungssteuerung zu einer neuen Seite navigieren.
Diese Methode wird an das Prop onPageChanged der Komponente Pagination übergeben.
Es gibt zwei Zeilen, die bei dieser Methode beachtet werden sollten.
Die erste ist diese Zeile:
Der Wert offset gibt den Startindex zum Abrufen der Datensätze für die aktuelle Seite an.
Die Verwendung von (currentPage - 1) stellt sicher, dass der Offset null ist.
Nehmen wir zum Beispiel an, dass Sie 25 Datensätze pro Seite anzeigen, und Sie betrachten derzeit Seite 5. Dann ist der offset ((5 - 1) * 25 = 100).
Wenn Sie beispielsweise Datensätze bei Bedarf aus einer Datenbank abrufen, ist dies eine Beispiel-SQL-Abfrage, die Ihnen zeigt, wie Offset verwendet werden kann:
Da Sie keine Datensätze aus einer Datenbank oder einer externen Quelle abrufen, benötigen Sie eine Möglichkeit, den erforderlichen Teil der Datensätze zu extrahieren, der für die aktuelle Seite angezeigt werden soll.
Die zweite ist diese Zeile:
Hier verwenden Sie die Methode Array.prototype.slice (), um den erforderlichen Teil der Datensätze aus allCountries zu extrahieren, indem Sie offset als Startindex für Slice und (offset + pageLimit) als den Index, vor dem Slice beendet werden soll, übergeben.
Anmerkung: In diesem Tutorial haben Sie keine Datensätze aus einer externen Quelle abgerufen.
In einer realen Anwendung werden Sie wahrscheinlich Datensätze aus einer Datenbank oder einer API abrufen.
Die Logik zum Abrufen der Datensätze kann in die Methode onPageChanged () der Komponente App eingebunden werden.
Nehmen wir an, Sie haben einen fiktiven API-Endpunkt / api / countries? page = {current _ page} & limit = {page _ limit}.
Der folgende Ausschnitt zeigt, wie Sie mithilfe des HTTP-Pakets axios Länder bei Bedarf von der API abrufen können:
Jetzt können Sie die Komponente App beenden, indem Sie die Methode render () hinzufügen.
Fügen Sie in der Klasse App, jedoch nach componentDidMount und onPageChanged, die folgende Methode render hinzu:
In der Methode render () rendern Sie die Gesamtzahl der Länder, die aktuelle Seite, die Gesamtzahl der Seiten, die Steuerung < Pagination > und dann die < CountryCard > für jedes Land auf der aktuellen Seite.
Beachten Sie, dass Sie die zuvor definierte Methode onPageChanged () an das Prop onPageChanged der Steuerung < Pagination > übergeben haben.
Dies ist sehr wichtig für die Erfassung der Seitenänderungen aus der Komponente Pagination.
Außerdem zeigen Sie 18 Länder pro Seite an.
Zu diesem Zeitpunkt sieht die Anwendung wie der folgende Screenshot aus:
Screenshot der Anwendung mit 248 aufgelisteten Ländern und Seitennummern oben, um durch die einzelnen Seiten zu gehen
Sie haben nun eine Komponente App, die mehrere Komponenten CountryCard anzeigt, und eine Komponente Pagination, die den Inhalt in separate Seiten aufteilt.
Als Nächstes werden Sie das Styling Ihrer Anwendung erkunden.
Schritt 5 - Hinzufügen von benutzerdefinierten Stilen
Vielleicht haben Sie bemerkt, dass Sie den zuvor erstellten Komponenten einige benutzerdefinierte Klassen hinzugefügt haben.
Lassen Sie uns einige Stil-Regeln für diese Klassen in der Datei src / App.scss definieren.
Die Datei App.scss wird wie der folgende Ausschnitt aussehen:
Ändern Sie Ihre Datei App.js so, dass sie auf App.scss anstelle von App.css verweist.
Anmerkung: Weitere Informationen hierzu finden Sie in der Dokumentation zu Create React App.
Nach dem Hinzufügen der Stile wird die Anwendung nun wie der folgende Screenshot aussehen:
Screenshot der Anwendung, Seite 1 von 14, mit Stilen
Sie haben nun eine vollständige Anwendung mit zusätzlichem benutzerdefiniertem Styling.
Sie können benutzerdefinierte Stile verwenden, um alle Standardstile, die von Bibliotheken wie Bootstrap bereitgestellt werden, zu ändern und zu verbessern.
In diesem Tutorial haben Sie ein benutzerdefiniertes Paginierungs-Widget in Ihrer React-Anwendung erstellt.
Obwohl Sie in diesem Tutorial keine Aufrufe an eine API getätigt oder mit einem Datenbank-Backend interagiert haben, kann Ihre Anwendung solche Interaktionen erfordern.
Sie sind in keiner Weise auf den in diesem Tutorial verwendeten Ansatz beschränkt. Sie können ihn beliebig erweitern, um den Anforderungen Ihrer Anwendung gerecht zu werden.
Außerdem können Sie eine Live-Demo dieses Tutorials auf Code-Sandbox erhalten.
Implementieren eines sanften Bildlaufs in React
Was ist sanfter Bildlauf?
Es bedeutet, dass der Benutzer nicht auf eine Schaltfläche klickt und sofort zu einem anderen Teil der (selben) Seite geführt wird, sondern über eine Bildlaufanimation dorthin navigiert wird.
Es ist eine dieser subtilen Funktionen auf einer Seite, die einen ästhetischen Unterschied ausmacht.
3602
Sanfter Bildlauf bedeutet, dass der Benutzer nicht auf eine Schaltfläche klickt und sofort zu einem anderen Teil der (selben) Seite geführt wird, sondern über eine Bildlaufnavigation dorthin navigiert wird.
Es ist eine dieser subtilen Funktionen der Benutzeroberfläche auf einer Seite, die einen ästhetischen Unterschied ausmacht.
In diesem Artikel verwenden Sie das Paket react-scroll auf npm zur Implementierung des sanften Bildlaufs.
Zur Absolvierung dieses Tutorials benötigen Sie Folgendes:
Eine gültige Git-Installation.
Um dies einzurichten, lesen Sie das Tutorial Erste Schritte mit Git.
Lokal installiertes Node.js. Dies können Sie tun, indem Sie dem Tutorial Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung folgen.
Dieses Tutorial wurde mit Node v13.14.0, npm v6.14.5, react v16.13.1 und react-scroll v.1.7.16 verifiziert.
Schnellstart: Verwenden von react-scroll
In diesem Tutorial erstellen Sie eine einfache Anwendung. Wenn Sie aber einen schnellen Überblick über die Funktionsweise von react-scroll erhalten möchten, können Sie diese komprimierten Schritte nachschlagen:
Installieren Sie react-scroll:
Importieren Sie das Paket react-scroll:
Fügen Sie die Link-Komponente hinzu.
Die Komponente < Link / > verweist auf einen bestimmten Bereich Ihrer Anwendung:
Sehen wir uns dies genauer an und erstellen eine kleine React-Anwendung mit sanftem Bildlauf.
Schritt 1 - Installieren und Ausführen einer React-Anwendung
Der Einfachheit halber verwendet dieses Tutorial ein React-Startprojekt (unter Verwendung von Create React App 2.0), das eine Navigationsleiste (oder navbar) am oberen Rand sowie fünf verschiedene Inhaltsbereiche aufweist.
Die Links in der Navigationsleiste sind an dieser Stelle nur Anker-Tags. Sie werden Sie jedoch in Kürze aktualisieren, um einen sanften Bildlauf zu ermöglichen.
Sie finden das Projekt unter React With Smooth Scrolling.
Bitte beachten Sie, dass dieser Link für den Start-Zweig ist.
Der Haupt-Zweig enthält alle abgeschlossenen Änderungen.
GitHub Repo-Screenshot
Um das Projekt zu klonen, können Sie den folgenden Befehl verwenden:
Wenn Sie sich das Verzeichnis src / Components ansehen, finden Sie eine Datei Navbar.js, die die < Navbar > mit nav-items enthält, die fünf verschiedene < Section > s entsprechen.
Wenn Sie dann die Datei App.js im Verzeichnis src öffnen, sehen Sie, wo die < Navbar > zusammen mit den fünf tatsächlichen < Section > s enthalten ist.
Jede Komponente < Section > nimmt einen title und einen subtitle auf.
Da in diesem Projekt in den verschiedenen Abschnitten Dummytext verwendet wird, wurde dieser Text zur Reduzierung von sich wiederholendem Code einer Datei DummyText.js hinzugefügt, importiert und an jede Komponente < Section > übergeben.
Um die Anwendung auszuführen, können Sie die folgenden Befehle verwenden.
Dadurch wird die Anwendung im Entwicklungsmodus gestartet und automatisch aktualisiert, wenn Sie eine Ihrer Dateien speichern.
Sie können sie im Browser unter localhost: 3000 anzeigen.
Screenshot der Anwendung im Browser
Schritt 2 - Installieren und Konfigurieren von React-Scroll
Jetzt ist es an der Zeit, das Paket react-scroll zu installieren und diese Funktionalität hinzuzufügen.
Informationen zu dem Paket finden Sie auf npm.
Das Paket react-scroll auf npm
Um das Paket zu installieren, führen Sie den folgenden Befehl aus:
Öffnen Sie als Nächstes die Sicherungskopie der Datei Navbar.js und fügen Sie einen import für zwei benannte Importe, Link und animateScroll hinzu.
Beachten Sie, dass ich animatedScroll zur einfacheren Verwendung zu scroll aliasiert habe.
Nachdem alle Ihre Importe definiert sind, können Sie nun Ihre nav-items aktualisieren, um die Komponente < Link > zu verwenden.
Diese Komponente nimmt mehrere Eigenschaften an.
Sie können auf der Dokumentationsseite über alle diese Eigenschaften nachlesen.
Achten Sie vorerst besonders auf activeClass, to, spy, smooth, offset und duration.
activeClass - die Klasse, die bei Erreichen des Elements angewendet wird.
to - das Ziel, zu dem geblättert werden soll.
smooth - um den Bildlauf zu animieren.
offset - um zusätzliche px zu scrollen (wie Padding).
duration - die Zeit der Bildlaufanimation.
Dies kann eine Zahl oder eine Funktion sein.
Die Eigenschaft to ist der wichtigste Teil, da sie der Komponente mitteilt, zu welchem Element gescrollt werden soll.
In diesem Fall ist dies jede Ihrer < Section > s.
Mit der Eigenschaft offset können Sie eine zusätzliche Anzahl von Bildläufen definieren, um zu jeder < Section > zu gelangen.
Hier ist ein Beispiel für die Eigenschaften, die Sie für jede Komponente < Link > verwenden werden.
Der einzige Unterschied zwischen ihnen ist die Eigenschaft to, da sie jeweils auf eine andere < Section > verweist.
Sie müssen jedes der nav-items entsprechend aktualisieren.
Wenn Sie diese hinzugefügt haben, sollten Sie in der Lage sein, zu Ihrem Browser zurückzukehren (Ihre Anwendung sollte bereits automatisch neu gestartet worden sein) und einen sanften Bildlauf zu sehen.
Schritt 3 - Gestaltung von aktiven Links
Die Eigenschaft activeClass ermöglicht es Ihnen, eine Klasse zu definieren, die auf die Komponente < Link > angewendet wird, wenn ihr Element to aktiv ist.
Ein < Link > wird als aktiv betrachtet, wenn sein Element to in der Nähe des oberen Seitenrandes sichtbar ist.
Dies kann durch Klicken auf den < Link > selbst oder durch manuelles herunterscrollen zu < Section > ausgelöst werden.
Um dies zu beweisen, öffnete ich die Chrome DevTools und inspizierte den fünften < Link >, wie nachfolgend gezeigt.
Als ich auf diesen < Link > klickte oder manuell bis um Ende der Seite scrollte, bemerkte ich, dass die aktive Klasse tatsächlich angewendet wird.
Browseransicht der React-Anwendung
Um dies auszunutzen, können Sie eine aktive Klasse erstellen und dem Link eine Unterstreichung hinzufügen.
Sie können dieses Stück CSS zu der Datei App.css im Verzeichnis src hinzufügen:
Wenn Sie nun zu Ihrem Browser zurückkehren und um ein wenig herumscrollen, sollten Sie sehen, dass der entsprechende < Link > unterstrichen ist.
Aktualisierte Browseransicht der React-Anwendung
Schritt 4 - Hinzufügen zusätzlicher Funktionen
Für einen letzten Teil des Inhalts bietet dieses Paket auch einige Funktionen, die direkt aufgerufen werden können, wie scrollToTop, scrollToBottom, usw., sowie verschiedene Ereignisse, die Sie handhaben können.
In Bezug auf diese Funktionen führt das Anwendungslogo in einer Navigationsleiste den Benutzer zu der Startseite oder zum Anfang der aktuellen Seite.
Als einfaches Beispiel dafür, wie eine dieser bereitgestellten Funktionen aufgerufen wird, habe ich dem nav-logo einen Click-Handler hinzugefügt, den den Benutzer wie folgt zu zum Anfang der Seite scrollt:
Zurück im Browser sollten Sie in der Lage sein, auf der Seite nach unten zu scrollen, auf das Logo in der Navigationsleiste zu klicken und wieder an den Anfang der Seite zurückgeleitet zu werden.
Der sanfte Bildlauf ist eine dieser Funktionen, die Ihrer Anwendung einen hohen ästhetischen Wert verleihen können.
Das Paket react-scroll ermöglicht Ihnen, diese Funktion ohne nennenswerten Overhead zu nutzen.
In diesem Tutorial haben Sie einer Anwendung einen sanften Bildlauf hinzugefügt und mit verschiedenen Einstellungen experimentiert.
Wenn Sie neugierig geworden sind, verbringen Sie etwas Zeit damit, die anderen Funktionen und Ereignisse zu erkunden, die dieses Paket zu bieten hat.
filter () Array-Metode in JavaScript
Verwenden Sie filter () auf Arrays, um ein Array zu durchsuchen und ein neues Array mit den Elementen zurückzugeben, die die Filterregeln erfüllen.
4539
Die Array-Methode filter () erstellt aus einem vorhandenen Array ein neues Array mit Elementen, die unter ein gegebenes Kriterium fallen:
Das obige Beispiel nimmt das Array numbers und gibt ein neues gefiltertes Array mit nur jenen Werten zurück, die größer als sieben sind.
Syntax des Filters
Das Argument < ^ > item < ^ > ist eine Referenz auf das aktuelle Element im Array, da filter () es gegen die Bedingung < ^ > condition < ^ > prüft.
Dies ist im Fall von Objekten für den Zugriff auf Eigenschaften nützlich.
Wenn das aktuelle < ^ > item < ^ > die Bedingung erfüllt, wird es an das neue Array gesendet.
Filtern eines Arrays von Objekten
Ein üblicher Anwendungsfall von .filter () ist die Verwendung eines Arrays von Objekten über ihre Eigenschaften:
Zusätzliche Informationen
Weitere Details zu filter () finden Sie in der MDN-Referenz.
Filter ist nur eine von mehreren Iterationsmethoden für Arrays in JavaScript. Lesen Sie Verwenden von Array-Iterationsmethoden in JavaScript, um mehr über die anderen Methoden wie map () und reduce () zu erfahren.
So starten Sie Ihre Node.js-Apps automatisch mit nodemon neu
In Node.js müssen Sie den Prozess neu starten, um Änderungen zu übernehmen.
Dadurch wird Ihrem Workflow ein zusätzlicher Schritt hinzugefügt, um die Änderungen durchzuführen.
Sie können diesen zusätzlichen Schritt durch Verwendung von nodemon eliminieren, um den Prozess automatisch neu zu starten.
In diesem Artikel erfahren Sie mehr über die Installation, Einrichtung und Konfiguration von nodemon.
4931
nodemon ist ein von @ rem entwickeltes CLI-Dienstprogramm (Command Line Interface), das Ihre Node-App umschließt, das Dateisystem überwacht und den Prozess automatisch neu startet.
Wenn Sie diesem Artikel folgen möchten, benötigen Sie Folgendes:
Lokal installiertes Node.js. Folgen Sie hierfür Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung.
Schritt 1 - Installieren von nodemon
Zuerst müssen Sie nodemon auf Ihrem Rechner installieren.
Installieren Sie das Dienstprogramm entweder global oder lokal mit npm oder Yarn:
Globale Installation
Sie können nodemon global mit npm installieren:
Oder mit Yarn:
Lokale Installation
Sie können nodemon auch lokal mit npm installieren.
Bei der Ausführung einer lokalen Installation können wir nodemon als dev-Abhängigkeiten mit --save-dev (oder --dev) installieren:
Eine Sache, die Sie bei einer lokalen Installation wissen sollten, ist, dass Sie den Befehl nodemon nicht direkt aus der Befehlszeile verwenden können:
Sie können es jedoch als Teil von einigen npm Scripts oder mit npx verwenden.
Dadurch wird der Prozess der Installation von nodemon abgeschlossen.
Als Nächstes verwenden wir nodemon mit unseren Projekten.
Schritt 2 - Einrichten eines Beispiel-Express-Projekts mit nodemon
Wir können nodemon verwenden, um ein Node Script zu starten.
Wenn wir beispielsweise ein Express-Server-Setup in einer server.js-Datei haben, können wir es starten und für Änderungen wie folgt ansehen:
Sie können Argumente so übergeben, als ob Sie das Script mit Node ausführen:
Jedesmal, wenn Sie eine Änderung in einer Datei mit einer der Standarderweiterung (.js, .mjs, .json, .coffee oder .litcoffee) im aktuellen Verzeichnis oder einem Unterverzeichnis vornehmen, wird der Prozess neu starten.
Nehmen wir an, wir schreiben eine Beispieldatei server.js, die die Nachricht ausgibt: Dolphin-App hört auf Port ${port} zu.
Wir können das Beispiel mit nodemon ausführen:
Wir sehen die Terminalausgabe:
Zwar wird nodemon noch immer ausgeführt, doch lassen Sie uns eine Änderung in der Datei server.js vornehmen, um die Nachricht auszugeben: < ^ > Shark < ^ > -App hört auf Port ${port} zu!
Wir sehen die folgende zusätzliche Terminalausgabe:
Die Terminalausgabe aus unserer Node.js-App wird wie erwartet angezeigt.
Sie können den Prozess jederzeit neu starten, indem Sie rs eingeben und die ENTER drücken.
Alternativ sucht nodemon auch nach einer Hauptdatei, die in der Datei package.json Ihres Projekts angegeben ist:
Oder ein Startskript:
Sobald Sie die Änderungen an package.json vornehmen, können Sie nodemon aufrufen, um die Beispiel-App im Beobachtungsmodus zu starten, ohne dass Sie server.js übergeben müssen.
Schritt 3 - Verwenden von Optionen
Sie können die Konfigurationseinstellungen für nodemon ändern.
Gehen wir über einige der wichtigsten Optionen:
--exec: Verwenden Sie den Schalter --exec, um ein Binärsystem anzugeben, mit dem die Datei ausgeführt werden soll.
In Kombination mit der Binärdatei ts-node kann --exec beispielsweise nützlich werden, um Änderungen zu beobachten und TypeScript-Dateien auszuführen.
--ext: Geben Sie verschiedene Dateierweiterungen an, um zu beobachten.
Stellen Sie für diesen Schalter eine mit Komma getrennte Liste der Dateierweiterungen (z. B. -ext js, ts) bereit.
--delay: Standardmäßig wartet nodemon eine Sekunde, um den Prozess neu zu starten, wenn sich eine Datei ändert, aber mit dem Schalter --delay können Sie eine andere Verzögerung angeben.
Beispielsweise nodemon --delay 3.2 für eine 3,2-Sekunden-Verzögerung.
--watch: Verwenden Sie den Schalter --watch, um mehrere Verzeichnisse oder Dateien anzugeben, die Sie beobachten können.
Fügen Sie für jedes Verzeichnis, das Sie beobachten möchten, einen --watch-Schalter hinzu.
Standardmäßig werden das aktuelle Verzeichnis und seine Unterverzeichnisse beobachtet, sodass Sie mit --watch die Beobachtung auf nur bestimmte Unterverzeichnisse oder Dateien beschränken können.
--ignore: Verwenden Sie den Schalter --ignore, um bestimmte Dateien, Dateimuster oder Verzeichnisse zu ignorieren.
--verbose: Eine ausführlichere Ausgabe mit Informationen darüber, welche Datei (en) geändert wurde (n), um einen Neustart auszulösen.
Sie können mit dem folgenden Befehl alle verfügbaren Optionen anzeigen:
Durch Verwendung dieser Optionen erstellen wir den Befehl, um das folgende Szenario zu erfüllen:
Beobachten des Server-Verzeichnisses
Spezifizieren von Dateien mit einer .ts-Erweiterung
Ignorieren von Dateien mit einer .test.ts-Endung
Ausführung der Datei (server / server.ts) mit ts-node
warten für drei Sekunden bis zum Neustart nach einer Dateiänderung
Dieser Befehl kombiniert --watch, --ext, --exec, --ignore und --delay-Optionen, um die Bedingungen für unser Szenario zu erfüllen.
Schritt 4 - Verwenden von Konfigurationen
Im vorherigen Beispiel kann das Hinzufügen von Konfigurationsschaltern bei der Ausführung von nodemon ziemlich mühsam werden.
Eine bessere Lösung für Projekte, die spezifische Konfigurationen benötigen, ist die Angabe dieser Konfigurationen in einer Datei nodemon.json.
Beispielsweise sind hier die gleichen Konfigurationen wie bei der vorherigen Befehlszeile, aber in einer Datei nodemon.json platziert:
Beachten Sie die Verwendung von execMap anstelle des Schalters --exec. execMap ermöglicht es Ihnen, Binärdateien anzugeben, die bei bestimmten Dateierweiterungen verwendet werden sollten.
Wenn Sie Ihrem Projekt lieber keine Konfigurationsdatei nodemon.json hinzufügen möchten, können Sie alternativ diese Konfigurationen unter einem Schlüssel nodemonConfig der Datei package.json hinzufügen:
Sobald Sie die Änderungen an entweder nodemon.json oder package.json vornehmen, können Sie nodemon mit dem gewünschten Script starten:
nodemon nimmt die Konfigurationen auf und verwendet sie.
Auf diese Weise können Ihre Konfigurationen gespeichert, geteilt und wiederholt werden, um Fehler beim Kopieren und Einfügen oder Tippfehler in der Befehlszeile zu vermeiden.
In diesem Artikel haben Sie erkundet, wie Sie nodemon mit Ihren Node.js-Anwendungen verwenden.
Dieses Tool hilft dabei, den Prozess des Anhaltens und Startens eines Node-Servers zu automatisieren, um die Änderungen anzuzeigen.
Weitere Informationen zu den verfügbaren Funktionen und Fehlerbehebungen finden Sie in der offiziellen Dokumentation.
Wenn Sie mehr über Node.js erfahren möchten, lesen Sie unsere Themenseite zu Node.js für Übungen und Programmierprojekte.
Wie man einen Parallax Scrolling-Effekt mit Pure CSS in Chrome erstellt.
In diesem Leitfaden werden Sie einige CSS-Zeilen einrichten, um einen scrolling parallax-Effekt auf einer Webseite zu erzeugen.
4882
Modernes CSS ist ein leistungsstarkes Werkzeug, mit dem Sie viele erweiterte Funktionen der Benutzeroberfläche (UI) erstellen können.
In der Vergangenheit waren diese Funktionen auf JavaScript-Bibliotheken angewiesen.
Sie werden Bilder aus placekitten.com als Platzhalter-Hintergrundbilder verwenden.
Sobald Sie das Tutorial abgeschlossen haben, haben Sie eine Webseite mit einem reinen CSS scrolling parallax-Effekt.
Warnung: Dieser Artikel verwendet experimentelle CSS-Eigenschaften, die nicht browserübergreifend funktionieren.
Dieses Projekt wurde getestet und funktioniert auf Chrome.
Diese Technik funktioniert nicht gut in Firefox, Safari und iOS aufgrund der Optimierungen einiger dieser Browser.
Schritt 1 - Erstellen eines neuen Projekts
In diesem Schritt verwenden Sie die Befehlszeile, um einen neuen Projektordner und -Dateien einzurichten.
Öffnen Sie zunächst Ihr Terminal und erstellen Sie einen neuen Projektordner.
Geben Sie den folgenden Befehl ein, um den Projektordner zu erstellen.
In diesem Fall haben Sie den Ordner css-parallax bezeichnet.
Wechseln Sie nun in den Ordner css-parallax:
Erstellen Sie als Nächstes eine index.html -Datei in Ihrem Ordner css-parallax mit dem Befehl nano:
Sie werden das gesamte HTML für das Projekt in diese Datei einfügen.
Im nächsten Schritt beginnen Sie, die Struktur der Webseite zu erstellen.
Schritt 2 - Einrichten der Anwendungsstruktur
In diesem Schritt fügen Sie das HTML hinzu, das benötigt wird, um die Struktur des Projekts zu erstellen.
In Ihrer Datei index.html fügen Sie den folgenden Code hinzu.
Dies ist die grundlegende Struktur der meisten Webseiten, die HTML verwenden.
Fügen Sie den folgenden Code in die < body > Funktion ein:
Dieser Code erstellt drei verschiedene Abschnitte.
Zwei werden ein Hintergrundbild haben, und einer wird ein statischer, einfacher Hintergrund sein.
In den nächsten Schritten fügen Sie die Stile für jeden Abschnitt mit den Klassen hinzu, die Sie in HTML verwenden.
Schritt 3 - Erstellen einer CSS-Datei und Hinzufügen von Initial-CSS
In diesem Schritt erstellen Sie eine CSS-Datei.
Dann fügen Sie die Initial-CSS hinzu, die benötigt wird, um die Website zu gestalten und den Parallax-Effekt zu erstellen.
Erstellen Sie als Nächstes eine styles.css -Datei in Ihrem Ordner css-parallax mit dem Befehl nano:
Hier legen Sie alle CSS an, die für den Parallaxen-Scrolling-Effekt erforderlich sind.
Als Nächstes beginnen Sie mit der Klasse .wrapper.
In Ihrer Datei styles.css fügen Sie den folgenden Code hinzu:
Die Klasse .wrapper legt die Perspektive und die Scroll-Eigenschaften für die gesamte Seite fest.
Die Höhe des Wrappers muss auf einen festen Wert eingestellt werden, damit der Effekt funktioniert.
Sie können die viewport-Einheit vh auf 100 einstellen, um die volle Höhe des Ansichtsfensters des Bildschirms zu erhalten.
Wenn Sie die Bilder skalieren, fügen Sie dem Bildschirm eine horizontale Bildlaufleiste hinzu, so dass Sie diese deaktivieren können, indem Sie overflow-x: hidden; hinzufügen.
Die perspektivische Eigenschaft simuliert den Abstand vom Viewport zu den Pseudo-Elementen, die Sie im CSS erstellen und weiter transformieren.
Im nächsten Schritt fügen Sie weitere CSS hinzu, um Ihre Webseite zu gestalten.
Schritt 4 - Hinzufügen von Stilen für die .section Klasse
In diesem Schritt fügen Sie Stile der Klasse .section hinzu.
In Ihrer Datei styles.css fügen Sie den folgenden Code unterhalb der wrapper-Klasse hinzu:
Die Klasse .section definiert die Größe, Anzeige und Texteigenschaften für die Hauptabschnitte.
Legen Sie eine Position relativ fest, damit das untergeordnete Element .parallax:: after absolut bezogen auf das übergeordnete Element .section positioniert werden kann.
Jeder Abschnitt hat eine Ansicht-Höhe (vh) von 100, um die volle Höhe des Ansichtsfensters einzunehmen.
Dieser Wert kann geändert und für jeden Abschnitt auf die von Ihnen gewünschte Höhe eingestellt werden.
Schließlich werden die restlichen CSS-Eigenschaften verwendet, um den Text innerhalb jedes Abschnitts zu formatieren und zu gestalten.
Das positioniert den Text in der Mitte jedes Abschnitts und fügt eine Farbe Weiß hinzu.
Als Nächstes fügen Sie ein Pseudo-Element hinzu und gestalten es so aus, dass der Parallaxen-Effekt in zwei Bereichen in Ihrem HTML erstellt wird.
Schritt 5 - Hinzufügen von Stilen für die .parallax -Klasse
In diesem Schritt fügen Sie Stile der .parallax -Klasse hinzu.
Zuerst fügen Sie ein Pseudo-Element in der zu gestaltenden .parallax -Klasse hinzu.
Anmerkung: Sie können MDN-Web-Dokumente aufrufen, um mehr über CSS zu erfahren.
Fügen Sie den folgenden Code unterhalb der .section -Klasse hinzu:
Die .parallax -Klasse fügt ein:: after -Pseudo-Element dem Hintergrundbild hinzu und liefert die für den Parallaxen-Effekt erforderlichen Transformationen.
Das Pseudo-Element ist das untergeordnetste Element mit der Klasse des .parallax.
Die erste Hälfte des Codes zeigt und positioniert das Pseudo-Element.
Die transformierende Eigenschaft verschiebt das Pseudo-Element zurück von der Kamera auf dem z-index und skaliert es dann wieder nach oben, um das Ansichtsfenster auszufüllen.
Da das Pseudo-Element weiter entfernt ist, scheint es sich langsamer zu bewegen.
Im nächsten Schritt fügen Sie die Hintergrundbilder und den statischen Hintergrundstil hinzu.
Schritt 6 - Hinzufügen der Bilder und des Hintergrunds für jeden Abschnitt
In diesem Schritt ergänzen Sie die endgültigen CSS-Eigenschaften, um die Hintergrundbilder und die Hintergrundfarbe des statischen Abschnitts hinzuzufügen.
Fügen Sie zunächst eine einfarbige Hintergrundfarbe dem .static Abschnitt mit dem folgenden Code nach der Klasse .parallax:: after hinzu.
Die .static Klasse fügt dem statischen Abschnitt, der kein Bild hat, einen Hintergrund hinzu.
Die beiden Abschnitte mit der .parallax -Klasse haben auch eine zusätzliche Klasse, die für jeden Bereich anders ist.
Verwenden Sie die Klassen .bg1 und .bg2, um die Kätzchen-Hintergrundbilder hinzuzufügen.
Fügen Sie den folgenden Code unterhalb der .static Klasse hinzu:
Die .bg1, .bg2-Klassen fügen die jeweiligen Hintergrundbilder für jeden Abschnitt hinzu.
Die Bilder sind aus der placekitten -Webseite.
Es handelt sich um einen Bilderservice von Kätzchen zur Verwendung als Platzhalter.
Nachdem nun der gesamte Code für den Parallax Scrolling-Effekt hinzugefügt wurde, können Sie Ihre Datei styles.css mit Ihrer index.html verlinken.
Schritt 7 - Verknüpfen von styles.css und Öffnen von index.html in Ihrem Browser
In diesem Schritt verknüpfen Sie Ihre Datei styles.css und öffnen das Projekt in Ihrem Browser, um den Parallax Scrolling-Effekt zu sehen.
Fügen Sie zunächst den folgenden Code dem < head > -Tag in der Datei index.html hinzu.
Jetzt können Sie Ihre Datei index.html in Ihrem Browser öffnen:
Scrolling Parallax-Effekt-GIF
Damit haben Sie eine funktionierende Webseite mit einem Scrolling-Effekt eingerichtet.
Sehen Sie sich dieses GitHub-Repository an, um den vollen Code zu sehen.
In diesem Artikel haben Sie ein Projekt mit einer index.html und einer Datei styles.css eingerichtet und haben nun eine funktionsfähige Webseite.
Sie haben die Struktur Ihrer Webseite hinzugefügt und Stile für die verschiedenen Abschnitte auf der Seite erstellt.
Es ist möglich, die von Ihnen verwendeten Bilder oder den Parallax-Effekt weiter weg zu legen, damit sie sich langsamer bewegen.
Sie müssen die Pixelmenge in den Eigenschaften perspective und transform ändern.
Wenn Sie nicht möchten, dass ein Hintergrundbild überhaupt scrollen soll, verwenden Sie background-attachment: fixed; anstelle von perspective / translate / scale.
Verwenden von SSH zum Herstellen einer Verbindung mit einem Remoteserver
SSH ist ein wichtiges Tool für die Verwaltung von Remote-Linux-Servern.
In diesem Leitfaden diskutieren wir die grundlegende Verwendung dieses Dienstprogramms und die Konfiguration Ihrer SSH-Umgebung.
579
Ein wesentliches Tool, das ein Systemadministrator beherrschen sollte, ist SSH.
SSH oder Secure Shell ist ein Protokoll, das zur sicheren Anmeldung bei Remotesystemen verwendet wird.
Es ist die häufigste Methode für den Zugriff auf Remote-Linux-Server.
In diesem Leitfaden diskutieren wir, wie SSH zur Herstellung einer Verbindung mit einem Remotesystem genutzt werden kann.
Grundlegende Syntax
Um per SSH eine Verbindung mit einem Remotesystem herzustellen, verwenden wir den Befehl ssh.
Die grundlegendste Form des Befehls ist:
Der < ^ > remote _ host < ^ > in diesem Beispiel ist die IP-Adresse oder der Domänenname, mit der oder dem Sie eine Verbindung herstellen möchten.
Dieser Befehl geht davon aus, dass Ihr Benutzername im Remotesystem der gleiche wie Ihr Benutzername in Ihrem lokalen System ist.
Wenn Ihr Benutzername im Remotesystem anders ist, können Sie ihn durch Verwendung dieser Syntax angeben:
Sobald Sie mit dem Server verbunden sind, werden Sie möglicherweise aufgefordert, Ihre Identität durch Angabe eines Passworts zu verifizieren.
Später werden wir auf die Erstellung von Schlüsseln zur Verwendung anstelle von Passwörtern eingehen.
Um die SSH-Sitzung zu beenden und zu Ihrer lokalen Shell-Sitzung zurückzukehren, geben Sie Folgendes ein:
Wie funktioniert SSH?
SSH arbeitet, indem ein Clientprogramm mit einem SSH-Server namens sshd verbunden wird.
Im vorherigen Abschnitt war ssh das Clientprogramm.
Der SSH-Server wird bereits auf dem von uns angegebenen remote _ host ausgeführt.
Auf Ihrem Server sollte der sshd-Server bereits ausgeführt werden.
Wenn dies nicht der Fall ist, müssen Sie möglicherweise über eine webbasierte Konsole oder eine lokale serielle Konsole auf Ihren Server zugreifen.
Der Prozess, der zum Starten eines SHH-Servers erforderlich ist, hängt von der Linux-Distribution ab, die Sie verwenden.
Unter Ubuntu können Sie den SSH-Server durch folgende Eingabe starten:
Damit sollte der sshd-Server starten und Sie können sich remote anmelden.
Konfigurieren von SSH
Wenn Sie die Konfiguration von SSH ändern, ändern Sie die Einstellungen des sshd-Servers.
In Ubuntu befindet sich die Hauptkonfigurationsdatei von sshd unter / etc / ssh / sshd _ config.
Sichern Sie vor der Bearbeitung die aktuelle Version dieser Datei:
Öffnen Sie sie mit einem Texteditor:
Die meisten Optionen in dieser Datei werden Sie nicht verändern wollen.
Es gibt jedoch einige Optionen, auf die Sie möglicherweise einen Blick werfen möchten:
Die Portdeklaration gibt an, an welchem Port der sshd-Server nach Verbindungen lauschen wird.
Standardmäßig ist dies Port 22. Sie sollten diese Einstellung wahrscheinlich unverändert lassen, es sei denn, Sie haben spezifische Gründe für ein anderes Vorgehen.
Wenn Sie Ihren Port tatsächlich ändern, zeigen wir Ihnen später, wie Sie eine Verbindung mit dem neuen Port herstellen können.
Die Hostschlüsseldeklarationen geben an, wo nach globalen Hostschlüsseln gesucht werden soll.
Wir werden später erörtern, was ein Hostschlüssel ist.
Diese beiden Elemente zeigen die Protokollierungsstufe an, die ausgeführt werden sollte.
Wenn Sie Probleme mit SSH haben, ist die Erhöhung der Protokollierungsstufe ggf. eine gute Möglichkeit, um zu ermitteln, was das Problem ist.
Diese Parameter geben einige der Anmeldedaten an.
LoginGraceTime gibt an, wie viele Sekunden die Verbindung ohne eine erfolgreiche Anmeldung aufrechterhalten wird.
Es ist ggf. eine gute Idee, diese Zeit nur etwas höher als die Zeit anzusetzen, die Sie normalerweise zum Anmelden benötigen.
PermitRootLogin gibt an, ob sich der root-Benutzer anmelden darf.
In den meisten Fällen sollte dies in no geändert werden, wenn Sie ein Benutzerkonto erstellt haben, das Zugriff auf erhöhte Berechtigungen hat (über su oder sudo) und sich über SSH anmelden kann.
strictModes ist ein Sicherheitswächter, der Anmeldeversuche verweigert, wenn die Authentifizierungsdateien für alle lesbar sind.
Dadurch werden Anmeldeversuche verhindert, wenn Konfigurationsdateien nicht sicher sind.
Diese Parameter konfigurieren eine Funktion namens X11 Forwarding.
Dadurch können Sie die grafische Benutzeroberfläche (GUI) eines Remotesystems im lokalen System anzeigen.
Diese Option muss auf dem Server aktiviert und mit dem SSH-Client während der Verbindung mit der Option -X übergeben werden.
Speichern und schließen Sie nach dem Vornehmen Ihrer Änderungen die Datei durch Eingabe von STRG + X und Y gefolgt von ENTER.
Wenn Sie Einstellungen in / etc / ssh / sshd _ config geändert haben, stellen Sie sicher, dass Sie Ihren sshd-Server zur Implementierung der Änderungen neu laden:
Sie sollten Ihre Änderungen sorgfältig testen, um sicherzustellen, dass sie wie erwartet funktionieren.
Es ist möglicherweise eine gute Idee, beim Vornehmen von Änderungen einige aktive Sitzungen zu haben.
Dadurch können Sie die Konfiguration bei Bedarf zurücksetzen.
Anmelden bei SSH mit Schlüsseln
Es ist zwar nützlich, sich mit Passwörtern bei einem Remotesystem anmelden zu können, doch ist es eine wesentlich bessere Idee, schlüsselbasierte Authentifizierung einzurichten.
Wie funktioniert schlüsselbasierte Authentifizierung?
Schlüsselbasierte Authentifizierung funktioniert durch Erstellen eines Schlüsselpaars, bestehend aus einem privaten Schlüssel und einem öffentlichen Schlüssel.
Der private Schlüssel befindet sich auf dem Clientrechner, ist gesichert und wird geheim gehalten.
Der öffentliche Schlüssel kann an beliebige Personen weitergegeben oder auf jedem Server platziert werden, auf den Sie zugreifen möchten.
Wenn Sie versuchen, eine Verbindung mit einem Schlüsselpaar herzustellen, verwendet der Server den öffentlichen Schlüssel zur Erstellung einer Nachricht für den Clientcomputer, die nur mit dem privaten Schlüssel gelesen werden kann.
Der Clientcomputer sendet dann die entsprechende Antwort zurück an den Server und der Server weiß, dass der Client legitim ist.
Das gesamte Verfahren erfolgt nach Einrichtung der Schlüssel automatisch.
Erstellen von SSH-Schlüsseln
SSH-Schlüssel sollten auf dem Computer erstellt werden, von dem aus Sie sich anmelden möchten.
Dies ist normalerweise Ihr lokaler Rechner.
Geben Sie Folgendes in die Befehlszeile ein:
Drücken Sie die Eingabetaste zum Akzeptieren der Standardeinstellungen.
Ihre Schlüssel werden unter ~ / .ssh / id _ rsa.pub und ~ / .ssh / id _ rsa erstellt.
Wechseln Sie durch folgende Eingabe zum Verzeichnis .ssh:
Sehen Sie sich die Berechtigungen der Dateien an:
Wie Sie sehen können, ist die Datei id _ rsa nur für den Besitzer lesbar und beschreibbar.
So sollte es sein, um sie geheim zu halten.
Die Datei id _ rsa.pub kann jedoch weitergegeben werden und verfügt über entsprechende Berechtigungen.
Übertragen Ihres öffentlichen Schlüssels auf den Server
Wenn Sie derzeit über passwortbasierten Zugriff auf einen Server verfügen, können Sie durch Eingabe des folgenden Befehls Ihren öffentlichen Schlüssel auf den Server kopieren:
Dadurch wird eine SSH-Sitzung gestartet.
Nach der Eingabe Ihres Passworts wird Ihr öffentlicher Schlüssel in die autorisierte Schlüsseldatei des Servers kopiert, sodass Sie sich beim nächsten Mal ohne das Passwort anmelden können.
Optionen auf der Clientseite
Es gibt eine Reihe von optionalen Flags, die Sie beim Verbinden über SSH wählen können.
Einige davon könnten erforderlich sein, um mit den Einstellungen in der sshd-Konfiguration des Remote-Hosts übereinzustimmen.
Wenn Sie beispielsweise die Portnummer in Ihrer sshd-Konfiguration geändert haben, müssen Sie durch folgende Angabe eine Anpassung an den Port auf der Clientseite vornehmen:
Wenn Sie nur einen einzigen Befehl in einem Remotesystem ausführen möchten, können Sie ihn nach dem Host wie folgt angeben:
Sie werden eine Verbindung mit dem Remoterechner herstellen und sich authentifizieren und der Befehl wird ausgeführt.
Wie bereits gesagt: Wenn auf beiden Computern X11 Forwarding aktiviert ist, können Sie durch folgende Eingabe auf diese Funktion zugreifen:
Wenn Sie über die entsprechenden Tools auf Ihrem Computer verfügen, öffnen die GUI-Programme, die Sie im Remotesystem verwenden, ihre Fenster nun auf Ihrem lokalen System.
Deaktivieren der Passwortauthentifizierung
Wenn Sie SSH-Schlüssel erstellt haben, können Sie durch Deaktivieren der ausschließlich passwortbasierten Authentifizierung die Sicherheit Ihres Servers erhöhen.
Abgesehen von der Konsole ist die einzige Möglichkeit zur Anmeldung bei Ihrem Server der private Schlüssel, der zum öffentlichen Schlüssel passt, den Sie auf dem Server installiert haben.
Warnung: Bevor Sie mit diesem Schritt fortfahren, stellen Sie sicher, dass Sie einen öffentlichen Schlüssel auf Ihrem Server installiert haben.
Andernfalls werden Sie ausgesperrt!
Öffnen Sie als root-Benutzer oder Benutzer mit sudo-Berechtigungen die sshd-Konfigurationsdatei:
Suchen Sie die Zeile, in der Password Authentication steht, und heben Sie die Kommentierung auf, indem Sie das führende # -Zeichen entfernen.
Sie können dann den Wert in no ändern:
Zwei weitere Einstellungen, die nicht geändert werden sollten (sofern Sie diese Datei noch nicht geändert haben), sind PubkeyAuthentication und ChallengeResponseAuthentication.
Sie werden standardmäßig gesetzt und sollten wie folgt aussehen:
Speichern und schließen Sie die Datei nach Vornahme Ihrer Änderungen.
Sie können den SSH-Daemon nun neu laden:
Die Passwortauthentifizierung sollte nun deaktiviert und Ihr Server nur per SSH-Schlüsselauthentifizierung zugänglich sein.
Sich mit SSH vertraut zu machen, ist eine lohnenswerte Aufgabe, schon weil sie so häufig erforderlich ist.
Bei Verwendung der verschiedenen Optionen werden Sie erweiterte Funktionen entdecken, die Ihr Leben erleichtern können.
SSH ist noch immer beliebt, da das Protokoll in verschiedenen Situationen sicher, leicht und hilfreich ist.
Verstehen von Datum und Zeit in JavaScript
JavaScript kommt mit dem integrierten Objekt Date und verwandten Methoden.
In diesem Tutorial wird erläutert, wie Sie Datum und Uhrzeit in JavaScript formatieren und verwenden.
2469
Datum und Uhrzeit sind ein fester Bestandteil unseres Alltags und spielen daher in der Computerprogrammierung eine große Rolle.
In JavaScript müssen Sie möglicherweise eine Website mit einem Kalender, einem Zugfahrplan oder einer Schnittstelle zum Einrichten von Terminen erstellen.
Diese Anwendungen müssen relevante Zeiten basierend auf der aktuellen Zeitzone des Benutzers anzeigen oder Berechnungen rund um Ankunfts- und Abfahrtzeiten oder Start- und Endzeiten durchführen.
Zusätzlich müssen Sie möglicherweise JavaScript verwenden, um jeden Tag zu einer bestimmten Zeit einen Bericht zu erstellen oder durch aktuell geöffnete Restaurants und Einrichtungen zu filtern.
Um alle diese Ziele und mehr zu erreichen, verfügt JavaScript über das eingebaute Objekt Date und entsprechende Methoden.
Das Objekt "Date"
Das Objekt Date ist ein eingebautes Objekt in JavaScript, das Datum und Uhrzeit speichert.
Es bietet eine Reihe von integrierten Methoden zur Formatierung und Verwaltung dieser Daten.
Standardmäßig wird bei einer neuen Instanz Date ohne Angabe von Argumenten ein Objekt erstellt, das dem aktuellen Datum und der aktuellen Uhrzeit entspricht.
Dieses wird gemäß den Systemeinstellungen des aktuellen Computers erstellt.
Um Date von JavaScript zu demonstrieren, erstellen wir eine Variable und weisen ihr das aktuelle Datum zu. Dieser Artikel wird am Mittwoch, dem 18. Oktober in London (GMT) geschrieben, also ist dies das aktuelle Datum, die aktuelle Uhrzeit und die Zeitzone, die unten dargestellt sind.
Wenn wir die Ausgabe betrachten, haben wir eine Datumszeichenfolge mit folgendem Inhalt:
Tag der Woche
Monat
Tag
Jahr
Stunde
Minute
Sekunde
Zeitzone
Mittwoch
Okt.
18
2017
12
41
34
GMT + 0000 (UTC)
Das Datum und die Uhrzeit werden aufgeschlüsselt und in einer Weise ausgegeben, die wir als Menschen verstehen können.
JavaScript hingegen versteht das Datum basierend auf einem Zeitstempel, der von der Unix-Zeit abgeleitet ist, also ein Wert, der aus der Anzahl von Millisekunden besteht, die seit Mitternacht am 1. Januar 1970 vergangen sind.
Wir können den Zeitstempel mit der Methode getTime () erhalten.
Die große Zahl, die in unserer Ausgabe für den aktuellen Zeitstempel angezeigt wird, repräsentiert denselben Wert wie oben, den 18. Oktober 2017.
Die Epochenzeit, auch als Nullzeit bezeichnet, wird durch die Datumszeichenfolge 01. Januar, 1970 00: 00: 00 Universalzeit (UTC) und den Zeitstempel 0 dargestellt.
Wir können dies im Browser testen, indem wir eine neue Variable erstellen und ihr eine neue Instanz Date basierend auf einem Zeitstempel von 0 zuweisen.
Die Epochenzeit wurde in früheren Tagen der Programmierung als Standard für die Zeitmessung von Computern ausgewählt, und ist die Methode, die JavaScript verwendet.
Es ist wichtig, das Konzept sowohl des Zeitstempels als auch der Datumszeichenfolge zu verstehen, da beide je nach Einstellungen und Zweck einer Anwendung verwendet werden können.
Bisher haben wir gelernt, wie wir eine neue Instanz Date basierend auf der aktuellen Zeit und eine basierend auf einem Zeitstempel erstellen.
Insgesamt gibt es vier Formate, mit denen Sie ein neues Date in JavaScript erstellen können.
Zusätzlich zu dem aktuellen Zeitstandard und dem Zeitstempel können Sie auch eine Datumszeichenfolge verwenden oder bestimmte Daten und Uhrzeiten angeben.
Datumserstellung
Ausgabe
new Date ()
Aktuelles Datum und aktuelle Uhrzeit
new Date (timestamp)
Erstellt Datum basierend auf Millisekunden seit Epochenzeit
new Date (date string)
Erstellt Datum basierend auf Datumszeichenfolge
new Date (year, month, day, hours, minutes, seconds, milliseconds)
Erstellt Datum basierend auf angegebenen Datum und angegebener Uhrzeit
Um die verschiedenen Möglichkeiten zu demonstrieren, sich auf ein bestimmtes Datum zu beziehen, erstellen wir ein neues Objekt Date, das den 4. Juli 1776 um 12: 30 Uhr GMT auf drei verschiedene Arten darstellen wird.
Die drei obigen Beispiele erzeugen alle ein Datum, das die gleichen Informationen enthält.
Sie werden feststellen, dass die Zeitstempel-Methode eine negative Zahl hat; jedes Datum vor der Epochenzeit wird als negative Zahl dargestellt.
In der Datum- und Uhrzeit-Methode werden die Sekunden und Millisekunden auf 0 gesetzt. Wenn eine Zahl bei der Erstellung von Date fehlt, wird sie standardmäßig auf 0 gesetzt. Die Reihenfolge kann jedoch nicht geändert werden. Denken Sie daran, wenn Sie eine Zahl auslassen möchten.
Eventuell fällt Ihnen auch auf, dass der Monat Juli durch eine 6 dargestellt wird und nicht die übliche 7. Dies liegt daran, dass die Datums- und Uhrzeitnummern bei 0 beginnen, wie es bei den meisten Zählungen in der Programmierung der Fall ist.
Im nächsten Abschnitt finden Sie eine detailliertere Tabelle.
Abrufen des Datums mit get
Sobald wir ein Datum haben, können wir mit verschiedenen integrierten Methoden auf alle Komponenten des Datums zugreifen.
Die Methoden geben jeden Teil des Datums relativ zur lokalen Zeitzone zurück.
Jede dieser Methoden beginnt mit get und gibt die relative Zahl zurück.
Im Folgenden finden Sie eine detaillierte Tabelle der Methoden get des Objekts Date.
Datum / Uhrzeit
Methode
Bereich
Beispiel
getFullYear ()
JJJJ
1970
getMonth ()
0-11
0 = Januar
Tag (des Monats)
getDate ()
1 = 1. des Monats
Tag (der Woche)
getDay ()
0-6
0 = Sonntag
getHours ()
0 = Mitternacht
getMinutes ()
getSeconds ()
Millisekunde
getMilliseconds ()
0-999
Zeitstempel
getTime ()
Millisekunden seit Epochenzeit
Erstellen wir ein neues Datum, das auf dem 31. Juli 1980 basiert, und weisen es einer Variable zu.
Jetzt können wir alle unsere Methoden verwenden, um jede Datumskomponente von Jahr bis Millisekunde zu erhalten.
Manchmal kann es notwendig sein, nur einen Teil eines Datums zu extrahieren, und die integrierten Methoden get sind das Tool, mit dem Sie dies erreichen.
Als ein Beispiel dafür können wir das aktuelle Datum gegen den Tag und den Monat des 3. Oktober testen, um zu sehen, ob es der 3. Oktober ist oder nicht.
Da es zum Zeitpunkt des Schreibens nicht der 3. Oktober ist, spiegelt die Konsole diese wider.
Die integrierten Methoden Date, die mit get beginnen, können wir auf Datumskomponenten zugreifen, die die Zahl zurückgeben, die zu dem gehört, was wir von dem instanziierten Objekt abrufen.
Ändern des Datums mit set
Für alle get-Methoden, die wir oben kennengelernt haben, gibt es eine entsprechende Methode set.
Während get verwendet wird, um eine bestimmte Komponente eines Datums abzurufen, wird set verwendet, um Komponenten eines Datums zu ändern.
Im Folgenden finden Sie eine detaillierte Tabelle der Methoden set des Objekts Date.
setFullYear ()
setMonth ()
setDate ()
setDay ()
setHours ()
setMinutes ()
setSeconds ()
setMilliseconds ()
setTime ()
Wir können diese Methoden set verwenden, um eine, mehrere oder alle Komponenten eines Datums zu ändern.
Wir können beispielsweise das Jahr unserer obigen Variable birthday (Geburtstag) so ändern, dass es 1997 anstatt 1980 ist.
Wir sehen in dem obigen Beispiel, dass wir bei dem Aufruf der Variable birthday das neue Jahr als Teil der Ausgabe erhalten.
Mit den integrierten Methoden, die mit set beginnen, können wir verschiedene Teile eines Objekts Date ändern.
Datumsmethoden mit UTC
Die oben besprochenen Methoden get rufen die Datumskomponenten basierend auf den lokalen Zeitzoneneinstellungen des Benutzers ab.
Für eine bessere Kontrolle über die Daten und Uhrzeiten können Sie die Methoden getUTC verwenden, die genau die gleichen sind wie die Methoden get, außer dass sie die Zeit basierend auf dem UTC-Standard (Coordinated Universal Time) berechnen.
Nachfolgend finden Sie eine Tabelle der UTC-Methoden für das JavaScript-Objekt Date.
getUTCFullYear ()
getUTCMonth ()
getUTCDate ()
getUTCDay ()
getUTCHours ()
getUTCMinutes ()
getUTCSeconds ()
getUTCMilliseconds ()
Um den Unterschied zwischen lokalen und UTC-Methoden get zu testen, können wir den folgenden Code ausführen.
Die Ausführung dieses Codes gibt die aktuelle Stunde und die Stunde der UTC-Zeitzone aus.
Wenn Sie sich derzeit in der UTC-Zeitzone befinden, werden die Zahlen, die bei der Ausführung des obigen Programms ausgegeben werden, die gleichen sein.
UTC ist insofern nützlich, als es eine internationale Zeitstandard-Referenz bietet und daher Ihren Code über Zeitzonen hinweg konsistent halten kann, wenn dies für Ihre Entwicklung relevant ist.
In diesem Tutorial haben wir gelernt, wie wir eine Instanz des Objekts Date erstellen und seine integrierten Methoden verwenden, um auf Komponenten eines bestimmten Datums zuzugreifen und diese zu ändern.
Für einen tieferen Einblick in Datums- und Zeitangaben in JavaScript können Sie die Datumsreferenz in dem Mozilla Developer Network lesen.
Zu wissen, wie man mit Datumsangaben arbeitet, ist für viele gängige Aufgaben in JavaScript unerlässlich, da Sie damit viele Dinge tun können, vom Einrichten eines sich wiederholenden Berichts bis hin zum Anzeigen von Daten und Zeitplänen in der richtigen Zeitzone.
Der Python-Zeichenfolgen-Datentyp ist eine Sequenz, die aus einem oder mehreren einzelnen Zeichen besteht, die aus Buchstaben, Zahlen, Leerzeichen oder Symbolen bestehen können.
Da es sich bei einer Zeichenfolge um eine Sequenz handelt, kann durch Indizieren und Schneiden auf dieselbe Weise wie bei anderen sequenzbasierten Datentypen auf sie zugegriffen werden.
S
y
h
!
9
Das Ausrufezeichen (!)
Zugriff auf Zeichen nach positiver Indexnummer
Da sich der Buchstabe y an der Indexnummer 4 der Zeichenfolge ss = "Sammy Shark!" befindet, erhalten wir beim Drucken von ss [4] y als Ausgabe.
Indexnummern ermöglichen es uns, auf bestimmte Zeichen innerhalb einer Zeichenfolge zuzugreifen.
Zugriff auf Zeichen nach negativer Indexnummer
sieht die negative Indexaufschlüsselung folgendermaßen aus:
-10
-7
Beim Erstellen einer Scheibe wie in [6: 11] beginnt bei der ersten Indexnummer die Scheibe (einschließlich), und bei der zweiten Indexnummer endet die Scheibe (exklusiv), weshalb in unserem obigen Beispiel der Bereich der Indexnummer gilt, der unmittelbar nach dem Ende der Zeichenfolge auftreten würde.
Wenn wir ss [6: 11] aufrufen, rufen wir den Teilstring Shark auf, der in der Zeichenfolge Sammy Shark! vorhanden ist.
Wenn wir eines der Enden einer Zeichenfolge einschließen möchten, können wir eine der Zahlen in der ZeichenfolgenSyntax [n: n] auslassen.
Sie können auch negative Indexnummern verwenden, um eine Zeichenfolge zu trennen.
Wenn man negative Indexzahlen verwendet, beginnen wir zuerst mit der niedrigeren Zahl, wie sie früher in der Zeichenfolge vorkommt.
Ein Schritt von 1 nimmt also jedes Zeichen zwischen zwei Indexnummern eines Schrittes auf.
< ^ > k < ^ > r < ^ > a < ^ > h < ^ > S < ^ > Leerzeichen < ^ > y < ^ > m < ^ > m < ^ > a < ^ > S
Drucken wir die Länge der Zeichenfolge ss:
ist 12 Zeichen lang, einschließlich des Leerzeichens und des Ausrufezeichens.
Arbeiten wir mit unserer Zeichenfolge ss = "Sammy Shark!"
Versuchen wir str.count () mit einer Sequenz von Zeichen:
Wir können dies mit der str.find () -Methode tun, die die Position des Zeichens basierend auf der Indexnummer zurückgibt.
Überprüfen wir, wo die erste Zeichenfolge "likes" in der Zeichenfolge likes vorkommt:
Was, wenn wir sehen möchten, wo die zweite Sequenz von "likes" beginnt?
In diesem zweiten Beispiel, das an der Indexnummer 9 beginnt, beginnt das erste Ereignis der Zeichenfolge "likes" bei der Indexnummer 34.
Wie beim Schneiden können wir dies tun, indem wir mit einer negativen Indexzahl rückwärts zählen:
Die Möglichkeit, bestimmte Indexnummern von Zeichenfolgen oder eine bestimmte Scheibe einer Zeichenfolge aufzurufen, bietet uns eine größere Flexibilität bei der Arbeit mit diesem Datentyp.
Da Zeichenfolgen wie Listen und Tupel ein sequenzbasierter Datentyp sind, kann durch Indizieren und Schneiden auf sie zugegriffen werden.
Auf dem Computer werden diese als "Prozesse" bezeichnet.
top
PID USER PR NI VIRT RES SHR S% CPU% MEM TIME + COMMAND
3 root 20 0 0 0 0 S 0.0 0.0 0: 00.07 ksoftirqd / 0
6 root RT 0 0 0 0 S 0.0 0.0 0: 00.00 migration / 0
Der oberste Informationsblock enthält Systemstatistiken wie die Systemlast und die Gesamtzahl der Aufgaben.
htop
Mem [| | | | | | | | | | | 49 / 995MB] Durchschnittslast: 0.00 0.03 0.05
root 1 0.0 0.2 24188 2120?
root 3 0.0 0.0 0 0?
root 6 0.0 0.0 0 0?
root 8 0.0 0.0 0 0?
ps axjf
2 6 0 0? -1 S 0 0: 00\ _ [migration / 0]
2 8 0 0? -1 S < 0 0: 00\ _ [cpuset]
Eine Anmerkung zu Prozess-IDs
Jede Kommunikation zwischen dem Benutzer und dem Betriebssystem über Prozesse umfasst die Übersetzung zwischen Prozessnamen und PIDs zu einem bestimmten Zeitpunkt während des Vorgangs.
Aus diesem Grund teilen Dienstprogramme Ihnen die PID mit.
Wenn sich das Programm schlecht verhält und bei Erhalt des TERM-Signals nicht beendet wird, können wir das Signal durch Weiterleiten des KILL-Signals eskalieren:
Sie können auch verwendet werden, um andere Aktionen auszuführen.
Beispielsweise werden viele Daemons neu gestartet, wenn sie das HUP- oder Auflegesignal erhalten.
Sie können alle Signale auflisten, die mit kill gesendet werden können, indem Sie Folgendes eingeben:
kill -l
Der obige Befehl ist das Äquivalent von:
Oft möchten Sie anpassen, welchen Prozessen in einer Serverumgebung Priorität eingeräumt wird.
Aufgaben: 56 insgesamt, 1 laufend, 55 inaktiv, 0 gestoppt, 0 Zombie
Swap: 0k insgesamt, 0k verwendet, 0k frei, 264812k zwischengespeichert
3 root 20 0 0 0 0 S 0.0 0.0 0: 00.11 ksoftirqd / 0
Hinweis: Während nice zwangsläufig mit einem Befehlsnamen funktioniert, ruft renice die Prozess-PID auf
Die Prozessverwaltung ist ein Thema, das für neue Benutzer manchmal schwer zu verstehen ist, da sich die verwendeten Tools von ihren grafischen Gegenstücken unterscheiden.
Die Ideen sind jedoch vertraut und intuitiv und werden mit ein wenig Übung zur Gewohnheit.
Es nutzt Pipfile, pip und virtualenv in einem einzigen Befehl.
Öffnen Sie Ihren Terminal und führen Sie folgenden Befehl aus:
Eine Abfragezeichenfolge ähnelt der folgenden:
Fügen wir der Abfragebeispielroute eine Abfragezeichenfolge hinzu.
Sie müssen den Teil programmieren, der die Abfrageargumente verarbeitet.
Durch den Aufruf von request.args.get (' language ') wird die Anwendung weiterhin ausgeführt, wenn der Schlüssel Sprache nicht in der URL vorhanden ist.
Das Argument aus der URL wird der Variable Sprache zugewiesen und dann an den Browser zurückgegeben.
Erstellen Sie einen Schlüssel für "Framework" und einen Wert für "Flask ":
Führen Sie dann die App aus und navigieren Sie zur URL:
Entfernen Sie den Schlüssel Sprache aus der URL:
Fahren wir mit dem nächsten Typ eingehender Daten fort.
Das Wichtigste, was Sie über dieses Formular wissen müssen, ist, dass es eine POST-Abfrage an dieselbe Route ausführt, die das Formular generiert hat. Die Schlüssel, die in der App gelesen werden, stammen alle aus den Namensattributen in unseren Formulareingaben.
Ändern Sie die Route form-example in app.py mit dem folgenden Code:
Füllen Sie das Feld Sprache mit dem Wert von Python und das Feld Framework mit dem Wert von Flask aus.
Drücken Sie dann Senden.
Das ist ziemlich antiklimatisch, aber zu erwarten, da der Code für die Verarbeitung der JSON-Datenantwort noch nicht geschrieben wurde.
Um die Daten zu lesen, müssen Sie verstehen, wie Flask JSON-Daten in Python-Datenstrukturen übersetzt:
Alles, was ein Objekt ist, wird in ein Python-Diktat konvertiert. {"key ": value"} in JSON entspricht somedict ['key'], das in Python einen Wert zurückgibt.
Die Werte in Anführungszeichen im JSON-Objekt werden Zeichenfolgen in Python.
Beachten Sie, wie Sie auf Elemente zugreifen, die nicht auf der oberen Ebene sind. ['version'] ['python'] wird verwendet, da Sie ein verschachteltes Objekt eingeben.
Wenn Sie nicht möchten, dass es fehlschlägt, wenn ein Schlüssel nicht vorhanden ist, müssen Sie überprüfen, ob der Schlüssel vorhanden ist, bevor Sie versuchen, darauf zuzugreifen.
Führen Sie die App aus und senden Sie die Beispiel-JSON-Abfrage mit Postman.
Jetzt verstehen Sie die Verarbeitung von JSON-Objekten.
Installation von Anaconda auf Ubuntu 18.04 Quickstart
2711
Anaconda, ein Open-Source-Paketmanager und Umgebungsmanager, wurde für Data-Science- und Machine-Learning-Workflows sowie die Verteilung der Programmiersprachen Python und R entwickelt.
Dieses Tutorial beschreibt die Installation von Anaconda auf einem Ubuntu 18.04 Server.
Eine ausführlichere Version dieses Tutorials mit besseren Erklärungen zu den einzelnen Schritten finden Sie unter Installation von Anaconda Python Distribution auf Ubuntu 18.04.
Schritt 1 - Laden Sie die neueste Version von Anaconda herunter
Gehen Sie in Ihrem Webbrowser auf die Anaconda Distribution-Seite, die unter dem folgenden Link verfügbar ist:
Finden Sie die neueste Linux-Version und kopieren Sie das Installer Bash Script.
Schritt 2 - Laden Sie das Anaconda Bash Script herunter
Melden Sie sich beim Ubuntu 18.04 Server als Sudo-Benutzer ohne Rootberechtigung an, gehen Sie ins Verzeichnis / tmp und verwenden Sie curl, um den Link herunterzuladen, den Sie von der Anaconda Website kopiert haben:
Schritt 3 - Verifizieren Sie die Datenintegrität des Installers
Garantieren Sie die Integrität des Installers mit kryptographischer Hash-Verifizierung mithilfe von SHA-256 Checksum:
Schritt 4 - Führen Sie das Anaconda Script aus
Wenn Sie die ENTER drücken, erhalten Sie so lange die folgende Meldung zur Ansicht der Lizenzvereinbarung, bis Sie ans Ende gelangt sind.
Wenn Sie das Ende der Lizenz erreicht haben, geben Sie Ja ein, wenn Sie die Lizenz zum Ausführen der Installation akzeptieren.
Schritt 5 - Schließen Sie den Installationsprozess ab
Wenn Sie die Lizenz akzeptieren, werden Sie dazu aufgefordert, den Ort für die Installation zu wählen.
Sie können die ENTER drücken, um den Standardort zu akzeptieren, oder einen anderen Ort angeben.
Jetzt wird die Installation fortgesetzt.
Bitte beachten Sie, dass der Installationsprozess einige Zeit in Anspruch nimmt.
Schritt 6 - Wählen Sie die Optionen aus
Sobald die Installation abgeschlossen ist, erhalten Sie die folgende Meldung:
Wir empfehlen, dass Sie yes eingeben, um den conda-Befehl zu verwenden.
Schritt 7 - Aktivieren Sie die Installation
Sie können die Installation nun mit dem folgenden Befehl aktivieren:
Schritt 8 - Testen Sie die Installation
Verwenden Sie den conda-Befehl, um die Installation und die Aktivierung zu testen:
In der Ausgabe sind alle Pakete enthalten, die Teil der Anaconda-Installation sind.
Schritt 9 - Richten Sie die Anaconda-Umgebung ein
Sie können Anaconda-Umgebungen mit dem Befehl conda create einrichten.
So kann beispielsweise eine Python-3-Umgebung mit dem Namen < ^ > my _ env < ^ > mit dem folgenden Befehl erstellt werden:
Aktivieren Sie die neue Umgebung wie folgt:
Ihr Eingabeaufforderungs-Präfix verändert sich, um anzuzeigen, dass Sie in einer aktiven Anaconda-Umgebung sind und jetzt mit der Arbeit an einem Projekt beginnen können.
Hier sind Links zu ausführlicheren Tutorials, die für diesen Leitfaden relevant sind:
Installation der Anaconda Python Distribution auf Ubuntu 18.04
Einrichten von Jupyter Notebook für Python 3
Installation des pandas-Pakets und Umgang mit Datenstrukturen in Python 3
Installation und Konfiguration von VNC auf Ubuntu 18.04
2701
Virtual Network Computing oder VNC ist ein Verbindungssystem, das es Ihnen ermöglicht, die Tastatur und die Maus zur Interaktion mit einer grafischen Desktop-Umgebung auf einem entfernten Server zu verwenden.
Es vereinfacht die Verwaltung von Dateien, Software und Einstellungen auf einem entfernten Server für Benutzer, die mit der Befehlszeile noch nicht so versiert sind.
In diesem Leitfaden erstellen Sie einen VNC-Server auf einem Ubuntu 18.04 Server und verbinden sich mit diesem über einen sicheren SSH-Tunnel.
Dazu verwenden Sie TightVNC, ein schnelles, leichtes Fernsteuerungspaket.
Mit dieser Auswahl stellen Sie sicher, dass unsere VNC-Verbindung auch bei langsameren Internet-Verbindungen reibungslos und stabil sein wird.
Einen lokalen Computer mit einem installierten VNC Client, der VNC-Verbindungen über SSH-Tunnel unterstützt.
In Windows können Sie TightVNC, RealVNC oder UltraVNC verwenden.
Auf MacOS können Sie das integrierte Screen Sharing-Programm verwenden oder eine plattformübergreifende App wie RealVNC.
Auf Linux können Sie aus zahlreichen Optionen auswählen, darunter Vinagre, krdc, RealVNC oder TightVNC.
Schritt 1 - Installation der Desktop-Umgebung und des VNC-Servers
Standardmäßig wird ein Ubuntu 18.04 Server nicht mit einer grafischen Desktop-Umgebung oder einem installierten VNC-Server geliefert, daher beginnen wir mit deren Installation.
Wir installieren vor allem Pakete für die aktuellste Xfce Desktop-Umgebung und das TightVNC-Paket, das im offiziellen Ubuntu Repository verfügbar ist.
Aktualisieren Sie Ihre Liste der Pakete auf Ihrem Server.
Installieren Sie jetzt die Xfce Desktop-Umgebung auf Ihrem Server:
Wenn diese Installation abgeschlossen ist, installieren Sie den TightVNC Server:
Um die Erstkonfiguration des VNC-Servers nach der Installation abzuschließen, verwenden Sie den Befehl vncserver, um ein sicheres Passwort einzurichten und die Erstkonfigurationsdateien zu erstellen:
Sie werden dazu aufgefordert, ein Passwort einzugeben und zu verifizieren, um ferngesteuert auf Ihren Rechner zuzugreifen.
Das Passwort muss zwischen sechs und acht Zeichen lang sein.
Passwörter mit mehr als 8 Zeichen werden automatisch verkürzt.
Sobald Sie das Passwort verifiziert haben, können Sie ein schreibgeschütztes Passwort einrichten.
Benutzer, die sich mit dem schreibgeschützten Passwort anmelden, können die VCN-Instanz nicht mit der Maus oder Tastatur steuern.
Das ist eine hilfreiche Option, wenn Sie mit Ihrem VCN Server anderen etwas zeigen möchten, ist aber nicht erforderlich.
Der Prozess erstellt dann die notwendigen Standard-Konfigurationsdateien und Verbindungsinformationen für den Server.
Jetzt wollen wir den VCN Server konfigurieren.
Schritt 2 - Konfiguration des VCN Servers
Der VCN Server muss wissen, welche Befehle er beim Start ausführen soll.
VCN muss vor allem wissen, mit welchem grafischen Desktop er sich verbinden soll.
Diese Befehle befinden sich in einer Konfigurationsdatei namens xstartup im Ordner .vnc in Ihrem Stammverzeichnis.
Das Start-Skript wurde erstellt, als Sie den vncserver im vorherigen Schritt ausgeführt haben, aber wir erstellen unser eigenes, um den Xfce Desktop zu starten.
Wenn VCN das erste Mal eingerichtet wird, startet er eine Standard-Server-Instanz auf Port 5901.
Dieser Port wird als Anzeige-Port bezeichnet und wird vom VCN: 1 genannt.
VCN kann mehrere Instanzen auf anderen Anzeige-Ports, wie: 2,: 3 usw. starten.
Da wir die Konfiguration des VCN Servers ändern werden, müssen wir zunächst die VCN Server-Instanz stoppen, die auf Port 5901 ausgeführt wird. Dazu verwenden wir den folgenden Befehl:
Die Ausgabe sollte wie folgt aussehen, auch wenn Sie eine andere PID sehen werden:
Bevor Sie die Datei xstartup ändern, sollten Sie ein Backup der Originaldatei vornehmen:
Erstellen Sie jetzt eine neue xstartup-Datei und öffnen Sie sie im Texteditor:
Die Befehle in dieser Datei werden automatisch ausgeführt, wenn Sie den VCN Server starten oder neu starten.
VCN muss unsere Desktop-Umgebung starten, sofern diese noch nicht gestartet wurde.
Fügen Sie der Datei diese Befehle hinzu:
Der erste Befehl in der Datei, xrdb $HOME /.
Xresources weist das GUI Framework des VNC mit, das des Serverbenutzers zu lesen.
Xresources-Datei..
Xresources ist der Ort, an dem ein Benutzer bestimmte Einstellungen des grafischen Desktops ändern kann, wie die Terminal-Farben, Cursor-Gestaltung und das Font-Rendering.
Der zweite Befehl weist den Server an, Xfce zu starten, wo Sie die grafische Software finden, die sie zur problemlosen Verwaltung Ihres Servers brauchen.
Um sicherzustellen, dass der VCN Server diese neue Start-Datei ordnungsgemäß verwenden kann, müssen wir sie ausführbar machen.
Starten Sie jetzt den VCN Server.
Sobald die Konfiguration vorgenommen wurde, wollen wir uns über unseren lokalen Rechner mit dem Server verbinden.
Schritt 3 - Eine sichere Verbindung mit dem VCN Desktop
VCN selbst verwendet keine sicheren Protokolle beim Verbinden.
Wir verwenden einen SSH-Tunnel, um eine sichere Verbindung mit unserem Server herzustellen und weisen dann unseren VCN Client an, anstelle der Direktverbindung diesen Tunnel zu verwenden.
Erstellen Sie auf Ihrem lokalen Computer eine SSH-Verbindung, die sicher an die localhost-Verbindung für VCN weiterleitet.
Sie können dies mit folgendem Befehl über den Terminal auf Linux oder macOS durchführen:
Der -L Switch gibt die Port-Bindung an.
In diesem Fall verbinden wir Port 5901 der entfernten Verbindung mit Port 5901 auf Ihrem lokalen Rechner.
Der -C Switch ermöglicht eine Kompression, während der -N Switch ssh mitteilt, dass wir keinen entfernten Befehl ausführen wollen.
Der -l Switch gibt den entfernten Login-Namen an.
Denken Sie daran, < ^ > sammy < ^ > und < ^ > your _ server _ ip < ^ > durch den Sudo-Benutzernamen ohne Rootberechtigung und die IP-Adresse Ihres Servers zu ersetzen.
Wenn Sie einen grafischen SSH Client, wie PuTTY, verwenden, verwenden Sie < ^ > your _ server _ ip < ^ > als Verbindungs-IP und setzen localhost: 5901 als einen neuen weitergeleiteten Port in der SSH-Tunnel-Einstellung des Programms ein.
Sobald der Tunnel läuft, verwenden Sie einen VCN Client, um sich mit localhost: 5901 zu verbinden.
Sie werden aufgefordert, sich mit dem in Schritt 1 eingestellten Passwort zu authentifizieren.
Sobald die Verbindung steht, sehen Sie den Xfce Standard-Desktop.
VNC Verbindung mit dem Ubuntu 18.04 Server
Sie können mit dem Dateimanager auf Dateien in Ihrem Stammverzeichnis zugreifen oder von der Befehlszeile aus, wie nachstehend zu sehen ist:
Dateien über die VNC Verbindung mit Ubuntu 18.04
Drücken Sie CTRL + C auf Ihrem Terminal, um den SSH-Tunnel zu stoppen und zu Ihrer Aufforderung zurückzukehren.
Damit wird auch Ihre VNC Sitzung unterbrochen.
Als nächstes richten wir unseren VNC Server als Dienst ein.
Schritt 4 - VCN als Systemdienst ausführen
Als nächstes stellen wir den VNC Server als Systemdienst ein, um ihn wie jeden anderen Dienst starten, stoppen und ggf. neu starten zu können.
Damit wird auch sichergestellt, dass VCN beim Neustart Ihres Servers gestartet wird.
Erstellen Sie zunächst in Ihrem bevorzugten Texteditor eine neue Unit-Datei mit dem Namen / etc / systemd / system / vncserver @ .service:
Das @ Symbol am Ende des Namens gestattet uns, ein Argument aufzunehmen, das wir in der Dienstkonfiguration verwenden können.
Wir verwenden dies, um den VCN Anzeige-Port anzugeben, den wir bei der Dienstverwaltung einsetzen möchten.
Fügen Sie der Datei folgende Zeilen hinzu.
Vergewissern Sie sich, dass Sie den Wert von User, Group, WorkingDirectory und den Benutzernamen im Wert von PIDFILE ändern, damit er mit Ihrem Benutzernamen übereinstimmt:
Der ExecStartPre Befehl stoppt VCN, wenn er bereits läuft.
Der ExecStart Befehl startet VCN und stellt die Farbtiefe auf 24-Bit-Farbe mit einer Auflösung von 1280 x 800.
Sie können diese Startoptionen auch je nach Wunsch ändern.
Als nächstes müssen Sie das System auf die neue Unit-Datei aufmerksam machen.
Aktivieren Sie die Unit-Datei.
Die 1 nach dem Zeichen @ gibt an, über welcher Anzeigenummer der Dienst angezeigt werden soll; in diesem Fall ist der Standard: 1, wie wir bereits in Schritt 2 besprochen haben.
Stoppen Sie die aktuelle Instanz des VCN Servers, wenn er noch läuft.
Starten Sie ihn dann wie jeden anderen Systemdienst.
Sie können verifizieren, dass er mit diesem Befehl gestartet wurde:
Falls er richtig gestartet wurde, sollte die Ausgabe wie folgt aussehen:
Ihr VCN Server steht jetzt beim Neustart des Rechners zur Verfügung.
Starten Sie Ihren SSH-Tunnel erneut:
Stellen Sie dann mit Ihrer VCN Client Software eine neue Verbindung zum localhost: 5901 her, um sich mit Ihrem Rechner zu verbinden.
Jetzt läuft ein sicherer VCN Server auf Ihrem Ubuntu 18.04 Server.
Jetzt können Sie Ihre Dateien, Software und Einstellungen in einer benutzerfreundlichen, vertrauten grafischen Oberfläche verwalten und grafische Software wie Web-Browser ferngesteuert ausführen.
Installation von Java mit apt auf Ubuntu 18.04
2630
Der Autor hat den Open Internet / Free Speech Fund ausgewählt, um eine Spende in Höhe von 100 USD im Rahmen des Programms "Write for DOnations" zu erhalten.
Sie installieren OpenJDK sowie offizielle Pakete von Oracle.
Einen Ubuntu 18.04-Server, der gemäß des Leitfadens zur Ersteinrichtung eines Ubuntu 18.04 Servers eingerichtet wurde, einschließlich eines sudo-Benutzers ohne Rootberechtigung und einer Firewall.
Ubuntu 18.04 enthält standardmäßig Open JDK, eine Open-Source-Variante der JRE und des JDK.
Dieses Paket installiert entweder OpenJDK 10 oder 11.
Vor September 2018 wurde OpenJDK 10 installiert.
Seit September 2018 wird OpenJDK 11 installiert.
Führen Sie den folgenden Befehl aus, um das OpenJDK zu installieren:
Dieser Befehl installiert die Java Runtime Environment (JRE).
Dies ermöglicht Ihnen die Ausführung fast aller Java-Programme.
Als nächstes sehen wir uns an, welche OpenJDK-Version wir installieren wollen.
Installation bestimmter OpenJDK-Versionen
Obwohl Sie das Standard-OpenJDK-Paket installieren können, ist auch die Installation unterschiedlicher Versionen des OpenJDK möglich.
OpenJDK 8
Java 8 ist die aktuelle und langfristig unterstützte Version und wird noch vielerorts unterstützt, obwohl die öffentliche Wartung im Januar 2019 endete.
Um OpenJDK 8 zu installieren, führen Sie den folgenden Befehl aus:
Stellen Sie sicher, dass dies installiert ist mit
Sie können auch nur die JRE installieren, indem Sie sudo apt install openjdk-8-jre ausführen.
OpenJDK 10 / 11
Ubuntus Repositorys enthalten ein Paket, das entweder Java 10 oder 11 installiert. Vor September 2018 wurde OpenJDK 10 mit diesem Paket installiert. Seit der Veröffentlichung von Java 11 wird Java 11 mit diesem Paket installiert.
Führen Sie den folgenden Befehl aus, um das OpenJDK 10 / 11 zu installieren:
Verwenden Sie den folgenden Befehl, um nur die JRE zu installieren:
Installation des Oracle JDK
Wenn Sie das offizielle Oracle JDK installieren möchten, müssen Sie ein neues Paket-Repository für die Version hinzufügen, die Sie verwenden möchten.
Fügen Sie zur Installation von Java 8 (der neuesten LTS-Version) zunächst das Paket-Repository hinzu:
Wenn Sie das Repository hinzufügen, erscheint eine Meldung wie diese:
Drücken Sie die ENTER, um fortzufahren.
Aktualisieren Sie dann Ihre Paketliste:
Nachdem die Paketliste aktualisiert wurde, installieren Sie Java 8:
Ihr System lädt das JDK von Oracle herunter und fordert Sie dazu auf, die Lizenzvereinbarung zu akzeptieren.
Nachdem Sie die Vereinbarung akzeptiert haben, wird das JDK installiert.
So würde die Ausgabe aussehen, wenn Sie in diesem Tutorial alle Java-Versionen installiert haben:
OpenJDK 8 befindet sich unter / usr / lib / jvm / java-8-openjdk-amd64 / jre / bin / java.
Oracle Java 8 befindet sich unter / usr / lib / jvm / java-8-oracle / jre / bin / java.
Fügen Sie am Ende dieser Datei die folgende Zeile hinzu, und achten Sie darauf, den markierten Pfad durch Ihren eigenen kopierten Pfad zu ersetzen:
Installation des Linux, Apache, MySQL, PHP (LAMP) Stacks auf Ubuntu 18.04
2614
Eine frühere Version dieses Tutorials wurde von Brennan Bearnes geschrieben.
Ein "LAMP" -Stack ist eine aus Open-Source-Software bestehende Gruppe, die normalerweise zusammenhängend installiert wird, damit ein Server dynamische Websites und Web-Apps hosten kann.
Dieser Begriff ist ein Acronym, das für das Linux Betriebssystem mit dem Apache Webserver steht.
In diesem Leitfaden installieren wir einen LAMP Stack auf einem Ubuntu 18.04 Server.
Um dieses Tutorial fertigzustellen, benötigen Sie einen Ubuntu 18.04 Server mit einem sudo-aktivierten Benutzerkonto ohne Rootberechtigung und einer einfachen Firewall.
Das kann mit unserem Leitfaden für das erstmalige Server-Setup für Ubuntu 18.04 konfiguriert werden.
Er ist umfassend dokumentiert und wird seit Beginn des Internets weitläufig eingesetzt, was ihn zur beliebten Standardoption für das Hosting einer Website macht.
Da es sich hierbei um einen sudo-Befehl handelt, werden diese Operationen mit Rootberechtigungen ausgeführt.
Sie werden aufgefordert, Ihre reguläres Benutzerpasswort einzugeben, um sich zu verifizieren.
Nach Eingabe des Passworts teilt Ihnen apt mit, welche Pakete installiert werden und wie viel zusätzlichen Festplattenplatz das erfordern wird.
Drücken Sie auf Y und dann die ENTER, um mit der Installation zu beginnen.
Stellen Sie die Firewall so ein, dass Webverkehr gestattet ist
Vorausgesetzt, dass Sie die Ersteinrichtung des Servers korrekt vorgenommen und die UFW-Firewall aktiviert haben, müssen Sie als nächstes sichergehen, dass Ihre Firewall HTTP- und HTTPS-Verkehr gestattet.
Sie können auch prüfen, ob UFW ein Anwendungsprofil für Apache aufweist:
Wenn Sie sich das gesamte Apache Full Profil ansehen, sollte es zeigen, dass es den Verkehr zu den Ports 80 und 443 gestattet:
Aktivieren Sie eingehenden HTTP- und HTTPS-Verkehr für dieses Profil:
Es wird die Standard-Webseite für Ubuntu 18.04 Apache angezeigt, die Informations- und Testzwecken dient.
Ubuntu 18.04 Apache Standard
Nachdem jetzt Ihr Webserver läuft, müssen Sie MySQL installieren.
MySQL ist ein Datenbank-Managementsystem.
Damit wird im Grunde genommen der Zugriff auf Datenbanken organisiert und bereitgestellt, in denen Ihre Website Informationen speichern kann.
< $> note Anmerkung: In diesem Fall müssen Sie sudo apt update nicht vor dem Befehl ausführen.
Sie haben es gerade erst oben ausgeführt, um Apache zu installieren.
Der Paketindex auf Ihrem Computer sollte bereits aktualisiert worden sein.
Auch mit diesem Befehl zeigen Sie eine Liste der Pakete an, die installiert werden, sowie den dafür benötigten Speicherplatz.
Geben Sie Y ein, um fortzufahren.
Wenn die Installation abgeschlossen ist, führen Sie ein einfaches Sicherheitsskript aus, das in MySQL vorinstalliert ist, um einige gefährliche Standardeinstellungen zu entfernen und den Zugriff auf Ihr Datenbanksystem zu sperren.
Das verursacht Probleme, wenn Sie ein schwaches Passwort mit Software verwenden, die automatisch die MySQL-Benutzerinformationen konfiguriert, wie die Ubuntu-Pakete für phpMyAdmin.
Dies ist ein Administratorkonto in MySQL mit mehr Privilegien.
Sie können das mit dem root-Konto für den Server vergleichen (obwohl das Konto, das Sie gerade konfigurieren, ein MySQL-spezifisches Konto ist).
Vergewissern Sie sich, dass es ein starkes, eindeutiges Passwort ist, und lassen Sie es nicht einfach leer.
Wenn Sie Passwortvalidierung aktiviert haben, wird Ihnen die Passwortstärke des soeben eingegebenen Root-Passworts gezeigt und Sie werden gefragt, ob Sie das Passwort ändern möchten.
Wenn Sie mit Ihrem aktuellen Passwort zufrieden sind, geben Sie in der Eingabeaufforderung N für "nein" ein:
Beachten Sie, dass in Ubuntu-Systemen, die MySQL 5.7 (oder später) ausführen, der MySQL root-Benutzer statt über das Passwort standardmäßig mit dem Plugin auth _ socket authentifiziert wird.
Dadurch entsteht in vielen Fällen mehr Sicherheit und Benutzerfreundlichkeit, kann aber auch Dinge komplizieren, wenn Sie einem externen Programm (z. B. phpMyAdmin) Zugriff auf den Benutzer erteilen müssen.
Wenn Sie zur Verbindung mit MySQL als root lieber ein Passwort verwenden, müssen Sie das Authentifizierungsverfahren von auth _ socket in mysql _ native _ password ändern.
Öffnen Sie dazu die MySQL -Eingabeaufforderung auf Ihrem Terminal:
Prüfen Sie dann mit folgendem Befehl, welches Authentifizierungsverfahren Ihre MySQL -Benutzerkonten verwenden:
In diesem Beispiel können Sie sehen, dass der root-Benutzer über das auth _ socket-Plugin authentifiziert wird.
Um das root-Konto zur Authentifizierung mit einem Passwort konfigurieren zu können, führen Sie den folgenden ALTER USER-Befehl aus.
Denken Sie daran, < ^ > password < ^ > in ein starkes Passwort Ihrer Wahl zu ändern:
Führen Sie dann FLUSH PRIVILEGES aus, die den Server anweisen, die Berechtigungstabellen neu zu laden und die Änderungen zu übernehmen:
Prüfen Sie die von Ihren Benutzern verwendeten Authentifizierungsmethoden erneut, um zu bestätigen, dass root nicht mehr über das auth _ socket-Plugin authentifiziert wird:
In diesem Beispiel können Sie sehen, dass der MySQL root-Benutzer jetzt mit einem Passwort authentifiziert wird.
Nach der Bestätigung auf Ihrem eigenen Server können Sie die MySQL-Shell beenden:
Jetzt ist Ihr Datenbanksystem eingerichtet und Sie können PHP installieren, den letzten Bestandteil des LAMP-Stacks.
PHP ist der Bestandteil Ihres Setups, der Code verarbeitet, um dynamischen Inhalt anzuzeigen.
Es kann Skripte ausführen, zwecks Datenabruf Verbindungen zu Ihren MySQL-Datenbanken herstellen und den verarbeiteten Inhalt zur Ansicht auf Ihren Webserver übertragen.
Verwenden Sie erneut das apt-System, um PHP zu installieren.
Setzen Sie hier auch ein paar Hilfspakete ein, damit PHP-Code auf dem Apache-Server laufen und mit Ihrer MySQL-Datenbank kommunizieren kann:
Damit sollte PHP ohne Probleme installiert werden.
Wir werden das gleich testen.
In den meisten Fällen sollten Sie die Art ändern, wie Apache Dateien überliefert, wenn ein Verzeichnis angefordert wird.
Wenn derzeit ein Benutzer ein Verzeichnis vom Server anfordert, sucht Apache zunächst nach der Datei index.html.
Wir möchten dem Webserver mitteilen, PHP-Dateien bevorzugt anzufordern. Daher sollte Apache erst nach der Datei index.php suchen.
Geben Sie dazu diesen Befehl ein, um die Datei dir.conf in einem Texteditor mit Rootberechtigung zu öffnen:
Sie wird ungefähr so aussehen:
Verschieben Sie die PHP-Indexdatei (oben hervorgehoben) auf die erste Position nach der DirectoryIndex-Spezifikation, wie folgt:
Speichern und schließen Sie dann die Datei durch Drücken von CTRL + X.
Bestätigen Sie das Speichern, indem Sie Y eingeben und dann mit der ENTER den Dateispeicherort verifizieren.
Starten Sie daran anschließend den Apache-Webserver neu, damit Ihre Änderungen erkannt werden.
Geben Sie dazu Folgendes ein:
Sie können auch mit systemctl den Status des apache2-Dienstes prüfen:
Drücken Sie Q, um diese Statusausgabe zu beenden.
Um die Funktionalität von PHP zu verbessern, haben Sie die Möglichkeit, zusätzliche Module zu installieren.
Um die verfügbaren Optionen für PHP-Module und Bibliotheken zu sehen, sollten Sie die Ergebnisse der apt Suche in less einspeisen, einem Pager, mit dem Sie durch die Ausgabe anderer Befehle scrollen können:
Verwenden Sie die Pfeiltasten, um nach oben und unten zu scrollen und drücken Sie zum Beenden auf Q.
Die Ergebnisse sind alle optionalen Komponenten, die Sie installieren können.
Sie erhalten eine kurze Beschreibung jeder Komponente:
Um mehr über die Funktion der einzelnen Module zu erfahren, können Sie im Internet nach weiteren Informationen suchen.
Sie können sich auch die lange Beschreibung des Pakets ansehen, indem Sie Folgendes eingeben:
Sie erhalten zahlreiche Ausgaben und ein Feld Description mit längeren Erklärungen zur Funktionalität, die das jeweilige Modul bereitstellt.
Um beispielsweise herauszufinden, was das php-cli-Modul macht, können Sie Folgendes eingeben:
Neben einer großen Menge weiterer Informationen finden Sie etwas, das ungefähr so aussieht:
Wenn Sie sich nach Ihren Recherchen entscheiden, ein Paket zu installieren, können Sie dies mit dem apt install-Befehl tun, wie Sie es bereits für die andere Software getan haben.
Wenn Sie sich für php-cli entschieden haben, können Sie Folgendes eingeben:
Wenn Sie mehr als ein Modul installieren möchten, können Sie die durch ein Leerzeichen getrennten Module nach dem apt install-Befehl auflisten, wie zum Beispiel:
Jetzt ist Ihr LAMP-Stack installiert und konfiguriert.
Bevor Sie etwas Anderes tun, empfehlen wir, dass Sie einen virtuellen Apache Host einrichten, wo Sie die Konfigurationsinformationen Ihres Servers speichern können.
Um mehr über die Einrichtung eines Domänenamens mit DigitalOcean zu erfahren, lesen Sie unsere Einführung zu DigitalOcean DNS.
Apache auf Ubuntu 18.04 hat einen Server-Block, der standardmäßig aktiviert und so konfiguriert ist, dass er Dokumente aus dem / var / www / html-Verzeichnis bereitstellt.
Statt / var / www / html zu ändern, erstellen wir eine Verzeichnisstruktur innerhalb / var / www für unsere your _ domain und lassen dabei / var / www / html als Standardverzeichnis, das bereitgestellt wird, wenn eine Client-Anfrage keine übereinstimmenden Sites ergibt.
Weisen Sie als Nächstes das Eigentum am Verzeichnis mit der $USER Umgebungsvariablen zu:
Erstellen Sie als Nächstes eine index.html Beispielsseite durch die Verwendung von nano oder Ihrem bevorzugten Texteditor:
Sie müssen eine virtuelle Host-Datei mit den richtigen Richtlinien erstellen, damit Apache diesen Inhalt bereitstellen kann.
Statt die Standardkonfigurationsdatei direkt unter / etc / apache2 / sites-available / 000-default.conf zu ändern, erstellen wir eine neue in / etc / apache2 / sites-available / < ^ > your _ domain < ^ > .conf:
Fügen Sie den folgenden Konfigurationsblock ein, der dem Standard ähnlich ist, aber für unser neues Verzeichnis und den Domänenamen aktualisiert wurde:
Sie werden feststellen, dass wir DocumentRoot auf unser neues Verzeichnis und ServerAdmin auf eine E-Mail aktualisiert haben, auf die der Site-Administator von your _ domain zugreifen kann.
Wir haben auch zwei Richtlinien hinzugefügt: ServerName, der die Basisdomäne festlegt, die zu dieser virtuellen Host-Definition passen sollte, und ServerAlias, der weitere Namen definiert, die wie der Basisname passen sollten.
Aktivieren wir jetzt die Datei über das Tool a2ensite:
Als Nächstes testen wir auf Konfigurationsfehler:
Sie können dies testen, indem Sie zu http: / / < ^ > your _ domain < ^ > navigieren, wo Sie etwas Ähnliches wie dies sehen sollten:
Damit ist Ihr virtueller Host vollständig eingerichtet.
Bevor Sie allerdings mehr Änderungen vornehmen oder eine Anwendung ensetzen, wäre es hilfreich, Ihre PHP-Konfiguration proaktiv zu testen, für den Fall, dass es Probleme gibt, die behoben werden müssen.
Um die für PHP korrekte Systemkonfiguration zu testen, erstellen Sie ein sehr einfaches PHP-Skript namens info.php.
Damit Apache diese Datei finden und sie korrekt bereitstellen kann, muss sie in Ihrem Web-Root-Verzeichnis gespeichert sein.
Erstellen Sie die Datei im zuvor erstellten Web-Root-Verzeichnis, indem Sie Folgendes ausführen:
Jetzt können Sie testen, ob Ihr Webserver den Inhalt korrekt anzeigen kann, der von diesem PHP-Skript generiert wurde.
Rufen Sie dazu diese Seite in Ihrem Web-Browser auf.
Sie benötigen erneut die öffentliche IP-Adresse des Servers.
Besuchen Sie dazu folgende Adresse:
Sie sollten auf einer Seite landen, die ungefähr so aussieht:
Ubuntu 18.04 Standard PHP Info
Diese Seite stellt einige grundlegende Informationen über den Server aus der Sicht von PHP bereit.
Wenn Sie diese Seite in Ihrem Browser sehen können, funktioniert Ihr PHP wie geplant.
Nach diesem Test sollten Sie die Datei entfernen, weil sie unberechtigten Benutzer Informationen über Ihren Server anzeigen könnte.
Führen Sie dazu folgenden Befehl aus:
Jetzt haben Sie einen LAMP-Stack installiert und verfügen über viele Möglichkeiten für die nächsten Schritte.
Sie haben kurz gesagt eine Plattform installiert, die es Ihnen ermöglicht, die meisten Arten von Websites und Web-Software auf Ihrem Server zu installieren.
Im nächsten Schritt sollten Sie sicherstellen, dass Verbindungen zu Ihrem Webserver gesichert sind, indem Sie sie über HTTPS bereitstellen.
Die einfachste Möglichkeit ist die Verwendung von Let 's Encrypt, um Ihre Website mit einem kostenlosen TLS / SSL-Zertifikat zu sichern.
Andere gängige Optionen sind:
Installieren Sie Wordpress, das beliebteste Content-Managementsystem im Internet.
Richten Sie PHPMyAdmin ein, um Ihre MySQL-Datenbanken im Web-Browser zu verwalten.
Erlernen Sie SFTP, um Dateien auf den Server und vom Server zu übertragen.
Installation von WordPress mit LAMP auf Ubuntu 18.04
2692
WordPress ist das beliebteste CMS (Content Management System) im Internet.
Damit können Sie flexible Blogs und Websites auf einem MySQL Backend mit PHP-Verarbeitung einrichten.
WordPress weist eine enorm hohe Akzeptanz auf und ist eine großartige Wahl, um eine Website schnell zu erstellen und freizuschalten.
Nach dem Einrichten kann fast die gesamte Verwaltung vom Web-Frontend aus erfolgen.
In diesem Leitfaden konzentrieren wir uns auf das Einrichten einer WordPress-Instanz auf einem LAMP-Stack (Linux, Apache, MySQL und PHP) auf einem Ubuntu 18.04 Server.
Um dieses Tutorial fertigzustellen, benötigen Sie Zugriff auf einen Ubuntu 18.04 Server.
Sie müssen die folgenden Aufgaben ausführen, bevor Sie diesen Leitfaden starten können:
Erstellen Sie einen sudo-Benutzer auf Ihrem Server: In diesem Leitfaden verwenden wir einen Benutzer ohne Rootberechtigung mit sudo-Rechten.
Sie können einen Benutzer mit sudo-Rechten erstellen, indem Sie unserem Leitfaden zur Ersteinrichtung des Ubuntu 18.04 Servers folgen.
Installation eines LAMP-Stacks: Wordpress benötigt einen Webserver, eine Datenbank und PHP, um richtig zu funktionieren.
Das Einrichten eines LAMP-Stacks (Linux, Apache, MySQL und PHP) erfüllt alle diese Anforderungen.
Folgen Sie diesem Leitfaden zur Installation und Konfiguration dieser Software.
Sicherung Ihrer Site mit SSL: WordPress stellt dynamischen Inhalt bereit und kümmert sich um die Benutzerauthentifizierung und Berechtigung.
TLS / SSL ist die Technologie, die es Ihnen ermöglicht, den Datenverkehr von Ihrer Website zu verschlüsseln, damit die Verbindung gesichert ist.
Die Art, wie Sie SSL einichten, hängt davon ab, ob Sie einen Domänenamen für Ihre Site haben.
Wenn Sie einen Domänenamen haben... können Sie Ihre Website am einfachsten mit Let 's Encrypt sichern, das kostenlose, vertrauenswürdige Zertifikate bereitstellt.
Folgen Sie dem Leitfaden Let 's Encrypt für Apache, um dieses Setup vorzunehmen.
Wenn Sie keine Domäne haben... und Sie diese Konfiguration nur zu Testzwecken oder privat verwenden, können Sie stattdessen ein selbstsigniertes Zertifikat verwenden.
Damit wird die gleiche Art der Verschlüsselung bereitgestellt, aber ohne Domänenvalidierung.
Folgen Sie dem Leitfaden für selbstsigniertes SSL für Apache, um dieses Setup vorzunehmen.
Wenn Sie dieses Setup abgeschlossen haben, melden Sie sich bei Ihrem Server als sudo-Benutzer an und fahren Sie dann wie unten gezeigt fort.
Schritt 1 - Erstellen einer MySQL-Datenbank und eines Benutzers für WordPress
Der erste Schritt dient der Vorbereitung.
WordPress verwendet MySQL, um Site- und Benutzerinformationen zu verwalten und zu speichern.
Wir haben bereits MySQL installiert, müssen aber eine Datenbank und einen Benutzer für WordPress erstellen.
Dazu melden Sie sich beim Konto MySQL Root (administrativ) an, indem Sie diesen Befehl eingeben:
Sie werden aufgefordert, das Passwort einzugeben, das Sie für das MySQL Root-Konto festgelegt haben, als Sie die Software installiert haben.
Zuerst können wir eine separate Datenbank erstellen, die von WordPress kontrolliert wird.
Sie können ihr einen beliebigen Namen geben, aber wir nennen sie der Einfachheit halber in diesem Leitfaden wordpress.
Erstellen Sie die Datenbank für WordPress, indem Sie Folgendes eingeben:
< $> note Anmerkung: Jede MySQL -Anweisung muss mit einem Semikolon (;) enden.
Vergewissern Sie sich, dass dies der Fall ist, falls ein Problem auftritt.
Als Nächstes erstellen wir ein separates MySQL-Benutzerkonto, das wir ausschließlich für die neue Datenbank verwenden werden.
Aus Verwaltungs- und Sicherheitsaspekten ist es sinnvoll, Datenbanken und Konten mit nur einer Funktion zu erstellen.
In diesem Leitfaden verwenden wir den Namen wordpressuser.
Sie können den Namen ändern.
Wir erstellen dieses Konto, legen ein Passwort fest und gewähren Zugriff auf die von uns erstellte Datenbank.
Dazu geben wir den folgenden Befehl ein.
Denken Sie daran, ein starkes Passwort für Ihren Datenbankbenutzer zu wählen:
Sie haben jetzt ein Datenbank- und ein Benutzerkonto, das jeweils speziell für WordPress eingerichtet wurde.
Wir müssen die Rechte leeren, damit die aktuelle Instanz von MySQL die neuesten Änderungen erkennt:
Beenden Sie MySQL, indem Sie Folgendes eingeben:
Schritt 2 - Installation von zusätzlichen PHP-Erweiterungen
Beim Einrichten des LAMP-Stacks benötigen wir nur eine sehr minimale Menge von Erweiterungen, damit PHP mit MySQL kommuniziert.
WordPress und viele seiner Plugins nutzen zusätzliche PHP-Erweiterungen.
Laden Sie einige der beliebtesten PHP-Erweiterungen zur Nutzung mit WordPress herunter und installieren diese, indem Sie Folgendes eingeben:
< $> note Anmerkung: Jedes WordPress-Plugin hat seinen eigenen Satz an Anforderungen.
Einige können die Installation zusätzlicher PHP-Pakete erfordern.
Prüfen Sie Ihre Plugin-Dokumentation, um deren PHP-Anforderungen herauszufinden.
Wenn sie verfügbar sind, können sie wie oben gezeigt mit apt installiert werden.
Wir starten dann Apache neu, um diese neuen Erweiterungen im nächsten Abschnitt zu laden.
Wenn Sie wieder hierher zurückkehren, um zusätzliche Plugins zu installieren, können Sie Apache jetzt neu starten, indem Sie Folgendes eingeben:
Schritt 3 - Anpassung der Apache-Konfiguration, um .htaccess Override und Rewrites zu erlauben
Als Nächstes werden wir einige kleinere Anpassungen an unsere Apache-Konfiguration vornehmen.
Basierend auf den erforderlichen Tutorials sollten Sie eine Konfigurationsdatei für Ihre Site im Verzeichnis / etc / apache2 / sites-available / haben.
Als Beispiel verwenden wir / etc / apache2 / sites-available / wordpress.conf, aber Sie sollten den Pfad zu Ihrer Konfigurationsdatei ggf. ersetzen.
Außerdem verwenden wir / var / www / wordpress als Root-Verzeichnis unserer WordPress-Installation.
Sie sollten das in Ihrer eigenen Konfiguration angegebene Web-Root verwenden.
< $> note Anmerkung: Es ist möglich, dass Sie die Standardkonfiguration 000-default.conf verwenden (mit / var / www / html als Ihre Web-Root).
Das ist in Ordnung, wenn Sie nur eine Website auf diesem Server hosten wollen.
Ansonsten ist es am besten, die notwendige Konfiguration in logische Chunks von einer Datei pro Site aufzuteilen.
Aktivieren von .htaccess Overrides
Derzeit ist die Verwendung von .htaccess-Dateien deaktiviert.
WordPress und viele WordPress-Plugins setzen diese Dateien weitläufig für kleine Veränderungen des Webserver-Verhaltens im Verzeichnis ein.
Öffnen Sie die Apache-Konfigurationsdatei für Ihre Website:
Um .htaccess-Dateien zu erlauben, müssen wir die Anweisung AllowOverride in einem Verzeichnisblock festlegen, der auf unsere Dokument-Root verweist.
Fügen Sie Ihrer Konfigurationsdatei im VirtualHost-Block den folgenden Textblock hinzu, wobei Sie sichergehen müssen, dass Sie das korrekte Web-Root-Verzeichnis verwenden:
Aktivierung des Rewrite-Moduls
Als Nächstes können wir mod _ rewrite aktivieren, damit wir die WordPress Permalink-Funktion verwenden können:
Aktivierung der Änderungen
Bevor wir die von uns vorgenommenen Änderungen implementieren, sollten Sie bestätigen, dass Ihnen keine Syntaxfehler unterlaufen sind:
Die Ausgabe kann eine vergleichbare Meldung enthalten:
Wenn Sie die oberste Zeile ausblenden wollen, fügen Sie Ihrer Haupt- (global) Apache-Konfigurationsdatei unter / etc / apache2 / apache2.conf einfach einen ServerName hinzu.
Der ServerName kann die Domäne oder IP-Adresse Ihres Servers sein.
Hier handelt sich allerdings nur um eine Nachricht, die die Funktionalität der Site nicht beeinträchtigt.
Solange die Ausgabe Syntax OK enthält, können Sie weitermachen.
Starten Sie Apache erneut, um Ihre Änderungen zu implementieren:
Als Nächstes laden wir WordPress selbst herunter und richten es ein.
Schritt 4 - Herunterladen von WordPress
Jetzt ist unsere Server-Software konfiguriert, und wir können WordPress herunterladen und einrichten.
Aus Sicherheitsgründen ist es immer ratsam, die neueste Version von WordPress von der Site zu erhalten.
Wechseln Sie in ein beschreibbares Verzeichnis und laden dann die komprimierte Version herunter, indem Sie Folgendes eingeben:
Extrahieren Sie die komprimierte Datei, um die WordPress-Verzeichnisstruktur zu erstellen:
Wir werden diese Dateien gleich in unsere Dokument-Root verschieben.
Zuvor können wir eine .htaccess-Stellvertreterdatei hinzufügen, damit diese für WordPress später verfügbar ist.
Erstellen Sie die Datei, indem Sie Folgendes eingeben:
Wir kopieren auch die Muster-Konfigurationsdatei in den Dateinamen, den WordPress tatsächlich liest:
Wir können auch das upgrade-Verzeichnis erstellen, damit WordPress keine Berechtigungsprobleme bekommt, wenn es nach einem Software-Update versucht, dies selbst abzuwickeln:
Jetzt können wir den gesamten Inhalt des Verzeichnisses in unsere Dokument-Root kopieren.
An das Ende unseres Quellverzeichnisses setzen wir einen Punkt, damit alles im Verzeichnis kopiert werden kann, einschließlich ausgeblendeter Dateien (wie die von uns erstellte .htaccess-Datei):
Schritt 5 - Konfiguration des WordPress-Verzeichnisses
Bevor wir das webbasierte WordPress Setup vornehmen, müssen wir einige Elemente in unserem WordPress-Verzeichnis anpassen.
Anpassung des Eigentums und der Berechtigungen
Das Wichtigste ist das Einrichten vernünftiger Dateiberechtigungen und Eigentumsrechte.
Wir beginnen damit, das Eigentum an allen Dateien auf www-data-Benutzer und -Gruppen zu übertragen.
Das ist der Benutzer, unter dem der Apache-Webserver läuft, und Apache muss WordPress-Dateien lesen und schreiben können, damit die Website verwendet werden kann und automatisch Updates ausgeführt werden können.
Aktualisieren Sie die Eigentsumsrechte mit chown:
Als Nächstes werden wir zwei find Befehle ausführen, um die richtigen Berechtigungen in den WordPress-Verzeichnissen und -Dateien einzustellen:
Das sollten von Anfang an angemessene Berechtigungen sein.
Einige Plugins und Verfahren können zusätzliche Veränderungen erfordern.
Einrichten der WordPress-Konfigurationsdatei
Jetzt müssen wir Änderungen an der WordPress-Hauptkonfigurationsdatei vornehmen.
Wenn wir die Datei öffnen, werden wir zunächst einige geheime Schlüssel ändern müssen, um unsere Installation zu sichern.
WordPress stellt einen sicheren Generator für diese Werte bereit, damit Sie sie sich nicht ausdenken müssen.
Sie werden nur intern verwendet, d. h. komplexe, sichere Werte haben keine Auswirkungen auf die Benutzerfreundlichkeit.
Um sichere Werte vom WordPress Secret Key Generator zu erhalten, geben Sie Folgendes ein:
Sie werden eindeutige Werte erhalten, die ungefähr so aussehen:
< $> warning Warnung!
Es ist wichtig, dass Sie jedes Mal eindeutige Werte anfordern.
Kopieren Sie NICHT die unten aufgeführten Werte!
Das sind Konfigurationszeilen, die wir direkt in unsere Konfigurationsdatei einfügen können, um sichere Schlüssel einzustellen.
Kopieren Sie die Ausgabe, die Sie erhalten haben.
Öffnen Sie jetzt die WordPress-Konfigurationsdatei:
Finden Sie den Abschnitt, der die Stellvertreterwerte für diese Einstellungen enthält.
Löschen Sie diese Zeilen und fügen Sie die Werte ein, die Sie aus der Befehlszeile kopiert haben:
Als Nächstes müssen wir einige der Datenbank-Verbindungseinstellungen am Anfang der Datei ändern.
Sie müssen den Datenbanknamen, den Datenbankbenutzer und das zugehörige Passwort anpassen, das wir in MySQL konfiguriert haben.
Wir müssen außerdem die Methode festlegen, die WordPress verwenden sol, um in das Dateisystem zu schreiben.
Da wir dem Webserver das Recht auf beliebiges Schreiben erteilt haben, können wir die Dateisystem-Methode auf "direct" festlegen.
Wenn wir das nicht auf unsere aktuellen Einstellungen anpassen, fragt WordPress bei bestimmten Handlungen nach FTP-Anmeldeangaben.
Diese Einstellung kann unterhalb der Datenbank-Verbindungseinstellungen oder anderswo in der Datei eingefügt werden:
Schritt 6 - Abschluss der Installation über die Web-Oberfläche
Jetzt ist die Serverkonfiguration abgeschlossen, und wir können die Installation über die Web-Oberfläche abschließen.
Navigieren Sie im Web-Browser zum Domänenamen oder zur öffentlichen IP-Adresse Ihres Servers:
Wählen Sie die Sprache aus, die Sie verwenden möchten:
WordPress-Sprachauswahl
Als Nächstes kommen Sie zur Setup-Hauptseite.
Wählen Sie einen Namen für Ihre WordPress-Website und einen Benutzernamen aus (aus Sicherheitsgründen sollten Sie kein Wort mit "admin" wählen).
Ein starkes Passwort wird automatisch erstellt.
Speichern Sie dieses oder wählen Sie anderes starkes Passwort aus.
Geben Sie Ihre E-Mail-Adresse ein und wählen Sie, ob Sie Suchmaschinen aus der Indexierung Ihrer Site ausschließen wollen:
WordPress-Setup-Installation
Wenn Sie weiter klicken, kommen Sie zu einer Seite, auf der Sie sich anmelden müssen:
WordPress-Anmeldung
Nach der Anmeldung gelangen Sie zum WordPress-Administrations-Dashboard:
WordPress sollte jetzt installiert und einsatzbereit sein!
Einige häufige nächste Schritte sind die Auswahl der Permalink-Einstellungen für Ihre Posts (diese sind unter Einstellungen > Permalinks zu finden) oder die Auswahl eines neuen Designs (in Darstellung > Design).
Wenn Sie WordPress das erste Mal verwenden, schauen Sie sich die Oberfläche an, um Ihre neue CMS kennenzulernen.
Einrichtung von SSH-Schlüsseln unter Ubuntu 18.04
2620
In diesem Leitfaden konzentrieren wir uns auf die Einrichtung von SSH-Schlüsseln für eine Vanilla Ubuntu 18.04-Installation.
Schritt 1 - RSA-Schlüsselpaar erstellen
Standardmäßig erstellt ssh-keygen ein 2048-Bit-RSA-Schlüsselpaar, das für die meisten Anwendungsfälle sicher genug ist (Sie können optional das Flag -b 4096 übergeben, um einen größeren 4096-Bit-Schlüssel zu erstellen).
Schritt 2 - Den öffentlichen Schlüssel auf Ubuntu-Server kopieren
Kopieren des öffentlichen Schlüssels mit ssh-copy-id
Um das Utility zu verwenden, müssen Sie lediglich den Remote-Host, zu dem Sie eine Verbindung herstellen möchten, und das Benutzerkonto angeben, auf das Sie mit einem Passwort SSH-Zugriff haben.
Schritt 3 - Authentifizierung bei Ubuntu-Server mit SSH-Schlüsseln
Wenn Sie eines der oben genannten Verfahren erfolgreich abgeschlossen haben, sollten Sie sich beim Remote-Host anmelden können, ohne das Passwort des Remote-Kontos zu verwenden.
Schritt 4 - Passwortauthentifizierung auf Ihrem Server deaktivieren
Dies kann auskommentiert werden. Kommentieren Sie die Zeile aus und setzen Sie den Wert auf "Nein".
Dadurch wird Ihre Fähigkeit, sich über SSH mit Kontopasswörtern anzumelden, deaktiviert:
Wenn Sie fertig sind, speichern und schließen Sie die Datei, indem Sie CTRL + X, dann Y drücken, um das Speichern der Datei zu bestätigen. Drücken Sie zum Abschluss die ENTER, um nano zu beenden.
Öffnen Sie vorsichtshalber ein neues Terminalfenster und testen Sie, ob der SSH-Dienst ordnungsgemäß funktioniert, bevor Sie diese Sitzung schließen:
Nachdem Sie Ihren SSH-Dienst überprüft haben, können Sie alle aktuellen Serversitzungen sicher schließen.
Der SSH-Daemon auf Ihrem Ubuntu-Server reagiert jetzt nur noch auf SSH-Schlüssel.
Grundlegendes zu Klassen in JavaScript
2626
JavaScript ist eine prototypbasierte Sprache, und jedes Objekt in JavaScript verfügt über eine versteckte interne Eigenschaft namens [[Prototype]], mit der Objekteigenschaften und -methoden erweitert werden können.
Weitere Informationen über Prototypen finden Sie in unserem Tutorial Prototypen und Vererbung in JavaScript verstehen.
Bis vor kurzem verwendeten fleißige Entwickler Konstruktorfunktionen, um ein objektorientiertes Entwurfsmuster in JavaScript nachzuahmen.
Die Sprachspezifikation ECMAScript 2015, oft als ES6 bezeichnet, führte Klassen in die JavaScript-Sprache ein.
Klassen in JavaScript bieten keine zusätzlichen Funktionen und werden häufig als "syntaktischer Zucker" gegenüber Prototypen und Vererbung bezeichnet, da sie eine klarere und elegantere Syntax bieten.
Da andere Programmiersprachen Klassen verwenden, erleichtert die Klassensyntax in JavaScript Entwicklern das Wechseln zwischen Sprachen.
Klassen sind Funktionen
Eine JavaScript-Klasse ist eine Art von Funktion.
Klassen werden mit dem Schlüsselwort class deklariert.
Wir werden die Syntax von Funktionsausdrücken verwenden, um eine Funktion zu initialisieren, und die Syntax von Klassenausdrücken, um eine Klasse zu initialisieren.
Mithilfe der Methode Object.getPrototypeOf () können wir auf den [[Prototype]] eines Objekts zugreifen.
Damit werden wir jetzt die von uns erstellte leere Funktion testen.
Wir können diese Methode auch für die Klasse verwenden, die wir gerade erstellt haben.
Der mit function und class deklarierte Code gibt eine Funktion [[Prototype]] zurück.
Bei Prototypen kann jede Funktion durch Verwendung des Schlüsselworts new zu einer Konstruktorinstanz werden.
Dies gilt auch für Klassen.
Diese Prototyp-Konstruktorbeispiele sind ansonsten leer, aber wir können sehen, wie unter der Syntax beide Methoden das gleiche Endergebnis erzielen.
Definieren einer Klasse
In dem Tutorial Prototypen und Vererbung haben wir ein Beispiel erstellt, das auf der Charaktererstellung in einem textbasierten Rollenspiel basiert.
Fahren wir hier mit diesem Beispiel fort, um die Syntax von Funktionen auf Klassen zu aktualisieren.
Eine Konstruktorfunktion wird mit einer Reihe von Parametern initialisiert, die als Eigenschaften von this zugewiesen werden, was die Referenz auf die Funktion selbst darstellt.
Der erste Buchstabe des Bezeichners wird entsprechend der Konvention großgeschrieben.
Wenn wir dies in die unten dargestellte class-Syntax übersetzen, sehen wir, dass sie sehr ähnlich strukturiert ist.
Anhand der Großschreibung des Anfangsbuchstabens des Initialisierers (optional) und durch die Kenntnis der Syntax wissen wir, dass eine Konstruktorfunktion ein Objektentwurf sein soll.
Das Schlüsselwort class kommuniziert auf einfachere Weise das Ziel unserer Funktion.
Der einzige Unterschied in der Syntax der Initialisierung besteht darin, das Schlüsselwort class anstatt von function zu verwenden und die Eigenschaften in einer constructor () -Methode zuzuweisen.
Definieren von Methoden
Die übliche Vorgehensweise bei Konstruktorfunktionen besteht darin, Methoden anstatt in der Initialisierung direkt dem prototype zuzuweisen, wie in der greet () -Methode unten dargestellt.
Mit Klassen wird diese Syntax vereinfacht, und die Methode kann direkt zur Klasse hinzugefügt werden.
Unter Verwendung der in ES6 eingeführten Kurzform method definition ist das Definieren einer Methode ein noch präziserer Prozess.
Sehen wir uns diese Eigenschaften und Methoden in Aktion an.
Wir erstellen eine neue Instanz von Hero mit dem Schlüsselwort new und weisen einige Werte zu.
Wenn wir mit console.log (hero1) mehr Informationen über unser neues Objekt ausdrucken, können wir mehr Details dazu sehen, was mit der Klasseninitialisierung passiert.
Aus der Ausgabe wird ersichtlich, dass die Funktionen constructor () und greet () auf _ _ proto _ _ oder [[Prototype] von hero1 angewendet wurden, nicht direkt als Methode für das Objekt hero1.
Während dies beim Erstellen von Konstruktorfunktionen klar ist, ist es beim Erstellen von Klassen nicht offensichtlich.
Klassen ermöglichen eine einfachere und prägnantere Syntax; sie büßen jedoch an Klarheit ein.
Erweitern einer Klasse
Ein Vorteil von Konstruktorfunktionen und -klassen besteht darin, dass sie auf der Basis des übergeordneten Objekts zu neuen Objektentwürfen erweitert werden können.
Dies verhindert die Wiederholung von Code für Objekte, die ähnlich sind, jedoch einige zusätzliche oder spezifischere Funktionen benötigen.
Neue Konstruktorfunktionen können vom übergeordneten Element mit der Methode call () erstellt werden.
Im nachstehenden Beispiel erstellen wir eine spezifischere Charakterklasse namens Mage und weisen ihr die Eigenschaften von Hero zu, indem wir call () verwenden. Außerdem fügen wir eine zusätzliche Eigenschaft hinzu.
Zu diesem Zeitpunkt können wir eine neue Instanz von Mage erstellen, die dieselben Eigenschaften wie Hero und die von uns neu hinzugefügte Eigenschaft verwendet.
Wenn wir hero2 an die Konsole senden, können wir sehen, dass wir einen neuen Mage erstellt haben, der auf dem Konstruktor basiert.
Bei ES6-Klassen wird das Schlüsselwort super anstelle von call verwendet, um auf die übergeordneten Funktionen zuzugreifen.
Wir werden extends verwenden, um auf die übergeordnete Klasse zu verweisen.
Jetzt können wir in derselben Weise eine neue Mage-Instanz erstellen.
Wir drucken hero2 auf der Konsole und zeigen die Ausgabe an.
Die Ausgabe ist nahezu identisch, mit der Ausnahme, dass in der Klassenkonstruktion [[Prototype]] mit dem übergeordneten Element verknüpft ist, in diesem Fall Hero.
Nachstehend finden Sie einen direkten Vergleich des gesamten Prozesses der Initialisierung, des Hinzufügens von Methoden und der Vererbung einer Konstruktorfunktion und einer Klasse.
Obwohl die Syntax sehr unterschiedlich aussieht, ist das zugrundeliegende Ergebnis in beiden Methoden nahezu identisch.
Klassen ermöglichen eine präzisere Erstellung von Objektentwürfen, und Konstruktorfunktionen beschreiben genauer, was unter der Haube geschieht.
In diesem Tutorial haben wir die Ähnlichkeiten und Unterschiede zwischen JavaScript-Konstruktorfunktionen und ES6-Klassen kennengelernt.
Sowohl Klassen als auch Konstruktoren imitieren ein objektorientiertes Vererbungsmodell für JavaScript, eine prototypbasierte Vererbungssprache.
Das Verständnis der prototypischen Vererbung ist von größter Bedeutung, damit ein JavaScript-Entwickler effektiv arbeiten kann.
Ein Verständnis von Klassen ist äußerst hilfreich, da gängige JavaScript-Bibliotheken wie React häufig die Syntax class verwenden.
So automatisieren Sie die Produktionsbereitstellung von Node.js mit Shipit unter CentOS 7
3858
Shipit ist ein universelles Automatisierungs- und Bereitstellungswerkzeug für Node.js-Entwickler.
Es bietet einen Aufgabenablauf, der auf dem populären Orchestrator-Paket, der Anmeldung und den interaktiven SSH-Befehlen über OpenSSH und einer erweiterbaren API basiert.
Entwickler können Shipit zur Automatisierung von Erstellungs- und Bereitstellungs-Workflows für eine breite Palette von Node.js-Anwendungen verwenden.
Der Shipit-Workflow ermöglicht Entwicklern nicht nur die Konfiguration von Aufgaben, sondern auch die Angabe der Reihenfolge, in der sie ausgeführt werden; ob sie synchron oder asynchron und in welcher Umgebung sie ausgeführt werden sollen.
In diesem Tutorial werden Sie Shipit installieren und konfigurieren, um eine Node.js-Anwendung aus Ihrer lokalen Entwicklungsumgebung in Ihrer Produktivumgebung bereitzustellen.
Sie werden Shipit verwenden, um Ihre Anwendung bereitzustellen und den Remote-Server zu konfigurieren, indem Sie:
die Dateien Ihrer Node.js-Anwendung von Ihrer lokalen Umgebung in die Produktivumgebung übertragen (mit rsync, git und ssh).
die Abhängigkeiten Ihrer Anwendung (Knotenmodule) installieren.
die auf dem Remote-Server laufenden Node.js-Prozesse mit PM2 konfigurieren und verwalten.
Bevor Sie dieses Tutorial beginnen, benötigen Sie Folgendes:
Zwei CentOS 7-Server (in diesem Tutorial werden sie App und Web genannt), die mit privaten Netzwerken konfiguriert sind, indem Sie dem Tutorial So richten Sie eine Node.js-Anwendung für die Produktion unter CentOS 7 ein folgen.
Nginx (auf Ihrem Web-Server), gesichert mit TLS / SSL, wie in dem Tutorial So sichern Sie Nginx mit Let 's Encrypt auf CentOS7 gezeigt.
Hinweis: Wenn Sie die Voraussetzungen chronologisch befolgen, müssen Sie nur die Schritte 1, 4 und 6 auf Ihrem Web-Server ausführen.
In Ihrer Entwicklungsumgebung installiertes Node.js und npm.
Einen lokalen Entwicklungsrechner mit installiertem rsync und git.
Auf macOS können Sie diese mit Homebrew installieren.
Um git auf Linux-Distributionen zu installieren, folgen Sie dem Tutorial So installieren Sie Git.
Einen Account mit GitHub oder einem anderen gehosteten Dienstanbieter von Git.
Dieses Tutorial verwendet GitHub.
< $> note Anmerkung: Windows-Benutzer müssen das Windows-Subsystem für Linux installieren, um die Befehle in diesem Leitfaden auszuführen.
Schritt 1 - Einrichten des Remote-Repositorys
Shipit erfordert ein Git-Repository zur Synchronisierung zwischen dem lokalen Entwicklungsrechner und dem Remote-Server.
In diesem Schritt erstellen Sie ein Remote-Repository auf Github.com.
Obwohl jeder Anbieter etwas anders ist, sind die Befehle in gewisser Weise übertragbar.
Um ein Repository zu erstellen, öffnen Sie Github.com in Ihrem Webbrowser und melden Sie sich an.
Sie werden feststellen, dass in der oberen rechten Ecke einer beliebigen Seite ein + -Symbol vorhanden ist.
Klicken Sie auf + und klicken Sie dann auf New Repository.
Github-new-repository
Geben Sie einen kurzen, einprägsamen Namen für Ihr Repository ein, z. B. hello-world.
Beachten Sie, dass der Name, den Sie hier wählen, als der Projektordner repliziert wird, von dem aus Sie auf Ihrem lokalen Rechner arbeiten werden.
Github-repository-name
Fügen Sie optional eine Beschreibung Ihres Repositorys hinzu.
Github-repository-description
Legen Sie die Sichtbarkeit Ihres Repository -entweder öffentlich oder privat - nach Ihren Wünschen fest.
Stellen Sie sicher, dass das Repository mit einem .gitignore initialisiert wird, wählen Sie Node aus der Dropdown-Liste Add gitgnore aus.
Dieser Schritt ist wichtig, um zu vermeiden, dass unnötige Dateien (wie der Ordner node _ modules) zu Ihrem Repository hinzugefügt werden.
Github-gitignore-node
Klicken Sie auf die Schaltfläche Create repository.
Das Repository muss jetzt von Github.com zu Ihrem lokalen Rechner geklont werden.
Öffnen Sie Ihr Terminal und navigieren Sie zu dem Ort, an dem Sie alle Node.js-Projektdateien speichern möchten.
Beachten Sie, dass bei diesem Vorgang ein Unterordner innerhalb des aktuellen Verzeichnisses erstellt wird.
Führen Sie den folgenden Befehl aus, um das Repository auf Ihren lokalen Rechner zu klonen:
Sie müssen < ^ > your-githug-username < ^ > und < ^ > your-github-repository-name < ^ > ersetzen, um Ihren Github-Benutzernamen und den zuvor angegebenen Repository-Namen widerzuspiegeln.
< $> note Anmerkung: Wenn Sie die Zweifaktor-Authentifizierung (2FA) auf Github.com aktiviert haben, müssen Sie beim Zugriff auf Github über die Befehlszeile anstelle Ihres Passworts ein persönliches Zugriffstoken oder einen SSH-Schlüssel verwenden.
Die Github-Hilfeseite zum Thema 2FA bietet weitere Informationen.
Wechseln Sie in das Repository, indem Sie den folgenden Befehl ausführen:
Innerhalb des Repositorys befinden sich eine einzige Datei und ein einziger Ordner, beides Dateien, die von Git zur Verwaltung des Repositorys verwendet werden.
Sie können dies wie folgt überprüfen:
Nachdem Sie nun ein funktionierendes git-Repository konfiguriert haben, erstellen Sie die Datei shipit.js, die Ihren Bereitstellungsprozess verwaltet.
Schritt 2 - Integration von Shipit in ein Node.js-Projekt
In diesem Schritt erstellen Sie ein Beispielprojekt Node.js und fügen dann die Shipit-Pakete hinzu.
Dieses Tutorial bietet eine Beispielanwendung - den Node.js Web-Server, der HTTP-Anfragen akzeptiert und mit Hello World im Klartext antwortet.
Um die Anwendung zu erstellen, führen Sie den folgenden Befehl aus:
Fügen Sie den folgenden Beispiel-Anwendungscode zu hello.js hinzu (aktualisiert die Variable APP _ PRIVATE _ IP _ ADDRESS auf die IP-Adresse Ihres App-Servers im privaten Netzwerk):
Erstellen Sie jetzt Ihre package.json-Datei für Ihre Anwendung:
Dieser Befehl erzeugt eine package.json-Datei, die Sie zur Konfiguration Ihrer Node.js-Anwendung verwenden werden.
Im nächsten Schritt fügen Sie mit der npm-Befehlszeilenschnittstelle Abhängigkeiten zu dieser Datei hinzu.
Als Nächstes installieren Sie die erforderlichen npm-Pakete mit folgendem Befehl:
Verwenden Sie hier das Flag --save-dev, da die Shipit-Pakete nur auf Ihrem lokalen Rechner benötigt werden.
Dadurch wurden auch die drei Pakete zu Ihrer package.json-Datei als Entwicklungsabhängigkeiten hinzugefügt:
Wenn Ihre lokale Umgebung konfiguriert ist, können Sie nun mit der Vorbereitung des Remote-App-Servers für Shipit-basierte Bereitstellungen fortfahren.
Schritt 3 - Vorbereitung des Remote-Anwendungsservers
In diesem Schritt verwenden Sie ssh, um sich mit Ihrem App-Server zu verbinden und Ihre Remote-Abhängigkeit rsync zu installieren.
Rsync ist ein Dienstprogramm zur effizienten Übertragung und Synchronisierung von Dateien zwischen lokalen und vernetzten Computern, indem die Änderungszeiten und Größen von Dateien verglichen werden.
Shipit verwendet rsync zur Übertragung und Synchronisierung von Dateien zwischen Ihrem lokalen Computer und dem Remote-App-Server.
Sie werden keine Befehle direkt an rsync erteilen; Shipit übernimmt das für Sie.
< $> note Anmerkung: Mit So richten Sie eine Node.js-Anwendung für die Produktion unter CentOS 7 ein haben Sie die zwei Server App und Web erstellt.
Diese Befehle sollten nur auf App ausgeführt werden.
Verbinden Sie Ihren Remote-App-Server über ssh:
Installieren Sie rsync auf Ihrem Server, indem Sie den folgenden Befehl ausführen:
Bestätigen Sie die Installation mit:
In der Ausgabe dieses Befehls sehen Sie eine ähnliche Zeile:
Sie können Ihre ssh-Sitzung durch Eingabe von exit beenden.
Wenn rsync installiert und in der Befehlszeile verfügbar ist, können Sie mit den Bereitstellungsaufgaben und ihrer Beziehung zu den Ereignissen fortfahren.
Schritt 4 - Konfigurieren und Ausführen von Bereitstellungsaufgaben
Sowohl Ereignisse als auch Aufgaben sind Schlüsselkomponenten von Shipit-Bereitstellungen, und es ist wichtig zu verstehen, wie sie die Bereitstellung Ihrer Anwendung ergänzen.
Die von Shipit ausgelösten Ereignisse stellen bestimmte Punkte im Lebenszyklus der Bereitstellung dar.
Ihre Aufgaben werden als Reaktion auf diese Ereignisse ausgeführt, basierend auf der Reihenfolge des Shipit-Lebenszyklus.
Ein gängiges Beispiel, bei dem dieses Aufgaben- / Ereignissystem in einer Node.js-Anwendung nützlich ist, ist die Installation der Abhängigkeiten der Anwendung (node _ modules) auf dem Remote-Server.
Später in diesem Schritt lassen Sie Shipit auf das Ereignis updated (das nach der Übertragung der Anwendungsdateien ausgegeben wird) lauschen und eine Aufgabe ausführen, um die Abhängigkeiten der Anwendung (npm install) auf dem Remote-Server zu installieren.
Zum Lauschen auf Ereignisse und Ausführen von Aufgaben benötigt Shipit eine Konfigurationsdatei, die Informationen über Ihren Remote-Server enthält (den App-Server) und die Ereignis-Listener und die von diesen Aufgaben auszuführenden Befehle registriert.
Diese Datei befindet sich auf Ihrem lokalen Entwicklungsrechner im Verzeichnis Ihrer Node.js-Anwendung.
Um zu beginnen, erstellen Sie diese Datei, einschließlich Informationen über Ihren Remote-Server, die Ereignis-Listener, die Sie abonnieren möchten, und einige Definitionen Ihrer Aufgaben.
Erstellen Sie shipitfile.js in Ihrem Anwendungs-Stammverzeichnis auf Ihrem lokalen Rechner, indem Sie den folgenden Befehl ausführen:
Nachdem Sie nun eine Datei erstellt haben, muss sie mit den anfänglichen Umgebungsinformationen, die Shipit benötigt, gefüllt werden.
Dies ist in erster Linie der Standort Ihres Remote Git-Repositorys und vor allem die öffentliche IP-Adresse und das SSH-Benutzerkonto Ihres App-Servers.
Fügen Sie diese Anfangskonfiguration hinzu und aktualisieren Sie die hervorgehobenen Zeilen, um sie an Ihre Umgebung anzupassen:
Die Aktualisierung der < ^ > variables < ^ > in Ihrer shipit.initConfig-Methode stellt Shipit eine für Ihre Bereitstellung spezifische Konfiguration bereit.
Diese stellen für Shipit Folgendes dar:
deployTo: ist das Verzeichnis, in dem Shipit den Code Ihrer Anwendung auf dem Remote-Server bereitstellt.
Hier verwenden Sie den Ordner / home / für einen Benutzer ohne Rootberechtigung und mit sudo-Berechtigungen (/ home / < ^ > sammy < ^ >), da er sicher ist und Probleme mit den Berechtigungen vermieden werden.
Die Komponente / < ^ > your-domain < ^ > ist eine Namenskonvention zur Unterscheidung des Ordners von anderen Ordnern im Home-Ordner des Benutzers.
repositoryUrl: ist die URL zu dem vollständigen Git-Repository. Shipit verwendet diese URL, um sicherzustellen, dass die Projektdateien vor der Bereitstellung synchronisiert sind.
keepReleases: ist die Anzahl der Versionen, die auf dem Remote-Server aufbewahrt werden sollen.
Ein Release ist ein mit einem Datumsstempel versehener Ordner, der die Dateien Ihrer Anwendung zum Zeitpunkt der Freigabe enthält.
Diese können für das Rollback einer Bereitstellung nützlich sein.
shared: ist eine mit keepReleases korrespondierende Konfiguration, die die gemeinsame Nutzung von Verzeichnissen (shared) zwischen Versionen ermöglicht.
In diesem Fall haben wir einen einzigen Ordner node _ modules, der von allen Versionen gemeinsam genutzt wird.
production: stellt einen Remote-Server dar, auf dem Ihre Anwendung bereitgestellt wird.
In diesem Fall haben Sie einen einzigen Server (App-Server), den Sie production nennen, wobei die Konfiguration servers: Ihrem SSH user und der public ip address entspricht.
Der Name production entspricht dem Shipit Bereitstellungsbefehl, der gegen Ende dieses Tutorials verwendet wird (npx shipit < ^ > server name < ^ > deploy oder in Ihrem Fall npx shipit production deploy).
Weitere Informationen zu dem Objekt Shipit-Bereitstellungskonfiguration finden Sie im Shiptit Github-Repository.
Bevor Sie mit der Aktualisierung Ihrer shipitfile.js fortfahren, lassen Sie uns das folgende Beispielcode-Snippet durchsehen, um die Shipit-Aufgaben zu verstehen:
Dies ist eine Beispielaufgabe, die die shipit.on-Methode verwendet, um das Ereignis deploy zu abonnieren.
Diese Aufgabe wartet auf die Auslösung des Ereignisses deploy durch den Shipit-Lebenszyklus, und wenn das Ereignis empfangen wird, führt die Aufgabe die Methode shipit.start aus, die Shipit anweist, die say-hello-Aufgabe zu starten.
Die Methode shipit.on nimmt zwei Parameter an, den Namen des Ereignisses, auf das zu lauschen ist, und die Rückruffunktion, die beim Empfang des Ereignisses ausgeführt werden soll.
Unter der Methodendeklaration shipit.on wird die Aufgabe mit der Methode shipit.blTask definiert.
Dadurch wird eine neue Shipit-Aufgabe erstellt, die andere Aufgaben während ihrer Ausführung blockiert (es handelt sich um eine synchrone Aufgabe).
Die Methode shipit.blTask benötigt auch zwei Parameter, den Namen der Aufgabe, die sie definiert, und eine Rückruffunktion, die ausgeführt wird, wenn die Aufgabe durch shipit.start ausgelöst wird.
Innerhalb der Rückruffunktion dieser Beispielaufgabe (say-hello) führt die Methode shipit.local einen Befehl auf dem lokalen Rechner aus.
Der lokale Befehl gibt "hello from your local computer" in die Terminalausgabe aus.
Wenn Sie einen Befehl auf dem Remote-Server ausführen wollten, würden Sie die Methode shipit.remote verwenden.
Die beiden Methoden, shipit.local und shipit.remote, stellen eine API zur Verfügung, um Befehle entweder lokal oder als Teil einer Bereitstellung per Fernzugriff auszugeben.
Aktualisieren Sie nun die Datei shipitfile.js, um Ereignis-Listener einzubeziehen und den Shipit-Lebenszyklus mit shipit.on zu abonnieren.
Fügen Sie die Ereignis-Listener zu Ihrer shipitfile.js hinzu, indem Sie sie nach dem Kommentar-Platzhalter aus der Anfangskonfiguration / / Our tasks will go here einfügen:
Diese beiden Methoden lauschen auf die Ereignisse updated und published, die als Teil des Shipit-Bereitstellungslebenszyklus ausgesendet werden.
Wenn das Ereignis empfangen wird, initiieren sie jeweils Aufgaben mit der Methode shipit.start, ähnlich wie bei der Beispielaufgabe.
Nachdem Sie nun die Listener eingeplant haben, fügen Sie die entsprechende Aufgabe hinzu.
Fügen Sie die folgende Aufgabe zu Ihrer shipitfile.js hinzu, indem Sie sie nach den Ereignis-Listenern einfügen:
Sie deklarieren zunächst eine Aufgabe namens copy-config.
Diese Aufgabe erstellt eine lokale Datei namens ecosystem.config.js und kopiert diese Datei dann auf Ihren Remote-App-Server.
PM2 verwendet diese Datei, um Ihre Node.js-Anwendung zu verwalten.
Sie stellt PM2 die erforderlichen Dateipfadinformationen zur Verfügung, um sicherzustellen, dass Ihre zuletzt bereitgestellten Dateien ausgeführt werden.
Später im Build-Prozess erstellen Sie eine Aufgabe, die PM2 mit ecosystem.config.js als Konfiguration ausführt.
Wenn Ihre Anwendung Umgebungsvariablen benötigt (wie z. B. einen Datenbank-Verbindungszeichenfolge), können Sie diese entweder lokal in env: oder auf dem Remote-Server in env _ production: auf dieselbe Weise deklarieren, wie Sie die Variable NODE _ ENV in diesen Objekten festlegen.
Fügen Sie nach der Aufgabe copy-config Ihrer shipitfile.js-Datei die nächste Aufgabe hinzu:
Als Nächstes deklarieren Sie eine Aufgabe namens npm-install.
Diese Aufgabe verwendet ein Remote-Bash-Terminal (über shipit.remote), um die Abhängigkeiten der Anwendung (npm-Pakete) zu installieren.
Fügen Sie nach der Aufgabe npm-install Ihrer shipitfile.js-Datei die letzte Aufgabe hinzu:
Abschließend deklarieren Sie eine Aufgabe namens pm2-server.
Diese Aufgabe verwendet ebenfalls ein Remote-Bash-Terminal, um PM2 zunächst durch den Befehl delete an der Verwaltung Ihrer vorherigen Bereitstellung zu hindern und dann eine neue Instanz Ihres Node.js-Servers zu starten, die die Datei ecosystem.config.js als Variable bereitstellt.
Sie teilen PM2 auch mit, dass es Umgebungsvariablen aus dem Block production Ihrer Anfangskonfiguration verwenden soll, und fordern PM2 auf, die Anwendung zu überwachen und bei einem Absturz neu zu starten.
Die vollständige Datei shipitfile.js:
Speichern und beenden Sie die Datei, wenn Sie fertig sind.
Wenn Ihre shipitfile.js konfiguriert ist, die Ereignis-Listener und die zugehörigen Aufgaben abgeschlossen sind, können Sie mit der Bereitstellung auf dem App-Server fortfahren.
Schritt 5 - Bereitstellen Ihrer Anwendung
In diesem Schritt werden Sie Ihre Anwendung remote bereitstellen und testen, ob die Bereitstellung Ihre Anwendung für das Internet verfügbar gemacht hat.
Da Shipit die Projektdateien aus dem Remote-Git-Repository klont, müssen Sie Ihre lokalen Node.js-Anwendungsdateien von Ihrem lokalen Rechner zu Github übertragen.
Navigieren Sie zum Anwendungsverzeichnis Ihres Node.js-Projekts (wo sich die Dateien hello.js und shiptitfile.js befinden) und führen Sie den folgenden Befehl aus:
Der Befehl git status zeigt den Status des Arbeitsverzeichnisses und des Stagingbereichs an.
Damit können Sie sehen, welche Änderungen bereitgestellt wurden, welche nicht, und welche Dateien nicht von Git verfolgt werden.
Ihre Dateien werden nicht verfolgt und erscheinen in der Ausgabe rot:
Sie können diese Dateien mit dem folgenden Befehl zu Ihrem Repository hinzufügen:
Dieser Befehl erzeugt keine Ausgabe, obwohl die Dateien bei einer erneuten Ausführung von git status grün und mit einem Hinweis darauf erscheinen würden, dass es Änderungen gibt, die übernommen werden müssen.
Sie können einen Commit erstellen, indem Sie den folgenden Befehl ausführen:
Die Ausgabe dieses Befehls liefert einige Git-spezifische Informationen über die Dateien.
Jetzt müssen Sie nur noch den Commit mittels Push in das Remote-Repository übertragen, damit Shipit während der Bereitstellung Ihren App-Server klonen kann.
Die Ausgabe enthält Informationen über die Synchronisation mit dem Remote-Repository:
Um Ihre Anwendung bereitzustellen, führen Sie den folgenden Befehl aus:
Die Ausgabe dieses Befehls (die zu groß ist, um sie in seiner Gesamtheit anzuzeigen) liefert Einzelheiten über die ausgeführten Aufgaben und das Ergebnis der spezifischen Funktion.
Die folgende Ausgabe für die Aufgabe pm2-server zeigt, dass die Anwendung Node.js gestartet wurde:
Um Ihre Anwendung aus der Perspektive eines Benutzers zu sehen, können Sie die URL Ihrer Webseite < ^ > your-domain < ^ > in Ihren Browser eingeben, um auf Ihren Web-Server zuzugreifen.
Dadurch wird die Node.js-Anwendung auf dem App-Server, auf dem Ihre Dateien bereitgestellt wurden, über Reverse-Proxy bereitgestellt.
Sie sehen die Begrüßung Hello World.
< $> note Anmerkung: Nach der ersten Bereitstellung wird Ihr Git-Repository eine neu erstellte Datei namens ecosystem.config.js verfolgen.
Da diese Datei bei jeder Bereitstellung neu erstellt wird und kompilierte Anwendungsgeheimnisse enthalten kann, sollte sie vor dem nächsten git-Commit zur Datei .gitignore im Anwendungs-Stammverzeichnis auf Ihrem lokalen Rechner hinzugefügt werden.
Sie haben Ihre Node.js-Anwendung auf Ihrem App-Server bereitgestellt, der auf Ihre neue Bereitstellung verweist.
Jetzt, wo alles läuft, können Sie zur Überwachung Ihrer Anwendungsprozesse übergehen.
Schritt 6 - Überwachung Ihrer Anwendung
PM2 ist ein ausgezeichnetes Werkzeug zur Verwaltung Ihrer Remote-Prozesse, bietet aber auch Funktionen zur Überwachung der Leistung dieser Anwendungsprozesse.
Stellen Sie mit diesem Befehl über SSH eine Verbindung zu Ihrem Remote-App-Server her:
Um spezifische Informationen zu den von PM2 verwalteten Prozessen zu erhalten, führen Sie Folgendes aus:
Sie sehen eine Zusammenfassung der Informationen, die PM2 gesammelt hat.
Um detaillierte Informationen zu sehen, können Sie Folgendes ausführen:
Die Ausgabe erweitert die durch den Befehl pm2 list bereitgestellten zusammenfassenden Informationen.
Sie liefert auch Informationen über eine Reihe von Zusatzbefehlen und gibt die Speicherorte der Protokolldateien an:
PM2 bietet auch ein In-Terminal-Überwachungstool, auf das wie folgt zugegriffen werden kann:
Die Ausgabe dieses Befehls ist ein interaktives Dashboard, in dem pm2 Prozessinformationen, Protokolle, Metriken und Metadaten in Echtzeit zur Verfügung stellt.
Dieses Dashboard kann bei der Überwachung von Ressourcen und Fehlerprotokollen helfen:
Wenn Sie wissen, wie Sie Ihre Prozesse mit PM2 überwachen können, können Sie fortfahren und lernen, wie Shipit Ihnen bei dem Rollback einer vorherigen funktionierenden Bereitstellung helfen kann.
Beenden Sie ihre ssh-Sitzung auf Ihrem App-Server, indem Sie exit ausführen.
Schritt 7 - Rollback einer fehlerhaften Bereitstellung
Bei der Bereitstellung werden gelegentlich unvorhergesehene Fehler oder Probleme aufgedeckt, die zum Ausfall Ihrer Website führen.
Die Entwickler und Betreuer von Shipit haben dies vorausgesehen und Ihnen die Möglichkeit gegeben, ein Rollback zu einer vorherigen (funktionierenden) Bereitstellung Ihrer Anwendung durchzuführen.
Um sicherzustellen, dass Ihre PM2-Konfiguration erhalten bleibt, fügen Sie shipitfile.js einen weiteren Ereignis-Listener für das Ereignis rollback hinzu:
Sie fügen dem Ereignis rollback einen Listener hinzu, um Ihre Aufgaben npm-install und copy-config auszuführen.
Dies ist erforderlich, da das Ereignis updated im Gegensatz zum Ereignis published beim Rollback einer Bereitstellung nicht durch den Shipit-Lebenszyklus ausgeführt wird.
Das Hinzufügen dieses Ereignis-Listeners stellt sicher, dass Ihr PM2-Prozessmanager selbst im Falle eines Rollbacks auf die letzte Bereitstellung verweist.
Dieser Prozess ähnelt der Bereitstellung, mit einer kleinen Befehlsänderung.
Um ein Rollback auf eine frühere Bereitstellung zu versuchen, können Sie Folgendes ausführen:
Wie der Befehl deploy stellt rollback Details zum Rollback-Prozess und zu den ausgeführten Aufgaben bereit:
Über die Konfiguration keepReleases: 5 in shipitfile.js haben Sie Shipit so konfiguriert, dass 5 Versionen behalten werden.
Shipit verfolgt diese Versionen intern, um sicherzustellen, dass es in der Lage ist, bei Bedarf ein Rollback durchzuführen.
Shipit bietet auch eine praktische Möglichkeit, die Releases zu identifizieren, indem es ein Verzeichnis erstellt, das als Zeitstempel benannt wird (JJJJMMTTHHmmss - Beispiel: / home / deployer / < ^ > your-domain < ^ > / releases / 20190420210548).
Wenn Sie den Rollback-Prozess weiter anpassen wollten, können Sie auf Ereignisse lauschen, die für den Rollback-Vorgang spezifisch sind.
Diese Ereignisse können Sie dann zur Ausführung von Aufgaben verwenden, die Ihren Rollback-Prozess ergänzen.
Sie können dazu auf die Ereignisliste in der Aufschlüsselung des Shipit-Lebenszyklus zurückgreifen und die Aufgaben / Listener innerhalb Ihrer shipitfile.js konfigurieren.
Die Fähigkeit zum Rollback bedeutet, dass Sie Ihren Benutzern immer eine funktionierende Version Ihrer Anwendung zur Verfügung stellen können, selbst wenn eine Bereitstellung unerwartete Fehler / Probleme mit sich bringt.
In diesem Tutorial haben Sie einen Workflow konfiguriert, mit dem Sie eine hochgradig anpassbare Alternative zu "Platform as a Service" erstellen können, und das alles von zwei Servern aus.
Dieser Workflow ermöglicht eine kundenspezifische Bereitstellung und Konfiguration, die Prozessüberwachung mit PM2, die Möglichkeit zur Skalierung und dem Hinzufügen von Diensten oder zusätzlichen Servern oder Umgebungen zu der Bereitstellung, wenn dies erforderlich ist.
Wenn Sie daran interessiert sind, Ihre Node.js-Fähigkeiten weiterzuentwickeln, sehen Sie sich den Inhalt von DigitalOcean Node.js sowie die Serie So codieren Sie in Node.js an.
So erstellen Sie eine Node.js-Anwendung mit Docker Schnellstart
3698
In diesem Tutorial lernen Sie das Erstellen eines Anwendungs-Images für eine statische Website, die das Express-Framework und Bootstrap verwendet.
Sie erstellen dann einen Container mit diesem Image, übertragen ihn mittels Push auf den Docker Hub und erstellen damit einen anderen Container. So wird gezeigt, wie eine Anwendung neu erstellt und skaliert werden kann.
Eine detailliertere Version dieses Tutorials mit schrittweisen Erklärungen finden Sie unter So erstellen Sie eine Node.js-Anwendung mit Docker.
Einen sudo-Benutzer auf Ihrem Server oder in ihrer lokalen Umgebung.
Docker.
Node.js und npm.
Ein Docker Hub-Konto.
Schritt 1 - Installieren Ihrer Anwendungsabhängigkeiten
Erstellen Sie zunächst ein Verzeichnis für Ihr Projekt im Home-Verzeichnis Ihres Benutzers ohne Rootberechtigung:
Navigieren Sie zu diesem Verzeichnis:
Dies ist das Stammverzeichnis des Projekts.
Erstellen Sie als Nächstes ein package.json mit den Abhängigkeiten Ihres Projekts:
Fügen Sie die nachstehenden Informationen zum Projekt zur Datei hinzu; vergewissern Sie sich dabei, dass Sie die Informationen zum Autor durch Ihren eigenen Namen und Ihre Kontaktinformationen ersetzen:
Installieren Sie die Abhängigkeiten Ihres Projekts:
Schritt 2 - Erstellen der Anwendungsdateien
Wir erstellen eine Website, die Benutzern Informationen über Haie bereitstellt.
Öffnen Sie app.js im Haupt-Projektverzeichnis, um die Routen des Projekts zu definieren:
Fügen Sie der Datei den folgenden Inhalt zum Erstellen der Express-Anwendung und der Router-Objekte hinzu, definieren Sie das Basisverzeichnis, den Port und den Host als Variablen, bestimmen Sie die Routen und binden Sie die Router Middleware gemeinsam mit den statischen Assets der Anwendung ein:
Als Nächstes fügen wir der Anwendung statischen Inhalt hinzu.
Erstellen Sie das Verzeichnis views:
Öffnen Sie index.html:
Fügen Sie den folgenden Code zur Datei hinzu, womit Bootstrap importiert wird, und erstellen Sie ein eine jumbotron-Komponente mit einem Link zu der detaillierteren sharks.html-Infoseite:
Öffnen Sie als Nächstes eine Datei namens sharks.html:
Fügen Sie den folgenden Code hinzu, der Bootstrap und die benutzerdefinierte Formatvorlage importiert und Benutzern detaillierte Informationen über bestimmte Haie bietet:
Erstellen Sie schließlich die benutzerdefinierte CSS-Formatvorlage, mit der Sie index.html und sharks.html verknüpft haben, indem Sie zunächst einen css-Ordner in dem Verzeichnis views erstellen:
Öffnen Sie die Formatvorlage und fügen Sie den folgenden Code hinzu, womit Sie die gewünschte Farbe und Schriftart für unsere Seiten festlegen:
Starten Sie die Anwendung:
Navigieren Sie Ihren Browser zu http: / / < ^ > your _ server _ ip < ^ >: 8080 oder localhost: 8080, wenn Sie lokal arbeiten.
Sie sehen die folgende Startseite:
Anwendungs-Startseite
Klicken Sie auf die Schaltfläche Get Shark Info.
Sie sehen die folgende Informationsseite:
Shark Info-Seite
Sie haben jetzt eine funktionierende Anwendung.
Wenn Sie fertig sind, beenden Sie den Server, indem Sie CTRL + C eingeben.
Schritt 3 - Schreiben des Dockerfiles
Erstellen Sie das Dockerfile in dem Stammverzeichnis Ihres Projekts:
Dieses Dockerfile verwendet ein Alpine-Basisimage und stellt sicher, dass die Anwendungsdateien zu einem Node-Benutzer ohne Rootberechtigung gehören, der standardmäßig vom Docker Node-Image bereitgestellt wird.
Fügen Sie als Nächstes Ihre lokalen Knotenmodule, npm-Protokolle, das Dockerfile und .dockerignore zu Ihrer Datei .dockerignore hinzu:
Erstellen Sie das Anwendungs-Image mit dem Befehl docker build:
Das. gibt an, dass der Build-Kontext das aktuelle Verzeichnis ist.
Überprüfen Sie Ihre Images:
Führen Sie den folgenden Befehl zum Erstellen eines Containers mit diesem Image aus:
Inspizieren Sie die Liste Ihrer laufenden Container mit docker ps:
Wenn Ihr Container ausgeführt wird, können Sie jetzt Ihre Anwendung besuchen, indem Sie Ihren Browser zu http: / / < ^ > your _ server _ ip < ^ > oder localhost navigieren.
Sie sehen Ihre Anwendungs-Startseite erneut:
Jetzt haben Sie ein Image für Ihre Anwendung erstellt und können es für spätere Zwecke mittels Push auf Docker Hub übertragen.
Schritt 4 - Ein Repository zum Arbeiten mit Images verwenden
Der erste Schritt zum Übertragen des Images mittels Push besteht darin, sich im Docker Hub-Konto anzumelden:
Bei dieser Art von Anmeldung wird eine ~ / .docker / config.json-Datei im Home-Verzeichnis Ihres Benutzers mit Ihren Docker Hub-Anmeldeinformationen erstellt.
Übertragen Sie Ihr Image mittels Push, indem Sie Ihren eigenen Benutzernamen anstelle von < ^ > your _ dockerhub _ username < ^ > verwenden:
Sie können auch den Nutzen der Image-Registrierung testen, indem Ihren aktuellen Anwendungs-Container und das Image zerstören und dann erneut erstellen.
Listen Sie zunächst alle ausgeführten Container auf:
Stoppen Sie den ausgeführten Anwendungs-Container mit der CONTAINER ID, die in Ihrer Ausgabe aufgelistet ist.
Vergewissern Sie sich, dass Sie die nachstehende hervorgehobene ID durch Ihre eigene CONTAINER ID ersetzen:
Listen Sie alle Images mit dem Flag -a auf:
Sie sehen die folgende Ausgabe mit dem Namen Ihres Images, < ^ > your _ dockerhub _ username < ^ > / < ^ > nodejs-image-demo < ^ >, gemeinsam mit dem node-Image und den anderen Images aus Ihrem Build:
Entfernen Sie den angehaltenen Container und alle Images, einschließlich nicht verwendeter oder nachstehender Images, mit dem folgenden Befehl:
Wenn all Ihre Images und Container gelöscht sind, können Sie jetzt das Anwendungs-Image von Docker Hub übertragen:
Listen Sie Ihre Images erneut auf:
Sie sehen Ihr Anwendungs-Image:
Sie können jetzt Ihren Container mit dem Befehl aus Schritt 3 neu erstellen:
Listen Sie zunächst all Ihre ausgeführten Container auf:
Besuchen Sie erneut http: / / < ^ > your _ server _ ip < ^ > oder localhost, um Ihre ausgeführte Anwendung zu sehen.
So installieren Sie Docker Compose auf Ubuntu 18.04.
So stellen Sie Remote Docker Hosts mit Docker Machine unter Ubuntu 18.04 bereit und verwalten sie.
So teilen Sie Daten zwischen Docker Containern.
So teilen Sie Daten zwischen Docker Containern und dem Host.
Sie können sich auch die längere Serie zu From Containers to Kubernetes with Node.js ansehen, welche die Grundlage für dieses Tutorial bildete.
Sie können sich auch auf unsere komplette Bibliothek mit Docker-Ressourcen für weitere Informationen zu Docker beziehen.
So stellen Sie Ihr DNS mit DNSControl unter Debian 10 bereit und verwalten es
3343
Der Autor wählte die Electronic Frontier Foundation Inc, um eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
DNSControl ist ein Infrastruktur-als-Code-Tool, mit dem Sie Ihre DNS-Zonen mithilfe von Standard-Software-Entwicklungsprinzipien, einschließlich Versionskontrolle, Tests und automatisierter Bereitstellung, bereitstellen und verwalten können.
DNSControl wurde von Stack Exchange erstellt und ist in Go geschrieben.
Die Verwendung von DNSControl beseitigt viele der Fallstricke der manuellen DNS-Verwaltung, da die Zonendateien in einem programmierbaren Format gespeichert werden.
Dadurch können Sie Zonen gleichzeitig bei mehreren DNS-Anbietern bereitstellen, Syntaxfehler identifizieren und Ihre DNS-Konfiguration automatisch mithilfe von Push übertragen, wodurch das Risiko menschlicher Fehler verringert wird.
Eine weitere häufige Anwendung von DNSControl ist die schnelle Migration Ihres DNS zu einem anderen Anbieter, z. B. im Falle eines DDoS-Angriffs oder eines Systemausfalls.
In diesem Tutorial installieren und konfigurieren Sie DNSControl, erstellen eine grundlegende DNS-Konfiguration und beginnen mit der Bereitstellung von DNS-Einträgen bei einem Live-Provider.
Als Teil dieses Tutorials werden wir DigitalOcean als DNS-Beispielanbieter verwenden.
Wenn Sie einen anderen Anbieter verwenden möchten, ist die Einrichtung sehr ähnlich.
Wenn Sie fertig sind, können Sie Ihre DNS-Konfiguration in einer sicheren Offline-Umgebung verwalten und testen und sie dann automatisch in der Produktion einsetzen.
Einen Debian-10-Server, der entsprechend dem Leitfaden Ersteinrichtung des Servers mit Debian 10 eingerichtet ist, einschließlich eines sudo-Benutzers ohne Rootberechtigung und einer aktivierten Firewall, um weniger wichtige Ports zu blockieren. < ^ > your-server-ipv4-address < ^ > ​ ​ ​ bezieht sich auf die IP-Adresse des Servers, auf dem Sie Ihre Website oder Domäne hosten. < ^ > your-server-ipv6-address < ^ > ​ ​ ​ ​ ​ ​ bezieht sich auf die IPv6-Adresse des Servers, auf dem Sie Ihre Website oder Domäne hosten.
Einen vollständig registrierten Domänennamen mit DNS, der von einem unterstützten Anbieter gehostet wird
In diesem Tutorial wird durchgängig < ^ > your _ domain < ^ > und als Dienstanbieter DigitalOcean verwendet.
Einen DigitalOcean-API-Schlüssel (Personal Access Token) mit Lese- und Schreibberechtigung.
Um einen solchen zu erstellen, besuchen Sie So erstellen Sie ein Personal Access Token.
Schritt 1 - Installation von DNSControl
DNSControl ist in Go geschrieben. Daher beginnen Sie diesen Schritt mit der Installation von Go auf Ihrem Server und der Einstellung Ihres GOPATH.
Go ist innerhalb der Standard-Software-Repositorys von Debian verfügbar und kann mit herkömmlichen Paketverwaltungsprogrammen installiert werden.
Sie müssen auch Git installieren, da dies erforderlich ist, damit Go die DNSControl-Software aus seinem Repository auf GitHub herunterladen und installieren kann.
Beginnen Sie damit, den lokalen Paketindex zu aktualisieren, um alle neuen Änderungen im Upstream zu berücksichtigen:
Installieren Sie dann die Pakete golang-go und git:
Nach der Bestätigung der Installation lädt apt Go und Git herunter und installiert diese sowie alle erforderlichen Abhängigkeiten.
Als Nächstes konfigurieren Sie die erforderlichen Pfad-Umgebungsvariablen für Go.
Wenn Sie mehr darüber erfahren möchten, können Sie das Tutorial GOPATH verstehen lesen.
Beginnen Sie mit der Bearbeitung der Datei ~ / .profile:
Fügen Sie die folgenden Zeilen ganz am Ende Ihrer Datei hinzu:
Wenn Sie diese Zeilen am Ende der Datei hinzugefügt haben, speichern und schließen Sie die Datei. Dann laden Sie Ihr Profil erneut, indem Sie sich entweder aus- und wieder einloggen oder die Datei erneut aufrufen:
Nachdem Sie nun Go installiert und konfiguriert haben, können Sie DNSControl installieren.
Mit dem Befehl go get kann eine Kopie des Codes abgerufen, automatisch kompiliert und in Ihr Go-Verzeichnis installiert werden:
Sobald dies abgeschlossen ist, können Sie die installierte Version überprüfen, um sicherzustellen, dass alles funktioniert:
Sie sehen eine Ausgabe, die der folgenden ähnelt:
Wenn Sie den Fehler dnscontrol: command not found sehen, überprüfen Sie die Einrichtung des Go-Pfades.
Nachdem Sie nun DNSControl installiert haben, können Sie ein Konfigurationsverzeichnis erstellen und DNSControl mit Ihrem DNS-Provider verbinden, damit dieser Änderungen an Ihren DNS-Einträgen vornehmen kann.
Schritt 2 - Konfigurieren von DNSControl
In diesem Schritt erstellen Sie die erforderlichen Konfigurationsverzeichnisse für DNSControl und verbinden es mit Ihrem DNS-Provider, damit dieser beginnen kann, Live-Änderungen an Ihren DNS-Einträgen vorzunehmen.
Erstellen Sie zunächst ein neues Verzeichnis, in dem Sie Ihre DNSControl-Konfiguration speichern können, und wechseln Sie in dieses:
< $> note Anmerkung: Dieses Tutorial konzentriert sich auf die Ersteinrichtung von DNSControl; für den produktiven Einsatz wird jedoch empfohlen, Ihre DNSControl-Konfiguration in einem Versionskontrollsystem (VCS) wie Git zu speichern.
Zu den Vorteilen dieses Systems gehören die vollständige Versionskontrolle, die Integration mit CI / CD für Tests, nahtlose Rollback-Bereitstellungen und so weiter.
Wenn Sie planen, DNSControl zum Schreiben von BIND-Zonendateien zu verwenden, sollten Sie auch das Verzeichnis zones erstellen:
BIND-Zonendateien sind eine rohe, standardisierte Methode zur Speicherung von DNS-Zonen / Einträgen im Klartextformat.
Sie wurden ursprünglich für die BIND-DNS-Server-Software verwendet, sind aber heute als Standardmethode zur Speicherung von DNS-Zonen weit verbreitet.
BIND-Zonendateien, die von DNSControl erstellt werden, sind nützlich, wenn Sie sie auf einen benutzerdefinierten oder selbst gehosteten DNS-Server importieren oder für Audit-Zwecke verwenden möchten.
Wenn Sie DNSControl jedoch nur dazu verwenden möchten, DNS-Änderungen mithilfe von Push an einen verwalteten Provider zu übertragen, wird das Verzeichnis zones nicht benötigt.
Als Nächstes müssen Sie die Datei creds.json konfigurieren, die es DNSControl ermöglicht, sich bei Ihrem DNS-Provider zu authentifizieren und Änderungen vorzunehmen.
Das Format von creds.json unterscheidet sich leicht, je nach dem von Ihnen verwendeten DNS-Provider.
Bitte lesen Sie die Liste der Dienstanbieter in der offiziellen DNSControl-Dokumentation, um die Konfiguration für Ihren eigenen Anbieter zu finden.
Erstellen Sie die Datei creds.json im Verzeichnis ~ / dnscontrol:
Fügen Sie der Datei die Beispielkonfiguration creds.json für Ihren DNS-Provider hinzu.
Wenn Sie DigitalOcean als Ihren DNS-Provider verwenden, können Sie Folgendes verwenden:
Diese Datei teilt DNSControl mit, mit welchen DNS-Providern es sich verbinden soll.
Sie müssen eine Form der Authentifizierung für Ihren DNS-Provider bereitstellen.
Dies ist normalerweise ein API-Schlüssel oder ein OAuth-Token, aber einige Anbieter benötigen zusätzliche Informationen, die in der Liste der Dienstanbieter in der offiziellen DNSControl-Dokumentation dokumentiert sind.
< $> ​ ​ ​ warning ​ ​ ​ ​ ​ ​ Warnung: Dieses Token gewährt Zugang zu Ihrem DNS-Provider-Konto, daher sollten Sie es wie ein Passwort schützen.
Stellen Sie außerdem sicher, dass bei Verwendung eines Versionskontrollsystems entweder die Datei, die das Token enthält, ausgeschlossen wird (z. B. mit .gitignore) oder auf eine andere Art und Weise sicher verschlüsselt wird.
Wenn Sie DigitalOcean als DNS-Provider verwenden, können Sie das erforderliche OAuth-Token in Ihren DigitalOcean-Kontoeinstellungen verwenden, das Sie als Teil der Voraussetzungen erstellt haben.
Wenn Sie mehrere verschiedene DNS-Provider haben, z. B. für mehrere Domänennamen oder delegierte DNS-Zonen, können Sie diese alle in derselben creds.json-Datei definieren.
Sie haben die anfänglichen DNSControl-Konfigurationsverzeichnisse eingerichtet und creds.json so konfiguriert, dass DNSControl sich bei Ihrem DNS-Provider authentifizieren und Änderungen vornehmen kann.
Als Nächstes erstellen Sie die Konfiguration für Ihre DNS-Zonen.
Schritt 3 - Erstellen einer DNS-Konfigurationsdatei
In diesem Schritt erstellen Sie eine anfängliche DNS-Konfigurationsdatei, die die DNS-Einträge für Ihren Domänennamen oder Ihre delegierte DNS-Zone enthält.
dnsconfig.js ist die Haupt-DNS-Konfigurationsdatei für DNSControl.
In dieser Datei werden die DNS-Zonen und die entsprechenden Einträge mithilfe der JavaScript-Syntax definiert.
Dies wird als DSL oder domänenspezifische Sprache bezeichnet.
Die Seite JavaScript-DSL in der offiziellen DNSControl-Dokumentation enthält weitere Einzelheiten.
Erstellen Sie zunächst die DNS-Konfigurationsdatei im Verzeichnis ~ / dnscontrol:
Fügen Sie dann die folgende Beispielkonfiguration in die Datei ein:
Diese Beispieldatei definiert einen Domänennamen oder eine DNS-Zone bei einem bestimmten Provider, in diesem Fall < ^ > your _ domain < ^ >, die von DigitalOcean gehostet wird.
Ein Beispieleintrag A wird auch in der Zone Root (@) definiert, der auf die IPv4-Adresse des Servers verweist, auf dem Sie die Domäne / Website hosten.
Es gibt drei Hauptfunktionen, die eine grundlegende DNSControl-Konfigurationsdatei ausmachen:
NewRegistrar (name, type, metadata): definiert die Domänen-Registrierungsstelle für Ihren Domänennamen.
Damit kann DNSControl die erforderlichen Änderungen vornehmen, z. B. die Änderung der autorisierenden Nameserver.
Wenn Sie DNSControl nur zur Verwaltung Ihrer DNS-Zonen verwenden möchten, kann dies im Allgemeinen als NONE belassen werden.
NewDnsProvider (name, type, metadata): definiert einen DNS-Dienstanbieter für Ihren Domänennamen oder Ihre delegierte Zone.
DNSControl wird die von Ihnen vorgenommenen DNS-Änderungen mithilfe von Push an diese Stelle übertragen.
D (name, registrar, modifiers): definiert einen Domänennamen oder eine delegierte DNS-Zone, die DNSControl verwalten soll, sowie die in der Zone vorhandenen DNS-Einträge.
Sie sollten NewRegistrar (), NewDnsProvider () und D () entsprechend konfigurieren, indem Sie die Liste der Dienstanbieter in der offiziellen DNSControl-Dokumentation verwenden.
Wenn Sie DigitalOcean als DNS-Provider verwenden und nur in der Lage sein müssen, DNS-Änderungen vorzunehmen (und nicht auch noch Änderungen an autoritativen Nameservern), ist das Beispiel im vorhergehenden Codeblock bereits korrekt.
Nach Abschluss speichern und schließen Sie die Datei.
In diesem Schritt richten Sie eine DNS-Konfigurationsdatei für DNSControl ein, in der die entsprechenden Provider definiert sind.
Als Nächstes werden Sie die Datei mit einigen nützlichen DNS-Einträgen füllen.
Schritt 4 - Dateneingabe in Ihre DNS-Konfigurationsdatei
Als Nächstes können Sie die DNS-Konfigurationsdatei mit nützlichen DNS-Einträgen für Ihre Website oder Ihren Dienst unter Verwendung der DNSControl-Syntax füllen.
Im Gegensatz zu herkömmlichen BIND-Zonendateien, bei denen DNS-Datensätze in einem rohen, zeilenweisen Format geschrieben werden, werden DNS-Einträge innerhalb von DNSControl als Funktionsparameter (Domänenmodifikator) der D () -Funktion definiert, wie in Schritt 3 kurz gezeigt wurde.
Ein Domänenmodifikator existiert für jeden der Standard-DNS-Eintragstypen, einschließlich A, AAAA, MX, TXT, NS, CAA und so weiter.
Eine vollständige Liste der verfügbaren Eintragstypen ist im Abschnitt Domänenmodifikatoren in der DNSControl-Dokumentation verfügbar.
Es sind auch Modifikatoren für einzelne Einträge (Datensatzmodifikatoren) verfügbar.
Diese werden derzeit in erster Linie für die Einstellung der TTL (time to live) der einzelnen Einträge verwendet.
Eine vollständige Liste der verfügbaren Eintragsmodifikatoren ist im Abschnitt Eintragsmodifikatoren in der DNSControl-Dokumentation verfügbar.
Eintragsmodifikatoren sind optional und können in den meisten grundlegenden Anwendungsfällen ausgelassen werden.
Die Syntax zur Einstellung von DNS-Einträgen variiert für jeden Eintragstyp leicht.
Nachfolgend finden Sie einige Beispiele für die gängigsten Eintragstypen:
A-Einträge:
Zweck: Verweisen auf eine IPv4-Adresse.
Syntax: A (' < ^ > name < ^ > ',' < ^ > address < ^ > ', optionale Eintragsmodifikatoren)
Beispiel: A (' < ^ > @ < ^ > ',' < ^ > your-server-ipv4-address < ^ > ', TTL (< ^ > 30 < ^ >))
AAAA-Einträge:
Zweck: Verweisen auf eine IPv6-Adresse.
Syntax: AAAA (' < ^ > name < ^ > ',' < ^ > address < ^ > ', optionale Eintragsmodifikatoren)
Beispiel: AAAA (' < ^ > @ < ^ > ',' < ^ > your-server-ipv6-address < ^ > ') (Eintragsmodifikator ausgelassen, daher wird die Standard-TTL verwendet)
CNAME-Einträge:
Zweck: Um Ihre Domäne / Subdomäne zu einem Alias einer anderen zu machen.
Syntax: CNAME (' < ^ > name < ^ > ',' < ^ > target < ^ > ', optionale Eintragsmodifikatoren)
Beispiel: CNAME (' < ^ > subdomain1 < ^ > ',' < ^ > example.org. < ^ > ') (Anmerkung: Es muss ein nachgestellter. eingefügt werden, wenn der Wert Punkte enthält)
MX-Einträge:
Zweck: Um E-Mail an bestimmte Server / Adressen zu leiten.
Syntax: MX (' < ^ > name < ^ > ',' < ^ > priority < ^ > ',' < ^ > target < ^ > ', optionale Eintragsmodifikatoren)
Beispiel: MX (' < ^ > @ < ^ > ', < ^ > 10 < ^ >,' < ^ > mail.example.net < ^ > ') (Anmerkung: Es muss ein nachgestellter. eingefügt werden, wenn der Wert Punkte enthält)
TXT-Einträge:
Zweck: Hinzufügen von beliebigem Klartext, der oft für Konfigurationen ohne eigenen dedizierten Eintragstyp verwendet wird.
Syntax: TXT (' < ^ > name < ^ > ',' < ^ > content < ^ > ', optionale Eintragsmodifikatoren)
Beispiel: TXT (' < ^ > @ < ^ > ',' < ^ > This is a TXT record.
< ^ > ')
CAA-Einträge:
Zweck: Beschränkung und Berichterstattung über Zertifizierungsstellen (CAs), die TLS-Zertifikate für Ihre Domäne / Subdomänen ausstellen können.
Syntax: CAA (' < ^ > name < ^ > ',' < ^ > tag < ^ > ',' < ^ > value < ^ > ', optionale Eintragsmodifikatoren)
Beispiel: CAA (' < ^ > @ < ^ > ',' < ^ > issue < ^ > ',' < ^ > letsencrypt.org < ^ > ')
Um mit dem Hinzufügen von DNS-Einträgen für Ihre Domäne oder delegierte DNS-Zone zu beginnen, bearbeiten Sie Ihre DNS-Konfigurationsdatei:
Als Nächstes können Sie damit beginnen, die Parameter für die vorhandene D () -Funktion mit der in der vorherigen Liste sowie dem Abschnitt Domänenmodifikatoren der offiziellen DNSConrtol-Dokumentation beschriebenen Syntax einzutragen.
Zwischen den einzelnen Einträgen muss ein Komma (,) verwendet werden.
Als Referenz enthält der Codeblock hier eine vollständige Beispielkonfiguration für eine grundlegende DNS-Ersteinrichtung:
Wenn Sie Ihre anfängliche DNS-Konfiguration abgeschlossen haben, speichern und schließen Sie die Datei.
In diesem Schritt richten Sie die anfängliche DNS-Konfigurationsdatei ein, die Ihre DNS-Einträge enthält.
Als Nächstes werden Sie die Konfiguration testen und bereitstellen.
Schritt 5 - Testen und Bereitstellen Ihrer DNS-Konfiguration
In diesem Schritt führen Sie eine lokale Syntaxprüfung Ihrer DNS-Konfiguration durch und stellen dann die Änderungen auf dem Live-DNS-Server / Provider bereit.
Gehen Sie zuerst in Ihr Verzeichnis dnscontrol:
Als Nächstes verwenden Sie die Funktion preview von DNSControl, um die Syntax Ihrer Datei zu überprüfen und die Änderungen auszugeben (ohne sie tatsächlich vorzunehmen):
Wenn die Syntax Ihrer DNS-Konfigurationsdatei korrekt ist, gibt DNSControl eine Übersicht über die Änderungen aus, die es vornehmen wird.
Diese sollte ähnlich wie die folgende aussehen:
Wenn Sie eine Fehlerwarnung in Ihrer Ausgabe sehen, stellt DNSControl Details dazu bereit, was der Fehler ist und wo er sich innerhalb Ihrer Datei befindet.
< $> warning Warnung: Der nächste Befehl nimmt Live-Änderungen an Ihren DNS-Einträgen und ggf. anderen Einstellungen vor.
Bitte stellen Sie sicher, dass Sie darauf vorbereitet sind, einschließlich der Erstellung einer Sicherungskopie Ihrer bestehenden DNS-Konfiguration, und stellen Sie sicher, dass Sie die Möglichkeit haben, bei Bedarf ein Rollback durchzuführen.
Schließlich können Sie die Änderungen mithilfe von Push an Ihren Live-DNS-Provider übertragen:
Sie sehen eine Ausgabe, die der folgenden ähnelt:
Wenn Sie nun die DNS-Einstellungen für Ihre Domäne im DigitalOcean-Bedienfeld überprüfen, sehen Sie die Änderungen.
Ein Screenshot des DigitalOcean-Bedienfelds, das einige der DNS-Änderungen zeigt, die DNSControl vorgenommen hat.
Sie können die Erstellung des Eintrags auch überprüfen, indem Sie mit dig eine DNS-Abfrage für Ihre Domäne / delegierte Zone ausführen.
Wenn Sie dig nicht installiert haben, müssen Sie das Paket dnsutils installieren:
Wenn Sie dig installiert haben, können Sie damit eine DNS-Abfrage für Ihre Domäne durchführen.
Sie werden sehen, dass die Einträge entsprechend aktualisiert wurden:
Sie sehen eine Ausgabe, die die IP-Adresse und den relevanten DNS-Eintrag aus Ihrer Zone zeigt, der mit DNSControl bereitgestellt wurde.
Das Propagieren von DNS-Einträgen kann einige Zeit dauern, sodass Sie möglicherweise warten und diesen Befehl erneut ausführen müssen.
In diesem letzten Schritt haben Sie eine lokale Syntaxprüfung der DNS-Konfigurationsdatei durchgeführt, sie dann bei Ihrem Live-DNS-Provider bereitgestellt und getestet, ob die Änderungen erfolgreich durchgeführt wurden.
In diesem Artikel richten Sie DNSControl ein und stellen eine DNS-Konfiguration bei einem Live-Provider bereit.
Jetzt können Sie Ihre DNS-Konfigurationsänderungen in einer sicheren Offline-Umgebung verwalten und testen, bevor Sie sie in der Produktion einsetzen.
Wenn Sie dieses Thema weiter vertiefen möchten, sei darauf verwiesen, dass DNSControl so konzipiert ist, dass es in Ihre CI / CD-Pipeline integriert werden kann, sodass Sie eingehende Tests durchführen können und mehr Kontrolle über Ihre Bereitstellung in der Produktion haben.
Sie könnten sich auch mit der Integration von DNSControl in Ihre Infrastruktur-Building- / Bereitstellungsprozesse befassen, sodass Sie Server bereitstellen und diese vollständig automatisch zu DNS hinzufügen können.
Wenn Sie mit DNSControl noch weiter gehen möchten, bieten die folgenden DigitalOcean-Artikel einige interessante nächste Schritte, die Ihnen bei der Integration von DNSControl in Ihre Änderungsmanagement- und Infrastruktur-Bereitstellungs-Workflows helfen können:
Eine Einführung in die kontinuierliche Integration, Übermittlung und Bereitstellung
CI / CD-Werkzeuge im Vergleich: Jenkins, GitLab CI, Buildbot, Drone und Concourse
Erste Schritte mit Konfigurationsmanagement
So importieren Sie vorhandene DigitalOcean-Assets in Terraform
3759
Terraform ist ein von HashiCorp entwickeltes Infrastruktur-als-Code-Tool, das Entwicklern bei der Bereitstellung, Aktualisierung und Entfernung verschiedener Assets ihrer Infrastruktur auf effiziente und skalierbare Weise hilft.
Entwickler können Terraform verwenden, um verschiedene Umgebungen zu organisieren, Änderungen durch Versionskontrolle zu verfolgen und sich wiederholende Arbeiten zu automatisieren, um menschliche Fehler einzuschränken.
Es bietet auch eine Möglichkeit für Teams, gemeinsam an der Verbesserung ihrer Infrastruktur durch freigegebene Konfigurationen zu arbeiten.
In diesem Tutorial werden Sie die bestehende DigitalOcean-Infrastruktur in Terraform importieren.
Am Ende dieses Tutorials werden Sie in der Lage sein, Terraform für Ihre gesamte bestehende Infrastruktur zu verwenden und zusätzlich neue Assets zu erstellen.
Ein persönliches DigitalOcean-Zugangs-Token.
Um dieses zu erstellen, können Sie den Leitfaden So erstellen Sie ein persönliches Zugangs-Token befolgen.
Ein DigitalOcean-Droplet mit einem Tag.
Sie können den folgenden Leitfaden So erstellen Sie ein Droplet über das DigitalOcean-Bedienfeld verwenden.
Eine DigitalOcean Cloud Firewall, die auf Ihr Droplet angewendet wird.
Sie können den Leitfaden So erstellen Sie Firewalls verwenden.
Den auf Ihrem lokalen Rechner installierten DigitalOcean Command Line Client, indem Sie die Installationsanweisungen auf der doctl GitHub-Seite befolgen.
Sie können das folgende Tutorial als Anleitung zur Verwendung von So verwenden Sie Doctl, den offiziellen DigitalOcean Comand-Line Client lesen.
Schritt 1 - Lokale Installation von Terraform
In diesem ersten Schritt installieren Sie Terraform auf Ihrem lokalen Rechner.
Dieser Schritt beschreibt die Installation der Linux-Binary.
Wenn Sie Windows oder Mac verwenden, können Sie die Seite Terraform herunterladen auf der Terraform-Website aufrufen.
Gehen Sie zu dem Ordner, in den Sie Terraform auf Ihrem lokalen Rechner herunterladen möchten, und verwenden Sie dann das Tool wget, um die Terraform < ^ > 0.12.12 < ^ > Binary herunterzuladen:
Zur Überprüfung, ob die sha256-Prüfsumme mit dem auf der Terraform-Website angegebenen Wert übereinstimmt, laden Sie die Prüfsummendatei mit dem folgenden Befehl herunter:
Führen Sie dann den folgenden Befehl zur Verifizierung der Prüfsummen aus:
Die heruntergeladene SHA256SUMS-Datei listet die Dateinamen und ihre Hashes auf.
Dieser Befehl sucht lokal nach der gleichen Datei terraform _ < ^ > 0.12.12 < ^ > _ SHA256SUMS und prüft dann die Übereinstimmung der Hashes it dem Flag -c.
Da in dieser Datei mehr als ein Dateiname und seine Plattform aufgeführt sind, verwenden Sie das Flag --ignore-missing, um Fehler in Ihrer Ausgabe zu vermeiden, da Sie keine Kopie der anderen Dateien haben.
Sie werden eine Ausgabe wie die folgende sehen:
Verwenden Sie unzip, um die Binary zu extrahieren:
Überprüfen Sie nun, ob Terraform korrekt installiert ist, indem Sie die Version überprüfen:
Nachdem Sie Terraform auf Ihrem lokalen Rechner installiert haben, werden Sie nun die Konfigurationsdateien vorbereiten.
Schritt 2 - Vorbereiten der Terraform-Konfigurationsdateien
In diesem Schritt importieren Sie Ihre vorhandenen Assets in Terraform, indem Sie ein Projektverzeichnis erstellen und Konfigurationsdateien schreiben.
Da Terraform derzeit die Erstellung von Konfigurationen aus dem Befehl import nicht unterstützt, müssen Sie diese Konfigurationen manuell erstellen.
Führen Sie den folgenden Befehl aus, um Ihr Projektverzeichnis zu erstellen:
Wechseln Sie dann in dieses Verzeichnis mit:
In diesem Schritt erstellen Sie drei zusätzliche Dateien, die die erforderlichen Konfigurationen enthalten.
Ihre Verzeichnisstruktur für dieses Projekt wird wie folgt aussehen:
Zu Beginn erstellen Sie die Datei provider.tf, um Ihr DigitalOcean Zugangs-Token als Umgebungsvariable zu definieren, anstatt es fest in Ihre Konfiguration zu kodieren.
< $> warning Warnung: Ihr Zugangs-Token ermöglicht den Zugriff auf Ihre komplette Infrastruktur mit uneingeschränktem Zugriff, behandeln Sie es also als solches.
Stellen Sie sicher, dass Sie die einzige Person mit Zugriff auf den Rechner sind, auf dem dieses Token gespeichert ist.
Neben Ihrem Zugangs-Token geben Sie auch an, welchen Provider Sie verwenden möchten.
In diesem Tutorial ist das digitalocean.
Eine vollständige Liste der verfügbaren Datenquellen und Ressourcen für DigitalOcean mit Terraform finden Sie auf der Seite der Provider auf deren Website.
Erstellen und bearbeiten Sie provider.tf mit dem folgenden Befehl:
Fügen Sie den folgenden Inhalt in die Datei provider.tf ein:
In dieser Datei fügen Sie Ihr DigitalOcean Zugangs-Token als Variable hinzu, die Terraform als Identifikation für die DigitalOcean-API verwendet.
Darüber hinaus geben Sie die Version des DigitalOcean-Provider-Plugins an.
Terraform empfiehlt, dass Sie die Version des von Ihnen verwendeten Providers angeben, damit zukünftige Aktualisierungen Ihre aktuelle Einrichtung nicht potenziell beeinträchtigen.
Jetzt erstellen Sie die Datei digitalocean _ droplet.tf.
Hier geben Sie die Ressource an, die Sie verwenden werden, in diesem Fall: droplet.
Erstellen Sie die Datei mit dem folgenden Befehl:
Fügen Sie die folgende Konfiguration hinzu:
Hier geben Sie vier Parameter an:
name: Der Droplet-Name.
region: Der Bereich, in dem sich das Droplet befindet.
tags: Eine Liste der Tags, die auf dieses Droplet angewendet werden.
count: die Anzahl der für diese Konfiguration erforderlichen Ressourcen.
Als Nächstes erstellen Sie eine Konfigurationsdatei für Ihre Firewall.
Erstellen Sie die Datei digitalocean _ firewall.tf mit dem folgenden Befehl:
Fügen Sie der Datei folgenden Inhalt hinzu:
Hier geben Sie den Namen der zu importierenden Firewall und die Tags der Droplets an, für die die Firewall-Regeln gelten sollen.
Schließlich definiert der Wert count von 1 die erforderliche Anzahl der jeweiligen Ressource.
< $> note Anmerkung: Sie können Firewall-Ressourcen auch in die Datei digitalocean _ droplet.tf aufnehmen. Aber wenn Sie mehrere Umgebungen haben, in denen sich mehrere Droplets dieselbe Firewall teilen, wird empfohlen, sie zu trennen, falls Sie nur ein einziges Droplet entfernen möchten.
Die Firewall wird dann nicht beeinträchtigt.
Jetzt ist es an der Zeit, diese Änderungen zu initialisieren, damit Terraform die erforderlichen Abhängigkeiten herunterladen kann.
Dazu verwenden Sie den Befehl terraform init, der es Ihnen ermöglicht, ein Arbeitsverzeichnis zu initialisieren, das Terraform-Konfigurationsdateien enthält.
Führen Sie diesen Befehl von Ihrem Projektverzeichnis aus:
Terraform hat das Arbeitsverzeichnis durch das Herunterladen von Plugins, die Suche nach Modulen usw. erfolgreich vorbereitet.
Als Nächstes beginnen Sie damit, Ihre Assets in Terraform zu importieren.
Schritt 3 - Importieren Ihrer Assets in Terraform
In diesem Schritt importieren Sie Ihre DigitalOcean-Assets in Terraform.
Sie verwenden doctl, um die ID-Nummern Ihrer Droplets zu finden, bevor Sie Ihre Assets importieren.
Anschließend überprüfen Sie die Importkonfiguration mit den Befehlen terraform show und terraform plan.
Zu Beginn exportieren Sie Ihr DigitalOcean Zugangs-Token als Umgebungsvariable, die Sie dann während der Laufzeit in Terraform injizieren.
Exportieren Sie es als Umgebungsvariable in Ihre aktuelle Shell-Sitzung mit dem folgenden Befehl:
Um Ihre vorhandenen Droplets und Firewalls zu importieren, benötigen Sie deren ID-Nummern.
Sie können doctl, die Befehlszeilenschnittstelle für die DigitalOcean-API, verwenden.
Führen Sie den folgenden Befehl aus, um Ihre Droplets aufzulisten und auf ihre IDs zuzugreifen:
Jetzt importieren Sie Ihr vorhandenes Droplet und Ihre Firewall in Terraform:
Sie verwenden das Flag -var, um den Wert Ihres DigitalOcean Zugangs-Tokens anzugeben, den Sie zuvor in Ihre Shell-Sitzung exportiert haben.
Dies ist erforderlich, damit die DigitalOcean-API überprüfen kann, wer Sie sind, und Änderungen an Ihrer Infrastruktur vornehmen kann.
Führen Sie nun denselben Befehl für Ihre Firewall aus:
Mit dem Befehl terraform show können Sie überprüfen, ob der Import erfolgreich war.
Dieser Befehl liefert eine von Menschen lesbare Ausgabe Ihres Infrastrukturstatus.
Er kann verwendet werden, um einen Plan zu überprüfen und sicherzustellen, dass die gewünschten Änderungen ausgeführt werden, oder um den aktuellen Zustand aus Sicht von Terraform zu inspizieren.
In diesem Zusammenhang bezieht sich state auf die Zuordnung Ihrer DigitalOcean-Assets zur Terraform-Konfiguration, die Sie geschrieben haben, und auf die Verfolgung von Metadaten.
Auf diese Weise können Sie bestätigen, dass es keinen Unterschied zwischen den vorhandenen DigitalOcean-Assets, die Sie importieren möchten, und den Assets, die Terraform nachverfolgt, gibt:
Sie werden in der Ausgabe zwei Ressourcen zusammen mit ihren Attributen sehen.
Nachdem Sie Ihr Droplet und Ihre Firewall in den Terraform-Status importiert haben, müssen Sie sicherstellen, dass die Konfigurationen den aktuellen Status der importierten Assets repräsentieren.
Dazu müssen Sie das image Ihres Droplets und dessen size angeben.
Diese beiden Werte finden Sie in der Ausgabe von terraform show für die Ressource digitalocean _ droplet.do _ droplet.
Öffnen Sie die Datei digitalocean _ droplet.tf:
In diesem Tutorial:
Ist das für unser vorhandenes Droplet verwendete Betriebssystem-Image ubuntu-16-04-x64.
Ist die Region, in der sich das Droplet befindet fra1.
Ist das Droplet-Tag für Ihr vorhandenes Droplet terraform-testing.
Das Droplet, das Sie mit der Konfiguration in digitalocean _ droplet.tf importiert haben, sieht wie folgt aus:
Als Nächstes fügen Sie die Firewall-Regeln hinzu.
In unserem Beispiel sind die offenen Ports für eingehenden Datenverkehr 22, 80 und 443. Alle Ports sind für ausgehenden Datenverkehr geöffnet.
Sie können diese Konfiguration entsprechend Ihrer offenen Ports anpassen.
Öffnen Sie digitalocean _ firewall.tf:
Diese Regeln replizieren den Status der vorhandenen Beispiel-Firewall.
Wenn Sie den Datenverkehr auf andere IP-Adressen, Ports oder ein anderes Protokoll beschränken möchten, können Sie die Datei so anpassen, dass sie Ihre vorhandene Firewall repliziert.
Nachdem Sie Ihre Terraform-Dateien aktualisiert haben, verwenden Sie den Befehl plan, um zu sehen, ob die von Ihnen vorgenommenen Änderungen den Status der vorhandenen Assets auf DigitalOcean replizieren.
Der Befehl terraform plan wird als Testlauf verwendet.
Mit diesem Befehl können Sie überprüfen, ob die Änderungen, die Terraform vornehmen wird, die Änderungen sind, die Sie vornehmen möchten.
Es wird empfohlen, diesen Befehl immer zur Bestätigung auszuführen, bevor Änderungen vorgenommen werden.
Führen Sie terraform plan wie folgt aus:
Sie sehen eine Ausgabe, die der folgenden Ausgabe ähnelt:
Sie haben vorhandene DigitalOcean-Assets erfolgreich in Terraform importiert und können jetzt über Terraform Änderungen an Ihrer Infrastruktur vornehmen, ohne das Risiko einzugehen, bestehende Assets versehentlich zu löschen oder zu modifizieren.
Schritt 4 - Erstellen von neuen Assets über Terraform
In diesem Schritt fügen Sie Ihrer bestehenden Infrastruktur zwei zusätzliche Droplets hinzu.
Das Hinzufügen von Assets zu Ihrer bestehenden Infrastruktur kann z. B. dann sinnvoll sein, wenn Sie eine Live-Website haben und während der Arbeit an dieser Website keine potenziell schädlichen Änderungen vornehmen wollen. Stattdessen können Sie ein weiteres Droplet als Entwicklungsumgebung hinzufügen und an Ihrem Projekt in derselben Umgebung wie das Produktions-Droplet arbeiten, ohne dass Sie dabei ein potenzielles Risiko eingehen.
Öffnen Sie jetzt digitalocean _ droplet.tf, um die Regeln für Ihre neuen Droplets hinzuzufügen:
Fügen Sie in Ihrer Datei die folgenden Zeilen hinzu:
Mit dem Meta-Argument count teilen Sie Terraform mit, wie viele Droplets mit den gleichen Spezifikationen Sie erstellen möchten.
Diese neuen Droplets werden ebenfalls zu Ihrer bestehenden Firewall hinzugefügt, da Sie das gleiche Tag wie bei Ihrer Firewall angeben.
Wenden Sie diese Regeln an, um die Änderungen zu überprüfen, die Sie in digitalocean _ droplet.tf angeben:
Vergewissern Sie sich, dass die Änderungen, die Sie vornehmen möchten, in der Ausgabe dieses Befehls repliziert werden.
Wenn Sie mit der Ausgabe zufrieden sind, verwenden Sie den Befehl terraform apply, um die von Ihnen angegebenen Änderungen auf den Status der Konfiguration anzuwenden:
Bestätigen Sie die Änderungen, indem Sie in der Befehlszeile yes eingeben.
Nach erfolgreicher Ausführung sehen Sie eine Ausgabe, die der folgenden ähnelt:
Sie sehen zwei neue Droplets in Ihrem DigitalOcean Web-Panel: Neue Droplets
Sie sehen sie auch an Ihre bestehende Firewall angebunden: Bestehende Firewall
Sie haben mit Terraform unter Verwendung Ihrer bereits vorhandenen Assets neue Assets erstellt.
Um zu erfahren, wie man diese Assets vernichtet, können Sie optional den nächsten Schritt durchführen.
Schritt 5 - Vernichtung importierter und erstellter Assets (optional)
In diesem Schritt vernichten Sie Assets, die Sie importiert und erstellt haben, indem Sie die Konfiguration anpassen.
Beginnen Sie, indem Sie digitalocean _ droplet.tf öffnen:
Setzen Sie in der Datei den count auf 0, wie im Folgenden gezeigt:
Öffnen Sie Ihre Firewall-Konfigurationsdatei, um count auch dort zu ändern:
Setzten Sie count auf 0, wie in der folgenden hervorgehobenen Zeile.
Wenden Sie nun diese Änderungen mit dem folgenden Befehl an:
Terraform wird Sie zu der Bestätigung auffordern, dass diese Droplets und die Firewall vernichtet werden sollen.
Dadurch werden alle über Terraform importierten und erstellten Assets vernichtet. Stellen Sie daher vor der Eingabe von yes sicher, dass Sie fortfahren möchten.
Sie haben alle von Terraform verwalteten Assets gelöscht.
Dies ist ein nützlicher Workflow, wenn Sie ein Asset nicht länger benötigen oder herunterskalieren möchten.
In diesem Tutorial haben Sie Terraform installiert, vorhandene Assets importiert, neue Assets erstellt und diese optional vernichtet.
Sie können diesen Workflow auf ein größeres Projekt skalieren, wie z. B. auf die Bereitstellung eines produktionsbereiten Kubernetes-Clusters.
Mit Terraform könnten Sie alle Knoten, DNS-Einträge, Firewalls, Speicher und andere Assets verwalten sowie die Versionskontrolle zur Verfolgung von Änderungen und zur Zusammenarbeit mit einem Team nutzen.
Um weitere Funktionen von Terraform zu erkunden, lesen Sie deren Dokumentation.
Sie können auch den Terraform-Inhalt von DigitalOcean für weitere Tutorials sowie Fragen und Antworten lesen.
So installieren und verwenden Sie TimescaleDB unter CentOS 7
3694
Der Autor wählte das Computer History Museum, um eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
Viele Anwendungen, wie z. B. Überwachungs- und Datenerfassungssysteme, sammeln Daten zur weiteren Analyse.
Mit diesen Analysen wird häufig die Art und Weise untersucht, wie sich ein Datenelement oder ein System im Verlauf der Zeit ändert.
In diesen Instanzen werden die Daten als Zeitreihen dargestellt, wobei jeder Datenpunkt mit einem Zeitstempel versehen ist.
Ein Beispiel würde wie folgt aussehen:
Die Relevanz von Zeitreihendaten hat in letzter Zeit dank der neuen Bereitstellungen des Internet der Dinge (IoT) und des industriellen Internets der Dinge zugenommen.
Es gibt immer mehr Geräte, die verschiedene Zeitreiheninformationen sammeln: Fitnesstracker, Smartwatches, Heim-Wetterstationen und verschiedene Sensoren, um nur einige zu nennen.
Diese Geräte sammeln eine Menge Informationen und alle diese Daten müssen irgendwo gespeichert werden.
Zur Speicherung von Daten werden meist klassische relationale Datenbanken verwendet, die jedoch nicht immer geeignet sind, wenn es um die riesigen Datenmengen von Zeitreihen geht.
Wenn Sie eine große Menge von Zeitreihendaten verarbeiten müssen, können relationale Datenbanken zu langsam sein.
Aus diesem Grund wurden speziell optimierte Datenbanken, so genannte NoSQL-Datenbanken erstellt, um die Probleme relationaler Datenbanken zu vermeiden.
TimescaleDB ist eine Open-Source-Datenbank, die für die Speicherung von Zeitreihendaten optimiert ist. Sie wird als eine Erweiterung von PostgreSQL implementiert und kombiniert die Benutzerfreundlichkeit rationaler Datenbanken mit der Geschwindigkeit von NoSQL-Datenbanken.
Dadurch können Sie PostgreSQL sowohl für die Speicherung von Geschäftsdaten als auch Zeitreihendaten an einem Ort verwenden.
Wenn Sie diesem Tutorial folgen, werden Sie TimescaleDB unter CentOS 7 einrichten, konfigurieren und lernen, wie damit gearbeitet wird. Dann erstellen Sie Zeitreihendatenbanken und führen einfache Abfragen durch.
Abschließend sehen Sie, wie unnötige Daten entfernt werden.
Einen CentOS-7-Server, der gemäß unseres Leitfadens Ersteinrichtung des Servers mit CentOS 7 eingerichtet wurde, einschließlich eines Benutzers ohne Root-Berechtigung und mit sudo-Berechtigungen und eine mit firewalld eingerichtete Firewall.
Für die Einrichtung von firewalld folgen Sie dem Abschnitt "Konfigurieren einer grundlegenden Firewall" des Tutorials Zusätzlich empfohlene Schritte für neue CentOS-7-Server.
Auf Ihrem Server installiertes PostgreSQL.
Folgen Sie unserem Leitfaden So installieren und verwenden Sie PostgreSQL unter CentOS 7, um es zu installieren und zu konfigurieren.
Schritt 1 - Installieren der TimescaleDB
TimescaleDB ist nicht in den Repositorys der CentOS-Standardpakete verfügbar. Daher installieren Sie es in diesem Schritt aus dem TimescaleDB-Repository eines Drittanbieters.
Erstellen Sie zunächst eine neue Repository-Datei:
Gehen Sie in den Einfügemodus, indem Sie i drücken, und fügen Sie folgende Konfiguration in die Datei ein:
Wenn Sie fertig sind, drücken Sie ESC, um den Einfügemodus zu verlassen, dann: wq und die EINGABETASTE, um die Datei zu speichern und zu verlassen.
Um mehr über den Texteditor vi und seinen Nachfolger vim zu erfahren, sehen Sie sich unser Tutorial Installieren und Verwenden des Vim Texteditors auf einem Cloud-Server an.
Sie können nun mit der Installation fortfahren.
Dieses Tutorial verwendet PostgreSQL Version 11; falls Sie eine andere Version von PostgreSQL (z. B. 9.6 oder 11) verwenden, ersetzen Sie den Wert im folgenden Befehl und führen Sie ihn aus:
TimescaleDB ist nun installiert und bereit für die Verwendung.
Als Nächstes aktivieren Sie es und passen einige der damit verbundenen Einstellungen in der PostgreSQL-Konfigurationsdatei an, um die Datenbank zu optimieren.
Schritt 2 - Konfigurieren der TimescaleDB
Das TimescaleDB-Modul funktioniert gut mit den Standard-Konfigurationseinstellungen, jedoch schlagen die Entwickler von TimescaleDB die Konfiguration einzelner Parameter vor, um die Leistung zu verbessern und die Ressourcen von Prozessor, Speicher und Festplatte besser zu nutzen.
Dies kann automatisch mit dem Tool timescaledb-tune oder durch die manuelle Bearbeitung der Datei postgresql.conf Ihres Servers erfolgen.
In diesem Tutorial werden Sie das Tool timescaledb-tune verwenden.
Es liest die Datei postgresql.conf und schlägt interaktiv Änderungen vor.
Führen Sie den folgenden Befehl aus, um den Konfigurationsassistenten zu starten:
Zuerst werden Sie aufgefordert, den Pfad zu der PostgreSQL-Konfigurationsdatei zu bestätigen:
Das Dienstprogramm erkennt den Pfad zu der Konfigurationsdatei automatisch, bestätigen Sie dies also durch Eingabe von y:
Als Nächstes aktivieren Sie das TimescaleDB-Modul, indem Sie an der nächsten Eingabeaufforderung y eingeben und die EINGABETASTE drücken:
Basierend auf den Eigenschaften Ihres Servers und der PostgreSQL-Version wird Ihnen dann die Optimierung Ihrer Einstellungen angeboten.
Drücken Sie y, um den Optimierungsvorgang zu starten:
timescaledb-tune erkennt automatisch den verfügbaren Speicher des Servers und berechnet empfohlene Werte für die Einstellungen shared _ buffers, effective _ cache _ size, maintenance _ work _ mem und work _ mem.
Wenn Sie mehr über die Ausführung erfahren möchten, sehen Sie sich die GitHub-Seite für timescaledb-tune an.
Wenn diese Einstellungen gut aussehen, geben Sie y ein:
Wenn Ihr Server über mehrere CPUs verfügt, finden Sie an dieser Stelle Empfehlungen für die Parallelitätseinstellungen.
Wenn Sie jedoch nur eine CPU haben, wird timescaledb-tune Sie direkt zu den WAL-Einstellungen leiten.
Bei mehreren CPUs werden in etwa folgende Empfehlungen angezeigt:
Diese Einstellungen regulieren die Anzahl der Workers, die Anfragen und Hintergrundaufgaben verarbeiten.
Mehr über diese Einstellungen können Sie der TimescaleDB- und der PostgreSQL-Dokumentation entnehmen.
Geben Sie y und dann die EINGABETASTE ein, um diese Einstellungen zu akzeptieren:
Als Nächstes finden Sie Empfehlungen für Write Ahead Log (WAL):
WAL bewahrt die Datenintegrität, aber die Standardeinstellungen können eine ineffiziente E / A verursachen, die die Schreibleistung verlangsamt.
Geben Sie y ein, um diese Einstellungen zu optimieren:
Sie finden nun verschiedene Empfehlungen:
Alle diese verschiedenen Parameter zielen auf eine Leistungssteigerung ab.
Beispielsweise können SSDs viele gleichzeitige Anfragen verarbeiten, sodass der beste Wert für die effective _ io _ concurrency im Hunderterbereich liegen könnte.
Weitere Informationen über diese Optionen finden Sie in der PostgreSQL-Dokumentation.
Drücken Sie y und dann die EINGABETASTE, um fortzufahren.
Als Ergebnis erhalten Sie eine gebrauchsfertige Konfigurationsdatei unter / var / lib / pgsql / < ^ > 11 < ^ > / data / postgresql.conf.
< $> note Anmerkung: Wenn Sie die Installation von Grund auf neu durchführen, könnten Sie den anfänglichen Befehl auch mit den Flags --quiet und --yes ausführen, wodurch automatisch alle Empfehlungen angewendet und Änderungen an der Konfigurationsdatei postgresql.conf vorgenommen werden:
Damit die Konfigurationsänderungen wirksam werden, müssen Sie den Dienst PostgreSQL neu starten:
Nun wird die Datenbank mit optimalen Parametern ausgeführt und ist bereit dazu, mit den Zeitreihendaten zu arbeiten. In den nächsten Schritten werden Sie das Arbeiten mit diesen Daten testen; Sie erstellen neue Datenbanken und Hypertabellen und führen Operationen aus.
Schritt 3 - Erstellen einer neuen Datenbank und Hypertabelle
Mit Ihrer optimierten TimescaleDB-Einrichtung sind Sie bereit, mit Zeitreihendaten zu arbeiten. TimescaleDB ist als eine Erweiterung von PostgreSQL implementiert, sodass sich Operationen mit Zeitreihendaten nicht wesentlich von relationalen Datenoperationen unterscheiden.
Gleichzeitig erlaubt die Datenbank, Daten aus Zeitreihen und relationalen Tabellen in Zukunft frei zu kombinieren.
Zuerst erstellen Sie eine neue Datenbank und aktivieren die TimescaleDB-Erweiterung dafür. Melden Sie sich in Ihrer PostgreSQL-Datenbank an:
Erstellen Sie nun eine neue Datenbank und stellen Sie eine Verbindung zu ihr her. In diesem Tutorial heißt die Datenbank < ^ > timeseries < ^ >:
Weitere Informationen über die Arbeit mit der PostgreSQL-Datenbank finden Sie in unserem Tutorial So erstellen, entfernen und verwalten Sie Tabellen in PostgreSQL auf einem Cloud-Server.
Aktivieren Sie zum Schluss die TimescaleDB-Erweiterung:
Der primäre Interaktionspunkt mit Ihren Zeitreihendaten sind Hypertabellen, eine Abstraktion vieler einzelner Tabellen, die die Daten enthalten, die als Chunks bezeichnet werden.
Um eine Hypertabelle zu erstellen, beginnen Sie mit einer regulären SQL-Tabelle und konvertieren diese dann mit der Funktion create _ hypertable in eine Hypertabelle.
Erstellen Sie eine Tabelle, die Daten zur Verfolgung von Temperatur und Feuchtigkeit für verschiedene Geräten im Zeitablauf speichert:
Dieser Befehl erstellt eine Tabelle namens conditions mit vier Spalten.
Die erste Spalte speichert den Zeitstempel, der die Zeitzone enthält und nicht leer sein darf.
Als Nächstes verwenden Sie die Zeitspalte, um Ihre Tabelle in eine nach Zeit partionierte Hypertabelle umzuwandeln:
Dieser Befehl ruft die Funktion create _ hypertable () auf, die eine TimescaleDB-Hypertabelle aus einer PostgreSQL-Tabelle erstellt und diese ersetzt.
Sie erhalten die folgende Ausgabe:
In diesem Schritt haben Sie eine neue Hypertabelle zur Speicherung von Zeitreihendaten angelegt. Nun können Sie sie mit Daten füllen, indem Sie in die Hypertabelle schreiben und dann den Löschvorgang ausführen.
Schritt 4 - Schreiben und Löschen von Daten
In diesem Schritt fügen Sie Daten unter Verwendung von SQL-Standardbefehlen ein und importieren große Datensätze aus externen Quellen.
Dies veranschaulicht Ihnen die relationalen Datenbankaspekte von TimescaleDB.
Probieren Sie zunächst die grundlegenden Befehle aus.
Daten können mit dem SQL-Standardbefehl INSERT in die Hypertabelle eingefügt werden.
Fügen Sie mit dem folgenden Befehl einige Beispieldaten für Temperatur und Feuchtigkeit für das theoretische Gerät weather-pro-000000 ein:
Sie können auch mehrere Datenreihen gleichzeitig einfügen.
Probieren Sie Folgendes aus:
Sie erhalten Folgendes:
Sie können auch festlegen, dass der Befehl INSERT einige oder alle eingefügten Daten mit der Anweisung RETURNING zurückgibt:
Wenn Sie Daten aus der Hypertabelle löschen möchten, verwenden Sie den SQL-Standardbefehl DELETE.
Führen Sie Folgendes aus, um alle Daten zu löschen, die eine Temperatur von mehr als 80 oder eine Feuchtigkeit von mehr als 50 haben:
Nach dem Löschvorgang wird empfohlen, den Befehl VACUUM zu verwenden, der den von den gelöschten Daten noch genutzten Speicherplatz zurückgewinnt.
Weitere Informationen über den Befehl VACUUM finden Sie in der PostgreSQL-Dokumentation.
Diese Befehle eignen sich gut für die Dateneingabe in kleinem Maßstab, aber da Zeitreihendaten oft riesige Datensätze von mehreren Geräten gleichzeitig generieren, ist es auch wichtig zu wissen, wie gleichzeitig Hunderte oder Tausende von Zeilen eingefügt werden.
Wenn Sie Daten aus externen Quellen in strukturierter Form, z. B. im Format csv, erstellt haben, kann diese Aufgabe schnell erledigt werden.
Um dies auszuprobieren, verwenden Sie einen Beispieldatensatz, der Temperatur- und Feuchtigkeitsdaten von verschiedenen Orten darstellt.
Er wurde von den Entwicklern der TimescaleDB erstellt, damit Sie ihre Datenbank testen können.
Weitere Informationen über Beispieldatensätze finden Sie in der TimescaleDB-Dokumentation.
Sehen wir uns an, wie Sie Daten aus dem Beispieldatensatz weather _ small in Ihre Datenbank importieren können.
Beenden Sie zunächst Postgresql:
Laden Sie dann den Datensatz herunter und extrahieren Sie ihn:
Als Nächstes importieren Sie die Temperatur- und Feuchtigkeitsdaten in Ihre Datenbank:
Damit wird eine Verbindung mit der Datenbank < ^ > timeseries < ^ > hergestellt und der Befehl\ COPY ausgeführt, der die Daten aus der gewählten Datei in die Hypertabelle conditions kopiert.
Er wird einige Sekunden lang ausgeführt.
Nachdem die Daten in Ihre Tabelle eingegeben wurden, erhalten Sie die folgende Ausgabe:
In diesem Schritt haben Sie Daten manuell und in Stapeln in die Hypertabelle eingefügt.
Als Nächstes fahren Sie mit der Durchführung von Abfragen fort.
Schritt 5 - Abfrage von Daten
Nachdem Ihre Tabelle nun Daten enthält, können Sie verschiedene Abfragen durchführen, um sie zu analysieren.
Um zu beginnen, melden Sie sich in der Datenbank an:
Wie bereits erwähnt, können Sie für die Arbeit mit Hypertabellen SQL-Standardbefehle verwenden.
Um beispielsweise die letzten 10 Einträge aus der Hypertabelle conditions anzuzeigen, führen Sie den folgenden Befehl aus:
Mit diesem Befehl können Sie sehen, welche Daten sich in der Datenbank befinden.
Da die Datenbank eine Million Datensätze enthält, haben Sie mit LIMIT 10 die Ausgabe auf 10 Einträge beschränkt.
Um die letzten Einträge zu sehen, sortieren Sie das Datenarray in absteigender Reihenfolge nach der Zeit:
Dadurch werden die 20 neuesten Einträge ausgegeben.
Sie können auch einen Filter hinzufügen.
Um beispielsweise Einträge von dem Gerät weather-pro-000000 anzuzeigen, führen Sie Folgendes aus:
In diesem Fall sehen Sie die 10 neuesten Temperatur- und Feuchtigkeitsdatenpunkte, die von dem Gerät weather-pro-000000 aufgezeichnet wurden.
Zusätzlich zu den SQL-Standardbefehlen bietet die TimescaleDB auch eine Reihe spezieller Funktionen, die für die Analyse von Zeitreihendaten nützlich sind.
Um beispielsweise den Mittelwert der Temperaturwerte zu finden, können Sie die folgende Abfrage mit der Funktion percentile _ cont verwenden:
Auf diese Weise sehen Sie den Medianwert der Temperatur für den gesamten Beobachtungszeitraum des Ortes, an dem sich der Sensor weather-pro-00000 befindet.
Um die neuesten Werte von jedem der Sensoren anzuzeigen, können Sie die Funktion last verwenden:
In der Ausgabe sehen Sie eine Liste aller Sensoren und der relevanten neuesten Werte.
Um die ersten Werte zu erhalten, verwenden Sie die Funktion first.
Das folgende Beispiel ist komplexer.
Es zeigt die stündlichen Durchschnitts-, Minimal- und Maximaltemperaturen für den gewählten Sensor innerhalb der letzten 24 Stunden:
Hier haben Sie die Funktion time _ bucket verwendet, die als eine leistungsfähigere Version der PostgreSQL-Funktion date _ trunc fungiert.
Als Ergebnis sehen Sie, in welchen Tagesabschnitten die Temperatur steigt oder fällt:
Weitere nützliche Funktionen finden Sie in der TimescaleDB-Dokumentation.
Nun wissen Sie, wie Sie mit Ihren Daten umgehen können. Als Nächstes löschen Sie unnötige Daten und komprimieren Daten.
Schritt 6 - Konfigurieren der Datenkomprimierung und -löschung
Wenn sich Daten ansammeln, nehmen sie immer mehr Platz auf Ihrer Festplatte ein.
Um Platz zu sparen, bietet die neueste Version der TimescaleDB eine Datenkomprimierungsfunktion.
Diese Funktion erfordert keine Anpassung der Dateisystemeinstellungen und kann dazu verwendet werden, Ihre Datenbank schnell effizienter zu gestalten.
Weitere Informationen über die Funktionsweise dieser Komprimierung finden Sie in diesem Artikel zur Komprimierung von TimescaleDB.
Aktivieren Sie zunächst die Komprimierung Ihrer Hypertabelle:
Sie erhalten die folgenden Ausgabedaten:
< $> note Anmerkung: Sie können TimescaleDB auch einrichten, damit Daten über den angegebenen Zeitraum komprimiert werden.
Beispielsweise können Sie Folgendes ausführen:
In diesem Beispiel werden die Daten nach einer Woche automatisch komprimiert.
Sie können die Statistiken der komprimierten Daten mit folgendem Befehl anzeigen:
Sie sehen dann eine Liste von Chunks mit ihrem Status: Komprimierungsstatus und wie viel Speicherplatz unkomprimierte und komprimierte Daten in Byte belegen.
Wenn es nicht notwendig ist, Daten über einen längeren Zeitraum zu speichern, können Sie veraltete Daten löschen, um noch mehr Speicherplatz freizugeben.
Dafür gibt es eine spezielle Funktion drop _ chunks.
Sie ermöglicht die Löschung von Chunks mit Daten, die älter als die angegebene Zeit sind:
Diese Abfrage löscht alle Chunks aus der Hypertabelle conditions, die nur Daten enthalten, die älter als ein Tag sind.
Um alte Daten automatisch zu löschen, können Sie eine Cron-Aufgabe konfigurieren.
In unserem Tutorial erfahren Sie mehr über die Verwendung von Cron zur Automatisierung verschiedener Systemaufgaben.
Verlassen Sie die Datenbank:
Bearbeiten Sie als Nächstes crontab mit dem folgenden Befehl, der von der Shell aus ausgeführt werden sollte:
Fügen Sie nun die folgende Zeile dem Ende der Datei hinzu:
Dieser Job löscht jeden Tag um 1: 00 Uhr veraltete Daten, die älter als ein Tag sind.
Sie haben nun TimescaleDB auf Ihrem CentOS-Server eingerichtet.
Sie haben auch das Erstellen von Hypertabellen, das Einfügen von Daten in diese, das Abfragen der Daten, das Komprimieren und Löschen unnötiger Datensätze ausprobiert.
Mit diesen Beispielen können Sie die wichtigsten Vorteile von TimescaleDB gegenüber traditionellen relationalen Datenbankverwaltungssystemen für die Speicherung von Zeitreihendaten nutzen, u. a. durch:
Höhere Datenaufnahmeraten
Schnellere Abfrageleistung
Zeitorientierte Funktionen
So richten Sie Apache Virtual Hosts unter Ubuntu 18.04 ein Schnellstart
3824
Dieses Tutorial führt Sie durch das Einrichten mehrerer Domänen und Websites mit virtuellen Apache-Hosts auf einem Ubuntu 18.04-Server.
Bei diesem Prozess lernen Sie, wie Sie je nach Art der angeforderten Domäne verschiedenen Benutzern unterschiedlichen Inhalt bereitstellen können.
Eine ausführlichere Version dieses Tutorials mit besseren Erklärungen zu den einzelnen Schritten finden Sie unter So installieren Sie Apache Virtual Hosts unter Ubuntu 18.04.
Einen Apache2-Webserver, den Sie mit sudo apt install apache2 installieren können
Schritt 1 - Verzeichnisstruktur erstellen
Zunächst erstellen wir eine Verzeichnisstruktur, in der die Site-Daten gespeichert werden, die wir den Besuchern in unserem Top-Level-Apache-Verzeichnis bereitstellen können.
Wir verwenden Beispiele von Domänennamen, die nachstehend hervorgehoben sind.
Sie müssen diese durch Ihre tatsächlichen Domänennamen ersetzen.
Schritt 2 - Berechtigungen erteilen
Jetzt müssen wir die Berechtigungen auf unseren aktuellen Benutzer ohne Rootberechtigung ändern, um diese Dateien ändern zu können.
Außerdem stellen wir sicher, dass der schreibgeschützte Zugriff auf das allgemeine Webverzeichnis und alle darin enthaltenen Dateien und Ordner gestattet ist, damit Seiten richtig bereitgestellt werden können.
Schritt 3 - Demo-Seiten für jeden virtuellen Host erstellen
Erstellen wir jetzt Inhalt, den wir bereitstellen können. Dazu erstellen wir eine index.html-Demo-Seite für jede Site.
Wir können eine index.html-Datei in einem Textbearbeitungsprogramm für unsere erste Site öffnen, beispielsweise nano.
In dieser Datei erstellen wir dann ein domänenspezifisches HTML-Dokument wie das folgende:
Speichern und schließen Sie die Datei und kopieren Sie dann diese Datei als Grundlage für unsere zweite Site:
Öffnen Sie die Datei und ändern Sie die relevanten Informationen:
Speichern und schließen Sie auch diese Datei.
Schritt 4 - Neue virtuelle Host-Dateien erstellen
Apache wird mit einer virtuellen Standard-Host-Datei namens 000-default.conf geliefert, die wir als Vorlage verwenden.
Wir kopieren sie, um eine virtuelle Host-Datei für jede unserer Domänen zu erstellen.
Erste virtuelle Host-Datei erstellen
Kopieren Sie als Erstes die Datei für die erste Domäne:
Öffnen Sie die neue Datei in Ihrem Textbearbeitungsprogramm (wir verwenden nano) mit Rootberechtigungen:
Wir werden diese Datei für unsere eigene Domäne anpassen.
Ändern Sie den nachstehend hervorgehobenen Text je nach Bedarf.
Speichern und schließen Sie dann die Datei.
Ersten virtuellen Host kopieren und für die zweite Domäne anpassen
Jetzt haben wir unsere erste virtuelle Host-Datei eingerichtet und können dann unsere zweite Datei erstellen, indem wir diese Datei kopieren und je nach Bedarf anpassen.
Kopieren Sie zu diesem Zweck die Datei:
Öffnen Sie die neue Datei mit Rootberechtigungen in Ihrem Textbearbeitungsprogramm:
Jetzt müssen Sie alle Informationen so ändern, dass sie auf Ihre zweite Domäne verweisen.
Die endgültige Datei sollte in etwa wie folgt aussehen, wobei der hervorgehobene Text Ihren eigenen Domäneninformationen entsprechen sollte.
Schritt 5 - Neue virtuelle Host-Dateien aktivieren
Nach Erstellung unserer virtuellen Host-Dateien müssen diese aktiviert werden.
Dazu verwenden wir das a2ensite-Tool.
Deaktivieren Sie als Nächstes die unter 000-default.conf definierte Standard-Site:
Wenn Sie fertig sind, müssen Sie Apache neu starten, damit diese Änderungen wirksam werden, und systemctl status verwenden, um den Erfolg des Neustarts zu verifizieren.
Ihr Server sollte jetzt für zwei Websites eingerichtet sein.
Schritt 6 - Lokale Hosts-Datei einrichten (optional)
Wenn Sie keine tatsächlichen Domänen verwenden, die ihnen gehören, um dieses Verfahren zu testen, und stattdessen Beispielsdomänen benutzen, können Sie Ihre Arbeit testen, indem Sie die hosts-Datei auf Ihrem lokalen Computer kurzzeitig ändern.
Geben Sie auf einem lokalen Mac- oder Linux-Rechner Folgendes ein:
Für einen lokalen Windows-Rechner finden Sie Anweisungen zur Änderung Ihrer Hosts-Datei hier.
Nach Verwenden der in diesem Leitfaden verwendeten Domänen und Ersetzen Ihrer Server IP im Text < ^ > your _ server _ IP < ^ > sollte Ihre Datei wie folgt aussehen:
Damit werden alle Anfragen z. B. example.com und test.com auf unseren Rechner weitergeleitet und auf unseren Server übertragen.
Schritt 7 - Ergebnisse testen
Nach der Konfiguration Ihrer virtuellen Hosts können Sie Ihr Setup testen, indem Sie zu den Domänen gehen, die Sie in Ihrem Web-Browser konfiguriert haben.
Sie sollten eine Seite sehen, die so aussieht:
Sie können auch Ihre zweite Seite besuchen und die Datei sehen, die Sie für Ihre zweite Site erstellt haben.
Wenn beide Sites wie erwartet funktionieren, haben Sie zwei virtuelle Hosts auf ein und demselben Server konfiguriert.
Wenn Sie die Host-Datei Ihres eigenen Rechners angepasst haben, löschen Sie die von Ihnen hinzugefügte Zeilen.
Hier sehen Sie Links zu detaillierteren Leitfäden, die im Zusammenhang zu diesem Tutorial stehen:
So richten Sie Apache Virtual Hosts unter Ubuntu 18.04 ein
Domänen und DNS auf DigitalOcean
So überschreiben Sie URLs mit mod _ rewrite für Apache unter Ubuntu 18.04
So richten Sie ReadWriteMany (RWX) Persistent Volumes mit NFS unter DigitalOcean Kubernetes ein
3800
Aufgrund der verteilten und dynamischen Natur von Containern ist die statische Verwaltung und Konfiguration von Speicher in Kubernetes zu einem schwierigen Problem geworden, da Arbeitslasten jetzt in der Lage sind, innerhalb von Sekunden von einer virtuellen Maschine (VM) auf eine andere zu wechseln.
Um dieses Problem zu lösen, verwaltet Kubernetes Volumes mit einem System aus Persistent Volumes (PV), API-Objekten, die eine Speicherkonfiguration / ein Volume darstellen, und PersistentVolumeClaims (PVC), einer Speicheranforderung, die von einem Persistent Volume erfüllt werden soll.
Darüber hinaus können Container Storage Interface (CSI) -Treiber dazu beitragen, die Handhabung und Bereitstellung von Speicher für containerisierte Workloads zu automatisieren und zu verwalten.
Diese Treiber sind für die Bereitstellung, das Einbinden, das Aufheben der Einbindung, das Entfernen und das Erstellen von Snapshots von Volumes verantwortlich.
Das digitalocean-csi integriert einen Kubernetes-Cluster mit dem Produkt DigitalOcean Block Storage.
Damit kann ein Entwickler Blockspeicher-Volumes für containerisierte Anwendungen in Kubernetes dynamisch bereitstellen.
Anwendungen können jedoch manchmal erfordern, dass Daten über mehrere Droplets hinweg persistent gespeichert und gemeinsam genutzt werden.
Die Blockspeicher-CSI-Standardlösung von DigitalOcean ist nicht in der Lage, das gleichzeitige Einbinden eines Blockspeicher-Volumes in mehrere Droplets zu unterstützen.
Dies bedeutet, dass es sich um eine ReadWriteOnce (RWO) -Lösung handelt, da das Volume auf einen Knoten beschränkt ist.
Das Protokoll Network File System (NFS) hingegen unterstützt das Exportieren derselben Freigabe an viele Consumer.
Dies wird ReadWriteMany (RWX) genannt, da viele Knoten das Volume als Read-Write einbinden können.
Wir können daher einen NFS-Server innerhalb unseres Clusters verwenden, um Speicher bereitzustellen, der die zuverlässige Unterstützung von DigitalOcean Block Storage mit der Flexibilität von NFS-Freigaben nutzen kann.
In diesem Tutorial werden Sie die dynamische Bereitstellung für NFS-Volumes innerhalb eines DigitalOcean Kubernetes (DOKS) -Clusters, in dem die Exporte auf DigitalOcean Block-Speichervolumes gespeichert werden, konfigurieren.
Anschließend werden Sie mehrere Instanzen einer Nginx-Demoanwendung bereitstellen und die gemeinsame Nutzung von Daten zwischen den einzelnen Instanzen testen.
Einen DigitalOcean Kubernetes-Cluster, bei dem Ihre Verbindung standardmäßig als kubectl konfiguriert ist.
Um einen Kubernetes-Cluster auf DigitalOcean zu erstellen, lesen Sie unseren Kubernetes-Schnellstart.
Eine Anleitung zur Konfiguration von kubectl finden Sie unter dem Schritt Verbinden mit Ihrem Cluster, wenn Sie Ihren Cluster erstellen.
Den auf Ihrem lokalen Rechner installierten Helm-Paketmanager und das auf Ihrem Cluster installierte Tiller.
Führen Sie dazu die Schritte 1 und 2 des Tutorials So installieren Sie Software auf Kubernetes Clustern mit dem Helm-Paketmanager aus.
< $> note Hinweis: Ab Helm Version 3.0 muss Tiller nicht mehr installiert werden, damit Helm funktioniert.
Wenn Sie die neueste Version von Helm verwenden, finden Sie die Anweisungen in der Helm-Installationsdokumentation.
Schritt 1 - Bereitstellen des NFS-Servers mit Helm
Für die Bereitstellung des NFS-Servers verwenden Sie ein Helm Chart.
Die Bereitstellung eines Helm Charts ist eine automatisierte Lösung, die schneller und weniger fehleranfällig ist als die manuelle Erstellung der NFS-Serverbereitstellung.
Stellen Sie zuerst sicher, dass das standardmäßige Chart-Repository stable verfügbar ist, indem Sie das Repo hinzufügen:
Als Nächstes rufen Sie die Metadaten für das Repository ab, das Sie gerade hinzugefügt haben.
Dadurch wird sichergestellt, dass der Helm-Client aktualisiert wird:
Um den Zugriff auf das Repo stable zu verifizieren, führen Sie eine Suche in den Charts aus:
Dadurch erhalten Sie eine Liste der verfügbaren Charts, ähnlich der folgenden:
Das Ergebnis bedeutet, dass Ihr Helm-Client ausgeführt wird und auf dem neuesten Stand ist.
Nachdem Helm nun eingerichtet ist, installieren Sie das Helm Chart nfs-server-provisioner, um den NFS-Server einzurichten.
Wenn Sie den Inhalt des Charts untersuchen möchten, werfen Sie einen Blick in seine Dokumentation auf GitHub.
Wenn Sie das Helm-Chart bereitstellen, legen Sie einige Variablen für Ihren NFS-Server fest, um die Konfiguration für Ihre Anwendung weiter zu spezifizieren.
Sie können auch andere Konfigurationsoptionen untersuchen und sie an die Bedürfnisse der Anwendung anpassen.
Verwenden Sie den folgenden Befehl, um das Helm-Chart zu installieren:
Dieser Befehl stellt einen NFS-Server mit den folgenden Konfigurationsoptionen bereit:
Fügt ein Persistent Volume für den NFS-Server mit dem Flag --set hinzu.
Dadurch wird sichergestellt, dass alle gemeinsam genutzten NFS-Daten auch bei Pod-Neustarts persistent gespeichert sind.
Für die persistente Speicherung wird die Speicherklasse do-block-storage verwendet.
Stellt insgesamt 200Gi für den NFS-Server bereit, die in Exporte aufgeteilt werden können.
< $> note Hinweis: Die Option persistence.size bestimmt die Gesamtkapazität aller NFS-Volumes, die Sie bereitstellen können.
Zum Zeitpunkt dieser Veröffentlichung unterstützen nur die DOKS Version 1.16.2-do.3 und spätere Versionen die Erweiterung von Volumes, sodass die Größenänderung dieses Volumes eine manuelle Aufgabe ist, wenn Sie eine frühere Version verwenden.
Stellen Sie in diesem Fall sicher, dass Sie diese Größe im Hinblick auf Ihre zukünftigen Bedürfnisse einstellen.
Nachdem dieser Befehl abgeschlossen ist, erhalten Sie eine Ausgabe, die der folgenden ähnelt:
Um den von Ihnen bereitgestellten NFS-Server zu sehen, führen Sie den folgenden Befehl aus:
Dadurch wird Folgendes angezeigt:
Prüfen Sie als Nächstes die von Ihnen erstellte storageclass:
Sie haben nun einen NFS-Server sowie einen storageclass, die Sie für die dynamische Bereitstellung von Volumes verwenden können.
Als Nächstes können Sie eine Bereitstellung erstellen, die diesen Speicher verwendet, und ihn über mehrere Instanzen hinweg gemeinsam nutzen.
Schritt 2 - Bereitstellen einer Anwendung unter Verwendung eines gemeinsam genutzten PersistentVolumeClaim
In diesem Schritt erstellen Sie eine beispielhafte Bereitstellung auf Ihrem DOKS-Cluster, um Ihre Speichereinrichtung zu testen.
Dies wird eine Nginx Webserver-Anwendung namens web sein.
Um diese Anwendung bereitzustellen, schreiben Sie zunächst die YAML-Datei, um die Bereitstellung zu spezifizieren.
Öffnen Sie eine Datei nginx-test.yaml mit Ihrem Texteditor; dieses Tutorial verwendet nano:
Fügen Sie in diese Datei die folgenden Zeilen ein, um die Bereitstellung mit einem PersistentVolumeClaim namens nfs-data zu definieren:
Speichern Sie die Datei und beenden Sie den Texteditor.
Diese Bereitstellung ist so konfiguriert, dass sie die zugehörigen PersistentVolumeClaim nfs-data verwendet und sie unter / data einbindet.
In der PVC-Definition werden Sie feststellen, dass der storageClassName auf nfs gesetzt ist.
Damit wird dem Cluster mitgeteilt, dass er diese Speicherung nach den Regeln der nfs storageClass, die Sie im vorherigen Schritt erstellt haben, erfüllen muss.
Der neue PersistentVolumeClaim wird verarbeitet und dann wird eine NFS-Freigabe bereitgestellt, um den Anspruch in Form eines Persistent Volumes zu erfüllen.
Der Pod wird versuchen, dieses PVC einzubinden, sobald es bereitgestellt wurde.
Nach dem Einbinden verifizieren Sie die Funktionalität ReadWriteMany (RWX).
Führen Sie die Bereitstellung mit dem folgenden Befehl aus:
Als Nächstes prüfen Sie, ob der Pod web anläuft:
Dadurch wird Folgendes ausgegeben:
Nachdem nun die Beispielbereitstellung läuft, können Sie sie mit dem Befehl kubectl scale auf drei Instanzen skalieren:
Dadurch erhalten Sie folgende Ausgabe:
Führen Sie nun den Befehl kubectl get erneut aus:
Sie finden die skalierten Instanzen der Bereitstellung:
Sie haben nun drei Instanzen Ihrer Nginx-Bereitstellung, die mit demselben Persistent Volume verbunden sind.
Im nächsten Schritt stellen Sie sicher, dass sie Daten untereinander gemeinsam nutzen können.
Schritt 3 - Validieren der gemeinsamen Nutzung von NFS-Daten
Im letzten Schritt überprüfen Sie, ob die Daten von allen Instanzen, die in die NFS-Freigabe eingebunden sind, gemeinsam genutzt werden.
Dazu erstellen Sie eine Datei im Verzeichnis / data in einem der Pods und verifizieren dann, ob die Datei in dem Verzeichnis / data eines anderen Pods vorhanden ist.
Um dies zu validieren, verwenden Sie den Befehl kubectl exec.
Mit diesem Befehl können Sie einen Pod angeben und einen Befehl innerhalb dieses Pods ausführen.
Um mehr über die Überprüfung von Ressourcen unter Verwendung von kubectl zu erfahren, werfen Sie einen Blick auf unseren kubectl-Spickzettel.
Um innerhalb eines Ihrer Pods web eine Datei namens hello _ world zu erstellen, verwenden Sie den Befehl kubectl exec, um den Befehl touch weiterzugeben.
Beachten Sie, dass die Zahl nach web im Pod-Namen für Sie unterschiedlich sein wird. Achten Sie also darauf, den hervorgehobenen Pod-Namen durch einen Ihrer eigenen Pods zu ersetzen, die Sie im letzten Schritt als Ausgabe von kubectl get pods gefunden haben.
Ändern Sie dann den Namen des Pod und verwenden Sie den Befehl ls, um die Dateien im Verzeichnis / data eines anderen Pods aufzulisten:
Ihre Ausgabe zeigt die Datei an, die Sie im ersten Pod erstellt haben:
Dies zeigt, dass alle Pods Daten über NFS gemeinsam nutzen und dass Ihre Einrichtung korrekt funktioniert.
In diesem Tutorial haben Sie einen NFS-Server erstellt, der durch DigitalOcean Block Storage unterstützt wurde.
Der NFS-Server verwendete dann diesen Blockspeicher zur Bereitstellung und zum Export von NFS-Freigaben für Workloads in einem RWX-kompatiblen Protokoll.
Auf diese Weise konnten Sie eine technische Beschränkung der Blockspeicherung von DigitalOcean umgehen und dieselben PVC-Daten über viele Pods hinweg gemeinsam nutzen.
Durch dieses Tutorial ist Ihr DOKS-Cluster nun so eingerichtet, dass er eine wesentlich größere Anzahl von Anwendungsfällen für die Bereitstellung unterstützt.
Wenn Sie mehr über Kubernetes erfahren möchten, sehen Sie sich unser Curriculum für Full-Stack-Entwickler oder die Produktdokumentation für DigitalOcean Kubernetes an.
So richten Sie die Code-Server-Cloud-IDE-Plattform unter Debian 10 ein
3331
Die Erstellung und Einführung von Cloud-IDE-Plattformen (IDE: Integrated Development Environment) nimmt zu, da Entwickler-Tools zunehmend cloudbasiert sind.
Cloud-IDEs ermöglichen Entwicklerteams eine Zusammenarbeit in Echtzeit in einer einheitlichen Entwicklungsumgebung, die Inkompatibilitäten minimiert und die Produktivität steigert.
Cloud-IDEs sind über Webbrowser zugänglich und auf allen modernen Geräten verfügbar.
Visual Studio Code ist ein moderner Code-Editor mit integrierter Git-Unterstützung, einem Code-Debugger, intelligenter automatischer Vervollständigung sowie anpassbaren und erweiterbaren Funktionen.
Dadurch können Sie verschiedene Geräte verwenden, die verschiedene Betriebssysteme ausführen und immer eine einheitliche Entwicklungsumgebung haben.
In diesem Tutorial richten Sie die Code-Server-Cloud-IDE-Plattform auf Ihrem Debian-10-Rechner ein und machen sie auf Ihrer Domäne verfügbar, geschützt durch kostenlose Let 's Encrypt TLS-Zertifikate.
Zum Schluss wird Microsoft Visual Studio Code auf Ihrem Debian-10-Server ausgeführt, ist auf Ihrer Domäne verfügbar und mit einem Passwort geschützt.
Ein Server mit Debian 10 mit mindestens 2 GB RAM, Root-Zugriff und einem Sudo-Konto ohne Rootberechtigung.
Sie können dies einrichten, indem Sie diesem Leitfaden zur Ersteinrichtung des Servers folgen.
Führen Sie dazu die Schritte 1 bis 4 aus So installieren Sie Nginx auf Debian 10 aus.
Dieses Tutorial verwendet durchgehend code-server. < ^ > your-domain < ^ >.
In diesem Abschnitt richten Sie Code-Server auf Ihrem Server ein.
Dabei laden Sie die neueste Version herunter und erstellen einen systemd-Dienst, der Code-Server immer im Hintergrund ausführt.
Des Weiteren geben Sie eine Neustart-Regel für den Dienst an, damit Code-Server nach möglichen Abstürzen oder Neustarts verfügbar bleibt.
Sie speichern alle Daten über Code-Server in einem Ordner namens ~ / code-server.
Erstellen Sie ihn, indem Sie folgenden Befehl ausführen:
Gehen Sie auf die Seite Github releases von Code-Server und wählen Sie den neuesten Linux-Build (die Datei enthält linux in ihrem Namen).
Als dieser Text verfasst wurde, war die neueste Version < ^ > 2.1692 < ^ >.
Laden Sie ihn mit wget herunter, indem Sie folgenden Befehl ausführen:
Entpacken Sie dann das Archiv durch Ausführung von:
Sie erhalten einen Ordner mit dem gleichen Namen wie die von Ihnen heruntergeladene Originaldatei, die die ausführbare Datei des Code-Servers enthält.
Kopieren Sie die ausführbare Datei des Code-Servers in / usr / local / bin, damit Sie systemweit mit dem folgenden Befehl darauf zugreifen können:
Erstellen Sie als Nächstes einen Ordner für Code-Server, in dem er Benutzerdaten speichert:
Nachdem Sie Code-Server heruntergeladen und systemweit verfügbar gemacht haben, erstellen Sie einen systemd-Dienst für die ständige Ausführung des Code-Servers im Hintergrund.
Speichern Sie die Dienstkonfiguration in einer Datei namens code-server.service im Verzeichnis / lib / systemd / system, in dem systemd seine Dienste speichert.
Erstellen Sie sie mit Ihrem Texteditor:
Hier geben Sie zunächst die Beschreibung des Dienstes an.
Dann bestätigen Sie, dass der Dienst nginx vor diesem gestartet werden muss.
Nach dem Abschnitt [Unit] definieren Sie die Art des Dienstes (simple bedeutet, dass der Prozess einfach ausgeführt werden soll) und geben den Befehl an, der ausgeführt wird.
Außerdem geben Sie an, dass die globale ausführbare Datei des Code-Servers mit einigen Argumenten gestartet werden soll, die für Code-Server spezifisch sind. --host 127.0.0.1 bindet ihn an localhost, damit er nur von ihrem Server aus direkt zugänglich ist. --user-data-dir / var / lib / code-server legt Ihr Benutzerdaten-Verzeichnis fest und --auth password spezifiziert, dass es Besucher mit einem Passwort authentifizieren soll, das in der Umgebungsvariable PASSWORD in der Zeile darüber angegeben ist.
Vergessen Sie nicht, < ^ > your _ password < ^ > durch Ihr gewünschtes Passwort zu ersetzen. Speichern und schließen Sie danach die Datei.
Die nächste Zeile teilt systemd mit, dass Code-Server bei allen Störungen (z. B. bei Absturz oder wenn der Vorgang beendet wird) neu starten soll.
Der Abschnitt [Install] weist systemd an, diesen Dienst zu starten, wenn Sie sich auf Ihrem Server anmelden können.
Starten Sie den Code-Server-Dienst durch Ausführung des folgenden Befehls:
Überprüfen Sie, ob er korrekt gestartet wurde, indem Sie seinen Status beachten:
Damit Code-Server nach einem Server-Neustart automatisch startet, aktivieren Sie seinen Dienst durch Ausführung des folgenden Befehls:
In diesem Schritt haben Sie Code-Server heruntergeladen und global verfügbar gemacht.
Dann haben Sie einen systemd-Dienst für ihn erstellt und aktiviert, damit Code-Server bei jedem Server-Boot startet.
Als Nächstes machen Sie diesen auf Ihrer Domäne verfügbar, indem Sie Nginx konfigurieren, um als Reverseproxy zwischen Besucher und Code-Server zu fungieren.
Schritt 2 - Code-Server auf Ihrer Domäne verfügbar machen
In diesem Abschnitt konfigurieren Sie Nginx als Reverseproxy für den Code-Server.
Wie Sie im Schritt Nginx-Voraussetzung erfahren haben, werden seine Site-Konfigurationsdateien unter / etc / nginx / sites-available gespeichert und müssen später symbolisch mit / etc / nginx / sites-enabled verknüpft werden, um aktiv zu sein.
Speichern Sie die Konfiguration, um Code-Server auf Ihrer Domäne verfügbar zu machen, in einer Datei namens code-server.conf unter / etc / nginx / sites-available.
Beginnen Sie mit dem Erstellen mithilfe Ihres Editors:
Ersetzen Sie code-server. < ^ > your-domain < ^ > ​ ​ ​ durch Ihre gewünschte Domäne, speichern und schließen Sie dann die Datei.
In dieser Datei definieren Sie, dass Nginx auf den HTTP-Port 80 lauschen soll. Dann geben Sie einen server _ name an, der Nginx mitteilt, für welche Domäne Anfragen akzeptiert und diese spezielle Konfiguration angewendet werden sollen.
Für den Root-Ort (/) geben Sie im nächsten Block an, dass Anfragen an den Code-Server auf localhost: 8080 ausgeführt werden sollen.
Die nächsten drei Zeilen (ab proxy _ set _ header) weisen Nginx an, einige HTTP-Anforderungsheader zu übertragen, die für das korrekte Funktionieren von WebSockets benötigt werden, die Code-Server extensiv verwendet.
Damit die Konfiguration dieser Site aktiv wird, müssen Sie eine symbolische Verknüpfung davon im Ordner / etc / nginx / sites-enabled ​ ​ ​ erstellen, indem Sie Folgendes ausführen:
Um die Gültigkeit der Konfiguration zu testen, führen Sie folgenden Befehl aus:
Damit die Konfiguration wirksam werden kann, müssen Sie Nginx neu starten:
Ihre Code-Server-Installation ist nun auf Ihrer Domäne verfügbar.
Im nächsten Schritt sichern Sie es durch Anwendung eines kostenlosen Let 's Encrypt TLS-Zertifikates.
In diesem Abschnitt sichern Sie Ihre Domäne mit einem Let 's Encrypt TLS-Zertifikat, das Sie mit Certbot bereitstellen.
Um die neueste Version von Certbot und dessen Nginx-Plugin zu installieren, führen Sie folgenden Befehl aus:
Als Teil der Voraussetzungen haben Sie ufw (Uncomplicated Firewall) aktiviert und so konfiguriert, dass ein unverschlüsselter HTTP-Verkehr erlaubt ist.
Damit Sie auf die gesicherte Site zugreifen können, müssen Sie sie so konfigurieren, dass sie verschlüsselten Verkehr akzeptiert. Dafür führen Sie folgenden Befehl aus:
Ähnlich wie bei Nginx müssen Sie es neu laden, damit die Konfiguration wirksam werden kann:
Navigieren Sie anschließend in Ihrem Browser zu der Domäne, die Sie für Code-Server verwendet haben.
Sie erhalten die Anmeldeaufforderung des Code-Servers.
Code-Server fragt Sie nach Ihrem Passwort.
Geben Sie dasjenige ein, das Sie im vorigen Schritt festgelegt haben und drücken Sie Enter IDE.
Jetzt rufen Sie Code-Server auf und sehen sofort seine Editor-GUI.
Nachdem Sie nun überprüft haben, ob Code-Server korrekt auf Ihrer Domäne verfügbar ist, installieren Sie Let 's Encrypt TLS-Zertifikate, um es mit Certbot zu sichern.
Um Zertifikate für Ihre Domäne anzufordern, führen Sie folgenden Befehl aus:
In diesem Befehl führen Sie certbot aus, um Zertifikate für Ihre Domäne anzufordern - Sie durchlaufen den Domänennamen mit dem Parameter -d.
Das Flag --nginx weist sie an, eine eine automatische Änderung der Nginx-Site-Konfiguration zur Unterstützung von HTTPS vorzunehmen.
Vergessen Sie nicht, code-server. < ^ > your-domain < ^ > durch Ihren Domänennamen zu ersetzen.
Wenn Sie Certbot zum ersten Mal ausführen, werden Sie aufgefordert, eine E-Mail-Adresse für dringende Bekanntmachungen anzugeben und die AGBs von EFF zu akzeptieren.
Certbot fordert daraufhin Zertifikate für Ihre Domäne von Let 's Encrypt an.
Dann werden Sie gefragt, ob Sie den gesamten HTTP-Verkehr an HTTPS weiterleiten möchten:
Es wird empfohlen, die zweite Option auszuwählen, um die Sicherheit zu maximieren.
Nachdem Sie Ihre Auswahl eingegeben haben, drücken Sie die EINGABETASTE.
Das bedeutet, dass Certbot TLS-Zertifikate erfolgreich generiert und auf die Nginx-Konfiguration für Ihre Domäne angewendet hat.
Sie können Ihre Code-Server-Domäne nun in Ihrem Browser neu laden und werden links von der Websiteadresse ein Schloss sehen: Das bedeutet, dass Ihre Verbindung korrekt gesichert ist.
Nachdem Code-Server nun auf Ihrer Domäne über einen gesicherten Nginx-Reverseproxy zugänglich ist, können Sie die Benutzeroberfläche von Code-Server überprüfen.
Schritt 4 - Verwendung der Code-Server-Benutzeroberfläche
In diesem Abschnitt werden Sie einige der Funktionen der Code-Server-Benutzeroberfläche verwenden.
Da Code-Server Visual Studio Code ist, der in der Cloud ausgeführt wird, hat er dieselbe Benutzeroberfläche wie die eigenständige Desktop-Edition.
Auf der linken Seite der IDE befindet sich eine vertikale Reihe von sechs Schaltflächen, welche die am häufigsten verwendeten Funktionen in einem Seitenbereich namens Aktivitätsleiste öffnen.
Code-Server-GUI - Seitenbereich
Diese Leiste ist anpassbar: Sie können diese Ansichten in eine andere Reihenfolge bringen oder von der Leiste entfernen.
Standardmäßig öffnet die erste Schaltfläche das allgemeine Menü in einem Dropdown, während die zweite Ansicht das Feld Explorer öffnet, das eine baumartige Navigation der Struktur des Projekts bereitstellt.
Hier können Sie Ihre Ordner und Dateien verwalten und sie je nach Bedarf erstellen, löschen, verschieben und umbenennen.
Die nächste Ansicht bietet Zugriff auf die Funktion Suchen und Ersetzen.
Im Anschluss daran wird in der standardmäßigen Reihenfolge Ihre Ansicht der Quellcodeverwaltungssysteme wie Git angezeigt.
Visual-Studio-Code unterstützt auch andere Quellcodeverwaltungsanbieter. Weitere Anweisungen für Arbeitsabläufe bei Quellcodeverwaltung mit dem Editor finden Sie in dieser Dokumentation.
Git-Bereich mit Kontextmenü geöffnet
Die Debugger-Option auf der Aktivitätsleiste bietet alle gängigen Aktionen zum Debuggen in der Kontrollleiste.
Visual Studio Code bietet integrierte Unterstützung für den Laufzeit-Debugger Node.js und jede Sprache, die zu Javascript transpiliert.
Für weitere Sprachen können Sie Erweiterungen für den erforderlichen Debugger installieren.
Debugging-Konfigurationen können Sie in der Datei launch.json speichern.
Debugger-Ansicht mit launch.json geöffnet
Die letzte Ansicht in der Aktivitätsleiste bietet ein Menü zum Zugriff auf verfügbare Erweiterungen auf dem Marketplace.
Code-Server-GUI - Registerkarten
Der zentrale Teil der GUI ist Ihr Editor, den Sie für Ihre Codebearbeitung durch Registerkarten trennen können.
Ihre Bearbeitungsansicht können ein Rastersystem oder nebeneinander angeordnete Dateien sein.
Editor-Rasteransicht
Nach der Erstellung einer neuen Datei über das Menü File öffnet sich eine leere Datei in einer neuen Registerkarte. Sobald sie gespeichert ist, wird der Name der Datei im Seitenfeld Explorer sichtbar.
Sie können Ordner durch einen Rechtsklick auf die Seitenleiste Explorer und durch Klicken auf New Folder erstellen.
Sie können einen Ordner erweitern, indem Sie auf seinen Namen klicken, sowie Dateien und Ordner in obere Teile der Hierarchie ziehen und ablegen, um sie an einen neuen Ort zu verschieben.
Code-Server-GUI - New Folder
Sie können Zugriff auf ein Terminal erhalten, indem Sie STRG + UMSCHALT + 'eingeben oder im oberen Dropdownmenü auf Terminal klicken und New Terminal auswählen.
Das Terminal öffnet sich in einem unteren Feld und sein Arbeitsverzeichnis wird auf den Arbeitsbereich des Projekts festgelegt, der die im Seitenbereich Explorer gezeigten Dateien und Ordner erhält.
Sie haben sich nun einen Überblick über die Code-Server-Benutzeroberfläche und einige der am häufigsten verwendeten Funktionen verschafft.
Code-Server, eine vielseitige Cloud-IDE, ist jetzt auf Ihrem Debian-10-Server installiert, auf Ihrer Domäne verfügbar und mit Let 's-Encrypt-Zertifikaten gesichert.
Jetzt können Sie einzeln oder als Team an Projekten arbeiten.
Die Ausführung einer Cloud-IDE gibt Ressourcen auf Ihrem lokalen Computer frei und ermöglicht es Ihnen, die Ressourcen bei Bedarf zu skalieren.
Wenn Sie Code-Server auf Ihrem DigitalOcean Kubernetes-Cluster ausführen möchten, sehen Sie sich unser Tutorial So richten Sie die Code-Server-Cloud-IDE-Plattform auf DigitalOcean Kubernetes ein an.
So installieren und richten Sie WordPress mit LAMP unter Ubuntu 18.04 mithilfe von Ansible ein
3340
Server-Automatisierung spielt aufgrund der Löschbarkeit von modernen Anwendungsumgebungen eine wesentliche Rolle bei der Systemverwaltung.
Konfigurationsverwaltungs-Tools wie Ansible werden üblicherweise verwendet, um den Prozess der automatischen Servereinrichtung zu optimieren, indem Standardverfahren für neue Server festgelegt und menschliche Fehler in Zusammenhang mit manueller Einrichtung reduziert werden.
Ansible bietet eine einfache Architektur, die keiner speziellen Software für die Installation auf Knoten bedarf.
Des Weiteren bietet es eine Reihe stabiler Funktionen und integrierter Module, die das Schreiben von Automatisierungsskripts erleichtern.
Dieser Leitfaden erklärt, wie Sie die Schritte in unserem Leitfaden So installieren Sie WordPress mit LAMP unter Ubuntu 18.04 ​ ​ ​ automatisieren können.
WordPress ist das beliebteste CMS (Content-Management-System) im Internet, das es Benutzern ermöglicht, flexible Blogs und Websites auf einem MySQL-Backend mit PHP-Verarbeitung einzurichten.
Für die automatisierte Einrichtung anhand des Playbooks, das wir in diesem Leitfaden besprechen, benötigen Sie Folgendes:
Einen Ansible-Steuerknoten: einen Ubuntu-18.04-Rechner, auf dem Ansible installiert und für die Verbindung mit Ihren Ansible-Hosts mittels SSH-Schlüsseln konfiguriert ist.
Stellen Sie sicher, dass der Steuerknoten einen regelmäßigen Benutzer mit sudo-Berechtigungen und einer aktivierten Firewall hat, wie in unserem Leitfaden Ersteinrichtung des Servers ausgeführt ist.
Um Ansible einzurichten, folgen Sie unserem Leitfaden So installieren und konfigurieren Sie Ansible unter Ubuntu 18.04.
Einen oder mehrere Ansible-Hosts: einen oder mehrere Remote-Ubuntu-18.04-Server, die zuvor gemäß dem Leitfaden So verwenden Sie Ansible zur Automatisierung der Ersteinrichtung des Servers unter Ubuntu 18.04 eingerichtet wurden.
< $> note Gehen Sie zunächst sicher, dass Ihr Ansible-Steuerknoten sich verbinden und Befehle auf Ihrem / Ihren Ansible-Host (s) ausführen kann.
Infos zu einem Verbindungstest finden Sie in Schritt 3 unter So installieren und konfigurieren Sie Ansible unter Ubuntu 18.04.
Welchen Zweck hat dieses Playbook?
Dieses Ansible-Playbook bietet eine Alternative zur manuellen Ausführung des Verfahrens, das in unserem Leitfaden So installieren Sie WordPress mit LAMP unter Ubuntu 18.04 beschrieben ist.
Eine Ausführung dieses Playbooks führt die folgenden Aktionen auf Ihren Ansible-Hosts durch:
Installieren Sie aptitude, das von Ansible als Alternative zum Paketmanager apt bevorzugt wird.
Installieren Sie die erforderlichen LAMP-Pakete und PHP-Erweiterungen.
Erstellen und aktivieren Sie einen neuen Apache VirtualHost für die WordPress-Website.
Aktivieren Sie das Apache-Rewrite-Modul (mod _ rewrite) ​ ​.
Deaktivieren Sie die Apache-Standard-Website.
Legen Sie das Passwort für den MySQL-Benutzer root fest.
Entfernen Sie anonyme MySQL-Konten und die Testdatenbank.
Erstellen Sie eine neue MySQL-Datenbank und einen Benutzer für die WordPress-Website.
Richten Sie UFW ein, um HTTP-Verkehr auf dem konfigurierten Port (standardmäßig 80) zuzulassen.
Laden Sie WordPress herunter und entpacken Sie es.
Richten Sie eine korrekte Verzeichniseigentümerschaft und korrekte Berechtigungen ein.
Richten Sie die Datei wp-config.php mit der bereitgestellten Vorlage ein.
Wenn die Ausführung des Playbooks abgeschlossen ist, wird eine WordPress-Installation basierend auf den Optionen, die Sie in Ihren Konfigurationsvariablen definiert haben, auf einer LAMP-Umgebung ausgeführt.
So verwenden Sie dieses Playbook
Als Erstes müssen Sie das Playbook WordPress auf LAMP und seine Abhängigkeiten aus dem Repository do-community / ansible-playbooks abrufen.
Klonen Sie dieses Repository in einen lokalen Ordner im Ansible-Steuerknoten.
Falls Sie dieses Repository bereits anhand eines anderen Leitfadens geklont haben, greifen Sie auf Ihre bestehende Kopie von ansible-playbooks ​ ​ ​ zu und führen Sie den Befehl git pull aus, um sicherzustellen, dass Sie aktualisierten Inhalt haben:
Wenn Sie das Repository do-community / ansible-playbooks ​ ​ zum ersten Mal verwenden, klonen Sie zuerst das Repository in Ihren Basisordner mit:
Die Dateien, die wir brauchen, befinden sich im Ordner wordpress-lamp _ ubuntu1804, der die folgende Struktur hat:
Hier ist eine Beschreibung jeder dieser Dateien:
files / apache.conf.j2: Vorlagendatei zur Einrichtung des Apache VirtualHost.
files / wp-config.php.j2: Vorlagendatei zum Einrichten der Konfigurationsdatei von WordPress.
vars / default.yml: Variablendatei zur Anpassung der Playbook-Einstellungen.
playbook.yml: Die Playbook-Datei mit Aufgaben zur Ausführung auf dem / den Remoteserver (n).
readme.md: Eine Textdatei mit Informationen über dieses Playbook.
Bearbeiten Sie die Variablendatei des Playbooks zur Anpassung der Optionen.
Greifen Sie auf das Verzeichnis wordpress-lamp _ ubuntu1804 zu und öffnen Sie die Datei vars / default.yml mit dem Befehlszeilen-Editor Ihrer Wahl:
Diese Datei enthält einige Variablen, die Ihrer Aufmerksamkeit bedürfen:
Die folgende Liste enthält eine kurze Erklärung jeder dieser Variablen und wie Sie diese ändern könnten:
php _ modules: Ein Array mit PHP-Erweiterungen, die zur Unterstützung Ihrer Einrichtung von WordPress installiert werden sollen.
Sie müssen diese Variable nicht ändern, aber Sie könnten neue Erweiterungen in die Liste aufnehmen, wenn Ihre spezifische Einrichtung es erfordert.
mysql _ root _ password: Das gewünschte Passwort für das MySQL-Konto root.
mysql _ db: Der Name der MySQL-Datenbank, die für WordPress erstellt werden soll.
mysql _ user ​ ​ ​: Der Name des MySQL-Benutzers, der für WordPress erstellt werden soll.
mysql _ password: Das Passwort für den neuen MySQL-Benutzer.
http _ host: Ihr Domänenname.
http _ conf: Der Name der Konfigurationsdatei, die in Apache erstellt wird.
http _ port: HTTP-Port für diesen virtuellen Host, bei dem 80 die Standardeinstellung ist.
Sobald Sie mit der Aktualisierung der Variablen in vars / default.yml fertig sind, speichern und schließen Sie diese Datei.
Sie können dieses Playbook jetzt auf einem oder mehreren Servern ausführen.
Die meisten Playbooks werden standardmäßig auf jedem Server in Ihrem Inventar ausgeführt.
Sie können das Flag -l verwenden, um sicherzustellen, dass nur eine Teilmenge von Servern oder ein einzelner Server von dem Playbook betroffen ist.
Sie können auch das Flag -u verwenden, um anzugeben, welchen Benutzer Sie auf dem Remoteserver für die Verbindung und Ausführung der Playbook-Befehle auf den Remote-Hosts verwenden.
Um das Playbook nur auf < ^ > server1 < ^ > mit Verbindung als < ^ > sammy < ^ > auszuführen, können Sie folgenden Befehl verwenden:
Sie erhalten eine Ausgabe, die der folgenden ähnelt:
< $> note Anmerkung: Weitere Informationen zur Ausführung von Ansible-Playbooks finden Sie in unserem Leitfaden Ansible-Schummelzettel.
Wenn das Playbook ausgeführt ist, können Sie zu Ihrem Webbrowser gehen, um die Installation von WordPress von dort aus zu beenden.
Navigieren Sie zu dem Domänennamen oder der öffentlichen IP-Adresse Ihres Servers:
Sie sehen eine Seite wie diese:
Seite der WordPress-Sprachauswahl
Nach Auswahl der Sprache, die Sie für Ihre WordPress-Installation verwenden möchten, kommen Sie zum letzten Schritt der Einrichtung Ihres WordPress-Benutzers und -Passworts, damit Sie sich in Ihrem Kontrollfeld anmelden können:
WordPress-Einrichtung
WP-Anmeldeaufforderung
WP-Adminbereich
Einige gängige Schritte zur Anpassung Ihrer WordPress-Installation sind u. a. die Auswahl der Permalink-Einstellung für Ihre Beiträge (in Einstellungen > Permalinks) und die Wahl eines neuen Designs (in Darstellung > Design).
Der Inhalt des Playbooks
Sie finden die Server-Einrichtung für WordPress auf LAMP in diesem Tutorial im Ordner wordpress-lamp _ ubuntu1804 ​ ​ ​ im Repository DigitalOcean Community Playbooks.
Klicken Sie zum direkten Kopieren oder Herunterladen des Skript-Inhalts auf die Schaltfläche Raw im oberen Bereich jedes Skripts.
Der vollständige Inhalt des Playbooks sowie seine zugehörigen Dateien finden Sie ebenfalls hier.
vars / default.yml
Die Variablendatei default.yml enthält Werte, die in den Playbook-Aufgaben verwendet werden, wie z. B. Datenbank-Einstellungen und den Domänennamen für die Konfiguration mit Apache.
files / apache.conf.j2
Die Datei apache.conf.j2 ist eine Jinja 2-Vorlagendatei, die einen neuen Apache VirtualHost konfiguriert.
Die in dieser Vorlage verwendeten Variablen sind in der Variablendatei vars / default.yml definiert.
files / wp-config.php.j2
Die Datei wp-config.php.j2 ist eine weitere Jinja-Vorlage für die Einrichtung der Hauptkonfigurationsdatei, die von WordPress verwendet wird.
Eindeutige Authentifizierungsschlüssel und Salts werden mit einer Hash-Funktion generiert.
playbook.yml
Alle Aufgaben dieser Einrichtung sind in der Datei playbook.yml definiert.
Am Anfang werden die Gruppen von Servern definiert, die das Ziel dieser Einrichtung (all) sein sollen. Anschließend wird mit become: true definiert, dass Aufgaben standardmäßig mit Rechteausweitung (sudo) ausgeführt werden sollen.
Dann enthält es die Variablendatei vars / default.yml, um Konfigurationsoptionen zu laden.
Sie können diese Dateien an Ihre individuellen Bedürfnisse in Ihrem Workflow anpassen.
In diesem Leitfaden haben wir den Prozess der Installation und Einrichtung einer WordPress-Website mit LAMP auf einem Ubuntu-18.04-Server automatisiert.
Wenn Sie weitere Aufgaben in dieses Playbook aufnehmen möchten, um Ihre Servereinrichtung weiter anzupassen, finden Sie diesbezügliche Informationen in unserer Ansible-Einführung Grundlagen des Konfigurationsmanagements: Ansible-Playbooks schreiben.
So schreiben Sie asynchronen Code in Node.js
3691
Bei vielen Programmen in JavaScript wird Code ausgeführt, während der Entwickler ihn schreibt - Zeile für Zeile.
Dies wird synchrone Ausführung genannt, da die Zeilen nacheinander in der Reihenfolge ausgeführt werden, in der sie geschrieben wurden.
Jedoch muss nicht jeden Befehl, den Sie dem Computer geben, sofort beachtet werden.
Wenn Sie beispielsweise eine Netzwerkanfrage senden, muss der Prozess, der Ihren Code ausführt, darauf warten, dass die Daten zurückkehren, bevor er damit arbeiten kann. In diesem Fall würde Zeit verschwendet werden, wenn er keinen anderen Code ausführen würde, während er darauf wartet, dass die Netzwerkanfrage abgeschlossen wird.
Um dieses Problem zu lösen, verwenden Entwickler asynchrone Programmierung, wobei Codezeilen in einer anderen Reihenfolge ausgeführt werden als in jener, in der sie geschrieben wurden.
Bei asynchroner Programmierung können wir einen anderen Code ausführen, während wir darauf warten, dass lange Aktivitäten wie Netzwerkanfragen abgeschlossen werden.
JavaScript-Code wird in einem einzelnen Thread innerhalb eines Computer-Prozesses ausgeführt.
Sein Code wird auf diesem Thread synchron verarbeitet, wobei immer nur ein Befehl ausgeführt wird.
Wenn wir daher eine zeitintensive Aufgabe auf diesem Thread ausführen, wird der gesamte verbleibende Code blockiert, bis die Aufgabe abgeschlossen ist.
Indem wir uns JavaScripts asynchrone Programmierung zunutze machen, können wir zeitintensive Aufgaben zu einem Hintergrund-Thread auslagern, um dieses Problem zu vermeiden.
Wenn die Aufgabe abgeschlossen ist, wird der Code, den wir für die Datenverarbeitung dieser Aufgabe benötigen, wieder zum einzelnen Haupt-Thread übertragen.
In diesem Tutorial lernen Sie, wie JavaScript asynchrone Aufgaben mithilfe von Event Loop verwaltet. Dies ist ein JavaScript-Konstrukt, das eine neue Aufgabe ausführt, während es auf eine andere wartet.
Dann erstellen Sie ein Programm, das mit asynchroner Programmierung eine Filmliste von einer Studio Ghibli API ​ ​ ​ anfordert und die Daten in einer CSV-Datei speichert.
Der asynchrone Code wird auf drei Arten geschrieben: Callbacks, Promises und mit den Schlüsselwörtern async / await.
< $> note Anmerkung: Seitdem dieser Beitrag verfasst wurde, wird asynchrone Programmierung nicht mehr nur mit Callbacks durchgeführt. Wenn man jedoch diese veraltete Methode lernt, versteht man besser, warum die JavaScript-Community jetzt Promises verwendet.
Die Schlüsselwörter async / await ermöglichen es uns, Promises auf eine weniger ausführliche Weise zu verwenden, und sind daher die standardmäßige Art der asynchronen Programmierung in JavaScript zur Zeit der Veröffentlichung dieses Artikels.
Node.js auf Ihrem Entwicklungs-Rechner installiert.
Außerdem müssen Sie sich mit der Installation von Paketen in Ihrem Projekt auskennen.
Infos dazu finden Sie in unserem Leitfaden So verwenden Sie Node.js-Module mit npm und package.json.
Es ist wichtig, dass Sie Funktionen in JavaScript erstellen und ausführen können, bevor Sie deren asynchrone Verwendung erlernen.
Wenn Sie eine Einführung oder Auffrischung benötigen, können Sie unseren Leitfaden So definieren Sie Funktionen in JavaScript lesen.
Die Ereignisschleife
Beginnen wir mit den internen Abläufen der Funktionsausführung von JavaScript.
Dies zu verstehen ermöglicht es Ihnen, asynchronen Code gezielt zu schreiben, und es hilft Ihnen in Zukunft bei der Code-Problembehebung.
Da JavaScript Interpreter den Code ausführt, wird jede Funktion, die aufgerufen wird, dem Call stack von JavaScript hinzugefügt.
Der Call stack ist ein Stapel - eine listenähnliche Datenstruktur, bei der Elemente nur oben hinzugefügt und entfernt werden können.
Stacks folgen dem Prinzip "Last in, first out" (LIFO-Prinzip).
Wenn Sie dem Stapel zwei Einträge hinzufügen, wird das zuletzt hinzugefügte Element zuerst entfernt.
Wir illustrieren es mit einem Beispiel mit dem Call stack.
Wenn JavaScript auf eine FunktionA () trifft, die aufgerufen wird, wird sie dem Call stack hinzugefügt.
Wenn diese Funktion functionA () eine andere Funktion functionB () aufruft, wird functionB () dem Call stack oben hinzugefügt.
Da JavaScript die Ausführung einer Funktion abgeschlossen hat, wird sie vom Call stack entfernt.
Daher führt JavaScript functionB () zuerst aus, löscht sie aus dem Stapel, wenn sie abgeschlossen ist und schließt die Ausführung von functionA () ab und entfernt sie aus dem Call stack.
Aus diesem Grund werden innere Funktionen immer vor ihren äußeren Funktionen ausgeführt.
Wenn JavaScript auf eine asynchrone Operation trifft, wie z. B. in eine Datei schreiben, fügt sie diese zu einer Tabelle in ihrem Speicher hinzu.
Diese Tabelle speichert die Operation, die Bedingung, um sie zu abzuschließen, und die Funktion, die nach Fertigstellung aufgerufen wird.
Wenn die Operation abgeschlossen ist, fügt JavaScript die verknüpfte Funktion der Message Queue hinzu.
Eine Warteschlange (Queue) ist eine weitere listenartige Datenstruktur, bei der Einträge nur unten hinzugefügt, jedoch von oben entfernt werden können.
Wenn zwei oder mehr asynchrone Operationen in der Message Queue bereit für die Ausführung ihrer Funktionen sind, wird die Funktion der asynchronen Operation, die zuerst abgeschlossen wurde, zuerst für die Ausführung markiert.
Funktionen in der Message Queue warten, dass sie dem Call stack hinzugefügt werden.
Die Ereignisschleife ist ein fortlaufender Vorgang, der überprüft, ob der Call stack leer ist.
Wenn das der Fall ist, wird der erste Eintrag in der Message Queue in den Call stack verschoben.
JavaScript priorisiert Funktionen in der Message Queue gegenüber Funktions-Calls, die es im Code interpretiert.
Der kombinierte Effekt des Call stacks, der Message Queue und Ereignisschleife ermöglicht die Verarbeitung von JavaScript-Code bei der Verwaltung asynchroner Aktivitäten.
Nachdem Sie sich jetzt einen Überblick darüber verschafft haben, wie die Ereignisschleife funktioniert, wissen Sie, wie der asynchrone Code, den Sie schreiben, ausgeführt wird.
Mit diesem Wissen können Sie jetzt asynchronen Code mit drei verschiedenen Ansätzen erstellen: Callbacks, Promises und async / await.
Asynchrone Programmierung mit Callbacks
Eine Callback-Funktion ist eine Funktion, die als Argument einer anderen Funktion übergeben und dann ausgeführt wird, wenn die andere Funktion abgeschlossen ist.
Wir verwenden Callbacks, um sicherzustellen, dass Code nur ausgeführt wird, wenn eine asynchrone Operation abgeschlossen ist.
Callbacks waren lange Zeit der gängigste Mechanismus für das Schreiben von asynchronem Code, aber jetzt sind sie weitgehend veraltet, da sie das Lesen des Codes erschweren können.
In diesem Schritt schreiben Sie ein Beispiel für asynchronen Code mit Callbacks, damit Sie ihn als Grundwert verwenden können, um die gesteigerte Effizienz anderer Strategien zu sehen.
Es gibt viele Möglichkeiten, um Callback-Funktionen in einer anderen Funktion zu verwenden.
In der Regel haben sie diese Struktur:
Obwohl JavaScript oder Node.js es syntaktisch nicht erfordert, dass die Callback-Funktion das letzte Argument der äußeren Funktion ist, ist es eine gängige Praxis, die das Identifizieren von Callbacks erleichtert.
JavaScript-Entwickler verwenden oft auch eine anonyme Funktion als Callback.
Anonyme Funktionen sind solche, die ohne Namen erstellt werden.
Es ist in der Regel viel lesbarer, wenn eine Funktion am Ende der Argumentliste definiert wird.
Um Callbacks zu demonstrieren, erstellen wir ein Node.js-Modul, das eine Liste aus Filmen von Studio Ghibli in eine Datei schreibt.
Erstellen Sie zunächst einen Ordner, in dem unsere JavaScript-Datei und ihre Ausgabe gespeichert werden:
Gehen Sie dann in diesen Ordner hinein:
Wir beginnen mit einer HTTP-Anfrage an die Studio Ghibli API, deren Ergebnisse unsere Callback-Funktion protokolliert.
Dazu installieren wir eine Bibliothek, die es uns ermöglicht, auf die Daten einer HTTP-Antwort in einem Callback zuzugreifen.
Initialisieren Sie npm in Ihrem Terminal, damit wir später eine Referenz für unsere Pakete haben:
Installieren Sie dann die Bibliothek request:
Öffnen Sie nun eine neue Datei namens callbackMovies.js in einem Texteditor wie nano:
Geben Sie den folgenden Code in Ihren Texteditor ein.
Zuerst senden wir eine HTTP-Anfrage mit dem Modul request:
In der ersten Zeile laden wir das Modul request, das über npm installiert wurde.
Das Modul gibt eine Funktion zurück, die HTTP-Anfragen vornehmen kann. Diese Funktion speichern wir dann in der Konstante request.
Danach erstellen wir die HTTP-Anfrage mit der Funktion request ().
Drucken wir nun die Daten von der HTTP-Anfrage an die Konsole, indem wir die hervorgehobenen Änderungen hinzufügen:
Wenn wir die Funktion request () verwenden, geben wir ihr zwei Parameter:
Die URL der Website, die wir anfragen
Eine Callback-Funktion, die Fehler oder erfolgreiche Antworten behandelt, nachdem die Anfrage abgeschlossen ist.
Unsere Callback-Funktion verfügt über drei Argumente: error, response und body.
Wenn die HTTP-Anfrage abgeschlossen ist, erhalten die Argumente automatisch Werte, die vom Ergebnis abhängen.
Wenn das Senden der Anfrage fehlgeschlagen ist, würde error ein Objekt enthalten, aber response und body wären null.
Wenn die Anfrage erfolgreich war, wird die HTTP-Antwort in response gespeichert.
Wenn unsere HTTP-Antwort Daten zurückgibt (in diesem Beispiel erhalten wir JSON), dann sind die Daten in body festgelegt.
Unsere Callback-Funktion prüft zunächst, ob wir einen Fehler erhalten.
Am besten ist es, einen Callback zuerst auf Fehler zu überprüfen, damit die Ausführung des Callback nicht mit fehlenden Daten fortgesetzt wird. In diesem Fall protokollieren wir den Fehler und die Ausführung der Funktion.
Dann überprüfen wir den Statuscode der Antwort.
Unser Server ist vielleicht nicht immer verfügbar und APIs können sich ändern, wodurch einst sinnvolle Anfragen fehlerhaft werden.
Indem Sie überprüfen, ob der Statuscode 200 ist - was bedeutet, dass die Anfrage "OK" war - können Sie sicher sein, dass unsere Antwort unseren Erwartungen entspricht.
Schließlich parsen wir den Body der Antwort an ein Array und durchlaufen jeden Film, um seinen Namen und sein Erscheinungsjahr zu protokollieren.
Nachdem Sie die Datei gespeichert und geschlossen haben, führen Sie dieses Skript aus:
Wir haben eine Liste von Studio-Ghibli-Filmen mit Ihrem Erscheinungsjahr erhalten.
Jetzt schließen wir dieses Programm ab, indem wir die Film-Liste, in die wir uns gerade einloggen, eine Datei schreiben.
Aktualisieren Sie die Datei callbackMovies.js in Ihrem Texteditor, um den folgenden hervorgehobenen Code aufzunehmen, der eine CSV-Datei mit unseren Filmdaten erstellt:
Anhand der hervorgehobenen Änderungen sehen wir, dass wir das Modul fs importieren.
Dieses Modul ist der Standard in allen Node.js-Installationen und enthält die Methode writeFile (), die asynchron in eine Datei schreiben kann.
Statt die Daten in der Konsole zu protokollieren, fügen wir sie nun der Zeichenfolgenvariablen movieList hinzu.
Dann speichern wir den Inhalt der movieList mit writeFile () in einer neuen Datei - callbackMovies.csv.
Schließlich geben wir der Funktion writeFile (), die das Argument error hat, ein Callback.
Dadurch können wir Fälle verarbeiten, bei denen wir nicht in der Lage sind, in eine Datei zu schreiben - z. B. wenn der Benutzer, der den node-Prozess ausführt, diese Berechtigungen nicht hat.
Speichern Sie die Datei und führen Sie dieses Node.js-Programm erneut aus mit:
In Ihrem Ordner ghibliMovies sehen Sie callbackMovies.csv, die folgenden Inhalt hat:
Beachten Sie, dass wir im Callback der HTTP-Anfrage in unsere CSV-Datei schreiben.
Sobald der Code in der Callback-Funktion ist, schreibt er erst in die Datei, nachdem die HTTP-Anfrage abgeschlossen wurde.
Wenn wir nach der Erstellung unserer CSV-Datei mit einer Datenbank kommunizieren wollten, würden wir eine weitere asynchrone Funktion erstellen, die im Callback von writeFile () abgerufen würde.
Je mehr asynchronen Code wir haben, desto mehr Callback-Funktionen müssen geschachtelt werden.
Stellen wir uns vor, dass wir fünf asynchrone Operationen ausführen möchten, wobei jede erst dann ausgeführt werden kann, wenn eine andere abgeschlossen ist.
Wenn wir das programmieren würden, hätten wir in etwa Folgendes:
Wenn geschachtelte Callbacks viele Zeilen von Code ausführen müssen, werden sie wesentlich komplexer und weniger lesbar.
Wenn Ihr JavaScript größer und komplexer wird, macht sich dieser Effekt immer mehr bemerkbar, bis er letztendlich nicht mehr kontrollierbar ist.
Aus diesem Grund verwenden Entwickler keine Callbacks mehr für die Handhabung asynchroner Operationen.
Um die Syntax unseres asynchronen Codes zu verbessern, können wir stattdessen Promises verwenden.
Verwendung von Promises für präzise asynchrone Programmierung
Ein Promise ist ein JavaScript-Objekt, das irgendwann in der Zukunft einen Wert zurückgeben wird.
Asynchrone Funktionen können Promise-Objekte anstatt konkreter Werte zurückgeben.
Wenn wir in der Zukunft einen Wert erhalten, bezeichnen wir das Promise (Versprechen) als erfüllt.
Wenn wir in der Zukunft einen Fehler erhalten, bezeichnen wir das Promise als abgelehnt.
Andernfalls ist das Promise immer noch in Bearbeitung und daher in einem ausstehenden Zustand.
Promises haben in der Regel die folgende Form:
Wie in dieser Vorlage gezeigt, verwenden Promises auch Callback-Funktionen.
Wir haben eine Callback-Funktion für die Methode then (), die ausgeführt wird, wenn ein Promise erfüllt wird.
Außerdem haben wir eine Callback-Funktion für die Methode catch (), um alle Fehler zu bearbeiten, die während der Ausführung des Promise auftreten.
Probieren wir Promise aus, indem wir unser Programm Studio Ghibli neu schreiben, sodass es stattdessen Promises verwendet.
Axios ist ein promise-basierter HTTP-Client für JavaScript, also installieren wir ihn gleich einmal:
Erstellen Sie nun mit dem Texteditor Ihrer Wahl die neue Datei promiseMovies.js:
Unser Programm erstellt eine HTTP-Anfrage mit axios und verwendet dann eine spezielle promise-basierte Version von fs zur Speicherung in einer neuen CSV-Datei.
Geben Sie diesen Code in promiseMovies.js ein, damit wir Axios laden und eine HTTP-Anfrage an die Film-API senden können:
In der ersten Zeile laden wir das Modul axios und speichern die zurückgegebene Funktion in einer Konstanten namens axios.
Dann verwenden wir die Methode axios.get (), um eine HTTP-Anfrage an die API zu senden.
Die Methode axios.get () gibt ein Promise zurück.
Verketten wir jetzt das Promise, damit wir die Liste der Ghibli-Filme zur Konsole drucken können:
Sehen wir uns genauer an, was passiert.
Nach einer HTTP-GET-Anfrage mit axios.get () verwenden wir die Funktion then (), die erst dann ausgeführt wird, wenn das Promise erfüllt ist.
In diesem Fall drucken wir die Filme auf den Bildschirm, so wie im Callback-Beispiel.
Fügen Sie zur Verbesserung dieses Programms den hervorgehobenen Code hinzu, um die HTTP-Daten in eine Datei zu schreiben:
Zusätzlich importieren wir das Modul fs erneut.
Sie werden sehen, dass wir nach dem Import von fs .Promises haben.
Node.js enthält eine promise-basierte Version der callback-basierten Bibliothek fs, sodass die Abwärtskompatibilität in Legacyprojekten nicht fehlerhaft ist.
Die erste then () -Funktion, die die HTTP-Anfrage verarbeitet, ruft nun fs.writeFile () auf, anstatt zur Konsole zu drucken.
Da wir die promise-basierte Version von fs importiert haben, gibt unsere Funktion writeFile () ein weiteres Promise zurück.
Daher fügen wir eine andere then () -Funktion hinzu für den Zeitpunkt, wenn das Promise writeFile () erfüllt ist.
Ein Promise kann ein neues Promise zurückgeben, sodass wir ein Promise nach dem anderen ausführen können.
Das ebnet uns den Weg für die Ausführung mehrerer synchroner Operationen.
Dies wird als Promise-Verkettung bezeichnet und ist analog zur Schachtelung von Callbacks.
Die zweite Funktion then () wird erst aufgerufen, nachdem wir erfolgreich in die Datei geschrieben haben.
< $> note Anmerkung: In diesem Beispiel haben wir den HTTP-Statuscode nicht wie im Callback-Beispiel überprüft.
Standardmäßig erfüllt axios sein Promise nicht, wenn es einen Statuscode erhält, der auf einen Fehler hindeutet.
Daher müssen wir es nicht mehr validieren. < $>
Um dieses Programm abzuschließen, verketten Sie das Promise mit einer catch () -Funktion, wie im Folgenden hervorgehoben:
Wenn ein Promise in der Kette von Promises nicht erfüllt ist, geht JavaScript automatisch zu der catch () -Funktion, wenn diese definiert wurde.
Aus diesem Grund haben wir nur eine catch () -Klausel, obwohl wir zwei asynchrone Operationen haben.
Bestätigen wir, dass unser Programm dieselbe Ausgabe erzeugt, indem wir Folgendes ausführen:
In Ihrem Ordner ghibliMovies sehen Sie die Datei promiseMovies.csv mit:
Mit Promises können wir viel präziseren Code als nur mit Callbacks schreiben.
Die Promise-Kette der Callbacks ist eine sauberere Option als die Schachtelung von Callbacks.
Wenn wir jedoch mehr asynchrone Calls vornehmen, wird unsere Promise-Kette länger und die Aufrechterhaltung schwieriger.
Die Ausführlichkeit der Callbacks und Promises kommt von der Notwendigkeit, Funktionen zu erstellen, wenn wir das Ergebnis einer asynchronen Aufgabe haben.
Besser wäre es, auf ein asynchrones Ergebnis zu warten und es in eine Variable außerhalb der Funktion zu legen.
Auf diese Weise können wir die Ergebnisse in den Variablen verwenden, ohne eine Funktion erstellen zu müssen.
Dies können wir mit den Schlüsselwörtern async und await erreichen.
JavaScript mit async / await schreiben
Die Schlüsselwörter async / await bieten eine alternative Syntax bei der Arbeit mit Promises.
Anstatt das Ergebnis eines Promise in der Methode then () zur Verfügung zu stellen, wird das Ergebnis als Wert wie in jeder anderen Funktion zurückgegeben.
Wir definieren eine Funktion mit dem Schlüsselwort async, um JavaScript mitzuteilen, dass es eine asynchrone Funktion ist, die ein Promise zurückgibt.
Wir verwenden das Schlüsselwort await, um JavaScript anzuweisen, die Ergebnisse des Promise zurückzugegeben, anstatt das Promise selbst zurückzugeben, wenn es erfüllt ist.
In der Regel sieht die Verwendung async / await folgendermaßen aus:
Sehen wir uns an, wie async / await unser Programm Studio Ghibli verbessern kann.
Verwenden Sie Ihren Texteditor, um eine neue Datei asyncAwaitMovies.js zu erstellen und zu öffnen:
Zuerst importieren Sie in Ihrer neu geöffneten JavaScript-Datei dieselben Module, die wir in unserem Promise-Beispiel verwendet haben:
Die Importe sind die gleichen wie promiseMovies.js, da async / await Promises verwendet.
Jetzt verwenden wir das Schlüsselwort async, um eine Funktion mit unserem asynchronen Code zu erstellen:
Wir erstellen eine neue Funktion namens saveMovies (), aber wir schließen async am Anfang seiner Definition ein.
Dies ist wichtig, da wir nur das Schlüsselwort await in einer asynchronen Funktion verwenden können.
Verwenden Sie das Schlüsselwort await, um eine HTTP-Anfrage vorzunehmen, die die Liste der Filme von der Ghibli-API erhält:
In unserer Funktion saveMovies () erstellen wir wie zuvor eine HTTP-Anfrage mit axios.get ().
Dieses Mal verbinden wir sie nicht mit einer then () -Funktion.
Stattdessen fügen wir await hinzu, bevor sie aufgerufen wird.
Wenn JavaScript await sieht, führt es den verbleibenden Code der Funktion erst aus, nachdem axios.get () die Ausführung beendet und die Variable response festgelegt hat.
Der andere Code speichert die Filmdaten, damit wir in eine Datei schreiben können.
Schreiben wir die Filmdaten in eine Datei:
Wir verwenden das Schlüsselwort await auch, wenn wir mit fs.writeFile () in die Datei schreiben.
Um diese Funktion abzuschließen, müssen wir Fehler erkennen, die unsere Promises auslösen können.
Dazu fügen wir unseren Code in einen try / catch-Block ein:
Da Promises fehlschlagen können, umhüllen wir unseren asynchronen Code mit einer try / catch-Klausel.
Dies erfasst alle Fehler, die bei Fehlschlagen der HTTP-Anfrage oder Dateischreiboperationen ausgelöst werden.
Schließlich rufen wir unsere asynchrone Funktion saveMovies () auf, damit sie ausgeführt wird, wenn wir das Programm mit node ausführen.
Auf einen Blick sieht dies wie ein typischer synchroner JavaScript-Codeblock aus.
Es werden weniger Funktionen durchlaufen, daher sieht es ein wenig ordentlicher aus.
Diese kleinen Änderungen erleichtern die Aufrechterhaltung von asynchronem Code mit async / await.
Testen Sie diese Iteration unseres Programms, indem Sie in Ihr Terminal Folgendes eingeben:
In Ihrem Ordner ghibliMovies wird eine neue Datei asyncAwaitMovies.csv mit dem folgenden Inhalt erstellt:
Sie haben nun die JavaScript-Funktionen async / await für die Verwaltung von asynchronem Code verwendet.
In diesem Tutorial haben Sie gelernt, wie JavaScript die Ausführung von Funktionen handhabt und asynchrone Operationen mit der Ereignisschleife verwaltet.
Dann haben Sie Programme geschrieben, die eine CSV-Datei erstellt haben, nachdem Sie mit verschiedenen asynchronen Programmierungstechniken eine HTTP-Anfrage für Filmdaten erstellt haben.
Zuerst haben Sie den überholten, auf Callback basierenden Ansatz verwendet.
Dann haben Sie Promises und schließlich async / await verwendet, um die Promise-Syntax kurz und bündig zu gestalten.
Mit Ihrem Verständnis für asynchronen Code mit Node.js können Sie nun Programme entwickeln, die von der asynchronen Programmierung profitieren, wie solche, die auf API-Calls angewiesen sind.
Sehen Sie sich diese Liste der öffentlichen APIs an.
Dazu müssen Sie asynchrone HTTP-Anfragen wie in diesem Tutorial erstellen.
Probieren Sie, eine App zu erstellen, die diese APIs verwendet, um die Techniken zu üben, die Sie hier gelernt haben.
Ersteinrichtung des Servers unter CentOS 8
3696
Bei der Erstellung eines neuen CentOS-8-Servers gibt es einige Konfigurationsschritte, die Sie im Rahmen des grundlegenden Setups frühzeitig durchführen sollten.
Dadurch erhöhen Sie die Sicherheit und Benutzerfreundlichkeit Ihres Servers und haben eine solide Grundlage für spätere Aktionen.
Schritt 1 - Anmeldung als Root
Für die Anmeldung bei Ihrem Server müssen Sie die öffentliche IP-Adresse Ihres Servers kennen.
Außerdem benötigen Sie das Passwort oder bei Installation eines SSH-Schlüssels zur Authentifizierung den privaten Schlüssel für das Konto des root-Benutzers.
Wenn Sie sich nicht bereits bei Ihrem Server angemeldet haben, können Sie der Anleitung in unserer Dokumentation So verbinden Sie sich mit Ihrem Droplet mittels SSH folgen, die diesen Prozess detailliert behandelt.
Wenn Sie nicht bereits mit Ihrem Server verbunden sind, melden Sie sich jetzt als root-Benutzer mit dem folgenden Befehl an (ersetzen Sie den hervorgehobenen Teil des Befehls mit der öffentlichen IP-Adresse Ihres Servers):
Akzeptieren Sie die Warnung über die Authentizität des Hosts, falls diese angezeigt wird.
Wenn Sie die Passwortauthentifizierung verwenden, geben Sie Ihr root-Passwort an, um sich anzumelden.
Wenn Sie einen SSH-Schlüssel verwenden, der passphrase-geschützt ist, werden Sie möglicherweise bei jeder Sitzung beim ersten Mal, wenn Sie den Schlüssel verwenden, zur Eingabe der Passphrase aufgefordert.
Wenn Sie sich das erste Mal mit einem Passwort beim Server anmelden, könnten Sie zur Änderung des root-Passworts aufgefordert werden.
Über Root
Der root-Benutzer ist der administrative Benutzer in einer Linux-Umgebung und verfügt über umfassende Berechtigungen.
Aufgrund der erweiterten Berechtigungen des Root-Kontos wird von einer regelmäßigen Verwendung abgeraten.
Das liegt daran, dass ein Teil der Leistung, die dem Root-Konto inhärent ist, die Fähigkeit ist, sehr destruktive Änderungen - selbst durch Zufall - zu verursachen.
Der nächste Schritt ist daher die Einrichtung eines alternativen Benutzerkontos mit einem reduzierten Einflussbereich für die alltägliche Arbeit.
Dieses Konto kann bei Bedarf immer noch vermehrte Berechtigungen erhalten.
Schritt 2 - Erstellung eines neuen Benutzers
Sobald Sie als root angemeldet sind, können Sie das neue Benutzerkonto erstellen, das wir ab jetzt zur Anmeldung verwenden werden.
Dieses Beispiel erstellt einen neuen Benutzer namens sammy, aber Sie sollten ihn durch einen beliebigen Benutzernamen ersetzen:
Legen Sie als Nächstes ein starkes Passwort für den Benutzer < ^ > sammy < ^ > fest:
Sie werden aufgefordert, das Passwort zweimal einzugeben.
Danach ist Ihr Benutzer bereit verwendet zu werden, aber zuerst geben wir diesem Benutzer zusätzliche Berechtigungen zur Verwendung des sudo-Befehls.
Dadurch können wir Befehle bei Bedarf als root ausführen.
Schritt 3 - Gewähren von Verwaltungsberechtigungen
Jetzt haben wir ein neues Benutzerkonto mit regulären Kontoberechtigungen.
Manchmal müssen wir jedoch administrative Aufgaben tätigen.
Damit wir uns nicht von unserem normalen Benutzer abmelden und beim root-Konto wieder anmelden müssen, können wir einen sogenannten "Superuser" oder root-Berechtigungen für unser normales Konto einrichten.
Dadurch kann unser normaler Benutzer Befehle mit administrativen Berechtigungen ausführen, indem das Wort sudo vor jeden Befehl gesetzt wird.
Um unserem neuen Benutzer diese Berechtigungen hinzuzufügen, müssen wir den neuen Benutzer der wheel-Gruppe hinzufügen.
Standardmäßig dürfen Benutzer, die zur wheel-Gruppe gehören, auf CentOS 8 den sudo-Befehl verwenden.
Führen Sie als root diesen Befehl aus, um Ihren neuen Benutzer der wheel-Gruppe hinzuzufügen (ersetzen Sie das hervorgehobene Wort mit Ihrem neuen Benutzernamen):
Wenn Sie als regulärer Benutzer angemeldet sind, können Sie jetzt sudo vor Befehlen eingeben, um Aktionen mit Superuser-Berechtigungen durchzuführen.
Schritt 4 - Einrichten einer grundlegenden Firewall
Firewalls gewähren ein Grundmaß an Sicherheit für Ihren Server.
Diese Anwendungen sind verantwortlich dafür, Datenverkehr über alle Ports auf Ihrem Server zu verweigern - außer für die Ports / Dienste, die Sie ausdrücklich genehmigt haben.
CentOS verfügt über einen Dienst namens firewalld, um diese Funktion auszuführen.
Firewall-Richtlinien von firewalld werden mithilfe eines Tools namens firewall-cmd konfiguriert.
< $> note Anmerkung: Wenn Ihre Server auf DigitalOcean ausgeführt werden, können Sie optional DigitalOcean Cloud Firewalls anstelle der UFW-Firewall verwenden.
Wir empfehlen, nicht mehr als eine Firewall zu verwenden, um widersprüchliche Regeln zu vermeiden, die schwierig zu debuggen sein könnten.
Installieren Sie zuerst firewalld:
Die Standardkonfiguration von firewalld ermöglicht ssh-Verbindungen, sodass wir die Firewall sofort aktivieren können:
Überprüfen Sie den Status des Dienstes, um sicherzustellen, dass er gestartet ist:
Beachten Sie, dass er sowohl active als auch enabled ist. Das bedeutet, dass er standardmäßig startet, wenn der Server neu gestartet wird.
Jetzt wo der Dienst ausgeführt wird, können wir mit dem Dienstprogramm firewall-cmd Richtlinieninformationen für die Firewall abrufen und festlegen.
Listen wir zuerst die Dienste auf, die bereits zugelassen sind:
Um die zusätzlichen Dienste zu sehen, die Sie anhand des Namens aktivieren können, geben Sie Folgendes ein:
Um einen Dienst hinzuzufügen, der zugelassen werden soll, verwenden Sie das Flag --add-service:
Dadurch fügen Sie den Dienst http hinzu und lassen eingehenden TCP-Verkehr auf Port 80 zu. Die Konfiguration wird nach dem Neustart der Firewall aktualisiert:
Denken Sie daran, dass Sie die Firewall (mit Diensten oder Ports) ausdrücklich für alle zusätzlichen Dienste öffnen müssen, die Sie eventuell später konfigurieren.
Schritt 5 - Aktivieren des externen Zugangs für Ihren regulären Benutzer
Jetzt wo wir einen regulären Benutzer ohne Rootberechtigung für die tägliche Verwendung haben, müssen wir sicherstellen, dass wir ihn für SSH in unseren Server verwenden können.
< $> note Anmerkung: Bis Sie verifiziert haben, dass Sie sich anmelden und sudo mit Ihrem neuen Benutzer verwenden können, empfehlen wir, als root angemeldet zu bleiben.
So können Sie als root eventuell auftretende Probleme beheben und alle erforderlichen Änderungen vornehmen.
Wenn Sie ein DigitalOcean Droplet verwenden und Probleme mit Ihrer root-SSH-Verbindung auftreten, können Sie sich mit der DigitalOcean Console beim Droplet anmelden.
Der Prozess der Konfiguration des SSH-Zugangs für Ihren neuen Benutzer hängt davon ab, ob das Root-Konto Ihres Servers ein Passwort oder einen SSH-Schlüssel zur Authentifizierung verwendet.
Wenn das Root-Konto Passwortauthentifizierung verwendet
Wenn Sie sich mit einem Passwort bei Ihrem Root-Konto angemeldet haben, ist die Passwortauthentifizierung für SSH aktiviert.
Sie können über SSH auf Ihr neues Konto zugreifen, indem Sie eine neue Terminal-Sitzung öffnen und SSH mit Ihrem neuen Benutzernamen verwenden.
Nach der Eingabe des Passworts Ihres regulären Benutzers werden Sie angemeldet.
Wenn Sie einen Befehl mit administrativen Berechtigungen ausführen müssen, denken Sie daran, davor sudo wie folgt einzugeben:
Sie werden bei jeder Sitzung zur Eingabe Ihres regulären Benutzerpassworts aufgefordert, wenn Sie sudo zum ersten Mal verwenden (und danach in regelmäßigen Abständen).
Um die Sicherheit Ihres Servers zu verbessern, empfehlen wir dringend die Einrichtung von SSH-Schlüsseln, anstatt die Passwortauthentifizierung zu verwenden.
Folgen Sie unserem Leitfaden zur Einrichtung von SSH-Schlüsseln unter CentOS 8, um mehr über die Konfiguration von schlüsselbasierter Authentifizierung zu erfahren.
Wenn das Root-Konto SSH-Schlüsselauthentifizierung verwendet
Wenn Sie sich mit SSH-Schlüsseln bei Ihrem Root-Konto angemeldet haben, ist die Passwortauthentifizierung für SSH deaktiviert.
Sie müssen eine Kopie Ihres öffentlichen Schlüssels in die Datei ~ / .ssh / authorized _ keys des neuen Benutzers aufnehmen, um sich erfolgreich anzumelden.
Da sich Ihr öffentlicher Schlüssel bereits in der Datei ~ / .ssh / authorized _ keys des root-Kontos auf dem Server befindet, können wir die Datei- und Verzeichnisstruktur in unser neues Benutzerkonto kopieren.
Die einfachste Möglichkeit, die Dateien mit richtigem Besitz und Berechtigungen zu kopieren, ist mit dem Befehl rsync.
Damit kopieren Sie das Verzeichnis .ssh des root-Benutzers, behalten die Berechtigungen bei und ändern die Dateibesitzer - und das alles mit einem einzigen Befehl.
Stellen Sie sicher, dass Sie die hervorgehobenen Teile des folgenden Befehls ändern, um dem Namen Ihres regulären Benutzers zu entsprechen:
< $> note Anmerkung: Der Befehl rsync behandelt Quellen und Ziele, die mit einem nachstehenden Schrägstrich enden, anders als jene ohne nachstehenden Schrägstrich.
Stellen Sie sicher, dass das Quellverzeichnis (~ / .ssh) keinen nachstehenden Schrägstrich enthält, wenn Sie nachfolgend rsync verwenden (vergewissern Sie sich, dass Sie nicht ~ / .ssh / verwenden).
Wenn Sie dem Befehl aus Versehen einen nachstehenden Schrägstrich hinzufügen, kopiert rsync den Inhalt des Verzeichnisses ~ / .ssh des root-Kontos in das Stammverzeichnis des sudo-Benutzers, anstatt die gesamte Verzeichnisstruktur ~ / .ssh zu kopieren.
Die Dateien befinden sich dann am falschen Ort und SSH wird sie nicht finden und verwenden können.
Wieder zurück im neuen Terminal auf ihrem lokalen Rechner, öffnen Sie jetzt eine neue SSH-Sitzung mit Ihrem Benutzer ohne Rootberechtigung:
Sie sollten beim neuen Benutzerkonto angemeldet werden, ohne ein Passwort zu verwenden.
Jetzt haben Sie eine solide Grundlage für Ihren Server.
Sie können nun jede benötigte Software auf Ihrem Server installieren.
Erstellen eines Hashicorp Vault Server unter Verwendung von Packer und Terraform auf DigitalOcean
3871
Vault von Hashicorp ist ein Open Source Tool, um Zugangsdaten und sensible Daten in dynamischen Cloud-Umgebungen sicher zu speichern.
Es bietet eine starke Datenverschlüsselung, identitätsbasierten Zugriff unter Verwendung von benutzerdefinierten Richtlinien, geheimes Leasen und Widerrufen sowie ein ausführliches Auditprotokoll, das fortlaufend aufgezeichnet wird.
Vault bietet zudem eine HTTP-API und ist damit die ideale Wahl für die Speicherung von Anmeldeinformationen in verstreuten serviceorientierten Implementierungen wie Kubernetes.
Packer und Terraform, ebenfalls von Hashicorp entwickelt, können zusammen verwendet werden, um Bilder von Vault zu kreieren und bereitzustellen.
Innerhalb dieses Workflows können Entwickler Packer verwenden, um unveränderliche Bilder für verschiedene Plattformen aus einer einzelnen Konfigurationsdatei zu erstellen, die angibt, was das Bild enthalten soll.
Terraform stellt dann so viele angepasste Exemplare der erstellten Bilder bereit, wie benötigt werden.
In diesem Tutorial erstellen Sie mit Packer einen unveränderlichen Snapshot des Systems mit installiertem Vault und orchestrieren dessen Bereitstellung mit Terraform.
Zum Schluss verfügen Sie über ein automatisiertes System zur Bereitstellung von Vault. So können Sie sich anstatt auf den zugrunde liegenden Installations- und Bereitstellungsprozess auf die Arbeit mit Vault selbst konzentrieren.
Auf Ihrem lokalen Rechner installierter Packer.
Lesen Sie für Anweisungen die Offizielle Dokumentation.
Auf Ihrem lokalen Rechner installiertes Terraform.
Lesen Sie die Offizielle Dokumentation für eine Anleitung.
Einen persönlichen Zugangs-Token (API-Schlüssel) mit Lese- und Schreibberechtigungen für Ihr DigitalOcean-Konto.
Um einen solchen zu erstellen, besuchen Sie Erstellen eines persönlichen Zugangs-Tokens bei den Dokumenten.
Einen SSH-Schlüssel, den Sie zur Authentifizierung mit den bereitgestellten Vault Droplets verwenden, die auf Ihrem lokalen Rechner verfügbar sind und Ihrem DigitalOcean-Konto hinzugefügt werden.
Sie benötigen auch dessen Fingerabdruck, den Sie von der Sicherheitsseite Ihres Kontos nach dem Erstellen kopieren können. Besuchen Sie die DigitalOcean Dokumentation für detaillierte Anweisungen oder das Tutorial Einrichten von SSH-Schlüsseln.
Schritt 1 - Erstellen einer Packer-Vorlage
In diesem Schritt schreiben Sie eine Packer-Konfigurationsdatei, die sogenannte Vorlage, die Packer anweist, wie ein Bild mit vorinstalliertem Vault erstellt werden kann.
Sie schreiben die Konfiguration im Format JSON, einem gängigen, visuell lesbaren Dateiformat.
Für die Zwecke dieses Tutorials speichern Sie alle Dateien unter ~ / vault-orchestration.
Erstellen Sie das Verzeichnis, indem Sie den folgenden Befehl ausführen:
Sie speichern die Konfigurationsdateien für Packer und Terraform separat in verschiedenen Unterverzeichnissen.
Erstellen Sie diese mit dem folgenden Befehl:
Da Sie zuerst mit Packer arbeiten, navigieren Sie in dessen Verzeichnis:
Verwenden von Vorlagenvariablen
Die Speicherung von privaten Daten und Zugangsdaten für Applikationen in einer separaten Variablendatei ist der ideale Weg, diese aus Ihrer Vorlage herauszuhalten.
Beim Erstellen des Bildes ersetzt Packer die angegebenen Variablen mit ihren Werten.
Die Hartkodierung von geheimen Datenwerten in Ihre Vorlage ist ein Sicherheitsrisiko, insbesondere wenn sie mit Teammitgliedern geteilt oder auf öffentlichen Websites wie GitHub veröffentlicht werden soll.
Sie speichern diese im Unterverzeichnis packer, in einer Datei namens variables.json ​ ​ ​ ​ ​ ​.
Erstellen Sie sie mit Ihrem bevorzugten Texteditor:
Die Variablendatei besteht aus einem JSON-Wörterbuch, das die Variablennamen ihren Werten zuordnet.
Sie verwenden diese Variablen in der Vorlage, die Sie erstellen.
Wenn Sie möchten, können Sie die Werte Basisbild, Region und Droplet-Größe entsprechend den Entwicklerdokumenten bearbeiten.
Denken Sie daran, < ^ > your _ do _ api _ key < ^ > durch Ihren API-Schlüssel zu ersetzen, den Sie als Teil der Voraussetzungen erstellt haben. Speichern und schließen Sie dann die Datei.
Erstellen von Buildern und Provisionern
Mit der fertigen Variablendatei erstellen Sie nun die Packer-Vorlage selbst.
Sie speichern die Packer-Vorlage für Vault in einer Datei namens template.json.
In der Vorlage definieren Sie Arrays von Buildern und Provisionern.
Builder weisen Packer an, wie das Systembild (je nach Typ) zu erstellen ist und wo es gespeichert werden soll, während Provisioner eine Reihe von Aktionen enthalten, die Packer auf dem System durchführen muss, bevor es in ein unveränderliches Bild umgewandelt wird, wie z. B. die Installation oder Konfiguration von Software.
Ohne Provisioner würden Sie zum Schluss ein unangetastetes Bild des Basissystems erhalten.
Sowohl Builder als auch Provisioner stellen Parameter zur weiteren Anpassung des Workflows zur Verfügung.
Sie definieren zuerst einen einzelnen Builder des Typs digitalocean, d. h. dass Packer beim Befehl zur Erstellung eines Bildes die bereitgestellten Parameter verwendet, um mit dem bereitgestellten API-Schlüssel ein temporäres Droplet der definierten Größe zu erstellen, mit dem angegebenen Basissystembild und der angegebenen Region.
Das Format zum Abrufen einer Variablen ist {{user '< ^ > variable _ name < ^ >'}}, wobei der hervorgehobene Teil der Name ist.
Wenn das provisorische Droplet bereitgestellt ist, stellt der Provisioner eine Verbindung über SSH mit dem angegebenen Benutzernamen her und führt nacheinander alle definierten Provisioner aus, bevor ein DigitalOcean Snapshot aus dem Droplet erstellt und es gelöscht wird.
Es handelt sich um eine Shell, die die angegebenen Befehle auf dem Ziel ausführt.
Die Befehle können entweder inline als Array von Zeichenfolgen angegeben oder in getrennten Skriptdateien definiert werden, wenn das Einfügen in die Vorlage aufgrund der Größe zu umständlich wird.
Die Befehle in der Vorlage warten 30 Sekunden, bis das System hochgefahren ist. Anschließend wird Vault < ^ > 1.3.2 < ^ > heruntergeladen und entpackt.
Konsultieren Sie die Offizielle Vault Downloadseite und ersetzen Sie den Link in den Befehlen durch eine neuere Version für Linux, wenn verfügbar.
Wenn Sie fertig sind, speichern und schließen Sie die Datei.
Um die Gültigkeit Ihrer Vorlage zu überprüfen, führen Sie den folgenden Befehl aus:
Packer akzeptiert einen Pfad zur Variablendatei über das Argument -var-file.
Sollten Sie eine Fehlermeldung erhalten, gibt Packer genau an, wo dieser aufgetreten ist, sodass Sie diesen korrigieren können.
Nun haben Sie eine funktionierende Vorlage, die ein Bild mit installiertem Vault erzeugt, wobei Ihr API-Schlüssel und andere Parameter in einer separaten Datei definiert sind.
Jetzt können Sie Packer aufrufen und den Snapshot erstellen.
Schritt 2 - Erstellen des Snapshot
In diesem Schritt erstellen Sie einen DigitalOcean Snapshot aus Ihrer Vorlage mit dem Packerbefehl build.
Um Ihren Snapshot zu erstellen, führen Sie den folgenden Befehl aus:
Die Ausführung des Befehls nimmt etwas Zeit in Anspruch.
Sie sehen eine Menge Ausgabe, die so aussehen wird:
Packer protokolliert alle Schritte, die es beim Erstellen Ihrer Vorlage vorgenommen hat.
Die letzte Zeile enthält den Namen des Snapshots (wie packer-1581537927) und seine rot markierte ID in Klammern. Notieren Sie die ID Ihres Snapshots, da Sie diese im nächsten Schritt benötigen.
Falls der Build-Prozess durch Fehler in der API fehlschlägt, warten Sie einige Minuten und versuchen Sie es dann erneut.
Sie haben nun einen DigitalOcean Snapshot entsprechend Ihrer Vorlage erstellt.
Auf dem Snapshot ist Vault vorinstalliert und Sie können nun Droplets mit ihm als Systembild bereitstellen.
Im nächsten Schritt schreiben Sie die Terraform-Konfiguration zur Automatisierung solcher Bereitstellungen.
Schritt 3 - Schreiben der Terraform-Konfiguration
In diesem Schritt schreiben Sie die Terraform-Konfiguration zur Automatisierung der Droplet-Bereitstellungen des Snapshots, der den gerade mit Packer erstellten Vault enthält.
Bevor Sie die eigentliche Terraform-Konfiguration zur Bereitstellung von Vault aus dem zuvor erstellten Snapshot schreiben, müssen Sie hierzu zunächst den DigitalOcean-Provider konfigurieren. Navigieren Sie zum Terraform-Unterverzeichnis, indem Sie Folgendes ausführen:
Erstellen Sie dann eine Datei namens do-provider.tf, in der Sie den Provider speichern:
Diese Datei gibt Parametervariablen an und verleiht dem digitalocean-Provider einen API-Schlüssel.
Sie verwenden diese Variablen später in Ihrer Terraform-Vorlage, aber Sie müssen zuerst deren Werte festlegen.
Zu diesem Zweck unterstützt Terraform die Angabe der variablen Werte in einer Variablendefinitionsdatei, ähnlich wie bei Packer.
Der Dateiname muss entweder in .tfvars oder .tfvars.json enden.
Sie übergeben diese Datei später an Terraform mit dem Argument -var-file.
Erstellen Sie eine Variablendefinitionsdatei mit dem Namen definitions.tfvars mithilfe Ihres Texteditors:
Ersetzen Sie < ^ > your _ do _ api _ key < ^ >, < ^ > your _ ssh _ key _ fingerprint < ^ > und < ^ > your _ do _ snapshot _ id < ^ > jeweils mit dem API-Schlüssel Ihres Kontos, dem Fingerabdruck Ihres SSH-Schlüssels und der aus dem vorherigen Schritt erhaltenen Snapshot-ID.
Die Parameter do _ region und do _ size müssen die gleichen Werte wie in der Packer-Variablendatei haben.
Wenn Sie mehrere Exemplare gleichzeitig bereitstellen möchten, stellen Sie instance _ count auf den gewünschten Wert.
Danach speichern und schließen Sie die Datei.
Weitere Informationen über den DigitalOcean Terraform Provider finden Sie in den Offiziellen Dokumenten.
Sie speichern die Vault Snapshot-Bereitstellungskonfiguration in einer Datei namens deployment.tf, unter dem Verzeichnis terraform.
Hier definieren Sie eine einzelne Ressource des Typs digitalocean _ droplet namens vault.
Danach legen Sie seine Parameter entsprechend den Variablenwerten fest und fügen einen SSH-Schlüssel (mit seinem Fingerabdruck) von Ihrem DigitalOcean-Konto zur Droplet-Ressource hinzu.
Zum Schluss erfolgt das output der IP-Adressen aller neu erstellten Exemplare an die Konsole.
Bevor Sie die Bereitstellungskonfiguration einsetzen, müssen Sie das Verzeichnis als Terraform-Projekt initialisieren:
Bei der Initialisierung eines Verzeichnisses als Projekt liest Terraform die verfügbaren Konfigurationsdateien ein und lädt die als notwendig erachteten Plugins herunter, wie in der Ausgabe protokolliert.
Sie verfügen jetzt über die Terraform-Konfiguration, um Ihren Vault Snapshot bereitzustellen.
Sie können diese nun validieren und in einem Droplet bereitstellen.
Schritt 4 - Bereitstellen von Vault mit Terraform
In diesem Abschnitt verifizieren Sie die Terraform-Konfiguration mit dem Befehl validate.
Nach erfolgreicher Verifizierung wenden Sie sie mit apply an und stellen als Ergebnis ein Droplet bereit.
Führen Sie den folgenden Befehl aus, um die Gültigkeit Ihrer Konfiguration zu testen:
Führen Sie als Nächstes den Befehl plan aus, um zu sehen, wie Terraform sich verhält, um die Infrastruktur entsprechend Ihrer Konfiguration bereitzustellen:
Terraform akzeptiert eine Variablendefinitionsdatei mit dem Parameter -var-file.
Die Ausgabe wird ähnlich sein wie diese:
Das grüne + am Anfang der Zeile resource "digitalocean _ droplet" "vault" bedeutet, dass Terraform ein neues Droplet mit dem Namen vault erstellt, wobei es folgende Parameter verwendet.
Das ist korrekt und nun können Sie den Plan mit terraform apply ausführen:
Geben Sie bei Nachfrage yes ein.
Nach einigen Minuten ist die Bereitstellung durch das Droplet abgeschlossen und Sie sehen eine Ausgabe ähnlich wie diese:
In der Ausgabe protokolliert Terraform die ausgeführten Aktionen (in diesem Fall zur Erstellung eines Droplets) und zeigt am Ende seine öffentliche IP-Adresse an.
Diese nutzen Sie, um sich im nächsten Schritt mit Ihrem neuen Droplet zu verbinden.
Sie haben ein neues Droplet aus dem Snapshot mit Vault erstellt und können es nun verifizieren.
Schritt 5 - Verifizieren Ihres bereitgestellten Droplets
In diesem Schritt greifen Sie mit SSH auf Ihr neues Droplet zu und verifizieren, dass Vault korrekt installiert wurde.
Wenn Sie mit Windows arbeiten, können Sie Software wie Kitty oder Putty verwenden, um sich mit dem Droplet über einen SSH-Schlüssel zu verbinden.
Auf Linux- und macOS-Rechnern können Sie den bereits verfügbaren Befehl ssh verwenden, um eine Verbindung herzustellen:
Geben Sie bei Nachfrage yes ein.
Nach der Anmeldung starten Sie Vault, indem Sie Folgendes ausführen:
Sie sehen seine "Hilfe" -Ausgabe, die folgendermaßen aussieht:
Sie können die Verbindung durch Eingabe von exit beenden.
Sie haben nun verifiziert, dass Ihr neu bereitgestelltes Droplet aus dem von Ihnen erstellten Snapshot kreiert wurde, und dass Vault korrekt installiert ist.
Sie verfügen jetzt über ein automatisiertes System zur Bereitstellung von Hashicorp Vault auf DigitalOcean Droplets mit Unterstützung von Terraform und Packer.
Sie können nun so viele Vault-Server bereitstellen, wie Sie benötigen.
Um Vault zu verwenden, müssen Sie es initialisieren und weiter konfigurieren. Anweisungen hierzu finden Sie in den Offiziellen Dokumenten.
Weitere Tutorials zur Verwendung von Terraform finden Sie auf der Terraform Content Page.
Installieren von Linux, Nginx, MySQL, PHP (LEMP) Stack auf CentOS 8
3801
Der LEMP Software Stack ist eine Gruppe von Software, die zur Bedienung von dynamischen Webseiten und Webanwendungen verwendet werden kann, die in PHP geschrieben sind.
Der Name "LEMP" ist ein Akronym, das ein Linux-Betriebssystem mit einem Nginx Webserver (betont wie "Engine-X ") beschreibt.
Die Datenbankschicht in einem LEMP Stack ist typischerweise ein MySQL-Datenbankserver, aber vor der Einführung von CentOS 8 war MySQL in den Standard-CentOS-Repositories nicht verfügbar.
Aus diesem Grund wurde MariaDB, eine weiterentwickelte Kopie von MySQL, eine weit akzeptierte Alternative zu MySQL als Standard-Datenbanksystem für LEMP Stacks auf CentOS-Rechnern.
In diesem Leitfaden installieren Sie einen LEMP Stack auf einem CentOS-8-Server.
Obwohl MySQL von den Standard-Repositories in CentOS 8 verfügbar ist, beschreibt dieser Leitfaden den Prozess der Einrichtung eines LEMP Stacks mit MariaDB als Datenbank-Managementsystem.
Schritt 1 - Installieren des Nginx Web Servers
Um den Besuchern Ihrer Website die Webseiten anzuzeigen, stellen Sie den leistungsfähigen Webserver Nginx zur Verfügung.
Installieren Sie das nginx Paket mit:
Geben Sie bei Nachfrage y ein, um zu bestätigen, dass Sie nginx installieren möchten.
Wenn Sie die Firewall firewalld gemäß unserem Leitfaden zur Ersteinrichtung des Servers aktiviert haben, müssen Sie die Verbindungen zu Nginx erlauben.
Geben Sie die Adresse ein, die Sie in Ihrem Webbrowser erhalten, und Sie werden zur Standard-Startseite von Nginx weitergeleitet:
Default Nginx Page CentOS 8
Jetzt ist Ihr Datenbanksystem eingerichtet und Sie können PHP installieren, den letzten Bestandteil des LEMP Stacks.
Schritt 3 - Installieren von PHP-FPM
Sie haben Nginx installiert, um Ihre Inhalte bereitzustellen, und MariaDB, um Ihre Daten zu speichern und zu verwalten. Jetzt können Sie PHP installieren, um Code zu verarbeiten und dynamische Inhalte für den Webserver zu generieren.
Während Apache den PHP-Interpreter in jede Anfrage einbindet, benötigt Nginx ein externes Programm, das die PHP-Verarbeitung durchführt und als Brücke zwischen dem PHP-Interpreter selbst und dem Webserver fungiert.
Dies ermöglicht eine bessere Gesamtleistung bei den meisten PHP-basierten Websites, erfordert jedoch eine zusätzliche Konfiguration.
Sie müssen php-fpm installieren, was für "PHP fastCGI-Prozessmanager" steht, und Nginx anweisen, PHP-Anfragen zur Verarbeitung an die Software zu übergeben.
Außerdem benötigen Sie php-mysqlnd, ein PHP-Modul, das PHP ermöglicht, mit MySQL-basierten Datenbanken zu kommunizieren.
Um php-fpm und php-mysql zu installieren, führen Sie Folgendes aus:
Wenn die Installation abgeschlossen ist, müssen Sie die Datei / etc / php-fpm.d / www.conf bearbeiten, um ein paar Einstellungen anzupassen.
Der in CentOS 8 integrierte Standard-Texteditor ist vi.vi ist ein extrem leistungsfähiger Texteditor, kann jedoch für Nutzer, die wenig Erfahrung mit ihm haben, etwas schwerfällig sein. Sie können bei Bedarf einen benutzerfreundlicheren Editor wie nano installieren, um die Bearbeitung von Konfigurationsdateien auf Ihrem CentOS-8-Server zu erleichtern:
Öffnen Sie dann die / etc / php-fpm.d / www.conf Konfigurationsdatei mit nano oder Ihrem bevorzugten Textbearbeitungsprogramm:
Suchen Sie nun die Direktiven für user und group.
Wenn Sie nano verwenden, können Sie STRG + W drücken, um nach diesen Begriffen in der offenen Datei zu suchen.
Sie werden sehen, dass sowohl die Variablen für user als auch group auf apache gestellt sind.
Das müssen Sie auf nginx ändern:
Wenn Sie nano verwenden, drücken Sie STRG + X, dann Y und ENTER.
Zur Aktivierung und zum Start des Services php-fpm führen Sie Folgendes aus:
Starten Sie zum Schluss den Nginx Webserver neu, damit er die Konfigurationsdateien lädt, die durch die Installation von php-fpm erstellt wurden:
Schritt 4 - Testen von PHP mit Nginx
In CentOS 8 erstellt die php-fpm Standardinstallation automatisch Konfigurationsdateien, die Ihrem Nginx Webserver erlauben, .php Dateien im Standard-Dokumentverzeichnis (der document root) zu bearbeiten, die sich unter / usr / share / nginx / html befindet.
Sie müssen keine Änderungen der Nginx-Konfiguration vornehmen, damit PHP korrekt in Ihrem Webserver funktioniert.
Ändern Sie lediglich die Standardeinstellungen für Berechtigungen in Ihrem Dokumentverzeichnis in Nginx.
Der folgende Befehl vergibt das Eigentumsrecht des Standard-Nginx-Dokumentverzeichnisses an einen Benutzer und eine Gruppe namens < ^ > sammy < ^ >. Achten Sie darauf, den hervorgehobenen Benutzernamen und die Gruppe in diesem Befehl zu ersetzen, um den Benutzernamen und die Gruppe Ihres Systems wiederzugeben.
Erstellen Sie eine neue PHP-Datei namens info.php im Verzeichnis / usr / share / nginx / html:
CentOS 8 Standard PHP Info
Mit diesem Leitfaden haben Sie eine flexible Grundlage für die Bereitstellung von PHP-Websites und -Anwendungen für Ihre Besucher unter Verwendung von Nginx als Webserver erstellt.
Sie haben Nginx eingerichtet, um PHP-Anfragen über php-fpm zu bearbeiten, sowie eine MariaDB-Datenbank zur Speicherung Ihrer Website-Daten erstellt.
Installieren und Verwenden von ClickHouse auf Debian 10
3268
ClickHouse ist eine Open-Source-basierte, spaltenorientierte Analysedatenbank, die von Yandex für OLAP- und Big-Data-Anwendungsfälle erstellt wurde.
Unterstützung von ClickHouse für eine echtzeitbasierte Verarbeitung von Abfragen sorgt dafür, dass sich ClickHouse für Anwendungen eignet, die in Sekundenbruchteilen Ergebnisse erfordern.
Die Abfragesprache von ClickHouse ist ein SQL-Dialekt, der leistungsfähige deklarative Abfragefunktionen bietet, während Endbenutzer von Vertrautheit und einer flacheren Lernkurve profitieren.
Spaltenorientierte Datenbanken speichern Einträge in nach Spalten (und nicht nach Zeilen) gruppierten Blöcken.
Da Daten für Spalten, die in der Abfrage fehlen, nicht geladen werden, brauchen spaltenorientierte Datenbanken beim Ausführen von Abfragen weniger Zeit zum Lesen von Daten.
Dadurch können diese Datenbanken bei bestimmten Workloads (wie z.B. OLAP) Ergebnisse wesentlich schneller berechnen als herkömmliche zeilenbasierte Systeme.
Online Analytics Processing (OLAP) -Systeme ermöglichen es, große Mengen von Daten zu organisieren und komplexe Abfragen durchzuführen.
Sie können Petabytes von Daten verwalten und Abfrageergebnisse schnell zurückgeben.
Somit eignet sich OLAP gut für Bereiche wie Datenwissenschaft und geschäftliche Analysen.
In diesem Tutorial installieren Sie den ClickHouse-Datenbankserver sowie den Client auf Ihrem Rechner.
Sie verwenden das DBMS für typische Aufgaben und aktivieren optional den Remotezugriff von einem anderen Server, damit Sie sich von einem anderen Rechner aus mit der Datenbank verbinden können.
Dann testen Sie ClickHouse, indem Sie Beispieldaten von Websitebesuchen modellieren und abfragen.
Ein Debian 10-Betriebssystem mit einem sudo-aktivierten non-root user und mit eingerichteter Firewall.
Folgen Sie dem Tutorial zur Ersteinrichtung des Servers, um den Benutzer zu erstellen und die Firewall einzurichten.
(Optional) Ein sekundäres Debian 10-Betriebssystem mit einem sudo-aktivierten non-root user und mit eingerichteter Firewall.
Folgen Sie dem Tutorial zur Ersteinrichtung des Servers.
Schritt 1 - Installieren von ClickHouse
In diesem Abschnitt installieren Sie die ClickHouse-Server- und Client-Programme mit apt.
Stellen Sie zunächst eine SSH-Verbindung mit Ihrem Server her durch Ausführung von:
dirmngr ist ein Server für die Verwaltung von Zertifikaten und Schlüsseln.
Er wird benötigt, um Remote-Repository-Schlüssel zu aktivieren und zu verifizieren. Installieren Sie den Server durch Ausführung von:
Yandex pflegt ein APT-Repository, das über die neueste Version von ClickHouse verfügt.
Fügen Sie den GPG-Schlüssel des Repository hinzu, damit Sie validierte ClickHouse-Pakete sicher herunterladen können:
Die Ausgabe bestätigt, dass die Verifizierung erfolgreich war und der Schlüssel hinzugefügt wurde.
Fügen Sie das Repository Ihrer Liste mit APT-Repositorys hinzu, indem Sie Folgendes ausführen:
Hier haben Sie die Ausgabe von echo an sudo tee weitergeleitet, damit sich diese Ausgabe in einer Datei mit Root-Besitz ausgeben lässt.
Führen Sie nun apt update aus, um Ihre Pakete zu aktualisieren:
Die Pakete clickhouse-server und clickhouse-client stehen nun für die Installation bereit.
Ab ClickHouse-Version 19.13.3 werden einige OpenSSL 1.1.1-Konfigurationen wie MinProtocol und CipherVersion nicht richtig gelesen.
Um das Problem der fehlenden Kompatibilität zu umgehen, ändern Sie die OpenSSL-Konfigurationsdatei und kommentieren Sie die Zeile ssl _ conf = ssl _ sect in / etc / ssl / openssl.cnf aus.
Bearbeiten Sie die Konfigurationsdatei, indem Sie Folgendes ausführen:
Kommentieren Sie dann die Zeile mit ssl _ conf = ssl _ sect aus, damit sie wie die folgende Datei aussieht:
Nachdem Sie die OpenSSL-Konfigurationsdatei gepatcht haben, können Sie nun die ClickHouse-Server- und Client-Pakete installieren.
Installieren Sie sie mit:
Bei der Installation werden Sie auch dazu aufgefordert, ein Passwort für den standardmäßigen ClickHouse-Benutzer festzulegen.
Sie haben den ClickHouse-Server und- Client erfolgreich installiert.
Sie können den Datenbankdienst nun starten und überprüfen, ob er richtig ausgeführt wird.
Schritt 2 - Starten des Dienstes
Das im vorherigen Abschnitt installierte Paket clickhouse-server erstellt einen systemd-Dienst, der Aktionen wie Starten, Anhalten und Neustart des Datenbankservers ausführt. systemd ist ein init-System für Linux, das der Initialisierung und Verwaltung von Diensten dient.
In diesem Abschnitt starten Sie den Dienst und überprüfen, ob er richtig ausgeführt wird.
Starten Sie den Dienst clickhouse-server, indem Sie Folgendes ausführen:
Der vorherige Befehl wird keine Ausgabe anzeigen.
Um zu überprüfen, ob der Dienst erfolgreich ausgeführt wird, führen Sie Folgendes aus:
Die Ausgabe gibt an, dass der Server ausgeführt wird.
Sie haben den ClickHouse-Server erfolgreich gestartet und können nun das CLI-Programm clickhouse-client verwenden, um eine Verbindung zum Server herzustellen.
Schritt 3 - Erstellen von Datenbanken und Tabellen
In ClickHouse können Sie Datenbanken erstellen und löschen, indem Sie SQL-Anweisungen direkt in der interaktiven Datenbankaufforderung ausführen.
Anweisungen bestehen aus Befehlen, die einer bestimmten Syntax folgen, und den Datenbankserver dazu bringen, eine angeforderte Operation mit allen erforderlichen Daten auszuführen.
Erstellen Sie Datenbanken, indem Sie die Syntax CREATE DATABASE < ^ > table _ name < ^ > verwenden.
Um eine Datenbank zu erstellen, starten Sie zunächst eine Client-Sitzung, indem Sie folgenden Befehl ausführen:
Sie werden dazu aufgefordert, das Passwort einzugeben, das Sie bei der Installation festgelegt haben - geben Sie es ein, um die Client-Sitzung erfolgreich zu starten.
Mit dem vorherigen Befehl können Sie sich in der Eingabeaufforderung des Clients anmelden, wo Sie ClickHouse SQL-Anweisungen ausführen können, um beispielsweise folgende Aktionen auszuführen:
Erstellen, Aktualisieren und Löschen von Datenbanken, Tabellen, Indizes, Partitionen und Ansichten.
Ausführen von Abfragen zum Abrufen von Daten, die optional mit verschiedenen Bedingungen gefiltert und gruppiert werden.
In diesem Schritt erstellen Sie mit dem ClickHouse-Client, der bereit für die Eingabe von Daten ist, eine Datenbank und eine Tabelle.
Für die Zwecke dieses Tutorials erstellen Sie eine Datenbank namens < ^ > test < ^ > und erzeugen darin eine Tabelle namens < ^ > visits < ^ >, die die Länge von Websitebesuchen verfolgt.
Da Sie sich in der Eingabeaufforderung von ClickHouse befinden, erstellen Sie nun Ihre Datenbank < ^ > test < ^ > durch Ausführung von:
Sie sehen die folgende Ausgabe, die zeigt, dass Sie die Datenbank erstellt haben:
Eine ClickHouse-Tabelle ähnelt Tabellen in anderen relationalen Datenbanken; sie enthält eine Sammlung verwandter Daten in einem strukturierten Format.
Sie können Spalten zusammen mit ihren Typen festlegen, Datenzeilen hinzufügen und verschiedene Arten von Abfragen in Tabellen ausführen.
Die Syntax zur Erstellung von Tabellen in ClickHouse folgt dieser Beispielstruktur:
Die Werte table _ name und column _ name können beliebige gültige ASCII-Bezeichner sein.
ClickHouse unterstützt eine breite Palette von Spaltentypen; einige der beliebtesten sind:
UInt64: dient der Speicherung von ganzzahligen Werten im Bereich 0 bis 18446744073709551615.
Float64: dient der Speicherung von Gleitkommazahlen wie 2039,23, 10,5 etc.
String: dient der Speicherung von Zeichen mit variabler Länge.
Der Wert benötigt kein Attribut für maximale Länge, da er beliebige Längen speichern kann.
Date: dient der Speicherung von Daten, die dem Format YYYY-MM-DD folgen.
DateTime: dient der Speicherung von Daten, die die Uhrzeit umfassen und dem Format YYYY-MM-DD HH: MM: SS folgen.
Nach den Spaltendefinitionen geben Sie die für die Tabelle verwendete Engine an.
In ClickHouse bestimmen Engines über die physikalische Struktur der zugrunde liegenden Daten, die Abfragefunktionen der Tabelle, die gleichzeitigen Zugriffsmodi und die Unterstützung von Indizes.
Verschiedene Engine-Typen eignen sich für verschiedene Anwendungsanforderungen.
Der am häufigsten verwendete und weit verbreitete Engine-Typ ist MergeTree.
Nachdem Sie sich einen Überblick über die Tabellenerstellung verschafft haben, erstellen Sie nun eine Tabelle.
Beginnen Sie mit der Auswahl der Datenbank, die Sie ändern möchten:
Sie sehen die folgende Ausgabe, die zeigt, dass Sie von der Datenbank default zur Datenbank < ^ > test < ^ > gewechselt sind:
Im Rest dieses Leitfadens wird davon ausgegangen, dass Sie Anweisungen im Kontext dieser Datenbank ausführen.
Erstellen Sie Ihre Tabelle < ^ > visits < ^ >, indem Sie den folgenden Befehl ausführen:
Hier ist eine Aufstellung dessen, was der Befehl tut.
Sie erstellen eine Tabelle namens < ^ > visits < ^ >, die vier Spalten aufweist:
id: Die Primärschlüsselspalte.
Ähnlich wie bei anderen RDBMS-Systemen identifiziert eine Primärschlüsselspalte in ClickHouse eine Zeile eindeutig; jede Zeile muss einen eindeutigen Wert für diese Spalte aufweisen.
duration: Eine Gleitkommaspalte zur Speicherung der Dauer einzelner Besuche (in Sekunden). float-Spalten können Dezimalwerte wie 12,50 speichern.
url: Eine Zeichnfolgenspalte, die die besuchte URL speichert, wie z.B. http: / / example.com.
created: Eine Datums- und Uhrzeitspalte, die vefolgt, wann der Besuch stattgefunden hat.
Nach den Spaltendefinitionen geben Sie MergeTree als Speicher-Engine für die Tabelle an.
Die MergeTree-Familie von Engines wird aufgrund ihrer optimierten Unterstützung für große echtzeitbasierte Einfügungen, die allgemeine Stabilität sowie Unterstützung von Abfragen für Datenbanken empfohlen.
Außerdem unterstützen MergeTree-Engines eine Sortierung von Zeilen nach Primärschlüssel, Partitionierung von Zeilen sowie Replikation und Abfrage von Daten.
Wenn Sie ClickHouse zur Archivierung von Daten, die nicht oft abgefragt werden, oder zur Speicherung von temporären Daten verwenden möchten, können Sie die Log-Familie von Engines verwenden, um eine Optimierung für diesen Anwendungsfall vorzunehmen.
Nach den Spaltendefinitionen definieren Sie andere Optionen auf der Tabellenebene.
Die Klausel PRIMARY KEY legt id als Primärschlüsselspalte fest; die Klausel ORDER BY speichert anhand der Spalte id sortierte Werte.
Ein Primärschlüssel identifiziert eine Zeile eindeutig und dient zum effizienten Zugreifen auf eine einzelne Zeile und ein effizientes Zuordnen von Zeilen.
Bei Ausführung der Anweisung create sehen Sie folgende Ausgabe:
In diesem Abschnitt haben Sie eine Datenbank und eine Tabelle erstellt, um Daten zu Websitebesuchen zu verfolgen. Im nächsten Schritt fügen Sie Daten in die Tabelle ein, aktualisieren vorhandene Daten und löschen diese Daten.
Schritt 4 - Einfügen, Aktualisieren und Löschen von Daten und Spalten
In diesem Schritt verwenden Sie Ihre Tabelle namens < ^ > visits < ^ >, um Daten einzufügen, zu aktualisieren und zu löschen. Folgender Befehl ist ein Beispiel für die Syntax zur Eingabe von Zeilen in eine ClickHouse-Tabelle:
Fügen Sie nun einige Zeilen von Beispieldaten zu Websitebesuchen in Ihre Tabelle < ^ > visits < ^ > ein, indem Sie nacheinander die folgenden Anweisungen ausführen:
Sie sehen die folgende Ausgabe, die für jede Einfügeanweisung wiederholt wird.
Die Ausgabe für die einzelnen Zeilen zeigt, dass Sie sie erfolgreich in die Tabelle < ^ > visits < ^ > eingefügt haben.
Nun fügen Sie der Tabelle < ^ > visits < ^ > eine zusätzliche Spalte hinzu.
Wenn Sie Spalten von bestehenden Tabellen hinzufügen oder löschen, unterstützt ClickHouse die Syntax ALTER.
Die grundlegende Syntax für das Hinzufügen einer Spalte zu einer Tabelle lautet beispielsweise:
Fügen Sie eine Spalte namens < ^ > location < ^ > hinzu, in der der Speicherort der Besuche auf einer Website gespeichert wird. Führen Sie dazu folgende Anweisung aus:
Die Ausgabe zeigt, dass Sie die Spalte < ^ > location < ^ > erfolgreich hinzugefügt haben.
Ab Version 19.13.3 unterstützt ClickHouse aufgrund von Implementierungsbeschränkungen nicht mehr das Aktualisieren und Löschen einzelner Datenzeilen.
ClickHouse unterstützt jedoch Massenaktualisierungen und -löschvorgänge und bietet eine eigene SQL-Syntax für diese Operationen, um ihre nicht standardmäßige Nutzung hervorzuheben.
Die folgende Syntax ist ein Beispiel für eine Massenaktualisierung von Zeilen:
Sie führen die folgende Anweisung aus, um die Spalte url aller Zeilen zu aktualisieren, die eine duration (Dauer) von weniger als 15 aufweisen. Geben Sie sie zur Ausführung in die Datenbankaufforderung ein:
Die Ausgabe der Anweisung zur Massenaktualisierung wird wie folgt aussehen:
Die Ausgabe zeigt, dass Ihre Aktualisierungsanfrage erfolgreich abgeschlossen wurde.
0 rows in set in der Ausgabe weist darauf hin, dass die Abfrage keine Zeilen zurückgegeben hat; dies wird bei allen Aktualisierungs- und Löschabfragen der Fall sein.
Die Beispielsyntax für das Massenlöschen von Zeilen ähnelt der Aktualisierung von Zeilen und weist die folgende Struktur auf:
Um das Löschen von Daten zu testen, führen Sie folgende Anweisung aus, um alle Zeilen zu entfernen, die eine duration (Dauer) von weniger als 5 haben:
Die Ausgabe der Anweisung zur Massenlöschung wird in etwa folgendermaßen aussehen:
Die Ausgabe bestätigt, dass Sie die Zeilen mit einer Dauer von weniger als fünf Sekunden gelöscht haben.
Um Spalten aus Ihrer Tabelle zu löschen, würde die Syntax dieser Beispielstruktur folgen:
Löschen Sie die zuvor hinzugefügte Spalte location durch Ausführung von Folgendem:
Die Ausgabe von DROP COLUMN, die bestätigt, dass die Spalte gelöscht wurde, sieht wie folgt aus:
Nachdem Sie Zeilen und Spalten in Ihrer Tabelle < ^ > visits < ^ > erfolgreich eingefügt, aktualisiert und gelöscht haben, fahren Sie nun im nächsten Schritt mit der Abfrage von Daten fort.
Die Abfragesprache von ClickHouse ist ein benutzerdefinierter SQL-Dialekt mit Erweiterungen und Funktionen, die für Analytics-Workloads geeignet sind.
In diesem Schritt führen Sie Auswahl- und Aggregationsanfragen zum Abrufen von Daten und Ergebnissen aus Ihrer Tabelle namens < ^ > visits < ^ > aus.
Auswahlabfragen ermöglichen es Ihnen, Zeilen und Spalten mit Daten, die nach den von Ihnen angegebenen Bedingungen gefiltert werden, zusammen mit Optionen wie Anzahl der zurückzugebenden Zeilen abzurufen.
Mit der Syntax SELECT können Sie Zeilen und Spalten mit Daten auswählen.
Die grundlegende Syntax für SELECT-Abfragen lautet:
Führen Sie die folgende Anweisung aus, um url- und duration-Werte für Zeilen abzurufen, bei denen die URL http: / / example.com lautet.
Die Ausgabe hat zwei Zeilen zurückgegeben, die den von Ihnen angegebenen Bedingungen entsprechen.
Nachdem Sie Werte ausgewählt haben, können Sie nun mit der Ausführung von Aggregationsabfragen fortfahren.
Aggregationsabfragen stellen Abfragen dar, die auf einen Satz von Werten angewendet werden und einzelne Ausgabewerte zurückgeben.
In Analysedatenbanken werden diese Abfragen häufig ausgeführt und von der Datenbank umfassend optimiert.
Zu den Aggregatfunktionen, die von ClickHouse unterstützt werden, gehören:
count: gibt die Zahl der Zeilen zurück, die den angegebenen Bedingungen entsprechen.
sum: gibt die Summe der ausgewählten Spaltenwerte zurück.
avg: gibt den Durchschnitt der ausgewählten Spaltenwerte zurück.
Zu ClickHouse-spezifischen Aggregatfunktionen gehören:
uniq: gibt eine ungefähre Zahl eindeutiger Zeilen zurück, die abgeglichen wurden.
topK: gibt mit einem Approximationsalgorithmus ein Array der häufigsten Werte einer bestimmten Spalte zurück.
Um die Ausführung von Aggregationsabfragen zu testen, berechnen Sie die Gesamtdauer von Besuchen durch Ausführung der Abfrage sum:
Berechnen Sie nun die führenden zwei URLs durch Ausführung von:
Nachdem Sie Ihre Tabelle < ^ > visits < ^ > erfolgreich abgefragt haben, löschen Sie nun im nächsten Schritt Tabellen und Datenbanken.
Schritt 6 - Löschen von Tabellen und Datenbanken
In diesem Abschnitt löschen Sie die Tabelle < ^ > visits < ^ > sowie die Datenbank < ^ > test < ^ >.
Die Syntax zur Löschung von Tabellen folgt diesem Beispiel:
Um die Tabelle < ^ > visits < ^ > zu löschen, führen Sie folgende Anweisung aus:
Die folgende Ausgabe zeigt an, dass die Tabelle erfolgreich gelöscht wurde:
Mit der Syntax DROP database < ^ > table _ name < ^ > können Sie Datenbanken löschen.
Um die Datenbank namens < ^ > test < ^ > zu löschen, führen Sie folgende Anweisung aus:
Die resultierende Ausgabe zeigt, dass die Datenbank erfolgreich gelöscht wurde.
In diesem Schritt haben Sie Tabellen und Datenbanken gelöscht.
Nachdem Sie Datenbanken, Tabellen und Daten in Ihrer ClickHouse-Instanz erstellt, aktualisiert und gelöscht haben, aktivieren Sie nun im nächsten Abschnitt den Remotezugriff auf Ihren Datenbankserver.
Schritt 7 - Einrichten von Firewall-Regeln (optional)
Wenn Sie ClickHouse nur lokal verwenden möchten (mit auf dem gleichen Server ausgeführten Anwendungen) oder auf Ihrem Server keine Firewall aktiviert haben, müssen Sie diesen Abschnitt nicht beachten.
Wenn Sie hingegen eine Remoteverbindung mit dem ClickHouse-Datenbankserver herstellen möchten, sollten Sie diesen Schritt durchführen.
Auf Ihrem Server ist derzeit eine Firewall aktiviert, die es Ihrer öffentlichen IP-Adresse verbietet, auf alle Ports zuzugreifen.
Führen Sie die folgenden beiden Schritte durch, um Remotezugriff zu aktivieren:
Ändern Sie die Konfiguration von ClickHouse und erlauben Sie es der Software, alle Schnittstellen abzuhören.
Fügen Sie eine Firewall-Regel hinzu, die eingehende Verbindungen zu Port 8123 zulässt. Dabei handelt es sich um den HTTP-Port, an dem der ClickHouse-Server ausgeführt wird.
Wenn Sie sich in der Datenbankaufforderung befinden, beenden Sie sie durch Eingabe von Strg + D.
Heben Sie dann die Auskommentierung der Zeile mit <! -- < listen _ host > 0.0.0.0 < / listen _ host > -- > auf, wie in der folgenden Datei dargestellt:
Speichern Sie die Datei und schließen Sie sie.
Starten Sie zum Anwenden der neuen Konfiguration den Dienst neu, indem Sie Folgendes ausführen:
Bei diesem Befehl werden Sie keine Ausgabe sehen.
Der ClickHouse-Server lauscht an Port 8123 nach HTTP-Verbindungen und an Port 9000 nach Verbindungen von clickhouse-client.
Aktivieren Sie mit dem folgenden Befehl Zugriff an beiden Ports für die IP-Adresse Ihres zweiten Servers:
Für beide Befehle wird die folgende Ausgabe angezeigt, die bestätigt, dass Sie den Zugriff auf beide Ports aktiviert haben:
ClickHouse wird nun über die von Ihnen hinzugefügte IP-Adresse zugänglich sein.
Fügen Sie bei Bedarf zusätzliche IP-Adressen hinzu, z. B. die Adresse Ihres lokalen Rechners.
Um zu überprüfen, ob Sie sich über den Remotecomputer mit dem ClickHouse-Server verbinden können, führen Sie zunächst die Schritte in Schritt 1 dieses Tutorials auf dem zweiten Server aus und stellen Sie sicher, dass Sie den clickhouse-client installiert haben.
Nachdem Sie sich beim zweiten Server angemeldet haben, starten Sie nun eine Client-Sitzung, indem Sie Folgendes ausführen:
Ihnen wird die folgende Ausgabe angezeigt, die bestätigt, dass Sie erfolgreich eine Verbindung zum Server hergestellt haben:
In diesem Schritt haben Sie Remotezugriff auf Ihren ClickHouse-Datenbankserver aktiviert, indem Sie Ihre Firewall-Regeln angepasst haben.
Sie haben auf Ihrem Server erfolgreich eine ClickHouse-Datenbank eingerichtet sowie eine Datenbank und Tabelle erstellt, Daten hinzugefügt, Abfragen ausgeführt und die Datenbank gelöscht.
In der Dokumentation von ClickHouse können Sie mehr über die Benchmarks der Software im Vergleich zu anderen Open-Source- und kommerziellen Analysedatenbanken erfahren sowie allgemeine Referenzdokumente lesen.
Zu weiteren Funktionen von ClickHouse gehört eine auf mehrere Server verteilte Abfrageverarbeitung, die für mehr Leistung und Schutz vor Datenverlusten sorgt, indem Daten in verschiedenen Shards gespeichert werden.
Migrieren von Redis-Daten mit Replikation unter Ubuntu 18.04
3213
Redis ist eine In-Memory-Datenbank mit einer Schlüssel-Werte-Datenstruktur, die für ihre Flexibilität, Leistung, breite Sprachunterstützung und integrierte Funktionen wie Replikation bekannt ist.
Replikation besteht aus der Praxis, Daten regelmäßig aus einer Datenbank in eine andere zu kopieren, um über ein Replikat zu verfügen, das immer ein exaktes Duplikat der Primärinstanz bleibt.
Ein häufiges Anwendungsgebiet der Redis-Replikation besteht darin, einen bestehenden Redis-Datenspeicher auf einen neuen Server zu migrieren, wie man es ggf. bei der Skalierung von Infrastruktur für bessere Leistung tut.
Dieses Tutorial beschreibt den Prozess zur Verwendung der integrierten Replikationsfunktionen von Redis, um Daten von einem Ubuntu 18.04-Server (die "Quelle ") auf einen anderen Server (das" Ziel ") zu migrieren.
Dabei werden für jeden Server einige Konfigurationsänderungen vorgenommen, wobei der Zielserver als Replikat des Quellservers eingerichtet und das Replikat anschließend nach Abschluss der Migration zurück zu einem primären Server hochgestuft wird.
Zwei Server, auf denen Ubuntu 18.04 ausgeführt wird.
Für jeden Server sollten ein Benutzer mit Administratorberechtigungen und mit ufw eine Firewall eingerichtet werden.
Zum Einrichten dieser Umgebung folgen Sie für beide Server unserem Tutorial Ersteinrichtung eines Servers für Ubuntu 18.04.
Die neueste Version von Redis wird auf beiden Servern installiert.
Folgen Sie zum Einrichten unserem Leitfaden Installieren von Redis von der Quelle unter Ubuntu 18.04.
Schritt 1 - (optional) Laden Ihrer Redis-Quellinstanz mit Beispieldaten
Dieser optionale Schritt beinhaltet das Laden Ihrer Redis-Quellinstanz mit einigen Beispieldaten, damit Sie mit der Migration von Daten zu Ihrer Zielinstanz experimentieren können.
Wenn Sie bereits über Daten verfügen, die Sie zu Ihrer Zielinstanz migrieren möchten, können Sie mit Schritt 2 fortfahren, wo es um ihre Sicherung geht.
Stellen Sie zunächst als non-root user eine Verbindung mit dem Ubuntu-Server her, den Sie als Ihre Redis-Quellinstanz verwenden werden:
Führen Sie dann den folgenden Befehl aus, um auf Ihren Redis-Server zuzugreifen:
Wenn Sie Ihren Redis-Server so konfiguriert haben, dass eine Authentifizierung mit Passwort vorgeschrieben ist, führen Sie den Befehl auth gefolgt von Ihrem Redis-Kennwort aus:
Führen Sie als Nächstes folgende Befehle aus.
Dadurch werden verschiedene Schlüssel erstellt, die einige Zeichenfolgen, ein Hash, eine Liste und ein Set enthalten:
Führen Sie zusätzlich folgende expire-Befehle aus, um einige dieser Schlüssel mit einem Timeout bereitzustellen.
Dadurch werden sie flüchtig, was bedeutet, dass Redis sie nach einer festgelegten Zeit (in diesem Fall nach 7500 Sekunden) löschen wird.
Somit verfügen Sie über einige Beispieldaten, die Sie in Ihre Redis-Zielinstanz exportieren können.
Behalten Sie die Eingabeaufforderung redis-cli vorerst geöffnet, da wir im nächsten Schritt einige weitere Befehle darüber ausführen, um diese Daten zu sichern.
Schritt 2 - Sichern Ihrer Redis-Quellinstanz
Jedes Mal, wenn Sie planen, Daten von einem Server auf einen anderen zu verschieben, besteht die Gefahr, dass etwas schiefgeht und Sie dadurch Daten verlieren.
Obwohl dieses Risiko gering ist, verwenden wir den Befehl bgsave von Redis zur Erstellung eines Backups Ihrer Redis-Quelldatenbank, sollte bei der Replikation ein Fehler auftreten.
Wenn Sie sie nicht bereits geöffnet haben, öffnen Sie zunächst die Redis-Befehlszeilenschnittstelle:
Wenn Sie Ihren Redis-Server so konfiguriert haben, dass eine Authentifizierung mit Passwort vorgeschrieben ist, führen Sie außerdem den Befehl auth gefolgt von Ihrem Redis-Passwort aus:
Führen Sie als Nächstes den Befehl bgsave aus.
Dadurch wird ein Snapshot Ihres aktuellen Datasets erstellt und in eine Dumpdatei exportiert, die im Arbeitsverzeichnis von Redis enthalten ist:
< $> note Anmerkung: Sie können mit den Befehlen save oder bgsave einen Snapshot Ihrer Redis-Datenbank erstellen.
Wir verwenden hier jedoch den Befehl bgsave, da der Befehl save synchron ausgeführt wird, d.h. er blockiert alle anderen Clients, die mit der Datenbank verbunden sind.
Aus diesem Grund wird in der Dokumentation zum Befehl save empfohlen, ihn praktisch nie in einer Produktionsumgebung auszuführen.
Stattdessen wird die Verwendung des Befehls bgsave empfohlen, der asynchron ausgeführt wird.
Dadurch wird Redis die Datenbank in zwei Prozesse forken: Der übergeordnete Prozess wird weiterhin Clients bedienen, während der untergeordnete Prozess die Datenbank vor dem Verlassen speichert:
Beachten Sie, dass wenn Clients Daten hinzufügen oder ändern, während die Operation bgsave ausgeführt wird, die entsprechenden Änderungen nicht im Snapshot erfasst werden.
Danach können Sie die Verbindung zu Ihrer Redis-Instanz schließen, indem Sie den Befehl exit ausführen:
Wenn Sie sie in Zukunft brauchen, finden Sie die Datendumpdatei im Arbeitsverzeichnis Ihrer Redis-Instanz.
Erinnern Sie sich daran, wie Sie Ihre Redis-Instanz im Tutorial zu den Voraussetzungen einer Redis-Installation so eingerichtet haben, dass sie / var / lib / redis als ihr Arbeitsverzeichnis verwendet.
Listen Sie die Inhalte Ihres Redis-Arbeitsverzeichnisses auf, um zu bestätigen, dass die Datendumpdatei darin enthalten ist.
Wenn die Dumpdatei richtig exportiert wurde, sehen Sie sie in der Ausgabe dieses Befehls.
Standardmäßig trägt diese Datei den Namen dump.rdb:
Nachdem Sie bestätigt haben, dass Ihre Daten erfolgreich gesichert wurden, können Sie Ihren Redis-Quellserver so konfigurieren, dass externe Verbindungen akzeptiert werden und Replikation zugelassen wird.
Schritt 3 - Konfigurieren Ihrer Redis-Quellinstanz
Standardmäßig ist Redis nicht so konfiguriert, dass nach externen Verbindungen gelauscht wird. Das bedeutet, dass jegliche Replikate, die Sie konfigurieren, nicht mit Ihrer Quellinstanz synchronisieren können, es sei denn, Sie aktualisieren Ihre Konfiguration.
Hier aktualisieren wir die Konfigurationsdatei der Quellinstanz, um externe Verbindungen zuzulassen und ein Passwort festzusetzen, das die Zielinstanz nach Beginn der Replikation zur Authentifizierung nutzen soll.
Danach fügen wir eine Firewall-Regel hinzu, um Verbindungen zu dem Port zuzulassen, an dem Redis ausgeführt wird.
Öffnen Sie die Konfigurationsdatei Ihrer Redis-Quellinstanz mit Ihrem bevorzugten Texteditor.
Navigieren Sie zu der Zeile, die mit der Direktive bind beginnt.
Standardmäßig sieht das so aus:
Diese Direktive bindet Redis an 127.0.0.1, eine IPv4-Loopbackadresse, die localhost repräsentiert.
Dadurch wird diese Redis-Instanz so konfiguriert, dass nur nach Verbindungen gelauscht wird, die von demselben Server stammen, auf dem die Instanz installiert ist.
Um zuzulassen, dass Ihre Quellinstanz alle Verbindungen zu ihrer öffentlichen IP-Adresse akzeptiert (wie z. B. die von Ihrer Zielinstanz), fügen Sie nach 127.0.0.1 die IP-Adresse Ihres Redis-Quellservers hinzu.
Beachten Sie, dass Sie nach 127.0.0.1 keine Kommas verwenden dürfen:
Wenn Sie es noch nicht getan haben, verwenden Sie die Direktive requirepass zur Konfiguration eines Passworts, das Benutzer eingeben müssen, bevor sie mit den Daten in der Quellinstanz interagieren können.
Heben Sie dazu die Auskommentierung der Direktive auf und legen Sie ein komplexes Passwort oder eine Passphrase fest:
Vergessen Sie nicht, sich das hier festgelegte Passwort zu notieren, da Sie es bei der Konfiguration des Zielservers benötigen.
Nach dieser Änderung können Sie die Redis-Konfigurationsdatei speichern und schließen.
Wenn Sie sie mit nano bearbeitet haben, drücken Sie dazu STRG + X, Y und dann die Eingabetaste.
Starten Sie dann den Redis-Dienst neu, um die Änderungen in Kraft zu setzen:
Das ist alles, was Sie zur Konfiguration von Redis tun müssen. Wenn Sie jedoch eine Firewall auf Ihrem Server konfiguriert haben, wird diese weiterhin alle Versuche durch Ihren Zielserver blockieren, sich mit der Quelle zu verbinden.
Wenn Sie Ihre Firewall mit ufw konfiguriert haben, können Sie sie mit dem folgenden Befehl aktualisieren, um Verbindungen zu dem Port zuzulassen, an dem Redis ausgeführt wird.
Beachten Sie, dass Redis standardmäßig so konfiguriert ist, dass Port 6379 verwendet wird:
Nachdem Sie diese letzte Änderung vorgenommen haben, haben Sie Ihren Redis-Quellserver fertig konfiguriert.
Fahren Sie fort, indem Sie Ihre Redis-Zielinstanz so konfigurieren, dass sie als Replikat der Quelle dient.
Schritt 3 - Konfigurieren Ihrer Redis-Zielinstanz
Bisher haben Sie Ihre Redis-Quellinstanz so konfiguriert, dass externe Verbindungen akzeptiert werden.
Da Sie jedoch den Zugriff auf die Quelle gesperrt haben, indem Sie die Auskommentierung der Direktive requirepass aufgehoben haben, wird Ihre Zielinstanz die in der Quelle gespeicherten Daten nicht replizieren können.
Hier werden Sie Ihre Redis-Zielinstanz so einrichten, dass sie ihre Verbindung zur Quelle authentifizieren kann. Dadurch wird Replikation möglich.
Verbinden Sie sich zunächst als non-root user mit Ihrem Redis-Zielserver:
Öffnen Sie als Nächstes die Redis-Konfigurationsdatei Ihres Zielservers:
Wenn Sie es noch nicht getan haben, sollten Sie mit der Direktive requirepass ein Passwort für Ihre Redis-Zielinstanz einrichten:
Heben Sie als Nächstes die Auskommentierung der Direktive masterauth auf und setzen Sie sie auf das Authentifizierungskennwort Ihrer Redis-Quellinstanz.
So kann sich Ihr Zielsever bei der Quellinstanz authentifizieren, wenn Sie Replikation aktivieren:
Wenn Sie über Clients verfügen, die Daten in Ihre Quellinstanz schreiben, wollen Sie sie letztlich so konfigurieren, dass sie Daten auch in Ihre Zielinstanz schreiben.
Wenn also ein Client Daten schreibt, nachdem Sie das Ziel wieder zu einer Primärinstanz hochgestuft haben, gehen diese nicht verloren.
Dazu müssen Sie jedoch die Direktive replica-read-only anpassen.
Diese ist standardmäßig auf yes gesetzt, was bedeutet, dass sie so konfiguriert ist, dass sie zu einem "schreibgeschützten" Replikat wird, in das Clients nicht schreiben können.
Setzen Sie diese Direktive auf no, damit Clients darin schreiben können:
Das sind alle Änderungen, die Sie an der Konfigurationsdatei des Ziels vornehmen müssen. Sie können die Datei also speichern und schließen.
Nach dem Neustart des Redis-Diensts ist Ihr Zielserver bereit, zu einem Replikat der Quelle zu werden.
Dazu müssen Sie lediglich einen einzelnen Befehl ausführen, was wir gleich tun werden.
< $> note Anmerkung: Wenn Sie über Clients verfügen, die Daten in Ihre Redis-Quellinstanz schreiben, wäre jetzt ein guter Zeitpunkt, um sie so zu konfigurieren, dass sie auch Daten in Ihr Ziel schreiben.
Schritt 5 - Starten und Verifizieren von Replikation
Bislang haben Sie Ihre Redis-Quellinstanz so konfiguriert, dass sie Verbindungen von Ihrem Zielserver akzeptiert, und Ihre Redis-Zielinstanz so eingerichtet, dass sie sich bei der Quelle als Replikat authentifizieren kann.
Nachdem Sie diese Aufgaben erledigt haben, können Sie nun Ihre Zielinstanz zu einem Replikat der Quelle machen.
Öffnen Sie zunächst auf Ihrem Redis-Server die Redis-Befehlszeilenschnittstelle:
Führen Sie den Befehl auth aus, um die Verbindung zu authentifizieren:
Verwandeln Sie als Nächstes die Zielinstanz mit dem Befehl replicaof in ein Replikat der Quelle.
Vergessen Sie nicht, < ^ > source _ server _ ip < ^ > durch die öffentliche IP-Adresse Ihrer Quellinstanz und < ^ > source _ port < ^ > durch den Port zu ersetzen, den Redis in Ihrer Quellinstanz verwendet:
Führen Sie in der Eingabeaufforderung folgenden scan-Befehl aus:
Dadurch werden alle Schlüssel zurückgegeben, die derzeit vom Replikat gehalten werden:
Wenn die Replikation wie erwartet funktioniert, sehen Sie alle Schlüssel von Ihrer Quellinstanz, die in der Replikation enthalten sind.
Wenn Sie Ihre Quelle mit den Beispieldaten in Schritt 1 geladen haben, sieht die Ausgabe des Befehls scan folgendermaßen aus:
< $> note Anmerkung: Beachten Sie, dass dieser Befehl die Schlüssel möglicherweise in einer anderen Reihenfolge als in diesem Beispiel zurückgibt.
Wenn dieser Befehl jedoch nicht dieselben Schlüssel zurückgibt, die in Ihrer Redis-Quellinstanz gespeichert sind, kann es sein, dass es in einer der Konfigurationsdateien Ihrer Server einen Fehler gibt, der verhindert, dass sich die Zieldatenbank mit der Quelle verbinden kann.
Schließen Sie in diesem Fall die Verbindung zu Ihrer Redis-Zielinstanz und überprüfen Sie, ob Sie die Konfigurationsdateien sowohl auf Ihrem Redis-Quell- als auch auf dem Zielserver richtig bearbeitet haben.
Während die Verbindung geöffnet ist, können Sie auch überprüfen, ob die Schlüssel, die Sie auf Ablauf gesetzt haben, noch flüchtig sind.
Führen Sie dazu den Befehl ttl mit einem dieser Schlüssel als Argument aus:
Dadurch wird die Zahl von Sekunden zurückgegeben, bevor dieser Schlüssel gelöscht wird:
Nachdem Sie sich vergewissert haben, dass die Daten in Ihrer Quellinstanz richtig mit Ihrem Ziel synchronisiert wurden, können Sie das Ziel wieder zu einer Primärinstanz hochstufen, indem Sie den Befehl replicaof noch einmal ausführen.
Lassen Sie dieses Mal jedoch auf replicaof keine IP-Adresse und keinen Port folgen, sondern no one.
Das führt unmittelbar dazu, dass die Zielinstanz nicht mehr mit der Quelle synchronisiert wird:
Um zu überprüfen, ob die von der Quelle replizierten Daten im Ziel persistent sind, führen Sie erneut den zuvor eingegebenen Befehl scan aus:
In der Ausgabe des Befehls sollten dieselben Schlüssel angezeigt werden wie bei der Ausführung des Befehls scan, als das Ziel die Quelle noch repliziert hat:
Damit haben Sie alle Daten von Ihrer Redis-Quellinstanz erfolgreich in Ihr Ziel migriert.
Wenn Sie über Clients verfügen, die noch Daten in die Quellinstanz schreiben, wäre jetzt ein guter Zeitpunkt, um sie so zu konfigurieren, dass sie ausschließlich in das Ziel schreiben.
Neben der Replikation gibt es weitere Methoden, die Sie zum Migrieren von Daten aus einer Redis-Instanz in eine andere verwenden können. Replikation bietet jedoch den Vorteil, dass relativ wenige Konfigurationsänderungen erforderlich sind und Sie nur einen einzigen Befehl zum Initiieren oder Anhalten benötigen.
Wenn Sie mehr über das Arbeiten mit Redis erfahren möchten, sehen Sie sich unsere Tutorialreihe Verwalten einer Redis-Datenbank an.
Wenn Sie Ihre Redis-Daten in eine Redis-Instanz verschieben möchten, die von DigitalOcean verwaltet wird, folgen Sie den entsprechenden Anweisungen in unserem Leitfaden.
Einrichten der Eclipse Theia Cloud IDE-Plattform unter DigitalOcean Kubernetes
3323
Cloud-IDEs sind per Webbrowser über beliebige moderne Geräte zugänglich und bieten zahlreiche Vorteile für echtzeitbasierte Zusammenarbeit.
Eine Cloud-IDE bietet Ihnen und Ihrem Team eine einheitliche Entwicklungs- und Testumgebung und minimiert gleichzeitig Plattform-Inkompatibilitäten.
Da die Plattformen nativ auf Cloud-Technologien basieren, können sie unter Verwendung des Clusters Aufgaben erledigen, die deutlich über die Leistung und Zuverlässigkeit einzelner Entwicklungscomputer hinausgehen.
Eclipse Theia ist eine erweiterbare Cloud IDE, die auf einem Remote-Server läuft und von einem Web-Browser aus zugänglich
Aussehen und Verhalten ähneln Microsoft Visual Studio-Code ​ ​ ​, was heißt, dass viele Programmiersprachen unterstützt werden und es ein flexibles Layout sowie ein integriertes Terminal gibt.
Was Eclipse Theia von einer anderen Cloud IDE-Software unterscheidet, ist die Erweiterbarkeit; sie kann mit benutzerdefinierten Erweiterungen modifiziert werden, damit Sie eine Cloud IDE für Ihre Bedürfnisse erstellen können.
In diesem Tutorial richten Sie die Standardversion der Eclipse Theia Cloud IDE-Plattform in Ihrem DigitalOcean Kubernetes-Cluster ein und machen sie in Ihrer Domäne verfügbar, gesichert mit Let 's Encrypt-Zertifikaten und mit vorgeschriebener Authentifizierung des Benutzers.
Zum Schluss wird Eclipse Theia in Ihrem Kubernetes-Cluster ausgeführt, der über HTTPS verfügbar ist; dabei muss sich der Benutzer anmelden.
Eine Anleitung zur Konfiguration von kubectl finden Sie unter dem Schritt Verbinden mit Ihrem Cluster, wenn Sie Ihren Cluster erstellen.
Um einen Kubernetes-Cluster in DigitalOcean zu erstellen, lesen Sie unser Dokument Kubernetes Schnellstart.
Der Nginx Ingress Controller und Cert Manager, in Ihrem Cluster mit Helm installiert, um Eclipse Theia mit Ingress Resources verfügbar zu machen.
Folgen Sie dazu den Anweisungen unter Einrichten eines Nginx Ingress unter DigitalOcean Kubernetes mit Helm.
Ein vollständig registrierter Domänenname zum Hosten von Eclipse Theia.
Dieses Tutorial verwendet in allen Bereichen < ^ > theia.your _ domain < ^ >.
Schritt 1 - Installieren und Verfügbarmachen von Eclipse Theia
Installieren Sie zunächst Eclipse Theia in Ihrem DigitalOcean Kubernetes-Cluster.
Dann machen Sie die Plattform in Ihrer gewünschten Domäne mit einem Nginx Ingress verfügbar.
Da Sie im Rahmen der Voraussetzungen zwei Beispielumgebungen und eine Ressource erstellt haben, können Sie diese durch Ausführung folgender Befehle bei Bedarf löschen:
In diesem Tutorial speichern Sie die Bereitstellungskonfiguration auf Ihrem lokalen Rechner, in einer Datei namens eclipse-theia.yaml.
Erstellen Sie diese mit dem folgenden Befehl:
Fügen Sie der Datei folgende Zeilen hinzu:
Diese Konfiguration definiert einen Namespace, eine Bereitstellung, einen Dienst und einen Ingress.
Der Namespace heißt theia und enthält alle Kubernetes-Objekte, die mit Eclipse Theia verbunden sind, getrennt vom Rest des Clusters.
Die Bereitstellung besteht aus einer Instanz des theiaide / theia: next-Docker-Image, wobei Port 3000 für den Container verfügbar ist.
Der Dienst sucht nach der Bereitstellung und ordnet den Container-Port dem üblichen HTTP-Port 80 zu, um Zugriff im Cluster auf Eclipse Theia zu ermöglichen.
Der Ingress enthält eine Regel, um den Dienst bei Port 80 an Ihrer gewünschten Domäne extern bereitzustellen.
In den Anmerkungen geben Sie an, dass zur Bearbeitung von Anfragen der Nginx Ingress Controller verwendet werden soll.
Vergessen Sie nicht, < ^ > theia.your _ domain < ^ > durch Ihre gewünschte Domäne zu ersetzen, die Sie auf den Load Balancer Ihres Clusters verwiesen haben. Speichern und schließen Sie dann die Datei.
Erstellen Sie anschließend die Konfiguration in Kubernetes, indem Sie folgenden Befehl ausführen:
Sie können die Erstellung des Eclipse Theia-Pods überprüfen, indem Sie folgenden Befehl ausführen:
Nach einer gewissen Zeit ändert sich der Status in RUNNING, was bedeutet, dass Sie Eclipse Theia erfolgreich in Ihrem Cluster installiert haben.
Navigieren Sie im Browser zu Ihrer Domäne.
Sie sehen die Standardoberfläche des Eclipse Theia-Editors.
Die Standardoberfläche des Eclipse Theia-Editors
Sie haben Eclipse Theia in Ihrem DigitalOcean Kubernetes-Cluster bereitgestellt und in Ihrer gewünschten Domäne mit einem Ingress verfügbar gemacht.
Als Nächstes sichern Sie den Zugriff auf Ihre Eclipse Theia-Bereitstellung, indem Sie die Anmeldeauthentifizierung aktivieren.
Schritt 2 - Aktivieren der Anmeldeauthentifizierung für Ihre Domäne
In diesem Schritt aktivieren Sie für Ihre Eclipse Theia-Umgebung die Authentifizierung mit Benutzername und Passwort.
Sie erreichen dies, indem Sie zunächst mit dem Dienstprogramm htpasswd eine Liste mit gültigen Anmeldekombinationen zusammenstellen.
Dann erstellen Sie ein Kubernetes-Geheimnis, das diese Liste enthält, und konfigurieren den Ingress, um eine entsprechende Authentifizierung von Besuchern vorzunehmen. Am Ende wird Ihre Domäne nur zugänglich sein, wenn der Besucher eine gültige Kombination aus Benutzername und Passwort eingibt.
Dadurch werden Gäste und andere unerwünschte Besucher daran gehindert, auf Eclipse Theia zuzugreifen.
Das Dienstprogramm htpasswd stammt vom Apache-Webserver und dient zum Erstellen von Dateien, in denen Listen von Anmeldekombinationen gespeichert werden.
Das Format von htpasswd ist eine username: hashed _ password-Kombination pro Zeile, was das Format ist, das der Nginx Ingress Controller von der Liste erwartet.
Installieren Sie zunächst htpasswd in Ihrem System, indem Sie folgenden Befehl ausführen:
Speichern Sie die Liste in einer Datei namens auth.
Erstellen Sie sie, indem Sie folgenden Befehl ausführen:
Die Datei muss auth heißen, da der Nginx Ingress Controller davon ausgeht, dass das Geheimnis einen Schlüssel namens data.auth enthält.
Wenn der Schlüssel fehlt, gibt der Controller den HTTP 503-Status Dienst nicht verfügbar zurück.
Fügen Sie auth eine Kombination aus Benutzername und Passwort hinzu, indem Sie folgenden Befehl ausführen:
Denken Sie daran, < ^ > username < ^ > durch Ihren gewünschten Benutzernamen zu ersetzen.
Sie werden nach einem begleitenden Passwort gefragt, bevor die Kombination dann der Datei auth hinzugefügt wird.
Sie können diesen Befehl für so viele Benutzer wiederholen, wie Sie hinzufügen möchten.
< $> note Anmerkung: Wenn im System, mit dem Sie arbeiten, htpasswd nicht installiert ist, können Sie stattdessen eine dockerisierte Version verwenden.
Dafür muss Docker auf Ihrem Rechner installiert sein.
Eine Anleitung dazu finden Sie in der offiziellen Dokumentation.
Führen Sie folgenden Befehl aus, um eine dockerisierte Version auszuführen:
Denken Sie daran, < ^ > < username > < ^ > durch den gewünschten Benutzernamen zu ersetzen.
Sie werden nach einem Passwort gefragt.
Die Anmeldekombination mit Hash wird in der Konsole ausgeschrieben; Sie müssen sie am Ende der Datei auth manuell hinzufügen.
Wiederholen Sie diesen Vorgang für so viele Anmeldungen, wie Sie hinzufügen möchten.
Wenn Sie damit fertig sind, erstellen Sie in Kubernetes ein neues Geheimnis mit dem Inhalt der Datei, indem Sie folgenden Befehl ausführen:
Sie können das Geheimnis anzeigen mit:
Als Nächstes müssen Sie den Ingress so bearbeiten, dass er das Geheimnis verwendet.
Öffnen Sie die Bereitstellungskonfiguration zum Bearbeiten:
Fügen Sie in Ihrer Datei die hervorgehobenen Zeilen hinzu:
Geben Sie zuerst in der Anmerkung auth-type an, dass der Authentifizierungstyp basic ist.
Das bedeutet, dass Nginx vom Benutzer verlangt, einen Benutzernamen und ein Passwort einzugeben.
Dann geben Sie in auth-secret an, dass das Geheimnis, das die Liste mit gültigen Kombinationen enthält, theia-basic-auth lautet. Dieses Geheimnis haben Sie gerade erstellt.
Die verbleibende Anmerkung auth-realm gibt eine Nachricht an, die dem Benutzer als Erklärung zur vorgeschriebenen Authentifizierung angezeigt wird.
Sie können die in diesem Feld enthaltene Nachricht nach Belieben ändern.
Um die Änderungen in Ihrem Cluster zu verteilen, führen Sie folgenden Befehl aus:
Sie sehen die folgende Ausgabe:
Navigieren Sie im Browser zu Ihrer Domäne, wo Sie sich anmelden müssen.
Sie haben für Ihren Ingress die grundlegende Anmeldeauthentifizierung aktiviert, indem Sie ihn so konfiguriert haben, dass das Geheimnis mit der Hash-Kombination aus Benutzername und Passwort verwendet wird.
Im nächsten Schritt sichern Sie den Zugriff durch Hinzufügen von TLS-Zertifikaten, sodass der Datenverkehr zwischen Ihnen und Ihrer Eclipse Theia-Umgebung verschlüsselt bleibt.
Schritt 3 - Anwenden von Let 's Encrypt-HTTPS-Zertifikaten
Als Nächstes sichern Sie Ihre Eclipse Theia-Installation, indem Sie auf Ihren Ingress Let 's Encrypt-Zertifikate anwenden, die von Cert-Manager automatisch bereitgestellt werden.
Nach Abschluss dieses Schritts ist Ihre Eclipse Theia-Installation über HTTPS aufrufbar.
Öffnen Sie eclipse-theia.yaml, um die Datei zu bearbeiten:
Fügen Sie die hervorgehobenen Zeilen Ihrer Datei hinzu und ersetzen Sie dabei die Platzhalterdomäne durch Ihre eigene Domäne:
Geben Sie zunächst den letsencrypt-prod-ClusterIssuer an, den Sie im Rahmen der Voraussetzungen erstellt haben, da dieser Aussteller der Bereitstellung von Zertifikaten für diesen Ingress dient.
Dann geben Sie im Abschnitt tls die genaue Domäne, die gesichert werden soll, sowie einen Namen für ein Geheimnis an, das diese Zertifikate enthalten wird.
Wenden Sie die Änderungen in Ihrem Cluster an, indem Sie folgenden Befehl ausführen:
Es dauert einige Minuten, bis die Zertifikate bereitgestellt und vollständig angewendet werden.
Sie können den Fortschritt verfolgen, indem Sie die Ausgabe des folgenden Befehls überprüfen:
Wenn der Befehl abgeschlossen ist, sieht das Ende der Ausgabe ungefähr so aus:
Aktualisieren Sie Ihre Domäne in Ihrem Browser.
Sie sehen auf der linken Seite der Adressleiste ein grünes Schloss, was darauf hinweist, dass die Verbindung gesichert ist.
Sie haben den Ingress so konfiguriert, dass Let 's Encrypt-Zertifikate verwendet werden, um Ihre Eclipse Theia-Installation sicherer zu machen.
Jetzt können Sie die Standardbenutzeroberfläche von Eclipse Theia überprüfen.
Schritt 3 - Benutzen der Eclipse Theia-Benutzeroberfläche
In diesem Abschnitt werden Sie einige Funktionen von Eclipse Theia erkunden.
Auf der linken Seite der IDE befindet sich eine vertikale Reihe von vier Schaltflächen, welche die am häufigsten verwendeten Funktionen in einem Seitenbereich öffnen.
Eclipse-Theia-GUI - Seitenbereich
Standardmäßig öffnet die erste Ansicht das Explorer-Feld, das eine baumartige Navigation der Struktur des Projekts bereitstellt.
Sobald Sie über das Menü Datei eine neue Datei erstellt haben, öffnet sich eine leere Datei in einer neuen Registerkarte. Sobald sie gespeichert wurde, können Sie den Namen der Datei im Seitenfeld des Explorers sehen.
Erstellen Sie Ordner durch Rechtsklick auf die Explorer-Seitenleiste und klicken Sie auf Neuer Ordner.
Eclipse Theia-GUI - Neuer Ordner
Die beiden nächsten Optionen bieten Zugriff auf die Funktion zum Suchen und Ersetzen.
Die darauffolgende Option bietet eine Ansicht von Versionsverwaltungssystemen, die Sie möglicherweise verwenden, wie z. B. Git.
Die letzte Ansicht ist die Debugger-Option, die alle gängigen Aktionen zum Debuggen in dem Feld bereitstellt.
Eclipse Theia unterstützt, wie alle modernen IDEs, Syntaxhervorhebung für Ihren Code.
Sie können sich Zugriff auf ein Terminal verschaffen, indem Sie Strg + Umschalt + 'eingeben oder im oberen Menü auf Terminal klicken und Neues Terminal auswählen.
Terminal geöffnet
Sie haben sich nun einen Überblick über die Eclipse Theia-Benutzeroberfläche und einige der am häufigsten verwendeten Funktionen verschafft.
Sie haben nun Eclipse Theia, eine vielseitige Cloud-IDE, in Ihrem DigitalOcean Kubernetes-Cluster installiert.
Sie haben sie mit einem kostenlosen Let 's Encrypt-TLS-Zertifikat gesichert und die Instanz so eingerichtet, dass eine Anmeldung des Benutzers vorgeschrieben ist.
Sie können damit an Ihrem Quellcode und Dokumenten einzeln arbeiten oder im Team zusammenarbeiten.
Sie können auch versuchen, Ihre eigene Version von Eclipse Theia zu erstellen, wenn Sie zusätzliche Funktionen benötigen.
Weitere Informationen dazu finden Sie unter Theia Docs.
Verwenden von Visual Studio Code für die Remoteentwicklung über das Remote-SSH-Plugin
3927
Visual Studio Code ist eine beliebte Integrated Developer Environment (IDE) für Entwickler.
Dank ihres großen Spektrums an Plugins, eines Minimaldesigns und plattformübergreifender Unterstützung eignet sie sich perfekt für Entwickler aller Kenntnisstufen.
In diesem Tutorial geht es um die Verwendung des Remote-SSH-Plugins, das eine Remoteentwicklung von Software ermöglicht.
Mit diesem Plugin können Sie Dateien auf Ihrer lokalen Workstation bearbeiten, Entwicklungsaufgaben wie Programmausführung, Komponententests oder statische Analysen aber auf einem Remoteserver ausführen.
Es gibt viele Gründe, warum dies hilfreich sein kann.
Möglicherweise verfügen Sie zum Beispiel über eine Windows-Workstation und möchten unter Windows entwickeln, während der Code später in Linux ausgeführt werden soll.
Vielleicht benötigen Sie mehr RAM- oder Verarbeitungsleistung, als Ihr aktueller Rechner hat, oder wollen aufgrund einer Unternehmensrichtlinie Code fern von Ihrem persönlichen Rechner halten, oder Ihre Workstation unberührt lassen.
In diesem Tutorial aktivieren Sie das Remote-SSH-Plugin, konfigurieren Visual Studio Code so, dass Code auf dem Remoteserver ausgeführt wird, und führen Code von Ihrer lokalen Visual Studio Code-Installation auf dem Remoteserver aus.
Um mit diesem Leitfaden fortfahren zu können, benötigen Sie:
Einen lokalen Entwicklungscomputer, auf dem Windows, MacOSX oder Linux ausgeführt wird.
Dieses Tutorial funktioniert nicht auf ChromeOS-Geräten.
Visual Studio Code, das Sie von der offiziellen Website herunterladen und installieren können.
Ein generiertes SSH-Schlüsselpaar:
Wenn Sie macOS oder Linux verwenden, können Sie Schritt 1 von Einrichten von SSH-Schlüsseln unter Ubuntu 18.04 folgen.
Die Befehle sind identisch; machen Sie sich also keine Sorgen, dass im Tutorial steht, es sei für Ubuntu 18.04 gedacht.
Wenn Sie Windows verwenden, folgen Sie dem Tutorial Erstellen von SSH-Schlüsseln mit PuTTY unter Windows, um Ihren SSH-Schlüssel zu erstellen.
Wenn Sie DigitalOcean verwenden, können Sie dem Leitfaden Hochladen von öffentlichen SSH-Schlüsseln in ein DigitalOcean-Konto folgen.
Einen Ubuntu 18.04-Server, der gemäß der Anleitung zur Einrichtung des Ubuntu 18.04-Servers eingerichtet wurde, einschließlich eines sudo-fähigen non-root users und einer Firewall.
Schritt 1 - Installieren des Remote-SSH-Plugins
Im Extensions Marketplace können Sie für verschiedene Tools und Programmiersprachen unterstützte Erweiterungen und Erweiterungen von Drittanbietern herunterladen.
Hier suchen Sie nach dem Remote-SSH-Plugin und installieren es.
Auf der linken Seite der IDE befindet sich eine vertikale Reihe mit fünf Symbolen.
Das unterste Symbol, das wie vier Quadrate in einem Kästchen aussieht, dessen oberes rechtes Quadrat nach außen ragt, ist das Symbol für den Extensions Marketplace:
Stelle des Extensions Marketplace-Symbols
Sie können auch auf diesen Abschnitt zugreifen, indem Sie Strg + Umschalttaste + X drücken.
Wenn Sie diese Seite öffnen, sehen Sie zum Herunterladen und Installieren vorgeschlagene Plugins.
Wenn der Extensions Marketplace geöffnet ist, geben Sie Remote-SSH in die Suchleiste Extensions in Marketplace durchsuchen ein.
Wenn Sie das Plugin finden, wählen Sie es aus und klicken Sie dann auf die grüne Schaltfläche Installieren, um die Erweiterung zu installieren.
Suchen nach dem Remote-SSH-Plugin
Die Erweiterung ist nun installiert.
Als Nächstes konfigurieren Sie die Erweiterung, damit Sie sich mit Ihrem Server verbinden können.
Schritt 2 - Konfigurieren des Remote-SSH-Plugins und Verbinden mit Ihrem Server
Nachdem Sie das Plugin installiert haben, können Sie es nun konfigurieren, um eine Verbindung mit einem Server herzustellen.
Dazu benötigen Sie folgende Informationen:
Die IP-Adresse oder den Hostnamen des Servers.
Den Benutzernamen, mit dem Sie sich verbinden.
Den privaten Schlüssel, den Sie zur Authentifizierung Ihres Benutzers verwenden werden.
Sie verwenden diese Informationen zum Erstellen einer SSH-Konfigurationsdatei, die Visual Studio Code zur Herstellung einer SSH-Verbindung mit dem Server verwenden kann, um Dateien zu synchronisieren und Code in Ihrem Namen auszuführen.
Diese Konfiguration erstellen Sie mit Visual Studio Code.
Nachdem Sie das Remote-SSH-Plugin installiert haben, sehen Sie nun ein kleines grünes Feld in der unteren linken Ecke der Benutzeroberfläche von Visual Studio Code.
Wenn Sie mit dem Mauszeiger über das Feld fahren, steht im Popup Remotefenster öffnen.
Die Schaltfläche sieht ungefähr wie ein Größer-als-Zeichen unter einem Kleiner-als-Zeichen > < aus, wie im folgenden Bild dargestellt:
Grüne Schaltfläche zum Öffnen eines Remotefensters
Klicken Sie auf die Schaltfläche, woraufhin oben in der Mitte ein Dialogfeld erscheint.
Wählen Sie Remote-SSH: Konfigurationsdatei öffnen... aus der Liste:
Auswählen von "SSH konfigurieren" in der Benutzeroberfläche
In der nächsten Eingabeaufforderung werden Sie gefragt, welche Konfigurationsdatei Sie öffnen möchten.
Wenn Sie Windows verwenden, sehen Sie zwei Speicherorte: einen in Ihrem persönlichen Benutzerverzeichnis und einen am Installationsspeicherort für SSH.
Zum Konfigurieren des Servers sollten Sie die Datei in Ihrem Benutzerverzeichnis verwenden.
Wählen Sie die Datei aus, woraufhin Ihr Editor die config-Datei öffnet.
Fügen Sie der Datei folgenden Code hinzu, um die Verbindung mit Ihrem Server festzulegen, indem Sie die hervorgehobenen Abschnitte durch die Informationen für Ihren Server ersetzen:
So funktioniert diese Konfigurationsdatei:
Host: Gibt einen Namen für Ihren Host an.
Damit können Sie beim Herstellen einer Verbindung mit dem Server einen kurzen Namen oder eine Abkürzung anstelle der vollständigen IP-Adresse oder des Hostnamens verwenden.
HostName: Der tatsächliche Hostname des Servers, der entweder eine IP-Adresse oder ein vollqualifizierter Domänenname ist.
User: Der Benutzer, mit dem Sie eine Verbindung herstellen möchten.
IdentityFile: Der Pfad zu Ihrem privaten SSH-Schlüssel.
In Mac- und Linux-Systemen finden Sie diesen in Ihrem Stammverzeichnis in einem versteckten .ssh-Verzeichnis, das typischerweise id _ rsa heißt.
Wenn Sie Windows verwenden, haben Sie einen Speicherort zur Speicherung dieser Datei angegeben, als Sie diese mit putty-gen erstellt haben.
Geben Sie die entsprechenden Werte in Ihrer Datei an und speichern Sie die Datei.
Visual Studio Code ist nun konfiguriert und bereit, sich mit Ihrem Server zu verbinden.
Klicken Sie unten links auf die grüne Schaltfläche Remotefenster öffnen und wählen Sie Remote-SSH: Mit Host verbinden...
Herstellen einer Verbindung mit dem Server von Visual Studio Code
Sobald Sie damit fertig sind, werden alle verfügbaren und konfigurierten Server im Dropdown-Menü angezeigt.
Wählen Sie den Server, mit dem Sie sich verbinden möchten, aus dieser Liste aus.
Wenn Sie sich von Ihrem Rechner zum ersten Mal mit diesem Server verbinden, werden Sie wahrscheinlich eine Aufforderung mit dem SSH Fingerprint-Verifizierungsdialog erhalten, wie im folgenden Bild zu sehen:
Bestätigen Ihres SSH Fingerprint
Dadurch wird sichergestellt, dass Sie sich wirklich mit dem richtigen Server verbinden.
Sie können dies überprüfen, indem Sie sich bei Ihrem Server manuell anmelden und ssh-keygen -l -f / etc / ssh / ssh _ host _ key.pub ausführen, um den Fingerabdruck des Servers anzuzeigen.
Wenn dieser Fingerabdruck mit dem übereinstimmt, der in Visual Studio Code angezeigt wird, dann verbinden Sie sich tatsächlich mit dem richtigen Server, sodass Sie auf Fortfahren klicken können.
Visual Studio Code öffnet standardmäßig ein neues Fenster, sobald eine neue Verbindung hergestellt wird.
Ein neues Fenster mit dem Empfangsbildschirm wird angezeigt.
Sie wissen, dass Ihre Verbindung erfolgreich hergestellt wurde, wenn unten links im grünen Feld SSH: < ^ > your _ ip _ address _ or _ hostname < ^ > angezeigt wird.
Das bedeutet, dass Visual Studio Code verbunden ist und mit Ihrem Remoteserver kommuniziert.
Erfolgreiche SSH-Verbindung
Nachdem Sie verbunden sind, können Sie nun Befehle und Code aus Ihrem Editor ausführen.
Schritt 3 - Ausführen von Code auf dem Remoteserver
Das Remote-SSH-Plugin ist fertig konfiguriert, sodass es an der Zeit ist, Code auf Ihrem Remotecomputer auszuführen.
Öffnen Sie ein Terminalfenster, indem Sie aus der Navigationsleiste oben im Fenster Visual Studio Terminal auswählen und auf Neues Terminal klicken.
Sie können auch ein Terminal öffnen, indem Sie Strg + Umschalttaste + 'drücken.
Das Terminal, das geöffnet wird, ist ein Terminal auf Ihrem Remoteserver, nicht auf Ihrem lokalen Rechner.
Wenn sich das Terminal öffnet, geben Sie folgenden Befehl aus, um die IP-Adresse Ihres Servers anzuzeigen und zu überprüfen, ob Sie mit Ihrem Remoteserver verbunden sind:
Sie sehen in Ihrem Terminal folgende Ausgabe:
Um die Fähigkeit zur Ausführung von Remotecode zu testen, erstellen Sie in Ihrem Editor eine neue Python-Datei namens hello.py.
Wenn Sie mit Ihrem Remoteserver verbunden sind, werden alle über Visual Studio Code erstellten Dateien auf diesem Server gespeichert, nicht auf Ihrem lokalen Rechner.
Um das Programm auf Ihrem Server auszuführen, öffnen Sie in Visual Studio Code ein Terminal über das Navigationsmenü oder drücken Sie die Tastenfolge Strg + Umschalt + '.
Da diese Terminalsitzung mit Ihrem Remoteserver verbunden ist, führen Sie im Terminal folgenden Befehl zur Ausführung Ihres hello.py-Programms aus:
Die Ausgabe Ihres Programms wird angezeigt.
Ausführen Ihres Python-Skripts
Sie können die Datei auch über das Kontextmenü Debug ausführen, indem Sie Ohne Debugging ausführen wählen.
< $> note Anmerkung: Wenn Sie in Visual Studio Code Entwicklungserweiterungen installiert haben (wie die Python-Erweiterung), müssen Sie diese Erweiterungen auf Ihrem Server über den Extension Marketplace neu installieren.
Falls Sie diese Plugins zuvor in Visual Studio Code installiert haben, zeigt der Marketplace, wenn Sie erneut nach ihnen suchen, Installieren in SSH: Hostname an.
Achten Sie stets darauf, in welchem Entwicklungskontext Sie sich befinden, da Visual Studio Code dort Ihre Plugins installieren und Dateien erstellen wird.
Wenn Sie versuchen, Code auszuführen, ohne diese Plugins installiert zu haben, werden in der unteren rechten Ecke des Bildschirms Fehlerdialogfelder angezeigt, in denen Sie zur Installation der Plugins auf Ihrem Remoteserver aufgefordert werden.
Nachdem Sie sie installiert haben, müssen Sie Visual Studio Code wahrscheinlich neu laden.
Wenn Sie es neu starten, wird es auf dem Remoteserver weiter ausgeführt, ohne dass Sie manuell eine neue Verbindung herstellen müssen.
Sie haben Visual Studio Code nun zur Entwicklung auf einem Remoteserver mit SSH konfiguriert.
Remoteausführung mit einer IDE bietet viele Vorteile, einschließlich der Fähigkeit, schnell zu testen, wie Ihr Code in verschiedenen Betriebssystemen und Hardwarespezifikationen ausgeführt wird.
Solange Sie über eine Internetverbindung verfügen, können Sie sich mit Ihrem Server verbinden und über beliebige Computer mit Ihrem Code arbeiten. Außerdem können Sie mit einer Linux-Umgebung entwickeln, selbst wenn Sie Windows als primäres Betriebssystem verwenden.
Erstellen einer Anwendung für ein inspirierendes Zitat unter Verwendung von AdonisJs und MySQL
3206
AdonisJs ist ein in einfachem JavaScript geschriebenes Node.js Web-Framework, das auf allen gängigen Betriebssystemen läuft.
Es verwendet das beliebte Designmuster MVC (Model - View - Controller) und bietet ein stabiles Ökosystem für das Schreiben serverseitiger Webanwendungen.
Das Framework bietet nahtlose Authentifizierung, SQL ORM (objekt-relationales Mapping), Migrationen und Datenbank-Seeding.
AdonisJs weist eine ähnliche Architektur wie das PHP Webanwendungs-Framework Laravel auf, einschließlich der gleichen Ordnerstruktur und mehrere gemeinsam genutzter Einrichtungskonzepte.
Standardmäßig verwendet AdonisJs die Edge Template Engine, die für eine intuitive Verwendung konzipiert ist.
Genau wie Laravel wird AdonisJs mit einem ORM namens Lucid bereitgestellt, das als Schnittstelle für die Kommunikation zwischen den Modellen einer Anwendung und der Datenbank dient.
Mit AdonisJs können Entwickler eine Full-Stack-Anwendung erstellen, bei der der Backend-Server für die Anwendung der Geschäftslogik, das Routing und das Rendering aller Seiten der Anwendung verantwortlich ist.
Es ist auch möglich, eine Webdienst-API zu erstellen, um JSON-Antworten von einem Controller zurückzugeben; diese Webdienste können dann über Frontend-Frameworks wie Vue.js, React und Angular konsumiert werden.
In diesem Tutorial erstellen Sie eine Anwendung mit AdonisJs unter Verwendung seiner CLI.
Sie erstellen Routen, Controller, Modelle und Ansichten innerhalb Ihrer Anwendung und führen Formularvalidierungen durch.
Das Beispiel in diesem Tutorial wird eine Anwendung für ein inspirierendes Zitat sein, bei der sich ein Benutzer registrieren und anmelden kann, um ein inspirierendes Zitat zu erstellen.
Diese Demo-Anwendung bietet Ihnen die Möglichkeit zur Ausführung von CRUD (Create, Read, Update und Delete) -Operationen.
Bevor Sie mit diesem Leitfaden beginnen, benötigen Sie Folgendes:
Eine lokale Installation von Node.js (mindestens v8) und npm (mindestens v3.0).
Node.js ist eine JavaScript-Laufzeitumgebung, mit der Sie Ihren Code außerhalb des Browsers ausführen können.
Sie beinhaltet einen vorinstallierten Paketmanager namens npm, mit dem Sie Pakete installieren und aktualisieren können.
Um diese unter MacOS oder Ubuntu 18.04 zu installieren, folgen Sie den Schritten in Installieren von Node.js und Erstellen einer lokalen Entwicklungsumgebung auf MacOS oder dem Abschnitt Installieren unter Verwendung eines PPA von Installieren von Node.js unter Ubuntu 18.04.
Auf Ihrem Rechner installiertes MySQL.
Folgen Sie den Anweisungen hier, um es für das Betriebssystem Ihrer Wahl herunterzuladen und zu installieren.
Um MySQL erfolgreich zu installieren, können Sie es entweder mit Homebrew unter Mac installieren oder für Ubuntu 18.04 unserem Tutorial Installieren von MySQL unter Ubuntu 18.04 folgen.
Ein grundlegendes Verständnis von JavaScript; siehe unsere Serie Codieren in JavaScript.
Einen installierten Texteditor wie Visual Studio Code, Atom oder Sublime Text.
< $> note Anmerkung: Dieses Tutorial verwendet für die Entwicklung einen macOS-Rechner.
Wenn Sie ein anderes Betriebssystem verwenden, müssen Sie in den ersten Schritten möglicherweise sudo für npm-Befehle verwenden.
Schritt 1 - Installieren der Adonis CLI
In diesem Abschnitt werden Sie Adonis CLI und alle erforderlichen Pakete auf Ihrem lokalen Rechner installieren.
Die CLI ermöglicht es Ihnen, ein neues AdonisJs-Projekt aufzubauen, sowie Boilerplates für Controller, Middlewares und Modelle in Ihrer Anwendung zu erstellen und zu generieren.
Außerdem erstellen Sie Ihre Datenbank für das Projekt.
Führen Sie den folgenden Befehl aus, um die AdonisJs CLI global auf Ihrem Rechner über npm zu installieren:
Geben Sie nach Abschluss der Installation den folgenden Befehl im Terminal ein, um die Installation von AdonisJs zu bestätigen und die aktuelle Version anzuzeigen:
Sie sehen eine Ausgabe, die die aktuelle Version von AdonisJs zeigt:
Mit der erfolgreichen Installation der AdonisJs CLI haben Sie nun Zugriff auf den Befehl adonis und können ihn zum Erstellen neuer Installationen eines AdonisJs-Projekts, zur Verwaltung Ihres Projekts und zum Erzeugen relevanter Dateien wie die Controller, Modelle usw. verwenden.
Nun können Sie mit der Erstellung eines neuen AdonisJs-Projekts fortfahren, indem Sie den Befehl adonis wie hier gezeigt verwenden:
Der vorhergehende Befehl erstellt eine Anwendung namens < ^ > adonis-quotes-app < ^ > ​ ​ ​ in einem neuen Verzeichnis mit demselben Namen in Ihrem lokalen Projektverzeichnis mit der entsprechenden AdonisJs MVC-Struktur.
Verschieben Sie sie in den neuen Anwendungsordner:
Starten Sie dann Ihre Anwendung durch Ausführen von:
Dadurch wird der Entwicklungsserver auf dem Standardport 3333 gestartet, wie in der Root-Datei .env für Ihre Anwendung angegeben.
Navigieren Sie zu http: / / localhost: 3333, um die Begrüßungsseite von AdonisJs anzuzeigen.
Begrüßungsseite von AdonisJs
Jetzt schließen Sie die Einrichtung Ihrer Datenbank ab.
Hier installieren Sie den Treiber mysql, um sich über npm von Ihrer Node.js-Anwendung mit Ihrem MySQL-Server zu verbinden.
Gehen Sie zunächst wieder zu Ihrem Terminal zurück, auf dem die Anwendung derzeit läuft, beenden Sie den Prozess mit STRG + C und führen Sie den folgenden Befehl aus:
Nachdem Sie nun den MySQL Node.js-Treiber für diese Anwendung erfolgreich installiert haben, müssen Sie die Anwendungsdatenbank erstellen und die entsprechende Verbindung zu ihr aufbauen.
Die neueste Version von MySQL, die Sie aus dem vorbereitendem Tutorial installiert haben, verwendet ein standardmäßiges Authentifizierungs-Plugin namens caching _ ha2 _ password.
Dieses wird derzeit von den Node.js-Treibern für MySQL nicht unterstützt.
Um Probleme mit der Datenbankverbindung in Ihrer Anwendung zu vermeiden, müssen Sie einen neuen MySQL-Benutzer anlegen und das derzeit unterstützte Authentifizierungs-Plugin mysql _ native _ password verwenden.
Um zu beginnen, greifen Sie mit dem Account root auf den MySQL-Client zu:
Sie werden aufgefordert, das während der MySQL-Installation eingerichtete Passwort für den Account root einzugeben.
Erstellen Sie als Nächstes den Benutzer und das Passwort unter Verwendung des Plugins mysql _ native _ password:
Erstellen Sie nachfolgend eine Datenbank für die Anwendung mit:
Damit haben Sie die Datenbank für diese Anwendung erfolgreich erstellt.
Aktivieren Sie jetzt den Zugriff auf die erstellte Datenbank für den neuen MySQL-Benutzer.
Führen Sie den folgenden Befehl aus, um dem Benutzer alle Berechtigungen in der Datenbank zu erteilen:
Laden Sie die Berechtigungstabelle neu, indem Sie den folgenden Befehl ausführen, um die soeben vorgenommenen Änderungen zu übernehmen:
Beenden Sie den MySQL-Client mit:
Sie haben die AdonisJs CLI erfolgreich installiert, ein neues AdonisJs-Projekt erstellt und mysql über npm installiert.
Außerdem haben Sie die Datenbank für diese Anwendung erstellt und einen MySQL-Benutzer mit den entsprechenden Berechtigungen eingerichtet. Dies ist die Grundeinrichtung für Ihre Anwendung. Im nächsten Abschnitt beginnen Sie mit der Erstellung der erforderlichen Ansichten für Ihre Anwendung.
Schritt 2 - Verwenden der Edge Templating Engine
AdonisJs wird mit einer eigenen Template-Engine namens Edge bereitgestellt.
Sie ermöglicht Ihnen die Erstellung einer wiederverwendbaren HTML-Vorlage und die Einführung von Front-End-Logik in Ihre Anwendung mit minimalem Code.
Edge stellt JavaScript-Entwickler während der Entwicklung einer Anwendung die Werkzeuge zur Verfügung, um ein komponentenbasiertes Layout zu erstellen, Bedingungen zu schreiben, Iterationen zu verwenden und Anzeigeebenen zur Aufnahme der Logik zu erstellen.
Alle Template-Dateien enden mit der Erweiterung .edge und werden im Verzeichnis resources / views gespeichert.
Nachfolgend sind die Ansichten aufgeführt, die Ihre Anwendung für die ordnungsgemäße Funktionsweise benötigt:
Master Layout: Mit Edge können Sie eine Seite erstellen, die das CSS, gängige JavaScript-Dateien, jQuery und gängige Bestandteile der Benutzeroberfläche enthält, die in der gesamten Anwendung gleich bleiben, wie zum Beispiel die Navigationsleiste, das Logo, der Header usw. Sobald Sie die Seite Master Layout erstellt haben, übernehmen andere Ansichten (Seiten) in Ihrer Anwendung diese.
Index View: Diese Seite verwendet das Master Layout, um gängige Dateien zu erben und Inhalte für die Homepage der Anwendung darzustellen.
Login-Seite: Diese Seite verwendet ebenfalls das Master Layout und stellt das Formular mit den Eingabefeldern für Benutzername und Passwort für die Anmeldung der Benutzer dar.
Register-Seite: Hier sehen Benutzer ein Formular zur Registrierung und zur dauerhaften Speicherung der Informationen in der Datenbank.
Create Quote-Seite: Benutzer werden diese Seite zum Erstellen eines inspirierenden Zitats verwenden.
Edit Quote-Seite: Benutzer werden diese Seite zum Bearbeiten eines Zitats verwenden.
View Quote-Seite: Benutzer werden diese Seite zum Anzeigen eines bestimmten Zitats verwenden.
Verwenden Sie zunächst den Befehl adonis, um die Seite Master Layout zu erstellen, indem Sie den folgenden Befehl ausführen:
Dieser Befehl erstellt automatisch eine Datei master.edge in Ihrem Ordner resources / views / layouts.
Öffnen Sie die neue Datei:
Fügen Sie den folgenden Code hinzu:
In dieser Datei schließen Sie die CDN-Dateien für Bootstrap CSS, Bootstrap JavaScript und jQuery ein.
Sie fügen einen globalen CSS-Dateinamen style.css hinzu und schließen innerhalb der div eine Partiale-Datei namens navbar ein.
Um Fragmente des HTML-Codes, die Sie auf mehreren Seiten Ihrer Anwendung benötigen, wie nav oder footer, wiederzuverwenden, können Sie Partiale einfügen.
Dabei handelt es sich um kleinere Dateien, die den sich wiederholenden Code enthalten, sodass der Code für diese Elemente schneller an einer Stelle und nicht bei jedem Auftreten aktualisiert werden kann.
Die navbar enthält Markierungen für die Schaltflächen Login und Register, ein Logo und einen Home-Link.
Auf diese Weise können alle nachfolgenden für diese Anwendung erstellten Seiten das Master-Layout erweitern und die navbar rendern, ohne dass der Inhalt neu geschrieben werden muss.
Sie erstellen diese Datei navbar später im Tutorial.
Abschließend definieren Sie einen Abschnitt-Tag @!
section (), um Inhalte anderer Seiten einzuschließen und sie vom Master-Layout rendern zu lassen.
Damit dies wie erwartet funktioniert, müssen alle neuen Seiten, die das Master-Layout erweitern, auch einen Abschnitt-Tag mit demselben Namen (d. h. @ section ("'" content "'")) definieren.
Speichern und schließen Sie die Datei, sobald Sie mit der Bearbeitung fertig sind.
Als Nächstes verwenden Sie den Befehl adonis, um die Navigationsleiste zu erstellen:
Öffnen Sie die neu erstellte Datei:
Fügen Sie dann den folgenden Code hinzu:
Zusätzlich zur Definition der Links zu der Homepage und einer Schaltfläche zum Registrieren und Anmelden fügen Sie ein Tag @ loggedIn hinzu.
Damit können Sie eine bedingte Anweisung um den authentifizierten Benutzer herum schreiben und bei Bedarf entsprechende Inhalte anzeigen.
Für einen authentifizierten Benutzer zeigt die Anwendung den Benutzernamen und eine Schaltfläche zum Erstellen eines neuen Zitats an.
Wenn ein Benutzer nicht angemeldet ist, zeigt Ihre Anwendung eine Schaltfläche für die Anmeldung oder die Registrierung an.
Diese Seite wird als Partiale auf jeder Seite eingefügt, so wie es zuvor im Master-Layout für diese Anwendung enthalten war.
Nun erstellen Sie die Indexseite, die Sie für die Homepage der Anwendung verwenden.
Sie wird die Liste aller inspirierenden Zitate, die die Benutzer schreiben, rendern und anzeigen:
Sie werden eine Ausgabe sehen, die der folgenden ähnelt:
Die hier erstellte Datei wird sich in resources / views / index.edge befinden.
Öffnen Sie die Datei:
Fügen Sie dann den folgenden Code hinzu:
Hier geben Sie an, dass diese Ansicht das Layout master durch Erweiterung verwenden wird. Diese Seite hat nun Zugriff auf alle Bibliotheken, Stylesheets und die navbar, die im Layout master enthalten sind.
Als Nächstes iterieren Sie über ein Array von quotes (Zitaten) unter Verwendung des integrierten Tags @ each.
Das Array quotes wird von dem QuoteController, den Sie später in diesem Tutorial erstellen werden, an diese Ansicht übergeben.
Wenn keine Zitate vorhanden sind, wird eine entsprechende Meldung angezeigt.
Um die Anmeldeseite zu erstellen, führen Sie nun den folgenden Befehl aus:
Sie werden eine Ausgabe sehen, die der folgenden ähnelt:
Dadurch wird automatisch ein Ordner auth innerhalb von resources / views erstellt und darin ebenfalls eine Datei login.edge angelegt. Öffnen Sie die Datei login.edge:
Diese Datei enthält ein Formular, das Eingabeelemente enthält, mit denen Sie den Benutzernamen und das Passwort eines registrierten Benutzers erfassen, bevor dieser sich erfolgreich authentifizieren und mit der Erstellung von Zitaten beginnen kann.
Ein weiteres wichtiges Element, das Sie auf dieser Seite beachten sollten, ist das {{csrfField ()}} ​ ​ ​.
Es handelt sich dabei um eine globale Variable, die AdonisJs verwendet, um das CSRF-Zugriffstoken zu übergeben, wenn eine POST-, PUT- und DELETE-Anfrage aus Ihrer Anwendung gesendet wird.
Dies wurde eingerichtet, um Ihre Anwendung vor Cross-Site Request Forgery (CSRF) -Angriffen zu schützen.
Es funktioniert, indem es für jeden Benutzer, der Ihre Website besucht, ein eindeutiges CSRF-Geheimnis erzeugt. Sobald Ihre Benutzer eine HTTP-Anfrage vom Frontend aus senden, wird ein entsprechendes Token für dieses Geheimnis generiert und mit der Anfrage weitergegeben.
Auf diese Weise kann die für diese Anfrage innerhalb von AdonisJs erstellte Middleware überprüfen, ob sowohl das Token als auch das CSRF-Geheimnis gültig sind und dem aktuell authentifizierten Benutzer gehören.
Speichern und schließen Sie die Datei, sobald Sie fertig sind.
Als Nächstes erstellen Sie die Registrierungsseite mit diesem Befehl:
Suchen und öffnen Sie die neu erstellte Datei in resources / views / auth / register.edge:
Ähnlich wie die Anmeldeseite enthält diese Datei ein HTML-Formular mit Eingabefeldern zur Erfassung von Name, E-Mail und Passwort eines Benutzers während des Registrierungsvorgangs.
Ebenfalls enthalten ist das {{csrfField ()}}, da es für jede Post-Anfrage für eine AdonisJs-Anwendung erforderlich ist.
Nun erstellen Sie eine neue Datei, um ein inspirierendes Zitat zu erstellen, indem Sie den folgenden Befehl vom Terminal aus ausführen:
Sie sehen eine Ausgabe wie:
Öffnen Sie resources / views / quotes / create-quote.edge:
Und fügen Sie den folgenden Inhalt hinzu:
Diese Seite erweitert das Master-Layout und enthält ein HTML-Formular mit einem Textbereichselement, das es dem Benutzer ermöglicht, Text über mehrere Zeilen einzugeben, bevor er über die entsprechende Route veröffentlicht und gehandhabt wird.
Erstellen Sie als Nächstes eine Seite zur Bearbeitung eines bestimmten Zitats.
Führen Sie den folgenden Befehl vom Terminal aus:
Öffnen Sie die Datei mit:
Fügen Sie den folgenden Inhalt zu resources / views / quotes / edit-quote hinzu:
Diese Seite enthält ähnliche Inhalte wie die Datei create-quote.edge - der Unterschied besteht darin, dass sie die Details eines bestimmten Zitats enthält, die zu bearbeiten sind, < form method = "POST" action = "/ update-quote / {{quote.id}}" >.
Erstellen Sie abschließend eine Seite, um ein einzelnes inspirierendes Zitat anzuzeigen:
Sie sehen eine Ausgabe, die dieser ähnelt:
Diese Seite gibt die Details eines bestimmten Zitats wieder, einschließlich des Zitatkörpers quote.body, und des Autors, der es erstellt hat, quote.username.
Wenn Sie mit der Datei fertig sind, speichern und schließen Sie sie.
Sie haben alle erforderlichen Seiten für Ihre Anwendung mit Hilfe der Edge Templating Engine erstellt.
Als Nächstes konfigurieren und stellen Sie eine Verbindung mit der Datenbank Ihrer Anwendung her.
Schritt 3 - Erstellen eines Datenbankschemas
Wenn Sie Ihre Anwendung jetzt bereitstellen, wird ein Fehler ausgegeben, da Sie die Anwendung noch mit einer Datenbank verbinden müssen.
In diesem Abschnitt richten Sie eine Verbindung mit der Datenbank ein und verwenden dann den Befehl adonis zur Erzeugung einer Migrationsdatei, die zur Erstellung der Tabellen für die Datenbank verwendet wird.
AdonisJs enthält ein ORM namens Lucid ORM, das eine aktive Datensatz-Implementierung für die Arbeit mit Ihrer Datenbank bereitstellt.
Damit entfällt das mühsame Schreiben von SQL-Abfragen, die Daten in Echtzeit aus der Datenbank abrufen.
Dies ist besonders hilfreich bei der Arbeit an einer komplexen Anwendung, die viele Abfragen erfordert.
So können Sie beispielsweise alle Zitate aus Ihrer Anwendung abrufen, indem Sie eingeben:
Um mit der entsprechenden Konfiguration für Ihre Anwendungsdatenbank fortzufahren, stellen Sie sicher, dass Sie sich immer noch im Stammverzeichnis Ihrer Anwendung befinden und erstellen Sie eine .env-Datei:
Öffnen Sie die neu erstellte Datei und fügen Sie den folgenden Inhalt hinzu:
Standardmäßig ist die Datenbankverbindung für eine AdonisJs-Anwendung SQLite, die Sie hier auf MySQL aktualisieren werden.
Des Weiteren geben Sie den PORT für die Anwendung, die Anwendungsumgebung und Berechtigungsnachweise für die Datenbank an.
Stellen Sie sicher, dass Sie die Platzhalter DB _ USER, DB _ PASSWORD und DB _ DATABASE mit Ihren Berechtigungsnachweisen ersetzen.
Als Nächstes erstellen Sie das Modell und eine Migrationsdatei für Quote unter Verwendung der Adonis CLI.
Führen Sie dazu den folgenden Befehl aus:
Sie sehen eine Ausgabe, die der folgenden ähnelt:
Dieser Befehl erstellt ein Modell für Quote im Ordner app / Models und eine Schemadatei in dem Ordner database / migrations.
Der neu erstellten Schemadatei wird der aktuelle Zeitstempel vorangestellt.
Öffnen Sie die Schemadatei mit:
Aktualisieren Sie den Inhalt mit dem folgenden Code:
Eine Schemadatei in AdonisJs erfordert zwei verschiedene Methoden:
up: Wird verwendet, um eine neue Tabelle zu erstellen oder eine bestehende Tabelle zu ändern.
down: Wird verwendet, um die bei der Methode up vorgenommene Änderung rückgängig zu machen.
Zusätzlich zu den Feldern timestamps () und increments () aktualisieren Sie den Inhalt der Schemadatei mit den Feldattributen user _ id, username und dem body eines zu erstellenden Zitats.
Die Felder user _ id und username verweisen auf die Details des Benutzers, der ein bestimmtes Zitat erstellt.
Dies definiert eine 1: n-Beziehung und bedeutet, dass ein Benutzer eine unendliche Anzahl von Zitaten besitzen kann, während ein einzelnes Zitat nur einem Benutzer gehören kann.
AdonisJs wird standardmäßig mit Modell User und der zugehörigen Migrationsdatei installiert. Dies erfordert nur eine kleine Änderung, um die Beziehung zwischen dem Modell User und Quote herzustellen.
Öffnen Sie das Modell User in app / Models / User.js:
Fügen Sie diese Methode unmittelbar nach der Methode tokens () hinzu:
Dadurch wird eine 1: n-Beziehung mit der Tabelle Quote unter Verwendung von user _ id als Fremdschlüssel festgelegt.
Um diesen Abschnitt abzuschließen, verwenden Sie den folgenden Befehl zur Ausführung von Migrationen, wodurch die Methode up () aller Migrationsdateien ausgeführt wird:
Sie haben eine Verbindung mit Ihrer Datenbank konfiguriert und gesichert.
Des Weiteren haben Sie ein Modell Quote und die dazugehörige Schemadatei erstellt und eine 1: n-Beziehung zwischen User und Quote hergestellt.
Als Nächstes generieren Sie die Routen und Controller zur Bearbeitung von HTTP-Anfragen und die Geschäftslogik zum Erstellen, Bearbeiten und Löschen eines inspirierenden Zitats.
Schritt 4 - Erstellen von Controllern und Einrichten von Routen
In diesem Abschnitt beginnen Sie mit der Erstellung von Controllern, die die gesamte Logik für die Anwendung verarbeiten und diese später an eine bestimmte Route anhängen, damit die Benutzer über eine URL darauf zugreifen können.
Zu Beginn werden Sie mit der Adonis CLI einen neuen HTTP-Request-Controller erstellen, der alle Authentifizierungsprozesse für Ihre Anwendung verarbeiten kann, indem Sie den folgenden Befehl ausführen:
Dieser Befehl erstellt eine Datei AuthController.js und speichert sie innerhalb des Ordners app / Controllers / Http.
Sie verwenden das Flag --type, um anzugeben, dass dieser Controller ein HTTP-Controller sein soll.
Öffnen Sie als Nächstes die neu erstellte Controller-Datei:
Aktualisieren Sie sie mit dem folgenden Inhalt:
In dieser Datei importieren Sie das Modell User und erstellen dann zwei Methoden namens loginView () und registerView (), um die Anmelde- bzw. Registrierungsseite zu rendern.
Abschließend erstellen Sie die folgenden asynchronen Methoden:
postLogin (): Mit dieser Methode erhalten Sie den Wert von email und password, die mit Hilfe der in AdonisJs eingebauten Methode request gepostet wurden, und validieren dann diesen Benutzer anhand der Details in der Datenbank.
Wenn ein solcher Benutzer in der Datenbank existiert und den korrekten Berechtigungsnachweis eingegeben hat, wird er zurück auf die Homepage umgeleitet und authentifiziert, bevor er ein neues Zitat erstellen kann.
Andernfalls wird eine Meldung angezeigt, die auf den falschen Berechtigungsnachweis hinweist.
postRegister (): Dies erhält den Wert von username, email und password für einen Benutzer, um ein Konto für diesen Benutzer in der Datenbank zu erstellen.
Eine Nachricht mit der Angabe, dass dieser Benutzer erfolgreich erstellt wurde, wird an die Sitzung weitergeleitet, und der Benutzer wird auf die Anmeldeseite umgeleitet, um authentifiziert zu werden und mit der Erstellung eines Zitats zu beginnen.
logout (): Diese Methode verarbeitet die Logout-Funktionalität und leitet den Benutzer zurück auf die Homepage.
Nachdem Sie nun den Controller für die Registrierung und Authentifizierung von Benutzern eingerichtet haben, fahren Sie mit der Erstellung eines HTTP-Request-Controllers für die Verwaltung aller Operationen in Bezug auf Zitate fort.
Führen Sie, zurück im Terminal, den folgenden Befehl aus, um den QuoteController zu erstellen:
Die Verwendung des Flags --resource erstellt einen Controller mit vordefinierten kreativen Methoden zur Durchführung von CRUD (Create, Read, Update und Delete) -Operationen.
Sie sehen:
Finden Sie diese Datei innerhalb von app / Controllers / Http / QuoteControllers.js:
In diesem Controller haben Sie das Modell Quote importiert und die folgenden Methoden aktualisiert, die automatisch mit AdonisJs CLI erstellt wurden:
index (): Um alle Zitate aus der Datenbank zu holen und sie auf der Homepage der Anwendung darzustellen.
create (): Um eine Seite zum Erstellen von Zitaten darzustellen.
store (): Um ein neu erstelltes Zitat dauerhaft in der Datenbank zu speichern und eine entsprechende Antwort zurückzugeben.
show (): Um die id eines bestimmten Zitats zu erhalten, es aus der Datenbank abzurufen und auf der Seite zur Bearbeitung von Zitaten anzuzeigen.
edit (): Um Details eines bestimmten Zitats aus der Datenbank abzurufen und zur Bearbeitung darzustellen.
update (): Um jede Aktualisierung eines Zitats zu verarbeiten und den Benutzer zurück auf die Homepage zu leiten.
destroy (): Um ein bestimmtes Zitat zu löschen und vollständig aus der Datenbank zu entfernen.
Nachdem Sie alle erforderlichen Controller für diese Anwendung erstellt haben, können Sie nun die Routen so einrichten, dass die Benutzer problemlos mit Ihrer Anwendung interagieren können.
Navigieren Sie zu Beginn zu der Datei start / routes.js:
Ersetzen Sie den Inhalt mit dem folgenden:
Hier definieren Sie den Pfad für jede Route in Ihrer Anwendung, geben die HTTP-Verben für jede Aktion an und binden die Route an eine bestimmte Methode in jedem Controller.
Außerdem benennen Sie jede dieser Routen so, wie sie in den Controllern und Ansichten referenziert wurden.
Um sicherzustellen, dass nur authentifizierte Benutzer auf alle Zitatrouten zugreifen können, ordnen Sie eine Gruppe namens Middleware zu. Schließlich hängen Sie eine Validierer-Methode an die Route register.store an, um die Benutzereingaben zu validieren.
Sie haben Ihre Controller erstellt und die Routen für Ihre Anwendung eingerichtet.
Als Nächstes erstellen Sie die in diesem Schritt definierte Validierer-Methode.
Schritt 5 - Validieren der Benutzereingabe
Standardmäßig verfügt AdonisJs nicht über integrierte Validierer.
Daher müssen Sie den Validierer für Ihre Anwendung manuell installieren und registrieren.
Führen Sie den folgenden Befehl aus, um ihn zu installieren:
Öffnen Sie die folgende Datei, um den Anbieter des Validieres (Validator Provider) zu registrieren:
Registrieren Sie dann den Anbieter des Validieres, indem Sie ihn an die Liste der Anbieter anhängen, wie nachfolgend gezeigt:
Nachdem Sie den Anbieter des Validieres innerhalb Ihrer Anwendung installiert und registriert haben, erstellen Sie nun mit dem folgenden Befehl einen benutzerdefinierten Validierer, um die Eingabe des Benutzers während der Registrierung zu validieren:
Dadurch wird eine Datei Register.js im Verzeichnis App / Validators erstellt.
In Ihrer Anwendung definieren Sie Regeln für bestimmte Felder.
Wenn Validierungen zu einem beliebigen Zeitpunkt fehlschlagen, setzt der Validierer den Fehler automatisch als Flash-Meldung und der Benutzer wird zurück zum Formular geleitet.
Speichern und schließen Sie die Datei, sobald Sie die Bearbeitung abgeschlossen haben.
Um Ihrer Anwendung ein Design hinzuzufügen, öffnen Sie schließlich die folgende Datei:
Ersetzen Sie die Inhalte mit dem folgenden:
In dieser Datei aktualisieren Sie das CSS-Design Ihrer Anwendung in der Datei style.css.
Sie haben einen Anbieter eines Validieres installiert und registriert, um die Eingabe von Benutzern während des Registrierungsvorgangs zu überprüfen.
Des Weiteren haben Sie den Inhalt Ihres Stylesheets aktualisiert, um der Anwendung mehr Design hinzuzufügen.
Im letzten Schritt testen Sie Ihre Anwendung.
Schritt 6 - Bereitstellen der Anwendung
In diesem Schritt stellen Sie Ihre Anwendung bereit und erstellen einen Benutzer und ein Passwort zum Testen der Authentifizierung.
Außerdem fügen Sie Ihrer Anwendung ein Zitate hinzu und zeigen es auf der Homepage an.
Um Ihre Anwendung zu testen, starten Sie den Entwicklungsserver mit dem folgenden Befehl aus dem Stammverzeichnis Ihrer Anwendung:
Dadurch wird die Anwendung auf dem innerhalb der Stammdatei .env definierten Port, 3333, gestartet.
Navigieren Sie in Ihrem Browser zu http: / / localhost: 3333.
Homepage der Zitat-Anwendung
Die Homepage ist zur Zeit leer, da Sie keine Zitate erstellt haben.
Klicken Sie auf die Schaltfläche Register.
Anmeldeseite
Geben Sie Ihre Informationen ein und klicken Sie auf die Schaltfläche Submit, um den Registrierungsvorgang abzuschließen.
Sie werden auf die Anmeldeseite umgeleitet.
Geben Sie Ihre E-Mail-Adresse und Ihr Passwort für die Authentifizierung ein.
Anmeldeseite
Nachdem Sie authentifiziert sind, klicken Sie auf die Schaltfläche Create Quote.
Seite "Zitat erstellen"
Geben Sie ein Zitat ein und navigieren Sie zu der Seite View all, um Ihr Zitat anzuzeigen.
Seite "Alle Zitate anzeigen"
Sie haben Ihre Anwendung getestet, indem Sie einen Benutzer erstellt und authentifiziert und anschließend ein Zitat verfasst haben.
In diesem Tutorial haben Sie eine Webanwendung mit AdonisJs erstellt.
Sie haben die Anwendung unter Verwendung der AdonisJs CLI eingerichtet und die CLI zum Erstellen anderer relevanter Dateien wie Controller, Modelle und Ansichten verwendet.
Sie können Webanwendungen mit diesem Framework unabhängig von Ihrer Größe und Komplexität erstellen.
Sie können den Quellcode für dieses Projekt hier auf GitHub herunterladen.
Um weitere Funktionen zu erkunden, können Sie auch die offizielle Dokumentation besuchen.
Wenn Sie einige unserer anderen JavaScript-Framework-Tutorials erkunden möchten, sehen Sie sich Folgendes an:
Erstellen einer Anwendung zur Verwaltung von Kundenlisten mit React und TypeScript
Erstellen eines Blogs mit Nest.js, MongoDB und Vue.js
Erstellen einer Wetter-App mit Angular, Bootstrap und der APIXU-API
Erstellen eines neuen Sudo-fähigen Benutzers unter CentOS 8 Schnellstart
4002
Dieser Leitfaden zeigt Ihnen, wie Sie unter CentOS 8 einen neuen Benutzer mit sudo-Zugriff erstellen, ohne die Datei / etc / sudoers Ihres Servers ändern zu müssen.
Wenn Sie sudo für einen bestehenden CentOS-Benutzer konfigurieren möchten, springen Sie zu Schritt 3.
Verwenden Sie die IP-Adresse oder den Hostnamen Ihres Servers anstelle von < ^ > your _ server _ ip _ address < ^ > oben.
Stellen Sie sicher, dass Sie < ^ > sammy < ^ > durch den Benutzernamen ersetzen, den Sie erstellen möchten.
Verwenden Sie den Befehl passwd, um das Passwort des neuen Benutzers zu aktualisieren:
Vergessen Sie nicht, < ^ > sammy < ^ > durch den Benutzer zu ersetzen, den Sie gerade erstellt haben.
Sie werden zweimal zur Eingabe eines neuen Passworts aufgefordert:
Schritt 3 - Hinzufügen des Benutzers zur Gruppe wheel
Verwenden Sie den Befehl usermod, um den Benutzer der Guppe wheel hinzuzufügen:
Vergewissern Sie sich erneut, dass Sie < ^ > sammy < ^ > durch den Benutzernamen ersetzen, dem Sie sudo-Berechtigungen gewähren möchten.
Standardmäßig haben unter CentOS alle Mitglieder der Gruppe wheel vollen sudo-Zugriff.
Um zu testen, ob die neuen sudo-Berechtigungen funktionieren, verwenden Sie zunächst den Befehl su, um vom Root-Benutzer zum neuen Benutzerkonto zu wechseln:
Wenn Sie sudo in einer Sitzung zum ersten Mal verwenden, werden Sie zur Eingabe des Passworts für dieses Benutzerkonto aufgefordert.
Geben Sie das Passwort des sudo-aktivierten Benutzers ein, nicht das Root-Passwort.
In diesem Schnellstart-Tutorial haben wir ein neues Benutzerkonto erstellt und der Gruppe wheel hinzugefügt, um sudo-Zugriff zu aktivieren.
Detaillierte Informationen zur Einrichtung eines CentOS 8-Servers finden Sie in unserem Tutorial zur Ersteinrichtung eines Servers unter CentOS 8.
Installieren von Discourse unter Ubuntu 18.04
3984
Ein Artikel von Discourse
Discourse ist eine Open-Source-basierte Diskussionsplattform.
Sie kann als Mailingliste, Diskussionsforum oder Langform-Chatraum verwendet werden.
In diesem Tutorial installieren wir Discourse in einer isolierten Umgebung unter Verwendung von Docker, einer Anwendung für Containerisierung.
Bevor wir beginnen, gibt es einige Dinge, die wir zuerst einrichten müssen:
Einen Ubuntu 18.04-Server mit mindestens 2 GB RAM, der gemäß der Ersteinrichtung eines Servers unter Ubuntu 18.04 eingerichtet wurde, einschließlich eines sudo non-root users und einer Firewall.
Eine auf Ihrem Server installierte Docker-Anwendung; das können Sie tun, indem Sie Schritt 1 des Docker-Installations-Tutorials für Ubuntu 18.04 befolgen.
Einen Domänennamen, der auf Ihren Server auflöst, den Sie gemäß diesem Hostnamen-Tutorial einrichten können.
Einen SMTP-Mail-Server.
Wenn Sie nicht Ihren eigenen Mailserver ausführen möchten, können Sie einen anderen Dienst nutzen, wie ein kostenloses Konto bei Mailgun.
Anmerkung: Discourse erfordert eine Auslagerungsdatei, wenn Sie 1 GB RAM verwenden.
Zwar wird Auslagerung im Allgemeinen für Systeme empfohlen, die traditionelle Festplatten verwenden, doch kann es bei SSDs zu Problemen mit Hardwarebeeinträchtigungen kommen.
Daher empfehlen wir, Auslagerung bei DigitalOcean oder anderen Anbietern, die SSD-Speicher verwenden, nicht zu aktivieren.
Eine Aktivierung kann die Zuverlässigkeit der zugrunde liegenden Hardware für Sie und Ihre Nachbarn beeinträchtigen.
Wir empfehlen daher mindestens 2 GB RAM, um Discourse in einem DigitalOcean-Droplet auszuführen.
Weitere Informationen zur Verwendung von Auslagerung finden Sie unter Hinzufügen von Auslagerungsplatz unter Ubuntu 18.04.
Schritt 1 - Herunterladen von Discourse
Wenn alle Voraussetzungen erfüllt sind, können Sie mit der Installation von Discourse fortfahren.
Sie müssen den Rest des Einrichtungs- und Bootstrap-Verfahrens als root user ausführen; wechseln Sie daher zunächst in eine Root-Shell.
Erstellen Sie als Nächstes das Verzeichnis / var / discourse, in dem alle mit Discourse verbundenen Dateien enthalten sein werden.
Klonen Sie abschließend das offizielle Discourse-Docker-Image in / var / discourse.
Da die von uns benötigten Dateien nun vorhanden sind, können wir mit der Konfiguration und dem Bootstrapping fortfahren.
Schritt 2 - Konfigurieren und Starten von Discourse
Rufen Sie das Verzeichnis / var / discourse auf, in dem sich die Discourse-Dateien befinden.
Von hier aus können Sie das enthaltene Einrichtungsskript starten.
Ihnen werden die folgenden Fragen gestellt:
Hostname für Ihren Discourse?
Geben Sie den Hostnamen ein, den Sie für Discourse verwenden möchten, z. B. discourse. < ^ > your _ domain.com < ^ >, wobei Sie < ^ > your _ domain.com < ^ > durch Ihren Domänennamen ersetzen.
Sie müssen einen Domänennamen verwenden, da eine IP-Adresse beim Senden von E-Mail nicht funktioniert.
E-Mail-Adresse für Administratorkonto?
Wählen Sie die E-Mail-Adresse aus, die Sie für das Discourse-Administratorkonto verwenden möchten.
Diese kann mit Ihrer Discourse-Domäne völlig unverbunden sein und jede E-Mail-Adresse sein, die Sie für praktisch erachten.
Beachten Sie, dass diese E-Mail-Adresse standardmäßig zum Discourse-Administrator wird, wenn sich der erste Benutzer mit dieser E-Mail registriert.
Sie benötigen diese E-Mail-Adresse auch dann, wenn Sie Discourse über dessen Weboberfläche einrichten.
Adresse des SMTP-Servers?
SMTP-Benutzername?
SMTP-Port?
SMTP-Passwort?
Geben Sie Ihre SMTP-Serverdetails für diese Fragen ein.
Wenn Sie Mailgun verwenden, lautet die Adresse des SMTP-Servers smtp.mailgun.org; der Benutzername und das Passwort sind SMTP-Anmeldeinformationen für Ihre Domäne unter der Registerkarte Domänen.
Schließlich werden Sie aufgefordert, alle gerade eingegebenen Einstellungen zu bestätigen.
Nachdem Sie Ihre Einstellungen bestätigt haben, generiert das Skript eine Konfigurationsdatei namens app.yml und startet den Bootstrap-Prozess.
Anmerkung: Wenn Sie diese Einstellungen nach dem Bootstrapping ändern oder korrigieren müssen, bearbeiten Sie Ihre Datei / containers / app.yml und führen Sie. / launcher rebuild app aus. Andernfalls werden Ihre Änderungen nicht wirksam.
Das Bootstrapping dauert zwischen 2 und 8 Minuten; danach wird Ihre Instanz ausgeführt.
Fahren wir nun mit der Erstellung eines Administratorkontos fort.
Schritt 3 - Registrieren eines Administratorkontos
Rufen Sie Ihre Discourse-Domäne in Ihrem bevorzugten Webbrowser auf, um die Discourse-Webseite anzuzeigen.
Herzlichen Glückwunsch
Wenn Sie einen 502-Fehler (Ungültiges Gateway) erhalten, warten Sie ein oder zwei Minuten und aktualisieren Sie dann; der Startvorgang von Discourse ist möglicherweise noch nicht abgeschlossen.
Wenn die Seite geladen ist, klicken Sie auf die blaue Schaltfläche Registrieren.
Sie sehen ein Formular mit dem Titel Administratorkonto registrieren, das folgende Felder aufweist:
E-Mail-Adresse: Wählen Sie die zuvor von Ihnen angegebene E-Mail-Adresse aus dem Dropdownmenü aus.
Benutzername: Wählen Sie einen Benutzernamen.
Passwort: Wählen Sie ein starkes Passwort.
Klicken Sie dann im Formular auf die blaue Schaltfläche Registrieren, um es zu übermitteln. Sie sehen ein Dialogfeld mit dem Inhalt E-Mail-Adresse bestätigen.
Überprüfen Sie Ihren Posteingang auf die Bestätigungs-E-Mail.
Wenn Sie sie nicht erhalten haben, klicken Sie auf die Schaltfläche Aktivierungs-E-Mail erneut senden.
Wenn Sie immer noch kein neues Administratorkonto registrieren können, lesen Sie bitte die Fehlerbehebungs-Checkliste für E-Mail von Discourse.
Nach der Registrierung Ihres Administratorkontos startet der Einrichtungsassistent und leitet Sie durch die grundlegende Konfiguration von Discourse.
Sie können sie nun durchlaufen oder auf Vielleicht später klicken, um diesen Schritt zu überspringen.
Assistent
Nach dem Abschließen oder Überspringen des Einrichtungsassistenten werden Ihnen einige Themen sowie der Leitfaden Admin-Schnellstart (namens READ ME FIRST) angezeigt, die Tipps zur weiteren Anpassung Ihrer Discourse-Installation enthalten.
Startseite
Sie sind fertig!
Wenn Sie Discourse in Zukunft aktualisieren müssen, können Sie dies über die Befehlszeile tun, indem Sie die neueste Version des Codes aus dem Git-Repository beziehen und die App so neu erstellen:
Sie können Discourse auch in Ihrem Browser aktualisieren, indem Sie http: / / discourse. < ^ > your _ domain.com < ^ > / admin / upgrade aufrufen, auf Auf die neueste Version aktualisieren klicken und die Anweisungen befolgen.
Aktualisieren
Sie können nun mit dem Verwalten Ihres Discourse-Forums starten und Benutzern die Möglichkeit bieten, sich anzumelden.
Erfahren Sie mehr über die Funktionen von Discourse auf der Seite Über Discourse.
Einrichten eines React-Projekts mit Create React App
3994
React ist ein beliebtes JavaScript-Framework für die Erstellung von Frontend-Anwendungen.
Ursprünglich von Facebook entwickelt, hat es an Popularität gewonnen, da es Entwicklern die Erstellung schneller Anwendungen unter Verwendung eines intuitiven Programmierparadigmas ermöglicht, das JavaScript mit einer HTML-ähnlichen Syntax, bekannt als JSX, verbindet.
Der Start eines neuen React-Projekts war früher ein komplizierter, mehrstufiger Prozess, der die Einrichtung eines Build-Systems, eines Code-Transpilers zur Umwandlung der modernen Syntax in für alle Browser lesbaren Code und einer Basis-Verzeichnisstruktur umfasste.
Aber jetzt enthält Create React App alle JavaScript-Pakete, die Sie zum Ausführen eines React-Projekts benötigen, einschließlich Code-Transpiling, grundlegendes Linting, Testen und Build-Systeme.
Es enthält auch einen Server mit Hot Reloading, der Ihre Seite aktualisiert, wenn Sie Code-Änderungen vornehmen.
Schließlich wird eine Struktur für Ihre Verzeichnisse und Komponenten erstellt, sodass Sie in wenigen Minuten einsteigen und mit der Codierung beginnen können.
Mit anderen Worten, Sie müssen sich nicht um die Konfiguration eines Build-Systems wie Webpack kümmern.
Sie brauchen Babel nicht einzurichten, um Ihren Code so zu transponieren, dass er browserübergreifend nutzbar ist.
Sie müssen sich keine Gedanken über die meisten komplizierten Systeme der modernen Front-End-Entwicklung machen.
Sie können mit minimaler Vorbereitung mit dem Schreiben von React-Code beginnen.
Am Ende dieses Tutorials werden Sie eine laufende React-Anwendung haben, die Sie als Grundlage für alle zukünftigen Anwendungen verwenden können.
Sie werden die ersten Änderungen am React-Code vornehmen, Stile aktualisieren und einen Build ausführen, um eine vollständig minimierte Version Ihrer Anwendung zu erstellen.
Weiterhin werden Sie einen Server mit Hot-Reloading für sofortiges Feedback nutzen und die Teile eines React-Projekts eingehend untersuchen.
Schließlich werden Sie damit beginnen, benutzerdefinierte Komponenten zu schreiben und eine Struktur zu erstellen, die mit Ihrem Projekt wachsen und sich anpassen kann.
Um diesem Tutorial zu folgen, benötigen Sie Folgendes:
Auf Ihrem Computer installiertes Node.js Version 10.16.0.
Es wird auch dazu beitragen, Ihnen ein Grundverständnis über JavaScript zu vermitteln, das Sie in der Reihe Codieren mit JavaScript finden können, zusammen mit dem grundlegenden Wissen über HTML und CSS.
Schritt 1 - Erstellen eines neuen Projekts mit der Create React App
In diesem Schritt erstellen Sie eine neue Anwendung unter Verwendung des npm-Paketmanagers zur Ausführung eines Remote-Skripts.
Das Skript kopiert die erforderlichen Dateien in ein neues Verzeichnis und installiert alle Abhängigkeiten.
Bei der Installation von Node haben Sie auch eine Paketverwaltungsanwendung namens npm installiert. npm installiert JavaScript-Pakete in Ihrem Projekt und verfolgt auch die Details des Projekts.
Wenn Sie mehr über npm erfahren möchten, werfen Sie einen Blick in unser Tutorial Verwenden von Node.js-Modulen mit npm und package.json.
npm enthält auch ein Tool namens npx, das ausführbare Pakete ausführt.
Das bedeutet, dass Sie den Code von Create React App ausführen, ohne zuerst das Projekt herunterzuladen.
Das ausführbare Paket führt die Installation von create-react-app in das von Ihnen angegebene Verzeichnis aus.
Es beginnt mit der Erstellung eines neuen Projekts in einem Verzeichnis, das in diesem Tutorial < ^ > digital-ocean-tutorial < ^ > genannt wird.
Auch dieses Verzeichnis muss vorher nicht existieren; das ausführbare Paket wird es für Sie erstellen.
Das Skript führt auch npm install innerhalb des Projektverzeichnisses aus, das alle zusätzlichen Abhängigkeiten herunterlädt.
Um das Basisprojekt zu installieren, führen Sie den folgenden Befehl aus:
Dieser Befehl löst einen Build-Prozess aus, der den Basiscode zusammen mit einer Reihe von Abhängigkeiten herunterlädt.
Wenn das Skript beendet ist, sehen Sie eine Erfolgsmeldung, die besagt:
< ^ > your _ file _ path < ^ > wird Ihr aktueller Pfad sein.
Wenn Sie ein Benutzer von macOS sind, wird er in etwa / Users / < ^ > your _ username < ^ > lauten, wenn Sie sich auf einem Ubuntu-Server befinden, wird er in etwa / home / < ^ > your _ username < ^ > ​ ​ ​ lauten.
Sie werden auch eine Liste von npm-Befehlen sehen, die Ihnen ermöglichen, Ihre Anwendung auszuführen, zu erstellen, zu starten und zu testen.
Sie werden diese im nächsten Abschnitt näher untersuchen.
< $> note Anmerkung: Es gibt einen weiteren Paketmanager für JavaScript namens yarn.
Er wird von Facebook unterstützt und führt viele der gleichen Dinge wie npm durch.
Ursprünglich bot yarn neue Funktionen wie Sperrdateien, aber jetzt sind diese auch in npm implementiert. yarn enthält auch einige andere Funktionen wie das Offline-Caching.
Weitere Unterschiede finden Sie in der Dokumentation zu yarn.
Wenn Sie zuvor yarn auf Ihrem System installiert haben, sehen Sie eine Liste der yarn-Befehle, wie yarn start, die genauso funktionieren wie npm-Befehle.
Sie können npm-Befehle ausführen, selbst wenn Sie yarn installiert haben.
Wenn Sie yarn bevorzugen, ersetzen Sie in allen zukünftigen Befehlen npm einfach durch yarn.
Die Ergebnisse werden die gleichen sein.
Ihr Projekt ist nun in einem neuen Verzeichnis eingerichtet.
Wechseln Sie in das neue Verzeichnis:
Sie befinden sich nun in dem Stammverzeichnis Ihres Projekts.
An diesem Punkt haben Sie ein neues Projekt erstellt und alle Abhängigkeiten hinzugefügt.
Allerdings haben Sie noch keine Maßnahmen ergriffen, um das Projekt auszuführen.
Im nächsten Abschnitt werden Sie benutzerdefinierte Skripte ausführen, um das Projekt zu erstellen und zu testen.
Schritt 2 - Verwendung von react-scripts
In diesem Schritt lernen Sie die verschiedenen react-scripts kennen, die mit dem Repo installiert werden.
Zuerst führen Sie das Skript test zur Ausführung des Testcodes aus.
Dann werden Sie das Skript build ausführen, um eine minimierte Version zu erstellen.
Schließlich sehen Sie sich an, wie das Skript eject Ihnen die vollständige Kontrolle über die Anpassung geben kann.
Nachdem Sie sich nun im Projektverzeichnis befinden, sehen Sie sich um.
Sie können entweder das gesamte Verzeichnis in Ihrem Texteditor öffnen, oder, wenn Sie sich auf dem Terminal befinden, können Sie die Dateien mit dem folgenden Befehl auflisten:
Das Flag -a stellt sicher, dass die Ausgabe auch versteckte Dateien enthält.
So oder so, Sie werden eine Struktur wie diese sehen:
Erklären wir diese eine nach der anderen:
node _ modules / enthält alle externen JavaScript-Bibliotheken, die von der Anwendung verwendet werden.
Sie werden sie nur selten öffnen müssen.
Das Verzeichnis public / enthält einige grundlegende HTML-, JSON- und Image-Dateien.
Dies sind die Stammverzeichnisse Ihres Projekts.
In Schritt 4 werden Sie Gelegenheit haben, sie näher zu erforschen.
Das Verzeichnis src / enthält den React JavaScript-Code für Ihr Projekt.
Der Großteil Ihrer Arbeit wird in diesem Verzeichnis erfolgen.
In Schritt 5 werden Sie dieses Verzeichnis ausführlich erkunden.
Die Datei .gitignore enthält einige Standardverzeichnisse und -dateien, die von git - ihrer Quellkontrolle - ignoriert werden, wie z. B. das Verzeichnis node _ modules.
Bei den ignorierten Elementen handelt es sich in der Regel um größere Verzeichnisse oder Protokolldateien, die Sie in der Versionsverwaltung nicht benötigen.
Es wird auch einige Verzeichnisse enthalten, die Sie mit einigen der React-Skripte erstellen werden.
README.md ist eine Markdown-Datei, die viele nützliche Informationen über Create React App enthält, wie z. B. eine Zusammenfassung der Befehle und Links zur erweiterten Konfiguration.
Im Moment ist es am besten, die Datei README.md so zu belassen, wie Sie sie sehen. Wenn Ihr Projekt fortschreitet, werden Sie die Standardinformationen durch detailliertere Informationen über Ihr Projekt ersetzen.
Die beiden letzten Dateien werden von Ihrem Paketmanager verwendet.
Wenn Sie den anfänglichen npx-Befehl ausgeführt haben, haben Sie das Basisprojekt erstellt, aber auch die zusätzlichen Abhängigkeiten installiert.
Als Sie die Abhängigkeiten installiert haben, haben Sie eine Datei package-lock.json erstellt.
Diese Datei wird von npm verwendet, um sicherzustellen, dass die Pakete mit exakten Versionen übereinstimmen.
Wenn eine andere Person Ihr Projekt installiert, können Sie auf diese Weise sicherstellen, dass sie identische Abhängigkeiten haben.
Da diese Datei automatisch erzeugt wird, werden Sie diese Datei selten direkt bearbeiten.
Die letzte Datei ist eine package.json.
Diese enthält Metadaten über Ihr Projekt, wie den Titel, die Versionsnummer und die Abhängigkeiten.
Sie enthält auch Skripte, die Sie zur Ausführung Ihres Projekts verwenden können.
Öffnen Sie die Datei package.json in Ihrem bevorzugten Texteditor:
Wenn Sie die Datei öffnen, sehen Sie ein JSON-Objekt, das alle Metadaten enthält.
Wenn Sie sich das Objekt scripts ansehen, finden Sie vier verschiedene Skripte: start, build, test und eject.
Diese Skripte sind in der Reihenfolge ihrer Wichtigkeit aufgelistet.
Das erste Skript startet die lokale Entwicklungsumgebung; dazu kommen Sie im nächsten Schritt.
Das zweite Skript wird Ihr Projekt erstellen.
Sie werden dies in Schritt 4 im Detail untersuchen, aber es lohnt sich, es jetzt auszuführen, um zu sehen, was passiert.
Das Skript build
Um ein beliebiges nmp-Skript auszuführen, müssen Sie nur npm run < ^ > script _ name < ^ > in Ihr Terminal eingeben.
Es gibt ein paar spezielle Skripte, bei denen Sie den run-Teil des Befehls weglassen können, aber es ist immer in Ordnung, den vollständigen Befehl auszuführen.
Um das Skript build auszuführen, geben Sie Folgendes in Ihr Terminal ein:
Sie sehen sofort die folgende Meldung:
Dies sagt Ihnen, dass Create React App Ihren Code in ein nutzbares Bundle kompiliert.
Nach Fertigstellung sehen Sie die folgende Ausgabe:
Listen Sie die Projektinhalte auf und Sie werden einige neue Verzeichnisse sehen:
Sie haben nun ein Verzeichnis build.
Wenn Sie die Datei .gitignore geöffnet haben, haben Sie vielleicht bemerkt, dass das Verzeichnis build von git ignoriert wird.
Das liegt daran, dass das Verzeichnis build nur eine minimierte und optimierte Version der anderen Dateien ist.
Es ist nicht erforderlich, die Versionskontrolle zu verwenden, da Sie immer den Befehl build ausführen können.
Sie werden die Ausgabe später erkunden; vorerst ist es an der Zeit, zu dem Skript test überzugehen.
Das Skript test
Das Skript test ist eines dieser speziellen Skripte, die das Schlüsselwort run nicht erfordern, aber auch dann funktionieren, wenn Sie es einschließen. Dieses Skript startet einen Testläufer namens Jest.
Der Testläufer durchsucht Ihr Projekt nach Dateien mit der Erweiterung .spec.js oder .test.js. und führt diese Dateien dann aus.
Zur Ausführung des Skripts test geben Sie den folgenden Befehl ein:
Nach Ausführung dieses Skripts hat Ihr Terminal die Ausgabe der Testsuite, und die Terminal-Eingabeaufforderung verschwindet.
Hier gibt es einige Dinge zu beachten.
Erstens erkennt es, wie bereits erwähnt, automatisch alle Dateien mit Test-Erweiterungen einschließlich .test.js und .spec.js.
In diesem Fall gibt es nur eine Testsuite - d. h. nur eine Datei mit der Erweiterung .test.js - und diese Testsuite enthält nur einen Test.
Jest kann Tests in Ihrer Code-Hierarchie erkennen, sodass Sie Tests in einem Verzeichnis verschachteln können und Jest wird sie finden.
Zweitens führt Jest Ihre Testsuite nicht einmal aus und beendet sich dann.
Vielmehr wird es im Terminal weiter ausgeführt.
Wenn Sie Änderungen im Quellcode vornehmen, führt es die Tests erneut aus.
Sie können auch einschränken, welche Tests Sie ausführen, indem Sie eine der Tastaturoptionen verwenden.
Wenn Sie z. B. o eingeben, werden die Tests nur für Dateien ausgeführt, die sich geändert haben.
Dies kann Ihnen viel Zeit sparen, wenn Ihre Testsuiten wachsen.
Schließlich können Sie den Testläufer durch Eingabe von q beenden. Tun Sie dies jetzt, um Ihre Eingabeaufforderung wiederherzustellen.
Das Skript eject
Das letzte Skript ist npm eject.
Dieses Skript kopiert Ihre Abhängigkeiten und Konfigurationsdateien in Ihr Projekt, sodass Sie die volle Kontrolle über Ihren Code haben, das Projekt aber aus der integrierten Toolchain von Create React App auswerfen können.
Sie werden dies jetzt nicht ausführen, denn wenn Sie dieses Skript einmal ausgeführt haben, können Sie diese Aktion nicht mehr rückgängig machen und Sie werden alle zukünftigen Aktualisierungen von Create React App verlieren.
Der Nutzen von Create React App besteht darin, dass Sie sich nicht um einen erheblichen Konfigurationsaufwand kümmern müssen.
Die Erstellung moderner JavaScript-Anwendungen erfordert eine Menge an Werkzeugen, von Build-Systemen wie Webpack bis hin zu Kompilierungswerkzeugen wie Babel.
Create React App übernimmt die gesamte Konfiguration für Sie, sodass ein Auswerfen bedeutet, dass Sie sich selbst um diese Komplexität kümmern müssen.
Der Nachteil von Create React App ist, dass Sie nicht in der Lage sein werden, das Projekt vollständig anzupassen.
Für die meisten Projekte ist das kein Problem, aber wenn Sie jemals die Kontrolle über alle Aspekte des Build-Prozesses übernehmen wollen, müssen Sie den Code auswerfen.
Wie bereits erwähnt, können Sie jedoch, sobald Sie den Code auswerfen, nicht mehr auf neue Versionen von Create React App aktualisieren, und Sie müssen alle Verbesserungen selbst manuell hinzufügen.
Zu diesem Zeitpunkt haben Sie Skripte ausgeführt, um Ihren Code zu erstellen und zu testen.
Im nächsten Schritt starten Sie das Projekt auf einem Live-Server.
Schritt 3 - Starten des Servers
In diesem Schritt werden Sie einen lokalen Server initialisieren und das Projekt in Ihrem Browser ausführen.
Sie starten Ihr Projekt mit einem anderen npm-Skript.
Wie npm test benötigt auch dieses Skript den Befehl run nicht.
Wenn Sie das Skript ausführen, starten Sie einen lokalen Server, führen den Projektcode aus, starten einen Watcher, der auf Codeänderungen wartet, und öffnen das Projekt in einem Webbrowser.
Starten Sie das Projekt, indem Sie den folgenden Befehl im Stammverzeichnis eingeben.
Für dieses Tutorial ist das Stammverzeichnis Ihres Projekts das Verzeichnis < ^ > digital-ocean-tutorial < ^ > ​ ​ ​.
Achten Sie darauf, dieses in einem separaten Terminal oder einer Registerkarte zu öffnen, denn dieses Skript läuft so lange weiter, wie Sie es zulassen:
Vor dem Start des Servers sehen Sie für einen kurzen Moment einen Platzhaltertext, der diese Ausgabe liefert:
Wenn Sie das Skript lokal ausführen, wird es das Projekt in Ihrem Browserfenster öffnen und den Fokus vom Terminal in den Browser verlagern.
Wenn das nicht geschieht, können Sie http: / / localhost: 3000 / besuchen, um die Seite in Funktion zu sehen.
Sollte bereits ein anderer Server auf Port 3000 laufen, ist das in Ordnung.
Create React App wird den nächsten verfügbaren Port erkennen und den Server darüber betreiben.
Mit anderen Worten, wenn Sie bereits ein Projekt auf Port 3000 ausführen, wird dieses neue Projekt auf Port 3001 beginnen.
Wenn Sie dies von einem Remote-Server aus ausführen, können Sie Ihre Seite ohne zusätzliche Konfiguration immer noch sehen.
Die Adresse wird http: / / < ^ > your _ server _ ip < ^ >: 3000 ​ ​ ​ ​ ​ ​ lauten.
Wenn Sie eine Firewall konfiguriert haben, müssen Sie den Port auf Ihrem Remote-Server öffnen.
Im Browser sehen Sie das folgende React-Vorlagenprojekt:
React-Vorlagenprojekt
Solange das Skript ausgeführt wird, haben Sie einen aktiven lokalen Server.
Um das Skript zu stoppen, schließen Sie entweder das Terminalfenster oder die Registerkarte oder geben Sie STRG + C oder ⌘ - + c in das Terminalfenster oder die Registerkarte ein, in dem / der Ihr Skript ausgeführt wird.
Zu diesem Zeitpunkt haben Sie den Server gestartet und führen Ihren ersten React-Code aus.
Bevor Sie jedoch Änderungen am JavaScript-Code von React vornehmen, werden Sie sehen, wie React die Seite überhaupt rendert.
Schritt 4 - Modifizieren der Homepage
In diesem Schritt ändern Sie Code im Verzeichnis public /.
Das Verzeichnis public enthält Ihre Basis-HTML-Seite.
Dies ist die Seite, die als Stammseite für Ihr Projekt dient.
Sie werden dieses Verzeichnis in Zukunft nur noch selten bearbeiten, aber es ist die Basis, von der aus das Projekt startet und ein wesentlicher Teil eines React-Projekts.
Wenn Sie Ihren Server abgebrochen haben, starten Sie ihn mit npm start neu und öffnen Sie dann public / in Ihrem bevorzugten Texteditor in einem neuen Terminalfenster:
Alternativ können Sie die Dateien mit dem Befehl ls auflisten:
Sie sehen dann eine Liste von Dateien wie diese:
favicon.ico, logo192.png und logo512.png sind Symbole, die ein Benutzer entweder in der Registerkarte seines Browsers oder auf seinem Handy sehen würde.
Der Browser wird die Icons in der richtigen Größe auswählen.
Eventuell werden Sie diese durch Symbole ersetzen wollen, die für Ihr Projekt besser geeignet sind.
Momentan können Sie sie so belassen.
Die Datei manifest.json ist ein strukturierter Satz von Metadaten, die Ihr Projekt beschreiben.
Unter anderem ist darin aufgeführt, welches Symbol für verschiedene Größenoptionen verwendet wird.
Die Datei robots.txt ist eine Information für Web-Crawler.
Sie teilt den Crawlern mit, welche Seiten sie indizieren dürfen und welche nicht.
Sie müssen keine der beiden Dateien ändern, es sei denn, es gibt einen zwingenden Grund, dies zu tun.
Wenn Sie beispielsweise einigen Benutzern eine URL zu speziellen Inhalten geben möchten, die nicht leicht zugänglich sein sollen, können Sie diese in die Datei robots.txt einfügen, und sie wird weiterhin öffentlich verfügbar sein, aber nicht von Suchmaschinen indiziert.
Die Datei index.html ist die Stammdatei Ihrer Anwendung.
Dies ist die Datei, die der Server liest, und die Datei, die Ihr Browser anzeigt.
Öffnen Sie sie in Ihrem Texteditor und sehen Sie sie sich an.
Wenn Sie von der Befehlszeile aus arbeiten, können Sie sie mit dem folgenden Befehl öffnen:
Sie werden Folgendes sehen:
Die Datei ist ziemlich kurz.
In der < body > gibt es keine Images und keine Worte.
Das liegt daran, dass React die gesamte HTML-Struktur selbst aufbaut und sie mit JavaScript injiziert.
Aber React muss wissen, wo der Code eingefügt werden muss, und das ist die Aufgabe von index.html.
Ändern Sie in Ihrem Texteditor die Markierung < title > von React App in Sandbox:
Speichern und schließen Sie Ihren Texteditor.
Überprüfen Sie Ihren Browser.
Der Titel ist der Name, der sich auf der Registerkarte des Browsers befindet. Er wird automatisch aktualisiert.
Falls nicht, aktualisieren Sie die Seite und beachten Sie die Änderung.
Gehen Sie nun zu Ihrem Texteditor zurück.
Jedes React-Projekt startet von einem Stammelement.
Auf einer Seite können mehrere Stammelemente vorhanden sein, aber es muss mindestens eines vorhanden sein.
So weiß React, wo der generierte HTML-Code abgelegt werden soll.
Finden Sie das Element < div id = "root" >.
Dies ist das div, das React für alle zukünftigen Aktualisierungen verwenden wird.
Ändern Sie die id von root in base:
Speichern Sie die Änderungen.
Sie werden in Ihrem Browser einen Fehler sehen:
Fehlermeldung mit der Angabe "Zielcontainer ist kein DOM-Element"
React suchte nach einem Element mit einer id von root.
Da dies nicht mehr vorhanden ist, kann React das Projekt nicht starten.
Ändern Sie den Namen von base wieder zu root:
Speichern und schließen Sie index.html.
An diesem Punkt haben Sie den Server gestartet und eine kleine Änderung an der Root-HTML-Seite vorgenommen.
Sie haben bisher noch keinen JavaScript-Code geändert.
Im nächsten Abschnitt werden Sie den JavaScript-Code von React aktualisieren.
Schritt 5 - Modifizieren des Überschriften-Tags und des Stylings
In diesem Schritt werden Sie Ihre erste Änderung an einer React-Komponente im Verzeichnis src / vornehmen.
Sie führen eine kleine Änderung an der CSS und dem JavaScript-Code durch, der in Ihrem Browser mit Hilfe des eingebauten Hot-Reloadings automatisch aktualisiert wird.
Wenn Sie den Server angehalten haben, starten Sie ihn mit npm start neu.
Nehmen Sie sich nun etwas Zeit, um die Teile des Verzeichnisses src / zu sehen.
Sie können entweder das gesamte Verzeichnis in Ihrem bevorzugten Texteditor öffnen oder Sie können das Projekt mit dem folgenden Befehl in einem Terminal auflisten:
Sie werden die folgenden Dateien in Ihrem Terminal oder Texteditor sehen.
Gehen wir diese Dateien der Reihe nach durch.
Sie werden anfangs nicht viel Zeit mit der Datei serviceWorker.js verbringen, doch sie kann wichtig sein, wenn Sie mit der Erstellung von progressiven Web-Anwendungen beginnen.
Der Service Worker kann viele Dinge tun, einschließlich Push-Benachrichtigungen und Offline-Caching. Momentan lassen wir ihn jedoch in Ruhe.
Die nächsten Dateien, die Sie sich ansehen sollten, sind setupTests.js und App.test.js.
Diese werden für Testdateien verwendet.
Wenn Sie in Schritt 2 npm test ausgeführt haben, hat das Skript diese Dateien tatsächlich ausgeführt.
Die Datei setupTests.js ist kurz und enthält nur einige benutzerdefinierte expect-Methoden.
Sie werden in zukünftigen Tutorials in dieser Reihe mehr darüber erfahren.
Öffnen Sie App.test.js:
Wenn Sie sie öffnen, sehen Sie einen Basistest:
Der Test sucht nach dem Vorhandensein der Phrase learn react im Dokument.
Wenn Sie zu dem Browser zurückgehen, in dem Ihr Projekt ausgeführt wird, sehen Sie die Phrase auf der Seite.
Das Testen mit React unterscheidet sich von den meisten Unit-Tests.
Da Komponenten visuelle Informationen enthalten können, wie z. B. Markup, zusammen mit Logik zur Manipulation von Daten, funktionieren traditionelle Unit-Tests nicht so leicht.
Die Tests mit React sind eher eine Form von Funktions- oder Integrationstests.
Als Nächstes sehen Sie einige Styling-Dateien: App.css, index.css und logo.svg.
Es gibt mehrere Möglichkeiten, mit Styling in React zu arbeiten, aber die einfachste ist das Schreiben von einfachem CSS, da dies keine zusätzliche Konfiguration erfordert.
Es gibt mehrere CSS-Dateien, da Sie die Stile in eine Komponente importieren können, als wären sie eine weitere JavaScript-Datei.
Da Sie die Möglichkeit haben, CSS direkt in eine Komponente zu importieren, können Sie das CSS auch aufteilen, um es nur auf eine einzelne Komponente anzuwenden.
Was Sie tun, ist, Bedenken zu trennen.
Sie halten nicht das gesamte CSS vom JavaScript getrennt.
Stattdessen halten Sie alle verwandten CSS, JavaScript, Markups und Bilder zusammen gruppiert.
Öffnen Sie App.css in Ihrem Texteditor.
Dies ist der Code, den Sie sehen werden:
Dies ist eine Standard-CSS-Datei ohne spezielle CSS-Präprozessoren.
Wenn Sie möchten, können Sie sie später hinzufügen, aber anfangs haben Sie nur einfaches CSS.
Create React App versucht, unvoreingenommen zu sein und dennoch eine sofort einsatzbereite Umgebung zu bieten.
Zurück zu App.css. Einer der Vorteile von Create React App ist, dass es alle Dateien überwacht. Wenn Sie also eine Änderung vornehmen, sehen Sie sie in Ihrem Browser, ohne sie neu laden zu müssen.
Um dies in Aktion zu sehen, nehmen Sie eine kleine Änderung an der background-color in App.css vor.
Ändern Sie sie von # 282c34 zu blue und speichern Sie dann die Datei.
Der endgültige Style sieht ungefähr so aus:
Schauen Sie sich Ihren Browser an.
So hat er vorher ausgesehen:
React App mit dunklem Hintergrund
So sieht er nach der Änderung aus:
React App mit blauem Hintergrund
Fahren Sie fort und ändern Sie die Hintergrundfarbe wieder in # 282c34.
Sie haben eine kleine CSS-Änderung vorgenommen.
Jetzt ist es an der Zeit, Änderungen am React JavaScript-Code vorzunehmen.
Beginnen Sie mit dem Öffnen von index.js.
Sie werden Folgendes sehen:
Ganz oben importieren Sie React, ReactDOM, index.css, App und serviceWorker.
Durch das Importieren von React ziehen Sie tatsächlich Code ein, um JSX in JavaScript zu konvertieren.
JSX sind die HTML-ähnlichen Elemente.
Beachten Sie beispielsweise, dass Sie, wenn Sie App verwenden, es wie ein HTML-Element < App / > behandeln.
Sie werden dies in zukünftigen Tutorials in dieser Reihe genauer untersuchen.
ReactDOM ist der Code, der Ihren React-Code mit den Basiselementen verbindet, wie beispielsweise die Seite index.html, die Sie in public / gesehen haben.
Sehen Sie sich die folgende hervorgehobene Zeile an:
Dieser Code weist React an, ein Element mit einer id von root zu finden und den React-Code dort zu injizieren.
< App / > ist Ihr Stammelement und von dort aus verzweigt sich alles.
Dies ist der Startpunkt für allen zukünftigen React-Code.
Am Anfang der Datei sehen Sie einige Importe.
Sie importieren index.css, machen aber eigentlich nichts damit. Indem Sie sie importieren, weisen Sie Webpack über die React-Skripte an, diesen CSS-Code in das endgültige kompilierte Bundle aufzunehmen.
Wenn Sie sie nicht importieren, wird es nicht angezeigt.
Verlassen Sie src / index.js.
Zu diesem Zeitpunkt haben Sie noch nicht gesehen, was Sie in Ihrem Browser sehen.
Um dies zu sehen, öffnen Sie App.js:
Der Code in dieser Datei wird wie eine Reihe regulärer HTML-Elemente aussehen.
Ändern Sie den Inhalt des Tags < p > von Edit < code > src / App.js < / code > and save to reload. ​ ​ ​ ​ ​ zu Hello, world und speichern Sie Ihre Änderungen.
Gehen Sie zu Ihrem Browser und Sie werden die Änderung sehen:
React App mit "Hello, world" in Absatzmarkierung
Sie haben nun Ihre erste Aktualisierung an einer React-Komponente vorgenommen.
Bevor Sie gehen, beachten Sie noch ein paar Dinge.
In dieser Komponente importieren Sie die Datei logo.svg und weisen sie einer Variablen zu.
Dann fügen Sie im Element < img > diesen Code als src hinzu.
Hier sind einige Dinge zu beachten.
Sehen Sie sich das Element img an:
Beachten Sie, wie Sie das Logo in geschweifte Klammern übergeben.
Sie müssen immer dann geschweifte Klammern verwenden, wenn Sie Attribute übergeben, die keine Zeichenketten oder Zahlen sind.
React behandelt diese als JavaScript anstelle von Zeichenketten.
In diesem Fall importieren Sie das Bild nicht wirklich; stattdessen referenzieren Sie das Bild.
Wenn Webpack das Projekt erstellt, wird es das Bild handhaben und die Quelle an die entsprechende Stelle setzen.
Beenden Sie den Texteditor.
Wenn Sie sich die DOM-Elemente in Ihrem Browser ansehen, sehen Sie, dass es einen Pfad hinzufügt.
Wenn Sie Chrome verwenden, können Sie das Element inspizieren, indem Sie mit der rechten Maustaste auf das Element klicken und Inspect wählen.
So würde es im Browser aussehen:
Inspizieren des Elements mit Chrome dev Tools
Das DOM hat diese Zeile:
Ihr Code wird etwas anders sein, da das Logo einen anderen Namen haben wird.
Webpack möchte sicherstellen, dass der Bildpfad eindeutig ist.
Selbst wenn Sie also Bilder mit dem gleichen Namen importieren, werden diese mit unterschiedlichen Pfaden gespeichert.
Zu diesem Zeitpunkt haben Sie eine kleine Änderung am React JavaScript-Code vorgenommen.
Im nächsten Schritt verwenden Sie den Befehl build, um den Code in eine kleine Datei zu verkleinern, die auf einem Server bereitgestellt werden kann.
Schritt 6 - Erstellen des Projekts
In diesem Schritt stellen Sie den Code zu einem Bundle zusammen, das auf externen Servern bereitgestellt werden kann.
Gehen Sie zurück zu Ihrem Terminal und erstellen Sie das Projekt.
Sie haben diesen Befehl schon einmal ausgeführt, aber zur Erinnerung: Dieser Befehl wird das Skript build ausführen.
Es wird ein neues Verzeichnis mit den kombinierten und verkleinerten Dateien erstellen.
Um den Build auszuführen, führen Sie den folgenden Befehl im Stammverzeichnis Ihres Projekts aus:
Es wird eine Verzögerung beim Kompilieren des Codes geben, und wenn er fertig ist, haben Sie ein neues Verzeichnis namens build /.
Öffnen Sie build / index.html in einem Texteditor.
Sie werden etwas wie das hier sehen:
Das Verzeichnis build nimmt Ihren gesamten Code und kompiliert und minimiert ihn bis zum kleinsten nutzbaren Zustand.
Es spielt keine Rolle, ob ein Mensch ihn lesen kann, da es sich hierbei nicht um einen öffentlich zugänglichen Teil des Codes handelt.
Durch eine solche Minimierung nimmt der Code weniger Platz in Anspruch und kann trotzdem funktionieren.
Im Gegensatz zu einigen Sprachen wie Python, ändert der Leerraum nicht, wie der Computer den Code interpretiert.
In diesem Tutorial haben Sie Ihre erste React-Anwendung erstellt und Ihr Projekt mit Hilfe von JavaScript-Build-Tools konfiguriert, ohne auf die technischen Details eingehen zu müssen.
Das ist der Nutzen von Create React App: Sie müssen nicht alles wissen, um anzufangen.
Es ermöglicht Ihnen, die komplizierten Build-Schritte zu ignorieren, sodass Sie sich ausschließlich auf den React-Code konzentrieren können.
Sie haben nun die Befehle zum Starten, Testen und Erstellen eines Projekts gelernt.
Sie werden diese Befehle regelmäßig verwenden. Daher sollten Sie sie für zukünftige Tutorials notieren.
Am wichtigsten ist, dass Sie Ihre erste React-Komponente aktualisiert haben.
Wenn Sie React in Aktion sehen möchten, probieren Sie unser Tutorial Anzeigen von Daten von der DigitalOcean-API mit React.
Einrichten und Konfigurieren einer Certificate Authority (CA) unter CentOS 8
4001
Eine Zertifizierungsstelle (Certificate Authority, CA) ist eine Stelle, die für die Ausstellung digitaler Zertifikate zur Überprüfung von Identitäten im Internet verantwortlich ist.
Obwohl öffentliche CAs eine beliebte Wahl für die Überprüfung der Identität von Websites und anderen Diensten sind, die der allgemeinen Öffentlichkeit zur Verfügung gestellt werden, werden private CAs normalerweise für geschlossene Gruppen und private Dienste verwendet.
Die Erstellung einer privaten Zertifizierungsstelle ermöglicht es Ihnen, Programme zu konfigurieren, zu testen und auszuführen, die verschlüsselte Verbindungen zwischen einem Client und einem Server erfordern.
Mit einer privaten CA können Sie Zertifikate für Benutzer, Server oder einzelne Programme und Dienste innerhalb Ihrer Infrastruktur ausstellen.
Einige Beispiele für Programme unter Linux, die ihre eigene private CA verwenden, sind OpenVPN und Puppet.
Sie können Ihren Webserver auch so konfigurieren, dass er Zertifikate verwendet, die von einer privaten CA ausgestellt wurden, um Entwicklungs- und Staging-Umgebungen an Produktionsserver anzupassen, die TLS zur Verschlüsselung von Verbindungen verwenden.
In diesem Leitfaden lernen wir, wie eine private Zertifizierungsstelle auf einem CentOS-8-Server eingerichtet wird und wie mit einer neuen CA ein Testzertifikat erzeugt und signiert wird.
Außerdem erfahren Sie, wie Sie das öffentliche Zertifikat des CA-Servers in den Zertifikatsspeicher Ihres Betriebssystems importieren, damit Sie die Vertrauenskette zwischen der CA und entfernten Servern oder Benutzern überprüfen können.
Schließlich werden Sie lernen, wie Sie Zertifikate widerrufen und eine Zertifikatswiderrufsliste verteilen, um sicherzustellen, dass nur autorisierte Benutzer und Systeme Dienste nutzen können, die auf Ihrer CA beruhen.
Um diesem Tutorial folgen zu können, benötigen Sie einen CentOS 8-Server mit einem sudo-fähigen non-root user und einer mit firewalld eingerichteten Firewall.
Sie können unserem Leitfaden Ersteinrichtung des Servers unter CentOS 8 folgen, um diese Einrichtung abzuschließen.
Dieser Server wird in diesem Tutorial als CA-Server bezeichnet.
Stellen Sie sicher, dass der CA-Server ein eigenständiges System ist.
Er wird nur zum Importieren, Signieren und Widerrufen von Zertifikatanforderungen verwendet.
Auf ihm sollten keine anderen Dienste ausgeführt werden, und idealerweise ist er offline oder wird vollständig heruntergefahren, wenn Sie nicht aktiv mit Ihrer CA arbeiten.
< $> note Anmerkung: Der letzte Abschnitt dieses Tutorials ist optional, wenn Sie über das Signieren und Widerrufen von Zertifikaten lernen möchten.
Wenn Sie diese Übungsschritte durchführen möchten, benötigen Sie einen zweiten CentOS 8-Server oder Sie können Ihren eigenen lokalen Linux-Rechner verwenden, auf dem CentOS 8, Fedora oder RedHat ausgeführt wird.
Schritt 1 - Installieren von Easy-RSA
Die erste Aufgabe in diesem Tutorial besteht darin, den Skriptsatz easy-rsa auf Ihrem CA-Server zu installieren. easy-rsa ist ein Verwaltungswerkzeug für Zertifizierungsstellen, mit dem Sie einen privaten Schlüssel und ein öffentliches Stammzertifikat erzeugen, die Sie dann zum Signieren von Anfragen von Clients und Servern verwenden, die auf Ihre CA angewiesen sind.
Das Paket easy-rsa ist in CentOS 8 nicht standardmäßig verfügbar, daher müssen Sie das Repository "Extra Packages for Enterprise Linux" (EPEL) aktivieren.
EPEL wird vom Fedora-Projekt verwaltet und enthält nicht-standardmäßige, aber beliebte Pakete für Fedora, CentOS und andere Linux-Distributionen, die das RPM-Paketformat verwenden.
Melden Sie sich bei Ihrem CA-Server als der non-root sudo user an, den Sie während der anfänglichen Einrichtungsschritte erstellt haben, und führen Sie Folgendes aus:
Sie werden aufgefordert, das Paket herunterzuladen und zu installieren. Drücken Sie y, um zu bestätigen, dass Sie das Paket installieren möchten.
Installieren Sie nun das Paket easy-rsa und geben Sie an der Eingabeaufforderung erneut y ein:
An diesem Punkt haben Sie alles Nötige eingerichtet und sind bereit, Easy-RSA zu benutzen.
Im nächsten Schritt werden Sie eine Public-Key-Infrastruktur erstellen und dann mit dem Erstellen Ihrer Zertifizierungsstelle beginnen.
Schritt 2 - Vorbereiten eines Public-Key-Infrastrukturverzeichnisses
Nachdem Sie nun easy-rsa installiert haben, ist es an der Zeit, eine grundlegende Public-Key-Infrastruktur (PKI) auf dem CA-Server zu erstellen.
Stellen Sie sicher, dass Sie immer noch als non-root user angemeldet sind und erstellen Sie ein easy-rsa-Verzeichnis.
Stellen Sie sicher, dass Sie sudo nicht verwenden, um einen der folgenden Befehle auszuführen, da Ihr normaler Benutzer die CA ohne erhöhte Berechtigungen verwalten und mit ihr interagieren sollte.
Dadurch wird ein neues Verzeichnis namens easy-rsa in Ihrem Home-Ordner erstellt.
Wir werden dieses Verzeichnis verwenden, um symbolische Links zu erstellen, die auf die easy-rsa-Paketdateien verweisen, die wir im vorigen Schritt installiert haben.
Diese Dateien befinden sich im Ordner / usr / share / easy-rsa / 3 auf dem CA-Server.
Erstellen Sie die Symlinks mit dem Befehl ln:
Um den Zugriff auf Ihr neues PKI-Verzeichnis einzuschränken, stellen Sie sicher, dass nur der Eigentümer mit dem Befehl chmod darauf zugreifen kann:
Anschließend initialisieren Sie die PKI innerhalb des easy-rsa-Verzeichnisses:
Nachdem Sie diesen Abschnitt abgeschlossen haben, haben Sie ein Verzeichnis, das alle Dateien enthält, die zur Erstellung einer Zertifizierungsstelle benötigt werden.
Im nächsten Abschnitt werden Sie den privaten Schlüssel und das öffentliche Zertifikat für Ihre CA erstellen.
Schritt 3 - Erstellen einer Zertifizierungsstelle
Bevor Sie den privaten Schlüssel und das Zertifikat Ihrer CA erstellen können, müssen Sie eine Datei namens vars erstellen und mit einigen Standardwerten füllen.
Zuerst werden Sie cd in das Verzeichnis easy-rsa, dann werden Sie die Datei vars mit nano oder Ihrem bevorzugten Texteditor erstellen und bearbeiten.
Wenn Sie zur Installation von nano aufgefordert werden, geben Sie y ein, um mit den Installationsschritten fortzufahren.
Nun sind Sie bereit, die Datei vars zu bearbeiten:
Sobald die Datei geöffnet ist, fügen Sie die folgenden Zeilen ein und bearbeiten Sie jeden hervorgehobenen Wert so, dass er Ihre eigenen Organisationsinformationen widerspiegelt.
Wichtig dabei ist, dass Sie keinen der Werte leer lassen:
Wenn Sie nano verwenden, können Sie dies durch Drücken von STRG + X, dann Y und ENTER zur Bestätigung tun.
Sie sind nun bereit, Ihre CA zu erstellen.
Um das öffentliche und private Stammschlüsselpaar für Ihre Zertifizierungsstelle zu erstellen, führen Sie den Befehl. / easy-rsa erneut aus, diesmal mit der Option build-ca:
In der Ausgabe sehen Sie einige Zeilen über die OpenSSL-Version und werden dazu aufgefordert, eine Passphrase für Ihr Schlüsselpaar einzugeben.
Achten Sie darauf, eine starke Passphrase zu wählen, und notieren Sie sie an einem sicheren Ort.
Sie müssen die Passphrase jedes Mal eingeben, wenn Sie mit Ihrer CA interagieren müssen, zum Beispiel zum Signieren oder Widerrufen eines Zertifikats.
Sie werden auch gebeten, den Common Name (CN) für Ihre CA zu bestätigen.
Der CN ist der Name, der verwendet wird, um im Kontext der Zertifizierungsstelle auf diesen Computer zu verweisen.
Sie können eine beliebige Zeichenfolge für den Common Name der CA eingeben, aber drücken Sie der Einfachheit halber ENTER, um den Standardnamen zu akzeptieren.
< $> note Anmerkung: Wenn Sie nicht bei jeder Interaktion mit Ihrer CA zur Eingabe eines Passworts aufgefordert werden möchten, können Sie den Befehl build-ca mit der Option nopass wie folgt ausführen:
Sie haben nun zwei wichtige Dateien - ~ / easy-rsa / pki / ca.crt und ~ / easy-rsa / pki / private / ca.key - die die öffentlichen und privaten Komponenten einer Zertifizierungsstelle bilden.
ca.crt ist die öffentliche Zertifikatsdatei der CA.
Benutzer, Server und Clients verwenden dieses Zertifikat, um zu überprüfen, ob sie Teil desselben vertrauenswürdigen Webs sind.
Jeder Benutzer und Server, der Ihre CA verwendet, muss eine Kopie dieser Datei haben.
Alle Parteien verlassen sich auf das öffentliche Zertifikat, um sicherzustellen, dass sich nicht jemand als System ausgibt und einen Man-in-the-middle-Angriff durchführt.
ca.key ist der private Schlüssel, den die CA zum Signieren von Zertifikaten für Server und Clients verwendet.
Wenn ein Angreifer Zugriff auf Ihre CA und damit auf Ihre ca.key-Datei erhält, müssen Sie Ihre CA vernichten.
Damit ist Ihre CA vorhanden und bereit, zum Signieren von Zertifikatanforderungen und zum Widerrufen von Zertifikaten verwendet zu werden.
Schritt 4 - Verteilen des öffentlichen Zertifikats Ihrer Zertifizierungsstelle
Nun ist Ihre CA konfiguriert und bereit, als Vertrauensgrundlage für alle Systeme zu fungieren, die Sie für ihre Verwendung konfigurieren möchten. Sie können das Zertifikat der CA zu Ihren OpenVPN-Servern, Webservern, Mail-Servern usw. hinzufügen.
Jeder Benutzer oder Server, der die Identität eines anderen Benutzers oder Servers in Ihrem Netzwerk überprüfen muss, sollte eine Kopie der Datei ca.crt haben, die in den Zertifikatsspeicher ihres Betriebssystems importiert ist.
Um das öffentliche Zertifikat der CA in ein zweites Linux-System wie einen anderen Server oder einen lokalen Computer zu importieren, besorgen Sie sich zunächst eine Kopie der ca.crt-Datei von Ihrem CA-Server.
Sie können den Befehl cat verwenden, um sie in einem Terminal auszugeben, und sie dann kopieren und in eine Datei auf dem zweiten Computer, der das Zertifikat importiert, einfügen.
Sie können auch Tools wie scp, rsync verwenden, um die Datei zwischen Systemen zu übertragen.
Wir werden in diesem Schritt jedoch Kopieren und Einfügen mit nano verwenden, da dies auf allen Systemen funktioniert.
Führen Sie als non-root user auf dem CA-Server den folgenden Befehl aus:
Es wird eine Ausgabe in Ihrem Terminal geben, die der folgenden ähnelt:
Kopieren Sie alles, einschließlich der Zeilen -----BEGIN CERTIFICATE----- und -----END CERTIFICATE----- und der Bindestriche.
Verwenden Sie auf Ihrem zweiten Linux-System nano oder Ihren bevorzugten Texteditor, um eine Datei namens / tmp / ca.crt zu öffnen:
Fügen Sie den Inhalt, den Sie gerade vom CA-Server kopiert haben, in den Editor ein.
Nachdem Sie nun eine Kopie der Datei ca.crt auf Ihrem zweiten Linux-System haben, ist es an der Zeit, das Zertifikat in den Zertifikatsspeicher des Betriebssystems zu importieren.
Führen Sie auf CentOS, Fedora oder anderen von RedHat abgeleiteten Linux-Systemen die folgenden Befehle aus, um das Zertifikat zu importieren:
Um das Zertifikat des CA-Servers auf einem Debian- oder Ubuntu-basierten System zu importieren, kopieren Sie den Inhalt der Datei und fügen Sie ihn wie im vorherigen Beispiel in das System in eine Datei namens / tmp / ca.crt ein.
Kopieren Sie als Nächstes das Zertifikat nach / usr / local / share / ca-certificates / und führen Sie dann den Befehl update-ca-certificates aus.
Nun wird Ihr zweites Linux-System auf jedes Zertifikat vertrauen, das vom CA-Server signiert wurde.
< $> note Anmerkung: Wenn Sie Ihre CA mit Web-Servern verwenden und Firefox als Browser verwenden, müssen Sie das öffentliche Zertifikat ca.crt direkt in Firefox importieren.
Firefox verwendet nicht den Zertifikatsspeicher des lokalen Betriebssystems.
Einzelheiten dazu, wie Sie das Zertifikat Ihrer CA in Firefox hinzufügen können, finden Sie in diesem Support-Artikel von Mozilla über das Einrichten von Zertifizierungsstellen (CAs) in Firefox.
Wenn Sie Ihre CA zur Integration in eine Windows-Umgebung oder in Desktop-Computer verwenden, lesen Sie bitte die Dokumentation über die Verwendung von certutil.exe zur Installation eines CA-Zertifikats.
Wenn Sie dieses Tutorial als Voraussetzung für ein anderes Tutorial verwenden oder mit dem Signieren und Widerrufen von Zertifikaten vertraut sind, können Sie hier aufhören.
Wenn Sie mehr zum Thema Signieren und Widerrufen von Zertifikaten erfahren möchten, dann wird im folgenden optionalen Abschnitt jeder Vorgang im Detail erklärt.
(Optional) - Erstellen von Zertifikat-Signierungsanfragen und Widerrufen von Zertifikaten
Die folgenden Abschnitte des Tutorials sind optional.
Wenn Sie alle vorherigen Schritte abgeschlossen haben, verfügen Sie über eine vollständig konfigurierte und funktionierende Zertifizierungsstelle, die Sie als Voraussetzung für andere Tutorials verwenden können.
Sie können die Datei ca.crt Ihrer CA importieren und Zertifikate in Ihrem Netzwerk überprüfen, die von Ihrer CA signiert wurde.
Wenn Sie üben und mehr über das Signieren von Zertifikatanforderungen und das Widerrufen von Zertifikaten erfahren möchten, dann werden diese optionalen Abschnitte erklären, wie beide Prozesse funktionieren.
(Optional) - Erstellen und Signieren einer Übungs-Zertifikatanforderung
Nachdem Sie nun eine einsatzbereite CA haben, können Sie das Erzeugen eines privaten Schlüssels und einer Zertifikatanforderung üben, um sich mit dem Signier- und Verteilungsprozess vertraut zu machen.
Eine Zertifikat-Signierungsanfrage (Certificate Signing Request, CSR) besteht aus drei Teilen: einem öffentlichen Schlüssel, dem Identifizieren von Informationen über das anfordernde System und einer Signatur der Anforderung selbst, die mit dem privaten Schlüssel der anfragenden Partei erstellt wird.
Der private Schlüssel wird geheim gehalten und wird zum Verschlüsseln von Informationen verwendet, die jeder mit dem signierten öffentlichen Zertifikat dann entschlüsseln kann.
Die folgenden Schritte werden auf Ihrem zweiten Linux-System mit CentOS, Fedora oder einer anderen von RedHat abgeleiteten Linux-Distribution ausgeführt.
Es kann sich um einen anderen Remote-Server oder einen lokalen Linux-Rechner wie einen Laptop oder einen Desktop-Rechner handeln.
Da easy-rsa nicht standardmäßig auf allen Systemen verfügbar ist, verwenden wir das Tool openssl zum Erstellen eines privaten Übungsschlüssels und -zertifikats.
openssl ist normalerweise standardmäßig auf den meisten Linux-Distributionen installiert, aber um sicherzugehen, führen Sie die folgenden Schritte auf Ihrem System aus:
Wenn Sie zur Installation von openssl aufgefordert werden, geben Sie y ein, um mit den Installationsschritten fortzufahren.
Nun sind Sie bereit, eine Übungs-CSR mit openssl zu erstellen.
Der erste Schritt, den Sie zum Erstellen einer CSR ausführen müssen, ist die Erzeugung eines privaten Schlüssels.
Um einen privaten Schlüssel mit openssl zu erstellen, erstellen Sie ein Verzeichnis practice-csr und erzeugen Sie darin einen Schlüssel. Wir werden diese Anfrage für einen fiktiven Server namens sammy-server stellen, im Gegensatz zur Erstellung eines Zertifikats, das zur Identifizierung eines Benutzers oder einer anderen CA verwendet wird.
Da Sie nun über einen privaten Schlüssel verfügen, können Sie eine entsprechende CSR erstellen, wiederum mit dem Dienstprogramm openssl.
Sie werden aufgefordert, eine Reihe von Feldern wie Land, Bundesland und Stadt auszufüllen.
Wenn Sie ein Feld leer lassen möchten, können Sie einen. eingeben. Beachten Sie jedoch, dass es am besten ist, die richtigen Werte für Ihren Standort und Ihre Organisation zu verwenden, wenn es sich um eine reale CSR handelt:
Wenn Sie diese Werte automatisch als Teil des openssl-Aufrufs statt über die interaktive Eingabeaufforderung hinzufügen möchten, können Sie das Argument -subj an OpenSSL übergeben.
Achten Sie darauf, die hervorgehobenen Werte so zu bearbeiten, dass sie mit dem Standort, der Organisation und dem Servernamen für die Übung übereinstimmen:
Zur Überprüfung des Inhalts einer CSR können Sie eine Anforderungsdatei mit openssl einlesen und die darin enthaltenen Felder untersuchen:
Wenn Sie mit dem Thema Ihrer Übungs-Zertifikatanforderung zufrieden sind, kopieren Sie die Datei sammy-server.req mit scp auf Ihren CA-Server:
In diesem Schritt haben Sie eine Zertifikatsignieranforderung für einen fiktiven Server namens sammy-server erzeugt.
In einem realen Szenario könnte die Anfrage z. B. von einem Staging- oder Entwicklungs-Webserver kommen, der ein TLS-Zertifikat zum Testen benötigt; oder sie könnte von einem OpenVPN-Server kommen, der ein Zertifikat anfordert, damit sich Benutzer mit einem VPN verbinden können.
Im nächsten Schritt fahren wir mit dem Signieren der Zertifikatsignieranforderung unter Verwendung des privaten Schlüssels des CA-Servers fort.
(Optional) - Signieren einer CSR
Im vorherigen Schritt haben Sie eine Übungs-Zertifikatanforderung und einen Übungsschlüssel für einen fiktiven Server erstellt.
Sie kopierten sie in das Verzeichnis / tmp auf Ihrem CA-Server und emulierten damit das Verfahren, das Sie verwenden würden, wenn Sie echte Clients oder Server hätten, die Ihnen CSR-Anfragen senden würden, die signiert werden müssen.
Um mit dem fiktiven Szenario fortzufahren, muss der CA-Server nun das Übungszertifikat importieren und signieren. Sobald eine Zertifikatanforderung von der CA validiert und an einen Server zurückgesendet wird, können Clients, die der Zertifizierungsstelle vertrauen, auch dem neu ausgestellten Zertifikat vertrauen.
Da wir innerhalb der PKI der CA arbeiten werden, in der das Dienstprogramm easy-rsa verfügbar ist, werden die Signierungsschritte das Dienstprogramm easy-rsa verwenden. Dies vereinfacht die Dinge im Gegensatz zur direkten Verwendung von openssl, wie wir es im vorherigen Beispiel getan haben.
Der erste Schritt zum Signieren der fiktiven CSR besteht darin, die Zertifikatanforderung mithilfe des Skripts easy-rsa zu importieren:
Jetzt können Sie die Anfrage signieren, indem Sie das Skript easyrsa mit der Option sign-req ausführen, gefolgt vom Anfragetyp und dem Common Name, der in der CSR enthalten ist.
Der Anfragetyp kann entweder Client, Server oder ca sein. Da wir mit einem Zertifikat für einen fiktiven Server üben, stellen Sie sicher, dass Sie den Anfragetyp Server verwenden:
In der Ausgabe werden Sie zur Überprüfung aufgefordert, ob die Anfrage von einer vertrauenswürdigen Quelle stammt.
Wenn Sie Ihren CA-Schlüssel verschlüsselt haben, werden Sie an dieser Stelle zur Eingabe Ihres Passworts aufgefordert.
Nach Abschluss dieser Schritte haben Sie die CSR sammy-server.req mit dem privaten Schlüssel des CA-Servers in / home / sammy / easy-rsa / pki / private / ca.key signiert.
Die resultierende Datei sammy-server.crt enthält den öffentlichen Verschlüsselungsschlüssel des Übungsservers sowie eine neue Signatur des CA-Servers.
Der Sinn der Signatur besteht darin, jedem, der der CA vertraut, mitzuteilen, dass auch dem sammy-server-Zertifikat vertraut werden kann.
Wenn es sich bei dieser Anfrage um einen echten Server wie einen Web- oder VPN-Server handelt, würde der letzte Schritt auf dem CA-Server darin bestehen, die neuen Dateien sammy-server.crt und ca.crt vom CA-Server an den Remote-Server zu verteilen, der die CSR-Anfrage gestellt hat:
Zu diesem Zeitpunkt könnten Sie das ausgestellte Zertifikat mit beispielsweise einem Webserver, einem VPN, einem Konfigurationsmanagement-Tool, einem Datenbanksystem oder für die Client-Authentifizierung verwenden.
(Optional) - Widerrufen eines Zertifikats
Gelegentlich kann es erforderlich sein, ein Zertifikat zu widerrufen, um zu verhindern, dass ein Benutzer oder Server es verwendet. Vielleicht wurde ein Laptop gestohlen, ein Webserver kompromittiert, oder ein Mitarbeiter oder ein Auftragnehmer hat Ihr Unternehmen verlassen.
Zum Widerrufen eines Zertifikats folgt der allgemeine Vorgang diesen Schritten:
Widerrufen Sie das Zertifikat mit dem Befehl. / easyrsa revoke < ^ > client _ name < ^ >.
Erstellen Sie eine neue CRL mit dem Befehl. / easyrsa gen-crl.
Übertragen Sie die aktualisierte Datei crl.pem auf den oder die Server, die sich auf Ihre CA verlassen, und kopieren Sie sie auf diesen Systemen in das oder die erforderlichen Verzeichnisse für Programme, die auf sie verweisen.
Starten Sie alle Dienste, die Ihre CA und die CRL-Datei verwenden, neu.
Mit diesem Vorgang können Sie alle Zertifikate, die Sie zuvor ausgestellt haben, jederzeit widerrufen.
In den folgenden Abschnitten gehen wir jeden Schritt im Detail durch, beginnend mit dem Befehl revoke.
Widerrufen eines Zertifikats
Um ein Zertifikat zu widerrufen, navigieren Sie zum Verzeichnis easy-rsa auf Ihrem CA-Server:
Führen Sie als Nächstes das Skript easyrsa mit der Option revoke aus, gefolgt von dem Client-Namen, den Sie widerrufen möchten.
Dem obigen Übungsbeispiel folgend lautet der Common Name des Zertifikats sammy-server:
Beachten Sie den hervorgehobenen Wert in der Zeile Revoking Certificate.
Dieser Wert ist die eindeutige Seriennummer des Zertifikats, das widerrufen wird.
Sie benötigen diesen Wert, wenn Sie die Widerrufsliste im letzten Schritt dieses Abschnitts prüfen möchten, um zu verifizieren, dass das Zertifikat darin enthalten ist.
Nach der Bestätigung der Aktion wird die CA das Zertifikat widerrufen.
Entfernte Systeme, die sich auf die CA verlassen, haben jedoch keine Möglichkeit zur Überprüfung, ob Zertifikate widerrufen wurden.
Benutzer und Server können das Zertifikat weiterhin verwenden, bis die Zertifikatswiderrufsliste (Certificate Revocation List, CRL) der CA an alle Systeme verteilt wird, die sich auf die CA verlassen.
Im nächsten Schritt erzeugen Sie eine CRL oder aktualisieren eine bestehende crl.pem-Datei.
Erzeugen einer Zertifikatswiderrufsliste
Nachdem Sie ein Zertifikat widerrufen haben, ist es jetzt wichtig, die Liste der widerrufenen Zertifikate auf Ihrem CA-Server zu aktualisieren.
Sobald Sie über eine aktualisierte Widerrufsliste verfügen, können Sie feststellen, welche Benutzer und Systeme in Ihrer CA über gültige Zertifikate verfügen.
Um eine CRL zu erzeugen, führen Sie den Befehl easy-rsa mit der Option gen-crl aus, während Sie sich noch im Verzeichnis ~ / easy-rsa befinden:
Wenn Sie bei der Erstellung Ihrer Datei ca.key eine Passphrase verwendet haben, werden Sie aufgefordert, diese einzugeben. Der Befehl gen-crl erzeugt eine Datei namens crl.pem, die die aktualisierte Liste der widerrufenen Zertifikate für diese CA enthält.
Als Nächstes müssen Sie jedes Mal, wenn Sie den Befehl gen-crl ausführen, die aktualisierte Datei crl.pem an alle Server und Clients übertragen, die auf diese CA angewiesen sind.
Andernfalls können die Clients und Systeme weiterhin auf Dienste und Systeme zugreifen, die Ihre CA verwenden, da diese Dienste über den widerrufenen Status des Zertifikats informiert sein müssen.
Übertragen einer Zertifikatswiderrufsliste
Nachdem Sie nun eine CRL auf Ihrem CA-Server erzeugt haben, müssen Sie sie an Remote-Systeme übertragen, die sich auf Ihre CA verlassen.
Um diese Datei auf Ihre Server zu übertragen, können Sie den Befehl scp verwenden.
< $> note Anmerkung: In diesem Tutorial wird erklärt, wie eine CRL manuell erzeugt und verteilt wird.
Es gibt zwar robustere und automatisierte Methoden zur Verteilung und Überprüfung von Widerrufslisten wie OCSP-Stapling, aber die Konfiguration dieser Methoden sprengt den Rahmen dieses Artikels.
Stellen Sie sicher, dass Sie bei Ihrem CA-Server als non-root user angemeldet sind, und führen Sie die folgenden Schritte aus, wobei Sie an Stelle von your _ server _ ip Ihre eigene Server-IP oder Ihren eigenen DNS-Namen eingeben:
Da sich die Datei nun auf dem Remote-System befindet, besteht der letzte Schritt darin, alle Dienste mit der neuen Kopie der Widerrufsliste zu aktualisieren.
Aktualisierung von Diensten, die eine CRL unterstützen
Die Liste der Schritte, die Sie zur Aktualisierung von Diensten verwenden müssen, die die Datei crl.pem verwenden, geht über den Umfang dieses Tutorials hinaus.
Im Allgemeinen müssen Sie die Datei crl.pem an den Speicherort kopieren, den der Dienst erwartet, und sie dann mit systemctl neu starten.
Sobald Sie Ihre Dienste mit der neuen crl.pem-Datei aktualisiert haben, sind Ihre Dienste in der Lage, Verbindungen von Clients oder Servern abzulehnen, die ein widerrufenes Zertifikat verwenden.
Überprüfen und Verifizieren der Inhalte einer CRL
Wenn Sie eine CRL-Datei überprüfen möchten, z. B. um eine Liste widerrufener Zertifikate zu bestätigen, verwenden Sie den folgenden openssl-Befehl aus Ihrem easy-rsa-Verzeichnis auf Ihrem CA-Server:
Sie können diesen Befehl auch auf jedem Server oder System ausführen, auf dem das openssl-Tool mit einer Kopie der Datei crl.pem installiert ist.
Wenn Sie beispielsweise die Datei crl.pem auf Ihr zweites System übertragen haben und überprüfen möchten, ob das Zertifikat sammy-server widerrufen wurde, können Sie einen openssl-Befehl wie den folgenden verwenden, wobei Sie die Seriennummer, die Sie zuvor beim Widerruf des Zertifikats notiert haben, an Stelle der hier markierten verwenden:
Beachten Sie, wie der Befehl grep verwendet wird, um die eindeutige Seriennummer zu überprüfen, die Sie im Widerrufsschritt notiert haben.
Jetzt können Sie den Inhalt Ihrer Zertifikatswiderrufsliste auf jedem System überprüfen, das darauf angewiesen ist, den Zugriff auf Benutzer und Dienste einzuschränken.
In diesem Tutorial haben Sie eine private Zertifizierungsstelle mit dem Easy-RSA-Paket auf einem eigenständigen CentOS 8-Server erstellt.
Sie haben gelernt, wie das Vertrauensmodell zwischen Parteien funktioniert, die sich auf die CA verlassen.
Sie haben auch eine Zertifikat-Signierungsanfrage (Certificate Signing Request, CSR) für einen Übungsserver erstellt und signiert und dann gelernt, wie man ein Zertifikat widerruft.
Abschließend haben Sie erfahren, wie Sie eine Zertifikatswiderrufsliste (Certificate Revocation List, CRL) für jedes System erstellen und verteilen, das auf Ihre CA angewiesen ist, um sicherzustellen, dass Benutzer oder Server, die nicht auf Dienste zugreifen sollen, daran gehindert werden.
Jetzt können Sie Zertifikate für Benutzer ausgeben und sie mit Diensten wie OpenVPN verwenden.
Sie können Ihre CA auch verwenden, um Entwicklungs- und Staging-Webserver mit Zertifikaten zu konfigurieren, um Ihre Nicht-Produktionsumgebungen zu sichern.
Die Verwendung einer CA mit TLS-Zertifikaten während der Entwicklung kann dazu beitragen, sicherzustellen, dass Ihr Code und Ihre Umgebungen so gut wie möglich zu Ihrer Produktionsumgebung passen.
Wenn Sie mehr über die Verwendung von OpenSSL erfahren möchten, bietet unser Tutorial OpenSSL-Grundlagen: Arbeiten mit SSL-Zertifikaten, privaten Schlüsseln und CSRs viele zusätzliche Informationen, die Ihnen helfen, sich mit den OpenSSL-Grundlagen vertraut zu machen.
Einrichten und Konfigurieren einer Certificate Authority (CA) unter Debian 10
4017
In diesem Leitfaden lernen wir, wie eine private Zertifizierungsstelle auf einem Debian 10-Server eingerichtet wird und wie mit einer neuen CA ein Testzertifikat erzeugt und signiert wird.
Um dieses Tutorial zu absolvieren, benötigen Sie Zugriff auf einen Debian 10-Server, der Ihren OpenVPN-Dienst hosten kann.
Sie können unserem Leitfaden zur Ersteinrichtung eines Debian-10-Servers folgen, um einen Benutzer mit entsprechenden Berechtigungen einzurichten.
Wenn Sie sich entscheiden, diese Übungsschritte durchzuführen, benötigen Sie einen zweiten Debian-10-Server oder Sie können Ihren eigenen lokalen Linux-Computer verwenden, auf dem Debian oder Ubuntu oder davon abgeleitete Distributionen ausgeführt werden.
Diese Dateien befinden sich im Ordner / usr / share / easy-rsa auf dem CA-Server.
Zuerst werden Sie cd in das Verzeichnis easy-rsa, dann werden Sie die Datei vars mit nano oder Ihrem bevorzugten Texteditor erstellen und bearbeiten:
Nun ist Ihre CA konfiguriert und bereit, als Vertrauensgrundlage für alle Systeme zu fungieren, die Sie für ihre Verwendung konfigurieren möchten. Sie können das Zertifikat der CA zu Ihren OpenVPN-Servern, Webservern, Mail-Servern usw. hinzufügen.
Führen Sie auf Debian- und Ubuntu-basierten Systemen die folgenden Befehle aus, um das Zertifikat zu importieren:
Um das Zertifikat des CA-Servers auf einem CentOS-, Fedora- oder RedHat-basierten System zu importieren, kopieren Sie den Inhalt der Datei und fügen Sie ihn wie im vorherigen Beispiel in das System in eine Datei namens / tmp / ca.crt ein.
Kopieren Sie als Nächstes das Zertifikat nach / etc / pki / ca-trust / source / anchors / und führen Sie dann den Befehl update-ca-trust aus.
Die folgenden Schritte werden auf Ihrem zweiten Linux-System Debian, Ubuntu oder einer Distribution, die von einem dieser Systeme abgeleitet ist, ausgeführt.
In diesem Tutorial haben Sie eine private Zertifizierungsstelle mit dem Easy-RSA-Paket auf einem eigenständigen Debian 10-Server erstellt.
Verstehen der Standardparameter in JavaScript
4004
In ECMAScript 2015 wurden Standardfunktionsparameter in die JavaScript-Sprache eingeführt.
Diese ermöglichen Entwicklern, eine Funktion mit Standardwerten zu initialisieren, wenn dem Funktionsaufruf die Argumente nicht geliefert werden.
Wenn Sie Funktionsparameter auf diese Weise initialisieren, werden Ihre Funktionen leichter lesbar und weniger fehleranfällig, und Sie erhalten ein Standardverhalten für Ihre Funktionen.
Dies hilft Ihnen, Fehler zu vermeiden, die aus der Übergabe von undefined Argumenten und der Destrukturierung nicht vorhandener Objekte resultieren.
In diesem Artikel werden Sie den Unterschied zwischen Parametern und Argumenten überprüfen, lernen, wie man Standardparameter in Funktionen verwendet, alternative Möglichkeiten zur Unterstützung von Standardparametern sehen und erfahren, welche Arten von Werten und Ausdrücken als Standardparameter verwendet werden können.
Sie werden auch Beispiele durcharbeiten, die zeigen, wie Standardparameter in JavaScript funktionieren.
Argumente und Parameter
Bevor die Standardfunktionsparameter erklärt werden, ist es wichtig zu wissen, worauf Parameter standardmäßig eingestellt werden können.
Aus diesem Grund werden wir zunächst den Unterschied zwischen Argumenten und Parametern in einer Funktion untersuchen.
Wenn Sie mehr über diese Unterscheidung erfahren möchten, lesen Sie unseren früheren Artikel in der JavaScript-Reihe, Definieren von Funktionen in JavaScript.
Im folgenden Codeblock erstellen Sie eine Funktion, die die Kubikzahl einer gegebenen Zahl, definiert als x, zurückgibt:
Die Variable x in diesem Beispiel ist ein Parameter - eine benannte Variable, die an eine Funktion übergeben wird.
Ein Parameter muss immer in einer Variable enthalten sein und darf niemals einen direkten Wert haben.
Werfen Sie nun einen Blick auf diesen nächsten Codeblock, der die soeben erstellte Funktion cube anruft:
In diesem Fall ist 10 ein Argument - ein Wert, der einer Funktion beim Aufrufen übergeben wird.
Häufig wird der Wert auch in einer Variable enthalten sein, wie in diesem nächsten Beispiel:
Dies führt zum gleichen Ergebnis:
Wenn Sie ein Argument nicht an eine Funktion übergeben, die ein solches erwartet, wird die Funktion implizit als Wert undefined verwendet:
Dies gibt zurück:
In diesem Fall versucht cube () den Wert von undefined * undefined * undefined zu berechnen, was zu NaN oder "Not a Number", also "Keine Zahl", führt.
Weitere Informationen hierzu finden Sie in dem Abschnitt Verstehen von Datenarten in JavaScript.
Dieses automatische Verhalten kann manchmal ein Problem darstellen.
In einigen Fällen möchten Sie vielleicht, dass der Parameter einen Wert hat, auch wenn kein Argument an die Funktion übergeben wurde.
Hier kommt die Standardparameter-Funktion ins Spiel, ein Thema, das Sie im nächsten Abschnitt behandeln werden.
Syntax der Standardparameter
Durch die Hinzufügung von Standardparametern in ES2015 können Sie nun jedem Parameter einen Standardwert zuweisen, den die Funktion beim Aufruf ohne Argument anstelle von undefined verwendet.
Dieser Abschnitt zeigt Ihnen zunächst, wie Sie dies manuell tun, und führt Sie dann durch die Einstellung von Standardparametern.
Ohne Standardparameter müssten Sie explizit auf undefined Werte prüfen, um Standardwerte zu setzen, wie in diesem Beispiel gezeigt:
Dabei wird mit Hilfe einer bedingten Anweisung geprüft, ob der Wert automatisch als undefined bereitgestellt wurde; dann wird der Wert von x auf 5 gesetzt. Dies führt zu der folgenden Ausgabe:
Im Gegensatz dazu wird durch die Verwendung von Standardparametern dasselbe Ziel mit wesentlich weniger Code erreicht.
Sie können dem Parameter in cube einen Standardwert zuweisen, indem Sie ihm den Gleichheitszuweisungsoperator (=), wie hier hervorgehoben, zuweisen:
Wenn die Funktion cube nun ohne Argument aufgerufen wird, weist sie 5 zu x zu und gibt anstelle von NaN die Berechnung zurück:
Bei der Übergabe eines Arguments funktioniert sie weiterhin wie beabsichtigt, indem sie den Standardwert ignoriert:
Ein wichtiger zu beachtender Vorbehalt ist jedoch, dass der Standardparameterwert auch eine explizite undefined Übergabe als Argument an eine Funktion überschreibt, wie hier gezeigt:
Dies ergibt die Berechnung mit x gleich 5:
In diesem Fall wurden die Standardparameterwerte berechnet und ein expliziter Wert undefined hat sie nicht überschrieben.
Nachdem Sie nun eine Vorstellung von der grundlegenden Syntax von Standardparametern haben, zeigt der nächste Abschnitt, wie Standardparameter mit verschiedenen Datentypen funktionieren.
Datentypen für Standardparameter
Jeder primitive Wert oder jedes Objekt kann als Standardparameterwert verwendet werden.
In diesem Abschnitt sehen Sie, wie diese Flexibilität die Möglichkeiten zur Verwendung von Standardparametern erhöht.
Zuerst setzen Sie Parameter, indem Sie eine Zahl, Zeichenfolge, booleschen Wert, ein Objekt, ein Array und einen Nullwert als Standardwert verwenden.
In diesem Beispiel wird die Syntax der Pfeilfunktion verwendet:
Wenn diese Funktionen ohne Parameter aufgerufen werden, verwenden sie alle die Standardwerte:
Beachten Sie, dass jedes in einem Standardparameter erzeugte Objekt bei jedem Aufruf der Funktion erzeugt wird.
Einer der häufigsten Anwendungsfälle für Standardparameter ist die Verwendung dieses Verhaltens, um Werte aus einem Objekt zu erhalten.
Wenn Sie versuchen, einen Wert aus einem nicht existierenden Objekt zu destrukturieren oder auf einen Wert zuzugreifen, wird ein Fehler ausgelöst.
Wenn der Standardparameter jedoch ein leeres Objekt ist, gibt er Ihnen einfach undefined Werte aus, anstatt einen Fehler zu verursachen:
Dadurch wird der Fehler vermieden, der durch die Destrukturierung nicht existierender Objekte verursacht wird.
Nachdem Sie nun gesehen haben, wie Standardparameter mit verschiedenen Datentypen arbeiten, erklärt der nächste Abschnitt, wie mehrere Standardparameter zusammenarbeiten können.
Verwenden mehrerer Standardparameter
Sie können in einer Funktion beliebig viele Standardparameter verwenden.
In diesem Abschnitt erfahren Sie, wie Sie dies tun und wie Sie damit das DOM in einem realen Beispiel manipulieren können.
Zuerst deklarieren Sie eine Funktion sum () mit mehreren Standardparametern:
Dies führt zu der folgenden Standardberechnung:
Zusätzlich kann der in einem Parameter verwendete Wert in jedem nachfolgenden Standardparameter von links nach rechts verwendet werden.
Beispielsweise erstellt diese Funktion createUser ein Benutzerobjekt userObj als dritten Parameter, und die Funktion selbst gibt nur userObj mit den ersten beiden Parametern zurück:
Wenn Sie user hier aufrufen, erhalten Sie Folgendes:
Es wird normalerweise empfohlen, alle Standardparameter an das Ende einer Parameterliste zu setzen, sodass Sie optionale Werte einfach weglassen können.
Wenn Sie einen Standardparameter zuerst verwenden, müssen Sie undefined explizit übergeben, um den Standardwert zu verwenden.
Hier ist ein Beispiel mit dem Standardparameter am Anfang der Liste:
Wenn Sie diese Funktion aufrufen, müssten Sie defaultFirst () mit zwei Argumenten aufrufen:
Dies würde Folgendes ergeben:
Hier ist ein Beispiel mit dem Standardparameter am Ende der Liste:
Dies würde den gleichen Wert ergeben:
Beide Funktionen haben das gleiche Ergebnis, aber diejenige mit dem Standardwert am Ende ermöglicht einen wesentlich saubereren Funktionsaufruf.
Für ein Beispiel aus der realen Welt ist hier eine Funktion, die ein DOM-Element erstellt und ein Text-Label und Klassen, falls vorhanden, hinzufügt.
Sie können die Funktion mit einigen Klassen in einem Array aufrufen:
Der Aufruf von greeting ergibt den folgenden Wert:
Wenn Sie jedoch das Array classNames aus dem Funktionsaufruf herauslassen, funktioniert die Funktion immer noch.
greeting2 hat nun den folgenden Wert:
In diesem Beispiel kann forEach () auf ein leeres Array ohne Ausgabe angewendet werden.
Wenn dieses leere Array nicht im Standardparameter festgelegt wäre, würden Sie den folgenden Fehler erhalten:
Nachdem Sie nun gesehen haben, wie mehrere Standardparameter zusammenwirken können, können Sie mit dem nächsten Abschnitt fortfahren, um zu sehen, wie Funktionsaufrufe als Standardparameter funktionieren.
Funktionsaufrufe als Standardparameter
Zusätzlich zu den Primitiven und Objekten kann das Ergebnis des Aufrufs einer Funktion als Standardparameter verwendet werden.
In diesem Codeblock erstellen Sie eine Funktion, die eine Zufallszahl zurückgibt und verwenden dann das Ergebnis als Standardparameterwert in einer Funktion cube:
Wenn Sie nun die Funktion cube ohne Parameter aufrufen, hat jeder Aufruf der Funktion potenziell unterschiedliche Ergebnisse:
Die Ausgabe dieser Funktionsaufrufe wird variieren:
Sie können sogar integrierte Methoden verwenden, wie beispielsweise die des Objekts Math, und den in einem Funktionsaufruf zurückgegebenen Wert als Parameter in einer anderen Funktion verwenden.
Im folgenden Beispiel wird x eine Zufallszahl zugewiesen, die als Parameter in der von Ihnen erstellten Funktion cube verwendet wird.
Der Parameter y berechnet dann die Würfelwurzel der Zahl und überprüft, ob x und y gleich sind:
Dadurch ergibt sich Folgendes:
Ein Standardparameter kann sogar, wie in diesem Beispiel, eine Funktionsdefinition sein, die einen Parameter als innere Funktion definiert und den Funktionsaufruf von parameter zurückgibt:
Die Funktion inner wird bei jedem Aufruf der Funktion outer von Grund auf neu erzeugt.
In diesem Artikel haben Sie gelernt, was Standardfunktionsparameter sind und wie sie verwendet werden.
Jetzt können Sie Standardparameter verwenden, um Ihre Funktionen sauber und leicht lesbar zu halten.
Sie können Parametern auch im Voraus leere Objekte und Arrays zuweisen, um sowohl die Komplexität als auch die Codezeilen zu reduzieren, wenn es um Situationen wie das Abrufen von Werten aus einem Objekt oder das Durchschleifen eines Arrays geht.
fcgid ist eine hochleistungsfähige Alternative zu mod _ cgi, die eine ausreichende Anzahl von Instanzen des CGI-Programms startet, um simultane Anfragen zu bearbeiten.
In dieser Datei haben Sie die DocumentRoot auf Ihr neues Verzeichnis und ServerAdmin auf eine E-Mail aktualisiert, auf die der Administrator der Website < ^ > your _ domain < ^ > zugreifen kann.
Installieren von PostgreSQL unter Ubuntu 20.04 Schnellstart
5246
Um PostgreSQL zu installieren, aktualisieren Sie zuerst den lokalen Paketindex Ihres Servers:
Diese ähneln auf gewisse Weise regulären Unix-Benutzern und -Gruppen.
Eine Methode besteht darin, auf Ihrem Server zum Konto postgres zu wechseln, indem Sie Folgendes eingeben:
Dann können Sie die Eingabeaufforderung von Postgres aufrufen, indem Sie Folgendes eingeben:
Damit melden Sie sich bei der PostgreSQL-Eingabeaufforderung an und von hier aus können Sie sofort mit dem Datenbank-Managementsystem arbeiten.
Um die PostgreSQL-Eingabeaufforderung zu beenden, führen Sie Folgendes aus:
Sie können den Befehl, den Sie mit dem postgres-Konto ausführen möchten, auch direkt mit sudo ausführen:
Wenn Sie als postgres-Konto angemeldet sind, können Sie eine neue Rolle erstellen, indem Sie Folgendes eingeben:
In jedem Fall zeigt Ihnen das Skript einige Auswahlmöglichkeiten an und führt basierend auf Ihren Antworten die korrekten Postgres-Befehle aus, um im Sinne Ihrer Angaben einen Benutzer zu erstellen.
Installieren von Python 3 und Einrichten einer Programmierumgebung unter Ubuntu 20.04 Schnellstart
5301
Python ist eine flexible und vielseitige Programmiersprache mit Stärken in den Bereichen Skripting, Automatisierung, Datenanalysen, maschinelles Lernen und Backend-Entwicklung.
Dieses Tutorial führt Sie durch die Installation von Python und das Einrichten einer Programmierumgebung auf einem Ubuntu 20.04-Server.
Eine ausführlichere Version dieses Tutorials mit genaueren Erklärungen zu den einzelnen Schritten finden Sie unter Installieren von Python 3 und Einrichten einer Programmierumgebung auf einem Ubuntu 20.04-Server.
Schritt 1 - Aktualisieren und Upgraden
Melden Sie sich als sudo Nicht-root-Benutzer bei Ihrem Ubuntu 20.04-Server an und führen Sie zuerst eine Aktualisierung und ein Upgrade Ihres Systems durch, um sicherzustellen, dass die bereitgestellte Version von Python 3 aktuell ist.
Bestätigen Sie die Installation, wenn Sie dazu aufgefordert werden.
Schritt 2 - Prüfen der Version von Python
Überprüfen Sie, welche Version von Python 3 installiert ist, indem Sie Folgendes eingeben:
Sie erhalten eine Ausgabe, die der folgenden ähnelt, je nach dem, wann Sie Ihr System aktualisiert haben.
Schritt 3 - Installieren von pip
Um Softwarepakete für Python zu verwalten, installieren Sie pip, ein Tool, das Sie zur Verwaltung von Bibliotheken oder Modulen in Ihren Projekten verwenden können.
Schritt 4 - Installieren von zusätzlichen Tools
Es gibt einige weitere Pakete und Entwicklungstools, die Ihnen dabei helfen, eine robuste Einrichtung für Ihre Programmierumgebung zu erreichen:
Schritt 5 - Installieren von venv
Virtuelle Umgebungen ermöglichen es Ihnen, für Python-Projekte einen isolierten Platz auf Ihrem Server einzurichten.
Wir verwenden venv, ein Teil der standardmäßigen Python 3-Bibliothek, das wir installieren können, indem wir Folgendes eingeben:
Schritt 6 - Erstellen einer virtuellen Umgebung
Sie können mit dem Befehl pyvenv eine neue Umgebung erstellen.
Hier nennen wir unsere neue Umgebung < ^ > my _ env < ^ >, aber Sie sollten Ihrer Umgebung einen aussagekräftigen Namen geben.
Schritt 7 - Aktivieren der virtuellen Umgebung
Aktivieren Sie die Umgebung mit dem folgenden Befehl, wobei < ^ > my _ env < ^ > der Name Ihrer Programmierumgebung ist.
Ihrer Befehlsaufforderung wird nun der Name Ihrer Umgebung vorangestellt:
Schritt 8 - Testen der virtuellen Umgebung
Öffnen Sie den Python-Interpreter:
Beachten Sie, dass Sie innerhalb der virtuellen Python 3-Umgebung den Befehl python anstelle von python3 und pip anstelle von pip3 verwenden können.
Sie wissen, dass Sie sich im Interpreter befinden, wenn Sie die folgende Ausgabe erhalten:
Verwenden Sie nun die Funktion print (), um das traditionelle "Hello, World" -Programm zu erstellen:
Schritt 9 - Deaktivieren der virtuellen Umgebung
Beenden Sie den Python-Interpreter:
Beenden Sie dann die virtuelle Umgebung:
Weiteres Lesematerial
Jetzt sind Sie bereit, um viel über Python zu lernen; hier sind Links, die thematisch mit dem Leitfaden verwandt sind:
Kostenloses E-Book Codieren in Python 3
Python-Tutorials
Ersteinrichtung des Servers mit Ubuntu 20.04
5248
Wenn Sie erstmals einen neuen Ubuntu 20.04 Server einrichten, sollten Sie im Rahmen der grundlegenden Einrichtung einige wichtige Konfigurationsschritte ausführen.
Dadurch erhöhen Sie die Sicherheit und Benutzerfreundlichkeit Ihres Servers und haben eine solide Grundlage für spätere Aktionen.
Schritt 1 - Anmeldung als root
Außerdem benötigen Sie das Passwort oder - wenn Sie einen SSH-Schlüssel zur Authentifizierung installiert haben - den privaten Schlüssel für das Konto des root user.
Wenn Sie sich nicht bereits bei Ihrem Server angemeldet haben, können Sie der Anleitung Verbinden mit Droplets über SSH folgen, die diesen Vorgang ausführlich behandelt.
Wenn Sie nicht bereits mit Ihrem Server verbunden sind, melden Sie sich jetzt als root user mit dem folgenden Befehl an (ersetzen Sie den hervorgehobenen Teil des Befehls mit der öffentlichen IP-Adresse Ihres Servers):
Über root
Der root user ist der administrative Benutzer in einer Linux-Umgebung und verfügt über umfassende Berechtigungen.
Der nächste Schritt ist die Einrichtung eines neuen Benutzerkontos mit reduzierten Berechtigungen für die alltägliche Verwendung.
Später erfahren Sie, wie Sie weitere Berechtigungen erhalten, wenn Sie diese vorübergehend benötigen.
Sobald Sie als root angemeldet sind, können wir das neue Benutzerkonto hinzufügen.
In Zukunft melden wir uns bei diesem neuen Konto anstelle von root an.
Dieses Beispiel erstellt einen neuen Benutzer namens sammy, aber Sie können ihn durch einen beliebigen Benutzernamen ersetzen:
Ihnen werden einige Fragen gestellt. Zunächst nach dem Kontopasswort.
Geben Sie ein starkes Passwort ein und füllen Sie die zusätzlichen Informationen aus, wenn Sie möchten.
Das ist jedoch nicht unbedingt erforderlich und Sie können einfach in jedem Feld, das Sie überspringen möchten, ENTER drücken.
Damit wir uns nicht von unserem normalen Benutzer abmelden und beim root-Konto wieder anmelden müssen, können wir einen sogenannten superuser oder root-Berechtigungen für unser normales Konto einrichten.
Um unserem neuen Benutzer diese Berechtigungen zu geben, müssen wir ihn der sudo-Gruppe hinzufügen.
Standardmäßig dürfen unter Ubuntu 20.04 Benutzer, die Mitglieder der sudo-Gruppe sind, den Befehl sudo verwenden.
Führen Sie als root diesen Befehl aus, um Ihren neuen Benutzer der sudo-Gruppe hinzuzufügen (ersetzen Sie den hervorgehobenen Benutzernamen durch den neuen Benutzer):
Ubuntu 20.04-Server können anhand der UFW-Firewall sicherstellen, dass nur Verbindungen zu bestimmten Diensten zugelassen werden.
Mit dieser Anwendung können wir eine einfache Firewall sehr leicht einrichten.
Anwendungen können bei der Installation ihre Profile mit UFW registrieren.
Diese Profile ermöglichen, dass UFW diese Anwendungen nach dem Namen verwalten kann.
OpenSSH, der Dienst, der es ermöglicht, sich mit unserem Server zu verbinden, hat jetzt ein mit UFW registriertes Profil.
Sie sehen das, wenn Sie Folgendes eingeben:
Wir müssen sicherstellen, dass die Firewall SSH-Verbindungen ermöglicht, sodass wir uns beim nächsten Mal wieder anmelden können.
Wir können diese Verbindungen zulassen, indem wir Folgendes eingeben:
Danach können wir die Firewall aktivieren, indem wir Folgendes eingeben:
Geben Sie y ein und drücken Sie ENTER, um fortzufahren.
Wenn Sie Folgendes eingeben, können Sie sehen, dass SSH-Verbindungen weiterhin erlaubt sind:
Da die Firewall derzeit alle Verbindungen blockiert, ausgenommen für SSH, müssen Sie, um zusätzliche Dienste zu installieren und zu konfigurieren, die Firewall-Einstellungen so anpassen, dass eingehender Datenverkehr erlaubt ist.
In dem Leitfaden UFW-Grundlagen lernen Sie einige der gängigsten UFW-Operationen.
Da es für die tägliche Verwendung jetzt einen regulären Benutzer gibt, müssen wir sicherstellen, dass wir SSH direkt in das Konto einfügen können.
Wenn das root-Konto Passwortauthentifizierung verwendet
Folgen Sie unserer Anleitung zur Einrichtung von SSH-Schlüsseln unter Ubuntu 20.04, um mehr über die Konfiguration von schlüsselbasierter Authentifizierung zu erfahren.
Sie müssen eine Kopie Ihres öffentlichen Schlüssels in die Datei ~ / .ssh / authorized _ keys des neuen Benutzers aufnehmen, um sich erfolgreich anzumelden.
Da sich Ihr öffentlicher Schlüssel bereits in der Datei ~ / .ssh / authorized _ keys des root-Kontos auf dem Server befindet, können wir die Datei- und Verzeichnisstruktur in der laufenden Sitzung in unser neues Benutzerkonto kopieren.
Öffnen Sie jetzt eine neue Terminalsitzung auf dem lokalen Computer und nutzen Sie SSH mit einem neuen Benutzernamen:
Wie geht es jetzt weiter?
So installieren und sichern Sie Redis unter Ubuntu 20.04 Quickstart
5420
Redis ist ein In-Memory-Schlüsselwertspeicher, der für seine Flexibilität, Leistung und breite Sprachunterstützung bekannt ist.
Dieses Quickstart-Tutorial zeigt, wie Sie Redis auf einem Ubuntu 20.04-Server installieren, konfigurieren und sichern.
Um diesen Leitfaden auszuführen, benötigen Sie Zugriff auf einen Ubuntu 20.04-Server, der einen non-root user mit sudo-Berechtigungen und eine mit ufw konfigurierte Firewall aufweist.
Hierzu können Sie unserem Leitfaden zur Ersteinrichtung eines Servers unter Ubuntu 20.04 folgen.
Schritt 1 - Installieren und Konfigurieren von Redis
Als Erstes aktualisieren Sie Ihren lokalen apt-Paketcache:
Installieren Sie anschließend Redis, indem Sie Folgendes eingeben:
Öffnen Sie als Nächstes die Redis-Konfigurationsdatei mit Ihrem bevorzugten Texteditor:
Suchen Sie in der Datei nach der Anweisung supervised, mit der Sie ein Init-System zur Verwaltung von Redis als Dienst deklarieren können.
Da Ubuntu das Init-System systemd verwendet, ändern Sie den Wert von no zu systemd:
Wenn Sie nano zum Bearbeiten der Datei verwendet haben, drücken Sie dazu STRG + X, Y und dann ENTER.
Starten Sie dann den Redis-Dienst neu, damit die Änderungen, die Sie in der Konfigurationsdatei vorgenommen haben, angewendet werden:
Um zu testen, ob Redis richtig funktioniert, stellen Sie mit dem Redis-Befehlszeilenclient redis-cli eine Verbindung zum Server her:
Testen Sie in der folgenden Eingabeaufforderung die Verbindung mit dem Befehl ping:
Diese Ausgabe bestätigt, dass die Serververbindung hergestellt ist.
Als Nächstes überprüfen Sie, ob Sie Schlüssel festlegen können, indem Sie Folgendes ausführen:
Rufen Sie den Wert ab, indem Sie Folgendes eingeben:
Wenn alles funktioniert, können Sie den gespeicherten Wert abrufen:
Nachdem Sie die Bestätigung haben, dass Sie den Wert abrufen können, beenden Sie die Redis-Eingabeaufforderung, um wieder zur Shell zu gelangen:
Schritt 2 - Konfigurieren eines Redis-Passworts
Sie können ein Redis-Passwort direkt in der Konfigurationsdatei von Redis, / etc / redis / redis.conf., konfigurieren.
Öffnen Sie die Datei erneut mit Ihrem bevorzugten Editor:
Scrollen Sie zum Abschnitt SECURITY und suchen Sie eine Anweisung mit der Kommentierung:
Heben Sie die Kommentierung auf, indem Sie # entfernen, und ändern Sie foobared in ein sicheres Passwort:
Nach dem Einrichten des Passworts speichern und schließen Sie die Datei. Starten Sie erneut Redis:
Öffnen Sie den Redis-Client, um zu testen, ob das Passwort funktioniert:
Folgendes zeigt eine Sequenz von Befehlen, mit denen getestet wird, ob das Redis-Passwort funktioniert.
Der erste Befehl versucht, einen Schlüssel auf einen Wert vor der Authentifizierung einzustellen:
Das funktioniert nicht, da Sie keine Authentifizierung durchgeführt haben. Daher gibt Redis einen Fehler aus:
Der nächste Befehl führt die Authentifizierung mit dem Passwort durch, das in der Redis-Konfigurationsdatei angegeben ist:
Redis bestätigt:
Danach wird der vorherige Befehl erfolgreich ausgeführt:
get key1 fragt Redis nach dem Wert des neuen Schlüssels.
Nachdem Sie die Bestätigung haben, dass Sie Befehle im Redis-Client nach Authentifizierung ausführen können, beenden Sie redis-cli:
Schritt 3 - Umbenennen von gefährlichen Befehlen
Die andere in Redis integrierte Sicherheitsfunktion besteht in der Umbenennung oder vollständigen Deaktivierung bestimmter Befehle, die als gefährlich eingestuft werden.
Einige der Befehle, die als gefährlich eingestuft werden, sind: FLUSHDB, FLUSHALL, KEYS, PEXPIRE, DEL, CONFIG, SHUTDOWN, BGREWRITEAOF, BGSAVE, SAVE, SPOP, SREM, RENAME und DEBUG.
Die Deaktivierung oder Umbenennung dieser und anderer Befehle erschwert es unautorisierten Benutzern, Ihre Daten anders zu konfigurieren, zu zerstören oder anderweitig zu vernichten.
Um Redis-Befehle umzubenennen oder zu deaktivieren, öffnen Sie erneut die Konfigurationsdatei:
< $> warning Achtung: Die folgenden Schritte zur Deaktivierung und Umbenennung von Befehlen sind Beispiele.
Sie sollten nur die Befehle deaktivieren oder umbenennen, die für Sie sinnvoll sind.
Sie können die vollständige Liste der Befehle unter redis.io / commands selbst überprüfen und eruieren, wie diese missbraucht werden könnten.
Um einen Befehl zu deaktivieren, benennen Sie ihn einfach wie unten gezeigt in eine leere Zeichenfolge um (gekennzeichnet durch ein Paar Anführungszeichen ohne Zeichen dazwischen):
Zur Umbenennung eines Befehls geben Sie diesem wie bei den unten gezeigten Beispielen einen anderen Namen.
Umbenannte Befehle sollten für andere schwierig zu erraten und für Sie selbst leicht zu merken sein:
Speichern Sie Ihre Änderungen und schließen Sie die Datei.
Nach der Umbenennung eines Befehls wenden Sie die Änderung an, indem Sie Redis neu starten:
Um den neuen Befehl zu testen, gehen Sie in die Redis-Befehlszeile:
Führen Sie dann eine Authentifizierung durch:
Wenn Sie den Befehl CONFIG wie im vorausgehenden Beispiel in ASC12 _ CONFIG umbenannt haben, probieren Sie nun den ursprünglichen Befehl CONFIG aus.
Das sollte fehlschlagen, da Sie diesen umbenannt haben:
Der umbenannte Befehl kann jedoch erfolgreich aufgerufen werden.
Die Groß- und Kleinschreibung muss dabei nicht beachtet werden:
In diesem Quickstart-Tutorial haben Sie Redis installiert und konfiguriert, die korrekte Funktion Ihrer Redis-Installation überprüft und die integrierten Sicherheitsfunktionen genutzt, um sie weniger anfällig für Angriffe böswilliger Akteure zu machen.
So installieren und sichern Sie Redis unter Ubuntu 20.04
5418
Dieses Tutorial zeigt, wie Sie Redis auf einem Ubuntu 20.04-Server installieren, konfigurieren und sichern.
Wir verwenden den APT-Paketmanager, um Redis aus den offiziellen Ubuntu-Repositorys zu installieren.
Zum Zeitpunkt dieses Schreibens ist die in den Standard-Repositorys verfügbare Version < ^ > 5.0.7 < ^ >.
Dadurch werden Redis und seine Abhängigkeiten heruntergeladen und installiert.
Danach müssen Sie eine wichtige Konfigurationsänderung in der Redis-Konfigurationsdatei vornehmen, die bei der Installation automatisch generiert wurde.
Öffnen Sie diese Datei mit Ihrem bevorzugten Texteditor:
Suchen Sie in der Datei die Anweisung supervised.
Mit dieser Anweisung können Sie ein Init-System deklarieren, um Redis als Dienst zu verwalten. Damit erhalten Sie mehr Kontrolle über seine Funktion.
Die Anweisung supervised ist standardmäßig auf no eingestellt.
Da Ubuntu das Init-System systemd verwendet, ändern Sie die Anweisung zu systemd:
Das ist die einzige Änderung, die Sie an dieser Stelle in der Redis-Konfigurationsdatei vornehmen müssen. Wenn Sie fertig sind, speichern und schließen Sie die Datei.
Redis ist nun installiert und konfiguriert und läuft auf Ihrem Rechner.
Bevor Sie Redis benutzen, ist es jedoch ratsam, zunächst zu prüfen, ob es korrekt funktioniert.
Schritt 2 - Testen von Redis
Wie bei jeder neu installierten Software ist es sinnvoll, sicherzustellen, dass Redis wie erwartet funktioniert, bevor weitere Änderungen an der Konfiguration vorgenommen werden.
Wir behandeln in diesem Schritt verschiedene Möglichkeiten, um zu prüfen, ob Redis korrekt funktioniert.
Zuerst überprüfen Sie, ob der Redis-Dienst ausgeführt wird:
Wenn er ohne Fehler ausgeführt wird, gibt dieser Befehl eine Ausgabe, die der folgenden ähnelt:
Hier können Sie sehen, dass Redis ausgeführt wird und bereits aktiviert ist. Das bedeutet, dass Redis bei jedem Boot des Servers automatisch startet.
< $> note Anmerkung: Diese Einstellung ist für viele häufige Anwendungsfälle von Redis wünschenswert.
Falls Sie Redis lieber manuell bei jedem Boot des Servers starten möchten, können Sie das mit folgendem Befehl konfigurieren:
Diese Ausgabe bestätigt, dass die Serververbindung nach wie vor besteht.
Als Letztes testen wir, ob Redis auch nach einem Stopp oder Neustart die Daten noch enthält.
Dazu starten Sie die Redis-Instanz zunächst neu:
Stellen Sie dann erneut mit dem Befehlszeilenclient eine Verbindung her:
Bestätigen Sie, dass Ihr Testwert immer noch verfügbar ist:
Der Wert Ihres Schlüssels sollte immer noch zugänglich sein:
Wenn Sie fertig sind, beenden Sie und kehren zur Shell zurück:
Damit ist die Redis-Installation voll funktionsfähig und kann von Ihnen verwendet werden.
Einige der Standardkonfigurationseinstellungen sind jedoch unsicher und bieten böswilligen Akteuren die Möglichkeit, Ihren Server und seine Daten anzugreifen und sich Zugang dazu zu verschaffen. Die verbleibenden Schritte in diesem Tutorial behandeln Methoden zur Milderung dieser Schwachstellen, wie sie in der offiziellen Redis-Website beschrieben sind.
Diese Schritte sind optional und Redis wird auch dann noch funktionieren, wenn Sie sie nicht befolgen. Es wird jedoch dringend empfohlen, die Schritte auszuführen, um die Sicherheit Ihres Systems zu erhöhen.
Schritt 3 - Binden an localhost
Standardmäßig ist Redis nur von localhost zugänglich.
Wenn Sie Redis jedoch mit einem anderen Tutorial als diesem installiert und konfiguriert haben, haben Sie die Konfigurationsdatei eventuell so geändert, dass sie Verbindungen von überall zulässt.
Das ist nicht so sicher wie eine Bindung an localhost.
Um dies zu korrigieren, öffnen Sie die Redis-Konfigurationsdatei zur Bearbeitung:
Suchen Sie diese Zeile und stellen Sie sicher, dass die Kommentierung aufgehoben ist (entfernen Sie das #, falls vorhanden):
Danach speichern und schließen Sie die Datei (drücken Sie STRG + X, Y und dann ENTER).
Starten Sie dann den Dienst neu, um sicherzustellen, dass systemd Ihre Änderungen liest:
Um zu prüfen, ob diese Änderung angenommen wurde, führen Sie den folgenden netstat-Befehl aus:
< $> note Anmerkung: Der Befehl netstat ist möglicherweise nicht standardmäßig auf Ihrem System verfügbar.
In diesem Fall können Sie ihn (sowie eine Reihe anderer praktischer Netzwerktools) mit dem folgenden Befehl installieren:
Die Ausgabe zeigt, dass das redis-server-Programm an localhost (127.0.0.1) gebunden ist und zeigt die Änderung, die Sie gerade in der Konfigurationsdatei vorgenommen haben.
Wenn Sie eine andere IP-Adresse in dieser Spalte sehen, z. B. (0.0.0.0), sollten Sie überprüfen, ob Sie die Kommentierung in der korrekten Zeile aufgehoben haben und den Redis-Dienst erneut starten.
Nachdem die Redis-Installation nun nur auf localhost lauscht, wird es für böswillige Akteure schwieriger, Anfragen zu stellen oder Zugriff auf Ihren Server zu erhalten.
Allerdings ist Redis derzeit nicht so eingestellt, dass Benutzer sich authentifizieren müssen, bevor sie Änderungen an der Konfiguration oder den gespeicherten Daten von Redis vornehmen können.
Um hier Abhilfe zu schaffen, können Sie mit Redis verlangen, dass Benutzer sich mit einem Passwort authentifizieren, bevor sie Änderungen über den Redis-Client redis-cli vornehmen.
Schritt 4 - Konfigurieren eines Redis-Passworts
Durch die Konfiguration eines Redis-Passworts wird eine der beiden in Redis integrierten Sicherheitsfunktionen befähigt - der Befehl auth, mit dem sich Clients für den Zugriff auf die Datenbank authentifizieren müssen.
Das Passwort wird direkt in der Konfigurationsdatei von Redis, / etc / redis / redis.conf, konfiguriert. Öffnen Sie diese Datei erneut mit Ihrem bevorzugten Editor:
Heben Sie die Kommentierung auf, indem Sie # entfernen, und ändern Sie foobared in ein sicheres Passwort.
< $> note Anmerkung: Über der Anweisung requirepass in der Datei redis.conf gibt es eine kommentierte Warnung:
Daher ist es wichtig, dass Sie einen sehr starken und sehr langen Wert als Ihr Passwort angeben.
Anstatt ein Passwort selbst einzurichten, können Sie den Befehl openssl verwenden, um ein Zufallspasswort zu generieren, wie im folgenden Beispiel.
Durch Weiterleiten der Ausgabe des ersten Befehls an den zweiten openssl-Befehl, wie hier gezeigt, werden alle durch den ersten Befehl erzeugten Zeilenumbrüche entfernt:
Ihre Ausgabe sollte ungefähr wie folgt aussehen:
Nach dem Kopieren und Einfügen der Ausgabe dieses Befehls als den neuen Wert für requirepass sollte es so aussehen:
Als Nächstes befassen wir uns mit der Umbenennung von Redis-Befehlen, die Ihrem Rechner schweren Schaden zufügen können, falls sie versehentlich oder durch einen bösartigen Akteur eingegeben werden.
Schritt 5 - Umbenennen von gefährlichen Befehlen
Diese Befehle können von unautorisierten Benutzern dazu verwendet werden, Ihre Daten anders zu konfigurieren, zu zerstören oder anderweitig zu löschen. Wie das Authentifizierungs-Passwort wird das Umbenennen oder Deaktivieren von Befehlen im gleichen SECURITY-Abschnitt der Datei / etc / redis / redis.conf konfiguriert.
Die Liste ist nicht umfassend, aber das Umbenennen oder Deaktivieren aller Befehle in dieser Liste ist ein guter Ausgangspunkt, um die Sicherheit Ihres Redis-Servers zu erhöhen.
Ob Sie einen Befehl deaktivieren oder umbenennen sollten, hängt von Ihren spezifischen Bedürfnissen oder von denen Ihrer Website ab.
Wenn Sie für einen Befehl, der missbraucht werden könnte, keine Verwendung haben, können Sie ihn deaktivieren. Andernfalls liegt es in Ihrem Interesse, diesen umzubenennen.
Führen Sie eine Authentifizierung durch:
Nehmen wir an, dass Sie den Befehl CONFIG so wie im voherigen Beispiel in ASC12 _ CONFIG umbenannt haben.
Versuchen Sie zunächst, den ursprünglichen Befehl CONFIG zu verwenden.
Zum Schluss können Sie redis-cli verlassen:
Beachten Sie, dass Sie sich erneut authentifizieren müssen, wenn Sie bereits die Befehlszeile von Redis verwenden und Redis neu starten.
Andernfalls erhalten Sie diesen Fehler, wenn Sie einen Befehl eingeben:
< $> warning Bezüglich der Umbenennung von Befehlen wird am Ende des Abschnitts SECURITY in / etc / redis / redis.conf folgende Warnung angegeben:
Hinweis: Das Redis-Projekt verwendet die Begriffe "master" und "slave", während DigitalOcean generell die Alternativen "primary" und "secondary" bevorzugt.
Um Verwirrungen zu vermeiden, werden an dieser Stelle die in der Redis-Dokumentation genutzten Begriffe verwendet.
Das bedeutet, dass es kein Problem geben sollte, wenn der umbenannte Befehl nicht in der AOF-Datei enthalten ist, oder wenn er enthalten ist, aber die AOF-Datei nicht an Slaves übertragen wurde.
Denken Sie bitte daran, wenn Sie Befehle umbenennen möchten.
Der beste Zeitpunkt zur Umbenennung eines Befehls ist dann, wenn Sie keine AOF-Persistenz nutzen. Ein anderer Zeitpunkt ist direkt nach der Installation, bevor die Anwendung bereitgestellt wird, die Redis verwendet.
Wenn Sie AOF verwenden und mit einer Master-Slave-Installation arbeiten, sollten Sie diese Antwort von der GitHub-Frageseite des Projekts beachten.
Nachfolgend eine Antwort auf die Frage des Autors:
Die Befehle werden in der AOF protokolliert und auf die gleiche Weise, in der sie gesendet werden, an den Slave repliziert. Wenn Sie also versuchen, die AOF auf einer Instanz, die nicht die gleiche Umbenennung hat, erneut wiederzugeben, kann es zu Inkonsistenzen kommen, da der Befehl nicht ausgeführt werden kann (dasselbe gilt für Slaves).
Daher ist es in Fällen wie diesen am besten, bei der Umbenennung sicherzustellen, dass umbenannte Befehle auf alle Instanzen in Master-Slave-Installationen angewendet werden.
In diesem Tutorial haben Sie Redis installiert und konfiguriert, die korrekte Funktion Ihrer Redis-Installation überprüft und die integrierten Sicherheitsfunktionen genutzt, um sie weniger anfällig für Angriffe böswilliger Akteure zu machen.
Denken Sie daran, dass die Sicherheitsfunktionen von Redis, die wir eingerichtet haben, sehr leicht zu umgehen sind, sobald sich jemand auf Ihrem Server angemeldet hat.
Daher ist die wichtigste Sicherheitseinrichtung auf Ihrem Redis-Server Ihre Firewall (die Sie konfiguriert haben, wenn Sie in den Voraussetzungen dem Leitfaden zur Ersteinrichtung eines Servers gefolgt sind). Diese Barriere ist für böswillige Akteure nur sehr schwer zu überwinden.
So sichern Sie Apache mit Let 's Encrypt unter Ubuntu 20.04
5386
Let 's Encrypt ist eine Zertifizierungsstelle (Certificate Authority, CA), die das Abrufen und Installieren von kostenlosen TLS- / SSL-Zertifikaten erleichtert und so verschlüsseltes HTTPS auf Webservern ermöglicht.
Es vereinfacht den Prozess, indem ein Software-Client, Certbot, bereitgestellt wird, der versucht, die meisten (wenn nicht alle) der erforderlichen Schritte zu automatisieren.
Derzeit ist der gesamte Prozess zum Abrufen und Installieren eines Zertifikats sowohl auf Apache als auch auf Nginx vollständig automatisiert.
In diesem Leitfaden verwenden wir Certbot, um ein kostenloses SSL-Zertifikat für Apache unter Ubuntu 20.04 zu erhalten, und stellen sicher, dass dieses Zertifikat so eingerichtet ist, dass es automatisch erneuert wird.
In diesem Tutorial wird anstelle der Standardkonfigurationsdatei von Apache eine separate virtuelle Hostdatei zum Einrichten der Website verwendet, die von Let 's Encrypt gesichert wird.
Wir empfehlen, neue virtuelle Apache-Hostdateien für jede auf einem Server gehostete Domäne zu erstellen, da dies dazu beiträgt, häufige Fehler zu vermeiden und die Standardkonfigurationsdateien als Fallback-Setup beizubehalten.
Einen Ubuntu 20.04-Server, der gemäß dem Tutorial Ersteinrichtung des Servers für Ubuntu 20.04 eingerichtet wurde, einschließlich eines sudo non-root users und einer Firewall.
In diesem Tutorial wird your _ domain durchgehend als Beispiel verwendet.
Apache gemäß Installieren von Apache unter Ubuntu 20.04 installiert.
Stellen Sie sicher, dass Sie eine virtuelle Hostdatei für Ihre Domäne haben.
In diesem Tutorial wird / etc / apache2 / sites-available / < ^ > your _ domain < ^ > .conf als Beispiel verwendet.
Schritt 1 - Installieren von Certbot
Um ein SSL-Zertifikat mit Let 's Encrypt zu erhalten, müssen wir zuerst die Certbot-Software auf Ihrem Server installieren.
Wir werden dafür die Standard-Ubuntu-Paket-Repositorys verwenden.
Wir benötigen zwei Pakete: certbot und python3-certbot-apache.
Letzteres ist ein Plugin, das Certbot in Apache integriert und ermöglicht, das Abrufen eines Zertifikats und das Konfigurieren von HTTPS auf Ihrem Webserver mit einem einzigen Befehl zu automatisieren.
Außerdem werden Sie zur Bestätigung der Installation aufgefordert, indem Sie Y und dann ENTER drücken.
Certbot ist jetzt auf Ihrem Server installiert.
Im nächsten Schritt verifizieren wir die Konfiguration von Apache, um sicherzustellen, dass Ihr virtueller Host angemessen festgelegt ist.
Dadurch wird sichergestellt, dass das Certbot-Client-Skript Ihre Domänen erkennen und Ihren Webserver so konfigurieren kann, dass Ihr neu generiertes SSL-Zertifikat automatisch verwendet wird.
Schritt 2 - Überprüfen Ihrer Apache Virtual Host-Konfiguration
Um SSL für Ihren Webserver automatisch abrufen und konfigurieren zu können, muss Certbot den richtigen virtuellen Host in Ihren Apache-Konfigurationsdateien finden.
Ihre Serverdomänennamen werden aus den Anweisungen ServerName und ServerAlias abgerufen, die in Ihrem VirtualHost-Konfigurationsblock definiert sind.
Wenn Sie den Schritt zum Einrichten des virtuellen Hosts im Tutorial zur Apache-Installation ausgeführt haben, sollten Sie einen VirtualHost-Block für Ihre Domäne unter / etc / apache2 / sites-available / < ^ > your _ domain < ^ > .conf mit dem ServerName und auch den ServerAlias-Anweisungen, die bereits entsprechend festgelegt wurden, einrichten.
Um dies zu überprüfen, öffnen Sie die virtuelle Hostdatei für Ihre Domäne mit nano oder Ihrem bevorzugten Texteditor:
Suchen Sie die bestehenden Zeilen ServerName und ServerAlias.
Sie sollten wie folgt aussehen:
Wenn Sie Ihren ServerName und Ihre ServerAlias bereits so eingerichtet haben, können Sie Ihren Texteditor beenden und mit dem nächsten Schritt fortfahren.
Wenn Sie nano verwenden, können Sie zum Beenden STRG + X drücken, dann Y und ENTER, um zu bestätigen.
Wenn Ihre aktuelle virtuelle Hostkonfiguration nicht dem Beispiel entspricht, aktualisieren Sie sie entsprechend.
Wenn Sie fertig sind, speichern Sie die Datei und beenden Sie den Editor.
Führen Sie dann den folgenden Befehl aus, um Ihre Änderungen zu validieren:
Sie sollten eine Syntax OK als Antwort erhalten.
Wenn Sie einen Fehler erhalten, öffnen Sie die virtuelle Hostdatei und überprüfen Sie sie auf Schreibfehler oder fehlende Zeichen.
Sobald die Syntax Ihrer Konfigurationsdatei korrekt ist, laden Sie Apache neu, sodass die Änderungen wirksam werden:
Mit diesen Änderungen kann Certbot den richtigen VirtualHost-Block finden und ihn aktualisieren.
Als Nächstes aktualisieren wir die Firewall, um den HTTPS-Datenverkehr zu ermöglichen.
Schritt 3 - Zulassen von HTTPS durch die Firewall
Wenn Sie die UFW-Firewall aktiviert haben, wie in den erforderlichen Anleitungen empfohlen, müssen Sie die Einstellungen anpassen, um den HTTPS-Datenverkehr zuzulassen.
Bei der Installation registriert Apache einige verschiedene UFW-Anwendungsprofile.
Wir können das Profil Apache Full nutzen, um sowohl HTTP- als auch HTTPS-Datenverkehr auf Ihrem Server zuzulassen.
Um zu verifizieren, welche Art von Datenverkehr derzeit auf Ihrem Server erlaubt ist, können Sie Folgendes verwenden:
Wenn Sie einem unserer Apache-Installationsleitfäden gefolgt sind, sollte Ihre Ausgabe ungefähr so aussehen, was bedeutet, dass derzeit nur HTTP-Datenverkehr auf Port 80 zulässig ist:
Um zusätzlich den HTTPS-Datenverkehr einzulassen, lassen Sie das Profil "Apache Full" zu und löschen Sie das redundante Profil "Apache ":
Ihr Status sieht nun wie folgt aus:
Sie können nun Certbot ausführen und Ihre Zertifikate abrufen.
Schritt 4 - Abrufen eines SSL-Zertifikats
Certbot bietet eine Vielzahl von Möglichkeiten, um SSL-Zertifikate über Plugins zu erhalten.
Das Apache-Plugin kümmert sich um die Neukonfiguration von Apache und das Neuladen der Konfiguration bei Bedarf.
Geben Sie Folgendes ein, um dieses Plugin zu verwenden:
In diesem Skript müssen Sie eine Reihe von Fragen beantworten, um Ihr SSL-Zertifikat zu konfigurieren.
Zunächst werden Sie nach einer gültigen E-Mail-Adresse gefragt.
Diese E-Mail wird für Erneuerungsbenachrichtigungen und Sicherheitshinweise verwendet:
Nach dem Bereitstellen einer gültigen E-Mail-Adresse drücken Sie ENTER, um mit dem nächsten Schritt fortzufahren.
Sie werden dann aufgefordert, zu bestätigen, ob Sie den Nutzungsbedingungen von Let 's Encrypt zustimmen.
Sie können dies bestätigen, indem Sie A drücken und dann ENTER:
Als Nächstes werden Sie gefragt, ob Sie Ihre E-Mail mit der Electronic Frontier Foundation teilen möchten, um Nachrichten und andere Informationen zu erhalten.
Wenn Sie ihren Inhalt nicht abonnieren möchten, geben Sie N ein. Andernfalls geben Sie Y ein. Drücken Sie anschließend ENTER, um mit dem nächsten Schritt fortzufahren.
Im nächsten Schritt werden Sie aufgefordert, Certbot darüber zu informieren, für welche Domänen Sie HTTPS aktivieren möchten.
Die aufgelisteten Domänennamen werden automatisch aus Ihrer Konfiguration des virtuellen Apache-Hosts abgerufen. Aus diesem Grund müssen Sie sicherstellen, dass auf Ihrem virtuellen Host die richtigen Einstellungen für ServerName und ServerAlias konfiguriert sind.
Wenn Sie HTTPS für alle aufgelisteten Domänennamen aktivieren möchten (empfohlen), können Sie die Eingabeaufforderung leer lassen und ENTER drücken, um fortzufahren.
Wählen Sie andernfalls die Domänen aus, für die Sie HTTPS aktivieren möchten, indem Sie die entsprechende Nummer durch Kommas und / oder Leerzeichen getrennt auflisten und dann ENTER drücken.
Als Nächstes werden Sie aufgefordert, auszuwählen, ob der HTTP-Datenverkehr an HTTPS umgeleitet werden soll.
In der Praxis bedeutet dies, dass jemand, der Ihre Website über unverschlüsselte Kanäle (HTTP) besucht, automatisch an die HTTPS-Adresse Ihrer Website umgeleitet wird.
Wählen Sie 2, um die Umleitung zu aktivieren, oder 1, wenn Sie sowohl HTTP als auch HTTPS als separate Methoden für den Zugriff auf Ihre Website beibehalten möchten.
Nach diesem Schritt ist die Konfiguration von Certbot abgeschlossen, und Sie erhalten die letzten Anmerkungen zu Ihrem neuen Zertifikat, wo sich die generierten Dateien befinden und wie Sie Ihre Konfiguration mit einem externen Tool testen können, das die Authentizität Ihres Zertifikats analysiert:
Ihr Zertifikat ist nun installiert und in die Konfiguration von Apache geladen.
Versuchen Sie, Ihre Website mit https: / / neu zu laden und beachten Sie den Sicherheitsindikator Ihres Browsers.
Er sollte darauf hinweisen, dass Ihre Website ordnungsgemäß gesichert ist, normalerweise durch Einfügen eines Schlosssymbols in die Adressleiste.
Mit dem SSL Labs Server-Test können Sie die Note Ihres Zertifikats überprüfen und detaillierte Informationen dazu aus der Sicht eines externen Dienstes abrufen.
Im nächsten und letzten Schritt testen wir die automatische Erneuerungsfunktion von Certbot, die garantiert, dass Ihr Zertifikat vor dem Ablaufdatum automatisch erneuert wird.
Schritt 5 - Überprüfen der automatischen Erneuerung von Certbot
Zertifikate von Let "s Encrypt sind nur neunzig Tage gültig.
Dies soll Benutzer dazu ermutigen, ihren Prozess zur Erneuerung von Zertifikaten zu automatisieren und sicherzustellen, dass missbrauchte Zertifikate oder gestohlene Schlüssel eher früher als später ablaufen.
Das von uns installierte certbot-Paket kümmert sich um Erneuerungen, indem es ein Erneuerungsskript in / etc / cron.d einfügt, das von einem systemctl-Dienst namens certbot.timer verwaltet wird.
Dieses Skript wird zweimal pro Tag ausgeführt und erneuert automatisch alle Zertifikate, die innerhalb von dreißig Tagen ablaufen.
Um den Status dieses Dienstes zu überprüfen und sicherzustellen, dass er aktiv ist und ausgeführt wird, können Sie Folgendes verwenden:
Sie sehen eine Ausgabe, die dieser ähnelt:
Um den Erneuerungsprozess zu testen, können Sie mit certbot einen Probelauf durchführen:
Wenn Sie keine Fehler sehen, sind Sie fertig. Bei Bedarf erneuert Certbot Ihre Zertifikate und lädt Apache neu, um die Änderungen zu übernehmen.
Wenn der automatische Erneuerungsprozess jemals fehlschlägt, sendet Let 's Encrypt eine Nachricht an die von Ihnen angegebene E-Mail und warnt Sie, wenn Ihr Zertifikat bald abläuft.
In diesem Lernprogramm haben Sie den Let 's Encrypt-Client Certbot installiert, ein SSL-Zertifikat für Ihre Domäne konfiguriert und installiert und bestätigt, dass der automatische Erneuerungsdienst von Certbot in systemctl aktiv ist.
Wenn Sie weitere Fragen zur Verwendung von Certbot haben, ist die Dokumentation ein guter Ausgangspunkt.
Entfernen von Docker-Images, -Containern und -Volumen
2109
Ein Spickzettel für Docker
Docker erleichtert das Einschließen Ihrer Anwendungen und Dienste in Containern, damit Sie sie überall ausführen können.
Bei der Arbeit mit Docker kann es leicht passieren, dass viele nicht verwendete Images, Container und Datenvolumen gesammelt werden, die die Ausgabe überladen und viel Festplattenspeicher verbrauchen.
Docker gibt Ihnen alle Tools zur Hand, die Sie für die Bereinigung Ihres Systems über die Befehlszeile benötigen.
Dieser Leitfaden in Form eines Spickzettels dient als schnelle Referenz zu Befehlen, die für das Freigeben von Festplattenspeicher und das Organisieren Ihres Systems durch das Entfernen nicht verwendeter Docker-Images, -Container und -Volumen nützlich sind.
Verwendung dieses Leitfadens:
Dieser Leitfaden weist ein Spickzettelformat mit in sich geschlossenen Befehlszeilensnippets auf.
Springen Sie zu einem beliebigen Abschnitt, der für die Aufgabe, die Sie ausführen möchten, relevant ist.
Die Befehlsersatzsyntax < ^ > command < ^ > $(< ^ > command < ^ >), die in vielen Befehlen genutzt wird, ist in vielen beliebten Shells wie Bash, Zsh und Windows Powershell verfügbar.
Löschen aller unbenutzten oder unreferenzierten Images, Container, Volumen und Netzwerke
Docker bietet einen einzelnen Befehl zur Bereinigung beliebiger Ressourcen - Images, Container, Volumen und Netzwerke -, die unreferenziert (also nicht mit einem Container verknüpft) sind:
Um auch angehaltene Container und alle nicht verwendeten Images zu entfernen (nicht nur unreferenzierte Images), fügen Sie dem Befehl das Flag -a hinzu:
Entfernen von Docker-Images
Entfernen von einem oder mehreren spezifischen Images
Verwenden Sie den Befehl docker images mit dem Flag -a, um die ID der Images zu finden, die Sie entfernen möchten.
Dadurch wird Ihnen jedes Image angezeigt, einschließlich dazwischenliegender Image-Ebenen.
Wenn Sie die Images gefunden haben, die Sie löschen möchten, können Sie ihre ID oder ihr Tag docker rmi übergeben:
Auflisten:
Entfernen:
Entfernen von unreferenzierten Images
Docker-Images bestehen aus mehreren Ebenen.
Unreferenzierte Images sind Ebenen, die keine Beziehung zu getaggten Images haben.
Sie dienen keinem Zweck mehr und verbrauchen Festplattenspeicher.
Sie lassen sich finden, indem Sie das Filter-Flag (-f) mit dem Wert dangling = true zum Befehl docker images hinzufügen.
Wenn Sie sicher sind, dass Sie sie löschen möchten, können Sie den Befehl docker images purge verwenden:
< $> note Anmerkung: Wenn Sie ein Image erstellen, ohne es mit einem Tag zu versehen, wird das Image in der Liste der unreferenzierten Images angezeigt, da es keine Zuordnung zu einem getaggten Image aufweist.
Sie können diese Situation vermeiden, indem Sie beim Erstellen ein Tag angeben; außerdem können Sie Images mit dem Befehl docker tag im Nachhinein mit einem Tag versehen.
Entfernen von Images gemäß einem Muster
Sie können alle Images finden, die mit einem Muster übereinstimmen, indem Sie eine Kombination aus docker images und grep nutzen.
Wenn Sie damit zufrieden sind, können Sie sie durch awk löschen, um die IDs an docker rmi zu übergeben.
Beachten Sie, dass diese Dienstprogramme nicht von Docker bereitgestellt werden und nicht unbedingt für alle Systeme verfügbar sind:
Alle Images entfernen
Alle Docker-Images in einem System können aufgelistet werden, indem Sie dem Befehl docker images -a hinzufügen.
Wenn Sie sicher sind, dass Sie alle löschen möchten, können Sie das Flag -q hinzufügen, um die Image-ID an docker rmi zu übergeben:
Entfernen von Containern
Entfernen von einem oder mehreren spezifischen Containern
Verwenden Sie den Befehl docker ps mit dem Flag -a, um den Namen oder die ID der Container zu finden, die Sie entfernen möchten.
Entfernen eines Containers beim Beenden
Falls Sie bei der Erstellung eines Containers wissen, dass Sie ihn nicht behalten möchten, wenn Sie damit fertig sind, können Sie docker run --rm ausführen, um ihn beim Beenden automatisch zu löschen.
Ausführen und entfernen:
Entfernen aller beendeten Container
Sie können Container mit docker ps -a finden und anhand ihres Status filtern: erstellt, wird neu gestartet, wird ausgeführt, angehalten oder beendet.
Zum Überprüfen der Liste der beendeten Container verwenden Sie das Flag -f, um anhand des Status zu filtern.
Wenn Sie wissen, dass Sie diese Container entfernen möchten, nutzen Sie -q, um die IDs an den Befehl docker rm zu übergeben.
Entfernen von Containern mit mehr als einem Filter
Docker-Filter lassen sich kombinieren, indem Sie das Filter-Flag mit einem zusätzlichen Wert wiederholen.
Dadurch wird eine Liste von Containern angezeigt, die beide Bedingungen erfüllen.
Wenn Sie zum Beispiel alle Container löschen möchten, die als entweder Created (ein Zustand, der bei der Ausführung eines Containers mit ungültigem Befehl auftreten kann) oder Exited markiert sind, können Sie zwei Filter verwenden:
Entfernen von Containern gemäß einem Muster
Sie können alle Container finden, die mit einem Muster übereinstimmen, indem Sie eine Kombination von docker ps und grep nutzen.
Wenn Sie soweit sind, dass Sie über die Liste verfügen, die Sie löschen möchten, können Sie awk und xargs verwenden, um die ID an docker rmi zu übergeben.
Beachten Sie, dass diese Dienstprogramme nicht von Docker bereitgestellt werden und nicht unbedingt für alle Systeme verfügbar sind:
Stoppen und Entfernen aller Container
Sie können die Container in Ihrem System mit docker ps überprüfen.
Durch Hinzufügen des Flag -a werden alle Container angezeigt.
Wenn Sie sicher sind, dass Sie sie löschen möchten, können Sie das Flag -q hinzufügen, um die IDs an die Befehle docker stop und docker rm zu übergeben:
Entfernen von Volumen
Entfernen von einem oder mehreren spezifischen Volumen (Docker 1.9 und höher)
Verwenden Sie den Befehl docker volume ls, um nach dem bzw. den Volumennamen zu suchen, die Sie löschen möchten.
Dann können Sie ein oder mehrere Volumen mit dem Befehl docker volume rm entfernen:
Entfernen unreferenzierter Images (Docker 1.9 und höher)
Da die Aufgabe von Volumen darin besteht, unabhängig von Containern zu existieren, wird ein Volumen bei der Entfernung eines Containers nicht automatisch entfernt.
Wenn ein Volumen vorhanden ist und nicht mehr mit Containern verbunden ist, wird es als unreferenziertes Volumen bezeichnet.
Um sie zu finden und zu bestätigen, dass Sie sie entfernen möchten, können Sie den Befehl docker volume ls mit einem Filter verwenden, um die Ergebnisse auf unreferenzierte Volumen zu beschränken.
Wenn Sie mit der Liste zufrieden sind, können Sie sie alle mit docker volume prune entfernen:
Entfernen eines Containers und seines Volumen
Wenn Sie ein nicht benanntes Volumen erstellt haben, können Sie es mit dem Flag -v zusammen mit dem Container löschen.
Beachten Sie, dass das nur bei nicht benannten Volumen funktioniert.
Wenn der Container erfolgreich entfernt wurde, wird seine ID angezeigt.
Beachten Sie, dass es keinen Hinweis auf die Entfernung des Volumens gibt.
Wenn es nicht benannt ist, wird es leise aus dem System entfernt.
Wenn es benannt ist, bleibt es im Stillen vorhanden.
Dieser Leitfaden behandelt gängige Befehle, die zum Entfernen von Images, Containern und Volumen bei Docker verwendet werden.
Es gibt viele weitere Kombinationen und Flags, die dafür genutzt werden können.
Einen umfassenden Leitfaden zu den verfügbaren Befehlen finden Sie in der Docker-Dokumentation für docker system prune, docker rmi, docker rm und docker volume rm.
Wenn es allgemeine Bereinigungsaufgaben gibt, die im Leitfaden besprochen werden sollten, fragen Sie nach oder machen Sie in den Kommentaren Vorschläge.
Installieren des Linux-, Apache-, MySQL-, PHP- (LAMP-) Stacks unter Ubuntu 20.04 Schnellstart
5476
In diesem Schnellstartleitfaden installieren wir einen LAMP-Stack auf einem Ubuntu 20.04-Server.
Eine detailliertere Version dieses Tutorials mit weiteren Erklärungen zu einzelnen Schritten finden Sie unter Installieren des Linux-, Apache-, MySQL-, PHP- (LAMP-) Stacks unter Ubuntu 20.04.
Um diesem Leitfaden zu folgen, müssen Sie als sudo-Benutzer auf einen Ubuntu 20.04-Server zugreifen können.
Schritt 1 - Installieren von Apache
Aktualisieren Sie den Cache Ihres Paketmanagers und installieren Sie dann Apache mit:
Sobald die Installation abgeschlossen ist, müssen Sie Ihre Firewalleinstellungen so anpassen, dass HTTP-Verkehr auf Ihrem Server zugelassen wird.
Führen Sie den folgenden Befehl aus, um externen Zugriff an Port 80 (HTTP) zuzulassen:
Schritt 2 - Installieren von MySQL
Wir installieren jetzt MySQL, ein beliebtes Datenbankverwaltungssystem, das in PHP-Umgebungen verwendet wird.
Ihr Server fordert Sie als Nächstes dazu auf, ein Passwort für den MySQL-Benutzer root auszuwählen und zu bestätigen.
Konsultieren Sie den Schritt 6 unseres detaillierten Leitfadens für LAMP unter Ubuntu 20.04, um mehr zu erfahren.
Schritt 3 - Installieren von PHP
Um PHP und seine Abhängigkeiten zu installieren, führen Sie Folgendes aus:
Schritt 4 - Erstellen eines virtuellen Hosts für Ihre Website
Öffnen Sie dann mit Ihrem bevorzugten Befehlszeileneditor eine neue Konfigurationsdatei im Verzeichnis sites-available von Apache:
Wenn Sie nano verwenden, können Sie STRG + X, dann Y und abschließend ENTER drücken.
Schritt 5 - Testen von PHP mit Apache
Wir erstellen jetzt ein PHP-Testskript, um zu prüfen, ob Apache Anfragen für PHP-Dateien bearbeiten kann.
Fügen Sie in der Datei folgenden Inhalt hinzu:
Rufen Sie in Ihrem Webbrowser den Domänenamen oder die IP-Adresse Ihres Servers auf, gefolgt vom Skriptnamen (in diesem Fall info.php):
Ersteinrichtung des Servers unter Ubuntu 20.04
Verwalten von DNS-Einträgen auf DigitalOcean
Wie man Apache mit Let 's Encrypt unter Ubuntu 20.04 sichert
So erstellen Sie ein neuronales Netz zum Übersetzen der Gebärdensprache ins Englische
5505
Der Autor hat Code.org ausgewählt, um im Rahmen des Programms Write for DOnations eine Spende zu erhalten.
Computer Vision (deutsch: computerbasiertes Sehen) ist ein Teilbereich der Informatik, mit dem ein höherrangiges Verstehen von Bildern und Videos ermöglicht werden soll.
Damit werden Technologien wie lustige Video-Chat-Filter, die Gesichtserkennung Ihres Mobilgeräts und selbstfahrende Autos unterstützt.
In diesem Tutorial nutzen Sie Computer Vision, um einen Übersetzer für die amerikanische Gebärdensprache zu entwickeln, der mithilfe Ihrer Webcam arbeitet.
Während des Tutorial werden Sie OpenCV, eine Computer-Vision-Bibliothek, PyTorch zum Einrichten eines tiefen neuronalen Netzes und onnx zum Exportieren Ihres neuronalen Netzes verwenden.
Zudem werden Sie eine Computer-Vision-Anwendung erstellen und dabei folgende Konzepte anwenden:
Sie verwenden dieselbe dreistufige Methode, die auch im Tutorial How To Apply Computer Vision to Build an Emotion-Based Dog Filter (So wenden Sie Computer Vision beim Erstellen eines emotionsbasierten Hundefilters an) genutzt wird: Vorverarbeitung eines Datensatzes, Trainieren eines Modells und Bewertung des Modells.
Außerdem werden Sie jeden dieser einzelnen Schritte erweitern: Sie nutzen Data Augmentation (Datenanreicherung) für den Umgang mit gedrehten oder nicht zentrierten Händen, Sie ändern die Learning Rate Schedules (Zeitpläne für die Lernrate), um die Modellgenauigkeit zu verbessern, und Sie exportieren Modelle für eine höhere Inferenzgeschwindigkeit.
Überdies werden Sie auch verwandte Konzepte im maschinellen Lernen erkunden.
Am Ende dieses Tutorials verfügen Sie über einen Übersetzer für die amerikanische Gebärdensprache sowie über ein umfassendes Know-how über das Deep Learning.
Sie können auch auf den kompletten Quellcode für dieses Projekt zugreifen.
Eine lokale Entwicklungsumgebung für Python 3 mit mindestens 1 GB RAM.
Unter How to Install and Set Up a Local Programming Environment for Python 3 (Installieren und Einrichten einer lokalen Programmierumgebung für Python 3) finden Sie Informationen darüber, wie Sie die benötigten Konfigurationen vornehmen.
Eine funktionierende Webcam zur Nutzung der Bilderkennung in Echtzeit.
(Empfohlen) Build an Emotion-Based Dog Filter (Erstellen eines emotionsbasierten Hundes); dieses Tutorial wird zwar nicht explizit verwendet, aber es wird dasselbe Wissen vermittelt und darauf aufgebaut.
Schritt 1 - Erstellen des Projekts und Installieren von Abhängigkeiten
Wir wollen einen Arbeitsbereich für dieses Projekt erstellen und die Abhängigkeiten installieren, die wir benötigen.
Beginnen Sie mit den Linux-Distributionen, indem Sie Ihre Systempaketverwaltung vorbereiten und das Python3 virtualenv-Paket installieren.
Verwenden Sie Folgendes:
Wir nennen unseren Arbeitsbereich SignLanguage (Gebärdensprache):
Navigieren Sie zum Verzeichnis SignLanguage:
Erstellen Sie dann eine neue virtuelle Umgebung für das Projekt:
Aktivieren Sie Ihre Umgebung:
Installieren Sie anschließend PyTorch, ein Deep-Learning-Framework für Python, das wir in diesem Tutorial verwenden werden.
Auf macOS installieren Sie Pytorch mit dem folgenden Befehl:
Auf Linux und Windows verwenden Sie die folgenden Befehle für einen reinen CPU-Build:
Installieren Sie nun vorgefertigte Binärdateien für OpenCV, numpy und onnx, die als Bibliotheken für Computer Vision, lineare Algebra, den Export des KI-Modells und die Ausführung des KI-Modells dienen.
OpenCV bietet Hilfsfunktionen wie Bilddrehung und numpy bietet Hilfsfunktionen für lineare Algebra an, z. B. eine Matrixinversion:
Auf Linux-Distributionen müssen Sie libSM.so installieren:
Wenn die Abhängigkeiten installiert wurden, erstellen wir die erste Version unseres Gebärdensprachenübersetzers: einen Gebärdensprachen-Classifier.
Schritt 2 - Vorbereiten des Datensatzes für die Klassifikation der Gebärdensprache
In diesen nächsten drei Abschnitten erstellen Sie einen Gebärdensprachen-Classifier mithilfe eines neuronalen Netzes.
Ihr Ziel besteht darin, ein Modell zu erstellen, das ein Bild einer Hand als Eingabe annimmt und einen Buchstaben ausgibt.
Für das Erstellen eines Klassifizierungsmodells für das maschinelle Lernen sind Sie die folgenden drei Schritte erforderlich:
Vorbearbeiten der Daten: Wenden Sie one-hot encoding (One-Hot-Kodierung) auf Ihre Labels an und umschließen Sie Ihre Daten mit PyTorch-Tensoren.
Trainieren Sie Ihr Modell auf angereicherten Daten, um es auf "unübliche" Eingabedaten vorzubereiten, z. B. eine außermittige oder eine gedrehte Hand.
Legen Sie das Modell fest und trainieren Sie es: Richten Sie ein neuronales Netz mit PyTorch ein.
Legen Sie die Hyperparameter für das Training fest (z. B. wie lange das Training dauern soll) und führen Sie ein stochastisches Gradientenverfahren durch.
Variieren Sie zudem einen bestimmten Hyperparameter für das Training: Learning Rate Schedule.
Dadurch wird die Modellgenauigkeit erhöht.
Führen Sie eine Vorhersage mit dem Modell aus: Bewerten Sie das neuronale Netz anhand Ihrer Validierungsdaten, um dessen Genauigkeit zu erfassen.
Exportieren Sie dann das Modell in ein Format namens ONNX, um höhere Inferenzgeschwindigkeiten zu erreichen.
In diesem Abschnitt des Tutorials führen Sie Schritt 1 von 3 durch. Sie werden die Daten herunterladen, ein Dataset-Objekt erstellen, das wiederholt auf Ihre Daten angewendet wird, und abschließend noch die Data Augmentation anwenden.
Am Ende dieses Schritts verfügen Sie über ein Programm, mit dem Sie auf Bilder und Labels in Ihrem Datensatz zugreifen können, um Ihr Modell zu füttern.
Laden Sie zuerst den Datensatz in Ihr aktuelles Arbeitsverzeichnis herunter:
< $> note Anmerkung: Auf makOS ist wget standardmäßig nicht verfügbar.
Installieren Sie dazu HomeBrew, indem Sie diesem DigitalOcean Tutorial folgen.
Führen Sie dann brew install wget aus.
Entzippen Sie die Zip-Datei, die das Verzeichnis data / enthält:
Erstellen Sie eine neue Datei namens step _ 2 _ dataset.py:
Importieren Sie wie zuvor die erforderlichen Hilfsfunktionen und erstellen Sie die Klasse, die Ihre Daten enthalten soll. Erstellen Sie die Trainings- und Testdaten zum Zwecke der Datenverarbeitung.
Sie implementieren die Dataset-Schnittstelle von PyTorch, damit Sie die integrierte Daten-Pipeline von PyTorch laden und für den Datensatz Ihrer Gebärdensprachenklassifikation verwenden können:
Löschen Sie den Platzhalter pass in der Klasse SignLanguageMNIST.
Fügen Sie an seiner Stelle eine Methode hinzu, um ein Label Mapping zu generieren:
Die Labels reichen von 0 bis 25. Die Buchstaben J (9) und Z (25) sind jedoch ausgeschlossen.
Das bedeutet, dass es nur 24 gültige Label-Werte gibt.
Damit der Satz aller von 0 ausgehenden Label-Werte zusammenhängend ist, werden alle Labels 0, 23 zugeordnet. Dieses Mapping von den Datensätzen 0, 23 bis zu den Buchstabenindizes 0, 25 wird mithilfe der Methode get _ label _ mapping herbeigeführt.
Als Nächstes fügen Sie eine Methode hinzu, um Labels und Beispielproben aus einer CSV-Datei zu extrahieren.
Im Folgenden wird davon ausgegangen, dass jede Zeile mit dem label startet, auf das 784-Pixelwerte folgen.
Diese 784 Pixelwerte repräsentieren ein 28x28 Bild:
Eine Erklärung darüber, wie diese 784 Werte ein Bild repräsentieren, finden Sie unter Build an Emotion-Based Dog Filter, Step 4 (Erstellen eines emotionsbasierten Hundes, Schritt 4).
Beachten Sie, dass jede Zeile im csv.reader-Iterable eine Liste von Zeichenfolgen ist; die Aufrufe int und map (int,...) wandeln alle Zeichenfolgen in Ganzzahlen um.
Fügen Sie direkt unter unserer statischen Methode eine Funktion hinzu, die unseren Datenbehälter initialisieren wird:
Diese Funktion startet mit dem Laden von Samples und Labels.
Dann umschließt sie die Daten mit NumPy-Arrays.
Die mittlere und Standardabweichung wird kurz im folgenden Abschnitt _ _ getitem _ _ erklärt.
Fügen Sie direkt nach der Funktion _ _ init _ _ eine Funktion _ _ len _ _ hinzu.
Diese Methode wird für das Dataset benötigt, um zu ermitteln, wann das Iterieren über die Daten beendet werden muss.
Fügen Sie abschließend die Methode _ _ getitem _ _ hinzu, die ein Wörterbuch zurückgibt, das das Sample und das Label enthält:
Sie verwenden eine Technik namens Data Augmentation, bei der Samples während des Trainings gestört werden, um die Robustheit des Modells gegenüber diesen Störungen zu erhöhen.
Hierfür wird insbesondere das Bild über RandomResizedCrop in variierenden Werten und an verschiedenen Stellen eingezoomt.
Beachten Sie, dass sich das Einzoomen nicht auf die finale Gebärdensprachenklasse auswirken sollte. So wird das Label nicht transformiert.
Sie normalisieren die Eingaben zusätzlich, damit die Bildwerte wie erwartet auf den Bereich 0, 1 neu skaliert werden anstatt auf 0, 255; verwenden Sie bei der Normalisierung den Datensatz _ mean und _ std, um dies zu erreichen.
Ihre abgeschlossene Klasse SignLanguageMNIST sieht wie folgt aus:
Wie zuvor überprüfen Sie unsere Datensatz-Hilfsfunktionen, indem Sie den Datensatz SignLanguageMNIST laden.
Fügen Sie am Ende Ihrer Datei hinter der Klasse SignLanguageMNIST den folgenden Code hinzu:
Dieser Code initialisiert den Datensatz mithilfe der Klasse SignLanguageMNIST.
Für die Trainings- und Validierungssätze wird dann der Datensatz mit einem DataLoader umschlossen.
Dadurch wird der Datensatz für den späteren Gebrauch in ein Iterable umgewandelt.
Nun überprüfen Sie, ob die Datensatz-Hilfsfunktionen funktionieren.
Erstellen Sie einen Sample-Datensatz-Loader mithilfe von DataLoader und drucken Sie das erste Element dieses Loaders aus.
Fügen Sie Folgendes am Ende der Datei hinzu:
Sie können überprüfen, ob Ihre Datei mit der Datei step _ 2 _ dataset in diesem (repository) übereinstimmt.
Beenden Sie Ihren Editor und führen Sie das Skript mit Folgendem aus:
Dadurch werden folgende Tensoren paarweise ausgegeben.
Unsere Datenpipeline gibt zwei Samples und zwei Labels aus.
Dies zeigt an, dass unsere Datenpipeline eingerichtet ist und bereit ist, fortzufahren:
Sie haben nun sichergestellt, dass Ihre Datenpipeline funktioniert.
Damit ist der erste Schritt - die Verarbeitung Ihrer Daten - abgeschlossen. Nun beinhaltet sie Data Augmentation für eine erhöhte Modellstabilität.
Als Nächstes definieren Sie das neuronale Netz und den Optimierer.
Schritt 3 - Erstellen und Trainieren des Gebärdensprachen-Classifiers mittels Deep Learning
Mit einer funktionierenden Datenpipeline definieren Sie nun ein Modell und trainieren es anhand der Daten. Insbesondere erstellen Sie ein neuronales Netz mit sechs Schichten, definieren einen Verlust, einen Optimierer und optimieren abschließend die Verlustfunktion für die Voraussagen mit Ihrem neuronalen Netz.
Am Ende dieses Schritts verfügen Sie über einen funktionierenden Gebärdensprachen-Klassifizierer.
Erstellen Sie eine neue Datei namens step _ 3 _ train.py:
Importieren Sie die erforderlichen Dienstprogramme:
Definieren Sie ein neuronales PyTorch-Netz, das drei Convolutional Layers (faltende Schichten) enthält, gefolgt von drei vollständig zusammenhängenden Schichten.
Fügen Sie dies am Ende Ihres bestehenden Skripts hinzu:
Initialisieren Sie nun das neuronale Netz, definieren Sie eine Verlustfunktion und legen Sie Hyperparameter zur Optimierung fest, indem Sie am Ende des Skripts den folgenden Code hinzufügen:
Schließlich trainieren Sie es für zwei Epochen:
Sie definieren eine Epoche, die eine Iteration des Trainings ist, bei der jedes Trainings-Sample genau einmal verwendet wurde.
Am Ende der Hauptfunktion werden die Modellparameter in einer Datei namens "checkpoint.pth" gespeichert.
Fügen Sie am Ende Ihres Skripts den folgenden Code hinzu, um Bild und Label aus dem Datensatz-Loader zu extrahieren und dann jede mit einer PyTorch Variable zu umschließen:
Dieser Code führt auch die Vorwärtsrechnung und dann die Fehlerrückführung über den Verlust und das neuronale Netz aus.
Fügen Sie am Ende Ihrer Datei Folgendes hinzu, um die Funktion main aufzurufen:
Prüfen Sie nochmals, ob Ihre Datei dem Folgenden entspricht:
Speichern und schließen Sie sie.
Starten Sie dann unser Proof-of-Concept-Training, indem Sie Folgendes ausführen:
Während das neuronale Netz trainiert wird, sehen Sie ein Ergebnis, das dem Folgenden ähnelt:
Um die Verluste niedrig zu halten, können Sie die Anzahl der Epochen auf 5, 10 oder sogar 20 erhöhen. Nach einer bestimmten Trainingszeit wird sich der Netzverlust trotz einer Steigerung der Trainingszeit nicht mehr verringern.
Geben Sie einen Learning Rate Schedule vor, der die Lernrate im Laufe der Zeit verringert, um das Problem zu umgehen, das mit der zunehmenden Trainingszeit einhergeht.
Wenn Sie verstehen wollen, warum das funktioniert, sehen Sie sich die Visualisierung von Distill an unter "Why Momentum Really Works" (" Warum das Momentum wirklich funktioniert ").
Ändern Sie Ihre Funktion main mit den folgenden zwei Zeilen ab, definieren Sie einen Scheduler und rufen Sie scheduler.step auf.
Ändern Sie außerdem die Anzahl der Epochen in 12:
Überprüfen Sie, ob Ihre Datei mit der Datei aus Schritt 3 in diesem Repository übereinstimmt.
Die Ausführung des Trainings dauert etwa 5 Minuten.
Ihre Ausgabe wird in etwa wie folgt aussehen:
Der erhaltene Verlust beträgt 0,007608, was 3 Größenordnungen kleiner als der anfängliche Verlust von 3,20 ist.
Damit wird der zweite Schritt unseres Workflows abgeschlossen, in dem wir das neuronale Netz eingerichtet und trainiert haben.
Doch auch ein kleiner Verlust hat eine Bedeutung, wenn auch nur eine sehr kleine.
Um die Leistung des Modells in Perspektive zu setzen, werden wir seine Genauigkeit berechnen - der Prozentsatz der Bilder, die das Modell richtig klassifiziert hat.
Schritt 4 - Bewerten des Gebärdensprachen-Classifiers
Sie werden nun Ihren Gebärdensprachen-Klassifizierer bewerten, indem Sie seine Genauigkeit im Validierungssatz berechnen, ein Satz von Bildern, die das Modell während des Trainings nicht gesehen hat.
Dadurch erhalten wir ein besseres Bild über die Leistung des Modells als durch den endgültigen Verlustwert.
Zudem fügen Sie Dienstprogramme hinzu, um unser trainiertes Modell am Ende des Trainings zu speichern, und laden unser vorab trainiertes Modell, während die Inferenz durchgeführt wird.
Erstellen Sie eine neue Datei namens step _ 4 _ evaluate.py.
Definieren Sie als Nächstes eine Hilfsfunktion, um die Leistung des neuronalen Netzes zu bewerten.
Die folgende Funktion vergleicht den vom neuronalen Netz vorhergesagten Buchstaben mit dem tatsächlichen Buchstaben:
outputs ist eine Liste von Klassenwahrscheinlichkeiten für jedes Sample.
Beispielsweise können Outputs für ein einziges Sample [0,1, 0,3, 0,4,] betragen. labels ist eine Liste von Label-Klassen.
Die Label-Klasse kann beispielsweise 3 sein.
Y =... wandelt die Labels in ein NumPy-Array um.
Als Nächstes konvertiert Yhat = np.argmax (...) die Wahrscheinlichkeiten der Klasse outputs in vorausgesagte Klassen.
Die Liste der Klassenwahrscheinlichkeiten [0,1, 0,3, 0,4, 0,2] würde die vorausgesagte Klasse 2 ergeben, da der Indexwert 2 von 0,4 der größte Wert ist.
Da Y und Yhat nun Klassen sind, können Sie sie vergleichen.
Yhat = = Y prüft, ob die vorhergesagte Klasse mit der Label-Klasse übereinstimmt, und np.sum (...) ist ein Trick, der die Anzahl der truth-y-Werte berechnet.
Anders ausgedrückt: np.sum gibt die Anzahl der Samples aus, die richtig klassifiziert wurden.
Fügen Sie die zweite Funktion batch _ evaluate hinzu, die die erste Funktion evaluate auf alle Bilder anwendet:
Batch ist eine Gruppe von Bildern, die als ein einzelner Tensor gespeichert werden.
Zuerst erhöhen Sie die Gesamtzahl der Bilder, die Sie evaluieren (n) um die Anzahl der Bilder in diesem Batch.
Als Nächstes führen Sie in dem neuronalen Netz eine Inferenz mit diesem Batch von Bildern aus, outputs = net (...).
Die Typenprüfung if isinstance (...) konvertiert die Ergebnisse in einen NumPy-Array bei Bedarf.
Schließlich verwenden Sie evaluate, um die Anzahl der richtig klassifizierten Samples zu berechnen.
Am Ende der Funktion berechnen Sie den prozentualen Anteil der Samples, die Sie richtig klassifiziert haben, score / n.
Fügen Sie abschließend das folgende Skript hinzu, um die vorherigen Hilfsfunktionen zu nutzen:
Dadurch wird ein vorab trainiertes neuronales Netz geladen und seine Leistung auf dem bereitgestellten Gebärdensprachen-Datensatz bewertet.
Das Skript gibt hier insbesondere Genauigkeit über die Bilder aus, die Sie für das Training verwendet haben, und einen separaten Satz von Bildern, die Sie für Testzwecke aufgehoben haben und die validation set heißen.
Als Nächstes exportieren Sie das PyTorch in eine ONNX-Binärdatei.
Diese Binärdatei kann dann in der Produktion verwendet werden, um mit Ihrem Modell eine Inferenz auszuführen.
Am wichtigsten ist, dass der Code, der diese Binärdatei ausführt, keine Kopie der ursprünglichen Netzwerkdefinition benötigt.
Fügen Sie am Ende der Funktion validate Folgendes hinzu:
Dadurch wird das ONNX-Modell exportiert, das exportierte Modell überprüft und dann eine Inferenz mit dem exportierten Modell ausgeführt.
Überprüfen Sie nochmals, ob Ihre Datei mit der Datei aus Schritt 4 in diesem Repository übereinstimmt.
Führen Sie Folgendes aus, um den Checkpoint vom letzten Schritt zu verwenden und zu evaluieren:
Dadurch erhalten Sie eine ähnliche Ausgabe wie die Folgende, die nicht nur bestätigt, dass Ihr exportiertes Modell funktioniert, sondern auch Ihr ursprüngliches PyTorch-Modell bestätigt:
Ihr neuronales Netz erreicht eine Trainingsgenauigkeit von 99,9% und eine Validierungsgenauigkeit von 97,4%.
Diese Lücke zwischen Trainings- und Validierungsgenauigkeit deutet auf eine Überanpassung Ihres Modells hin.
Das heißt, dass Ihr Modell die Trainingsdaten gespeichert hat, anstatt generalisierbare Muster zu erlernen. Weiterführende Informationen zu den Implikationen und Ursachen der Überanpassung finden Sie in Understanding Bias-Variance Tradeoffs (Das Spannungsfeld von Verzerrungsvarianzen verstehen).
Nun haben wir einen Gebärdensprachen-Klassifizierer fertiggestellt.
Im Grunde genommen kann unser Modell die Gebärden fast immer korrekt erkennen und voneinander unterscheiden.
Da dies bereits ein recht gutes Modell ist, fahren wir nun mit der letzten Stufe unserer Anwendung fort.
Wir werden diesen Gebärdensprachen-Classifier in einer Webcam-Anwendung in Echtzeit einsetzen.
Schritt 5 - Verknüpfen der Kameraaufzeichnungen
Ihr nächstes Ziel besteht darin, die Kamera des Computers mit Ihrem Gebärdensprachen-Klassifizierer zu verknüpfen.
Sie erfassen die Eingangsdaten der Kamera, klassifizieren die angezeigte Gebärdensprache und melden dann die klassifizierte Gebärde an den Benutzer zurück.
Erstellen Sie nun ein Python-Skript für die Gesichtserkennung.
Erstellen Sie die Datei step _ 6 _ camera.py mit nano oder Ihrem bevorzugten Texteditor:
Fügen Sie in der Datei den folgenden Code hinzu:
Dieser Code importiert OpenCV, die Ihre Bildprogramme enthält, und die ONNX-Laufzeit. Das ist alles, was Sie mit Ihrem Modell in der Inferenz ausführen müssen.
Der Rest des Codes ist eine typische Python-Programmvorgabe.
Ersetzen Sie nun pass in der Funktion main durch den folgenden Code, der einen Gebärdensprachen-Classifier mit den zuvor trainierten Parametern initialisiert.
Fügen Sie zudem ein Mapping zwischen den Indizes und den Buchstaben sowie den Bildstatistiken hinzu:
Sie werden Elemente dieses Testskripts aus der offiziellen OpenCV verwenden.
Aktualisieren Sie insbesondere den Körper der Funktion main.
Zuerst initialisieren Sie ein VideoCapture-Objekt, das so eingestellt ist, dass es Live-Einspeisungen von der Kamera Ihres Computers erfassen kann.
Platzieren Sie das ans Ende der Funktion main:
Fügen Sie dann eine while-Schleife hinzu, die bei jedem Zeitschritt von der Kamera liest:
Schreiben Sie eine Hilfsfunktion, die das zentrale Cropping für das Kamerabild übernimmt.
Platzieren Sie diese Funktion vor main:
Führen Sie als Nächstes das zentrale Cropping für das Kamerabild durch, konvertieren Sie es in Graustufen, normalisieren Sie es and ändern Sie die Größe auf 28x28.
Platzieren Sie dies in die while-Schleife innerhalb der Funktion main:
Führen Sie noch innerhalb der while-Schleife eine Inferenz mit der ONNX-Laufzeit aus.
Konvertieren Sie die Ausgaben in einen Klassenindex und dann in einen Buchstaben:
Zeigen Sie den vorhergesagten Buchstaben innerhalb des Bildrahmens und das Bild wieder dem Benutzer an:
Fügen Sie am Ende der while-Schleife diesen Code hinzu, um zu prüfen, ob der Benutzer das Zeichen q wählt und falls das der Fall ist, beenden Sie die Anwendung.
Diese Zeile hält das Programm für 1 Millisekunde an.
Fügen Sie Folgendes hinzu:
Lösen Sie abschließend die Aufnahme aus und schließen Sie alle Fenster.
Platzieren Sie sie außerhalb der while-Schleife, um die Funktion main zu beenden.
Überprüfen Sie nochmals, ob Ihre Datei mit dem folgenden oder diesem Repository übereinstimmt:
Schließen Sie Ihre Datei und führen Sie das Skript aus.
Sobald das Skript ausgeführt wird, wird ein Fenster mit der Live-Einspeisung der Webcam angezeigt.
Der vorhergesagte Gebärdensprachenbuchstabe wird oben links angezeigt.
Halten Sie die Hand hoch und machen Sie Ihre Lieblingsgebärde, um Ihren Classifier in Aktion zu sehen.
Hier sind einige Beispiel-Ergebnisse, die den Buchstaben L und D zeigen.
Screenshot Ihres Sample-OpenCV-Programms für den Fingerbuchstaben "L".
Screenshot Ihres Sample-OpenCV-Programms für den Fingerbuchstaben, D ".
Beachten Sie während des Tests, dass der Hintergrund für diesen Übersetzer ziemlich klar sein muss, damit er funktioniert.
Dies bringt die Sauberkeit des Datensatzes leider mit sich.
Würde der Datensatz Bilder von Handzeichen mit verschiedenen Hintergründen enthalten, hätte das Netz kein Problem mit rauschenden Hintergründen.
Der Datensatz bietet jedoch leere Hintergründe und ziemlich zentrierte Hände.
Die Webcam funktioniert daher am besten, wenn die Hand ebenfalls zentriert ist und sich vor einem leeren Hintergrund befindet.
Damit ist die Übersetzungsanwendung für die Gebärdensprache vollendet.
In diesem Tutorial haben Sie einen Übersetzer für die amerikanische Gebärdensprache mittels Computer Vision und einem Modell für maschinelles Lernen entwickelt.
Sie haben vor allem neue Aspekte des Trainings eines Modells für maschinelles Lernen gesehen - genauer gesagt Data Augmentation für die Modellstabilität, Learning Rate Schedules für geringere Verluste und Exportvorgänge von KI-Modellen mithilfe von ONNX zu Produktionszwecken.
Dies führte schlussendlich zu der Schaffung einer Echtzeit-Computer-Vision-Anwendung, die Gebärdensprache mithilfe einer Pipeline, die Sie erstellt haben, in Buchstaben übersetzt.
Es sollte allerdings bedacht werden, dass der finale Klassifizierer instabil ist. Glücklicherweise gibt es jedoch Methoden, die man einzeln oder auch zusammen einsetzen kann, um gegen diese Instabilität vorzugehen. Diese stellen wir nachfolgend vor.
Die folgenden Themen beinhalten weiterführende Erläuterungen zu Verbesserung Ihrer Anwendung:
Generalisierung: Hierbei handelt es sich keineswegs um einen Unterbereich von Computer Vision, sondern um ein beständiges Problem, das im Grunde bei jedem Aspekt zum Thema maschinelles Lernen auftritt.
Siehe Understanding Bias-Variance Tradeoffs ​ ​ ​ (Das Spannungsfeld von Verzerrungsvarianzen verstehen) ​ ​ ​
Domain Adaptation (Domänenanpassung): Angenommen, Ihr Modell ist für Domäne A trainiert (z. B. sonnige Umgebungen).
Können Sie das Modell in diesem Fall auf Domäne B umstellen (z. B. wolkige Umgebungen)?
Adversarial Examples (gegnerische Beispiele): Angenommen ein Kontrahent erstellt absichtlich Bilder, um Ihr Modell zu täuschen.
Wie können Sie solche Bilder gestalten?
Wie können Sie gegen solche Bilder vorgehen?
So konfigurieren Sie Apache HTTP mit MPM Event und PHP-FPM unter Ubuntu 18.04
5607
Der Apache-HTTP-Webserver wurde im Laufe der Jahre weiterentwickelt, damit er in verschiedenen Umgebungen arbeitet und verschiedene Anforderungen erfüllt.
Ein wichtiges Problem, das Apache HTTP wie jeder andere Webserver auch lösen muss, ist die Handhabung verschiedener Prozesse bei der Bearbeitung von http-basierten Anfragen.
Dazu zählt das Öffnen eines Sockets, das die Anforderung verarbeitet, das Offenhalten der Verbindung für eine bestimmte Zeit, die Handhabung neuer Ereignisse, die während dieser Verbindung eintreten und die Rückgabe des produzierten Contents durch ein Programm, dass in einer bestimmten Sprache geschrieben wurde (wie PHP, Perl oder Python).
Diese Aufgaben werden von einem Multi-Processing-Module (MPM) ausgeführt und gesteuert.
Apache HTTP ist mit drei verschiedenen MPM ausgestattet:
Prefork: Für jede eingehende Verbindung, die den Server erreicht, wird ein neuer Vorgang erstellt.
Jeder Vorgang ist isoliert von den anderen und es wird kein Speicher zwischen ihnen geteilt, selbst dann, wenn sie in der Ausführung identische Anrufe an einem bestimmten Punkt ausführen.
Auf diese Weise können Sie mit Bibliotheken verknüpfte Anwendungen, die Thread-Ausführungen nicht unterstützen, sicher ausführen - meist ältere Anwendungen oder Bibliotheken.
Worker: Ein Elternprozess ist für das Starten eines Bündels von Kindprozessen verantwortlich, von denen einige neu eingehende Verbindungen erfassen und andere den angeforderten Content bereitstellen.
Für jeden Prozess gibt es einen dazugehörigen Thread (ein einzelner Thread kann jeweils eine Verbindung verwalten), sodass ein Prozess mit mehreren Anfragen gleichzeitig umgehen kann.
Diese Methode für die Handhabung von Verbindungen fördert eine bessere Ressourcennutzung und gewährleistet die Aufrechterhaltung der Stabilität.
Das ist auf das Bündel von verfügbaren Prozessen zurückzuführen, bei denen oft frei verfügbare Threads bereitstehen, die neue Verbindungen sofort bedienen können.
Event: Basierend auf Worker geht dieses MPM noch einen Schritt weiter, indem es die Art und Weise optimiert, wie der Elternprozess Aufgaben für die Kindprozesse und für die Threads, die damit verknüpft sind, vorgibt.
Eine Verbindung bleibt für 5 Sekunden standardmäßig geöffnet und schließt sich bei jedem neuen Ereignis, das eintritt; das ist der Standardwert für die Keep-Alive-Anweisung, der den mit ihm verknüpften Thread beibehält. Das Event MPM ermöglicht dem Prozess das Verwalten von Threads, damit einige Threads für die Verwaltung neuer eingehender Verbindungen bereitstehen, während andere weiterhin mit den Live-Verbindungen verknüpft sind.
Die Ressourcennutzung und Leistungsfähigkeit wird dadurch verbessert, dass die den Threads zugewiesenen Aufgaben neu verteilt werden können.
Mit dem MPM Event-Modul ist ein schnelles Multi-Processing-Modul auf dem Apache-HTTP-Webserver verfügbar.
PHP-FPM ist der FastCGI-Prozessmanager für PHP.
Das FastCGI-Protokoll basiert auf dem Common Gateway Interface (CGI), einem Protokoll, das zwischen Anwendungen und Webservern wie Apache HTTP steht.
Dadurch können Entwickler Anwendungen schreiben, ohne das Verhalten der Webserver berücksichtigen zu müssen.
Die Programme führen ihre Prozesse unabhängig aus und übergeben ihr Produkt über dieses Protokoll an den Webserver.
Jede neue Verbindung, die von einer Anwendung verarbeitet werden muss, erstellt einen neuen Prozess.
Durch die Kombination von MPM Event in Apache HTTP mit dem PHP FastCGI-Prozessmanager (PHP-FPM) kann eine Website schneller laden und mehr gleichzeitige Verbindungen mit weniger Ressourcen verarbeiten.
In diesem Tutorial verbessern Sie die Leistung des LAMP-Stacks, indem Sie das standardmäßige Multi-Processing-Module von Prefork auf Event umstellen und den PHP-FPM-Prozessmanager für die Handhabung des PHP-Codes nutzen anstelle des klassischen mod _ php in Apache HTTP.
Der LAMP-Stack, der auf Ihrem Server gemäß How To Install Linux, Apache, MySQL, PHP (LAMP stack) on Ubuntu 18.04 (So installieren Sie Linux, Apache, MySQL, PHP (LAMP-Stack) auf Ubuntu 18.04.) installiert ist.
Schritt 1 - Umstellen des Multi-Processing-Module
Ubuntu übernimmt Skripte, um Apache-HTTP-Module über die eigene übergeordnete Distribution Debian zu aktivieren oder zu deaktivieren.
Sie werden dieses Toolset in diesem Schritt verwenden, um das Prefork-Modul zu deaktivieren und das Event-Modul zu aktivieren.
In diesem Schritt halten Sie Apache HTTP an, deaktivieren das Modul < ^ > PHP 7.2 < ^ >, das mit dem Prefork-Modul verknüpft ist, und deaktivieren anschließend Prefork, um das Event-Modul unmittelbar aktivieren zu können.
Zuerst halten Sie den Apache-HTTP-Dienst an:
Nun können Sie das Modul < ^ > PHP 7.2 < ^ > deaktivieren, das mit dem Prefork-Modul in Verbindung steht:
Deaktivieren Sie dann das Prefork MPM-Modul:
Nun Aktivieren Sie das Event MPM-Modul:
Sie haben das MPM von Prefork auf Event umgestellt und die Modulverbindung < ^ > PHP 7.2 < ^ > zwischen PHP und Apache HTTP entfernt.
Im nächsten Schritt installieren Sie das php-fpm-Modul sowie die verwandten Bibliotheken und Proxy-Module.
Sie konfigurieren Apache HTTP so, dass es auch mit PHP kommunizieren kann.
Schritt 2 - Konfigurieren von Apache HTTP für die Nutzung des FastCGI-Prozesses
In dieser Phase haben Sie die Verarbeitung von Verbindungen durch Apache HTTP umgestellt, indem Sie sie von dem Prefork-MPM auf Event verlagert haben.
Im Zuge dessen haben Sie jedoch das PHP-Modul deaktiviert, das Apache HTTP mit jedem Programm verbunden hatte, das mit PHP ausgeführt wird.
In diesem Schritt installieren Sie den PHP-FPM-Prozessor, damit Apache HTTP wieder PHP-Programme verarbeiten kann.
Außerdem installieren Sie die Abhängigkeitsbibliotheken und aktivieren die Module, damit beide reibungslos und schneller zusammenarbeiten können als zuvor.
Installieren Sie zuerst php-fpm.
Der folgende Befehl installiert das PHP-FPM und aktiviert automatisch den Dienst < ^ > php7.2-fpm < ^ >, der in systemd integriert ist, sodass der Dienst beim Booten gestartet wird:
Apache HTTP und PHP benötigen für die Kommunikation eine Bibliothek, die diese Funktion ermöglicht.
Nun installieren Sie libapache2-mod-fcgid, das als Schnittstelle zwischen Programmen mit Webservern dient und Apache-HTTP-spezifisch ist.
Diese Kommunikation erfolgt über ein UNIX-Socket.
Installieren Sie diese Bibliothek:
Sie haben php-fpm und das libapache2-mod-fcgid installiert, aber noch keines davon aktiviert.
Aktivieren Sie zuerst das php-fpm-Modul mit folgendem Befehl:
Aktivieren Sie in einem zweiten Schritt das Apache HTTP-Proxy-Modul:
Aktivieren Sie in einem dritten Schritt das FastCGI-Proxy-Modul auf Apache HTTP:
< $> note Hinweis: Sie können die Konfiguration dieser Interaktion zwischen PHP-Programmen und Apache HTTP über einen UNIX-Socket mit Folgendem lesen:
Nun wurden alle Vorkehrungen getroffen, damit Sie Apache HTTP starten können.
Führen Sie eine Konfigurationsüberprüfung durch:
Danach können Sie mit dem Neustart von Apache HTTP fortfahren, da es beim Installieren der FastCGI-Bibliothek libapache2-mod-fcgid automatisch gestartet wurde:
Sie haben das php-fpm-Modul installiert und Apache HTTP so konfiguriert, dass es damit funktioniert. Zudem haben Sie ermöglicht, dass die erforderlichen Module für das FastCGI-Protokoll funktionieren, und die entsprechenden Dienste gestartet.
Nachdem Apache das Event MPM-Modul aktiviert hat und PHP-FPM verfügbar ist und ausgeführt wird, ist es an der Zeit sicherzustellen, das alles wie geplant funktioniert.
Schritt 3 - Testen Ihrer Konfiguration
Führen Sie einige Tests aus, um zu prüfen, ob die Konfigurationsänderungen angewendet wurden.
Beim ersten Test wird geprüft, welches Multi-Processing-Modul Apache HTTP verwendet.
Beim zweiten Test wird sichergestellt, dass PHP den FPM-Manager verwendet.
Überprüfen Sie den Apache-HTTP-Server, indem Sie den folgenden Befehl ausführen:
Sie erhalten folgende Ausgabe:
Für das Proxy-Modul und FastCGI können Sie diese Prozedur wiederholen:
Wenn Sie die gesamte Liste der Module sehen möchten, können Sie den zweiten Teil des Befehls nach -M entfernen.
Nun ist es Zeit zu prüfen, ob PHP den FastCGI-Prozessmanager verwendet.
Dazu schreiben Sie ein kleines PHP-Skript, das Ihnen alle Informationen zeigt, die mit PHP in Verbindung stehen.
Führen Sie den folgenden Befehl aus, um eine Datei zu schreiben, deren Name wie folgt lautet:
Fügen Sie den folgenden Inhalt in die Datei info.php ein:
Rufen Sie nun die URL Ihres Servers auf und fügen Sie info.php am Ende hinzu: http: / / < ^ > your _ domain < ^ > / info.php.
Der Server-API-Eintrag lautet FPM / FastCGI.
PHP Screen the Server API entry FPM / FastCGI
Löschen Sie die Datei info.php nach diesem Test, damit keine Informationen über den Server veröffentlicht werden:
Sie haben den Betriebszustand des MPM-Moduls und der Module, die für die Handhabung von FastCGI zuständig sind, sowie die Handhabung des PHP-Codes überprüft.
Sie haben Ihren ursprünglichen LAMP-Stack optimiert, sodass sich die Anzahl der Verbindungen zur Erstellung neuer Apache HTTP-Prozesse erhöht hat, PHP-FPM den PHP-Code effizienter verwaltet und sich die Ressourcennutzung insgesamt verbessert.
Weitere Informationen zu den verschiedenen Modulen und verwandten Projekten finden Sie in der Projekt-Dokumentation zum Apache HTTP-Server.
Installieren des Apache-Webservers unter CentOS 8 Schnellstart
5509
Eine ausführlichere Version dieses Tutorials finden Sie unter Installieren des Apache-Webservers unter CentOS 8.
Schritt 2 - Anpassen der Firewall
Erstellen Sie das html-Verzeichnis für
example.com < ^ > wie folgt, indem Sie das Flag -p verwenden, um alle erforderlichen übergeordneten Verzeichnisse zu erstellen:
Installieren und Konfigurieren von Postfix unter Ubuntu 20.04
5685
Postfix ist ein beliebter Open-Source Mail Transfer Agent (MTA), der zur Weiterleitung und Zustellung von E-Mails auf einem Linux-System verwendet werden kann.
Schätzungen zufolge führen etwa 25% der öffentlichen Mail-Server im Internet Postfix aus.
In diesem Leitfaden zeigen wir Ihnen, wie Sie Postfix auf einem Ubuntu 20.04 Server installieren und konfigurieren.
Anschließend testen Sie, ob Postfix in der Lage ist, E-Mails korrekt weiterzuleiten, indem Sie s-nail, einen Mail User Agent (MUA), auch als E-Mail-Client bekannt, installieren.
Beachten Sie, dass das Ziel dieses Tutorials darin besteht, Ihnen dabei zu helfen, Postfix schnell zum Laufen zu bringen, und zwar mit nur einigen grundlegenden E-Mail-Funktionen.
Am Ende dieses Leitfadens werden Sie zwar noch nicht über einen voll funktionsfähigen E-Mail-Server verfügen, aber einige der grundlegenden Komponenten einer solchen Einrichtung werden Ihnen den Einstieg erleichtern.
Um diesem Leitfaden zu folgen, benötigen Sie Folgendes:
Einen Server mit Ubuntu 20.04, der als Postfix Mail-Server fungiert.
Dieser Server sollte über einen Nicht-root-Benutzer mit sudo-Berechtigungen und eine mit UFW konfigurierte Firewall verfügen.
Um das einzurichten, können Sie unserem Leitfaden zur Ersteinrichtung des Servers unter Ubuntu 20.04 folgen.
Einen vollständig qualifizierten Domänennamen, der auf Ihren Ubuntu 20.04-Server verweist.
Hilfe bei der Einrichtung Ihres Domänennamens mit DigitalOcean finden Sie, indem Sie unserer Dokumentation Domänen und DNS-Netzwerke folgen.
Beachten Sie, dass Sie, wenn Sie den Zugriff auf E-Mails von einem externen Standort planen, sicherstellen müssen, dass Sie auch über einen MX-Datensatz verfügen, der auf auf Ihren Mail-Server verweist.
Beachten Sie, dass dieses Tutorial davon ausgeht, dass Sie einen Host konfigurieren, der den FQDN von mail.example.com hat.
Falls erforderlich, stellen Sie sicher, dass Sie example.com oder mail.example.com so ändern, dass sie Ihren eigenen FQDN widerspiegeln.
Postfix ist in den Standard-Repositorys von Ubuntu enthalten, sodass Sie es mit APT installieren können.
Um zu beginnen, aktualisieren Sie Ihren lokalen apt-Paketcache:
Installieren Sie dann das Paket postfix mit dem folgenden
Beachten Sie, dass wir hier die Umgebungssvariable DEBIAN _ PRIORITY = low an diesen Installationsbefehl übergeben.
Dies führt dazu, dass der Installationsprozess Sie auffordert, einige zusätzliche Optionen zu konfigurieren:
Dieser Installationsprozess öffnet eine Reihe von interaktiven Eingabeaufforderungen.
Verwenden Sie für die Zwecke dieses Tutorials die folgenden Informationen, um Ihre Eingabeaufforderungen auszufüllen:
General type of mail configuration? Wählen Sie hierfür Internet Site, da dies unseren Infrastrukturanforderungen entspricht.
System mail name: Dies ist die Basisdomäne, die zur Erstellung einer gültigen E-Mail-Adresse verwendet wird, wenn nur der Kontoteil der Adresse angegeben wird.
Nehmen wir beispielsweise an, dass der Hostname Ihres Servers mail. < ^ > example.com < ^ > lautet.
Wahrscheinlich möchten Sie den System-Mail-Namen auf < ^ > example.com < ^ > setzen, sodass Postfix bei der Angabe des Benutzernamens user1 die Adresse user1 @ < ^ > example.com < ^ > verwendet.
Root and postmaster mail recipient: Dies ist das Linux-Konto, das an root @ und postmaster @ adressierte Mail weitergeleitet wird.
Verwenden Sie hierfür Ihr primäres Konto.
In diesem Beispielfall sammy.
Other destinations to accept mail for: Dies definiert die E-Mail-Ziele, die diese Postfix-Instanz akzeptiert.
Wenn Sie weitere Domänen hinzufügen müssen, für deren Empfang dieser Server verantwortlich ist, fügen Sie diese hier hinzu.
Andernfalls ist die Standardeinstellung ausreichend.
Force synchronous updates on mail queue?: Da Sie wahrscheinlich ein journalisiertes Dateisystem verwenden, akzeptieren Sie hier No.
Local networks: Dies ist eine Liste der Netzwerke, für die Ihr Mail-Server für die Weiterleitung von Nachrichten konfiguriert ist.
Die Standardeinstellung wird für die meisten Szenarien funktionieren.
Wenn Sie sie jedoch modifizieren möchten, stellen Sie sicher, dass Sie in Bezug auf den Netzwerkbereich sehr restriktiv sind.
Mailbox size limit: Dies kann verwendet werden, um die Größe von Nachrichten zu begrenzen.
Eine Einstellung auf 0 deaktiviert jede Größenbeschränkung.
Local address extension character: Dies ist as Zeichen, das verwendet werden kann, um den regulären Teil der Adresse von einer Erweiterung zu trennen (verwendet zur Erstellung dynamischer Aliase)
Die Standardeinstellung + funktioniert für dieses Tutorial.
Internet protocols to use: Wählen Sie, ob die IP-Version, die Postfix unterstützt, eingeschränkt werden soll.
Wählen Sie für die Zwecke dieses Tutorials all aus.
Um genau zu sein, sind dies die in diesem Leitfaden verwendeten Einstellungen:
General type of mail configuration?: Internet Site
System mail name: < ^ > example.com < ^ > (nicht < ^ > mail.example.com < ^ >)
Root and postmaster mail recipient: Der Benutzername Ihres primären Linux-Kontos (in unserem Beispiel sammy).
Other destinations to accept mail for: $myhostname, < ^ > example.com < ^ >, < ^ > mail.example.com < ^ >, < ^ > localhost.example.com < ^ >, localhost
Force synchronous updates on mail queue?: No
Local networks: 127.0.0.0 / 8 [:: ffff: 127.0.0.0] / 104 [:: 1] / 128
Mailbox size limit: 0
Local address extension character: +
Internet protocols to use: all
< $> note Anmerkung: Wenn Sie jemals zurückkehren müssen, um diese Einstellungen zu ändern, können Sie dies durch die folgende Eingabe tun:
Die Eingabeaufforderungen werden mit Ihren vorherigen Antworten vorausgefüllt.
Wenn der Installationsprozess abgeschlossen ist, können Sie einige Aktualisierungen an Ihrer Postfix-Konfiguration vornehmen.
Schritt 2 - Ändern der Postfix-Konfiguration
Jetzt können Sie einige Einstellungen anpassen, zu denen Sie bei der Paketinstallation nicht aufgefordert wurden.
Viele der Konfigurationseinstellungen von Postfix sind in der Datei / etc / postfix / main.cf definiert.
Anstatt diese Datei direkt zu bearbeiten, können Sie den Befehl postconf von Postfix verwenden, um Konfigurationseinstellungen abzufragen oder festzulegen.
Legen Sie zunächst den Speicherort für das Postfach Ihres Nicht-root-Ubuntu-Benutzers fest.
In diesem Leitfaden verwenden wir das Format Maildir, das Nachrichten in einzelne Dateien trennt, die dann basierend auf Benutzeraktionen zwischen Verzeichnissen verschoben werden.
Die alternative Option, die in diesem Leitfaden nicht abgedeckt ist, ist das mbox-Format, das alle Nachrichten innerhalb einer einzigen Datei speichert.
Setzen Sie die Variable home _ mailbox auf Maildir /.
Später erstellen Sie eine Verzeichnisstruktur unter diesem Namen innerhalb des Home-Verzeichnisses Ihres Benutzers.
Konfigurieren Sie home _ mailbox durch Eingabe von:
Als Nächstes legen Sie den Speicherort für die Tabelle virtual _ alias _ maps fest, die beliebige E-Mail-Konten an Linux-Systemkonten abbildet.
Führen Sie den folgenden Befehl aus, der den Tabellenspeicherort einer Hash-Datenbankdatei namens / etc / postfix / virtual zuordnet:
Nachdem Sie nun den Speicherort der virtuellen Zuordnungsdatei in Ihrer Datei main.cf definiert haben, können Sie die Datei selbst erstellen und mit der Zuordnung von E-Mail-Konten zu Benutzerkonten auf Ihrem Linux-System beginnen.
Erstellen Sie die Datei mit Ihrem bevorzugten Texteditor; in diesem Beispiel verwenden wir nano:
Listen Sie alle Adressen auf, für die Sie E-Mail akzeptieren möchten, gefolgt von einem Leerzeichen und dem Linux-Benutzer, an den die E-Mail zugestellt werden soll.
Wenn Sie beispielsweise E-Mails bei contact @ < ^ > example.com < ^ > und admin @ < ^ > example.com < ^ > akzeptieren möchten und diese E-Mails an den Linux-Benutzer sammy zugestellt werden sollen, können Sie Ihre Datei wie folgt einrichten:
Nachdem Sie alle Adressen den entsprechenden Serverkonten zugeordnet haben, speichern und schließen Sie die Datei.
Wenn Sie nano verwendet haben, drücken Sie STRG + X, Y, dann ENTER.
Wenden Sie die Zuordnung durch die folgende Eingabe an:
Starten Sie den Postfix-Prozess neu, um sicherzustellen, dass alle Ihre Änderungen angewendet wurden:
Wenn Sie dem Leitfaden zur Ersteinrichtung des Servers gefolgt sind, der als Voraussetzung gilt, haben Sie eine mit UFW konfigurierte Firewall.
Diese Firewall blockiert standardmäßig externe Verbindungen zu Diensten auf Ihrem Server, es sei denn, diese Verbindungen werden ausdrücklich zugelassen. Sie müssen daher eine Firewall-Regel hinzufügen, um eine Ausnahme für Postfix zuzulassen.
Sie können Verbindungen zu dem Dienst zulassen, indem Sie Folgendes eingeben:
Damit ist Postfix konfiguriert und bereit, externe Verbindungen zu akzeptieren.
Sie sind jedoch noch nicht bereit, es mit einem Mail-Client auszutesten.
Bevor Sie einen Client installieren und ihn für die Interaktion mit der an Ihren Server zugestellten E-Mail verwenden können, müssen Sie einige Änderungen an der Einrichtung Ihres Ubuntu-Servers vornehmen.
Schritt 3 - Installieren des Mail-Clients und Initialisieren der Maildir-Struktur
Um mit der zugestellten Mail zu interagieren, führt Sie dieser Schritt durch den Prozess des Installierens des Pakets s-nail.
Dies ist eine funktionsreiche Variante des BSD-xmail-Clients, die das Maildir-Format korrekt verarbeiten kann.
Vor dem Installieren des Clients wäre es jedoch ratsam, sicherzustellen, dass Ihre Umgebungsvariable MAIL korrekt festgelegt ist. s-nail wird nach dieser Variable suchen, um herauszufinden, wo die Mail für Ihren Benutzer zu finden ist.
Um sicherzustellen, dass die Variable MAIL unabhängig davon festgelegt ist, wie Sie auf Ihr Konto zugreifen - ob beispielsweise über ssh, su, su- oder sudo - müssen Sie die Variable in der Datei / etc / bash.bashrc festlegen und sie zu einer Datei innerhalb von / etc / bash.bashrc hinzufügen, um sicherzustellen, dass sie standardmäßig für alle Benutzer festgelegt ist.
Um die Variable zu diesen Dateien hinzuzufügen, geben Sie Folgendes ein:
Um die Variable in Ihre aktuelle Sitzung einzulesen, rufen Sie die Datei / etc / profile.d / mail.sh auf:
Installieren Sie dann den E-Mail-Client s-nail mit APT:
Bevor Sie den Client ausführen, müssen Sie noch einige Einstellungen vornehmen:
Öffnen Sie die Datei / etc / s-nail.rc in Ihrem Editor:
Fügen Sie am Ende der Datei die folgenden Optionen hinzu:
Diese Zeilen bewirken Folgendes:
set emptystart: ermöglicht das Öffnen des Clients auch mit einem leeren Postfach
set folder = Maildir: setzt das Verzeichnis Maildir auf die interne Folder-Variable
set record = + sent erzeugt eine mbox-Datei sent zum Speichern gesendeter E-Mails in dem Verzeichnis, das als Variable folder festgelegt ist, in diesem Fall Maildir
Sie sind nun bereit, die Maildir-Struktur Ihres Systems zu initialisieren.
Eine schnelle Möglichkeit, die Maildir-Struktur in Ihrem Home-Verzeichnis zu erstellen, besteht darin, sich selbst mit dem Befehl s-nail eine E-Mail zu senden.
Da die Datei sent erst zur Verfügung steht, wenn Maildir erstellt ist, sollten Sie das Schreiben an diese Datei für diese erste E-Mail deaktivieren.
Führen Sie dies durch die Übergabe der Option -Snorecord aus.
Senden Sie die E-Mail, indem Sie einen Zeichenfolge an den Befehl s-nail übergeben.
Passen Sie den Befehl so an, dass Ihr Linux-Benutzer als Empfänger markiert wird:
< $> note Anmerkung: Sie erhalten möglicherweise die folgende Antwort:
Dies ist normal und kann nur beim Senden dieser ersten Nachricht erscheinen.
Sie können überprüfen, ob das Verzeichnis erstellt wurde, indem Sie nach Ihrem Verzeichnis ~ / Maildir suchen:
Sie werden sehen, dass die Verzeichnisstruktur erstellt wurde und dass sich eine neue Nachrichtendatei im Verzeichnis ~ / Maildir / new befindet:
Nachdem die Verzeichnisstruktur nun erstellt wurde, können Sie den s-nail-Client testen, indem Sie die von Ihnen gesendete init-Nachricht anzeigen und eine Nachricht an eine externe E-Mail-Adresse senden.
Schritt 5 - Testen des Clients
Führen Sie zum Öffnen des Clients den Befehl s-nail aus:
In Ihrer Konsole sehen Sie einen rudimentären Posteingang, in dem die init-Nachricht wartet:
Drücken Sie ENTER, um die Nachricht anzuzeigen:
Sie können zurück zu der Nachrichtenliste gelangen, indem Sie h eingeben und dann ENTER drücken.
Beachten Sie, dass die Nachricht jetzt den Status R hat, was anzeigt, dass sie gelesen wurde.
Da diese Nachricht nicht sehr nützlich ist, können Sie sie löschen, indem Sie d und dann ENTER drücken.
Um zurück zum Terminal zu gelangen, geben Sie q und dann ENTER ein:
Prüfen Sie abschließend, ob s-nail in der Lage ist, E-Mail-Nachrichten korrekt zu senden.
Dazu können Sie den Inhalt einer Textdatei an den s-nail-Prozess übergeben, wie Sie es mit der init-Nachricht getan haben, die Sie im vorherigen Schritt gesendet haben.
Beginnen Sie mit dem Schreiben einer Testnachricht in einem Texteditor:
Geben Sie im Editor den Text ein, den Sie senden möchten:
Speichern und schließen Sie die Datei, nachdem Sie Ihre Nachricht geschrieben haben.
Verwenden Sie dann den Befehl cat, um die Nachricht an den s-nail-Prozess zu übergeben.
Sie können dies mit dem folgenden Beispiel tun, das diese Optionen verwendet:
-s: Dies definiert die Betreffzeile der E-Mail-Nachricht
-r: Eine optionale Änderung des Felds "Von:" der E-Mail.
Standardmäßig wird der Linux-Benutzer, unter dem Sie angemeldet sind, zum Ausfüllen dieses Feldes verwendet.
Die Option -r ermöglicht es Ihnen, diese mit einer gültigen Adresse zu überschreiben, z. B. mit einer der Adressen, die Sie in der Datei / etc / postfix / virtual definiert haben.
Zur Veranschaulichung verwendet der folgende Befehl contact @ example.com.
Stellen Sie außerdem sicher, dass Sie < ^ > user < ^ > @ < ^ > email.com < ^ > in eine gültige E-Mail-Adresse ändern, auf die Sie Zugriff haben:
Navigieren Sie dann zum Posteingang für die E-Mail-Adresse, an die Sie die Nachricht geschickt haben.
Dort sehen Sie Ihre Nachricht fast sofort warten.
< $> note Anmerkung: Wenn sich die Nachricht nicht in Ihrem Posteingang befindet, wurde sie möglicherweise an Ihren Spam-Ordner geleitet.
Sie können Ihre gesendeten Nachrichten in Ihrem s-nail-Client anzeigen.
Starten Sie den interaktiven Client erneut:
Sie können Ihre gesendeten Nachrichten in dem E-Mail-Client anzeigen, indem Sie Folgendes eingeben:
Sie können gesendete Mail mit den gleichen Befehlen verwalten, die Sie für die eingehende Mail verwenden.
Sie haben Postfix nun auf Ihrem Ubuntu 20.04-Server konfiguriert.
Die Verwaltung von E-Mail-Servern kann für neue Systemadministratoren eine schwierige Aufgabe sein, aber mit dieser Konfiguration sollten Sie über ausreichend MTA-E-Mail-Funktionen verfügen, um sofort loslegen zu können.
Installieren und Verwenden von Docker Compose unter Ubuntu 20.04
5810
Docker vereinfacht die Verwaltung von Anwendungsprozessen in Containern.
Obwohl Container in gewisser Hinsicht virtuellen Rechnern ähneln, sind sie leichter und ressourcenschonender.
Dies ermöglicht Entwicklern, eine Anwendungsumgebung in mehrere isolierte Dienste zu unterteilen.
Bei Anwendungen, die von mehreren Diensten abhängig sind, kann die Organisation aller Container zum gemeinsamen Starten, Kommunizieren und Herunterfahren schnell unhandlich werden.
Docker Compose ist ein Tool, mit dem Sie Anwendungsumgebungen mit mehreren Containern basierend auf in einer YAML-Datei festgelegten Definitionen ausführen können.
In diesem Leitfaden zeigen wir Ihnen, wie Sie Docker Compose auf einem Ubuntu 20.04-Server installieren und wie Sie mit der Verwendung dieses Tools beginnen können.
Um diese Anleitung mitzuverfolgen, benötigen Sie:
Zugriff auf einen lokalen Ubuntu-20.04-Rechner oder einen Entwicklungsserver als Nicht-root-Benutzer und mit sudo-Berechtigungen.
Um diese Einzurichten, lesen Sie bitte unseren Leitfaden zur Ersteinrichtung eines Servers für Ubuntu 20.04.
Auf Ihrem Server oder Ihrem lokalen Rechner gemäß den Schritten 1 und 2 von Installieren und Verwenden von Docker unter Ubuntu 20.04 installiertes Docker.
Schritt 1 - Installieren von Docker Compose
Um sicherzustellen, dass wir die aktuellste stabile Version von Docker Compose erhalten, werden wir diese Software von ihrem offiziellen Github-Repository herunterladen.
Bestätigen Sie zunächst die auf ihrer Seite verfügbare neueste Version.
Zum Zeitpunkt des Schreibens dieses Artikels ist die aktuellste stabile Version 1.26.0.
Mit dem folgenden Befehl wird die Version 1.26.0 herunterladen und die ausführbare Datei unter / usr / local / bin / docker-compose gespeichert, wodurch diese Software global als docker-compose zugänglich wird:
Legen Sie als Nächstes die richtigen Berechtigungen fest, damit der Befehl docker-compose ausführbar ist:
Um zu überprüfen, ob die Installation erfolgreich war, können Sie Folgendes ausführen:
Docker Compose ist nun erfolgreich auf Ihrem System installiert.
Im nächsten Abschnitt sehen wir uns an, wie Sie eine docker-compose.yml-Datei einrichten und mit diesem Tool eine containerisierte Umgebung zum Laufen bringen.
Schritt 2 - Einrichten einer docker-compose.yml-Datei
Um zu zeigen, wie eine docker-compose.yml-Datei eingerichtet und mit Docker Compose gearbeitet wird, erstellen wir unter Verwendung des offiziellen Nginx-Images vom Docker Hub, der öffentlichen Docker-Registry, eine Webserverumgebung
Diese containerisierte Umgebung wird eine einzelne statische HTML-Datei bereitstellen.
Beginnen Sie mit der Erstellung eines neuen Verzeichnisses in Ihrem Home-Ordner und verschieben Sie dieses dann zu:
Richten Sie in diesem Verzeichnis einen Anwendungsordner ein, der als Dokumentenstamm für Ihre Nginx-Umgebung dient:
Erstellen Sie mit Ihrem bevorzugten Texteditor eine neue index.html-Datei innerhalb des Ordners app:
Geben Sie den folgenden Inhalt in diese Datei ein:
Erstellen Sie als Nächstes die Datei docker-compose.yml:
Geben Sie den folgenden Inhalt in Ihre Datei docker-compose.yml ein:
Die Datei docker-compose.yml beginnt typischerweise mit der version-Definition.
Dadurch wird Docker Compose mitgeteilt, welche Konfigurationsversion wir verwenden.
Dann haben wir den services-Block, in dem wir die Dienste einrichten, die Teil dieser Umgebung sind.
In unserem Fall haben wir einen einzigen Dienst namens web.
Dieser Dienst verwendet das Image nginx: alpine und richtet mit der Anweisung ports eine Portumleitung ein.
Alle Anfragen auf Port 8000 des Host-Rechners (das System, von dem aus Docker Compose ausgeführt wird) werden auf den Web-Container auf Port 80 umgeleitet, wo Nginx ausgeführt wird.
Die Anweisung volumes erstellt ein gemeinsames Volumen zwischen dem Host-Rechner und dem Container.
Dadurch wird der lokale Ordner app mit dem Container geteilt und das Volumen befindet sich unter / usr / share / nginx / html innerhalb des Containers, der dann den Standard-Dokumentenstamm für Nginx überschreibt.
Wir haben eine Demo-Seite und eine Datei docker-compose.yml eingerichtet, um eine containerisierte Webserverumgebung zu erstellen, die sie bedienen wird. Im nächsten Schritt stellen wir diese Umgebung mit Docker Compose bereit.
Schritt 3 - Ausführen von Docker Compose
Mit der docker-compose.yml-Datei können wir nun Docker Compose ausführen, um unserer Umgebung aufzurufen.
Der folgende Befehl lädt die erforderlichen Docker-Images herunter, erstellt einen Container für den web-Dienst und führt die containerisierte Umgebung im Hintergrundmodus aus:
Docker Compose sucht zunächst nach dem definierten Image auf Ihrem lokalen System, und wenn es das Image nicht lokalisieren kann, wird es das Image vom Docker Hub herunterladen.
Ihre Umgebung ist nun aktiviert und wird im Hintergrund ausgeführt.
Um zu überprüfen, ob der Container aktiv ist, können Sie Folgendes ausführen:
Dieser Befehl zeigt Ihnen Informationen über die ausgeführten Container und ihren Zustand sowie alle derzeit bestehenden Portumleitungen an:
Sie können nun auf die Demo-Anwendung zugreifen, indem Sie Ihren Browser entweder auf localhost: 8000 verweisen, wenn Sie diese Demo auf Ihrem lokalen Rechner ausführen, oder auf < ^ > your _ server _ domain _ or _ IP < ^ >: 8000, wenn Sie diese Demo auf einem Remote-Server ausführen.
Docker Compose Demo-Seite
Da das gemeinsame Volumen, das Sie in der Datei docker-compose.yml eingerichtet haben, Ihre app-Orderndateien mit dem Dokumentenstamm des Containers synchronisiert hält.
Wenn Sie Änderungen an der Datei index.html vornehmen, werden diese automatisch vom Container übernommen und somit in Ihrem Browser angezeigt, wenn Sie die Seite neu laden.
Im nächsten Schritt sehen Sie, wie Ihre containerisierte Umgebung mit den Docker Compose-Befehlen verwaltet wird.
Schritt 4 - Kennenlernen der Befehle von Docker Compose
Sie haben gesehen, wie Sie eine docker-compose.yml-Datei einrichten und Ihre Umgebung mit docker-compose aufrufen.
Nun sehen Sie, wie Sie die Docker Compose-Befehle verwenden, um Ihre containerisierte Umgebung zu verwalten und mit ihr zu interagieren.
Um die von Ihrem Nginx-Container erzeugten Protokolle zu überprüfen, können Sie den Befehl logs verwenden:
Wenn Sie die Ausführung der Umgebung unterbrechen möchten, ohne den aktuellen Zustand Ihrer Container zu ändern, können Sie Folgendes verwenden:
Um die Ausführung nach einer Unterbrechung fortzusetzen:
Der Befehl stop beendet die Ausführung des Containers, zerstört jedoch keine Daten, die mit Ihren Containern verknüpft sind:
Wenn Sie die mit dieser containerisierten Umgebung verknüpften Container, Netzwerke und Volumen entfernen möchten, verwenden Sie den Befehl down:
Beachten Sie, dass dadurch nicht das Basis-Image entfernt wird, das von Docker Compose zum Hochfahren Ihrer Umgebung verwendet wird (in unserem Fall nginx: alpine).
Auf diese Weise wird der Prozess jedes Mal, wenn Sie Ihre Umgebung mit einem docker-compose up wieder hochfahren, viel schneller sein, da sich das Image bereits auf Ihrem System befindet.
Falls Sie auch das Basis-Image von Ihrem System entfernen möchten, können Sie Folgendes verwenden:
< $> note Anmerkung: Eine ausführliche Referenz zu den Docker-Befehlen finden Sie in unserem Leitfaden zum Installieren und Verwenden von Docker.
In diesem Leitfaden haben wir gesehen, wie wir Docker Compose installieren und eine containerisierte Umgebung basierend auf einem Nginx Webserver-Image einrichten.
Wir haben auch gesehen, wie diese Umgebung mit den Compose-Befehlen verwaltet wird.
Eine vollständige Referenz aller verfügbaren docker-compose Befehle finden Sie in der offiziellen Dokumentation.
Installieren und Konfigurieren von Zabbix zur sicheren Überwachung von Remoteservern unter Ubuntu 20.04
6049
Zabbix ist eine Open-Source-basierte Überwachungssoftware für Netzwerke und Anwendungen.
Sie ermöglicht eine Echtzeitüberwachung von Tausenden von Metriken, die von Servern und virtuellen Maschinen, Netzwerkgeräten und Webanwendungen gesammelt werden.
Diese Metriken können Ihnen helfen, den aktuellen Status Ihrer IT-Infrastruktur zu ermitteln und Probleme mit Hardware- oder Softwarekomponenten zu erkennen, bevor sich Kunden beschweren.
Nützliche Informationen werden in einer Datenbank gespeichert, damit Sie diese im Laufe der Zeit analysieren und die Qualität der bereitgestellten Dienste verbessern oder Upgrades für Ihre Geräte planen können.
Zabbix verwendet verschiedene Optionen zum Sammeln von Metriken, darunter Überwachung ohne Agent von Benutzerdiensten und Client-Server-Architektur.
Um Servermetriken zu sammeln, gibt es auf dem überwachten Client einen kleinen Agenten zum Erfassen von Daten und Senden dieser Daten an den Zabbix-Server.
Zabbix unterstützt verschlüsselte Kommunikation zwischen dem Server und verbundenen Clients. So werden Ihre Daten geschützt, wenn sie über unsichere Netzwerke übertragen werden.
Der Zabbix-Server speichert seine Daten in einer relationalen Datenbank, die auf MySQL oder PostgreSQL basiert.
Außerdem können Sie historische Daten in NoSQL-Datenbanken wie Elasticsearch und TimescaleDB speichern.
Zabbix bietet eine Weboberfläche, mit der Sie Daten anzeigen und Systemeinstellungen konfigurieren können.
In diesem Tutorial konfigurieren Sie Zabbix auf zwei Ubuntu 20.04-Rechnern.
Einer wird als Zabbix-Server konfiguriert, der andere als Client, den Sie überwachen möchten.
Der Zabbix-Server wird eine MySQL-Datenbank nutzen, um Überwachungsdaten zu erfassen, und Nginx zur Bereitstellung der Weboberfläche verwenden.
Zwei Ubuntu 20.04-Server, die gemäß des Leitfadens zur Ersteinrichtung des Servers für Ubuntu 20.04 eingerichtet wurden, einschließlich eines Nicht-root-Benutzers, der über sudo-Berechtigungen verfügt, und einer mit ufw konfigurierten Firewall.
Auf einem Server installieren Sie Zabbix; in diesem Tutorial wird dieser Server als Zabbix-Server bezeichnet.
Er wird Ihren zweiten Server überwachen; dieser zweite Server wird als zweiter Ubuntu-Server bezeichnet.
Der Server, auf dem der Zabbix-Server ausgeführt wird, setzt installiertes Nginx, MySQL und PHP voraus.
Folgen Sie Schritt 1-3 unserer Anleitung zum Ubuntu 20.04-LEMP-Stack, um diese Anwendungen auf Ihrem Zabbix-Server zu konfigurieren.
Einen registrierten Domänennamen.
Dieses Tutorial verwendet in allen Bereichen your _ domain.
Sie können einen Domänennamen unter Namecheap erwerben oder einen kostenlosen von Freenom herunterladen oder einfach die Domänenregistrierungsstelle Ihrer Wahl verwenden.
Die beiden folgenden DNS-Einträge, eingerichtet für Ihren Zabbix-Server.
Wenn Sie DigitalOcean verwenden, lesen Sie bitte unsere DNS Dokumentation für Details, wie Sie sie hinzufügen.
Einen A-Datensatz, wobei < ^ > your _ domain < ^ > auf die öffentliche IP-Adresse Ihres Zabbix-Servers verweist.
Einen A-Datensatz, wobei www. < ^ > your _ domain < ^ > auf die öffentliche IP-Adresse Ihres Zabbix-Servers verweist.
Da der Zabbix-Server dazu dient, wertvolle Informationen über Ihre Infrastruktur aufzurufen, auf die nicht autorosierte Benutzer nicht zugreifen sollen, sollten Sie Ihren Server unbedingt schützen, indem Sie ein TLS / SSL-Zertifikat installieren.
Wenn Sie Ihren Server schützen möchten, folgen Sie nach Schritt 3 dieses Tutorials dem Leitfaden Let 's Encrypt unter Ubuntu 20.04.
Schritt 1 - Installieren des Zabbix-Servers
Zuerst müssen Sie Zabbix auf dem Server installieren, auf dem Sie MySQL, Nginx und PHP installiert haben.
Melden Sie sich bei diesem Rechner als Ihr non-root user an:
Zabbix ist im Paketmanager von Ubuntu verfügbar, ist jedoch veraltet. Verwenden Sie daher das offizielle Ubuntu-Repository zum Installieren der neuesten stabilen Version.
Laden Sie das Repository-Konfigurationspaket herunter und installieren Sie es:
Aktualisieren Sie den Paketindex, sodass das neue Repository enthalten ist:
Installieren Sie dann den Zabbix-Server und das Web-Frontend mit Unterstützung der MySQL-Datenbank:
Installieren Sie außerdem den Zabbix-Agenten, mit dem Sie Daten über den Status des Zabbix-Servers selbst sammeln können.
Bevor Sie Zabbix nutzen können, müssen Sie eine Datenbank einrichten, um die Daten zu speichern, die der Zabbix-Server von seinen Agenten erfassen wird.
Sie können dies im nächsten Schritt tun.
Schritt 2 - Konfigurieren der MySQL-Datenbank für Zabbix
Sie müssen eine neue MySQL-Datenbank erstellen und sie mit einigen grundlegenden Daten befüllen, um sie bereit für Zabbix zu machen.
Außerdem erstellen Sie einen bestimmten Benutzer für diese Datenbank, damit Zabbix sich nicht mit dem root-Konto bei MySQL anmeldet.
Melden Sie sich als root user bei MySQL an:
Erstellen Sie die Zabbix-Datenbank mit Unterstützung für UTF-8-Zeichen:
Erstellen Sie dann einen Benutzer, den der Zabbix-Server verwenden wird, gewähren Sie ihm Zugriff auf die neue Datenbank und legen Sie das Passwort für den Benutzer fest:
Damit sind der Benutzer und die Datenbank einsatzbereit.
Beenden Sie die Datenbankkonsole.
Als Nächstes müssen Sie das erste Schema und die Daten importieren. Die Zabbix-Installation bietet Ihnen eine Datei, die diese Einrichtung übernimmt.
Führen Sie folgenden Befehl aus, um das Schema einzurichten und die Daten in die zabbix-Datenbank zu importieren.
Verwenden Sie zcat, da die Daten in der Datei komprimiert sind:
Geben Sie das Passwort für den von Ihnen konfigurierten zabbix-MySQL-Benutzer ein, wenn Sie dazu aufgefordert werden.
Die Ausführung des Befehls kann eine oder zwei Minuten dauern.
Damit der Zabbix-Server diese Datenbank nutzt, müssen Sie in der Konfigurationsdatei für den Zabbix-Server das Datenbankpasswort festlegen.
Öffnen Sie die Konfigurationsdatei in Ihrem bevorzugten Editor.
Dieses Tutorial verwendet nano:
Suchen Sie nach dem folgenden Abschnitt der Datei:
Diese Kommentare in der Datei erklären, wie Sie eine Verbindung mit der Datenbank herstellen können.
Sie müssen den Wert von DBPassword in der Datei auf das Passwort für Ihren Datenbankbenutzer setzen.
Fügen Sie die Zeile nach den Kommentaren hinzu, um die Datenbank zu konfigurieren:
Speichern und schließen Sie zabbix _ server.conf, indem Sie Strg + X drücken, gefolgt von Y und dann der Eingabetaste, wenn Sie nano verwenden.
Sie haben den Zabbix-Server nun so konfiguriert, dass er eine Verbindung zur Datenbank herstellt.
Als Nächstes konfigurieren Sie den Nginx-Webserver, um das Zabbix-Frontend bereitzustellen.
Schritt 3 - Konfigurieren von Nginx für Zabbix
Installieren Sie das automatische Konfigurationspaket, um Nginx automatisch zu konfigurieren:
Dadurch erhalten Sie die Konfigurationsdatei / etc / zabbix / nginx.conf sowie einen Link dahin im Nginx-Konfigurationsverzeichnis / etc / nginx / conf.d / zabbix.conf.
Als Nächstes müssen Sie Änderungen an dieser Datei vornehmen.
Öffnen Sie die Konfigurationsdatei:
Die Datei enthält eine automatisch generierte Nginx-Serverblock-Konfiguration.
Sie enthält zwei Zeilen, die den Namen des Servers bestimmen und festlegen, an welchem Port er lauscht:
Heben Sie die Kommentierung der beiden Zeilen auf und ersetzen Sie example.com durch Ihren Domänennamen.
Ihre Einstellungen werden wie folgt aussehen:
Als Nächstes testen Sie, um sicherzustellen, dass es in keiner Ihrer Nginx-Dateien Syntaxfehler gibt, und laden Sie die Konfiguration neu:
Nachdem Nginx so eingerichtet ist, dass das Zabbix-Frontend bereitgestellt wird, werden Sie nun einige Änderungen an Ihrer PHP-Einrichtung vornehmen, damit die Zabbix-Weboberfläche ordnungsgemäß funktioniert.
Wenn Sie dies tun möchten, folgen Sie unserem Ubuntu 20.04-Tutorial für Let 's Encrypt, bevor Sie mit Schritt 4 fortfahren, um ein kostenloses SSL-Zertifikat für Nginx zu erhalten.
Dieser Prozess wird Ihren Zabbix-Serverblock automatisch erkennen und ihn für HTTPS konfigurieren.
Schritt 4 - Konfigurieren von PHP für Zabbix
Die Zabbix-Weboberfläche ist in PHP geschrieben und erfordert einige spezielle PHP-Servereinstellungen.
Der Zabbix-Installationsprozess hat eine PHP-FPM-Konfigurationsdatei erstellt, die diese Einstellungen enthält.
Sie befindet sich im Verzeichnis / etc / zabbix und wird von PHP-FPM automatisch geladen.
Sie müssen eine kleine Änderung in dieser Datei vornehmen; öffnen Sie sie also folgendermaßen:
Die Datei enthält PHP-Einstellungen, die die erforderlichen Anforderungen für die Zabbix-Weboberfläche erfüllen.
Die Einstellung der Zeitzone ist jedoch standardmäßig auskommentiert.
Um sicherzustellen, dass Zabbix die richtige Zeit verwendet, müssen Sie die richtige Zeitzone festlegen:
Heben Sie die Kommentierung der im vorigen Codeblock markierten Zeitzonenzeile auf und ändern Sie sie in Ihre Zeitzone.
Sie können diese Liste der unterstützten Zeitzonen verwenden, um die richtige für Sie zu finden.
Speichern und schließen Sie die Datei.
Starten Sie PHP-FPM nun neu, um die neuen Einstellungen anzuwenden:
Sie können den Zabbix-Server nun starten:
Überprüfen Sie dann, ob der Zabbix-Server ordnungsgemäß ausgeführt wird:
Sie werden den folgenden Status sehen:
Aktivieren Sie den Server abschließend so, dass er beim Booten gestartet wird:
Der Server ist eingerichtet und mit der Datenbank verbunden.
Erstellen Sie als Nächstes das Web-Frontend.
Schritt 5 - Konfigurieren von Einstellungen für die Zabbix-Weboberfläche
Mit der Weboberfläche können Sie Berichte anzeigen und Hosts hinzufügen, die Sie überwachen möchten. Dies muss jedoch vor der Verwendung eingerichtet werden. Starten Sie dazu Ihren Browser und rufen Sie die Adresse http: / / < ^ > zabbix _ server _ name < ^ > oder https: / / < ^ > zabbix _ server _ name < ^ > auf, wenn Sie Let 's Encrypt verwenden.
Im ersten Bildschirm sehen Sie eine Willkommensnachricht.
Klicken Sie auf Nächster Schritt um fortzufahren.
Im nächsten Bildschirm sehen Sie die Tabelle, die alle Voraussetzungen zur Ausführung von Zabbix auflistet.
Alle Werte in dieser Tabelle müssen OK sein. Überprüfen Sie also, ob sie es sind.
Stellen Sie sicher, dass Sie nach unten scrollen und alle Voraussetzungen ansehen.
Sobald Sie geprüft haben, ob alles bereit ist, klicken Sie auf Nächster Schritt um fortzufahren.
Der nächste Bildschirm bittet um Informationen zur Datenbankverbindung.
DB-Verbindung
Sie haben den Zabbix-Server über Ihre Datenbank informiert. Die Zabbix-Weboberfläche benötigt jedoch auch Zugriff auf die Datenbank, um Hosts verwalten und Daten lesen zu können. Geben Sie daher die MySQL-Anmeldedaten ein, die Sie in Schritt 2 konfiguriert haben. Klicken Sie auf Nächster Schritt um fortzufahren.
Im nächsten Bildschirm können Sie die Optionen bei ihren Standardwerten belassen.
Zabbix Server-Details
Der Name ist optional; er wird in der Weboberfläche verwendet, um einen Server vom anderen zu unterscheiden, falls Sie über mehrere Überwachungsserver verfügen.
Klicken Sie auf Nächster Schritt um fortzufahren.
Im nächsten Bildschirm wird die Zusammenfassung vor der Installation angezeigt, sodass Sie prüfen können, ob alles korrekt ist.
Zusammenfassung
Klicken Sie auf Nächster Schritt, um mit dem letzten Bildschirm fortzufahren.
Die Einrichtung der Weboberfläche ist nun abgeschlossen.
Dieser Prozess erstellt die Konfigurationsdatei / usr / share / zabbix / conf / zabbix.conf.php, die Sie sichern und in Zukunft verwenden können.
Klicken Sie auf Fertig stellen, um mit dem Anmeldebildschirm fortzufahren.
Der Standardbenutzer ist Admin und das Passwort lautet zabbix.
Richten Sie, bevor Sie sich anmelden, den Zabbix-Agenten auf Ihrem zweiten Ubuntu-Server ein.
Schritt 6 - Installieren und Konfigurieren des Zabbix-Agenten
Jetzt müssen Sie die Agentensoftware konfigurieren, die Überwachungsdaten an den Zabbix-Server senden wird.
Melden Sie sich beim zweiten Ubuntu-Server an:
Führen Sie wie auf dem Zabbix-Server die folgenden Befehle aus, um das Repository-Konfigurationspaket zu installieren:
Aktualisieren Sie als Nächstes den Paketindex:
Installieren Sie dann den Zabbix-Agenten:
Zwar unterstützt Zabbix zertifikatbasierte Verschlüsselung, doch ist die Einrichtung einer Zertifizierungsstelle nicht Teil dieses Tutorials.
Sie können jedoch Pre-Shared Keys (PSK) verwenden, um die Verbindung zwischen dem Server und Agenten zu sichern.
Erstellen Sie zunächst einen PSK:
Zeigen Sie den Schlüssel mit cat an, damit Sie ihn kopieren können:
Der Schlüssel wird in etwa so aussehen:
Speichern Sie ihn für später; Sie brauchen ihn noch, um den Host konfigurieren.
Bearbeiten Sie nun die Zabbix-Agenteneinstellungen, um die sichere Verbindung zum Zabbix-Server einzurichten.
Öffnen Sie die Konfigurationsdatei für den Agenten in Ihrem Texteditor:
Jede Einstellung innerhalb dieser Datei wird über informative Kommentare in der Datei dokumentiert; Sie müssen jedoch nur einige von ihnen bearbeiten.
Zuerst müssen Sie die IP-Adresse des Zabbix-Servers bearbeiten.
Ändern Sie den Standardwert in die IP-Adresse Ihres Zabbix-Servers:
Standardmäßig verbindet sich der Zabbix-Server mit dem Agenten.
Bei einigen Überprüfungen (z. B. Überwachung der Protokolle) ist jedoch eine umgekehrte Verbindung erforderlich.
Für eine korrekte Funktionsweise müssen Sie die Adresse des Zabbix-Servers und einen eindeutigen Hostnamen angeben.
Finden Sie den Abschnitt, der die aktiven Prüfungen konfiguriert, und ändern Sie die Standardwerte:
Suchen Sie als Nächstes nach dem Bereich, der die sichere Verbindung mit dem Zabbix-Server konfiguriert, und aktivieren Sie PSK-Unterstützung.
Finden Sie den Abschnitt TLSConnect, der so aussieht:
Fügen Sie dann diese Zeile hinzu, um PSK-Unterstützung zu konfigurieren:
Suchen Sie als Nächstes den Abschnitt TLSAccept, der so aussieht:
Konfigurieren Sie eingehende Verbindungen so, dass sie Pre-Shared Keys unterstützen, indem Sie folgende Zeile hinzufügen:
Suchen Sie als Nächstes den Abschnitt TLSPSKIdentity, der so aussieht:
Wählen Sie einen eindeutigen Namen, um Ihren Pre-Shared Key zu identifizieren, indem Sie diese Zeile hinzufügen:
Sie werden ihn als PSK ID verwenden, wenn Sie Ihren Host über die Zabbix-Weboberfläche hinzufügen.
Legen Sie dann die Option fest, die auf Ihren zuvor erstellten Pre-Shared Key verweist.
Suchen Sie nach der Option TLSPSKFile:
Fügen Sie diese Zeile hinzu, um den Zabbix-Agent auf die von Ihnen erstellte PSK-Datei zu verweisen:
Jetzt können Sie den Zabbix-Agenten neu starten und ihn so einrichten, dass er beim Booten gestartet wird:
Überprüfen Sie zur Sicherheit, ob der Zabbix-Agent ordnungsgemäß ausgeführt wird:
Sie werden den folgenden Status sehen, was zeigt, dass der Agent ausgeführt wird:
Der Agent wird an Port 10050 nach Verbindungen vom Server lauschen.
Konfigurieren Sie UFW so, dass Verbindungen zu diesem Port zugelassen werden:
Mehr über UFW erfahren Sie in Einrichten einer Firewall mit UFW unter Ubuntu 20.04.
Ihr Agent ist nun bereit, Daten an den Zabbix-Server zu senden.
Um ihn verwenden zu können, müssen Sie ihn jedoch noch über die Webkonsole des Servers verlinken.
Im nächsten Schritt werden Sie die Konfiguration abschließen.
Schritt 7 - Hinzufügen des neuen Host zum Zabbix-Server
Die Installation des Agenten auf einem Server, den Sie überwachen möchten, ist nur die halbe Arbeit.
Jeder Host, den Sie überwachen möchten, muss beim Zabbix-Server registriert werden. Das können Sie über die Weboberfläche tun.
Melden Sie sich bei der Weboberfläche des Zabbix-Servers an, indem Sie zur Adresse http: / / < ^ > zabbix _ server _ name < ^ > oder https: / / < ^ > zabbix _ server _ name < ^ > navigieren:
Der Zabbix-Anmeldebildschirm
Nachdem Sie sich angemeldet haben, klicken Sie auf Konfiguration und dann in der linken Navigationsleiste auf Hosts.
Klicken Sie dann in der rechten oberen Ecke des Bildschirms auf die Schaltfläche Host erstellen.
Dadurch wird die Hostkonfigurationsseite geöffnet.
Erstellen eines Hosts
Passen Sie den Hostnamen und die IP-Adresse so an, dass sie den Hostnamen und die IP-Adresse Ihres zweiten Ubuntu-Servers widerspiegeln. Fügen Sie den Host dann einer Gruppe hinzu.
Sie können eine bestehende Gruppe auswählen, z. B. Linux-Server, oder eine eigene Gruppe erstellen.
Der Host kann Mitglied in mehreren Gruppen sein.
Geben Sie dazu den Namen einer bestehenden oder neuen Gruppe im Feld Gruppen ein und wählen Sie den gewünschten Wert aus der vorgeschlagenen Liste aus.
Klicken Sie vor dem Hinzufügen der Gruppe auf die Registerkarte Vorlagen.
Hinzufügen einer Vorlage zum Host
Geben Sie im Suchfeld Template OS Linux by Zabbix agent ein und wählen Sie dann die Vorlage aus der Liste aus, um sie dem Host hinzuzufügen.
Navigieren Sie als Nächstes zur Registerkarte Verschlüsselung. Wählen Sie sowohl für Verbindungen zu Host als auch für Verbindungen von Host PSK aus.
Setzen Sie dann die PSK-Identität auf PSK 001, was der Wert der zuvor konfigurierten TLSPSKIdentity-Einstellung des Zabbix-Agenten ist.
Setzen Sie dann den PSK-Wert auf den Schlüssel, den Sie für den Zabbix-Agenten generiert haben.
Es ist der Wert, den Sie in der Datei / etc / zabbix / zabbix _ agentd.psk auf dem Agentengerät gespeichert haben.
Einrichten der Verschlüsselung
Klicken Sie abschließend unten im Formular auf die Schaltfläche Hinzufügen, um den Host zu erstellen.
Sie werden Ihren neuen Host in der Liste sehen.
Warten Sie eine Minute lang und laden Sie die Seite neu, bis Sie grüne Markierungen sehen, was bedeutet, dass alles ordnungsgemäß funktioniert und die Verbindung verschlüsselt ist.
Zabbix zeigt Ihren neuen Host
Wenn Sie zusätzliche Server haben, die Sie überwachen möchten, müssen Sie sich bei jedem Host anmelden, den Zabbix-Agenten installieren, einen PSK generieren, den Agenten konfigurieren und den Host zur Weboberfläche hinzufügen (genauso wie beim ersten Host).
Der Zabbix-Server überwacht nun Ihren zweiten Ubuntu-Server.
Erstellen Sie nun E-Mail-Benachrichtigungen, um über Probleme informiert zu werden.
Schritt 8 - Konfigurieren von E-Mail-Benachrichtigungen
Zabbix unterstützt automatisch verschiedene Arten von Benachrichtigungen: E-Mail, OTRS, Slack, Telegram, SMS etc. Eine vollständige Liste von Integrationen finden Sie auf der Zabbix-Website.
Als Beispiel werden wir in diesem Tutorial Benachrichtigungen für den Medientyp E-Mail konfigurieren.
Sie werden eine Liste mit allen Medientypen sehen.
Es gibt zwei vorkonfigurierte Optionen für E-Mails: für die Klartextbenachrichtigung und für die HTML-Benachrichtigungen.
In diesem Tutorial werden Sie eine Klartextbenachrichtigung verwenden.
Klicken Sie auf E-Mail.
Passen Sie die SMTP-Optionen gemäß den Einstellungen Ihres E-Mail-Dienstes an.
In diesem Tutorial werden SMTP-Funktionen von Gmail verwendet, um E-Mail-Benachrichtigungen einzurichten; wenn Sie weitere Informationen dazu wünschen, lesen Sie Verwenden des SMTP-Servers von Google.
< $> note Anmerkung: Wenn Sie eine 2-Schritt-Verifizierung mit Gmail verwenden, müssen Sie ein App-Passwort für Zabbix generieren.
Sie müssen bei der Einrichtung ein App-Passwort nur einmal eingeben.
Anweisungen zum Generieren dieses Passworts finden Sie im Google Help Center.
Wenn Sie Gmail verwenden, geben Sie smtp.gmail.com in das Feld SMTP-Server ein, 465 in das Feld SMTP-Serverport, gmail.com in SMTP helo und Ihre E-Mail-Adresse in SMTP-E-Mail.
Wählen Sie dann SSL / TLS für Verbindungssicherheit und Benutzername und Passwort für Authentifizierung.
Geben Sie Ihre Gmail-Adresse als Benutzernamen und das von Ihnen über Ihr Google-Konto generierte App-Passwort als Passwort ein.
Einrichten des Medientyps E-Mail
Auf der Registerkarte Nachrichtenvorlagen können Sie die Liste der vordefinierten Nachrichten für verschiedene Arten von Benachrichtigungen sehen.
Klicken Sie abschließend unten im Formular auf die Schaltfläche Aktualisieren, um die E-Mail-Parameter zu aktualisieren.
Jetzt können Sie das Senden von Benachrichtigungen testen.
Klicken Sie dazu in der entsprechenden Zeile auf den unterstrichenen Link Testen.
Sie werden ein Popup-Fenster sehen.
Geben Sie Ihre E-Mail-Adresse in das Feld Senden an ein und klicken Sie auf Testen.
Sie werden eine Nachricht zum erfolgreichen Versand sehen und eine Testnachricht erhalten.
Testen von E-Mail
Schließen Sie das Popup-Fenster durch Klicken auf die Schaltfläche Abbrechen.
Erstellen Sie nun einen neuen Benutzer.
Klicken Sie auf Administration und dann in der linken Navigationsleiste auf Benutzer.
Sie werden die Liste der Benutzer sehen.
Klicken Sie dann in der rechten oberen Ecke des Bildschirms auf die Schaltfläche Benutzer erstellen.
Damit wird die Seite für die Benutzerkonfiguration geöffnet:
Erstellen eines Benutzers
Geben Sie den neuen Benutzernamen im Feld Alias ein und legen Sie ein neues Passwort fest.
Fügen Sie als Nächstes den Benutzer zur Administratorgruppe hinzu.
Geben Sie Zabbix administrators in das Feld Gruppen ein und wählen Sie die Gruppe aus der vorgeschlagenen Liste aus.
Sobald Sie die Gruppe hinzugefügt haben, klicken Sie auf die Registerkarte Medien und dann auf den unterstrichenen Link Hinzufügen (nicht auf die Schaltfläche Hinzufügen darunter).
Hinzufügen einer E-Mail-Adresse
Wählen Sie die Option E-Mail aus dem Dropdownmenü Typ aus.
Geben Sie Ihre E-Mail-Adresse in das Feld Senden an ein.
Sie können die restlichen Optionen bei ihren Standardwerten belassen.
Klicken Sie unten zum Übermitteln auf die Schaltfläche Hinzufügen.
Navigieren Sie nun zur Registerkarte Berechtigungen. Wählen Sie Zabbix Super Admin aus dem Dropdownmenü Benutzertyp aus.
Klicken Sie abschließend unten im Formular auf die Schaltfläche Hinzufügen, um den Benutzer zu erstellen.
< $> note Anmerkung: Eine Verwendung des Standardpassworts ist nicht sicher.
Um das Passwort des integrierten Benutzers Admin zu ändern, klicken Sie in der Benutzerliste auf das entsprechende Alias.
Klicken Sie dann auf Passwort ändern, geben Sie ein neues Passwort ein und bestätigen Sie die Änderung durch Klicken auf Aktualisieren.
Jetzt müssen Sie Benachrichtigungen aktivieren.
Klicken Sie auf die Registerkarte Konfiguration und dann in der linken Navigationsleiste auf Aktionen.
Sie werden eine vorkonfigurierte Aktion sehen, die zum Senden von Benachrichtigungen an alle Zabbix-Administratoren dient.
Sie können die Einstellungen überprüfen und ändern, indem Sie auf den Namen klicken.
Verwenden Sie für die Zwecke dieses Tutorials die Standardparameter.
Um die Aktion zu aktivieren, klicken Sie in der Spalte Status auf den roten Link für Deaktiviert.
Jetzt sind Sie bereit dazu, Warnungen zu erhalten.
Im nächsten Schritt werden Sie eine Warnung erstellen, um Ihre Benachrichtigungskonfiguration zu testen.
Schritt 9 - Erstellen einer Testwarnung
In diesem Schritt erstellen Sie eine Testwarnung, um sicherzustellen, dass alles richtig verbunden ist.
Standardmäßig behält Zabbix den Überblick über die Menge an freien Speicherplatz auf Ihrem Server.
Der Server erkennt automatisch alle bereitgestellten Datenträger und fügt die entsprechenden Prüfungen hinzu.
Diese Erkennungsfunktion wird jede Stunde ausgeführt; Sie müssen also eine Weile warten, bis die Benachrichtigung ausgelöst wird.
Erstellen Sie eine temporäre Datei, die groß genug ist, um die Zabbix-Warnung bei Auslastung des Dateisystems auszulösen.
Melden Sie sich dazu bei Ihrem zweiten Ubuntu-Server an, wenn Sie nicht bereits verbunden sind:
Überprüfen Sie als Nächstes den freien Speicherplatz auf dem Server.
Sie können den Befehl df verwenden, um das herauszufinden:
Der Befehl df wird die Auslastung des Datenträgers in Ihrem Dateisystem melden. Der Befehl -h wird die Ausgabe für Menschen lesbar machen.
Sie sehen eine Ausgabe wie die folgende:
In diesem Fall beträgt der freie Speicherplatz 77 GB.
Ihr freier Speicherplatz kann sich davon unterscheiden.
Verwenden Sie den Befehl fallocate, mit dem Sie den Speicherplatz für eine Datei vorab zuweisen oder freigeben können, um eine Datei zu erstellen, die mehr als 80% des verfügbaren Speicherplatzes benötigt.
Das wird ausreichen, um die Warnung auszulösen:
Nach etwa einer Stunde wird Zabbix eine Warnung zur Menge des freien Speicherplatzes auslösen und die von Ihnen konfigurierte Aktion ausführen (also die Benachrichtigungsmeldung senden).
Sie können Ihren Posteingang auf die Nachricht vom Zabbix-Server prüfen.
Sie werden eine Nachricht sehen, die wie folgt aussieht:
Sie können auch zur Registerkarte Überwachung und dann zum Dashboard navigieren, um die Benachrichtigung und ihre Details anzuzeigen.
Haupt-Dashboard
Nachdem Sie nun wissen, dass die Warnungen funktionieren, löschen Sie die von Ihnen erstellte temporäre Datei, um den Speicherplatz wieder freizugeben:
Nach einer Minute wird Zabbix die Wiederherstellungsnachricht senden und die Warnmeldung aus dem Haupt-Dashboard verschwindet.
In diesem Tutorial haben Sie gelernt, wie Sie eine einfache und sichere Überwachungslösung einrichten können, die Ihnen bei der Überwachung des Status Ihrer Server hilft.
Die Lösung kann Sie nun vor Problemen warnen und bietet Ihnen die Möglichkeit, die in Ihrer IT-Infrastruktur vorkommenden Prozesse zu analysieren.
Um mehr über die Einrichtung einer Überwachungsinfrastruktur zu erfahren, lesen Sie unsere Seite zum Thema Überwachung.
Protokolle zentralisieren mit Journald unter Debian 10
6077
Zwei Debian 10-Server.
Anweisungen dazu finden Sie im Leitfaden Ersteinrichtung des Servers mit Debian 10.
< $> note Hinweis: Während des gesamten Tutorials werden Befehlsblöcke mit dem Servernamen (Client oder Server) gekennzeichnet, auf dem der Befehl ausgeführt werden soll.
Installieren Sie zunächst certbot und das Dienstprogramm curl auf beiden Hosts:
Schließlich müssen Sie eine Kopie der Let 's Encrypt-Zertifizierungsstelle und der Zwischenzertifikate herunterladen und in dieselbe Datei einfügen. journald verwendet diese Datei, um die Authentizität der Zertifikate auf dem Client und dem Server zu überprüfen, wenn sie miteinander kommunizieren.
Mit dem folgenden Befehl werden die beiden Zertifikate von der Let 's Encrypt-Website heruntergeladen und in einer einzigen Datei mit dem Namen letsencrypt-combined-certs.pem im Home-Verzeichnis Ihres Benutzers abgelegt.
SplitMode = host: Die Protokolle der Remoteclients werden nach Host in / var / log / journal / remote geteilt.
Ändern Sie als Nächstes das Gruppeneigentum des privaten Schlüssels in die Gruppe von systemd-journal-remote:
Installieren von Discourse unter Ubuntu 20.04
6065
Sie können Discourse als Mailingliste, Diskussionsforum oder Langform-Chatraum verwenden.
In diesem Tutorial installieren Sie Discourse in einer isolierten Umgebung unter Verwendung von Docker, einer Anwendung für Containerisierung.
Bevor Sie beginnen, gibt es einige Dinge, die Sie benötigen:
Einen Ubuntu 20.04-Server mit mindestens 2 GB RAM, einen Sudo-Nicht-root-Benutzer und eine Firewall.
Zur Anleitung können Sie unser Tutorial zur Ersteinrichtung des Servers für Ubuntu 20.04 hinzuziehen.
Docker, das auf Ihrem Server installiert ist.
Dazu können Sie Schritt 1 unseres Tutorials zur Docker-Installation für Ubuntu 20.04 folgen.
Eine Domäne (oder Subdomäne) mit einem verfügbaren A-Datensatz, der auf die IP-Adresse Ihres Servers verweist.
Wenn Sie Ihr DNS auf DigitalOcean verwalten, können Sie diesem Leitfaden folgen, um Ihre IP mit Ihrer Domäne zu verknüpfen.
Schritt 1 - Herunterladen von Discourse
Erstellen Sie vor dem Herunterladen und Installieren von Discourse das Verzeichnis / var / discourse.
Hier befinden sich alle Ihre Discourse-bezogenen Dateien:
Klonen Sie abschließend das offizielle Discourse-Docker-Image in / var / discourse.
Mit dem vorhandenen Discourse Docker-Image können Sie nun Ihre Plattform installieren und konfigurieren.
Schritt 2 & mdash; Installieren und Konfigurieren von Discourse
Gehen Sie zum Verzeichnis / var / discourse:
Starten Sie nun das enthaltene Setup-Skript:
Das Skript zur Discourse-Installation stellt die folgenden Fragen:
Geben Sie < ^ > discourse.your _ domain < ^ > oder den Hostnamen ein, den Sie für Ihre Plattform gewählt haben.
Diese kann mit Ihrer Discourse-Domäne völlig unverbunden und jede E-Mail-Adresse sein, die Sie für praktisch erachten.
Beachten Sie, dass diese E-Mail-Adresse zum Standard für den Discourse-Administrator wird.
Später müssen Sie diese E-Mail-Adresse wieder verwenden, wenn Sie Discourse über das Control Panel einrichten.
Wenn Sie Mailgun verwenden, lautet die Adresse des SMTP-Servers smtp.mailgun.org; der Benutzername und das Passwort sind SMTP-Anmeldeinformationen für Ihre Domäne unter der Mailgun-Registerkarte Domains.
Schließlich werden Sie im Discourse-Installationsskript aufgefordert, alle diese Einstellungen zu bestätigen.
Bestätigen Sie Ihre Einstellungen und das Skript generiert eine Konfigurationsdatei namens app.yml.
Der Installationsprozess wird automatisch gestartet.
Hinweis: Wenn Sie diese Einstellungen nach der Installation ändern oder korrigieren müssen, bearbeiten Sie Ihre Datei / containers / app.yml und führen Sie. / launcher rebuild app aus. Andernfalls werden Ihre Änderungen nicht wirksam.
Die Discourse-Installation dauert ca. 2-8 Minuten. Danach wird Ihre Instanz ausgeführt.
Jetzt können Sie einen Webbrowser öffnen und ein Administratorkonto erstellen.
Schritt 3 & mdash; Registrierung eines Administrator-Kontos
Besuchen Sie < ^ > discourse.your _ domain < ^ > in Ihrem bevorzugten Webbrowser, und Sie sehen den Begrüßungsbildschirm "Glückwunsch" für Discourse.
Discourse-Glückwunsch-Bildschirm
Wenn Sie einen 502 Bad Gateway-Fehler erhalten, warten Sie ein oder zwei Minuten und aktualisieren Sie dann Ihren Browser. Ihre Discourse-Installation ist möglicherweise noch nicht abgeschlossen.
Discourse-Konfigurationsassistent
Nachdem Sie den Setup-Assistenten abgeschlossen oder übersprungen haben, werden einige Themen und die Admin-Kurzanleitung für Discourse angezeigt.
Die Kurzanleitung trägt die Bezeichnung READ ME FIRST und enthält Tipps zum weiteren Anpassen Ihrer Discourse-Installation.
Discourse-Homepage und Link zur Admin-Kurzanleitung
Ihre Discourse-Plattform ist nun einsatzbereit.
Wenn Sie Discourse in Zukunft aktualisieren müssen, können Sie dies über die Befehlszeile tun, indem Sie die neueste Version des Codes aus dem Git-Repository beziehen und die App so neu erstellen:
Außerdem können Sie Discourse in Ihrem Browser aktualisieren.
Besuchen Sie http: / / < ^ > discourse.your _ domain < ^ > / admin / upgrade, klicken Sie auf Auf die neueste Version upgraden und folgen Sie den Anweisungen.
Discourse-Upgrade Admin-Upgrade-Seite
Erfahren Sie mehr über die Funktionen von Discourse auf der offiziellen Seite Über Discourse.
Verwenden des pathlib-Moduls zum Bearbeiten von Dateisystempfaden in Python 3
6079
Python 3 enthält das pathlib-Modul zur betriebssystemunabhängigen Bearbeitung von Dateisystempfaden. pathlib ähnelt dem os.path-Modul; pathlib bietet jedoch eine übergeordnete - und oft bequemere - Oberfläche als os.path.
Wir können Dateien auf einem Computer mit hierarchischen Pfaden identifizieren.
Beispielsweise können wir die Datei wave.txt auf einem Computer mit diesem Pfad identifizieren: / Users / < ^ > sammy < ^ > / ocean / wave.txt.
Betriebssysteme stellen Pfade etwas anders dar.
Windows kann den Pfad zur Datei wave.txt folgendermaßen darstellen: C:\ Users\ < ^ > sammy < ^ >\ ocean\ wave.txt.
Sie finden das pathlib-Modul ggf. nützlich, wenn Sie in Ihrem Python-Programm Dateien im Dateisystem erstellen oder verschieben, Dateien im Dateisystem auflisten, die alle einer bestimmten Erweiterung oder einem Muster entsprechen, oder basierend auf Sammlungen roher Zeichenfolgen dem jeweiligen Betriebssystem entsprechende Dateipfade erstellen.
Zwar ließen sich auch andere Tools (wie das Modul os.path) zur Erledigung vieler dieser Aufgaben verwenden, doch können Sie mit dem pathlib-Modul solche Operationen mit hoher Lesbarkeit und einer minimalen Codemenge ausführen.
In diesem Tutorial sehen wir uns einige der Wege an, um das Modul pathlib zum Darstellen und Bearbeiten von Dateisystempfaden zu verwenden.
Um das Beste aus diesem Tutorial herauszuholen, empfiehlt es sich, eine gewisse Vertrautheit mit Programmierung in Python 3 aufzuweisen. Sie können sich für die erforderlichen Hintergrundinformationen folgende Tutorials ansehen:
Codieren in Python 3
Erstellen von Path-Instanzen
Das pathlib-Modul bietet mehrere Klassen; eine der wichtigsten ist jedoch die Path-Klasse.
Instanzen der Path-Klasse stellen einen Pfad zu einer Datei oder einem Verzeichnis im Dateisystem unseres Computers dar.
Beispielsweise instanziiert der folgende Code eine Path-Instanz, die einen Teil des Pfads zu einer Datei wave.txt darstellt:
Wenn wir diesen Code ausführen, erhalten wir eine Ausgabe wie die folgende:
from pathlib import Path macht die Path-Klasse für unser Programm verfügbar.
Dann instanziiert Path (" ocean "," wave.txt ") eine neue Path-Instanz.
Ein Drucken der Ausgabe zeigt, dass Python das entsprechende Betriebssystemtrennzeichen / zwischen den beiden Pfadkomponenten hinzugefügt hat, die wir angegeben haben: "ocean" und "wave.txt".
< $> note Anmerkung: Je nach Betriebssystem kann Ihre Ausgabe von den Beispielausgaben in diesem Tutorial leicht abweichen.
Wenn Sie Windows ausführen, könnte Ihre Ausgabe für dieses erste Beispiel so aussehen: ocean\ wave.txt.
Aktuell enthält das Path-Objekt, das der Variable wave zugeordnet ist, einen relative path (relativen Pfad).
Anders gesagt kann ocean / wave.txt an mehreren Stellen in unserem Dateisystem vorkommen.
Beispielsweise kann die Datei in / Users / < ^ > user _ 1 < ^ > / ocean / wave.txt oder / Users / < ^ > user _ 2 < ^ > / research / ocean / wave.txt vorhanden sein; wir haben jedoch noch nicht genau angegeben, worauf wir uns beziehen.
Ein absoluter Pfad dagegen verweist eindeutig auf einen Speicherort im Dateisystem.
Mit Path.home () können Sie den absoluten Pfad zum Stammverzeichnis des aktuellen Benutzers abrufen:
Wenn wir diesen Code ausführen, erhalten wir eine Ausgabe, die der folgenden ähnelt:
< $> note Anmerkung: Wie zuvor erwähnt, wird Ihre Ausgabe je nach Betriebssystem variieren.
Auch Ihr Stammverzeichnis wird sich natürlich von / Users / < ^ > sammy < ^ > unterscheiden.
Path.home () gibt eine Path-Instanz mit einem absoluten Pfad zum Stammverzeichnis des aktuellen Benutzers zurück.
Dann übergeben wir diese Path-Instanz und die Zeichenfolgen "ocean" und "wave.txt" in einen anderen Path-Konstruktor, um einen absoluten Pfad zur Datei wave.txt zu erstellen.
Die Ausgabe zeigt, dass die erste Zeile das Stammverzeichnis ist und die zweite Zeile das Stammverzeichnis plus ocean / wave.txt ist.
Dieses Beispiel veranschaulicht auch ein wichtiges Merkmal der Path-Klasse: Der Path-Konstruktor akzeptiert sowohl Zeichenfolgen als auch bereits vorhandene Path-Objekte.
Sehen wir uns die Unterstützung für beide Zeichenfolgen und Path-Objekte im Path-Konstruktor etwas genauer an:
Wenn wir diesen Python-Code ausführen, erhalten wir eine Ausgabe wie die folgende:
shark ist ein Path (Pfad) zu einer Datei, die wir sowohl mit beiden Path-Objekten (Path.home () und Path (" fish "," shark.txt ")) als auch Zeichenfolgen (" ocean "und" animals ") erstellt haben.
Der Path-Konstruktor behandelt beide Arten von Objekten auf intelligente Weise und verknüpft sie sauber mit dem entsprechenden Betriebssystemtrennzeichen, in dem Fall /.
Zugriff auf Dateiattribute
Nachdem wir die Path-Instanzen erstellt haben, erfahren Sie nun, wie Sie diese Instanzen zum Zugriff auf Informationen über eine Datei verwenden können.
Wir können die Attribute name und suffix zum Zugreifen auf Dateinamen und Dateiendungen nutzen:
Wenn wir diesen Code ausführen, erhalten wir eine Ausgabe wie die folgende:
Diese Ausgabe zeigt, dass der Name der Datei am Ende unseres Pfades wave.txt ist und das Suffix dieser Datei .txt lautet.
Path-Instanzen bieten außerdem die Funktion with _ name, mit der Sie nahtlos ein neues Path-Objekt mit einem anderen Namen erstellen können:
Wenn wir dies ausführen, erhalten wir eine Ausgabe wie die folgende:
Der Code erstellt zunächst eine Path-Instanz, die auf eine Datei namens wave.txt verweist.
Dann rufen wir die Methode with _ name für wave auf, um eine zweite Path-Instanz zurückzugeben, die auf eine neue Datei namens tides.txt verweist.
Der Verzeichnisteil ocean / des Pfads bleibt unverändert, sodass der endgültige Pfad ocean / tides.txt lautet.
Zugriff auf Vorgänger
Manchmal ist es nützlich, Verzeichnisse aufzurufen, die einen bestimmten Pfad enthalten.
Betrachten wir ein Beispiel:
Wenn wir diesen Code ausführen, erhalten wir eine Ausgabe, die der folgenden ähnelt:
Das Attribut parent bei einer Path-Instanz gibt den unmittelbarsten Vorgänger eines bestimmten Dateipfads zurück.
In diesem Fall wird das Verzeichnis zurückgegeben, das die Datei shark.txt enthält: ocean / animals / fish.
Wir können auf das Attribut parent mehrmals hintereinander zugreifen, um die Vorgängerstruktur einer bestimmten Datei nach oben zu durchlaufen:
Wenn wir diesen Code ausführen, erhalten wir die folgende Ausgabe:
Die Ausgabe ähnelt der früheren Ausgabe, doch befinden wir uns nun noch eine Stufe höher, da wir .parent ein zweites Mal aufgerufen haben.
Zwei Verzeichnisse nach oben von shark.txt befindet sich das Verzeichnis ocean / animals.
Verwenden von Glob zum Auflisten von Dateien
Es ist auch möglich, die Path-Klasse zum Auflisten von Dateien mit der Methode glob zu verwenden.
Gehen wir davon aus, wir haben eine Verzeichnisstruktur, die so aussieht:
Ein Verzeichnis ocean enthält die Dateien tides.txt und wave.txt.
Wir haben eine Datei namens shark.txt, die unter dem Verzeichnis ocean, einem Verzeichnis animals und einem Verzeichnis fish geschachtelt ist: ocean / animals / fish.
Um alle .txt-Dateien im Verzeichnis ocean aufzulisten, können wir sagen:
Dieser Code würde eine Ausgabe ergeben wie:
Das glob-Muster "* .txt" findet alle Dateien, die auf .txt enden.
Da das Codebeispiel dieses Glob im Verzeichnis ocean ausführt, werden die beiden .txt-Dateien im Verzeichnis ocean zurückgegeben: wave.txt und tides.txt.
< $> note Anmerkung: Wenn Sie die in diesem Beispiel angegebenen Ausgaben replizieren möchten, müssen Sie die hier dargestellte Verzeichnisstruktur auf Ihrem Computer nachahmen.
Wir können die glob-Methode auch rekursiv verwenden.
Um alle .txt-Dateien im Verzeichnis ocean und allen seinen Unterverzeichnissen aufzulisten, können wir sagen:
Wenn wir diesen Code ausführen, erhalten wir eine Ausgabe wie die folgende:
Der * * -Teil des glob-Musters wird dieses Verzeichnis und alle Verzeichnisse darunter rekursiv abgleichen.
Wir erhalten also nicht nur die Dateien wave.txt und tides.txt in der Ausgabe, sondern auch die Datei shark.txt, die unter ocean / animals / fish geschachtelt war.
Berechnung relativer Pfade
Wir können die Methode Path.relative _ to verwenden, um Pfade in Relation zueinander zu berechnen.
Die relative _ to-Methode ist zum Beispiel nützlich, wenn Sie einen Teil eines langen Dateipfads abrufen möchten.
Erwägen Sie folgenden Code:
Die relative _ to-Methode gibt ein neues Path-Objekt in Relation zum angegebenen Argument zurück.
In unserem Beispiel berechnen wir den Pfad zu shark.txt in Relation zum Verzeichnis ocean und dann in Relation zu den Verzeichnissen ocean und animals.
Wenn relative _ to keine Antwort berechnen kann, da wir ihm einen nicht zugeordneten Pfad geben, wird ein ValueError ausgelöst:
Wir erhalten eine ValueError-Ausnahme, die von dem Code ausgelöst wurde und ungefähr so aussehen wird:
unrelated / path ist kein Teil von ocean / animals / fish / shark.txt, sodass Python keinen relativen Pfad für uns berechnen kann.
Das pathlib-Modul ist eine leistungsfähige Komponente der Python-Standardbibliothek, mit der wir Dateisystempfade bei jedem Betriebssystem schnell bearbeiten können.
In diesem Tutorial haben wir gelernt, einige der wichtigsten Dienstprogramme von pathlib zum Aufrufen von Dateiattributen, zum Auflisten von Dateien mit glob-Mustern und Durchlaufen von übergeordneten Dateien und Verzeichnissen zu verwenden.
Das pathlib-Modul macht zusätzliche Klassen und Dienstprogramme verfügbar, die wir in diesem Tutorial nicht abgedeckt haben.
Nachdem Sie nun über Grundkenntnisse verfügen, können Sie die Dokumentation des pathlib-Moduls nutzen, um mehr über andere verfügbare Klassen und Dienstprogramme zu erfahren.
Wenn Sie daran interessiert sind, andere Python-Bibliotheken zu verwenden, sehen Sie sich die folgenden Tutorials an:
Verwenden des collections-Moduls in Python 3
Verwenden des sqlite3-Moduls in Python 3
Verwenden von ThreadPoolExecutor in Python 3
Skalieren und Sichern einer Django-Anwendung mit Docker, Nginx und Let "s Encrypt
6039
In cloudbasierten Umgebungen gibt es mehrere Möglichkeiten, eine Django-Anwendung zu skalieren und zu sichern.
Durch horizontale Skalierung und die Ausführung mehrerer Kopien Ihrer Anwendung können Sie ein fehlertolerantes und hochverfügbares System aufbauen und gleichzeitig den Durchsatz erhöhen, sodass Anfragen gleichzeitig verarbeitet werden können.
Eine Möglichkeit zur horizontalen Skalierung einer Django-Anwendung besteht darin, zusätzliche App-Server bereitzustellen, die Ihre Django-Anwendung und Ihren WSGI-HTTP-Server (wie Gunicorn oder uWSGI) ausführen.
Um eingehende Anfragen über diesen Satz von App-Servern zu leiten und zu verteilen, können Sie einen Load Balancer und Reverse Proxy wie Nginx verwenden.
Nginx kann auch statische Inhalte zwischenspeichern und Transport Layer Security-Verbindungen (TLS-Verbindungen) beenden, die zur Bereitstellung von HTTPS- und sicheren Verbindungen zu Ihrer Anwendung verwendet werden.
Darüber hinaus bieten Container viele Funktionen, die das Paketieren und Konfigurieren Ihrer Anwendung erleichtern.
In diesem Tutorial skalieren Sie eine containerisierte Django- und Gunicorn-Umfrageanwendung horizontal, indem Sie zwei App-Server bereitstellen, die jeweils eine Kopie eines Django- und Gunicorn-Anwendungscontainers ausführen.
Des Weiteren aktivieren Sie HTTPS, indem Sie einen dritten Proxy-Server bereitstellen und konfigurieren, auf dem ein Nginx Reverse-Proxy-Container und ein Certbot-Client-Container ausgeführt werden.
Certbot stellt TLS-Zertifikate für Nginx von der Let "s Encrypt Zertifizierungsstelle bereit.
Dadurch wird sichergestellt, dass Ihre Website von SSL Labs eine hohe Sicherheitsbewertung erhält.
Dieser Proxy-Server empfängt alle externen Anfragen Ihrer Anwendung und sitzt vor den beiden upstream Django-App-Servern.
Schließlich härten Sie dieses verteilte System ab, indem Sie den externen Zugriff auf nur den Proxy-Server beschränken.
Drei Ubuntu 18.04-Server:
Zwei Server werden App-Server sein, die zum Ausführen Ihrer Django- und Gunicorn-Anwendung verwendet werden.
Ein Server wird ein Proxy-Server sein, auf dem Nginx und Certbot ausgeführt werden.
Alle Server sollten einen Nicht-Root-Benutzer mit sudo-Berechtigungen und eine aktive Firewall haben.
Eine Anleitung für das Setup finden Sie im Leitfaden für die Ersteinrichtung des Servers.
Auf allen drei Servern installiertes Docker.
Eine Anleitung zur Installation von Docker finden Sie in den Schritten 1 und 2 von Installieren und Verwenden von Docker unter Ubuntu 18.04.
In diesem Tutorial wird durchgängig < ^ > your _ domain.com < ^ > verwendet.
Einen Domänennamen können Sie kostenlos bei Freenom erhalten oder Sie nutzen eine Domänenregistrierungsstelle Ihrer Wahl.
Einen DNS-A-Eintrag mit < ^ > your-domain.com < ^ >, der auf die öffentliche IP-Adresse Ihres Proxy-Servers verweist.
Falls Sie ein DigitalOcean-Konto nutzen, können Sie in dieser Einführung in DigitalOcean-DNS im Einzelnen nachlesen, wie Sie ihn hinzufügen.
Einen S3-Objektspeicher-Bucket wie beispielsweise einen DigitalOcean Space zur Speicherung der statischen Dateien Ihres Django-Projekts und einen Satz von Zugriffsschlüsseln für diesen Space.
Um zu erfahren, wie Sie einen Space erstellen können, lesen Sie die Produktdokumentation Erstellen von Spaces.
Um zu erfahren, wie Sie Zugriffsschlüssel für Spaces erstellen können, lesen Sie Zugriff auf Spaces mit Zugriffsschlüsseln gemeinsam nutzen.
Mit geringfügigen Änderungen können Sie jeden Objektspeicherdienst verwenden, der das Plugin django-storages verwendet.
Die PostgreSQL-Datenbank sollte polls genannt werden (oder einen anderen einprägsamen Namen erhalten, den Sie unten in Ihre Konfigurations datei eingeben können) und in diesem Tutorial wird der Datenbankbenutzer sammy genannt.
Führen Sie zum Erstellen dieser den Schritt 1 von Erstellen einer Django- und Gunicorn-Anwendung mit Docker aus.
Sie können diese Schritte von einem beliebigen der drei Server ausführen.
In diesem Tutorial wird ein DigitalOcean Managed PostgreSQL-Cluster verwendet.
Um mehr über die Erstellung eines Clusters zu erfahren, lesen Sie die Produktdokumentation zu verwalteten Datenbanken von DigitalOcean.
Sie können auch Ihre eigene PostgreSQL-Instanz installieren und ausführen.
Eine Anleitung zur Installation und Verwaltung von PostgreSQL auf einem Ubuntu-Server finden Sie unter Installieren und Verwenden von PostgreSQL unter Ubuntu 18.04.
Schritt 1 - Konfigurieren des ersten Django-App-Servers
Wir klonen zunächst das Django-Anwendungs-Repository auf den ersten App-Server.
Dann konfigurieren und erstellen wir das Docker-Image und testen die Anwendung durch Ausführung des Django-Containers.
< $> note Anmerkung: Wenn Sie von Erstellen einer Django- und Gunicorn-Anwendung mit Docker fortfahren, haben Sie Schritt 1 bereits abgeschlossen und können mit Schritt 2 fortfahren, um den zweiten App-Server zu konfigurieren.
Beginnen Sie mit der Anmeldung beim ersten der beiden Django-App-Server und verwenden Sie git, um den Zweig polls-docker der Django Tutorial Umfrageanwendung GitHub-Repository zu klonen.
Dieses Repository enthält Code für die Umfrage-Beispielanwendung der Django-Dokumentation.
Der Zweig polls-docker enthält eine dockerisierte Version der Umfrageanwendung. Um zu erfahren, wie die Umfrageanwendung modifiziert wurde, um effektiv in einer containerisierten Umgebung zu arbeiten, lesen Sie bitte Erstellen einer Django- und Gunicorn-Anwendung mit Docker.
Navigieren Sie in das Verzeichnis django-polls:
Dieses Verzeichnis enthält den Python-Code der Django-Anwendung, ein Dockerfile, das Docker zum Erstellen des Container-Images verwendet, sowie eine Datei env, die eine Liste von Umgebungsvariablen enthält, die an die laufende Umgebung des Containers übergeben werden müssen.
Prüfen Sie das Dockerfile mit cat:
Dieses Dockerfile verwendet das offizielle Python 3.7.4 Docker-Image als Basis und installiert die Python-Paketanforderungen von Django und Gunicorn, wie sie in der Datei django-polls / requirements.txt definiert sind.
Anschließend entfernt es einige unnötige Builddateien, kopiert den Anwendungscode in das Image und legt den Ausführungspfad PATH fest.
Schließlich gibt es an, dass Port 8000 verwendet wird, um eingehende Container-Verbindungen zu akzeptieren und gunicorn mit 3 Workern ausgeführt wird, die Port 8000 abhören.
Um mehr über die einzelnen Schritte in diesem Dockerfile zu erfahren, lesen Sie bitte Schritt 6 von Erstellen einer Django- und Gunicorn-Anwendung mit Docker.
Erstellen Sie nun das Image mit docker build:
Wir benennen das Image polls mit dem Flag -t und übergeben im aktuellen Verzeichnis als Build-Kontext den Satz von Daten, auf den beim Erstellen des Images verwiesen werden soll.
Nachdem Docker das Image erstellt und mit Tags versehen hat, listen wir die verfügbaren Images mit docker images auf:
Sie sollten die polls-Images aufgelistet sehen:
Bevor wir den Django-Container ausführen, müssen wir seine Betriebsumgebung mithilfe der im aktuellen Verzeichnis vorhandenen Datei env konfigurieren.
Diese Datei wird an den Befehl docker run übergeben, der zum Ausführen des Containers verwendet wird, und Docker injiziert die konfigurierten Umgebungsvariablen in die Betriebsumgebung des Containers.
Öffnen Sie die Datei env mit nano oder Ihrem bevorzugten Editor:
Wir werden die Datei wie nachfolgend beschrieben konfigurieren und Sie müssen einige zusätzliche Werte hinzufügen.
Geben Sie die fehlenden Werte für die folgenden Schlüssel ein:
DJANGO _ SECRET _ KEY: Setzen Sie diesen auf einen eindeutigen, nicht vorhersagbaren Wert, wie in den Django-Dokumentationen beschrieben.
Eine Methode zur Generierung dieses Wertes wird in Anpassen der Anwendungseinstellungen in dem Tutorial Skalierbare Django-Anwendung angeboten.
DJANGO _ ALLOWED _ HOSTS: Diese Variable sichert die Anwendung und verhindert HTTP-Host-Header-Angriffe.
Setzen Sie diese Variable für Testzwecke auf *, einen Platzhalter, der auf alle Hosts zutrifft.
In der Produktion sollten Sie diese Variable auf < ^ > your _ domain.com < ^ > setzen.
Um mehr über diese Django-Einstellungen zu erfahren, konsultieren Sie die Core-Einstellungen der Django-Dokumentation.
DATABASE _ USERNAME: Setzen Sie diesen auf den in den vorbereitenden Schritten erstellten PostgreSQL Datenbankbenutzer.
DATABASE _ NAME: Setzen Sie diesen auf polls oder den in den vorbereitenden Schritten erstellten Namen der PostgreSQL-Datenbank.
DATABASE _ PASSWORD: Setzen Sie dieses auf das in den vorbereitenden Schritten erstellte Passwort für den PostgreSQL Benutzer.
DATABASE _ HOST: Setzen Sie diesen Wert auf den Hostnamen Ihrer Datenbank.
DATABASE _ PORT: Setzen Sie diesen Wert auf den Port Ihrer Datenbank.
STATIC _ ACCESS _ KEY _ ID: Setzen Sie diesen Wert auf den Zugriffsschlüssel Ihres S3-Buckets oder Space.
STATIC _ SECRET _ KEY: Setzen Sie diesen Wert auf das Zugriffsschlüsselgeheimnis Ihres S3-Bucket oder Space
STATIC _ BUCKET _ NAME: Setzen Sie diesen auf Ihren S3-Bucket- oder Space-Namen.
STATIC _ ENDPOINT _ URL: Setzen Sie diese auf die entsprechenden S3-Bucket- oder Space-Endpunkt-URL, z.B. https: / / < ^ > space-name < ^ > .nyc3.digitaloceanspaces.com, wenn sich Ihr Space in der Region nyc3 befindet.
Wenn Sie die Bearbeitung abgeschlossen haben, speichern und schließen Sie die Datei.
Wir verwenden nun docker run, um den CMD-Satz in dem Dockerfile zu überschreiben und das Datenbankschema mit den Befehlen manage.py makemigrations und manage.py migrate zu erstellen:
Wir führen das Container-Image polls: latest aus, übergeben die von uns gerade modifizierte Umgebungsvariablendatei und überschreiben den Dockerfile-Befehl mit sh -c "python manage.py makemigrations & & python manage.py migrate", wodurch das durch den Anwendungscode definierte Datenbankschema erstellt wird.
Wenn Sie dies zum ersten Mal ausführen, sollten Sie Folgendes sehen:
Dies zeigt an, dass das Datenbankschema erfolgreich erstellt wurde.
Wenn Sie die Migration zu einem späteren Zeitpunkt ausführen, führt Django eine Nulloperation durch, es sei denn, das Datenbankschema wurde geändert.
Als Nächstes führen wir eine weitere Instanz des Anwendungscontainers aus und verwenden darin eine interaktive Shell, um einen Administratorbenutzer für das Django-Projekt zu erstellen.
Dadurch erhalten Sie eine Shell-Eingabeaufforderung innerhalb des laufenden Containers, die Sie zum Erstellen des Django-Benutzers verwenden können:
Geben Sie einen Benutzernamen, eine E-Mail-Adresse und ein Passwort für Ihren Benutzer ein. Drücken Sie nach dem Erstellen des Benutzers STRG + D, um den Container zu verlassen und zu beenden.
Schließlich generieren wir die statischen Dateien für die Anwendung und laden sie mit collectstatic in den DigitalOcean Space hoch.
Beachten Sie, dass dies möglicherweise einige Zeit dauern kann.
Nachdem diese Dateien generiert und hochgeladen sind, erhalten Sie folgende Ausgabe.
Wir können die Anwendung nun ausführen:
Hier führen wir den in dem Dockerfile definierten Standardbefehl gunicorn ---bind: 8000 --workers 3 mysite.wsgi: application aus und stellen den Container-Port 8000 frei, sodass Port 80 auf dem Ubuntu-Server dem Port 8000 des Containers poll zugeordnet wird.
Sie sollten nun über Ihren Webbrowser zu der Anwendung polls navigieren können, indem Sie http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ > in der URL-Leiste eingeben.
Da für den Pfad / keine Route definiert ist, erhalten Sie wahrscheinlich einen 404 Page Not Found-Fehler, der zu erwarten ist.
< $> warning Warnung: Wenn Sie die UFW-Firewall mit Docker verwenden, umgeht Docker alle konfigurierten UFW-Firewallregeln, wie in diesem GitHub-Problem dokumentiert.
Dies erklärt, warum Sie Zugriff auf Port 80 Ihres Servers haben, obwohl Sie in keinem vorbereitenden Schritt explizit eine UFW-Zugriffsregel erstellt haben.
In Schritt 5 werden wir diese Sicherheitslücke schließen, indem wir die UFW-Konfiguration patchen.
Wenn Sie UFW nicht verwenden und die Cloud Firewalls von DigitalOcean einsetzen, können Sie diese Warnung getrost ignorieren.
Navigieren Sie zu http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ > / polls, um die Benutzeroberfläche der Umfrageanwendung zu sehen:
Oberfläche der Umfrageanwendung
Um die administrative Oberfläche anzuzeigen, besuchen Sie http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ > / admin.
Sie sollten das Authentifizierungsfenster für den Administrator der Umfrageanwendung sehen:
Authentifizierungsseite für Polls-Administrator
Geben Sie den administrativen Benutzernamen und das Passwort ein, das Sie mit dem Befehl createsuperuser erstellt haben.
Nach der Authentifizierung können Sie auf die administrative Oberfläche der Umfrageanwendung zugreifen:
Administrative Hauptoberfläche von Polls
Beachten Sie, dass statische Assets für die Anwendungen admin und polls direkt aus dem Objektspeicher bereitgestellt werden.
Um dies zu bestätigen, konsultieren Sie Prüfen der statischen Dateizustellung von Spaces.
Wenn Sie die Erkundung abgeschlossen haben, drücken Sie Strg + C im Terminalfenster, in dem der Docker-Container ausgeführt wird, um den Container zu beenden.
Nachdem Sie nun bestätigt haben, dass der App-Container wie erwartet ausgeführt wird, können Sie ihn im getrennten (detached) Modus ausführen, wodurch er im Hintergrund ausgeführt wird und Ihnen ermöglicht, sich von Ihrer SSH-Sitzung abzumelden:
Das Flag -d weist Docker an, den Container im getrennten Modus auszuführen, das Flag -rm säubert das Dateisystem des Containers nach dem Verlassen des Containers und wir benennen den Container polls.
Melden Sie sich von dem ersten Django App-Server ab und navigieren Sie zu http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ > / polls, um zu bestätigen, dass der Container wie erwartet ausgeführt wird.
Nachdem Ihr erster Django-App-Server ausgeführt wird, können Sie nun Ihren zweiten Django-App-Server einrichten.
Schritt 2 - Konfigurieren des zweiten Django-App-Servers
Da viele der Befehle zur Einrichtung dieses Servers die gleichen sind wie im vorherigen Schritt, werden sie hier in abgekürzter Form dargestellt. Bitte lesen Sie Schritt 1 für weitere Informationen zu einem bestimmten Befehl in diesem Schritt.
Beginnen Sie damit, sich bei dem zweiten Django-App-Server anzumelden.
Klonen Sie den Zweig polls-docker des GitHub-Repositorys von django-polls:
Erstellen Sie das Image mit docker build:
Geben Sie die fehlenden Werte wie in Schritt 1 ein. Wenn Sie die Bearbeitung abgeschlossen haben, speichern und schließen Sie die Datei.
Führen Sie den App-Container anschließend im getrennten Modus aus:
Navigieren Sie zu http: / / < ^ > APP _ SERVER _ 2 _ IP < ^ > / polls, um zu bestätigen, dass der Container wie erwartet ausgeführt wird.
Sie können sich sicher von dem zweiten App-Server anmelden, ohne Ihren laufenden Container zu beenden.
Da beide Django App-Container ausgeführt werden, können Sie mit der Konfiguration des Reverse-Proxy-Containers von Nginx fortfahren.
Schritt 3 - Konfigurieren des Nginx Docker-Containers
Nginx ist ein vielseitiger Webserver, der eine Reihe von Funktionen wie Reverse-Proxying, Load Balancing und Caching bietet.
In diesem Tutorial haben wir die statischen Assets von Django in den Objektspeicher ausgelagert, sodass wir die Caching-Funktionen von Nginx nicht verwenden werden.
Wir werden Nginx jedoch als Reverse-Proxy für unsere beiden Backend-Django-App-Server verwenden und eingehende Anfragen zwischen ihnen verteilen.
Darüber hinaus wird Nginx TLS-Terminierung und -Umleitung unter Verwendung eines von Certbot bereitgestellten TLS-Zertifikats durchführen.
Das bedeutet, dass es die Clients zwingen wird, HTTPS zu verwenden, und eingehende HTTPS-Anfragen an Port 443 umzuleiten. Anschließend entschlüsselt Nginx HTTPS-Anfragen und leitet sie an die vorgelagerten Django-Server weiter.
In diesem Tutorial haben wir die Designentscheidung getroffen, die Nginx-Container von den Backend-Servern zu entkoppeln.
Abhängig von Ihrem Anwendungsfall können Sie sich dafür entscheiden, den Nginx-Container auf einem der Django-App-Server auszuführen, das Proxying von Anfragen sowohl lokal als auch auf dem anderen Django-Server auszuführen.
Eine weitere mögliche Architektur wäre die Ausführung von zwei Nginx-Containern, einer auf jedem Backend-Server, mit einem Cloud Load Balancer davor.
Jede Architektur bietet andere Sicherheits- und Leistungsvorteile und Sie sollten Ihr System einem Lasttest unterziehen, um Engpässe aufzudecken.
Die in diesem Tutorial beschriebene flexible Architektur ermöglicht es Ihnen, sowohl die Backend-Django-Anwendungsschicht als auch die Nginx-Proxying-Schicht zu skalieren.
Sobald der einzelne Nginx-Container zum Engpass wird, können Sie auf mehrere Nginx-Proxys skalieren und einen Cloud Load Balancer oder schnellen L4 Load Balancer wie HAProxy hinzufügen.
Da beide Django-App-Server ausgeführt werden, können wir mit dem Einrichten des Proxy-Servers beginnen.
Melden Sie sich an Ihrem Proxy-Server an und erstellen Sie ein Verzeichnis namens conf:
Erstellen Sie mit nano oder Ihrem bevorzugten Editor eine Konfigurationsdatei namens nginx.conf:
Fügen Sie die folgende Nginx-Konfiguration ein:
Diese Blöcke upstream, server und location konfigurieren Nginx so, dass HTTP-Anfragen an HTTPS umgeleitet werden und sorgen für einen Lastausgleich zwischen den beiden in Schritt 1 und 2 konfigurierten Django-App-Servern. Um mehr über die Nginx-Konfigurationsdatei zu erfahren, lesen Sie bitte in diesem Artikel über das Verstehen der Nginx-Konfigurationsdateistruktur und der Konfigurationskontexte.
Außerdem kann dieser Artikel zum Verstehen von Nginx-Server und Location-Block-Auswahlalgorithmen hilfreich sein.
Diese Konfiguration wurde aus Beispielkonfigurationsdateien zusammengestellt, die von Gunicorn, Certbot und Nginx bereitgestellt wurden, und ist als eine minimale Nginx-Konfiguration gedacht, um diese Architektur betriebsbereit zu machen.
Die Feineinstellung dieser Nginx-Konfiguration geht über den Umfang dieses Artikels hinaus, Sie können jedoch ein Tool wie NGINXConfig verwenden, um performante und sichere Nginx-Konfigurationsdateien für Ihre Architektur zu generieren.
Der Block upstream definiert die Gruppe von Servern, die zum Proxying von Anfragen zur Verwendung der Anweisung proxy _ pass verwendet werden:
In diesem Block nennen wir den upstream django und schließen die IP-Adressen der beiden Django-App-Server ein.
Wenn die App-Server auf DigitalOcean ausgeführt werden und VPC Networking aktiviert haben, sollten Sie hier ihre privaten IP-Adressen verwenden.
Um zu erfahren, wie Sie VPC-Networking auf DigitalOcean aktivieren können, lesen Sie bitte Aktivieren von VPC-Networking auf vorhandenen Droplets.
Der erste Block server erfasst Anfragen, die nicht Ihrer Domäne entsprechen und beendet die Verbindung.
Beispielsweise würde eine direkte HTTP-Anfrage an die IP-Adresse Ihres Servers von diesem Block bearbeitet werden:
Der nächste Block server leitet HTTP-Anfragen an Ihre Domäne über eine HTTP 301-Umleitung an HTTPS um.
Diese Anfragen werden dann vom letzten Block server bearbeitet:
Diese zwei Anweisungen definieren die Pfade zum TLS-Zertifikat und geheimen Schlüssel.
Diese werden mit Certbot bereitgestellt und im nächsten Schritt in den Nginx-Container eingebunden.
Bei diesen Parametern handelt es sich um die von Certbot empfohlenen SSL-Sicherheitsstandards.
Um mehr über sie zu erfahren, lesen Sie bitte das Modul ngx\ _ http\ _ ssl\ _ module der Nginx-Dokumentation.
Mozillas Sicherheits- / Serverseitiges TLS ist ein weiterer hilfreicher Leitfaden, den Sie für die Feinabstimmung Ihrer SSL-Konfiguration verwenden können.
Diese beiden Anweisungen aus Gunicorns Nginx-Beispielkonfiguration legen die maximal zulässige Größe des Client-Anfragekörpers fest und weisen das Timeout für Keep-Alive-Verbindungen mit dem Client zu.
Nginx schließt Verbindungen mit dem Client nach keepalive _ timeout-Sekunden.
Der erste Block location weist Nginx zum Proxying von Anfragen über HTTP an die upstream django-Server an.
Er bewahrt zusätzlich Client HTTP-Header auf, die die Ursprungs-IP-Adresse, das zur Verbindung verwendete Protokoll und den Ziel-Host erfassen:
Um mehr über diese Anweisungen zu erfahren, lesen Sie bitte Bereitstellen von Gunicorn und das Modul ngx\ _ http\ _ proxy\ _ module der Nginx-Dokumentation.
Der letzte Block location erfasst Anfragen an den Pfad / well-known / acme-Challenge /, der von Certbot für HTTP-01-Challenges verwendet wird, um Ihre Domäne mit Let 's Encrypt zu verifizieren und TLS-Zertifikate bereitzustellen oder zu erneuern.
Weitere Informationen über die von Certbot verwendete HTTP-01-Challenge finden Sie unter Challenge-Arten in der Let "s Encrypt-Dokumentation.
Sie können diese Konfigurationsdatei nun verwenden, um einen Nginx Docker-Container auszuführen.
In diesem Tutorial verwenden wir das Image nginx: 1.19.0, Version 1.19.0 des offiziellen Docker-Images, das von Nginx verwaltet wird.
Wenn wir den Container zum ersten Mal ausführen, wird Nginx einen Fehler ausgeben und fehlschlagen, da wir die in der Konfigurationsdatei definierten Zertifikate noch nicht bereitgestellt haben.
Wir werden jedoch trotzdem den Befehl zum Herunterladen des Nginx-Images lokal ausführen und testen, ob alles andere korrekt funktioniert:
Hier nennen wir den Container nginx und ordnen die Host-Ports 80 und 443 den jeweiligen Container-Ports zu.
Das Flag -v bindet die Konfigurationsdatei unter / etc / nginx / conf.d / nginx.conf in den Nginx-Container ein, für dessen Laden das Nginx-Image vorkonfiguriert ist.
Es wird im Modus ro oder "read-only" eingebunden, sodass der Container die Datei nicht verändern kann.
Das Web-Stammverzeichnis / var / www / html ist ebenfalls in den Container eingebunden.
Schließlich weist nginx: 1.19.0 Docker an, das Image nginx: 1.19.0 aus Dockerhub zu ziehen und auszuführen.
Docker wird das Image ziehen und ausführen, dann wird Nginx einen Fehler ausgeben, wenn es das konfigurierte TLS-Zertifikat und den geheimen Schlüssel nicht findet.
Im nächsten Schritt stellen wir diese mithilfe eines dockerisierten Certbot-Clients und der Zertifizierungsstelle Let "s Encrypt bereit.
Schritt 4 - Konfigurieren von Certbot und Let "s Encrypt-Zertifikaterneuerung
Certbot ist ein Let "s Encrypt-Client, der von der Electronic Frontier Foundation entwickelt wurde.
Er stellt kostenlose TLS-Zertifikate von der Let "s Encrypt-Zertifizierungsstelle zur Verfügung, mit denen Browser die Identität Ihrer Webserver überprüfen können.
Da wir auf unserem Nginx-Proxy-Server Docker installiert haben, verwenden wir das Certbot Docker-Image zur Bereitstellung und Erneuerung der TLS-Zertifikate.
Stellen Sie zunächst sicher, dass Sie über einen DNS-A-Eintrag verfügen, der der öffentlichen IP-Adresse des Proxy-Servers zugeordnet ist.
Stellen Sie dann auf Ihrem Proxy-Server eine Staging-Version der Zertifikate unter Verwendung des Docker-Images certbot bereit:
Dieser Befehl führt das certbot Docker-Image im interaktiven Modus aus und leitet Port 80 auf dem Host an den Container-Port 80 weiter. Er erstellt zwei Host-Verzeichnisse und bindet sie in die Container ein: / etc / letsencrypt / und / var / lib / letsencrypt /. certbot wird im Modus standalone ohne Nginx ausgeführt und verwendet die Staging-Server von Let "s Encrypt, um die Domänenvalidierung durchzuführen.
Geben Sie, wenn Sie dazu aufgefordert werden, Ihre E-Mail-Adresse ein und stimmen Sie den Nutzungsbedingungen zu.
Wenn die Domänenvalidierung erfolgreich war, sollten Sie die folgende Ausgabe sehen:
Sie können das Zertifikat mit cat inspizieren:
Mit dem bereitgestellten TLS-Zertifikat können wir die im vorherigen Schritt eingebundene Nginx-Konfiguration testen:
Dies ist derselbe Befehl, der in Schritt 3 ausgeführt wurde, mit dem Hinzufügen der beiden kürzlich erstellten Let "s Encrypt-Verzeichnisse:
Sobald Nginx ausgeführt wird, navigieren Sie zu http: / / < ^ > your _ domain.com < ^ >.
Möglicherweise erhalten Sie in Ihrem Browser eine Warnung, dass die Zertifizierungsstelle ungültig ist.
Dies ist zu erwarten, da wir Staging-Zertifikate und keine Produktions-Let "s Encrypt-Zertifikate bereitgestellt haben.
Überprüfen Sie die URL-Leiste Ihres Browsers, um zu bestätigen, dass Ihre HTTP-Anfrage an HTTPS umgeleitet wurde.
Drücken Sie zum Beenden von Nginx in Ihrem Terminal Strg + C und führen Sie den Client certbot erneut aus, diesmal ohne das Flag --staging:
Wenn Sie dazu aufgefordert werden, entweder das vorhandene Zertifikat beizubehalten oder es zu erneuern und zu ersetzen, drücken Sie 2 zum Erneuern und dann ENTER, um Ihre Wahl zu bestätigen.
Wenn das Produktions-TLS-Zertifikat bereitgestellt ist, führen Sie den Nginx-Server erneut aus:
Navigieren Sie in Ihrem Browser zu http: / / < ^ > your _ domain.com < ^ >.
Bestätigen Sie in der URL-Leiste, dass die HTTP-Anfrage an HTTPS umgeleitet wurde.
Da für die Umfrageanwendung keine Standardroute konfiguriert ist, sollten Sie den Django-Fehler Page not found (Seite nicht gefunden) sehen.
Navigieren Sie zu https: / / < ^ > your _ domain.com < ^ > / polls und Sie sehen die Standardoberfläche der Umfrageanwendung:
Zu diesem Zeitpunkt haben Sie mit dem Certbot Docker-Client ein Produktions-TLS-Zertifikat bereitgestellt und externe Anfragen an die beiden Django-App-Server durch Reverse Proxying und Lastausgleich umgeleitet.
Die Let "s Encrypt-Zertifikate laufen alle 90 Tage ab.
Um sicherzustellen, dass Ihr Zertifikat gültig bleibt, sollten Sie es regelmäßig vor dessen geplantem Ablauf erneuern.
Wenn Nginx ausgeführt wird, sollten Sie den Certbot-Client im Modus webroot anstelle des Modus standalone verwenden.
Das bedeutet, dass Certbot die Validierung durch die Erstellung einer Datei im Verzeichnis / var / www / html / .well-known / acme-challenge / durchführt, und die Validierungsanforderungen von Let "s Encrypt an diesen Pfad werden von der in der Nginx-Konfiguration in Schritt 3 definierten Regel location erfasst. Certbot wird dann die Zertifikate rotieren, und Sie können Nginx neu laden, sodass es dieses neu bereitgestellte Zertifikat verwendet.
Es gibt mehrere Möglichkeiten, um dieses Verfahren zu automatisieren und die automatische Erneuerung von TLS-Zertifikaten geht über den Umfang dieses Tutorials hinaus.
Ein ähnliches Verfahren unter Verwendung des Scheduling-Dienstprogramms cron finden Sie in Schritt 6 von Sichern einer containerisierten Node.js-Anwendung mit Nginx, Let "s Encrypt, und Docker Compose.
Drücken Sie zum Beenden des Nginx-Containers in Ihrem Terminal Strg + C.
Führen Sie ihn erneut im getrennten Modus aus, indem Sie das Flag -d anhängen:
Verwenden Sie mit im Hintergrund ausgeführtem Nginx den folgenden Befehl, um einen Probelauf des Zertifikatserneuerungsverfahrens auszuführen:
Wir verwenden das Plugin --webroot, geben den Web-Stammpfad ein und verwenden das Flag --dry-run zum Überprüfen der ordnungsgemäßen Funktion, ohne die Zertifikatserneuerung tatsächlich durchzuführen.
Wenn die Erneuerungssimulation erfolgreich ist, sollten Sie die folgende Ausgabe sehen:
In einer Produktionseinstellung sollten Sie nach der Erneuerung von Zertifikaten Nginx neu laden, damit die Änderungen wirksam werden.
Führen Sie zum Neuladen von Nginx den folgenden Befehl aus:
Dieser Befehl sendet ein HUP Unix-Signal an den Nginx-Prozess, der innerhalb des Docker-Containers nginx ausgeführt wird.
Nach Empfang dieses Signals lädt Nginx seine Konfiguration und erneuerten Zertifikate neu.
Wenn HTTPS aktiviert ist und alle Komponenten dieser Architektur ausgeführt werden, besteht der letzte Schritt darin, die Einrichtung zu sperren, indem der externe Zugriff auf die beiden Backend-App-Server verhindert wird; alle HTTP-Anfragen sollten über den Nginx-Proxy laufen.
Schritt 5 - Verhindern des externen Zugriffs auf Django-App-Server
In der in diesem Tutorial beschriebenen Architektur erfolgt die SSL-Terminierung am Nginx-Proxy.
Das bedeutet, dass Nginx die SSL-Verbindung entschlüsselt und die Pakete unverschlüsselt an die Django-App-Server weitergeleitet werden.
Für viele Anwendungsfälle ist diese Sicherheitsstufe ausreichend.
Für Anwendungen mit Finanz- oder Gesundheitsdaten sollten Sie eventuell eine End-to-End-Verschlüsselung implementieren.
Sie können dies tun, indem Sie verschlüsselte Pakete über den Load Balancer weiterleiten und auf den App-Servern entschlüsseln oder am Proxy neu verschlüsseln und auf den Django-App-Servern wieder entschlüsseln.
Diese Techniken gehen über den Rahmen dieses Artikels hinaus. Um mehr zu erfahren, lesen Sie bitte End-to-End-Verschlüsselung.
Der Nginx-Proxy fungiert als Gateway zwischen externem Datenverkehr und dem internen Netzwerk.
Theoretisch sollten keine externen Clients direkten Zugriff auf die internen App-Server haben, und alle Anfragen sollten über den Nginx-Server laufen.
Der Hinweis in Schritt 1 beschreibt kurz ein offenes Problem mit Docker, bei dem Docker standardmäßig die ufw-Firewalleinstellungen umgeht und Ports extern öffnet, die möglicherweise unsicher sind.
Um dieses Sicherheitsproblem zu beheben wird empfohlen, bei der Arbeit mit Docker-fähigen Servern Cloud Firewalls zu verwenden.
Weitere Informationen zum Erstellen von Cloud Firewalls mit DigitalOcean finden Sie unter Erstellen von Firewalls.
Sie können iptables auch direkt manipulieren, anstatt ufw zu verwenden.
Um mehr über die Verwendung von iptables mit Docker zu erfahren, lesen Sie bitte Docker und iptables.
In diesem Schritt ändern wir die UFW-Konfiguration so, dass der externe Zugriff auf die von Docker geöffneten Host-Ports blockiert wird.
Bei der Ausführung von Django auf den App-Servern haben wir das Flag -p 80: 8000 an docker übergeben, das Port 80 auf dem Host an den Container-Port 8000 weiterleitet.
Dadurch wurde Port 80 auch für externe Clients geöffnet, was Sie unter http: / / < ^ > your _ app _ server _ 1 _ IP < ^ > überprüfen können.
Um den direkten Zugriff zu verhindern, ändern wir die UFW-Konfiguration mithilfe der im ufw-docker GitHub-Repository beschriebenen Methode.
Beginnen Sie damit, sich bei dem ersten Django-App-Server anzumelden.
Öffnen Sie dann die Datei / etc / ufw / after.rules mit superuser-Berechtigungen, indem Sie nano oder Ihren bevorzugten Editor verwenden:
Geben Sie bei Aufforderung Ihr Passwort ein und drücken Sie zur Bestätigung ENTER.
Sie sollten die folgenden ufw-Regeln sehen:
Scrollen Sie nach unten und fügen Sie den folgenden Block mit UFW-Konfigurationsregeln ein:
Diese Regeln schränken den öffentlichen Zugriff auf die von Docker geöffneten Ports ein und ermöglichen den Zugriff aus den privaten IP-Bereichen 10.0.0 / 8, 172.16.0.0 / 12 und 192.168.0.0 / 16.
Wenn Sie VPC mit DigitalOcean verwenden, dann haben Droplets in Ihrem VPC-Netzwerk über die private Netzwerkschnittstelle Zugriff auf den offenen Port, externe Clients jedoch nicht.
Weitere Informationen über VPC finden Sie in der offiziellen VPC-Dokumentation.
Um mehr über die in diesem Snippet implementierten Regeln zu erfahren, lesen Sie bitte Funktionsweise
in der ufw-docker README.
Wenn Sie VPC nicht mit DigitalOcean verwenden und die öffentlichen IP-Adressen der App-Server in den Block upstream Ihrer Nginx-Konfiguration eingegeben haben, müssen Sie die UFW-Firewall explizit ändern, um den Datenverkehr vom Nginx-Server über Port 80 auf den Django-App-Servern zuzulassen.
Eine Anleitung zur Erstellung von allow-Regeln mit der UFW-Firewall finden Sie unter UFW Grundlagen: Allgemeine Firewallregeln und -befehle.
Wenn Sie die Bearbeitung abgeschlossen haben, speichern und schließen Sie die Datei.
Starten Sie ufw neu, damit die neue Konfiguration übernommen wird:
Navigieren Sie in Ihrem Webbrowser zu http: / / < ^ > APP _ SERVER _ 1 _ IP < ^ >, um zu bestätigen, dass Sie über Port 80 nicht mehr auf die App-Server zugreifen können.
Wiederholen Sie diesen Vorgang auf dem zweiten Django-App-Server.
Melden Sie sich bei dem ersten App-Server ab oder öffnen Sie ein anderes Terminalfenster und melden Sie sich bei dem zweiten Django-App-Server an.
Navigieren Sie in Ihrem Webbrowser zu http: / / < ^ > APP _ SERVER _ 2 _ IP < ^ >, um zu bestätigen, dass Sie über Port 80 nicht mehr auf die App-Server zugreifen können.
Navigieren Sie abschließend zu https: / / < ^ > your _ domain _ here < ^ > / polls, um zu bestätigen, dass der Nginx-Proxy weiterhin Zugriff auf die upstream Django-Server hat.
Sie sollten die Standardoberfläche der Umfrageanwendung sehen.
In diesem Tutorial haben Sie mit Docker-Containern eine skalierbare Django Umfrageanwendung eingerichtet.
Wenn Ihr Datenverkehr steigt und die Last auf dem System zunimmt, können Sie jede Schicht separat skalieren: die Nginx-Proxying-Schicht, die Django-Backend-Anwendungsschicht und die PostgreSQL-Datenbankschicht.
Beim Aufbau eines verteilten Systems müssen Sie oft mehrere Designentscheidungen treffen, und mehrere Architekturen können Ihrem Anwendungsfall gerecht werden.
Die in diesem Tutorial beschriebene Architektur ist als flexible Blaupause für den Entwurf skalierbarer Anwendungen mit Django und Docker gedacht.
Möglicherweise möchten Sie das Verhalten Ihrer Container steuern, wenn sie auf Fehler stoßen, oder Container automatisch ausführen, wenn Ihr System gestartet wird.
Zu diesem Zweck können Sie einen Prozessmanager wie Systemd verwenden oder Neustartrichtlinien implementieren.
Weitere Informationen hierzu finden Sie unter Automatisches Starten von Containern in der Docker-Dokumentation.
Wenn Sie im großen Maßstab mit mehreren Hosts arbeiten, die dasselbe Docker-Image ausführen, kann es effizient sein, Schritte mit einem Konfigurations-Managementtool wie Ansible oder Chef zu automatisieren.
Um mehr über das Konfigurationsmanagement zu erfahren, lesen Sie bitte Eine Einführung in das Konfigurationsmanagement und Automatisieren der Servereinrichtung mit Ansible: Ein DigitalOcean-Workshop-Kit.
Anstatt auf jedem Host dasselbe Image zu erstellen, können Sie die Bereitstellung auch mithilfe einer Image-Registrierung wie Docker Hub rationalisieren, bei der Docker-Images zentral erstellt, gespeichert und an mehrere Server verteilt werden.
Zusammen mit einer Image-Registrierung kann Ihnen eine kontinuierliche Integrations- und Bereitstellungspipeline dabei helfen, Images zu erstellen, zu testen und auf Ihre App-Server zu verteilen.
Weitere Informationen zu CI / CD finden Sie unter Eine Einführung in die CI / CD Best Practices.
Installieren und Konfigurieren von Postfix als Send-Only-SMTP-Server unter Ubuntu 20.04
6158
Das ist in Fällen nützlich, in denen Sie regelmäßig E-Mail-Benachrichtigungen von Ihren Anwendungen senden oder über viel ausgehenden Datenverkehr verfügen, den E-Mail-Drittanbieter nicht zulassen.
Ein Ubuntu 20.04-Server, der gemäß Ersteinrichtung eines Servers unter Ubuntu 20.04 eingerichtet wurde, einschließlich eines non-root user mit sudo-Berechtigungen.
In diesem Schritt konfigurieren Sie Postfix so, dass E-Mails nur von dem Server gesendet und empfangen werden, auf dem Postfix ausgeführt wird - d. h. von localhost.
Dazu muss Postfix so konfiguriert werden, dass nur an der Loopback-Schnittstelle gelauscht wird; das ist die virtuelle Netzwerkschnittstelle, die der Server zur internen Kommunikation verwendet.
Eine weitere Anweisung, die Sie ändern müssen, ist mydestination; sie gibt die Liste der Domänen an, die über den Mail Delivery Transport local _ transport bereitgestellt werden.
Wenn Ihre Domäne in Wahrheit eine Subdomäne ist und Sie möchten, dass E-Mail-Nachrichten aussehen, als wären sie von der Hauptdomäne gesendet worden, können Sie am Ende von main.cf die folgende Zeile hinzufügen:
Die optionale Einstellung masquerade _ domains gibt an, bei welchen Domänen der Subdomänenteil in der E-Mail-Adresse entfernt wird.
Wenn Sie einen Fehler vom Befehl mail erhalten oder auch nach längerer Zeit keine Nachricht empfangen haben, dann vergewissern Sie sich, dass die von Ihnen bearbeitete Postfix-Konfiguration gültig ist und der Name sowie Hostname Ihres Servers auf Ihre Domäne festgelegt sind.
Die einzige vorhandene Anweisung gibt an, dass systemgenerierte E-Mails an root gesendet werden.
In diesem Schritt haben Sie eine Weiterleitung systemgenerierter Nachrichten an Ihre E-Mail-Adresse eingerichtet.
Ubuntu enthält Certbot in seinen standardmäßigen Paket-Repositorys, sodass Sie für dessen Installation den folgenden Befehl ausführen können:
Wenn Sie zur Bestätigung aufgefordert werden, geben Sie J ein und drücken Sie die Eingabetaste.
Ändern Sie ihn, damit er wie folgt aussieht, wobei Sie < ^ > your _ domain < ^ > ggf. durch Ihre Domäne ersetzen.
Dadurch werden Ihre TLS-Einstellungen für Postfix aktualisiert:
Das Verschlüsseln aller ausgehenden Nachrichten ist ein guter erster Schritt, damit E-Mail-Anbieter Ihre Nachrichten nicht von vornherein als Spam markieren.
Wenn Ihr Anwendungsfall jedoch darin besteht, E-Mails an potenzielle Websitebenutzer zu senden (wie Bestätigungs-E-Mails für die Anmeldung bei einem Nachrichtenforum), sollten Sie sich mit der Einrichtung von SPF-Einträgen befassen, damit E-Mails Ihres Servers mit noch höherer Wahrscheinlichkeit als legitim gelten.
Auslesen einer Website mit Node.js und Puppeteer
6187
Der Autor wählte den Free and Open Source Fund, um eine Spende im Rahmen des Programms Write for DOnations zu erhalten.
Web Scraping ist ein Prozess zur Automatisierung der Erfassung von Daten aus dem Web.
Der Prozess stellt in der Regel einen "Crawler" bereit, der automatisch im Web surft und Daten von ausgewählten Seiten ausliest.
Es gibt viele Gründe dafür, Daten auslesen zu wollen. In erster Linie macht das die Erfassung von Daten wesentlich schneller, da der manuelle Datenerfassungsprozess eliminiert wird.
Zudem ist Scraping eine Lösung, wenn Datenerfassung gewünscht oder benötigt wird, die Website jedoch keine API bereitstellt.
In diesem Tutorial erstellen Sie mit Node.js und Puppeteer eine Web-Scraping-Anwendung.
Ihre Anwendung wird mit dem Fortschreiten komplexer.
Zuerst werden Sie Ihre Anwendung so codieren, dass sie Chromium öffnet und eine spezielle Website lädt, die als Web-Scraping-Sandbox konzipiert ist: books.toscrape.com.
In den nächsten beiden Schritten werden Sie alle Bücher auf einer Seite von books.toscrape und dann alle Bücher über mehrere Seiten hinweg auslesen.
In den verbleibenden Schritten filtern Sie Ihr Scraping nach Buchkategorie und speichern Ihre Daten dann als JSON-Datei.
Warnung: Die Ethik und Legalität von Web Scraping sind sehr komplex und entwickeln sich ständig weiter.
Sie unterscheiden sich außerdem je nach Ihrem Standort, dem Standort der Daten und der entsprechenden Website.
Dieses Tutorial wird eine spezielle Website (books.toscrape.com) ausgelesen, die speziell zum Testen von Scraper-Anwendungen entwickelt wurde.
Das Scraping anderer Domänen geht über den Umfang dieses Tutorials hinaus.
Dieses Tutorial wurde unter Node.js Version 12.18.3 und npm Version 6.14.6 getestet.
Sie können diesem Leitfaden folgen, um Node.js unter macOS oder Ubuntu 18.04 zu installieren, bzw. diesem Leitfaden folgen, um Node.js unter Ubuntu 18.04 mit einem PPA zu installieren.
Schritt 1 & mdash; Einrichten des Web Scrapers
Wenn Node.js installiert ist, können Sie mit dem Einrichten Ihres Web Scrapers beginnen.
Zuerst erstellen Sie ein root-Verzeichnis für das Projekt und installieren dann die erforderlichen Abhängigkeiten.
Dieses Tutorial erfordert nur eine Abhängigkeit. Sie installieren sie mit dem standardmäßigen Paketmanager npm von Node.js. Bei npm ist Node.js vorinstalliert, sodass Sie keine Installation mehr vornehmen müssen.
Erstellen Sie einen Ordner für dieses Projekt und öffnen Sie ihn:
Sie werden alle folgenden Befehle aus diesem Verzeichnis ausführen.
Wir müssen mit npm oder dem Node-Paketmanager ein Paket installieren.
Initialisieren Sie zunächst npm, um eine Datei namens packages.json zu erstellen, die Abhängigkeiten und Metadaten Ihres Projekts verwalten wird.
Initialisieren Sie npm für Ihr Projekt:
npm wird eine Sequenz von Eingabeaufforderungen anzeigen.
Sie können bei jeder Eingabeaufforderung auf ENTER drücken oder personalisierte Beschreibungen hinzufügen.
Stellen Sie sicher, dass Sie ENTER drücken und die Standardwerte lassen, wie sie sind, wenn Sie zur Eingabe von entry point und test command aufgefordert werden.
Alternativ können Sie das y-Flag an npm & mdash; npm init -y & mdash; übergeben; damit werden alle Standardwerte für Sie übergeben.
Ihre Ausgabe wird etwa wie folgt aussehen:
Geben Sie yes ein und drücken Sie ENTER. npm speichert diese Ausgabe als Ihre package.json-Datei.
Verwenden Sie nun npm zum Installieren von Puppeteer:
Dieser Befehl installiert sowohl Puppeteer als auch eine Version von Chromium, von der das Puppeteer-Team weiß, dass sie mit der API funktionieren wird.
Auf Linux-Rechnern kann Puppeteer möglicherweise zusätzliche Abhängigkeiten benötigen.
Wenn Sie Ubuntu 18.04 verwenden, lesen Sie das Dropdown 'Debian Dependencies' im Abschnitt 'Chrome headless doesn' t launch on UNIX 'in den Fehlerbehebungsdokumenten von Puppeteer.
Sie können folgenden Befehl nutzen, um fehlende Abhängigkeiten zu finden:
Nachdem npm, Puppeteer und weitere Abhängigkeiten installiert sind, erfordert Ihre package.json-Datei eine letzte Konfiguration, bevor Sie mit dem Codieren beginnen.
In diesem Tutorial starten Sie Ihre Anwendung mit npm run start aus der Befehlszeile.
Sie müssen einige Informationen über dieses start-Skript zu package.json hinzufügen.
Vor allem müssen Sie unter der scripts-Anweisung eine Zeile in Bezug auf Ihren start-Befehl hinzufügen.
Öffnen Sie die Datei in Ihrem bevorzugten Texteditor:
Suchen Sie nach dem Abschnitt scripts: und fügen Sie die folgenden Konfigurationen hinzu.
Denken Sie daran, am Ende der test-Skriptzeile ein Komma zu platzieren; sonst wird Ihre Datei wird nicht korrekt analysiert.
Sie werden auch feststellen, dass puppeteer nun unter dependencies am Ende der Datei erscheint.
Ihre package.json-Datei wird keine weiteren Überarbeitungen mehr benötigen.
Speichern Sie Ihre Änderungen und schließen Sie den Editor.
Sie sind nun bereit, Ihren Scraper zu codieren.
Im nächsten Schritt werden Sie eine Browserinstanz einrichten und die grundlegende Funktionalität Ihres Scrapers testen.
Schritt 2 & mdash; Einrichten der Browserinstanz
Wenn Sie einen traditionellen Browser öffnen, können Sie auf Schaltflächen klicken, mit Ihrer Maus navigieren, Text eingeben, dev-Tools öffnen und mehr.
Ein Headless-Browser wie Chromium ermöglicht Ihnen, dasselbe zu tun, aber programmatisch und ohne Benutzeroberfläche.
In diesem Schritt werden Sie die Browserinstanz Ihres Scrapers einrichten.
Wenn Sie Ihre Anwendung starten, öffnet sie Chromium automatisch und navigiert zu books.toscrape.com.
Diese anfänglichen Aktionen werden die Grundlage Ihres Programms bilden.
In diesem Schritt erstellen Sie alle vier Dateien und aktualisieren sie dann weiter, sobald Ihr Programm komplexer wird.
Beginnen Sie mit browser.js; diese Datei enthält das Skript, das Ihren Browser startet.
Erstellen und öffnen Sie browser.js in einem Texteditor aus dem root-Verzeichnis Ihres Projekts:
Zuerst werden Sie Puppeteer erfordern und dann eine async-Funktion namens startBrowser () erstellen.
Diese Funktion startet den Browser und gibt eine Instanz davon zurück. Fügen Sie folgenden Code hinzu:
Diese Methode gibt eine Zusage zurück; Sie müssen also sicherstellen, dass die Zusage mit einem .then- oder await-Block aufgelöst wird.
Sie verwenden await um sicherzustellen, dass die Zusage aufgelöst wird, indem diese Instanz um einen try-catch-Codeblock eingeschlossen und dann eine Instanz des Browsers zurückgeben wird.
Beachten Sie, dass die Methode .launch () einen JSON-Parameter mit mehreren Werten nutzt:
headless - false bedeutet, dass der Browser mit einer Oberfläche ausgeführt wird, sodass Sie Ihr Skript bei der Ausführung sehen können; true hingegen bedeutet, dass der Browser im headless-Modus ausgeführt wird.
Beachten Sie unbedingt, dass Sie bei Bereitstellen Ihres Scrapers in der Cloud headless auf true zurücksetzen müssen.
Die meisten virtuellen Rechner sind headless und enthalten keine Benutzeroberfläche; daher kann der Browser nur im headless-Modus ausgeführt werden.
Puppeteer umfasst auch einen headful-Modus, der aber ausschließlich für Testzwecke verwendet werden sollte.
ignoreHTTPSErrors - true ermöglicht Ihnen, Websites zu besuchen, die nicht über ein sicheres HTTPS-Protokoll gehostet werden, und jegliche HTTPS-Fehler zu ignorieren.
Erstellen Sie nun Ihre zweite .js-Datei, index.js:
Hier werden Sie browser.js und pageController.js erfordern.
Dann werden Sie die Funktion startBrowser () aufrufen und die erstellte Browserinstanz an den Seitencontroller übergeben, der die Aktionen steuern wird.
Erstellen Sie Ihre dritte .js-Datei, pageController.js:
pageController.js steuert Ihren Scraping-Prozess.
Es verwendet die Browserinstanz zum Steuern der Datei pageScraper.js, in der alle Scraping-Skripte ausgeführt werden.
Schließlich werden Sie sie verwenden, um anzugeben, welche Buchkategorie Sie auslesen möchten.
Sie wollen jedoch zunächst sicherstellen, dass Sie Chromium öffnen und zu einer Webseite navigieren können:
Dieser Code exportiert eine Funktion, die die Browserinstanz nimmt und an eine Funktion namens scrapeAll () übergibt.
Diese Funktion wiederum übergibt diese Instanz an pageScraper.scraper () als Argument, das sie zum Auslesen von Seiten verwendet wird.
Erstellen Sie schließlich Ihre letzte .js-Datei, pageScraper.js:
Hier erstellen Sie ein Objektliteral mit einer url-Eigenschaft und einer scraper () -Methode.
Die url ist die Web-URL der Webseite, die Sie auslesen möchten, während die Methode scraper () den Code enthält, der das tatsächliche Scraping ausführt; in diesem Stadium navigiert sie jedoch lediglich zu einer URL.
Puppeteer hat eine newPage () -Methode, die eine neue Seiteninstanz im Browser erstellt. Diese Seiteninstanzen können einiges erledigen.
In unserer scraper () -Methode haben Sie eine Seiteninstanz erstellt und dann die Methode page.goto () verwendet, um zur Homepage books.toscrape.com zu navigieren.
Die Dateistruktur Ihres Programms ist nun fertig.
Die erste Ebene der Verzeichnisstruktur Ihres Projekts wird wie folgt aussehen:
Führen Sie nun den Befehl npm run start aus und sehen Sie, wie Ihre Scraper-Anwendung ausgeführt wird:
Dadurch wird automatisch eine Chromium-Browserinstanz geöffnet, im Browser eine neue Seite geöffnet und zu books.toscrape.com navigiert.
In diesem Schritt haben Sie eine Puppeteer-Anwendung erstellt, die Chromium geöffnet und die Homepage für einen Dummy-Online-Buchladen (books.toscrape.com) geladen hat.
Im nächsten Schritt werden Sie die Daten für jedes einzelne Buch auf dieser Homepage auslesen.
Schritt 3 & mdash; Auslesen von Daten von einer einzelnen Seite
Bevor Sie Ihrer Scraper-Anwendung mehr Funktionen hinzufügen, öffnen Sie Ihren bevorzugten Webbrowser und navigieren Sie manuell zur Books-to-scrape-Homepage.
Durchsuchen Sie die Website und finden Sie heraus, wie die Daten strukturiert sind.
Bild der Books-to-scrape-Websites
Sie finden links einen Kategorienabschnitt und rechts Bücher.
Wenn Sie auf ein Buch klicken, navigiert der Browser zu einer neuen URL, die relevante Informationen zu diesem bestimmten Buch anzeigt.
In diesem Schritt werden Sie dieses Verhalten replizieren, aber mit Code; Sie werden das Navigieren der Website automatisieren und deren Daten konsumieren.
Wenn Sie zunächst mithilfe der Dev-Tools in Ihrem Browser den Quellcode für die Homepage prüfen, werden Sie feststellen, dass die Seite Daten der einzelnen Bücher unter einem section Tag auflistet.
Im Inneren des section-Tags befindet sich jedes Buch unter einem list (li) -Tag; hier finden Sie den Link zur Seite des jeweiligen Buchs, den Preis und die Verfügbarkeit.
books.toscrape-Quellcode, in dev-Tools angezeigt
Sie werden diese Buch-URLs auslesen, nach vorrätigen Büchern filtern, zur Seite des jeweiligen Buchs navigieren und Daten des Buchs auslesen.
Öffnen Sie erneut Ihre pageScraper.js-Datei:
Fügen Sie den folgenden hervorgehobenen Inhalt hinzu:
Sie werden einen weiteren await-Block in der Datei await page.goto (this.url); schachteln:
In diesem Codeblock haben Sie die Methode page.waitForSelector () aufgerufen.
Diese hat, bis div alle buchbezogenen Daten enthält, die im DOM dargestellt werden sollen; dann haben Sie die Methode page. $$eval () aufgerufen.
Diese Methode ruft das URL-Element mit dem Selektor section ol li ab (Sie sollten sicherstellen, dass Sie aus den Methoden page. $$eval () und page. $eval () immer nur eine Zeichenfolge oder Zahl zurückgeben.
Jedes Buch verfügt über zwei Status; ein Buch ist entweder In Stock (Vorrätig) oder Out of stock (Nicht vorrätig).
Sie möchten nur Bücher auslesen, die In Stock sind.
Da page. $$eval () ein Array aller übereinstimmenden Elemente zurückgibt, haben Sie dieses Array gefiltert, um sicherzustellen, dass Sie nur mit vorrätigen Büchern arbeiten.
Sie haben dazu die Klasse .instock.availability gesucht und ausgewertet.
Sie haben dann die Eigenschaft href der Buchlinks zugeordnet und aus der Methode zurückgegeben.
Führen Sie Ihre Anwendung neu aus:
Der Browser öffnet sich, navigiert zur Webseite und schließt nach Abschluss der Aufgabe.
Überprüfen Sie nun Ihre Konsole; sie enthält alle ausgelesenen URLs:
Das ist ein guter Anfang; Sie möchten jedoch alle relevanten Daten für ein bestimmtes Buch und nicht nur dessen URL auslesen.
Sie werden nun diese URLs verwenden, um die einzelnen Seiten zu öffnen und Titel, Autor, Preis, Verfügbarkeit, UPC, Beschreibung und Bild-URL auszulesen.
Öffnen Sie pageScraper.js neu:
Fügen Sie den folgenden Code hinzu, der die einzelnen ausgelesenen Links durchläuft, eine neue Seiteninstanz öffnen und dann die entsprechenden Daten abruft:
Sie verfügen über ein Array mit allen URLs.
Sie möchten dieses Array durchlaufen, die URL in einer neuen Seite öffnen, Daten auf dieser Seite auslesen, diese Seite schließen und eine neue Seite für die nächste URL im Array öffnen.
Beachten Sie, dass Sie diesen Code in einer Zusage eingeschlossen haben.
Das liegt daran, dass Sie warten möchten, bis jede Aktion in Ihrer Schleife abgeschlossen ist.
Daher wird jede Zusage eine neue URL öffnen und erst aufgelöst, wenn das Programm alle Daten in der URL ausgelesen hat und die Seiteninstanz geschlossen wurde.
Achtung: Beachten Sie, dass Sie mit einer for-in-Schleife auf die Zusage gewartet haben.
Jede andere Schleife wird ausreichen; vermeiden Sie jedoch, mithilfe einer array-iteration-Methode wie forEach oder einer anderen Methode, die eine Callback-Funktion verwendet, über Ihre URL-Arrays zu iterieren.
Die Callback-Funktion muss nämlich zunächst die Callback-Warteschlange und die Ereignisschleife durchlaufen; daher werden mehrere Seiteninstanzen auf einmal geöffnet.
Dadurch wird Ihr Arbeitsspeicher deutlich stärker belastet.
Werfen Sie einen genaueren Blick auf Ihre pagePromise-Funktion.
Ihr Scraper hat zunächst für jede URL eine neue Seite erstellt und dann die Funktion page. $eval () verwendet, um Selektoren für relevante Details auszuwählen, die Sie auf der neuen Seite auslesen möchten.
Einige der Texte enthalten Leerzeichen, Tabs, Zeilenumbrüche und andere nicht-alphanumerische Zeichen, die Sie mit einem regulären Ausdruck verworfen haben.
Sie haben dann den Wert für jedes Datenelement, das in dieser Seite ausgelesen wurde, einem Objekt angefügt und dieses Objekt aufgelöst.
Führen Sie das Skript erneut aus:
Der Browser öffnet die Homepage, öffnet dann jede Buchseite und protokolliert die ausgelesenen Daten der einzelnen Seiten.
Diese Ausgabe wird in Ihrer Konsole ausgedruckt:
In diesem Schritt haben Sie für jedes Buch auf der Homepage von books.toscrape.com relevante Daten ausgelesen; Sie können jedoch noch viel mehr Funktionen hinzufügen.
Jede Seite mit Büchern zum Beispiel ist paginiert; wie erhalten Sie Bücher von diesen anderen Seiten?
Außerdem haben Sie auf der linken Seite der Website Buchkategorien gefunden; was ist, wenn Sie gar nicht alle Bücher sehen möchten, sondern lediglich Bücher aus einem bestimmten Genre?
Sie werden nun die entsprechenden Funktionen hinzufügen.
Schritt 4 & mdash; Auslesen von Daten von verschiedenen Seiten
Seiten auf books.toscrape.com, die paginiert sind, verfügen unter ihrem Inhalt über eine next-Schaltfläche; Seiten, die nicht paginiert sind, haben das nicht.
Sie werden anhand der Anwesenheit dieser Schaltfläche erkennen, ob eine Seite paginiert ist oder nicht.
Da die Daten auf jeder Seite dieselbe Struktur haben und das gleiche Markup aufweisen, müssen Sie nicht für jede mögliche Seite einen eigenen Scraper schreiben.
Vielmehr werden Sie die Praxis der Rekursion nutzen.
Zuerst müssen Sie die Struktur Ihres Codes etwas ändern, um eine rekursive Navigation zu mehreren Seiten zu ermöglichen.
Öffnen Sie pagescraper.js erneut:
Sie werden eine neue Funktion namens scrapeCurrentPage () zu Ihrer scraper () -Methode hinzufügen.
Diese Funktion wird den gesamten Code enthalten, der Daten von einer bestimmten Seite ausliest, und dann auf die next-Schaltfläche klicken (so vorhanden).
Fügen Sie den folgenden hervorgehobenen Code hinzu:
Sie setzen die nextButtonExist-Variable zunächst auf false und überprüfen dann, ob die Schaltfläche vorhanden ist.
Wenn die next-Schaltfläche vorhanden ist, haben Sie nextButtonExists auf true gesetzt; klicken Sie dann auf die next-Schaltfläche und rufen diese Funktion rekursiv auf.
Wenn nextButtonExists false ist, wird wie gewöhnlich das Array scrapedData zurückgegeben.
Führen Sie Ihr Skript erneut aus:
Das kann eine Weile dauern; Ihre Anwendung liest nun schließlich Daten von über 800 Büchern aus.
Sie können entweder den Browser schließen oder Strg + C drücken, um den Prozess zu beenden.
Sie haben nun die Funktionen Ihres Scrapers maximiert, dabei aber ein neues Problem geschaffen.
Jetzt besteht das Problem nicht aus zu wenig Daten, sondern aus zu viel Daten. Im nächsten Schritt werden Sie Ihre Anwendung so anpassen, dass Ihr Scraping nach Buchkategorien gefiltert wird.
Schritt 5 & mdash; Auslesen von Daten nach Kategorie
Um Daten nach Kategorien auszulesen, müssen Sie sowohl Ihre pageScraper.js - als auch Ihre pageController.js-Datei ändern.
Öffnen Sie pageController.js in einem Texteditor:
Rufen Sie den Scraper so auf, dass nur Reisebücher ausgelesen werden.
Sie übergeben nun zwei Parameter in Ihre pageScraper.scraper () -Methode, wobei der zweite Parameter die Kategorie von Büchern ist, die Sie auslesen möchten; in diesem Beispiel: Travel.
Doch erkennt Ihre pageScraper.js-Datei diesen Parameter noch nicht.
Sie müssen auch diese Datei anpassen.
Öffnen Sie pageScraper.js:
Fügen Sie den folgenden Code hinzu, der Ihren Kategorieparameter hinzufügen wird, navigieren zu dieser Kategorieseite und dann lesen Sie dann die paginierten Ergebnisse aus:
Dieser Codeblock verwendet die von Ihnen übergebene Kategorie, um die URL zu erhalten, in der sich die Bücher dieser Kategorie befinden.
Die page. $$eval () -Methode kann Argumente übernehmen, indem das Argument als dritter Parameter an die $$eval () -Methode übergeben und so als dritter Parameter im Callback definiert wird:
Genau das haben Sie in Ihrem Code getan; Sie haben die Kategorie von Büchern übergeben, die Sie auslesen möchten, sind alle Kategorien durchlaufen um zu überprüfen, welche übereinstimmt, und dann die URL dieser Kategorie zurückgegeben.
Diese URL wird dann verwendet, um zu der Seite zu navigieren, die die Kategorie von Büchern anzeigt, die Sie mithilfe der Methode page.goto (selectedCategory) auslesen möchten.
Führen Sie Ihre Anwendung erneut aus.
Sie werden feststellen, dass sie zur Kategorie Travel navigiert, Bücher in dieser Kategorieseite nach Seite rekursiv öffnet und die Ergebnisse protokolliert:
In diesem Schritt haben Sie Daten über mehrere Seiten hinweg ausgelesen und dann Daten aus einer bestimmten Kategorie über mehrere Seiten hinweg ausgelesen.
Im letzten Schritt werden Sie Ihr Skript so ändern, dass Daten über mehrere Kategorien hinweg ausgelesen und dann in einer Zeichenfolgen-förmigen JSON-Datei gespeichert werden.
Schritt 6 & mdash; Auslesen von Daten aus verschiedenen Kategorien und Speichern der Daten als JSON
In diesem letzten Schritt werden Sie dafür sorgen, dass Ihr Skript Daten aus so vielen Kategorien abliest, wie Sie möchten, und dann die Art der Ausgabe ändern.
Anstatt die Ergebnisse zu protokollieren, speichern Sie sie in einer strukturierten Datei namens data.json.
Sie können schnell mehr Kategorien zum Auslesen hinzufügen; dazu ist nur eine zusätzliche Zeile pro Genre erforderlich.
Öffnen Sie pageController.js:
Passen Sie Ihren Code so an, dass zusätzliche Kategorien enthalten sind.
Das folgende Beispiel fügt HistoricalFiction und Mystery zu unserer vorhandenen Travel-Kategorie hinzu:
Führen Sie das Skript erneut aus und sehen Sie, wie Daten für alle drei Kategorien ausgelesen werden:
Da der Scraper nun voll funktional ist, besteht Ihr letzter Schritt darin, Ihre Daten in einem nützlicheren Format zu speichern.
Sie werden sie jetzt in einer JSON-Datei mit dem fs-Modul in Node.js speichern.
Öffnen Sie zunächst pageController.js neu:
Zuerst fordern Sie das fs-Modul von Node.js in pageController.js an.
Dadurch wird sichergestellt, dass Sie Ihre Daten als JSON-Datei speichern können.
Dann fügen Sie Code hinzu, damit das Programm eine neue Datei namens data.json erstellt, sobald das Scraping abgeschlossen ist und der Browser schließt.
Beachten Sie, dass der Inhalt von data.json Zeichenfolgen-förmiges JSON ist.
Wenn Sie den Inhalt von data.json lesen, analysieren Sie ihn also immer als JSON, bevor Sie die Daten erneut verwenden.
Sie haben nun eine Web-Scraping-Anwendung erstellt, die Bücher über verschiedene Kategorien hinweg ausliest und die ausgelesenen Daten dann in einer JSON-Datei speichert.
Wenn Ihre Anwendung komplexer wird, möchten Sie die ausgelesenen Daten möglicherweise in einer Datenbank speichern oder über eine API bereitstellen.
Wie diese Daten verbraucht werden, liegt ganz bei Ihnen.
In diesem Tutorial haben Sie einen Web Crawler erstellt, der Daten über mehrere Seiten rekursiv ausliest und dann in einer JSON-Datei speichert.
Kurz gesagt: Sie haben eine neue Methode erlernt, um die Datenerfassung von Websites zu automatisieren.
Puppeteer hat eine Menge von Funktionen, die im Rahmen dieses Tutorials nicht abgedeckt wurden.
Um mehr darüber zu erfahren, lesen Sie Verwenden von Puppeteer für einfache Kontrolle über Headless Chrome.
Sie können auch die offizielle Dokumentation von Puppeteer besuchen.
Verwenden von Font Awesome 5 mit React
Font Awesome ist ein Toolkit für Websites, das Symbole und soziale Logos zur Verfügung stellt.
React ist eine Programmierbibliothek, die JavaScript zur Erstellung von Benutzeroberflächen verwendet.
Obwohl das Font Awesome-Team eine React-Komponente zur Förderung der Integration entwickelt hat, gibt es einige Grundlagen über Font Awesome 5 und seine Strukturierung zu verstehen.
In diesem Tutorial lernen Sie, wie Sie die Komponente React Font Awesome verwenden können.
3618
Font Awesome ist ein Toolkit für Websites, das Symbole und soziale Logos zur Verfügung stellt.
React ist eine Programmierbibliothek, die zur Erstellung von Benutzeroberflächen verwendet wird.
Obwohl das Font Awesome-Team eine React-Komponente zur Förderung der Integration entwickelt hat, gibt es einige Grundlagen über Font Awesome 5 und seine Strukturierung zu verstehen.
In diesem Tutorial erfahren Sie, wie Sie die Komponente React Font Awesome verwenden können.
Font Awesome-Website mit ihren Symbolen
Für dieses Tutorial ist keine Programmierung erforderlich. Wenn Sie jedoch mit einigen der Beispiele experimentieren möchten, benötigen Sie Folgendes:
Create React-App. Folgen Sie hierfür Einrichten eines React-Projekts.
Schritt 1 - Verwenden von Font Awesome
Das Font Awesome-Team hat eine React-Komponente erstellt, damit Sie beide zusammen verwenden können.
Mit dieser Bibliothek können Sie dem Tutorial folgen, nachdem Sie Ihr Symbol ausgewählt haben.
In diesem Beispiel verwenden wir das Home-Symbol und führen alles in der Datei App.js aus:
Ihre Anwendung verfügt nun über ein kleines Home-Symbol.
Sie werden feststellen, dass dieser Code nur das Home-Symbol auswählt, sodass nur ein Symbol zu unserer Bundle-Größe hinzugefügt wird.
Code-Sandbox mit angezeigtem Home-Symbol
Nun wird Font Awesome dafür sorgen, dass diese Komponente sich durch die SVG-Version dieses Symbols ersetzt, sobald diese Komponente installiert ist.
Schritt 2 - Auswählen der Symbole
Vor dem Installieren und Verwenden der Symbole ist es wichtig zu wissen, wie die Font Awesome Bibliotheken strukturiert sind.
Da es viele Symbole gibt, hat das Team beschlossen, sie in mehrere Pakete aufzuteilen.
Bei der Auswahl und Entscheidung über die gewünschten Symbole empfiehlt es sich, die Seite Font Awesome Icons zu besuchen, um Ihre Optionen zu erkunden.
Auf der linken Seite der Seite sehen Sie verschiedene Filter, aus denen Sie auswählen können.
Diese Filter sind sehr wichtig, da sie angeben, aus welchem Paket Ihr Symbol importiert werden soll.
Im obigen Beispiel haben wir das Home-Symbol aus dem Paket @ fortawesome / free-solid-svg-icons abgerufen.
Bestimmen, zu welchem Paket ein Symbol gehört
Sie können herausfinden, zu welchem Paket ein Symbol gehört, indem Sie sich die Filter auf der linken Seite ansehen.
Sie können auch in ein Symbol klicken und das Paket sehen, zu dem es gehört.
Sobald Sie wissen, zu welchem Paket eine Schriftart gehört, ist es wichtig, sich die dreibuchstabige Kurzform für dieses Paket zu merken:
Stil "Solid" - fas
Stil "Regular" - far
Stil "Light" - fal
Stil "Duotone" - fad
Auf der Symbole-Seite können Sie nach einem bestimmten Typ suchen:
Symbol-Seite mit einigen der Paketnamen auf der linken Seite
Verwenden von Symbolen aus bestimmten Paketen
Wenn Sie die Font Awesome Symbole-Seite durchblättern, werden Sie feststellen, dass es normalerweise mehrere Versionen desselben Symbols gibt.
Sehen wir uns beispielsweise das Symbol boxing-glove an:
Boxhandschuh-Symbol, drei verschiedene Versionen
Um ein bestimmtes Symbol zu verwenden, müssen Sie < FontAwesomeIcon > anpassen.
Im Folgenden werden mehrere Arten des gleichen Symbols aus verschiedenen Paketen angezeigt.
Dazu gehören die bereits erwähnten dreibuchstabigen Kurzformen.
Anmerkung: Die folgenden werden nicht funktionieren, bis wir in einigen Abschnitten eine Symbol-Bibliothek aufgebaut haben.
Hier ist ein Beispiel für die "Solid" -Version:
Wenn kein Typ angegeben ist, wird standardmäßig die "Solid" -Version verwendet:
Und die "Light" -Version, unter Verwendung von fal:
Wir mussten unser icon-Prop zu einem Array statt einer einfachen Zeichenfolge ändern.
Schritt 3 - Installieren von Font Awesome
Da es mehrere Versionen eines Symbols, mehrerer Pakete und kostenlose / Pro-Pakete gibt, erfordert das Installieren aller mehr als ein npm-Paket.
Möglicherweise müssen Sie mehrere installieren und dann die gewünschten Symbole auswählen.
Für diesen Artikel installieren wir alles, damit wir zeigen können, wie mehrere Pakete installiert werden.
Führen Sie den folgenden Befehl aus, um die Basispakete zu installieren:
Führen Sie die folgenden Befehle aus, um die regulären Symbole zu installieren:
Dadurch werden die "Solid" -Symbole (gefüllte Symbole) installiert:
Verwenden Sie diesen Befehl für "Light" -Symbole (Symbole mit dünnem Rand):
Dies installiert die "Duotone" -Symbole (Symbole mit zwei Farben):
Zum Schluss installiert dies die Marken-Symbole:
Oder, wenn Sie es vorziehen, alle auf einmal zu installieren, können Sie diesen Befehl verwenden, um die kostenlosen Symbol-Sätze zu installieren:
Wenn Sie ein Pro-Konto mit Font Awesome haben, können Sie den folgenden Befehl verwenden, um alle Symbole zu installieren:
Sie haben die Pakete installiert, aber sie noch nicht in Ihrer Anwendung verwendet oder zu unseren App-Bundles hinzugefügt.
Sehen wir uns im nächsten Schritt an, wie Sie dies tun können.
Schritt 4 - Erstellen einer Symbol-Bibliothek
Es kann mühsam sein, das gewünschte Symbol in mehrere Dateien zu importieren.
Angenommen, Sie verwenden das Twitter-Logo an mehreren Stellen, dann wollen Sie das nicht mehrfach schreiben müssen.
Um alles an einem Ort zu importieren, anstatt jedes Symbol in jede separate Datei zu importieren, erstellen wir eine Font Awesome Bibliothek.
Erstellen wir fontawesome.js im Ordner src und importieren diese dann in index.js.
Es steht Ihnen frei, diese Datei an beliebiger Stelle hinzuzufügen, solange die Komponenten, in denen Sie die Symbole verwenden möchten, Zugriff haben (untergeordnete Komponenten sind).
Sie können dies sogar direkt in Ihrer index.js oder App.js tun.
Es kann jedoch besser sein, dies in eine separate Datei auszulagern, da es groß werden kann:
Wenn Sie dies in einer eigenen Datei getan haben, müssen Sie in index.js importieren:
Importieren eines kompletten Symbol-Pakets
Das Importieren eines kompletten Pakets wird nicht empfohlen, da Sie jedes einzelne Symbol in Ihre Anwendung importieren, wodurch ein großes Bundle entstehen kann.
Wenn Sie ein komplettes Paket importieren müssen, können Sie dies selbstverständlich tun.
Nehmen wir in diesem Beispiel an, Sie wollen alle Marken-Symbole in @ fortawesome / free-brands-svg-icons haben.
Um das komplette Paket zu importieren, würden Sie Folgendes verwenden:
fab repräsentiert das gesamte Markensymbol-Paket.
Importieren einzelner Symbole
Es wird empfohlen, die Symbole von Font Awesome einzeln zu importieren, damit Ihre endgültigen Bundle-Größen so klein wie möglich sind, da Sie nur das importieren, was Sie benötigen.
Auf diese Weise können Sie eine Bibliothek aus mehreren Symbolen aus den verschiedenen Paketen erstellen:
Importieren desselben Symbols aus mehreren Stilen
Wenn Sie alle Arten von boxing-glove für die Pakete fal, far und fas wünschen, können Sie sie alle unter einem anderen Namen importieren und dann hinzufügen.
Sie können sie dann verwenden, indem Sie die verschiedenen Präfixe implementieren:
Schritt 5 - Verwenden von Symbolen
Nachdem Sie nun alles Notwendige installiert haben und Ihre Symbole zu Ihrer Font Awesome Bibliothek hinzugefügt haben, können Sie sie verwenden und Größen zuweisen.
In diesem Tutorial verwenden wir das Paket "light" (fal).
Dieses erste Beispiel verwendet die normale Größe:
Das zweite Beispiel kann benannte Größenanpassung verwenden, wobei Abkürzungen für klein (sm), mittel (md), groß (lg) und extragroß (xl) verwendet werden:
Die dritte Möglichkeit ist die Verwendung der nummerierten Größenanpassung, die bis zu 6 gehen kann:
Wenn Sie die nummerierte Größenanpassung verwenden, können Sie auch Dezimalzahlen verwenden, um die perfekte Größe zu finden:
Font Awesome formatiert die von ihm verwendeten SVGs, indem es die Textfarbe der CSS übernimmt.
Wenn Sie ein Tag < p > an der Stelle platzieren würden, an der das Symbol erscheinen soll, wäre die Farbe des Absatzes die Farbe des Symbols:
Font Awesome verfügt auch über eine Power Transforms-Funktion, mit der Sie verschiedene Transformationen aneinander reihen können:
Sie können jede der Transformationen verwenden, die auf der Website Font Awesome zu finden sind.
Damit können Sie Symbole nach oben, unten, links oder rechts verschieben, um eine perfekte Positionierung neben dem Text oder innerhalb von Schaltflächen zu erzielen.
Symbole mit fester Breite
Wenn Symbole an einer Stelle verwendet werden, an der sie alle gleich breit und einheitlich sein müssen, können wir mit Font Awesome das Prop fixedWidth verwenden.
Nehmen wir zum Beispiel an, dass Sie feste Breiten für Ihre Navigation-Dropdown-Liste benötigen:
Scotch Website mit Menü-Dropdown und "Courses" hervorgehoben
Sich drehende Symbole
Das Implementieren von sich drehenden Symbolen auf Formularschaltflächen ist hilfreich, wenn ein Formular verarbeitet wird.
Sie können das Dreh-Symbol verwenden, um einen schönen Ladeeffekt zu erzielen:
Sie können das Prop spin auf allem Möglichen verwenden!
Erweitert: Maskierende Symbole
Mit Font Awesome können Sie zwei Symbole kombinieren, um Effekte mit Maskierung zu erzielen.
Sie definieren Ihr normales Symbol und verwenden dann das Prop mask, um ein zweites Symbol zu definieren, das darüber gelegt wird.
Das erste Symbol wird innerhalb des Maskierungssymbols beschränkt.
In diesem Beispiel haben wir Tag-Filter mit Maskierung erstellt:
Tag-Filter mit Font Awesome
Beachten Sie, wie Sie mehrere Props transform aneinanderketten können, um das innere Symbol so zu verschieben, dass es in das Maskierungssymbol passt.
Wir färben und ändern mit Font Awesome sogar das Hintergrundlogo:
Erneut die Tag-Filter, aber jetzt mit einem blauen Hintergrund
Schritt 6 - Verwenden von react-fontawesome und Symbolen außerhalb von React
Wenn Ihre gesamte Website keine einseitige Anwendung (Single Page Application, SPA) ist und Sie stattdessen eine traditionelle Website und React darüber hinzugefügt haben.
Um den Import der SVG / JS-Hauptbibliothek und auch der Bibliothek react-fontawesome zu vermeiden, hat Font Awesome eine Möglichkeit geschaffen, die React-Bibliotheken zu verwenden, um auf Symbole außerhalb der React-Komponenten zu achten.
Wenn Sie eine < i class = "fas fa-stroopwafel" > < / i > haben, können wir Font Awesome mit dem Folgenden anweisen, diese zu überwachen und zu aktualisieren:
MutationObserver ist eine Webtechnologie, die es uns ermöglicht, DOM auf Änderungen performant zu überwachen.
Weitere Informationen über diese Technik finden Sie in den Dokumenten zu React Font Awesome.
Die gemeinsame Verwendung von Font Awesome und React ist eine großartige Kombination, macht es jedoch erforderlich, mehrere Pakete zu verwenden und verschiedene Kombinationen zu berücksichtigen.
In diesem Tutorial haben Sie einige der Möglichkeiten untersucht, wie Sie Font Awesome und React zusammen verwenden können.
